index,text
26220,accurately predicting spatio temporal patterns of algal bloom is important and also challenging this study developed a three dimensional hydro ecological dynamics model 3dhed to predict cyanobacterial biomass in lakes and applied ensemble kalman filter to assimilate multi source data into 3dhed for model improvement the model was applied in lake taihu using in situ measurements and remote sensing rs retrievals two data assimilation experiments named enkf1 and enkf2 were conducted enkf1 assimilated only in situ measurements while enkf2 assimilated both in situ measurements and rs data the results revealed that 3dhed simulated the spatio temporal patterns of cyanobacterial biomass in taihu with an acceptable index of agreement ioa enkf1 significantly improved the model fitness and increased the ioa of 85 measurement sites to 0 85 especially better captured the peak values compared with enkf1 enkf2 gave more improvements in spatial patterns besides model fitness implying that assimilating multi source data was helpful to improving the model performance keywords cyanobacterial biomass three dimensional ecological dynamics model data assimilation ensemble kalman filter 1 introduction during the past decades harmful algal blooms habs have been reported throughout the world duan et al 2009 michalak et al 2013 glibert et al 2014 degraded water quality due to increased nutrient pollution is one of the reasons for the expansion of habs heisler et al 2008 habs are characterized by the proliferation and occasional dominance of particular species of toxic or harmful algae that can lead to adverse impacts such as oxygen depletion from the decay of biomass the death of higher organisms and occasionally human poisoning via the ingestion of contaminated food sources allen et al 2008 huppert et al 2005 since the 1990s algal blooms in lake taihu have significantly affected industrial and agricultural production as well as the life of city dwellers in the taihu basin for instance algal blooms led to massive mortality of fish and shrimp and triggered a serious drinking water crisis in the summer of 1998 huang et al 2014 during late may to early june of 2007 lake taihu received infamous recognition when a monstrous cyanobacterial bloom deprived 5 million people of drinking water in wuxi city guo 2007 yang et al 2008 the monitoring and prediction of habs has become an urgent task for risk management many hab forecasting models have been developed to simulate phytoplankton dynamics in lakes and can be divided into two categories empirical statistical models and process based models the empirical statistical models e g multivariate linear regression mlr piecewise regression auto regressive integrated moving average arima and artificial neural networks ann have been widely developed and applied to hab forecasting recknagel 1997 chen and mynett 2004 lui et al 2007 chen et al 2015a b however these empirical statistical models have the weakness of lacking physical mechanisms their performance is highly dependent on the measured data used for model development process based models such as efdc hamrick 1992 wasp ambrose et al 1993 ce qualw2 cole and buchak 1995 and delft3d eco los et al 2008 have been increasingly developed to describe algae dynamics and predict habs these models have the potential to track the entire development of a bloom thus allowing early warning of its likely timing position magnitude and duration davidson et al 2016 from the 1970s process based models have progressed from a single element to a variety of elements a local process to a systematic process and from homogeneity to heterogeneity in space and time and have achieved remarkable success in many lakes hood et al 2006 zhang et al 2013 chen et al 2015a b although most process based models cannot distinguish algae species and the model solving as well as parameters calibration are complicated smayda 1997 roelke and buyukates 2001 process based models are widely used in many lakes with severe algal blooms such as lake taihu for simulating hydrodynamics water quality and phytoplankton dynamics hu et al 2006 mao et al 2008 huang et al 2012 zhang et al 2013 zhang 2014 li et al 2015 janssen et al 2017 among these studies salmo simulation by means of an analytical lake model model describes the seasonal dynamics of po4 p no3 n detritus chlorophyta bacillariophyta cyanophyta and cladocerans of the epilimnion and hypolimnion of stratified lakes by complex ordinary differential equations chen et al 2014a zhang 2014 established a three dimensional hydro ecological dynamics model 3dhed by coupling the lake ecosystem model salmo with the hydrodynamic model selfe semi implicit eulerian lagrangian finite element to simulate the spatio temporal dynamics of phosphate nitrate dissolved oxygen zooplankton biomass and algal biomass of three functional groups diatoms green algae and blue green algae however due to model conceptualization parameter calibration boundary configuration and etc and the prediction accuracy of the model is still poor in parallel with model development multi source observation data provide a direct way to monitor habs traditional ship borne or laboratory borne methods for examining algal blooms are constrained in both space and time due to the coarse sampling frequencies and limited sampling points remote sensing technology provides rapid synchronic and ongoing water property data overcoming the shortcomings of traditional methods lou and hu 2014 shang et al 2017 making it a potential choice for detecting and monitoring algal blooms so far various satellite sensors had been used for studying ocean color and for monitoring inland water quality such as the moderate resolution imaging spectroradiometer modis becker et al 2009 the medium resolution imaging spectrometer meris wynne et al 2010 hu et al 2010 duan et al 2017 and landsat data vincent et al 2004 ekstrand 1992 however the quality of optical satellite data is usually affected by the cloud cover and other weather conditions especially in the water surface which makes it difficult to obtain time series data for instant habs detections given that both process based models and multi source observation data have their own advantages and shortcomings combining process based models and observation data to improve the accuracy of algal blooms prediction is increasingly viable data assimilation technology is one of the best ways to achieve this goal by merging various measurement data into models data assimilation has been demonstrated to improve model reliability and reduce model uncertainties in broad fields evensen 2003 liu et al 2012 and different assimilation techniques have been developed and applied chen 2013 seo et al 2014 franssen and neuweiler 2015 the ensemble kalman filter enkf proposed by evensen 1994 is one of the most promising data assimilation techniques for updating model parameters and variables in real time it has the advantages of solving non linear problems and reducing the computation load in data assimilation chen et al 2015a b hut et al 2015 applications of enkf are significantly increased in recent years due to the quick expansion of available data particularly in hydrological ecological and environmental modeling eknes and evensen 2002 hut et al 2015 panzeri et al 2015 sun et al 2015 huang and gao 2017 these studies have demonstrated the high potential of enkf in improving model performance however assimilating multi source data into process based models using enkf is so far scarcely investigated the objectives of this study are 1 to evaluate the model performance of 3dhed in predicting cyanobacterial biomass in lake taihu 2 to assimilate multi source data satellite image and measured data into 3dhed using enkf 3 and to compare the effects of different source data collected from in situ surveys and meris on the assimilation system this is so far the first study that assimilate both measured data and satellite images to improve accuracy as well as spatial patterns of algal bloom modeling in lakes 2 materials and methods 2 1 study area lake taihu located in the yangtze river delta southeast china n 30 55 40 31 32 58 e 119 52 32 120 36 10 is one of the three largest freshwater lakes in china fig 1 taihu is usually divided into eight regions zhushan bay meiliang bay gonghu bay nw lake sw lake east lake central lake and xuhu bay with a water surface area about 2338 km2 and mean water depth of 1 89 m it is a typically shallow lake the lake is of great social and economic importance because it is the key source of fresh water for four province level regions in eastern china it also plays an essential role in tourism and ecological functions thus the water quality in lake taihu has always been under the research spotlight shi et al 2017 yang et al 2016 otten et al 2012 lake taihu has an annually average air temperature of 16 c and a maximum air temperature of 39 4 c in summer the water temperature can reach 30 c along with sufficient sunlight and low flow velocity forming a very suitable environment for cyanobacteria growth liu et al 2014 accelerated eutrophication and cyanobacterial blooms have occurred since the 1980s due to the rapid increase in agricultural and industrial development which has led to dramatic rise of nutrient loading into the lake qin et al 2007 hu et al 2010 habs have most frequently occurred in spring and summer however recent studies have reported an expansion of the spatial distribution of the cyanobacterial bloom surface as well as an increasing frequency of habs zhang and chen 2011 shi et al 2017 2 2 datasets from in situ observation and remote sensing two datasets in situ observation and remote sensing were collected for lake taihu from january 2009 to december 2011 the in situ observation dataset included hydrological and water quality data monthly hydrological data discharge and water level of the main in out flow rivers were collected the daily water level was monitored at 4 stations dapukou xishan jiapu and xiaomeikou water quality data were collected from 18 sites in rivers and 29 sites in the lake fig 1 and table 1 the water quality data included water transparency sd water temperature wt illumination i ph concentrations of suspended solids ss dissolved oxygen do nitrates no3 n nitrites no2 n ammonium nitrogen nh4 n total nitrogen tn total phosphorus tp phosphate po4 chlorophyll a chla diatom biomass green algae biomass and cyanobacterial biomass remote sensing imagery were derived from the medium resolution imaging spectrometer instrument meris the meris sensor provides radiance measurements with a ground spatial resolution of 300 m a temporal resolution of 3 days and 15 spectral bands in the spectral range of 412 5 900 nm with narrow bandwidths shi et al 2014 the operation period of the meris mission envisat was from march 1 2002 to april 8 2012 all the meris images from january 1 2010 to december 31 2011 were collected in this study 37 images without cloud cover were selected for cyanobacterial biomass retrieval there were in total 37 images being used for chla concentrations retrieval the datasets are available on http merisfrs merci ds eo esa int merci so far several algorithms such as eutrophic lake eul free university of berlin fub fluorescence line height flh maximum chlorophyll index mci and maximum peak height mph had been proposed to retrieve chla concentrations or chla related products from the meris data salem et al 2017 according to the previous research in lake taihu qi et al 2015 the mci algorithm was adopted to estimate the near surface chlorophyll a concentrations in the study more details could be seen in qi et al 2015 the chlorophyll a concentration can then be converted to algal biomass concentration based on kasprzak et al 2008 since the 1990s the biomass concentration of cyanobacteria has been observed in lake taihu nearly throughout the year since cyanobacteria is the dominant species during the bloom period and the highest ratio of cyanobacteria biomass concentration has reached 91 6 of the total algal biomass concentration in lake taihu gu et al 2011 using chla to estimate cyanobacterial biomass is reasonable li 2015 found an empirical relationship between the retrieved chla from meris and in situ cyanobacteria biomass in lake taihu 1 c h l c b l u e 0 03 g c h l g b l u e where c b l u e represents the cyanobacterial biomass μg l g c h l and g b l u e are the weight of chlorophyll a and cyanobacteria respectively μg 2 3 three dimensional hydro ecological dynamics model 3dhed to describe the spatio temporal pattern of cyanobacterial biomass a three dimensional 3d hydrodynamic model selfe zhang and baptista 2008 was coupled with a process based ecological model salmo benndorf and recknagel 1982 the selfe is able to estimate water level 3d velocity salinity and water temperature by solving the 3d shallow water equations it is widely used in simulations of three dimensional baroclinic circulation around the world liu et al 2008 roland et al 2012 the salmo model is a deterministic model that describes the dynamics of po4 p no3 n do detritus and biomass of cyanobacteria green algae and diatoms through a time dependent ordinary differential equation ode based on mass balance previous studies have proved the applicability of this model in eutrophication simulation of many lakes and reservoirs all over the world chen et al 2014a recknagel et al 2008 the basic idea for coupling these two models is to use the hydrodynamic outputs of selfe as the inputs to salmo however the spatial dimension is different for these two models selfe is a 3d model while salmo is a zero dimensional model therefore a reduction is assumed for salmo such that all variables and parameters are well mixed in each cell of selfe each discrete cell in selfe is treated as an independent lake so that the salmo model can be applied at the beginning only the selfe model was run until it reached the required accuracy and stability at time t then salmo read the results of selfe as inputs to start calculation from time t the combination of selfe and salmo avoids solving the complicated 3d partial differential equations for mass and momentum conservations and thereby significantly improved the computational efficiency in addition the coupled model has the advantage of distinguishing the biomass of different functional groups of algae compared to the traditional eutrophication model 2 4 data assimilation multi source data in situ observation and remote sensing data were assimilated into 3dhed to update the simulated cyanobacterial biomass fig 2 by using the enkf method enkf assimilation includes forecast and analysis steps at the forecast step the simulated cyanobacterial biomass for each ensemble member at time t δt is made as follows 2 c ˆ t δ t i m c ˆ t i u t δ t i θ t i ξ t δ t i 3 u t δ t i u t δ t 1 η i 4 θ t i θ t 1 σ i 5 ξ t δ t i n 0 ξ where c ˆ t δ t i is the updated cyanobacterial biomass at time t δt m c ˆ t i u t δ t i θ t i is the simulated cyanobacterial biomass from 3dhed u t δ t i is the forcing data with noise a zero mean random variable with a normal distribution and covariance η the forcing data of this model include solar radiation and water temperature a parameter ensemble θ t i is produced by adding σ i noise a zero mean random variable with a normal distribution and covariance σ to the initial value of θ t the temperature dependent growth rate of cyanobacteria which was sensitive to model performance was selected as the parameter for updating recknagel 1984 huang et al 2013 li et al 2015 ξ t δ t i noise a zero mean random variable with a normal distribution and covariance ξ represents the uncertainty in the model structure in the analysis step the ensemble of parameter and variable was updated by 6 θ t δ t i θ t i k t δ t c t δ t i c ˆ t δ t i where k i 1 is kalman gain for correcting the parameter trajectories and is calculated by 7 k t δ t t δ t θ c t δ t c c t δ t c ˆ c ˆ 1 where t δ t c c and t δ t c ˆ c ˆ are the error covariance of c t δ t and c ˆ t δ t respectively t δ t θ c is the cross covariance of parameter ensemble and prediction ensemble the performance of data assimilation is highly related to the parameter values in enkf including the ensemble size model error and observation error ensemble size was generally determined by testing the assimilation performance with different values model and observation errors were generally determined empirically in previous studies chen et al 2015a b huang et al 2013 sun et al 2015 because it is very difficult to know the true value of observation and the model structure error in this study six 30 50 70 100 200 and 500 ensemble sizes and four 1 10 20 and 30 observation in situ observation and remote sensing errors and model errors were tested to choose the optimal settings for enkf the testing results showed that the assimilation results remained stable when the ensemble size was set as 100 the rmse value was the smallest when the model error was 10 and observation error in situ observation and remote sensing was 1 this parameter set was consistent with previous studies liu et al 2017 based on the above setting three different modeling exercises sim enkf1 and enkf2 were carried out to simulate cyanobacterial biomass in lake taihu the simulation time period was from january 1 2009 to december 31 2011 before the assimilation the 3dhed model was run for one year to make it stable in sim data assimilation was not applied in enkf1 29 sampling points were interpolated into all grid cells when the in situ observations were available the interpolated data were used as the initial field to calculate the cyanobacterial biomass at the next time step in enkf2 both in situ observations and meris remote sensing estimates were assimilated into the 3dhed model because the concentration of cyanobacterial biomass is low from january to may in lake taihu the assimilation of enkf2 started from the beginning of june the assimilation time was from june 2010 to june 2011 during this time period 11 days in situ measured data and 37 days remote sensing data were available table 3 the meris data were interpolated to match the corresponding meshes of the 3dhed model the model performance of sim enkf1 and enkf2 was evaluated by the root mean square error rmse and the index of agreement ioa rmse reflects the deviation of the estimation compared to the observation while ioa reflects the uniformity of the estimation and observation calculation details of rmse and ioa can be found in previous studies qi et al 2014 huang and gao 2017 3 results 3 1 validation of the three dimensional hydro ecological dynamics model 3dhed the salmo model included 104 parameters in this study the initial values of these parameters were obtained from guo et al 2012 which was acceptable because guo et al 2012 calibrated 20 key parameters related to algal growth in meiliang bay the mostly representative and concerned area of lake taihu fig 3 showed the validation of the 3dhed model at four typical monitoring station dapukou xishan jiapu and xiaomeikou in lake taihu it was seen that the simulated and observed water levels at the four stations were consistent during 2009 2011 the relatively high r 2 0 78 0 81 indicated that the model boundary conditions the initial conditions and the parameters were set properly a widely refereed site sanhaobiao was selected to show the simulation results of po4 p no3 n zooplankton do diatoms green algae cyanobacteria and total algae fig 4 it could be seen that the dynamics of do po4 p no3 n and green algae concentrations were relatively well simulated however no3 n concentration during the summer and autumn in 2011 was over estimated the trends of the simulated biomass of cyanobacterial and total algae were similar and consistent with the measured data however the peak values were not well captured in addition the seasonal dynamics of zooplankton was not well simulated one sampling site in each lake area boldfaced sites in table 1 was selected to calculate the rmse and ioa values tables 2 and 3 and each site included 36 monthly water quality data from january 2009 to december 2011 the concentrations of major variables in the 3dhed model such as po4 p no3 n zooplankton do diatoms green algae cyanobacteria and total algal biomass were evaluated to assess model performance relatively low rmse and high ioa were obtained for po4 p no3 n and do in particular the mean ioa of do 0 85 was the highest among all the variables the mean rmse of cyanobacteria 14 23 mg l and total algae 16 11 mg l were almost at the same level and much higher than that of green algae 2 18 mg l indicating that cyanobacteria was the main component of the total algae in lake taihu overall the simulation of cyanobacterial biomass concentration was not as good as that of other variables as shown by the relatively low ioa in summary the 3dhed model used in this study can effectively simulate the spatio temporal pattern of the hydrodynamics nutrients and algal biomass concentrations in lake taihu the simulated results of each monitoring stations coincided with the dynamic change of the measured values indicating that 3dhed is applicable to eutrophication simulation in lake taihu however the plankton peaks which are the most serious concern in algal bloom forecasting were not well captured 3 2 data assimilation based on in situ observations fig 5 showed the cyanobacterial biomass from observation obs and two simulations sim and enkf1 at eight stations of each lake region in lake taihu it could be seen that the accuracy of cyanobacterial biomass simulation was markedly improved by enkf1 in which the state variables and parameters of the model were updated simultaneously compared with the simulation without data assimilation sim enkf1 better simulated the dynamic changes and the peak values of cyanobacterial biomass however it is important to note that some peak values e g peak value in 2010 at pingtaishan of cyanobacterial biomass were not well captured fig 6 showed the ioa of the simulation without data assimilation sim and the ioa improvement of the enkf1 the blue column bar represented the ioa of sim and the red column bar indicated the ioa increase by the enkf1 it could be seen from fig 5 that the ioa values in most sites were significantly increased implying that enkf1 could effectively improve the simulation accuracy of the complex 3dhed model 3 3 data assimilation based on meris data and in situ observations fig 7 showed the comparison of cyanobacterial biomass prediction results from the observations sim and enkf2 in july and november of 2010 the simulated cyanobacterial biomass from sim in july 10 2010 in the zhushan bay of taihu lake was clearly lower than observation however the simulation results of the cyanobacterial biomass obtained by the enkf2 model were greatly improved similar performance was also found for november 9 in the sw lake in july 22 enkf2 performed better than sim in the whole lake taihu however both enkf2 and sim failed to simulate the cyanobacterial biomass in november 10 2010 in general the simulation results of cyanobacterial biomass from sim were lower than the observed values but when the meris remote sensing data were used for assimilation enkf2 performed better than sim fig 8 showed the comparison of cyanobacterial biomass between the observations sim and enkf2 modeled results in february and may of 2011 it could be seen that the modeled results of cyanobacterial biomass results were markedly improved by enkf2 in the central lake on february 13 in the meiliang bay on february 24 and in the north lake taihu on may 15 these revealed that enkf could improve the model performance in simulating cyanobacterial biomass however there were still considerable difference between simulation results of enkf2 and measured data meris data or in situ observations for example the cyanobacterial biomass on may 10 2011 was overestimated and it was slightly underestimated on may 15 2011 by enkf2 fig 9 presented the rmse of simulation results of cyanobacterial biomass for the time series in the entire lake taihu by the 3dhed model and the enkf2 model it can be seen that the simulation results obtained by the enkf2 model were in general better than those of the 3dhed model by integrating multi source data into the model the spatial and temporal patterns of cyanobacterial biomass prediction were eminently improved which was of great importance to the management of eutrophic lakes however it was worth noting that in some cases such as in january the rmse was noticeably increased after the assimilation 4 discussion 4 1 performance of the three dimensional hydro ecological dynamics model 3dhed this study demonstrated the use of 3dhed in simulating cyanobacterial biomass in lake taihu the modeled concentrations of po4 p no3 n and do from the 3dhed agreed well with the in situ observations fig 4 however the cyanobacteria biomass concentration was not well modeled tables 2 and 3 and should be further improved the peak values of cyanobacterial biomass were not captured fig 5 probably because of the uncertainties from model conceptualization and parameters calibration the salmo model does not take into account the portion of cyanobacteria from the sediment surface according to kong et al 2009 cyanobacteria can sink to sediment surface in winter and recover in the spring contributing a visible amount to the blooms this could partially explain that the simulated cyanobacterial blooms in summer by 3dhed model had lower peak values compared to observations the discrepancy in spatial pattern of cyanobacteria biomass between modeled results and observations could be due to the inadequate description of wind driven horizontal drift of cyanobacteria according to chen et al 2014b the prevailing southeast wind in summer causes algae to continuously drift leeward which is too complex to be well described so far besides the uncertainty of model structure parameter calibration plays an important role in model performance in the measured data for model calibration the cases with high peak value of algal biomass concentration were sporadic the 3dhed model could be biased to the normal conditions during parameter calibration and thereby misses capturing the event like high blooms this could somehow be remediated by adopting resampling technique during model calibration huang et al 2012 furthermore it was assumed basing on the research of huang et al 2013 that the model error was normally distributed as the relatively scarce sampling sites make it difficult to estimate a statistical distribution of the model error however it should be pointed out that this assumption could also bring some uncertainties into the model although only one algal group cyanobacteria was simulated in this study because lake taihu was dominated by cyanobacteria 3dhed can be used to simulate the interaction of three different algal groups including cyanobacteria green algae and diatom it should be mentioned that salmo does not describe the biological processes of macrophyte so it cannot be used in a lake that is dominated by macrophyte 4 2 performance of data assimilation this study demonstrated the high potential of the proposed strategy in assimilating multi source data into the three dimensional hydro ecological dynamics model using the ensemble kalman filter the in situ measurement data provided relatively accurate cyanobacterial biomass information in the sampling sites while the meris remote sensing provided the entire spatial distribution of cyanobacterial biomass with acceptable temporal resolution combination of these two data sources could effectively describe the spatio temporal dynamics of cyanobacterial biomass in large lakes such as lake taihu in the simulation of enkf1 the state variables and parameters of the model were updated simultaneously which significantly improved the model performance it was seen that the model with data assimilation could not only better simulate the dynamics of cyanobacterial biomass but also better capture the peak of cyanobacterial biomass fig 5 the initial field of the enkf1 was updated at the moment of observation in order to provide a more accurate initial field to the model calculation system at the current moment in addition the parameters were optimized by the newly measured data and became more suitable for the modeled system at the current moment therefore we argued that assimilating measured data using enkf could be an efficient strategy to improve model performance especially for modeling of event like algal blooms mao et al 2009 the accuracy of the cyanobacterial biomass simulation from enkf2 was better than that of sim without data assimilation the simulated values from sim were relatively low compared to the observed data and such situation was greatly improved in the simulation of enkf2 by integrating both the meris remote sensing data and the in situ measurements however in some periods such as july 11 of 2010 and may 10 of 2011 the modeled results of cyanobacterial biomass from the enkf2 were a bit worse this was mainly because the measured data and meris remote sensing data appeared alternately in the time series and two data sources were alternately applied to the data assimilation process the continuously updating of parameters by alternately assimilating different source data could bring some oscillations to the model leading to occasionally unexpected model performance overall the cyanobacterial biomass retrieved by the meris data was higher than the measured values and at some times the difference between the two sources was more eminent the assimilation process of state variables could only effectively modify the cyanobacterial biomass in the observed grid and its nearby grid therefore when the in situ measured value was obviously lower than the meris remote sensing data in the same month it was difficult to eliminate the influence of higher cyanobacterial biomass in the previous time step which made the assimilation ineffective or even worsened the model performance however it was clearly seen that the multi source data for enkf2 could effectively improve the simulation accuracy of the 3dhed model with respect to the spatio temporal distribution of cyanobacteria in lake taihu it should be noticed that the assimilation accuracy was affected by the spatiotemporal scale and the accuracy of the data source indicating that model performance could be further improved by assimilating high quality data from multi sources the proposed data assimilation strategy could be more promising along with advances in the remote sensing resolution and retrieval method with the development of the satellite sensor the revisit period of the satellite remote sensing data becomes more frequent and the spectral and spatial resolution becomes higher fusion of multi source satellite remote sensing data to obtain cyanobacterial bloom results with high precision and high spatiotemporal resolution will be the focus of our future research in addition this study used a simple weight ratio of chlorophyll a and algal gravity to estimate the cyanobacterial biomass it is important to note that although cyanobacteria were dominant in the total algae of taihu lake the proportion of cyanobacteria to total algae varies with the season in spring and early summer there was still a certain amount of green algae and diatoms development of the retrieval method can reduce this uncertainty in estimating cyanobacteria biomass 5 conclusions multi source observation data were assimilated into a three dimensional hydro ecological dynamics model 3dhed to predict the cyanobacterial biomass in a shallow eutrophic lake lake taihu during 2009 2011 three simulations sim enkf1 and enkf2 with different data assimilation strategies were carried out to evaluate the potential of enkf to improve model performance it was shown that the 3dhed model without data assimilation sim could simulate the hydrodynamics and the concentration of main nutrients as well as algal biomass with acceptable accuracy however the most concerned peak values were not well captured the accuracy of cyanobacterial biomass simulation was obviously improved in the enkf1 in particular the peak values of cyanobacterial biomass were well captured by assimilating meris data enkf2 simulated the spatial distribution of cyanobacterial biomass with relatively good performance multi source data could be fused to improve the simulation accuracy of hydro ecological models however the timing of site observations and remote sensing data and the spatiotemporal accuracy of the remote sensing data have a great influence on the assimilation efficiency acknowledgements this work is supported by the national nature science foundation of china no 91547206 51425902 51709179 the innovation cluster fund of jiangsu province no sc917001 and the innovation cluster fund of nanjing hydraulic research institute no y917020 we are grateful to dr catherine rice for proofreading the english 
26220,accurately predicting spatio temporal patterns of algal bloom is important and also challenging this study developed a three dimensional hydro ecological dynamics model 3dhed to predict cyanobacterial biomass in lakes and applied ensemble kalman filter to assimilate multi source data into 3dhed for model improvement the model was applied in lake taihu using in situ measurements and remote sensing rs retrievals two data assimilation experiments named enkf1 and enkf2 were conducted enkf1 assimilated only in situ measurements while enkf2 assimilated both in situ measurements and rs data the results revealed that 3dhed simulated the spatio temporal patterns of cyanobacterial biomass in taihu with an acceptable index of agreement ioa enkf1 significantly improved the model fitness and increased the ioa of 85 measurement sites to 0 85 especially better captured the peak values compared with enkf1 enkf2 gave more improvements in spatial patterns besides model fitness implying that assimilating multi source data was helpful to improving the model performance keywords cyanobacterial biomass three dimensional ecological dynamics model data assimilation ensemble kalman filter 1 introduction during the past decades harmful algal blooms habs have been reported throughout the world duan et al 2009 michalak et al 2013 glibert et al 2014 degraded water quality due to increased nutrient pollution is one of the reasons for the expansion of habs heisler et al 2008 habs are characterized by the proliferation and occasional dominance of particular species of toxic or harmful algae that can lead to adverse impacts such as oxygen depletion from the decay of biomass the death of higher organisms and occasionally human poisoning via the ingestion of contaminated food sources allen et al 2008 huppert et al 2005 since the 1990s algal blooms in lake taihu have significantly affected industrial and agricultural production as well as the life of city dwellers in the taihu basin for instance algal blooms led to massive mortality of fish and shrimp and triggered a serious drinking water crisis in the summer of 1998 huang et al 2014 during late may to early june of 2007 lake taihu received infamous recognition when a monstrous cyanobacterial bloom deprived 5 million people of drinking water in wuxi city guo 2007 yang et al 2008 the monitoring and prediction of habs has become an urgent task for risk management many hab forecasting models have been developed to simulate phytoplankton dynamics in lakes and can be divided into two categories empirical statistical models and process based models the empirical statistical models e g multivariate linear regression mlr piecewise regression auto regressive integrated moving average arima and artificial neural networks ann have been widely developed and applied to hab forecasting recknagel 1997 chen and mynett 2004 lui et al 2007 chen et al 2015a b however these empirical statistical models have the weakness of lacking physical mechanisms their performance is highly dependent on the measured data used for model development process based models such as efdc hamrick 1992 wasp ambrose et al 1993 ce qualw2 cole and buchak 1995 and delft3d eco los et al 2008 have been increasingly developed to describe algae dynamics and predict habs these models have the potential to track the entire development of a bloom thus allowing early warning of its likely timing position magnitude and duration davidson et al 2016 from the 1970s process based models have progressed from a single element to a variety of elements a local process to a systematic process and from homogeneity to heterogeneity in space and time and have achieved remarkable success in many lakes hood et al 2006 zhang et al 2013 chen et al 2015a b although most process based models cannot distinguish algae species and the model solving as well as parameters calibration are complicated smayda 1997 roelke and buyukates 2001 process based models are widely used in many lakes with severe algal blooms such as lake taihu for simulating hydrodynamics water quality and phytoplankton dynamics hu et al 2006 mao et al 2008 huang et al 2012 zhang et al 2013 zhang 2014 li et al 2015 janssen et al 2017 among these studies salmo simulation by means of an analytical lake model model describes the seasonal dynamics of po4 p no3 n detritus chlorophyta bacillariophyta cyanophyta and cladocerans of the epilimnion and hypolimnion of stratified lakes by complex ordinary differential equations chen et al 2014a zhang 2014 established a three dimensional hydro ecological dynamics model 3dhed by coupling the lake ecosystem model salmo with the hydrodynamic model selfe semi implicit eulerian lagrangian finite element to simulate the spatio temporal dynamics of phosphate nitrate dissolved oxygen zooplankton biomass and algal biomass of three functional groups diatoms green algae and blue green algae however due to model conceptualization parameter calibration boundary configuration and etc and the prediction accuracy of the model is still poor in parallel with model development multi source observation data provide a direct way to monitor habs traditional ship borne or laboratory borne methods for examining algal blooms are constrained in both space and time due to the coarse sampling frequencies and limited sampling points remote sensing technology provides rapid synchronic and ongoing water property data overcoming the shortcomings of traditional methods lou and hu 2014 shang et al 2017 making it a potential choice for detecting and monitoring algal blooms so far various satellite sensors had been used for studying ocean color and for monitoring inland water quality such as the moderate resolution imaging spectroradiometer modis becker et al 2009 the medium resolution imaging spectrometer meris wynne et al 2010 hu et al 2010 duan et al 2017 and landsat data vincent et al 2004 ekstrand 1992 however the quality of optical satellite data is usually affected by the cloud cover and other weather conditions especially in the water surface which makes it difficult to obtain time series data for instant habs detections given that both process based models and multi source observation data have their own advantages and shortcomings combining process based models and observation data to improve the accuracy of algal blooms prediction is increasingly viable data assimilation technology is one of the best ways to achieve this goal by merging various measurement data into models data assimilation has been demonstrated to improve model reliability and reduce model uncertainties in broad fields evensen 2003 liu et al 2012 and different assimilation techniques have been developed and applied chen 2013 seo et al 2014 franssen and neuweiler 2015 the ensemble kalman filter enkf proposed by evensen 1994 is one of the most promising data assimilation techniques for updating model parameters and variables in real time it has the advantages of solving non linear problems and reducing the computation load in data assimilation chen et al 2015a b hut et al 2015 applications of enkf are significantly increased in recent years due to the quick expansion of available data particularly in hydrological ecological and environmental modeling eknes and evensen 2002 hut et al 2015 panzeri et al 2015 sun et al 2015 huang and gao 2017 these studies have demonstrated the high potential of enkf in improving model performance however assimilating multi source data into process based models using enkf is so far scarcely investigated the objectives of this study are 1 to evaluate the model performance of 3dhed in predicting cyanobacterial biomass in lake taihu 2 to assimilate multi source data satellite image and measured data into 3dhed using enkf 3 and to compare the effects of different source data collected from in situ surveys and meris on the assimilation system this is so far the first study that assimilate both measured data and satellite images to improve accuracy as well as spatial patterns of algal bloom modeling in lakes 2 materials and methods 2 1 study area lake taihu located in the yangtze river delta southeast china n 30 55 40 31 32 58 e 119 52 32 120 36 10 is one of the three largest freshwater lakes in china fig 1 taihu is usually divided into eight regions zhushan bay meiliang bay gonghu bay nw lake sw lake east lake central lake and xuhu bay with a water surface area about 2338 km2 and mean water depth of 1 89 m it is a typically shallow lake the lake is of great social and economic importance because it is the key source of fresh water for four province level regions in eastern china it also plays an essential role in tourism and ecological functions thus the water quality in lake taihu has always been under the research spotlight shi et al 2017 yang et al 2016 otten et al 2012 lake taihu has an annually average air temperature of 16 c and a maximum air temperature of 39 4 c in summer the water temperature can reach 30 c along with sufficient sunlight and low flow velocity forming a very suitable environment for cyanobacteria growth liu et al 2014 accelerated eutrophication and cyanobacterial blooms have occurred since the 1980s due to the rapid increase in agricultural and industrial development which has led to dramatic rise of nutrient loading into the lake qin et al 2007 hu et al 2010 habs have most frequently occurred in spring and summer however recent studies have reported an expansion of the spatial distribution of the cyanobacterial bloom surface as well as an increasing frequency of habs zhang and chen 2011 shi et al 2017 2 2 datasets from in situ observation and remote sensing two datasets in situ observation and remote sensing were collected for lake taihu from january 2009 to december 2011 the in situ observation dataset included hydrological and water quality data monthly hydrological data discharge and water level of the main in out flow rivers were collected the daily water level was monitored at 4 stations dapukou xishan jiapu and xiaomeikou water quality data were collected from 18 sites in rivers and 29 sites in the lake fig 1 and table 1 the water quality data included water transparency sd water temperature wt illumination i ph concentrations of suspended solids ss dissolved oxygen do nitrates no3 n nitrites no2 n ammonium nitrogen nh4 n total nitrogen tn total phosphorus tp phosphate po4 chlorophyll a chla diatom biomass green algae biomass and cyanobacterial biomass remote sensing imagery were derived from the medium resolution imaging spectrometer instrument meris the meris sensor provides radiance measurements with a ground spatial resolution of 300 m a temporal resolution of 3 days and 15 spectral bands in the spectral range of 412 5 900 nm with narrow bandwidths shi et al 2014 the operation period of the meris mission envisat was from march 1 2002 to april 8 2012 all the meris images from january 1 2010 to december 31 2011 were collected in this study 37 images without cloud cover were selected for cyanobacterial biomass retrieval there were in total 37 images being used for chla concentrations retrieval the datasets are available on http merisfrs merci ds eo esa int merci so far several algorithms such as eutrophic lake eul free university of berlin fub fluorescence line height flh maximum chlorophyll index mci and maximum peak height mph had been proposed to retrieve chla concentrations or chla related products from the meris data salem et al 2017 according to the previous research in lake taihu qi et al 2015 the mci algorithm was adopted to estimate the near surface chlorophyll a concentrations in the study more details could be seen in qi et al 2015 the chlorophyll a concentration can then be converted to algal biomass concentration based on kasprzak et al 2008 since the 1990s the biomass concentration of cyanobacteria has been observed in lake taihu nearly throughout the year since cyanobacteria is the dominant species during the bloom period and the highest ratio of cyanobacteria biomass concentration has reached 91 6 of the total algal biomass concentration in lake taihu gu et al 2011 using chla to estimate cyanobacterial biomass is reasonable li 2015 found an empirical relationship between the retrieved chla from meris and in situ cyanobacteria biomass in lake taihu 1 c h l c b l u e 0 03 g c h l g b l u e where c b l u e represents the cyanobacterial biomass μg l g c h l and g b l u e are the weight of chlorophyll a and cyanobacteria respectively μg 2 3 three dimensional hydro ecological dynamics model 3dhed to describe the spatio temporal pattern of cyanobacterial biomass a three dimensional 3d hydrodynamic model selfe zhang and baptista 2008 was coupled with a process based ecological model salmo benndorf and recknagel 1982 the selfe is able to estimate water level 3d velocity salinity and water temperature by solving the 3d shallow water equations it is widely used in simulations of three dimensional baroclinic circulation around the world liu et al 2008 roland et al 2012 the salmo model is a deterministic model that describes the dynamics of po4 p no3 n do detritus and biomass of cyanobacteria green algae and diatoms through a time dependent ordinary differential equation ode based on mass balance previous studies have proved the applicability of this model in eutrophication simulation of many lakes and reservoirs all over the world chen et al 2014a recknagel et al 2008 the basic idea for coupling these two models is to use the hydrodynamic outputs of selfe as the inputs to salmo however the spatial dimension is different for these two models selfe is a 3d model while salmo is a zero dimensional model therefore a reduction is assumed for salmo such that all variables and parameters are well mixed in each cell of selfe each discrete cell in selfe is treated as an independent lake so that the salmo model can be applied at the beginning only the selfe model was run until it reached the required accuracy and stability at time t then salmo read the results of selfe as inputs to start calculation from time t the combination of selfe and salmo avoids solving the complicated 3d partial differential equations for mass and momentum conservations and thereby significantly improved the computational efficiency in addition the coupled model has the advantage of distinguishing the biomass of different functional groups of algae compared to the traditional eutrophication model 2 4 data assimilation multi source data in situ observation and remote sensing data were assimilated into 3dhed to update the simulated cyanobacterial biomass fig 2 by using the enkf method enkf assimilation includes forecast and analysis steps at the forecast step the simulated cyanobacterial biomass for each ensemble member at time t δt is made as follows 2 c ˆ t δ t i m c ˆ t i u t δ t i θ t i ξ t δ t i 3 u t δ t i u t δ t 1 η i 4 θ t i θ t 1 σ i 5 ξ t δ t i n 0 ξ where c ˆ t δ t i is the updated cyanobacterial biomass at time t δt m c ˆ t i u t δ t i θ t i is the simulated cyanobacterial biomass from 3dhed u t δ t i is the forcing data with noise a zero mean random variable with a normal distribution and covariance η the forcing data of this model include solar radiation and water temperature a parameter ensemble θ t i is produced by adding σ i noise a zero mean random variable with a normal distribution and covariance σ to the initial value of θ t the temperature dependent growth rate of cyanobacteria which was sensitive to model performance was selected as the parameter for updating recknagel 1984 huang et al 2013 li et al 2015 ξ t δ t i noise a zero mean random variable with a normal distribution and covariance ξ represents the uncertainty in the model structure in the analysis step the ensemble of parameter and variable was updated by 6 θ t δ t i θ t i k t δ t c t δ t i c ˆ t δ t i where k i 1 is kalman gain for correcting the parameter trajectories and is calculated by 7 k t δ t t δ t θ c t δ t c c t δ t c ˆ c ˆ 1 where t δ t c c and t δ t c ˆ c ˆ are the error covariance of c t δ t and c ˆ t δ t respectively t δ t θ c is the cross covariance of parameter ensemble and prediction ensemble the performance of data assimilation is highly related to the parameter values in enkf including the ensemble size model error and observation error ensemble size was generally determined by testing the assimilation performance with different values model and observation errors were generally determined empirically in previous studies chen et al 2015a b huang et al 2013 sun et al 2015 because it is very difficult to know the true value of observation and the model structure error in this study six 30 50 70 100 200 and 500 ensemble sizes and four 1 10 20 and 30 observation in situ observation and remote sensing errors and model errors were tested to choose the optimal settings for enkf the testing results showed that the assimilation results remained stable when the ensemble size was set as 100 the rmse value was the smallest when the model error was 10 and observation error in situ observation and remote sensing was 1 this parameter set was consistent with previous studies liu et al 2017 based on the above setting three different modeling exercises sim enkf1 and enkf2 were carried out to simulate cyanobacterial biomass in lake taihu the simulation time period was from january 1 2009 to december 31 2011 before the assimilation the 3dhed model was run for one year to make it stable in sim data assimilation was not applied in enkf1 29 sampling points were interpolated into all grid cells when the in situ observations were available the interpolated data were used as the initial field to calculate the cyanobacterial biomass at the next time step in enkf2 both in situ observations and meris remote sensing estimates were assimilated into the 3dhed model because the concentration of cyanobacterial biomass is low from january to may in lake taihu the assimilation of enkf2 started from the beginning of june the assimilation time was from june 2010 to june 2011 during this time period 11 days in situ measured data and 37 days remote sensing data were available table 3 the meris data were interpolated to match the corresponding meshes of the 3dhed model the model performance of sim enkf1 and enkf2 was evaluated by the root mean square error rmse and the index of agreement ioa rmse reflects the deviation of the estimation compared to the observation while ioa reflects the uniformity of the estimation and observation calculation details of rmse and ioa can be found in previous studies qi et al 2014 huang and gao 2017 3 results 3 1 validation of the three dimensional hydro ecological dynamics model 3dhed the salmo model included 104 parameters in this study the initial values of these parameters were obtained from guo et al 2012 which was acceptable because guo et al 2012 calibrated 20 key parameters related to algal growth in meiliang bay the mostly representative and concerned area of lake taihu fig 3 showed the validation of the 3dhed model at four typical monitoring station dapukou xishan jiapu and xiaomeikou in lake taihu it was seen that the simulated and observed water levels at the four stations were consistent during 2009 2011 the relatively high r 2 0 78 0 81 indicated that the model boundary conditions the initial conditions and the parameters were set properly a widely refereed site sanhaobiao was selected to show the simulation results of po4 p no3 n zooplankton do diatoms green algae cyanobacteria and total algae fig 4 it could be seen that the dynamics of do po4 p no3 n and green algae concentrations were relatively well simulated however no3 n concentration during the summer and autumn in 2011 was over estimated the trends of the simulated biomass of cyanobacterial and total algae were similar and consistent with the measured data however the peak values were not well captured in addition the seasonal dynamics of zooplankton was not well simulated one sampling site in each lake area boldfaced sites in table 1 was selected to calculate the rmse and ioa values tables 2 and 3 and each site included 36 monthly water quality data from january 2009 to december 2011 the concentrations of major variables in the 3dhed model such as po4 p no3 n zooplankton do diatoms green algae cyanobacteria and total algal biomass were evaluated to assess model performance relatively low rmse and high ioa were obtained for po4 p no3 n and do in particular the mean ioa of do 0 85 was the highest among all the variables the mean rmse of cyanobacteria 14 23 mg l and total algae 16 11 mg l were almost at the same level and much higher than that of green algae 2 18 mg l indicating that cyanobacteria was the main component of the total algae in lake taihu overall the simulation of cyanobacterial biomass concentration was not as good as that of other variables as shown by the relatively low ioa in summary the 3dhed model used in this study can effectively simulate the spatio temporal pattern of the hydrodynamics nutrients and algal biomass concentrations in lake taihu the simulated results of each monitoring stations coincided with the dynamic change of the measured values indicating that 3dhed is applicable to eutrophication simulation in lake taihu however the plankton peaks which are the most serious concern in algal bloom forecasting were not well captured 3 2 data assimilation based on in situ observations fig 5 showed the cyanobacterial biomass from observation obs and two simulations sim and enkf1 at eight stations of each lake region in lake taihu it could be seen that the accuracy of cyanobacterial biomass simulation was markedly improved by enkf1 in which the state variables and parameters of the model were updated simultaneously compared with the simulation without data assimilation sim enkf1 better simulated the dynamic changes and the peak values of cyanobacterial biomass however it is important to note that some peak values e g peak value in 2010 at pingtaishan of cyanobacterial biomass were not well captured fig 6 showed the ioa of the simulation without data assimilation sim and the ioa improvement of the enkf1 the blue column bar represented the ioa of sim and the red column bar indicated the ioa increase by the enkf1 it could be seen from fig 5 that the ioa values in most sites were significantly increased implying that enkf1 could effectively improve the simulation accuracy of the complex 3dhed model 3 3 data assimilation based on meris data and in situ observations fig 7 showed the comparison of cyanobacterial biomass prediction results from the observations sim and enkf2 in july and november of 2010 the simulated cyanobacterial biomass from sim in july 10 2010 in the zhushan bay of taihu lake was clearly lower than observation however the simulation results of the cyanobacterial biomass obtained by the enkf2 model were greatly improved similar performance was also found for november 9 in the sw lake in july 22 enkf2 performed better than sim in the whole lake taihu however both enkf2 and sim failed to simulate the cyanobacterial biomass in november 10 2010 in general the simulation results of cyanobacterial biomass from sim were lower than the observed values but when the meris remote sensing data were used for assimilation enkf2 performed better than sim fig 8 showed the comparison of cyanobacterial biomass between the observations sim and enkf2 modeled results in february and may of 2011 it could be seen that the modeled results of cyanobacterial biomass results were markedly improved by enkf2 in the central lake on february 13 in the meiliang bay on february 24 and in the north lake taihu on may 15 these revealed that enkf could improve the model performance in simulating cyanobacterial biomass however there were still considerable difference between simulation results of enkf2 and measured data meris data or in situ observations for example the cyanobacterial biomass on may 10 2011 was overestimated and it was slightly underestimated on may 15 2011 by enkf2 fig 9 presented the rmse of simulation results of cyanobacterial biomass for the time series in the entire lake taihu by the 3dhed model and the enkf2 model it can be seen that the simulation results obtained by the enkf2 model were in general better than those of the 3dhed model by integrating multi source data into the model the spatial and temporal patterns of cyanobacterial biomass prediction were eminently improved which was of great importance to the management of eutrophic lakes however it was worth noting that in some cases such as in january the rmse was noticeably increased after the assimilation 4 discussion 4 1 performance of the three dimensional hydro ecological dynamics model 3dhed this study demonstrated the use of 3dhed in simulating cyanobacterial biomass in lake taihu the modeled concentrations of po4 p no3 n and do from the 3dhed agreed well with the in situ observations fig 4 however the cyanobacteria biomass concentration was not well modeled tables 2 and 3 and should be further improved the peak values of cyanobacterial biomass were not captured fig 5 probably because of the uncertainties from model conceptualization and parameters calibration the salmo model does not take into account the portion of cyanobacteria from the sediment surface according to kong et al 2009 cyanobacteria can sink to sediment surface in winter and recover in the spring contributing a visible amount to the blooms this could partially explain that the simulated cyanobacterial blooms in summer by 3dhed model had lower peak values compared to observations the discrepancy in spatial pattern of cyanobacteria biomass between modeled results and observations could be due to the inadequate description of wind driven horizontal drift of cyanobacteria according to chen et al 2014b the prevailing southeast wind in summer causes algae to continuously drift leeward which is too complex to be well described so far besides the uncertainty of model structure parameter calibration plays an important role in model performance in the measured data for model calibration the cases with high peak value of algal biomass concentration were sporadic the 3dhed model could be biased to the normal conditions during parameter calibration and thereby misses capturing the event like high blooms this could somehow be remediated by adopting resampling technique during model calibration huang et al 2012 furthermore it was assumed basing on the research of huang et al 2013 that the model error was normally distributed as the relatively scarce sampling sites make it difficult to estimate a statistical distribution of the model error however it should be pointed out that this assumption could also bring some uncertainties into the model although only one algal group cyanobacteria was simulated in this study because lake taihu was dominated by cyanobacteria 3dhed can be used to simulate the interaction of three different algal groups including cyanobacteria green algae and diatom it should be mentioned that salmo does not describe the biological processes of macrophyte so it cannot be used in a lake that is dominated by macrophyte 4 2 performance of data assimilation this study demonstrated the high potential of the proposed strategy in assimilating multi source data into the three dimensional hydro ecological dynamics model using the ensemble kalman filter the in situ measurement data provided relatively accurate cyanobacterial biomass information in the sampling sites while the meris remote sensing provided the entire spatial distribution of cyanobacterial biomass with acceptable temporal resolution combination of these two data sources could effectively describe the spatio temporal dynamics of cyanobacterial biomass in large lakes such as lake taihu in the simulation of enkf1 the state variables and parameters of the model were updated simultaneously which significantly improved the model performance it was seen that the model with data assimilation could not only better simulate the dynamics of cyanobacterial biomass but also better capture the peak of cyanobacterial biomass fig 5 the initial field of the enkf1 was updated at the moment of observation in order to provide a more accurate initial field to the model calculation system at the current moment in addition the parameters were optimized by the newly measured data and became more suitable for the modeled system at the current moment therefore we argued that assimilating measured data using enkf could be an efficient strategy to improve model performance especially for modeling of event like algal blooms mao et al 2009 the accuracy of the cyanobacterial biomass simulation from enkf2 was better than that of sim without data assimilation the simulated values from sim were relatively low compared to the observed data and such situation was greatly improved in the simulation of enkf2 by integrating both the meris remote sensing data and the in situ measurements however in some periods such as july 11 of 2010 and may 10 of 2011 the modeled results of cyanobacterial biomass from the enkf2 were a bit worse this was mainly because the measured data and meris remote sensing data appeared alternately in the time series and two data sources were alternately applied to the data assimilation process the continuously updating of parameters by alternately assimilating different source data could bring some oscillations to the model leading to occasionally unexpected model performance overall the cyanobacterial biomass retrieved by the meris data was higher than the measured values and at some times the difference between the two sources was more eminent the assimilation process of state variables could only effectively modify the cyanobacterial biomass in the observed grid and its nearby grid therefore when the in situ measured value was obviously lower than the meris remote sensing data in the same month it was difficult to eliminate the influence of higher cyanobacterial biomass in the previous time step which made the assimilation ineffective or even worsened the model performance however it was clearly seen that the multi source data for enkf2 could effectively improve the simulation accuracy of the 3dhed model with respect to the spatio temporal distribution of cyanobacteria in lake taihu it should be noticed that the assimilation accuracy was affected by the spatiotemporal scale and the accuracy of the data source indicating that model performance could be further improved by assimilating high quality data from multi sources the proposed data assimilation strategy could be more promising along with advances in the remote sensing resolution and retrieval method with the development of the satellite sensor the revisit period of the satellite remote sensing data becomes more frequent and the spectral and spatial resolution becomes higher fusion of multi source satellite remote sensing data to obtain cyanobacterial bloom results with high precision and high spatiotemporal resolution will be the focus of our future research in addition this study used a simple weight ratio of chlorophyll a and algal gravity to estimate the cyanobacterial biomass it is important to note that although cyanobacteria were dominant in the total algae of taihu lake the proportion of cyanobacteria to total algae varies with the season in spring and early summer there was still a certain amount of green algae and diatoms development of the retrieval method can reduce this uncertainty in estimating cyanobacteria biomass 5 conclusions multi source observation data were assimilated into a three dimensional hydro ecological dynamics model 3dhed to predict the cyanobacterial biomass in a shallow eutrophic lake lake taihu during 2009 2011 three simulations sim enkf1 and enkf2 with different data assimilation strategies were carried out to evaluate the potential of enkf to improve model performance it was shown that the 3dhed model without data assimilation sim could simulate the hydrodynamics and the concentration of main nutrients as well as algal biomass with acceptable accuracy however the most concerned peak values were not well captured the accuracy of cyanobacterial biomass simulation was obviously improved in the enkf1 in particular the peak values of cyanobacterial biomass were well captured by assimilating meris data enkf2 simulated the spatial distribution of cyanobacterial biomass with relatively good performance multi source data could be fused to improve the simulation accuracy of hydro ecological models however the timing of site observations and remote sensing data and the spatiotemporal accuracy of the remote sensing data have a great influence on the assimilation efficiency acknowledgements this work is supported by the national nature science foundation of china no 91547206 51425902 51709179 the innovation cluster fund of jiangsu province no sc917001 and the innovation cluster fund of nanjing hydraulic research institute no y917020 we are grateful to dr catherine rice for proofreading the english 
26221,two way nesting is becoming a popular method to resolve limited area domains at high resolution however implementation of two way nesting into ocean models is not a trivial process we present an alternative two way nesting framework that leverages off existing functionality present in many ocean codes including arrays to store data exchanged between grids data exchange protocols and re gridding an important element of this is the use of existing open boundary infrastructure to store and apply the data exchanged in two way nesting reduce specification error at the boundary and circumvent coupling at the barotropic levels the resulting two way system consists of models that operate in isolation of each other and can be considered autonomous in the sense that no overarching coupling infrastructure is required to orchestrate the two way nesting and models for respective grids can operate independently with communication achieved only via the sending and receiving of packets of self describing information keywords modelling mathematical modelling boundary conditions open boundary conditions time synchronization 1 introduction accurately capturing events at small scales in ocean models is best achieved by increasing the resolution of an ocean model however this increases computational demands through an increased number of cells to simulate and stability and accuracy constraints dictating the use of a small time step the throughput of high resolution models that cover large areas can be prohibitive and the technique of nesting high resolution models in larger scale coarser resolution models is a popular approach to circumvent this issue these models are usually one way nested where information only flows from the coarse resolution model the parent grid to the high resolution model the child grid e g mason et al 2010 penven et al 2006 recently two way nesting is becoming increasingly used where information from the child is allowed to propagate back into the parent domain e g debreu et al 2012 cailleau et al 2008 jouanno et al 2008 an excellent review of the issues surrounding two way nesting can be found in debreu and blayo 2008 the process of two way nesting requires a number of elements to be carried out and this is typically handled by a coupler e g agrif debreu et al 2008 agrif adaptive grid refinement in fortran is a package that facilitates programs written in fortran to communicate information between parent and child grids in both a one way and two way sense agrif has been implemented in several ocean models viz roms debreu et al 2012 nemo jouanno et al 2008 and opa8 1 cailleau et al 2008 agrif consists of a series of model independent procedures such as interpolation and update procedures refinement clustering and time integration algorithms and model dependent routines that includes the replacement of the main time integration loop of the model with an agrif equivalent and writing an interface routine that transforms the model s configuration file controlled by keywords the implementation of such a package into a new model is not a trivial process however it needs to be performed only once after which the benefits of the agrif coupling can be realized the disadvantage is that such a process often diverges ones model code from the core branch many contemporary regional coastal ocean models codes contain existing functionality that can be exploited to facilitate two way nesting in this paper we demonstrate this to be the case using the code described by herzfeld 2006 this is a mode split finite difference model based on blumberg and herring 1987 using an orthogonal curvilinear arakawa c grid in the horizontal and z or σ coordinates in the vertical leapfrog time stepping is used for momentum and the model is explicit except for the vertical mixing which uses an implicit scheme a variety of momentum and tracer advection 2 equation turbulence and open boundary schemes may be used the 2 way nesting framework developed using this model is applied to an idealized test domain and a real application to demonstrate its utility the implementation of such a system requires little modification to existing code and preserves the integrity of the core branch with full backward compatibility it also results in a system that can leverage functionality inherent in the code that improves the 2 way nesting model solution this is particularly true of the open boundary infrastructure especially the application of open boundary algorithms designed to cope with specification error due to boundary data and interior solution mismatches resulting in increased model stability the two way system is also autonomous in the sense that no overarching coupling infrastructure is required to orchestrate the two way nesting the parent and child models run independently as stand alone implementations on the same or different processors or machines using the same or potentially different model codes without knowing of the other s existence all communication is achieved only via the sending and receiving of packets of self describing information in section 2 we review the two way nesting problem and articulate the elements required for two way nesting to operate we then show how these elements can be accomplished within existing infrastructure inherent in the ocean model an idealized test case that models the propagation of a baroclinic vortex is introduced in section 3 after which we demonstrate the functionality of the two way system a real application is undertaken in section 4 and concluding remarks are presented in section 5 and 6 2 the autonomous two way system a simplified view of two way nesting involves the insertion of a high resolution child grid ωc operating with a small time step δt in a coarser resolution parent grid ωp using a longer time step δt and allowing information from both models to propagate freely over the interfaces of the grids the procedure is similar to one way nesting where the parent model will advance an increment of time typically δt then the child model will perform many integrations up to this time using boundary information from the parent at the start of its step once the parent time increment is reached the parent advances further and the whole process repeats this may be performed sequentially with parent information stored offline the passing of information from the parent to the child is termed the interpolation step since an interpolation of data in time and space onto the high resolution child grid points is generally required the region of the child grid that information is interpolated onto is termed the dynamic interface γ the difference with two way nesting is that at the parent s time increment the child provides information back to the parent on a region of the parent grid called the feedback interface this step is called the update or restriction step and information from the child is usually filtered to remove small scale structure by a restriction operator in practice the whole area in ωp occupied by ωc is often subjected to the restriction step but it is only those values on the feedback interface that affect the solution in ωp in practice the problem is more complex due to the fact that ocean models are usually mode split where a barotropic mode operates on a smaller time step than a baroclinic mode the two way nesting must take into account information transfer between both these modes which adds complexity effectively data must be exchanged every barotropic step in the case of split explicit models such as the one used in this study there are many issues involved in two way nesting that we do not attempt to explore in detail here the reader is referred to debreu and blayo 2008 for a comprehensive account of the details of two way nesting a schematic of the two way nesting process is provided in fig 2 1 to facilitate two way nesting a coupler is generally required that performs the following essential functions 1 provide infrastructure to accommodate the transfer of information e g agrif uses pointers 2 provide algorithms that perform re gridding necessary for the interpolation and update steps 3 perform the physical transfer of data e g agrif uses direct memory access 4 synchronize the information exchange in time many contemporary regional or coastal ocean models already contain most of the infrastructure required to perform these tasks and if re configured enable two way nesting to be performed with minimal code alteration and without the need for a coupler to orchestrate these tasks in the following we assume the model uses mode splitting on an arakawa c grid the above tasks are considered in turn data transfer the open boundary infrastructure inherent in the ocean model can be used to accommodate the information required for transfer between the grids open boundaries can be constructed along the dynamic interface for the child grid and feedback interface for the parent grid and these can be populated with information at the interpolation and update steps to be used by the respective grids at the next time step the update step only occurs along the open boundary in the parent rather than the whole region occupied by the child in the parent domain two way nesting usually directly copies data onto dynamic and feedback interfaces with interpolation or update operators applied as necessary a direct copy of information from one grid into the boundary interface of the other essentially corresponds to a clamped boundary condition the performance open boundary conditions has been extensively assessed in the literature e g palma and matano 1998 with a consensus that a clamped condition i e direct prescription of data onto the boundary is undesirably reflective this can lead to an ill specified boundary marchesiello et al 2001 and ultimately model instability in the 2 way nesting context models using direct transfer of information usually need to be coupled at the barotropic level to improve boundary specification and maintain stability regional ocean models usually contain a suite of open boundary conditions obcs many of which are expressly designed to prevent reflection of information and minimize boundary specification error these can be leveraged in 2 way nesting to allow the grid interfaces to be transmissive to transient perturbations between grid interfaces e g short waves propagating from child to parent and result in a solution that is more tolerant of error introduced where transfer of data between parent and child does not contain enough information to accurately drive the dynamics under specification or information that is not compatible with the interior dynamics over specification this use of obcs to add an additional layer of stability is not commonly practiced in 2 way frameworks and can circumvent the need to stabilize the system by coupling at the barotropic level in section 3 we demonstrate that a viable two way solution is achieved using non reflective obcs without coupling at the barotropic level but when clamped obcs are used instability results the relative orientation of normal velocity on the model grid relative to elevation i e the cell centre on an arakawa c grid can be manipulated to allow the obc exactly mimic the dynamic feedback interfaces utilized in traditional grid refinement this relative orientation is termed boundary implementation by herzfeld 2009 where nvoe normal velocity outside elevation refers to the situation where normal velocity is located at the outside face from the cell centre and nvie normal velocity inside elevation refers to an inside orientation fig 2 2 illustrates this process for western and eastern edges where a fine mesh is nested inside a coarse mesh fig 2 2 a corresponds to a traditional arrangement of grid cells for two way nesting as described by zhang et al 1986 spall and holland 1991 or fox and maskell 1995 fig 2 2 b shows how an implementation using nvoe for the child grid and nvie for the parent grid exactly mimics this structure when used in conjunction with clamped obcs this can be further generalised to non reflective obcs e g the condition of herzfeld and andrewartha 2012 as in fig 2 2 c in principle any obc may be used to facilitate the transfer of information and any supplementary boundary approaches may be imposed e g sponge zones nudging zones flow relaxation re gridding most modern codes perform re gridding of data during model output where data can be interpolated onto predefined latitude and longitude locations e g stations in roms this capability can be used to pre package data at the exact locations required for information exchange in two way nesting alternatively many models also perform inline interpolation when inputting forcing data including open boundary data the routines that manage data input are often accessible as a model independent library function and are capable of interpolating in space and time sophisticated approaches including buffering bit shifting caching of weights and binary searches in tree structures for conversion of grid indices to latitude longitude coordinates can be used to increase efficiency of input to the extent that it comprises only a negligible fraction of total runtime during output a spatial mean around a particular geographic coordinate may be computed in this way the restriction operators used to filter the child solutions can be embedded and allow filtered data to be supplied to the parent s open boundary vectors data communication the two way nesting approach being presented in this paper aims to minimize the code changes in the model and to make use of as much of the existing boundary functionality as possible as almost all models already support one way nesting using files on disk the exact same approach may be applied here with only minor changes for them to work in the two way paradigm all that is required is for the models to adopt the time synchronization scheme as described in the next section the self contained output data packet with extra timing information gets written to a file and the target model reads in the boundary data similar to a one way scheme but with additional waiting to maintain synchronization when needed indeed while this system works and has been tested by the authors it is not ideal in terms of the performance penalty imposed by the filesystem it also forces the models operating in two way nesting mode to have access to the same filesystem thus severely limiting the ability for true distribution of models across compute resources the data files are also largely redundant in the sense that they are temporary only existing for a short period and then overwritten another approach is to use some form of inter process communication ipc protocol for the exchange of data a common framework that is already familiar to the modelling community is the message passing interface mpi we have made use of its remote memory access rma or one sided communication feature as another implementation of two way nesting this method does away with files entirely and data is exchanged over the network i e the models operate in a distributed sense we have tested this with one model running locally on a desktop and the other on a remote hpc node model results of both the file based and mpi systems were identical this autonomous approach implies that the parent and child models need not use the same code base potentially allowing different models operating on different platforms to be two way coupled ultimately the choice of application and available infrastructure in the model to be leveraged for data communication will dictate the data exchange approach adopted time synchronization the synchronization of data exchange in time is the one element of two way nesting that is not commonly present in ocean codes however by inserting one additional piece of information in data packets that are exchanged and adding an extra line of code at the appropriate location in input routines then time synchronization may be achieved with a high degree of flexibility we introduce the concept of a synchronization increment δs given that a packet of data is dumped at intervals of δtd and contains the time that the data was dumped td then the synchronization time is the time period after td that the data should be used in the alternative grid i e if the model time is t then the data should only be used if t t d δ s inverting this condition leads to a conditional statement that can be inserted in the input routines that allows time synchronization 2 1 if t td δs wait where wait is a function that continuously reads the information exchanged to extract td if the traditional two way time synchronization is to be achieved where the parent advances one time step waits for the child to catch up then advances again then the child grid should send data packets with δtd δs δt and the parent grid sends data with δtd δt with δs 0 conversely this means the child uses δs 0 and the parent uses δs δt this situation is illustrated in fig 2 3 a where at the model start time both parent and child dump data with td 0 the model time is t 0 hence for the child eq 2 1 is satisfied so the child waits for the parent eq 2 1 is not satisfied and the parent advances 1 time step the parent model time is now t δt δtd so a dump step occurs and the parent sends a packet of data to the child with td δt eq 2 1 is now satisfied for the parent t td δs t δt td 0 δs δt and the parent goes into wait mode the child has access to the packet dumped from the parent with td δt so now eq 2 1 is not satisfied and the child will integrate until its model time t δt if the previous packet from t 0 was cached then interpolation between t 0 and t δt can occur for the child when t δt the child will dump a packet with td δt so now for the parent eq 2 1 no longer holds and the parent advances one time step further using obc data dumped at td δt from the child and the whole sequence repeats for another time step note that the above synchronization assumes an execution sequence of model integration followed by model time update then data dumping it may be necessary to use in the conditional statement in eq 2 1 or use δs ε where ε 10 10 is a very small number depending on the sequencing used in a particular code however the general concept of controlling synchronization remotely via the synchronization increment remains the same additional infrastructure must be developed to read and write the synchronization time in data packets exchanged between parent and child but the essence of the time synchronization lies in eq 2 1 in the traditional two way configuration described above the parent and child models execute sequentially however a large degree of flexibility with the data exchange is accommodated using this method for example the models can be made to run concurrently thus decreasing total wall time by allowing the parent and child to race each other in this case both parent and child use δs n δt where n is an integer this situation is illustrated in fig 2 3 b for n 3 and the case where the parent executes faster than the child both models use data from the start of the time step at time t when the parent reaches the wait point then the child may start to interpolate between data exchanged at time t and t 3δt the case where the child executes faster is shown in fig 2 3 c in this case the parent will begin to use data interpolated between t and 3δt when the child reaches the wait point the point at which interpolation begins is dependent on the relative speeds of parent and child therefore the solutions using this configuration may not be exactly reproducible since interpolation may begin at different times depending on system load and compute architecture note that this only applies to the configurations of fig 2 3 b and c the configuration described in fig 2 3 a is exactly reproducible we investigate the practical repercussions of this in the next section the above elements were implemented in the ocean code described by herzfeld 2006 to result in a system capable of two way nesting the parent and child models are simulated as separate stand alone models with all communication achieved by the exchange of data packets we now test the utility of the system in an idealized test domain in the following experiments the models were run on distributed compute architecture with each respective model executing on shared memory across a single node 3 baroclinic vortex test the test case chosen to test the two way nesting system is the baroclinic vortex described by penven et al 2006 and used by debreu et al 2012 a full description of the test case configuration can be found in penven et al 2006 and we summarize the main points here a baroclinic vortex is embedded in the initial condition where a gaussian distribution of pressure is imposed elevated sea level and temperature having an e folding length scale of 60 km and initial velocities that are in geostrophic balance with an imposed maximum geostrophic velocity of 1 ms 1 the vortex exists in the presence of continuous stratification where the brunt vaissala frequency is n 0 003 s 1 the centre of the vortex is located at 38 5os on a β plane a flat bottom exists with depth 5000 m the parent grid size is 1800 1800 km with 30 km grid resolution the child domain is centred in the parent with size of 580 580 km with resolution of 10 km there are 10 vertical layers using a z vertical coordinate system the parent grid is assumed to be unbounded and a passive flather condition is imposed palma and matano 2001 on its open boundaries at the grid limits no bottom friction or vertical mixing is used in either grid the parent grid uses horizontal viscosity of 1000 ms 1 smagorinsky diffusivity using a constant of 0 1 and baroclinic barotropic time step of 900 30 s the child grid uses horizontal viscosity of 375 ms 1 smagorinsky diffusivity using a constant of 0 1 and baroclinic barotropic time step of 150 5 s the grid refinement factor is therefore 3 and the time refinement factor is 6 two reference solutions were generated where the whole parent domain was resolved at high and low resolution 10 and 30 km respectively the initial conditions for the high resolution reference test is shown in fig 3 1 the models run for 120 days the reference solution at intervals of 30 days are shown in fig 3 2 the baroclinic vortex is seen to propagate north westward shedding weak rossby waves in its wake and largely retaining its axisymmetry the vortex weakens as it propagates consistent with the northern hemisphere equivalent presented by debreu at al 2012 when this experiment is represented with a fine grid nested within a coarse grid we are looking for the vortex to transition across the nesting boundary without distortion numerous experiments were conducted using the parent and child grid to replicate the reference solution with various forms of coupling a summary of experiments attempted is provided in table 3 1 the one way nested solution is presented in fig 3 3 where it is seen that some distortion of the vortex occurs as it approaches the north west boundary it weakens becomes asymmetric and has difficulty in transiting the open boundary the two way experiment dri was conducted without sponges and interface separation uses no restriction operator for sea level and t s and is coupled at the baroclinic level i e no coupling at the barotropic level with the grid stencil corresponding to that of fig 2 2 c the open boundary condition embedded in the scheme is that of herzfeld and andrewartha 2012 solutions are shown in fig 3 4 showing that the symmetry of the vortex is preserved to a greater degree than one way nesting as it traverses the open boundary after which the vortex weakens more rapidly due to the coarser resolution in the parent resulting in greater dispersion the two way nested solution is clearly superior to the one way solution interface separation increases the accuracy of traditional two way nested applications debreu et al 2012 where the stencil used is that depicted in fig 3 5 this is similar to that shown in debreu and blayo 2008 fig 14 with the exception that the child normal velocity at the dynamic interface is placed interior to the parent normal velocity making these boundaries nvoe this allows an interpolation of child normal velocities between two non interface parent velocities to occur which was observed to increase stability using this configuration in conjunction with clamped open boundaries i e the traditional two way approach but without coupling at the barotropic level resulted in the model rapidly becoming unstable at 6 h degenerating into large amplitude long wave features propagating diagonally across the domain fig 3 6 a the reflective clamped obc is accurate only if the obc data is well specified with the interior solution in this case under specification of the barotropic mode occurs as the parent uses constant child data over the barotropic step and the child uses a linear interpolation over the parent baroclinic time step the under specification error may be reduced by increasing the frequency of data exchange indeed penven et al 2006 achieved this by coupling parent and child at the barotropic time step we take the different approach of using an obc at the dynamic and feedback interfaces that may be non reflective and thus tolerant to specification error i e fig 3 4 using a less reflective gravity wave radiation condition in conjunction with the active radiation condition of blumberg and kantha 1985 where the relaxation timescale is 900 s improves the solution from the clamped configuration however this condition is known to be prone to basin filling and emptying palma and matano 2001 which occurs in the two way grids at different rates resulting in large gradients across the interface and ultimately instability at 5 4 days fig 3 6b if not for this effect the solutions appeared reasonable with the inclusion of a more passive obc resulting in increased model stability and accuracy the flather boundary condition does not suffer basin filling emptying since it is conservative palma and matano 2001 solutions using this obc are displayed in fig 3 7 showing reasonable agreement with the reference solution although the vortex does distort as it passes the dynamic interface the most prescriptive data applied to the obc in this case is the depth averaged normal velocity hence this obc may benefit from some coupling at the barotropic level the domain averaged rmse is displayed in fig 3 8 for the dirichlet and flather obc solutions from which it is seen that the former provides the more accurate solution these tests demonstrate the benefit or lack thereof of using a well behaved obc to minimize boundary specification issues and improve stability in the case of fig 3 4 circumventing the need to couple at the barotropic level to maintain stability the two way nesting using the dirichlet obc was also run where the child uses a synchronization time of δs δt and the parent uses δs 0 flip case this means that the child advances first and the parent catches up allowing no interpolation in time for the child both models use data exchanged at the dump increment for a period of time equal to the parent s time step the area averaged rmse is displayed in fig 3 8 from which it is seen that there is only a small deterioration in accuracy compared to the dri case and considerable more skill than the one way simulation allowing the models to run concurrently and race each other is achieved by using δs δt for both models race case in this case we don t exchange data every time step but use δs δtd 2δt results are also displayed in fig 3 8 where the rmse is almost identical to the flip case however since the models now run concurrently rather than sequentially for part of the parent s time step in this case runtime is approximately 22 faster than dri these experiments indicate that the temporal exchange of information is not critical in terms of accuracy and stability and data need not be exchanged at the parent time step with temporal interpolation by the child the nature of the obc applied appears to be the critical component in the scheme with the dirichlet condition of herzfeld and andrewartha 2012 providing best results the total volume and heat for several experiments is shown in fig 3 9 this is presented as a percentage deviation from the initial volume and heat content note that the parent and child totals are added to achieve a domain total some deviation with the reference is expected since the parent uses a coarser resolution it is seen that total volume of the dri case follows the temporal evolution of the reference case with a mean deviation of around 0 0003 from the reference volume the fla case exhibits more variability than the reference with a mean difference of around 5 5 total heat deviates from the baseline for both the two way coupled tests where heat content marginally decreases in the reference run but increases in the dri case the fla case shows an increase in heat content followed by a decrease mean differences from the reference are the same as for total volume the two way nesting schemes do not include any explicit conservation preserving techniques hence while the conservation is acceptable it is not perfect the dirichlet obc used is however conservative elevation and tracers obey the continuity and tracer conservations equations respectively in the boundary cell additionally a local flux adjustment herzfeld and andrewartha 2012 is employed to maintain consistency in elevation over the dynamic and feedback interfaces and prevent the parent child sea level from diverging any conservation error is therefore due to interpolation error during the interpolation and update steps noting that a conservative interpolation scheme is not currently applied in this scheme conservation techniques such as flux correction or the kurihara method are not pursued this is elaborated in the next section 4 real application the ereefs initiative seeks to model the whole of the great barrier reef gbr on australia s east coast schiller et al 2014 the hydrodynamic component of this initiative represents the first time that a three dimensional model is applied at the whole reef scale from papua new guinea to the nsw qld border many of the issues of interest to managers of the gbr relate to nutrient supply from the catchment into the gbr lagoon and estuaries joining the gbr lagoon with the catchments are key conduits for the processing and delivery of these nutrients however the regional gbr models are too coarse to adequately resolve these estuaries resulting in crude approximations of nutrient and freshwater delivery to the lagoon using open boundary conditions at the coast in the worst case it has been shown that sub tropical macro tidal estuaries pump suspended sediment up estuary dyer 1988 margvelashvili et al 2003 hence prescribing a sediment flux measured or modelled at the estuary head into an open boundary at the coast will not even approximate the direction of the flux correctly two way nesting offers a solution to resolving the estuaries and improving tracer delivery into coarser models where a child model of the estuary is resolved to beyond the influence of salt wedge propagation and tidal influence and may be coupled to a regional model which can only resolve tracer input at the coast rather than the head of the estuary such a two way coupled system is displayed in fig 4 1 where the fitzroy estuary is coupled to a regional 4 km model of the whole gbr parent and child grids are developed independently to optimise resolution with respect to geography bathymetry and nesting ratios this typically results in non aligned grids with no bathymetry matching and large interface separation any two way scheme should be capable of handling these characteristics a model of the whole gbr has been developed within ereefs with resolution of 4 km gbr4 and we use this as the parent domain this model was forced at the open boundaries with 10 km resolution global model described by oke et al 2008 this global model is based on mom4p1 griffies 2009 and delivers daily mean outputs of sea level t s and 3d velocity low frequency sea level from this model was superimposed with a tidal signal from the global tide model of cartwright and ray 1990 using the implementation of eanes and bettadpur 1995 this model uses surface fluxes derived from the australian bureau of meteorology s operational atmospherics models access a http www bom gov au nwp doc access nwpdata shtml at resolution of 12 km the gbr4 model has 48 vertical layers at the surface resolution is 1 m extending from 2 m above mean sea level to 2 m below vertical resolution decreases with depth to 200 m at the bottom 4000 m the model is wetting and drying so that tidal flats and shallow reefs are exposed at low tide maximum tidal range is 6 m in the central gbr the k ε turbulence closure scheme is used and laplacian horizontal friction is used with the smagorisnky parameterisation lateral boundaries use a free slip condition the advection scheme for tracers is that of ultimate quickest leonard 1991 the heat flux was computed from standard meteorological variables provided by access a wet and dry bulb temperature air pressure wind speed and cloud amount using short and longwave calculations outlined in zillman 1972 and the bulk method for sensible and latent heat using bulk coefficients of kondo 1975 for the surface freshwater fluxes precipitation was provided by access a and evaporation was computed from the latent heat flux wind speed was converted to stress using the bulk scheme of large and pond 1981 the model is also forced with 22 freshwater inputs corresponding to the discharge of the major river systems the 3d time step is 90 s and the 2d step is 5 s the child model of the fitzroy estuary uses an orthogonal curvilinear grid to resolve the estuary landward to the estuary head beyond the limit of tidal influence where a barrage exists and beyond which salinity is zero the model is more highly resolved at the coast allowing numerous tidal creeks to be represented and resolution decreases seaward to be approximately double the parent model resolution at the dynamic interface mean grid size is 700 m and 20 layers exist in the vertical with 0 4 m resolution at the surface and 5 m at the bottom time steps are 6 and 3 s for the baroclinic and barotropic modes respectively surface forcing turbulent closure and advection schemes are the same as the parent model the parent open boundary lies within the mouth of the fitzroy estuary labelled obc in the child domain fig 4 1 resulting in large interface separation grids are also arbitrarily aligned and bathymetry can differ at the dynamic interface the update is only performed at the feedback interface in the parent in fact much of the child grid lies outside the parent domain limits hence an update is not possible the parent and child models ran on independent compute architectures with data transferred via file this is not the most efficient approach but serves to demonstrate the autonomous nature of the method parent and child domains with overlap are displayed in fig 4 1 the grid mesh seaward of the dynamic interface in the child domain is unused and was required to optimise resolution in the child orthogonal curvilinear grid the non alignment of grids for these types of target applications i e two way nesting of optimized estuary models in coarser resolution regional models makes the implementation of conservation techniques such as flux correction or the kurihara method impractical even if implemented conservation techniques can lead to instability or inaccuracy debreu and blayo 2008 conservation error using the dirichlet obc is acceptable and conservation of parent and child models are no worse than the respective one way models in isolation however more sophisticated techniques are required to guarantee perfect conservation we consider the issue of conservation a research question in its own right and consequently do not attempt a detailed consideration in this study the coupled models were simulated from june to august 2012 during which there were moderate flow events of 1000 m3 s 1 fig 4 2 there exists little data to verify the correct response of the model to a flow event however data obtained at buoy 1 fig 4 1 during december 2004 to april 2005 which lies outside the simulation period hence cannot be used for direct comparison displays similar characteristics to that during the modelled period where a smaller flood peak of 500 preceded a larger peak of 1000 m3 s 1 by around 1 month allowing some comparison of the model response to observation under similar flows note that in the parent grid the buoy 1 measurement site is located in the next cell adjacent to the feedback interface the two way coupling should result in more accurate salinity prescription in the boundary cell at the coast in the regional model and preserve continuity of the freshwater plume across the interface salinity at the feedback interface not shown is similar between parent and child models this is a consequence of the smoothing scheme used in the update step which increases salinity before transferring to the parent the salinity input into the parent model is brackish due to the mixing occurring along the estuary in the child if freshwater is directly introduced into the parent coastal boundary cell using 1 way nesting i e the estuary is not resolved the salinity is considerably lower this is illustrated in fig 4 3 where fresh water was input directly into the parent boundary cell over the full water column cyan plot and surface layer only magenta plot these results have been low pass filtered to remove oscillations at the tidal frequency herzfeld 2015 developed a dynamic freshwater input method for the boundary cell that accounts for landward baroclinic flow and in estuary mixing this scheme results in a similar response to the 2 way approach black plot in fig 4 3 although direct comparison cannot be made to observation the relative response to similar flow conditions in different years can be assessed notably the direct input responses appear too fresh during the flow events compared to observations under similar magnitude flows specifically the first flood peak in 2012 595 m3s 1 is smaller than the second peak in 2005 870 m3s 1 but modelled salinity corresponding to the former is 6 4 psμ less than that observed under the higher flows in the latter this is because fresh water is delivered directly into the model at the coast taking no account of mixing processes acting along the estuary length that act to increase salinity the 2 way nested and dynamic methods provide a solution similar to observation in this regard because in estuary mixing is accounted for explicitly in the child solution of the 2 way scheme and implicitly via parameterisation in the dynamic method for the 2 way solution this is a consequence of tuning the child fitzroy model to behave in this manner and is largely independent of the 2 way nesting process indeed a spectrum of responses may be achieved by altering fitzroy river bathymetry and vertical and horizontal mixing parameterisations the advantage of the 2 way approach is that if the fitzroy model is calibrated to an acceptable level of skill independently using 1 way nesting if necessary then adopting that parameterisation in a 2 way configuration allows the parent model to inherit a similar level of skill the child model is easier to calibrate in isolation since a known salinity of zero can be used at the upstream riverine boundary condition if the estuary is accurately resolved far enough inland whereas if the estuary is not resolved in the parent then the prescribed salinity at the coast is a function of many processes and is generally unknown herzfeld 2015 discusses this issue in detail continuity of the salinity response across the dynamic interface is smooth fig 4 4 shows the salinity distribution before during and after the july flood peak which shows that a continuous salinity transition exists across the dynamic interface despite the misaligned grid orientation of parent and child at this interface this example demonstrates that the 2 way method is capable of producing realistic solutions in a complex environment 5 discussion this study presents a framework capable of performing two way nesting largely using existing infrastructure present in a modern ocean code base while the framework accommodates the elements necessary for two way nesting it must still be optimally configured by the user to achieve stable and accurate solutions this includes optimizing restriction operators interface separation conservation and sponge layers as reported by debreu et al 2012 such optimisation is not addressed in detail in this study additionally within this framework the user has the choice of open boundary condition to implement at their disposal to assist in model optimisation the issue of conservation is a weak point in two way nesting since unless conservative interpolation is performed there is an associated loss of conservation conservation issues are considered in debreu and blayo 2008 the techniques used to achieve conservation flux correction kurihara method may be implemented within the two way framework presented here but again this is not the focus of this study no doubt the quality of the test solutions presented in this study could be improved by using alternative restriction operators interface separation etc however the full parameter space of these techniques has not been explored information exchange using this approach need not be confined to 2 way nesting conceptually different models may exchange data using the same methodology e g ocean atmosphere ocean wave or hydrodynamic biogeochemical coupling these coupled model application have traditionally communicated via an overarching coupler e g oasis valcke 2013 these sorts of couplers present as an additional binary library that must be configured initialized and synchronized with the additional component binaries this includes insertion of library function calls at the appropriate places in the component source code and the coupler controls re gridding and information transfer usually through mpi couplers such as oasis dispatch data to the component binaries via the library coupler routines that are embedded in the coupler source code in oasis the receiving functions are blocking they only return when all coupling data is received whereas the send routines return even if data exchange is not complete in our approach data is dispatched directly from the component codes and the reading of data is also blocking in both cases each component does not know of the other s existence but in the former the components must know of the coupler s existence and must be configured via coupling initialisation variable and grid definition after which all communication is controlled via the coupler the autonomous approach outlined above is simpler and less intrusive into the component code and has the advantage that a controlling coupler is absent so the component binaries can be executed autonomously similar to the real test case presented above it could also be possible to two way nest existing regional models in existing global models such that for example improved dynamics around regions of significant freshwater discharge could be achieved the attraction of using this methodology for such a task is that individual models could be developed and operated independently with linkage only occurring via the mpi boundary exchange 6 conclusions the aim of this paper is to contribute two new concepts to two way nesting methodologies 1 perform the mechanics of two way nesting using existing in code infrastructure without the requirement of overarching orchestration and 2 use non reflective open boundary conditions at the dynamic and feedback interfaces to improve the solution the former has been demonstrated in an idealized and real environment where in the idealized case a travelling baroclinic vortex is simulated and two way nested solutions compare favourably with a baseline large domain high resolution solution in the real application the salinity response due to a flood event in the fitzroy estuary within the great barrier reef compared favourably with observations and delivered realistic solutions with smooth salinity transition across the dynamic interface stable solutions were achieved in both instances the use of open boundaries in the interfaces is demonstrated in the idealized case where a number of open boundaries were trialled revealing that a clamped open boundary performed the worst becoming unstable and the obc of herzfeld and andrewartha 2012 coupled at the baroclinic level produced solutions most comparable to the baseline the clamped open boundary was configured in a manner analogous to traditional two way nesting and become unstable due to under specification error introduced at the interfaces stability was achieved by replacing the clamped open boundary with non reflective conditions e g gravity wave radiation flather and the obc of herzfeld and andrewartha 2012 the use of these open boundary conditions may potentially provide an extra layer of optimisation in general two way nesting approaches and the use of in code functionality to perform two way nesting may potentially make these approaches more accessible to the wider modelling community acknowledgements we gratefully acknowledge the ereefs sponsors for making this research possible the ereefs project is a collaboration between the great barrier reef foundation bureau of meteorology commonwealth scientific and industrial research organization australian institute of marine science and the queensland government supported by funding from the australian and queensland governments the bhp billiton mitsubishi alliance and the science and industry endowment fund software availability the software used in this study is the environmental modelling suite https research csiro au cem software and in particular the hydrodynamic model shoc contained within this suite https research csiro au cem software ems hydro strucutured shoc documentation for the model can be found at https research csiro au cem software ems ems documentation this software was developed by the csiro coastal environmental modelling team within the o a coasts program contact mike herzfeld csiro au the software is written in c operates on any linux operating system and is freely available from github https github com csiro coasts ems for 2 way nesting file transfer the simplest conceptual data transfer the relevant write routines df memory write are located in ems main model hd outputs dumpfile c and read routines in ems main lib io datafile c 
26221,two way nesting is becoming a popular method to resolve limited area domains at high resolution however implementation of two way nesting into ocean models is not a trivial process we present an alternative two way nesting framework that leverages off existing functionality present in many ocean codes including arrays to store data exchanged between grids data exchange protocols and re gridding an important element of this is the use of existing open boundary infrastructure to store and apply the data exchanged in two way nesting reduce specification error at the boundary and circumvent coupling at the barotropic levels the resulting two way system consists of models that operate in isolation of each other and can be considered autonomous in the sense that no overarching coupling infrastructure is required to orchestrate the two way nesting and models for respective grids can operate independently with communication achieved only via the sending and receiving of packets of self describing information keywords modelling mathematical modelling boundary conditions open boundary conditions time synchronization 1 introduction accurately capturing events at small scales in ocean models is best achieved by increasing the resolution of an ocean model however this increases computational demands through an increased number of cells to simulate and stability and accuracy constraints dictating the use of a small time step the throughput of high resolution models that cover large areas can be prohibitive and the technique of nesting high resolution models in larger scale coarser resolution models is a popular approach to circumvent this issue these models are usually one way nested where information only flows from the coarse resolution model the parent grid to the high resolution model the child grid e g mason et al 2010 penven et al 2006 recently two way nesting is becoming increasingly used where information from the child is allowed to propagate back into the parent domain e g debreu et al 2012 cailleau et al 2008 jouanno et al 2008 an excellent review of the issues surrounding two way nesting can be found in debreu and blayo 2008 the process of two way nesting requires a number of elements to be carried out and this is typically handled by a coupler e g agrif debreu et al 2008 agrif adaptive grid refinement in fortran is a package that facilitates programs written in fortran to communicate information between parent and child grids in both a one way and two way sense agrif has been implemented in several ocean models viz roms debreu et al 2012 nemo jouanno et al 2008 and opa8 1 cailleau et al 2008 agrif consists of a series of model independent procedures such as interpolation and update procedures refinement clustering and time integration algorithms and model dependent routines that includes the replacement of the main time integration loop of the model with an agrif equivalent and writing an interface routine that transforms the model s configuration file controlled by keywords the implementation of such a package into a new model is not a trivial process however it needs to be performed only once after which the benefits of the agrif coupling can be realized the disadvantage is that such a process often diverges ones model code from the core branch many contemporary regional coastal ocean models codes contain existing functionality that can be exploited to facilitate two way nesting in this paper we demonstrate this to be the case using the code described by herzfeld 2006 this is a mode split finite difference model based on blumberg and herring 1987 using an orthogonal curvilinear arakawa c grid in the horizontal and z or σ coordinates in the vertical leapfrog time stepping is used for momentum and the model is explicit except for the vertical mixing which uses an implicit scheme a variety of momentum and tracer advection 2 equation turbulence and open boundary schemes may be used the 2 way nesting framework developed using this model is applied to an idealized test domain and a real application to demonstrate its utility the implementation of such a system requires little modification to existing code and preserves the integrity of the core branch with full backward compatibility it also results in a system that can leverage functionality inherent in the code that improves the 2 way nesting model solution this is particularly true of the open boundary infrastructure especially the application of open boundary algorithms designed to cope with specification error due to boundary data and interior solution mismatches resulting in increased model stability the two way system is also autonomous in the sense that no overarching coupling infrastructure is required to orchestrate the two way nesting the parent and child models run independently as stand alone implementations on the same or different processors or machines using the same or potentially different model codes without knowing of the other s existence all communication is achieved only via the sending and receiving of packets of self describing information in section 2 we review the two way nesting problem and articulate the elements required for two way nesting to operate we then show how these elements can be accomplished within existing infrastructure inherent in the ocean model an idealized test case that models the propagation of a baroclinic vortex is introduced in section 3 after which we demonstrate the functionality of the two way system a real application is undertaken in section 4 and concluding remarks are presented in section 5 and 6 2 the autonomous two way system a simplified view of two way nesting involves the insertion of a high resolution child grid ωc operating with a small time step δt in a coarser resolution parent grid ωp using a longer time step δt and allowing information from both models to propagate freely over the interfaces of the grids the procedure is similar to one way nesting where the parent model will advance an increment of time typically δt then the child model will perform many integrations up to this time using boundary information from the parent at the start of its step once the parent time increment is reached the parent advances further and the whole process repeats this may be performed sequentially with parent information stored offline the passing of information from the parent to the child is termed the interpolation step since an interpolation of data in time and space onto the high resolution child grid points is generally required the region of the child grid that information is interpolated onto is termed the dynamic interface γ the difference with two way nesting is that at the parent s time increment the child provides information back to the parent on a region of the parent grid called the feedback interface this step is called the update or restriction step and information from the child is usually filtered to remove small scale structure by a restriction operator in practice the whole area in ωp occupied by ωc is often subjected to the restriction step but it is only those values on the feedback interface that affect the solution in ωp in practice the problem is more complex due to the fact that ocean models are usually mode split where a barotropic mode operates on a smaller time step than a baroclinic mode the two way nesting must take into account information transfer between both these modes which adds complexity effectively data must be exchanged every barotropic step in the case of split explicit models such as the one used in this study there are many issues involved in two way nesting that we do not attempt to explore in detail here the reader is referred to debreu and blayo 2008 for a comprehensive account of the details of two way nesting a schematic of the two way nesting process is provided in fig 2 1 to facilitate two way nesting a coupler is generally required that performs the following essential functions 1 provide infrastructure to accommodate the transfer of information e g agrif uses pointers 2 provide algorithms that perform re gridding necessary for the interpolation and update steps 3 perform the physical transfer of data e g agrif uses direct memory access 4 synchronize the information exchange in time many contemporary regional or coastal ocean models already contain most of the infrastructure required to perform these tasks and if re configured enable two way nesting to be performed with minimal code alteration and without the need for a coupler to orchestrate these tasks in the following we assume the model uses mode splitting on an arakawa c grid the above tasks are considered in turn data transfer the open boundary infrastructure inherent in the ocean model can be used to accommodate the information required for transfer between the grids open boundaries can be constructed along the dynamic interface for the child grid and feedback interface for the parent grid and these can be populated with information at the interpolation and update steps to be used by the respective grids at the next time step the update step only occurs along the open boundary in the parent rather than the whole region occupied by the child in the parent domain two way nesting usually directly copies data onto dynamic and feedback interfaces with interpolation or update operators applied as necessary a direct copy of information from one grid into the boundary interface of the other essentially corresponds to a clamped boundary condition the performance open boundary conditions has been extensively assessed in the literature e g palma and matano 1998 with a consensus that a clamped condition i e direct prescription of data onto the boundary is undesirably reflective this can lead to an ill specified boundary marchesiello et al 2001 and ultimately model instability in the 2 way nesting context models using direct transfer of information usually need to be coupled at the barotropic level to improve boundary specification and maintain stability regional ocean models usually contain a suite of open boundary conditions obcs many of which are expressly designed to prevent reflection of information and minimize boundary specification error these can be leveraged in 2 way nesting to allow the grid interfaces to be transmissive to transient perturbations between grid interfaces e g short waves propagating from child to parent and result in a solution that is more tolerant of error introduced where transfer of data between parent and child does not contain enough information to accurately drive the dynamics under specification or information that is not compatible with the interior dynamics over specification this use of obcs to add an additional layer of stability is not commonly practiced in 2 way frameworks and can circumvent the need to stabilize the system by coupling at the barotropic level in section 3 we demonstrate that a viable two way solution is achieved using non reflective obcs without coupling at the barotropic level but when clamped obcs are used instability results the relative orientation of normal velocity on the model grid relative to elevation i e the cell centre on an arakawa c grid can be manipulated to allow the obc exactly mimic the dynamic feedback interfaces utilized in traditional grid refinement this relative orientation is termed boundary implementation by herzfeld 2009 where nvoe normal velocity outside elevation refers to the situation where normal velocity is located at the outside face from the cell centre and nvie normal velocity inside elevation refers to an inside orientation fig 2 2 illustrates this process for western and eastern edges where a fine mesh is nested inside a coarse mesh fig 2 2 a corresponds to a traditional arrangement of grid cells for two way nesting as described by zhang et al 1986 spall and holland 1991 or fox and maskell 1995 fig 2 2 b shows how an implementation using nvoe for the child grid and nvie for the parent grid exactly mimics this structure when used in conjunction with clamped obcs this can be further generalised to non reflective obcs e g the condition of herzfeld and andrewartha 2012 as in fig 2 2 c in principle any obc may be used to facilitate the transfer of information and any supplementary boundary approaches may be imposed e g sponge zones nudging zones flow relaxation re gridding most modern codes perform re gridding of data during model output where data can be interpolated onto predefined latitude and longitude locations e g stations in roms this capability can be used to pre package data at the exact locations required for information exchange in two way nesting alternatively many models also perform inline interpolation when inputting forcing data including open boundary data the routines that manage data input are often accessible as a model independent library function and are capable of interpolating in space and time sophisticated approaches including buffering bit shifting caching of weights and binary searches in tree structures for conversion of grid indices to latitude longitude coordinates can be used to increase efficiency of input to the extent that it comprises only a negligible fraction of total runtime during output a spatial mean around a particular geographic coordinate may be computed in this way the restriction operators used to filter the child solutions can be embedded and allow filtered data to be supplied to the parent s open boundary vectors data communication the two way nesting approach being presented in this paper aims to minimize the code changes in the model and to make use of as much of the existing boundary functionality as possible as almost all models already support one way nesting using files on disk the exact same approach may be applied here with only minor changes for them to work in the two way paradigm all that is required is for the models to adopt the time synchronization scheme as described in the next section the self contained output data packet with extra timing information gets written to a file and the target model reads in the boundary data similar to a one way scheme but with additional waiting to maintain synchronization when needed indeed while this system works and has been tested by the authors it is not ideal in terms of the performance penalty imposed by the filesystem it also forces the models operating in two way nesting mode to have access to the same filesystem thus severely limiting the ability for true distribution of models across compute resources the data files are also largely redundant in the sense that they are temporary only existing for a short period and then overwritten another approach is to use some form of inter process communication ipc protocol for the exchange of data a common framework that is already familiar to the modelling community is the message passing interface mpi we have made use of its remote memory access rma or one sided communication feature as another implementation of two way nesting this method does away with files entirely and data is exchanged over the network i e the models operate in a distributed sense we have tested this with one model running locally on a desktop and the other on a remote hpc node model results of both the file based and mpi systems were identical this autonomous approach implies that the parent and child models need not use the same code base potentially allowing different models operating on different platforms to be two way coupled ultimately the choice of application and available infrastructure in the model to be leveraged for data communication will dictate the data exchange approach adopted time synchronization the synchronization of data exchange in time is the one element of two way nesting that is not commonly present in ocean codes however by inserting one additional piece of information in data packets that are exchanged and adding an extra line of code at the appropriate location in input routines then time synchronization may be achieved with a high degree of flexibility we introduce the concept of a synchronization increment δs given that a packet of data is dumped at intervals of δtd and contains the time that the data was dumped td then the synchronization time is the time period after td that the data should be used in the alternative grid i e if the model time is t then the data should only be used if t t d δ s inverting this condition leads to a conditional statement that can be inserted in the input routines that allows time synchronization 2 1 if t td δs wait where wait is a function that continuously reads the information exchanged to extract td if the traditional two way time synchronization is to be achieved where the parent advances one time step waits for the child to catch up then advances again then the child grid should send data packets with δtd δs δt and the parent grid sends data with δtd δt with δs 0 conversely this means the child uses δs 0 and the parent uses δs δt this situation is illustrated in fig 2 3 a where at the model start time both parent and child dump data with td 0 the model time is t 0 hence for the child eq 2 1 is satisfied so the child waits for the parent eq 2 1 is not satisfied and the parent advances 1 time step the parent model time is now t δt δtd so a dump step occurs and the parent sends a packet of data to the child with td δt eq 2 1 is now satisfied for the parent t td δs t δt td 0 δs δt and the parent goes into wait mode the child has access to the packet dumped from the parent with td δt so now eq 2 1 is not satisfied and the child will integrate until its model time t δt if the previous packet from t 0 was cached then interpolation between t 0 and t δt can occur for the child when t δt the child will dump a packet with td δt so now for the parent eq 2 1 no longer holds and the parent advances one time step further using obc data dumped at td δt from the child and the whole sequence repeats for another time step note that the above synchronization assumes an execution sequence of model integration followed by model time update then data dumping it may be necessary to use in the conditional statement in eq 2 1 or use δs ε where ε 10 10 is a very small number depending on the sequencing used in a particular code however the general concept of controlling synchronization remotely via the synchronization increment remains the same additional infrastructure must be developed to read and write the synchronization time in data packets exchanged between parent and child but the essence of the time synchronization lies in eq 2 1 in the traditional two way configuration described above the parent and child models execute sequentially however a large degree of flexibility with the data exchange is accommodated using this method for example the models can be made to run concurrently thus decreasing total wall time by allowing the parent and child to race each other in this case both parent and child use δs n δt where n is an integer this situation is illustrated in fig 2 3 b for n 3 and the case where the parent executes faster than the child both models use data from the start of the time step at time t when the parent reaches the wait point then the child may start to interpolate between data exchanged at time t and t 3δt the case where the child executes faster is shown in fig 2 3 c in this case the parent will begin to use data interpolated between t and 3δt when the child reaches the wait point the point at which interpolation begins is dependent on the relative speeds of parent and child therefore the solutions using this configuration may not be exactly reproducible since interpolation may begin at different times depending on system load and compute architecture note that this only applies to the configurations of fig 2 3 b and c the configuration described in fig 2 3 a is exactly reproducible we investigate the practical repercussions of this in the next section the above elements were implemented in the ocean code described by herzfeld 2006 to result in a system capable of two way nesting the parent and child models are simulated as separate stand alone models with all communication achieved by the exchange of data packets we now test the utility of the system in an idealized test domain in the following experiments the models were run on distributed compute architecture with each respective model executing on shared memory across a single node 3 baroclinic vortex test the test case chosen to test the two way nesting system is the baroclinic vortex described by penven et al 2006 and used by debreu et al 2012 a full description of the test case configuration can be found in penven et al 2006 and we summarize the main points here a baroclinic vortex is embedded in the initial condition where a gaussian distribution of pressure is imposed elevated sea level and temperature having an e folding length scale of 60 km and initial velocities that are in geostrophic balance with an imposed maximum geostrophic velocity of 1 ms 1 the vortex exists in the presence of continuous stratification where the brunt vaissala frequency is n 0 003 s 1 the centre of the vortex is located at 38 5os on a β plane a flat bottom exists with depth 5000 m the parent grid size is 1800 1800 km with 30 km grid resolution the child domain is centred in the parent with size of 580 580 km with resolution of 10 km there are 10 vertical layers using a z vertical coordinate system the parent grid is assumed to be unbounded and a passive flather condition is imposed palma and matano 2001 on its open boundaries at the grid limits no bottom friction or vertical mixing is used in either grid the parent grid uses horizontal viscosity of 1000 ms 1 smagorinsky diffusivity using a constant of 0 1 and baroclinic barotropic time step of 900 30 s the child grid uses horizontal viscosity of 375 ms 1 smagorinsky diffusivity using a constant of 0 1 and baroclinic barotropic time step of 150 5 s the grid refinement factor is therefore 3 and the time refinement factor is 6 two reference solutions were generated where the whole parent domain was resolved at high and low resolution 10 and 30 km respectively the initial conditions for the high resolution reference test is shown in fig 3 1 the models run for 120 days the reference solution at intervals of 30 days are shown in fig 3 2 the baroclinic vortex is seen to propagate north westward shedding weak rossby waves in its wake and largely retaining its axisymmetry the vortex weakens as it propagates consistent with the northern hemisphere equivalent presented by debreu at al 2012 when this experiment is represented with a fine grid nested within a coarse grid we are looking for the vortex to transition across the nesting boundary without distortion numerous experiments were conducted using the parent and child grid to replicate the reference solution with various forms of coupling a summary of experiments attempted is provided in table 3 1 the one way nested solution is presented in fig 3 3 where it is seen that some distortion of the vortex occurs as it approaches the north west boundary it weakens becomes asymmetric and has difficulty in transiting the open boundary the two way experiment dri was conducted without sponges and interface separation uses no restriction operator for sea level and t s and is coupled at the baroclinic level i e no coupling at the barotropic level with the grid stencil corresponding to that of fig 2 2 c the open boundary condition embedded in the scheme is that of herzfeld and andrewartha 2012 solutions are shown in fig 3 4 showing that the symmetry of the vortex is preserved to a greater degree than one way nesting as it traverses the open boundary after which the vortex weakens more rapidly due to the coarser resolution in the parent resulting in greater dispersion the two way nested solution is clearly superior to the one way solution interface separation increases the accuracy of traditional two way nested applications debreu et al 2012 where the stencil used is that depicted in fig 3 5 this is similar to that shown in debreu and blayo 2008 fig 14 with the exception that the child normal velocity at the dynamic interface is placed interior to the parent normal velocity making these boundaries nvoe this allows an interpolation of child normal velocities between two non interface parent velocities to occur which was observed to increase stability using this configuration in conjunction with clamped open boundaries i e the traditional two way approach but without coupling at the barotropic level resulted in the model rapidly becoming unstable at 6 h degenerating into large amplitude long wave features propagating diagonally across the domain fig 3 6 a the reflective clamped obc is accurate only if the obc data is well specified with the interior solution in this case under specification of the barotropic mode occurs as the parent uses constant child data over the barotropic step and the child uses a linear interpolation over the parent baroclinic time step the under specification error may be reduced by increasing the frequency of data exchange indeed penven et al 2006 achieved this by coupling parent and child at the barotropic time step we take the different approach of using an obc at the dynamic and feedback interfaces that may be non reflective and thus tolerant to specification error i e fig 3 4 using a less reflective gravity wave radiation condition in conjunction with the active radiation condition of blumberg and kantha 1985 where the relaxation timescale is 900 s improves the solution from the clamped configuration however this condition is known to be prone to basin filling and emptying palma and matano 2001 which occurs in the two way grids at different rates resulting in large gradients across the interface and ultimately instability at 5 4 days fig 3 6b if not for this effect the solutions appeared reasonable with the inclusion of a more passive obc resulting in increased model stability and accuracy the flather boundary condition does not suffer basin filling emptying since it is conservative palma and matano 2001 solutions using this obc are displayed in fig 3 7 showing reasonable agreement with the reference solution although the vortex does distort as it passes the dynamic interface the most prescriptive data applied to the obc in this case is the depth averaged normal velocity hence this obc may benefit from some coupling at the barotropic level the domain averaged rmse is displayed in fig 3 8 for the dirichlet and flather obc solutions from which it is seen that the former provides the more accurate solution these tests demonstrate the benefit or lack thereof of using a well behaved obc to minimize boundary specification issues and improve stability in the case of fig 3 4 circumventing the need to couple at the barotropic level to maintain stability the two way nesting using the dirichlet obc was also run where the child uses a synchronization time of δs δt and the parent uses δs 0 flip case this means that the child advances first and the parent catches up allowing no interpolation in time for the child both models use data exchanged at the dump increment for a period of time equal to the parent s time step the area averaged rmse is displayed in fig 3 8 from which it is seen that there is only a small deterioration in accuracy compared to the dri case and considerable more skill than the one way simulation allowing the models to run concurrently and race each other is achieved by using δs δt for both models race case in this case we don t exchange data every time step but use δs δtd 2δt results are also displayed in fig 3 8 where the rmse is almost identical to the flip case however since the models now run concurrently rather than sequentially for part of the parent s time step in this case runtime is approximately 22 faster than dri these experiments indicate that the temporal exchange of information is not critical in terms of accuracy and stability and data need not be exchanged at the parent time step with temporal interpolation by the child the nature of the obc applied appears to be the critical component in the scheme with the dirichlet condition of herzfeld and andrewartha 2012 providing best results the total volume and heat for several experiments is shown in fig 3 9 this is presented as a percentage deviation from the initial volume and heat content note that the parent and child totals are added to achieve a domain total some deviation with the reference is expected since the parent uses a coarser resolution it is seen that total volume of the dri case follows the temporal evolution of the reference case with a mean deviation of around 0 0003 from the reference volume the fla case exhibits more variability than the reference with a mean difference of around 5 5 total heat deviates from the baseline for both the two way coupled tests where heat content marginally decreases in the reference run but increases in the dri case the fla case shows an increase in heat content followed by a decrease mean differences from the reference are the same as for total volume the two way nesting schemes do not include any explicit conservation preserving techniques hence while the conservation is acceptable it is not perfect the dirichlet obc used is however conservative elevation and tracers obey the continuity and tracer conservations equations respectively in the boundary cell additionally a local flux adjustment herzfeld and andrewartha 2012 is employed to maintain consistency in elevation over the dynamic and feedback interfaces and prevent the parent child sea level from diverging any conservation error is therefore due to interpolation error during the interpolation and update steps noting that a conservative interpolation scheme is not currently applied in this scheme conservation techniques such as flux correction or the kurihara method are not pursued this is elaborated in the next section 4 real application the ereefs initiative seeks to model the whole of the great barrier reef gbr on australia s east coast schiller et al 2014 the hydrodynamic component of this initiative represents the first time that a three dimensional model is applied at the whole reef scale from papua new guinea to the nsw qld border many of the issues of interest to managers of the gbr relate to nutrient supply from the catchment into the gbr lagoon and estuaries joining the gbr lagoon with the catchments are key conduits for the processing and delivery of these nutrients however the regional gbr models are too coarse to adequately resolve these estuaries resulting in crude approximations of nutrient and freshwater delivery to the lagoon using open boundary conditions at the coast in the worst case it has been shown that sub tropical macro tidal estuaries pump suspended sediment up estuary dyer 1988 margvelashvili et al 2003 hence prescribing a sediment flux measured or modelled at the estuary head into an open boundary at the coast will not even approximate the direction of the flux correctly two way nesting offers a solution to resolving the estuaries and improving tracer delivery into coarser models where a child model of the estuary is resolved to beyond the influence of salt wedge propagation and tidal influence and may be coupled to a regional model which can only resolve tracer input at the coast rather than the head of the estuary such a two way coupled system is displayed in fig 4 1 where the fitzroy estuary is coupled to a regional 4 km model of the whole gbr parent and child grids are developed independently to optimise resolution with respect to geography bathymetry and nesting ratios this typically results in non aligned grids with no bathymetry matching and large interface separation any two way scheme should be capable of handling these characteristics a model of the whole gbr has been developed within ereefs with resolution of 4 km gbr4 and we use this as the parent domain this model was forced at the open boundaries with 10 km resolution global model described by oke et al 2008 this global model is based on mom4p1 griffies 2009 and delivers daily mean outputs of sea level t s and 3d velocity low frequency sea level from this model was superimposed with a tidal signal from the global tide model of cartwright and ray 1990 using the implementation of eanes and bettadpur 1995 this model uses surface fluxes derived from the australian bureau of meteorology s operational atmospherics models access a http www bom gov au nwp doc access nwpdata shtml at resolution of 12 km the gbr4 model has 48 vertical layers at the surface resolution is 1 m extending from 2 m above mean sea level to 2 m below vertical resolution decreases with depth to 200 m at the bottom 4000 m the model is wetting and drying so that tidal flats and shallow reefs are exposed at low tide maximum tidal range is 6 m in the central gbr the k ε turbulence closure scheme is used and laplacian horizontal friction is used with the smagorisnky parameterisation lateral boundaries use a free slip condition the advection scheme for tracers is that of ultimate quickest leonard 1991 the heat flux was computed from standard meteorological variables provided by access a wet and dry bulb temperature air pressure wind speed and cloud amount using short and longwave calculations outlined in zillman 1972 and the bulk method for sensible and latent heat using bulk coefficients of kondo 1975 for the surface freshwater fluxes precipitation was provided by access a and evaporation was computed from the latent heat flux wind speed was converted to stress using the bulk scheme of large and pond 1981 the model is also forced with 22 freshwater inputs corresponding to the discharge of the major river systems the 3d time step is 90 s and the 2d step is 5 s the child model of the fitzroy estuary uses an orthogonal curvilinear grid to resolve the estuary landward to the estuary head beyond the limit of tidal influence where a barrage exists and beyond which salinity is zero the model is more highly resolved at the coast allowing numerous tidal creeks to be represented and resolution decreases seaward to be approximately double the parent model resolution at the dynamic interface mean grid size is 700 m and 20 layers exist in the vertical with 0 4 m resolution at the surface and 5 m at the bottom time steps are 6 and 3 s for the baroclinic and barotropic modes respectively surface forcing turbulent closure and advection schemes are the same as the parent model the parent open boundary lies within the mouth of the fitzroy estuary labelled obc in the child domain fig 4 1 resulting in large interface separation grids are also arbitrarily aligned and bathymetry can differ at the dynamic interface the update is only performed at the feedback interface in the parent in fact much of the child grid lies outside the parent domain limits hence an update is not possible the parent and child models ran on independent compute architectures with data transferred via file this is not the most efficient approach but serves to demonstrate the autonomous nature of the method parent and child domains with overlap are displayed in fig 4 1 the grid mesh seaward of the dynamic interface in the child domain is unused and was required to optimise resolution in the child orthogonal curvilinear grid the non alignment of grids for these types of target applications i e two way nesting of optimized estuary models in coarser resolution regional models makes the implementation of conservation techniques such as flux correction or the kurihara method impractical even if implemented conservation techniques can lead to instability or inaccuracy debreu and blayo 2008 conservation error using the dirichlet obc is acceptable and conservation of parent and child models are no worse than the respective one way models in isolation however more sophisticated techniques are required to guarantee perfect conservation we consider the issue of conservation a research question in its own right and consequently do not attempt a detailed consideration in this study the coupled models were simulated from june to august 2012 during which there were moderate flow events of 1000 m3 s 1 fig 4 2 there exists little data to verify the correct response of the model to a flow event however data obtained at buoy 1 fig 4 1 during december 2004 to april 2005 which lies outside the simulation period hence cannot be used for direct comparison displays similar characteristics to that during the modelled period where a smaller flood peak of 500 preceded a larger peak of 1000 m3 s 1 by around 1 month allowing some comparison of the model response to observation under similar flows note that in the parent grid the buoy 1 measurement site is located in the next cell adjacent to the feedback interface the two way coupling should result in more accurate salinity prescription in the boundary cell at the coast in the regional model and preserve continuity of the freshwater plume across the interface salinity at the feedback interface not shown is similar between parent and child models this is a consequence of the smoothing scheme used in the update step which increases salinity before transferring to the parent the salinity input into the parent model is brackish due to the mixing occurring along the estuary in the child if freshwater is directly introduced into the parent coastal boundary cell using 1 way nesting i e the estuary is not resolved the salinity is considerably lower this is illustrated in fig 4 3 where fresh water was input directly into the parent boundary cell over the full water column cyan plot and surface layer only magenta plot these results have been low pass filtered to remove oscillations at the tidal frequency herzfeld 2015 developed a dynamic freshwater input method for the boundary cell that accounts for landward baroclinic flow and in estuary mixing this scheme results in a similar response to the 2 way approach black plot in fig 4 3 although direct comparison cannot be made to observation the relative response to similar flow conditions in different years can be assessed notably the direct input responses appear too fresh during the flow events compared to observations under similar magnitude flows specifically the first flood peak in 2012 595 m3s 1 is smaller than the second peak in 2005 870 m3s 1 but modelled salinity corresponding to the former is 6 4 psμ less than that observed under the higher flows in the latter this is because fresh water is delivered directly into the model at the coast taking no account of mixing processes acting along the estuary length that act to increase salinity the 2 way nested and dynamic methods provide a solution similar to observation in this regard because in estuary mixing is accounted for explicitly in the child solution of the 2 way scheme and implicitly via parameterisation in the dynamic method for the 2 way solution this is a consequence of tuning the child fitzroy model to behave in this manner and is largely independent of the 2 way nesting process indeed a spectrum of responses may be achieved by altering fitzroy river bathymetry and vertical and horizontal mixing parameterisations the advantage of the 2 way approach is that if the fitzroy model is calibrated to an acceptable level of skill independently using 1 way nesting if necessary then adopting that parameterisation in a 2 way configuration allows the parent model to inherit a similar level of skill the child model is easier to calibrate in isolation since a known salinity of zero can be used at the upstream riverine boundary condition if the estuary is accurately resolved far enough inland whereas if the estuary is not resolved in the parent then the prescribed salinity at the coast is a function of many processes and is generally unknown herzfeld 2015 discusses this issue in detail continuity of the salinity response across the dynamic interface is smooth fig 4 4 shows the salinity distribution before during and after the july flood peak which shows that a continuous salinity transition exists across the dynamic interface despite the misaligned grid orientation of parent and child at this interface this example demonstrates that the 2 way method is capable of producing realistic solutions in a complex environment 5 discussion this study presents a framework capable of performing two way nesting largely using existing infrastructure present in a modern ocean code base while the framework accommodates the elements necessary for two way nesting it must still be optimally configured by the user to achieve stable and accurate solutions this includes optimizing restriction operators interface separation conservation and sponge layers as reported by debreu et al 2012 such optimisation is not addressed in detail in this study additionally within this framework the user has the choice of open boundary condition to implement at their disposal to assist in model optimisation the issue of conservation is a weak point in two way nesting since unless conservative interpolation is performed there is an associated loss of conservation conservation issues are considered in debreu and blayo 2008 the techniques used to achieve conservation flux correction kurihara method may be implemented within the two way framework presented here but again this is not the focus of this study no doubt the quality of the test solutions presented in this study could be improved by using alternative restriction operators interface separation etc however the full parameter space of these techniques has not been explored information exchange using this approach need not be confined to 2 way nesting conceptually different models may exchange data using the same methodology e g ocean atmosphere ocean wave or hydrodynamic biogeochemical coupling these coupled model application have traditionally communicated via an overarching coupler e g oasis valcke 2013 these sorts of couplers present as an additional binary library that must be configured initialized and synchronized with the additional component binaries this includes insertion of library function calls at the appropriate places in the component source code and the coupler controls re gridding and information transfer usually through mpi couplers such as oasis dispatch data to the component binaries via the library coupler routines that are embedded in the coupler source code in oasis the receiving functions are blocking they only return when all coupling data is received whereas the send routines return even if data exchange is not complete in our approach data is dispatched directly from the component codes and the reading of data is also blocking in both cases each component does not know of the other s existence but in the former the components must know of the coupler s existence and must be configured via coupling initialisation variable and grid definition after which all communication is controlled via the coupler the autonomous approach outlined above is simpler and less intrusive into the component code and has the advantage that a controlling coupler is absent so the component binaries can be executed autonomously similar to the real test case presented above it could also be possible to two way nest existing regional models in existing global models such that for example improved dynamics around regions of significant freshwater discharge could be achieved the attraction of using this methodology for such a task is that individual models could be developed and operated independently with linkage only occurring via the mpi boundary exchange 6 conclusions the aim of this paper is to contribute two new concepts to two way nesting methodologies 1 perform the mechanics of two way nesting using existing in code infrastructure without the requirement of overarching orchestration and 2 use non reflective open boundary conditions at the dynamic and feedback interfaces to improve the solution the former has been demonstrated in an idealized and real environment where in the idealized case a travelling baroclinic vortex is simulated and two way nested solutions compare favourably with a baseline large domain high resolution solution in the real application the salinity response due to a flood event in the fitzroy estuary within the great barrier reef compared favourably with observations and delivered realistic solutions with smooth salinity transition across the dynamic interface stable solutions were achieved in both instances the use of open boundaries in the interfaces is demonstrated in the idealized case where a number of open boundaries were trialled revealing that a clamped open boundary performed the worst becoming unstable and the obc of herzfeld and andrewartha 2012 coupled at the baroclinic level produced solutions most comparable to the baseline the clamped open boundary was configured in a manner analogous to traditional two way nesting and become unstable due to under specification error introduced at the interfaces stability was achieved by replacing the clamped open boundary with non reflective conditions e g gravity wave radiation flather and the obc of herzfeld and andrewartha 2012 the use of these open boundary conditions may potentially provide an extra layer of optimisation in general two way nesting approaches and the use of in code functionality to perform two way nesting may potentially make these approaches more accessible to the wider modelling community acknowledgements we gratefully acknowledge the ereefs sponsors for making this research possible the ereefs project is a collaboration between the great barrier reef foundation bureau of meteorology commonwealth scientific and industrial research organization australian institute of marine science and the queensland government supported by funding from the australian and queensland governments the bhp billiton mitsubishi alliance and the science and industry endowment fund software availability the software used in this study is the environmental modelling suite https research csiro au cem software and in particular the hydrodynamic model shoc contained within this suite https research csiro au cem software ems hydro strucutured shoc documentation for the model can be found at https research csiro au cem software ems ems documentation this software was developed by the csiro coastal environmental modelling team within the o a coasts program contact mike herzfeld csiro au the software is written in c operates on any linux operating system and is freely available from github https github com csiro coasts ems for 2 way nesting file transfer the simplest conceptual data transfer the relevant write routines df memory write are located in ems main model hd outputs dumpfile c and read routines in ems main lib io datafile c 
26222,two dimensional hydraulic models are useful to reconstruct maximum discharges and uncertainties of historic flood events since many model runs are needed to include the effects of uncertain input parameters a sophisticated 2d model is not applicable due to computational time therefore this papers studies whether a lower fidelity model can be used instead the presented methodological framework shows that a 1d 2d coupled model is capable of simulating maximum discharges with high accuracy in only a fraction of the calculation time needed for the high fidelity model therefore the lower fidelity model is used to perform the sensitivity analysis multiple linear regression analysis and the computation of the sobol indices are used to apportion the model output variance to the most influential input parameters we used the 1926 flood of the rhine river as a case study and found that the roughness of grassland areas was by far the most influential parameter keywords lower fidelity model sensitivity analysis uncertainty historic flood reconstruction 1 introduction currently the dutch water policy is changing from a probability exceedance approach towards a risk based approach in addition to the probabilities of floods due to multiple failure mechanisms this new approach also considers the consequences of a flood the risk based approach results in a significant increase in the safety levels in areas where the consequences are large dutch ministry of infrastructure and the environment and ministry of economic affairs 2014 a maximum return period of 1250 years was defined for the river areas in the probability exceedance approach while the risk based approach has maximum return periods of 100 000 years the prediction of design discharges corresponding to such rare events is highly uncertain these predictions are most often based on relatively short data sets of measured weather conditions or discharges therefore the data set does not include the natural phenomena characterised by a very low frequency barriendos et al 2003 the confidence interval of large design discharges can be reduced by extending the data set of measured discharges with historical and paleo data of extreme flood events neppel et al 2010 sheffer et al 2003 many studies have reconstructed historic floods to expand the data set of measured discharges e g herget et al 2015 herget and meurs 2010 llasat et al 2005 neppel et al 2010 o connell et al 2002 sheffer et al 2003 toonen et al 2015 zhou et al 2002 herget et al 2015 and herget and meurs 2010 reconstructed historic discharges in the city of cologne germany based on historical documents they predicted mean flow velocities at the time of the historic flood events with the use of a reconstructed river channel and floodplain bathymetry the empirical manning s equation was used to estimate the historic discharges of a specific cross section near the city of cologne neppel et al 2010 used hydraulic modelling of a reach of about two kilometres length to account for geomorphological changes with this model present and historic rating curves were constructed and applied to determine flood discharge series neppel et al 2010 o connell et al 2002 used bayesian statistics to create paleohydrologic bound data for flood frequency analysis paleohydrologic bound data represent stages and discharges that have not been exceeded since the geomorphic surface stabilized o connell et al 2002 these bounds are not actual floods but are limits on flood stage over a measured time interval o connell et al 2002 found that paleohydrologic bounds reduce the uncertainties of the flood distribution curve by placing large observed discharges in their proper long term contexts toonen et al 2015 reconstructed lower rhine historical flood magnitudes of the last 450 years with the use of grain size measurements of flood deposits at two separate research locations they made use of linear regression plots between various grain size descriptors and measured discharges to determine the discharges of the historic events above mentioned studies tried to gain insight in the maximum discharge of a historic flood however none of these studies used hydraulic models to describe maximum discharges and its uncertainties along a long stretch of a river including possible bifurcations during the historic events however the use of hydraulic models may decrease the confidence intervals of the predicted maximum discharges of the reconstructed flood events furthermore hydraulic models provide insight in the flow patterns and inundation extents of the historic events for these reasons hydraulic models will be used for historic flood reconstructions in this study hydraulic models require a reconstruction of the historical geometry as input data in addition they require proper boundary conditions to determine the flood wave propagation along the model domain however the data available to reconstruct historic flood events is limited measured discharges or water levels are generally not available also the geometry of the river its floodplains and the hinterland may be uncertain this uncertainty is reflected in the uncertainty of the model input parameters affecting the maximum discharges during a flood event for this reason a sensitivity analysis on the maximum discharge will be necessary to find the input parameter that mostly influences the model output this analysis will also gain insight in the confidence interval of the reconstructed maximum discharge this insight provides us with useful information for other historical geometry reconstructions since parameter prioritization can be used during the reconstruction commonly sophisticated two dimensional 2d hydraulic models in this context also called a high fidelity model see section 2 1 are used for hydraulic modelling this is because they are capable of describing maximum discharges flood extent and inundation patterns with high accuracy however they have the disadvantage that a single run of a discharge wave usually takes at least several hours since sensitivity analyses require many model runs 2d models are not suitable for this purpose to reduce computational time a surrogate model will be set up a lower fidelity model is developed since this type of surrogate model does not lose many physical processes of the original system therefore the objective of this paper is to study whether a lower fidelity hydraulic model can be used for historic flood reconstructions lower fidelity surrogate modelling has just recently started to gain popularity in the water resources literature razavi et al 2012b the modelling approach has been applied to groundwater models to reduce model complexity for optimization and calibration purposes e g maschler and savic 1999 mcphee and yeh 2008 ulanicki et al 1996 it has also been applied in combination with the monte carlo framework for uncertainty analysis e g efendiev et al 2005 keating et al 2010 however almost no studies have applied a lower fidelity surrogate model for hydraulic modelling purposes these models may have great benefits in this field since computational time can be reduced significantly while model accuracy remains sufficient for an elaborated review on surrogate models in environmental modelling see razavi et al 2012b razavi et al 2012b argue that the response patterns of a lower fidelity model and of a sophisticated 2d model can differ even if both models are based on the same input data therefore the results of a 2d model will be used for validation purposes if the model output of the lower fidelity model is close to those predicted by the 2d model the lower fidelity model is capable of accurately simulating the system behaviour hence the lower fidelity model can be used to perform the sensitivity analysis for future work the lower fidelity model can be treated as a high fidelity model the proposed method fig 1 will answer the following three research questions under what circumstances can a lower fidelity model be used to simulate a historic flood event how can we apply a lower fidelity model to compute the maximum discharge and its uncertainty of a historic flood event which uncertain input parameter contributes most to the uncertainty of the maximum discharge we apply the proposed method to the 1926 flood of the rhine river sufficient information is available to reconstruct the 1926 geometry in addition water levels were measured during the event due to high rainfall intensities in the lower rhine catchment area and increased amount of melting water as a result of relatively high temperatures in switzerland the 1926 discharge resulted in the highest discharge at lobith since measurements have been performed the outline of the paper is as follows firstly the high fidelity 2d model is described in section 2 1 after which the surrogate model is set up section 2 2 then the 1926 case is provided and the methodology of the sensitivity analysis is given in section 2 3 and section 3 respectively subsequently the calibration results of the high fidelity model section 4 1 and the validation results of the surrogate model section 4 2 are provided finally the results of the sensitivity analysis are elaborated on section 4 3 the paper ends with a discussion and the main conclusions in section 5 and 6 respectively 2 methodology of surrogate modelling in this section the model structure of a fully 2d model is explained this model represents the high fidelity model in this study and is used to validate the lower fidelity model thereafter the 2d model is simplified to decrease computational time significantly many methods exist to simplify a high fidelity model to create a lower fidelity model why a 1d 2d coupled model is used in this study is explained in section 2 2 2 1 high fidelity model most often 2d flood models are used to get insight in the consequences of high discharge stages with 2d models it is possible to get a high detailed and accurate representation of potential floods along a river up till now the 2d shallow water equations are usually solved with the use of a curvilinear grid fig 2 the curvilinear grid cells are aligned with the flow direction since flow variations in the channel length direction are often smaller than those in channel cross direction kernkamp et al 2011 this is convenient in terms of computational time however a curvilinear grid has several disadvantages firstly grid lines are focused and sometimes even intersect in sharp inner bends fig 2 where the dashed lines indicate the focused grid lines the focused grid lines result in unnecessarily small grid cells if the model domain is extended in the inner bend these small grid cells significantly increase computational time additionally the grid will lead to a staircase representation along closed boundaries since the grid is not capable of following the smooth boundaries of the model domain kernkamp et al 2011 finally the grid is restrictive in representing a natural river system with different geometric features such as main channels junction points and wide floodplains due to the curvilinear shape of the grid cells lai 2010 due to the above mentioned shortcomings of a curvilinear grid a hybrid grid is used to solve the 2d shallow water equations in this study fig 2 the summer bed is discretized by curvilinear grid cells these cells are aligned with the flow direction the winter bed is discretized by triangular grid cells such that each triangular grid cell is connected to a single curvilinear grid cell as a result a smooth transition exists between the curvilinear and triangular grid cells fig 2 this hybrid grid overcomes the shortcomings of a curvilinear grid it also reduces the computational time while model accuracy stays sufficient bomers et al 2019 fig 3 shows the hybrid grid and a typical example of model output the open source software d flow flexible mesh fm is used to set up the 2d model deltares 2016 in each grid cell parameters such as water level and flow velocity can be computed for every time step a variable time step is used based on the maximum courant number as a result the model stays stable during the simulation 1 c u δ t δ x where u represents the velocity magnitude m s δ t the time step s and δ x the grid size in x direction m a maximum courant number of 0 95 is used and δ t is adapted accordingly d flow fm allows multiple roughness definitions to be implemented in a single model run e g a manning s value a nikuradse value or a van rijn predictor in general the land use classifications and hence the roughness classes are based on an input database a database provided by the dutch ministry of infrastructure and water management is used this database includes multiple roughness definitions that coincide with the land use classification of the studied area calibration of a 2d grid is required since each 2d grid has its own numerical friction caused by the resolution of the grid cells caviedes voullième et al 2012 a coarser grid results in a somewhat dampened discharge wave this effect can even become larger than those generated by physical friction caviedes voullième et al 2012 during calibration this numerical grid generated friction will be compensated such that reliable water levels are predicted hydraulic model calibration is most commonly done by changing the roughness of the summer bed until simulated water levels are close to measured water levels e g bomers et al 2019 and caviedes voullième et al 2012 in this study the same approach was used the calibration procedure was performed with the use of the open source software openda http www openda org the basic idea of the procedures of openda is to find the set of model parameters which minimizes the cost function measuring the distance between the measured water level and the model prediction the openda association 2016 the quadratic cost function is used in combination with the sparse dud does not use derivate algorithm for n calibration parameters in this study n 10 the algorithm requires n 1 set of parameter estimates the cost function based on the model predictions and measured data is used to get a new estimate if the cost function does not produce a better estimate the sparse dud algorithm will search in opposite direction and or decreases the searching step until a better estimate is found the openda association 2016 in this study the calibration procedure is stopped if the average rmse of each measurement station is smaller than 0 05 m for more information on the calibration procedure of openda see the openda association 2016 2 2 lower fidelity physically based surrogate model a hybrid 2d grid reduces computational time compared to a curvilinear grid however the computational time of simulating a discharge wave of approximately three weeks is still in the order of many hours for sensitivity analysis purposes many model runs 120 in this study have to be performed therefore a model with a computational time in the order of minutes is desirable for this reason a surrogate model based on the high detailed 2d model is developed this model is explained in more detail in the next sections 2 2 1 types of surrogate modelling surrogate models approximate the response pattern of a high detailed and computationally intensive simulation model razavi et al 2012a many methods to construct a surrogate model exist in literature these methods can be divided into two classes namely 1 response surface surrogates which are statistical or empirical data driven models emulating the original system and 2 lower fidelity physically based surrogates which are simplified models of the high detailed model razavi et al 2012b regardless of the type of response surface surrogates usually three steps are involved simpson et al 2001 1 choosing a design of experiment for generating the training data 2 choosing a statistical or empirical data driven model e g artificial neural network support vector machine gaussian progress regression model to represent the data and 3 fitting the surrogate model to the training data response surface surrogates are commonly used for automatic model calibration razavi et al 2012b to fit the response surface surrogate training data is required therefore the high fidelity model still needs to be run multiple times because of the relatively long simulation time of this model the methods based on response surface surrogates are not desirable for this reason the high fidelity model is simplified using method 2 creating a lower fidelity physically based surrogate model lower fidelity surrogate models are set up based on the original input data therefore for lower fidelity modelling only a single run with the high fidelity model is required for validation purposes moreover lower fidelity models are more reliable in predicting the output of the high fidelity model in unexplored regions of the input space since they predict model output based on the original input data razavi et al 2012b different methods exist to simplify the original model e g larger grid size less strict numerical convergence tolerances or ignoring or approximation physics of the original system razavi et al 2012b those methods were not sufficient to reduce the computational time of the high fidelity model significantly therefore it was decided to approximate several physical processes of the original system by 1 lowering the dimension of the model 2 increasing the computational time step and 3 simplifying the shallow water equations of the fully 2d model the set up of the lower fidelity model is explained in the next section 2 2 2 set up lower fidelity model the surrogate model developed represents a 1d 2d coupled model to combine the advantages of both a fully 2d and a fully 1d model 1d profiles give an accurate representation of flood wave propagation in case of in channel flows tayefi et al 2007 additionally the computational cost is relatively low compared to a fully 2d model however the use of 1d profiles may be insufficient for more complex flow patterns because of the simplified assumptions in the computational schemes in the embanked areas rapidly changes in flow velocity and direction may occur for this reason 1d profiles are solely used for the flow between the winter dikes i e the summer bed and winter bed the 1d profiles are coupled with 2d embanked areas that are possible to inundate the embanked areas refer to the areas protected by dikes and are therefore not part of the river system the embanked areas are discretized with a rectangular 2d grid flexible grid shapes are used along the boundaries of the model domain such that the 2d grid cells follow these boundaries the flexible grid cells along the boundaries can have a maximum of eight boundary edges fig 4 shows an example in which the 1d profiles of the rivers and the 2d embanked areas are given by yellow lines and grey areas respectively a close up of the 2d grid and its flexible grid shapes along the grid boundaries is also provided hec ras v 5 0 3 developed by the hydrologic engineering centre hec of the us army corps of engineers is used for the 1d 2d flood modelling hec ras is well known for its 1d flood modelling applications horritt and bates 2002 even showed that hec ras produces flood extents more accurately than the 2d models of lisflood fp and telemac 2d in cases of a confined and relatively narrow river in 2016 hec ras 5 0 was officially released with this version it is possible to perform 1d 2d coupled computations several studies have shown the applicability of 1d 2d flood modelling most software programs e g mike 11 hec ras that allow 1d 2d coupling are based on mass conservation the conservation of momentum is often neglected bladé et al 2012 argue that neglecting the momentum in the coupling of a 1d profile and the 2d grid cells affects flow patterns in the floodplains in most cases the more connected the river and the floodplains are e g in case of overland flows the more important momentum becomes since an increase in flow velocity results in an increase in momentum bladé et al 2012 conservation of momentum can only be neglected if the 1d profiles are coupled with 2d grid cells by a weir embankment since the hypothesis of the shallow water equations are not fulfilled for this specific case bladé et al 2012 with hec ras the weir equation can be used to compute the flow over the embankment using the results of the 1d and 2d solution algorithms on a time step by time step basis this allows for direct feedback at each time step between the 1d profiles and 2d grid cells brunner 2014 neglecting conservation of momentum is justified for this modelling purpose since the 1d profiles are coupled with the 2d grid cells by an embankment hence the 1d 2d coupling can be treated as a weir type connection 2 2 3 differences between the high fidelity and lower fidelity model a 1d 2d coupled model requires the same input data as a fully 2d model therefore we use the same input data of the high fidelity model to set up the 1d 2d coupled model the digital elevation model dem of the 2d model is used to establish the 1d profiles and 2d grid cells of the 1d 2d coupled model also the boundary conditions consisting of measured discharges and water levels as well as the land use classification for both models are identical therefore we can conclude that the differences in the representations of the input parameters of the high fidelity and the lower fidelity model are solely caused by the level of detail of the two models itself and the different settings of d flow fm and hec ras these differences are explained in more detail below and are summarized in table 1 firstly the 2d shallow water equations of the high fidelity model are simplified to the diffusive wave equations the diffusive wave equations are applicable if flow separation and turbulence eddies can be neglected this is the case if the inertial terms are much smaller than the gravity friction and pressure terms test runs showed that neglecting the inertial terms of the momentum equations did not result in a change in model results on the other hand the use of the diffusive wave equations resulted in a significant reduction of the computational time therefore the diffusive wave equations are used to compute the flow characteristics at each 1d profile and 2d grid cell the applicability of the diffusive wave equations for flood modelling purposes has also been shown by e g moya quiroga et al 2016 moussa and bocquillon 2009 and leandro et al 2014 secondly the computational time step of the lower fidelity model is increased compared to the fully 2d model to speed up computational time in a 2d model the river is usually the time step limiting factor since the depths and velocities in the main channel are larger than in the embanked areas bladé et al 2012 see equation 1 the high fidelity model had an average time step of 3 9 s based on the maximum courant number a fixed time step of five minutes can be used for the lower fidelity model this time step is based on a convergence argument reducing the time step further did not result in a reasonable improvement of the model accuracy the land use classification of the high fidelity model is used as input for the lower fidelity model d flow fm allows multiple roughness definitions to be implemented in a single model however hec ras only allows a manning s roughness coefficient for the various land use classes therefore the roughness classes as used in the high fidelity model were transformed towards manning s roughness values based on tables 5 6 values of the roughness coefficient n of chow 1959 we recall that it is necessary to calibrate the summer bed roughness of the high fidelity model since each 2d grid has its own numerical friction on the other hand it is decided to not calibrate the lower fidelity model as a result the summer bed roughness can be included in the sensitivity analysis as a random parameter this is justified since no inundations along the lower rhine occurred during the 1926 flood event therefore correct prediction of the water levels becomes irrelevant the lower fidelity model is set up to accurately predict maximum discharges at lobith during flood events instead during the simulation the entire discharge wave flows in downstream direction independent of simulated water levels since inundations are not possible to occur consequently it is expected that simulated maximum discharges of the uncalibrated surrogate model are close to those predicted by the calibrated high fidelity model however validation is recommended to study whether the lower fidelity model is capable of simulating the system behaviour sufficiently 2 2 4 validation lower fidelity model razavi et al 2012b argue that even though the lower fidelity model may be based on the same input parameters as the high fidelity model the response pattern can differ somewhat this was also shown by thokala and martins 2007 they neglected the fluid viscosity in the navier stokes equations to set up a lower fidelity model this resulted in less accurate results compared to the high fidelity model the discrepancies between the response patterns of the lower fidelity and high fidelity models mostly influence the local and global minimum and maximum of the system razavi et al 2012b since this study tries to predict maximum discharges during a historic flood event it is of high importance that the global maximum of the system is correctly modelled by the lower fidelity model if this is not the case the discrepancies between the lower fidelity and high fidelity model can be addressed with a correction function razavi et al 2012b these kind of functions correct the response of the lower fidelity model and align it with the response pattern of the high fidelity model it is thus of high importance to validate the lower fidelity model to study whether a correction function is required to tune the model results if the response pattern of the lower fidelity model is close to that of the high fidelity model the lower fidelity model can be treated as the high fidelity representation of the underlying system consequently the lower fidelity model can replace the sophisticated 2d model razavi et al 2012b the sensitivity analysis can then be safely performed with the lower fidelity model since the input parameters of the lower fidelity model are based on the input parameters of the high fidelity model 2 3 the 1926 casus the 1926 flood event of the rhine river is used to examine the methodology of developing a lower fidelity model for historic flood reconstruction the study area stretches from the areas downstream of andernach in germany to the three rhine river branches in the netherlands fig 5 in this paper the german part of the river is referred to as the lower rhine the river enters the netherlands at lobith where it bifurcates into the waal river and pannerdensch canal subsequently the pannerdensch canal bifurcates into the nederrijn and ijssel rivers only the summer bed its floodplains and two embanked areas that are connected by an inlet ooijpolder and rijnstrangen area fig 5 are captured in the model domain the term inlet is used for a dike section with a relatively low crest level due to this low crest level a part of the discharge wave will enter the lower lying area behind the inlet as soon as a certain water level is exceeded as a result the maximum discharge further downstream decreases the dikes represent the boundaries of the model domain and are assumed not to overflow 2 3 1 geographical situation to reconstruct a historical geometry the changes in the river system between the current geometry and the historical period of interest must be defined an existing data set representing the 1995 geometry is made available by the dutch ministry of infrastructure and water management this data set is used as starting point and is adapted such that it represents the historical geometry the following measures were taken to create the 1926 situation fig 5 increase summer bed level due to erosion measurements of the summer bed levels were available for the entire model domain the changes in summer bed level between the 1995 measurements and the oldest measurements available at each location were used to estimate the 1926 summer bed level by linear extrapolation decrease winter bed level due to sedimentation no measured sedimentation rates along the study area were available therefore the following sedimentation rates were used to predict the 1926 winter bed level 1 mm year along the ijssel river pannerdensch canal and lower rhine 3 mm year along the waal river and 0 5 mm year along the nederrijn river silva et al 2001 a linear decrease of the sedimentation rate in channel cross direction was assumed as a result the sedimentation near the summer bed equals the predicted sedimentation rates according to silva et al 2001 the sedimentation near the outside border of the floodplain equals zero dike relocation on the left side of the lower rhine close to the city of emmerich germany the floodplains of the river were much larger in 1926 than they are nowadays the 1926 dike locations and hence the 1926 winter bed were based on old maps dating back to 1895 fig 5 dike relocation provided by the german deichverband xanten kleve der oberdeichinspektor dusseldorf 1895 the current summer dikes along the pannerdensch canal close to the pannerdensche kop were the 1926 winter dikes therefore the present floodplains were not part of the 1926 river system the area outside the 1995 summer dikes were removed from the geometry fig 5 pannerdensche kop restoration of inlets in 1926 two retention areas were possible to inundate at high discharge stages as a result of inlets the spijke inlet caused inundation of the rijnstrangen area when the water level exceeded 15 m nap equal to the crest level of the inlet fig 5 rijnstrangen area in the ooijpolder three inlets were active the total length of the inlets was 150 m the ooijpolder started to inundate at a water level of 12 5 m nap equalling the height of the three inlets the location of the inlets was based on historical 1926 maps fig 5 ooijpolder restoration of meander cut offs in 1955 and 1969 two meanders near doesburg and rheden were cut off fig 5 meander cut offs due to these meander cut offs the total length of the ijssel river decreased with almost nine kilometres the location of the meander bends are based on historical 1926 maps 2 3 2 boundary conditions the 1926 flood event is simulated for a period of approximately three weeks starting on the 22nd of december 1925 till the 8th of january 1926 from the 26th of december onwards the weather conditions changed drastically high rainfall intensities occurred in almost the entire catchment area of the rhine river dutch ministry of infrastructure and the environment 1926 this resulted in a rapid rise of the discharge wave starting on the 27th of december fig 6 shows the discharge wave at andernach representing the upstream boundary condition data source german federal waterways and shipping administration wsv communicated by the german federal institute of hydrology bfg the downstream boundary conditions consist of h t relations based on daily measured water levels available at http waterinfo rws nl and provided by the dutch ministry of infrastructure and water management three streams enter the lower rhine namely the lippe ruhr and sieg rivers these streams were included in the model domain by source points discharge inflow figs 5 and 6 the presented boundary conditions and source points are used in both the high fidelity as well as the lower fidelity model to set up the models 3 methodology of sensitivity analysis in this study uncertainty and sensitivity analyses are performed an uncertainty analysis is executed to compute the maximum discharge at lobith with its standard deviation as a result of the uncertain input parameters next a sensitivity analysis is performed to study which parameter mostly influence the uncertainty of the model output the main objective of the sensitivity analysis is the so called factor prioritization with this prioritization it becomes clear on which parameter to focus during historical geometry reconstruction for flood modelling purposes in order to reduce the potential uncertainty in the model output during the analyses we only focus on the parameters that influence the maximum discharge at lobith a test run was performed in which all roughness parameters along the dutch river branches were increased with 20 in this run the roughness values are close to the upper bound of the truncated normal distributions the run showed that the increase in roughness resulted in only a minor decrease of the maximum discharge at lobith of approximately 0 2 from 12 402 to 12 373 m3 s this minor decrease suggests that the dutch river branches are sufficiently downstream such that the effects of different summer bed roughness on the maximum discharge are negligible therefore the study only focuses on the uncertainties of the input parameters in the most upstream part of the model domain the city of andernach until the location where the rhine river bifurcates into the waal river and pannderdench canal the dutch rhine river branches are seen as fixed boundary conditions of the model since they do not influence model response therefore they can be excluded from the global sensitivity analysis 3 1 input parameters the lower fidelity model is used to establish the uncertainty and sensitivity of the 1926 discharge at lobith only the input parameters that are based on an estimation i e those that are uncertain are included in the analysis in addition parameters that require the development of a new surrogate model when changed e g a planometric change are excluded from the analysis for pragmatic reasons the following parameters are considered during the sensitivity analysis 1 roughness parameters of the various types of land use classes and 2 the bed levels of the summer bed and winter bed in general two kinds of uncertainties exist the first uncertainty is as a result of the randomness of variations in nature inherent uncertainty the second uncertainty is caused by limited knowledge epistemic uncertainty warmink et al 2013 the uncertainty of the different roughness classes is mainly caused by inherent uncertainty since it depends amongst others on the season e g grass grows faster during summer periods resulting in a larger roughness as well as on maintenance e g the frequency of mowing grass fields the uncertainty of the summer bed and winter bed levels are caused by epistemic uncertainty no measured 1926 bed levels are present therefore the bed levels are based on extrapolation techniques and estimated sedimentation rates for all roughness parameters we link the value with the largest probability of occurrence as well as its minimum and maximum bounds to the tables of chow 1959 truncated normal distributions are used in this study since a normal distribution better fits the data if some information about the input parameters is available tails of the distribution and the expected value contrarily a uniform distribution assumes that there is no knowledge about the value with the largest probability of occurrence only a range of input values is known therefore we can conclude that for older historic events the distributions of the uncertain input parameters will shift towards uniform distributions since less and less information is available the roughness parameters are divided into five land use classes summer bed lakes grasslands forest and urban areas a smooth channel with no vegetation is assumed for the entire summer bed having a minimum mannings roughness of 0 025 a normal value of 0 028 which is used as the expected value and a maximum value of 0 033 chow 1959 these numbers are used to set up the truncated normal distribution the same method was used to define the truncated normal distributions of the other roughness classes table 2 a comparable method is used to set up the truncated normal distributions of the summer bed levels and winter bed levels the 1926 summer bed levels were computed based on extrapolation of measured bed level changes see section 2 3 the uncertainty ranges of the summer bed levels were based on these extrapolation values the minimum change in bed level corresponds to no change compared to the oldest measured bed value consequently the 1926 bed level equals the oldest measured bed level the maximum change in bed level equals the extrapolation of the trend between 1995 and the latest measured bed level multiplied with a factor two a factor of two is chosen to include a large uncertainty range the summer bed is divided into three classes 1 from the most upstream location andernach river km 614 until walsum river km 789 here almost no erosion has occurred between 1995 and 1926 additionally the bed level has been compensated for bed level decrease due to mining activities at several locations 2 from walsum until the german dutch border river km 857 here there is relatively much uncertainty in the amount of erosion since the oldest measured bed level dates back to only 1960 3 from the german dutch border till the first bifurcation point of the rhine river river km 867 here there is little uncertainty in the 1926 bed level since the oldest measurements date back to 1934 the winter bed level consists of just one class since no deviations in uncertainty along the lower rhine exist the estimated sedimentation rate of 1 mm year is used to define the ranges of the winter bed level in the lower rhine silva et al 2001 the minimum value equals no change in bed level compared to the 1995 situation the maximum range equals the sedimentation rate of 1 mm year multiplied with a factor of two again a factor of two is chosen to include a large uncertainty range since the 1 mm year sedimentation rate is relatively speculative since the summer bed and winter bed levels vary along the study area their truncated normal distributions and corresponding minimum and maximum values are given as change from its 1926 reference value table 2 these values will be referred to as standardized st bed levels from now on a value equal to zero correspond with the reconstructed 1926 geometry 3 2 design of experiment before a sensitivity analysis can be performed a design of experiment doe has to be defined does employ different space filling strategies to capture the behaviour of the underlying system over limited ranges of the input parameters razavi et al 2012b a doe results in a sample in which the boundary values of the input parameters are based on physical conditions this sample can be used in a monte carlo analysis most commonly used doe methods in literature appear to be full factorial design fractional factorial design central composite design and latin hypercube sampling lhs razavi et al 2012b in general a full factorial design a fractional factorial design and a central composite design require a relatively large number of simulations to generate all combinations to represent the corners of the input space razavi et al 2012b saltelli et al 2008 contrarily lhs can easily scale to different numbers of input parameters without the need for extra simulation runs razavi et al 2012b thus a stratified lhs sample has as advantage that less model runs are requried since a stratified sample achieves a better coverage of the sample space of the input parameters saltelli et al 2000 for this reason a lhs design is used in this study the nine input parameters are divided into eight levels each level has an equal probability of occurrence of 12 5 based on the determined truncated normal distributions in section 3 1 for each run each level is randomly selected constraining that if a level is already selected it cannot be selected again this results in a set of eight simulations in which all eight levels of the nine input parameters are present no clear guidelines exist concerning the minimal number of runs required in a monte carlo analysis this number depends on the number and range of the input parameters and on the shape of the response surface theses features are largely unknown in advance pappenberger et al 2005 in this study convergence of the uncertainty of the discharge at lobith expressed as standard deviation is used as stopping criteria following the method of pappenberger et al 2005 if an additional run results in a change of the standard deviation smaller than 0 05 m3 s it is assumed that the sample sufficiently represents the input space of the different input parameters this criteria resulted in 120 model runs corresponding with 15 latin hypercube sets to check whether the input space is sufficiently captured by the sample two additional model runs were performed with the most extreme situations these scenarios represent the limits of the probability distribution functions of the input parameters table 3 and fig 7 show the range of maximum discharges at lobith modelled in the 120 monte carlo runs and the range found with the two most extreme cases note that all runs are performed with the lower fidelity surrogate model the minimum and maximum values of the sample are close to the predicted values of the two most extreme runs therefore we can conclude that the input space is sufficiently captured by the sampling data set 3 3 stratified monte carlo analysis the results of the monte carlo analysis are used to determine the uncertainty in model predictions additionally the results are used to apportion this uncertainty to the contribution of the individual input parameters two sensitivity analysis methods are used namely multiple linear regression analysis and sobol indices explained in sections 3 3 1 and 3 3 2 respectively 3 3 1 multiple linear regression analysis if the number of simulations is much larger than the number of input parameters a lhs can be very effective in revealing the influence of each parameter using a regression analysis saltelli et al 2008 if the model does not contain any interactions between the input parameters i e the model is additive the linear regression function can be given as scheidt et al 2018 2 y β 0 i 1 n β i x i where y represents the model output in this study the maximum discharge at lobith and x i the different input parameters the coefficients β 0 and β i are determined by the least square computation based on the squared differences between the model output produced by the regression model and the actual model output produced by the surrogate model saltelli et al 2008 the coefficient β i is used to determine the importance of each parameter x i with respect to the model output if the input parameters are independent the absolute standardized regression coefficient β ˆ i can be used as a measure of sensitivity scheidt et al 2018 3 β ˆ i β i σ i σ y where β ˆ i represents the standardized regression coefficient and σ i and σ y represent the standard deviations for the input parameter x i and the model output respectively however the applicability of a linear regression analysis depends on the degree of linearity of the model saltelli et al 2008 a measure for linearity is expressed by saltelli et al 2008 4 r 2 i 1 n β ˆ i 2 where r 2 represents the model coefficient of determination this value is equal to the fraction of the variance of the original data that is explained by the regression model a value of r 2 equal to one indicates that the model is linear saltelli et al 2008 and that the multiple linear regression model is capable of expressing all variance of the original data 3 3 2 sobol indices if the model is not linear sobol indices can be used to determine the sensitivity of the input parameters sobol indices are widely used as global sensitivity analysis method in literature we are specifically interested in the first order indices i e the effect without interactions of input parameters since the sensitivity analysis is used for factor prioritization purposes saltelli et al 2008 li and mahadevan 2016 present an effective method to estimate the first order sobol indices analytically this method can be applied to any kind of data set and is not restricted to a specific sampling strategy furthermore the method can be applied to models with correlated input parameters li and mahadevan 2016 found that the method is highly efficient and that it is especially useful in ranking and identifying important parameters the formula used is as follow li and mahadevan 2016 5 s i 1 e xi v x i y x i v y where s i represents the sobol first order index v x i y x xi indicates the conditional variance of y caused by all input parameters other than x i e xi represents the expected value as a result of fixing input parameter x i and v y represents the variance of y the monte carlo sample has a relatively small size therefore the 95 confidence intervals of the sobol indices are computed based on a resampling strategy the matlab statistics toolbox is used to perform the computation the method to compute the 95 confidence intervals is based on the work of dubreuil et al 2014 in which a bootstrap resampling strategy is used computation of confidence intervals by bootstrap resampling is widely used in global sensitivity analysis and has been used in combination with surrogate models by gayton et al 2003 and janon et al 2011 bootstrap resampling aims at determining confidence intervals of a parameter of interest using only one design of experiment efron and tibshirani 1993 the method consists of the creation of new designs of experiment by drawing with replacement in the original design the method used is presented in fig 8 the lhs sample consisting of 120 model runs is resampled after which the confidence intervals of the first order sobol indices are computed if these confidence intervals have not reached a specific convergence criterion yet more bootstrap resamples are drawn the computation is repeated until the convergence criterion is met the criterion as suggested by dubreuil et al 2014 is used they suggested to stop the procedure at the iteration for which all confidence interval sizes have reached a range which is less than x percent of the maximum bootstrap mean of the sensitivity indices the choice of parameter x depends on the goal of the sensitivity analysis if the goal is only determining the most dominant input parameter a relatively large value of x in the order of 30 can be used however if the model has many variables of equal sensitivity indices it is better to look at the convergence graph at each bootstrap iteration and decide manually when to stop the procedure dubreuil et al 2014 the first convergence criteria 30 is used which will be evaluated by checking the convergence graphs of the sobol indices as suggested by dubreuil et al 2014 4 results 4 1 calibration high fidelity model the river branches lower rhine waal river and pannerdensch canal were calibrated with the use of measured water levels the discharge partitioning along the dutch river branches was based on the report of the dutch ministry of infrastructure and the environment 1952 during the calibration procedure this discharge partitioning had to be met the ijssel and nederrijn rivers were excluded from the calibration procedure since many inundations along the ijssel river have occurred during the 1926 flood event these inundations influence the water levels at both river branches even a very low summer bed roughness near the locations of the inundations did not result in the correct water levels for this study purpose it is accepted that the water levels along the ijssel and nederrijn rivers were not calibrated correctly these branches are located more than 15 km downstream of lobith such that backwater effects has vanished at lobith the ijssel and nederrijn rivers have thus no effect on the maximum discharge at this location in the data set only daily measured water levels are available hence the maximum measured water level may be lower than the occurred maximum water level therefore we calibrated on the three days with the highest water levels for each measurement station present along the river branches if the model is capable of predicting the correct shape and correct water levels at three moments in time near the peak discharge it is likely that also the correct maximum water level is predicted by the model the 1926 discharge wave was simulated maximum water levels at 10 measurement stations were validated after model calibration it was found that simulated maximum water levels only deviated 2 cm on average compared to the measurements therefore it can be concluded that the high fidelity model is capable of simulating maximum water levels with high accuracy after calibration of the summer bed roughness 4 2 validation and uncertainty of the lower fidelity model the model output was compared with the model output of the high fidelity model to study whether it is justified to use the lower fidelity model to perform the sensitivity analysis we found that the high fidelity model simulates a maximum discharge at lobith of 12 282 m3 s with the 1926 measured discharge wave at andernach as upstream boundary condition the lower fidelity model with all random input parameters set to their expected value predicts a maximum discharge of 12 402 m3 s this deviates less than 1 0 compared to the high fidelity model although correct prediction of the maximum discharge at lobith has the focus in this study it is also desirable that the lower fidelity predicts correct discharge stages at other locations table 4 shows that the lower fidelity model predicts maximum discharges along the lower rhine with high accuracy having a maximum deviation of 2 1 compared to the high fidelity model in addition the lower fidelity model is capable of accurately predicting the discharge partitioning along the dutch rhine river branches table 4 these values indicate that the surrogate model is capable of representing the system behaviour of the high fidelity model therefore no correction function is needed to tune the model results of the lower fidelity model we can thus conclude that the lower fidelity model can be treated as a high fidelity model from now on hence the sensitivity analysis can be performed with the 1d 2d coupled model the results of the uncertainty analysis show that the average maximum discharge at lobith as a result of the monte carlo sample equals 12 424 m3 s this value has a standard deviation of 49 m3 s caused by the uncertainty in the input parameters this relatively low standard deviation shows that uncertainties in the input parameters only have a limited effect on the maximum discharge at lobith during the 1926 flood event 4 3 sensitivity analysis 4 3 1 multiple linear regression analysis a multiple linear regression analysis was performed in which it was assumed that the model response as a result of the varying input parameters was linear this is not the case since the model coefficient of determination r 2 equation 4 equals 0 81 this value means that the regression model is capable of explaining 81 of the variance of the surrogate output the remaining 19 is ignored by the regression model however table 5 clearly shows that the roughness of grasslands highly influences the maximum discharge at lobith because of its high sensitivity measure β ˆ i equation 3 the high standardized regression coefficient of the roughness of grasslands can be explained by the fact that grassland is the most dominant land cover in the model domain with a surface area of 55 6 table 5 in addition the uncertainty within the class itself is relatively large table 2 since grasslands most often have a higher roughness during summer periods due to growing season compared to the winter periods only the roughness of forest has a larger uncertainty range however the surface area covered by forest is much less 6 4 4 3 2 sobol indices in the previous section it was shown that with the multiple linear regression analysis only 81 of the variance of the surrogate model output could be explained in order to check the results of the multiple linear regression analysis the sobol indices are computed these indices are independent of model linearity the results show that the roughness of grasslands is dominant with respect to influencing the uncertainty of the maximum discharge at lobith table 6 this is in line with the results of the multiple linear regression analysis if i 1 r s i 1 the variance of the model output is solely caused by the variance of the input parameters itself in that case there are no interactions between the different input parameters resulting in an increase in the variance of the model output in other words the model is additive the results show that the first order sobol indices are approximately 1 indicating that the model does not include any interactions of the input parameters in principle i 1 r s i cannot be larger than 1 in addition the first order sobol index computed for each uncertain input parameter cannot be lower than 0 saltelli et al 2008 in this study the computed i 1 r s i is slightly larger than 1 and the sobol index for the roughness of urban areas is smaller than 0 this is caused by the relatively little sample size of only 120 runs to overcome this problem we resampled the 120 runs as explained in section 3 3 with this resampled data set the 95 confidence intervals of the first order sobol indices are computed fig 9 fig 10 shows that the first order sobol indices have converged after approximately 700 bootstrap resamples this results in a data set of 700 120 model runs the outcomes then show that the roughness of grasslands remains the most dominant input parameter the lower bound of its confidence interval is under any condition larger than the sensitivity index of the other input parameters therefore we can conclude that for this specific case most attention must be paid to the roughness class with the largest surface area and which has a relatively large uncertainty range correct prediction of this parameter will result in a significant reduction of the output variance it must be noted that the uncertainty of the model output was small in this study in general the output variance depends on the probability distribution functions of the uncertain input parameters it can be expected that the output variance will increase for older historic events hence a significant reduction in model output variance can be reached if the most influential input parameter is correctly predicted this influential input parameter can be found by applying the method for factor prioritization as presented in this study 5 discussion in this study a methodology was developed to reconstruct historic flood events with the use of a lower fidelity model the maximum discharge is predicted as well as its uncertainty as a result of the uncertain input parameter general problems that arose were mostly related to the choice of the surrogate model type and the characteristics of the flood event therefore another historic event may ask for a different approach since the assumptions made for the 1926 event may not apply to put things into perspective an overview and discussion are presented of the problems that may arise during historic flood reconstruction and resulting sensitivity analysis 1 to predict a historic discharge an associated geometry should be reconstructed the geometry during the 1926 event was well known since maps of this time period are available however for events further in the past the geometry might be more uncertain these spatial uncertainties must be included in the analyses a major drawback is that for each uncertain geometric situation a separate model must be set up consequently for each model the sensitivity analysis must be performed separately this significantly increases the total number of simulations furthermore for older events the uncertainties in the input parameters may become larger hence the shape of their probability distributions may change we assumed that the uncertain input parameters of the 1926 flood event could be described by truncated normal distributions these distributions will shift towards uniform distributions for older events if less information is available 2 a lower fidelity based surrogate model was developed to reduce computational time many other methods exist to set up a surrogate model each with their own benefits and drawbacks a different study approach may lead to the need of another type of surrogate model in general a 1d 2d coupled model is capable of simulating any kind of flood event the 1d profiles enable correct prediction of discharge stages below bankfull conditions horritt and bates 2002 these 1d profiles can be coupled by 2d grid cells to include the possibility of simulating overland flows if the discharge exceeds the bankfull discharge referring to the situation in which the discharge is larger than the main channel and floodplain capacity therefore this type of lower fidelity model can be used to accurately simulate flood wave propagation for both discharges below as well as above bankfull conditions 3 the 1d 2d coupled model was not calibrated on maximum water levels the objective of the surrogate model was accurate prediction of maximum discharges at lobith however calibration on maximum water levels is required if dike breaches and or overtopping have evolved during the flood event for such a case correct prediction of maximum water levels becomes important since this value indicates whether overtopping occurs this influences the maximum discharge further downstream therefore it is recommended to use the summer bed roughness of the lower fidelity model as calibration parameter to correctly predict water levels in case of discharges exceeding bankfull conditions 4 to perform the sensitivity analysis a decision had to be made about the range of the truncated normal distributions of the input parameters the ranges of the roughness parameters were based on the tables of chow 1959 a smooth channel with no vegetation was assumed to determine the roughness of the summer bed this results in a relatively low expected mannings roughness value of 0 028 with a total range of between 0 023 and 0 033 it is expected that the dimensions of sand dunes during flood events are highly uncertain this uncertainty may influence summer bed roughness significantly the measured mannings roughness of the summer bed during the 1998 event with a maximum discharge of 9464 m3 s at lobith ranges of between 0 030 and 0 035 julien et al 2002 these values are higher than the values that we used paarlberg et al 2010 found a clear dependency between increase in the discharge and increase in the dune heights however it is still unclear to what extent dune heights increase during flood events some literature even suggest that the dunes are washed out under extreme conditions e g best 2005 and naqshband et al 2014 resulting in much lower values of the roughness parameter it is not the roughness value itself that influences the uncertainty of the maximum discharge but rather the uncertainty range of the summer bed roughness therefore the relatively broad roughness range for the summer bed used in this study is considered appropriate for the 1926 flood event 5 in this study only geometrical uncertainties in the input parameters are included in the sensitivity analyses these parameters are the bed levels of the summer bed and winter bed and the roughness of the various land use classes however much more uncertainties exist which can be related to the model structure model parameters and boundary conditions these inherent uncertainties can be considered in the sensitivity analysis by including them as random input parameters in the lhs this will result in more insight in the most dominant type of uncertainty i e uncertainty as a result of the input parameters model parameters or model set up this study is recommended for future work since here we only focused on the uncertainties of the geometrical input parameters to illustrate our method 6 conclusions the objective of this paper was to study whether a lower fidelity hydraulic model can be used for historic flood reconstruction in this paper a general framework is presented that shows which problems have to be tackled in order to enable historic flood reconstruction with the use of a surrogate model a 1d 2d coupled model was developed as lower fidelity model that is capable of simulating flood wave propagation with high accuracy it was found that model results predicted by the lower fidelity model were close to those predicted by the high fidelity model the lower fidelity model is thus capable of accurately predicting system behaviour in addition the proposed 1d 2d coupled model can be applied to any type of historic flood event this is because it is capable of accurately simulating flood wave propagation for both discharges below as above bankfull conditions however if the simulated discharges exceed the bankfull discharge model calibration is recommended since correct prediction of water levels becomes highly relevant for these cases a sensitivity analysis is required to determine the parameters that mostly influence the uncertainty in the model output the lower fidelity model could be used to perform this analysis this significantly decreased computational time compared to the use of a fully 2d model for future work we propose that a 1d 2d coupled model can be treated as a high fidelity model in general therefore setting up a sophisticated 2d model for validation will not be needed the proposed methodology was tested with the use of the 1926 flood event of the rhine river the lower fidelity model predicts a maximum discharge at lobith of 12 402 m3 s for this historic event deviating only 1 0 compared to the high fidelity model 12 282 m3 s the uncertainty of this maximum discharge at lobith equals 49 m3 s the uncertainty in model output is relatively small because a large amount of data of the 1926 flood event was available reconstruction of an older flood event will probably result in larger uncertainties of the input parameters since less information is available as a result the truncated normal distributions used to describe the uncertainty of the various input parameters will shift towards uniform distributions this will have a negative effect on the model output uncertainty the sensitivity analysis showed that the model output was most sensitive to the roughness class with the largest share in surface area in this case the roughness of the grassland areas moreover the location of the roughness class was important since areas close to the river have a relatively large impact on model results these two aspects in combination with the uncertainty range of the input parameter itself determined the influence on model response acknowledgement this research is supported by the netherlands organisation for scientific research nwo project 14506 which is partly funded by the ministry of economic affairs and climate policy furthermore the research is supported by the ministry of infrastructure and water management and deltares this research has benefited from cooperation within the network of the netherlands centre for river studies ncr www ncr web org the authors would like to thank the dutch ministry of infrastructure and water management and the german federal institute of hydrology for providing the data besides the authors would like to thank the following persons for their suggestions and valuable insights prof dr herget university of bonn dr aguilar lopez technical university delft dr lammersen dutch ministry of infrastructure and water management van doornik msc lievense cso berends msc university of twente and dr mara university of la réunion in addition the authors would like to thank the anonymous reviewer sheikholeslami msc unversity of saskatchewan and the associate editor dr razavi for their suggestions during the review process which greatly improved the quality of the paper 
26222,two dimensional hydraulic models are useful to reconstruct maximum discharges and uncertainties of historic flood events since many model runs are needed to include the effects of uncertain input parameters a sophisticated 2d model is not applicable due to computational time therefore this papers studies whether a lower fidelity model can be used instead the presented methodological framework shows that a 1d 2d coupled model is capable of simulating maximum discharges with high accuracy in only a fraction of the calculation time needed for the high fidelity model therefore the lower fidelity model is used to perform the sensitivity analysis multiple linear regression analysis and the computation of the sobol indices are used to apportion the model output variance to the most influential input parameters we used the 1926 flood of the rhine river as a case study and found that the roughness of grassland areas was by far the most influential parameter keywords lower fidelity model sensitivity analysis uncertainty historic flood reconstruction 1 introduction currently the dutch water policy is changing from a probability exceedance approach towards a risk based approach in addition to the probabilities of floods due to multiple failure mechanisms this new approach also considers the consequences of a flood the risk based approach results in a significant increase in the safety levels in areas where the consequences are large dutch ministry of infrastructure and the environment and ministry of economic affairs 2014 a maximum return period of 1250 years was defined for the river areas in the probability exceedance approach while the risk based approach has maximum return periods of 100 000 years the prediction of design discharges corresponding to such rare events is highly uncertain these predictions are most often based on relatively short data sets of measured weather conditions or discharges therefore the data set does not include the natural phenomena characterised by a very low frequency barriendos et al 2003 the confidence interval of large design discharges can be reduced by extending the data set of measured discharges with historical and paleo data of extreme flood events neppel et al 2010 sheffer et al 2003 many studies have reconstructed historic floods to expand the data set of measured discharges e g herget et al 2015 herget and meurs 2010 llasat et al 2005 neppel et al 2010 o connell et al 2002 sheffer et al 2003 toonen et al 2015 zhou et al 2002 herget et al 2015 and herget and meurs 2010 reconstructed historic discharges in the city of cologne germany based on historical documents they predicted mean flow velocities at the time of the historic flood events with the use of a reconstructed river channel and floodplain bathymetry the empirical manning s equation was used to estimate the historic discharges of a specific cross section near the city of cologne neppel et al 2010 used hydraulic modelling of a reach of about two kilometres length to account for geomorphological changes with this model present and historic rating curves were constructed and applied to determine flood discharge series neppel et al 2010 o connell et al 2002 used bayesian statistics to create paleohydrologic bound data for flood frequency analysis paleohydrologic bound data represent stages and discharges that have not been exceeded since the geomorphic surface stabilized o connell et al 2002 these bounds are not actual floods but are limits on flood stage over a measured time interval o connell et al 2002 found that paleohydrologic bounds reduce the uncertainties of the flood distribution curve by placing large observed discharges in their proper long term contexts toonen et al 2015 reconstructed lower rhine historical flood magnitudes of the last 450 years with the use of grain size measurements of flood deposits at two separate research locations they made use of linear regression plots between various grain size descriptors and measured discharges to determine the discharges of the historic events above mentioned studies tried to gain insight in the maximum discharge of a historic flood however none of these studies used hydraulic models to describe maximum discharges and its uncertainties along a long stretch of a river including possible bifurcations during the historic events however the use of hydraulic models may decrease the confidence intervals of the predicted maximum discharges of the reconstructed flood events furthermore hydraulic models provide insight in the flow patterns and inundation extents of the historic events for these reasons hydraulic models will be used for historic flood reconstructions in this study hydraulic models require a reconstruction of the historical geometry as input data in addition they require proper boundary conditions to determine the flood wave propagation along the model domain however the data available to reconstruct historic flood events is limited measured discharges or water levels are generally not available also the geometry of the river its floodplains and the hinterland may be uncertain this uncertainty is reflected in the uncertainty of the model input parameters affecting the maximum discharges during a flood event for this reason a sensitivity analysis on the maximum discharge will be necessary to find the input parameter that mostly influences the model output this analysis will also gain insight in the confidence interval of the reconstructed maximum discharge this insight provides us with useful information for other historical geometry reconstructions since parameter prioritization can be used during the reconstruction commonly sophisticated two dimensional 2d hydraulic models in this context also called a high fidelity model see section 2 1 are used for hydraulic modelling this is because they are capable of describing maximum discharges flood extent and inundation patterns with high accuracy however they have the disadvantage that a single run of a discharge wave usually takes at least several hours since sensitivity analyses require many model runs 2d models are not suitable for this purpose to reduce computational time a surrogate model will be set up a lower fidelity model is developed since this type of surrogate model does not lose many physical processes of the original system therefore the objective of this paper is to study whether a lower fidelity hydraulic model can be used for historic flood reconstructions lower fidelity surrogate modelling has just recently started to gain popularity in the water resources literature razavi et al 2012b the modelling approach has been applied to groundwater models to reduce model complexity for optimization and calibration purposes e g maschler and savic 1999 mcphee and yeh 2008 ulanicki et al 1996 it has also been applied in combination with the monte carlo framework for uncertainty analysis e g efendiev et al 2005 keating et al 2010 however almost no studies have applied a lower fidelity surrogate model for hydraulic modelling purposes these models may have great benefits in this field since computational time can be reduced significantly while model accuracy remains sufficient for an elaborated review on surrogate models in environmental modelling see razavi et al 2012b razavi et al 2012b argue that the response patterns of a lower fidelity model and of a sophisticated 2d model can differ even if both models are based on the same input data therefore the results of a 2d model will be used for validation purposes if the model output of the lower fidelity model is close to those predicted by the 2d model the lower fidelity model is capable of accurately simulating the system behaviour hence the lower fidelity model can be used to perform the sensitivity analysis for future work the lower fidelity model can be treated as a high fidelity model the proposed method fig 1 will answer the following three research questions under what circumstances can a lower fidelity model be used to simulate a historic flood event how can we apply a lower fidelity model to compute the maximum discharge and its uncertainty of a historic flood event which uncertain input parameter contributes most to the uncertainty of the maximum discharge we apply the proposed method to the 1926 flood of the rhine river sufficient information is available to reconstruct the 1926 geometry in addition water levels were measured during the event due to high rainfall intensities in the lower rhine catchment area and increased amount of melting water as a result of relatively high temperatures in switzerland the 1926 discharge resulted in the highest discharge at lobith since measurements have been performed the outline of the paper is as follows firstly the high fidelity 2d model is described in section 2 1 after which the surrogate model is set up section 2 2 then the 1926 case is provided and the methodology of the sensitivity analysis is given in section 2 3 and section 3 respectively subsequently the calibration results of the high fidelity model section 4 1 and the validation results of the surrogate model section 4 2 are provided finally the results of the sensitivity analysis are elaborated on section 4 3 the paper ends with a discussion and the main conclusions in section 5 and 6 respectively 2 methodology of surrogate modelling in this section the model structure of a fully 2d model is explained this model represents the high fidelity model in this study and is used to validate the lower fidelity model thereafter the 2d model is simplified to decrease computational time significantly many methods exist to simplify a high fidelity model to create a lower fidelity model why a 1d 2d coupled model is used in this study is explained in section 2 2 2 1 high fidelity model most often 2d flood models are used to get insight in the consequences of high discharge stages with 2d models it is possible to get a high detailed and accurate representation of potential floods along a river up till now the 2d shallow water equations are usually solved with the use of a curvilinear grid fig 2 the curvilinear grid cells are aligned with the flow direction since flow variations in the channel length direction are often smaller than those in channel cross direction kernkamp et al 2011 this is convenient in terms of computational time however a curvilinear grid has several disadvantages firstly grid lines are focused and sometimes even intersect in sharp inner bends fig 2 where the dashed lines indicate the focused grid lines the focused grid lines result in unnecessarily small grid cells if the model domain is extended in the inner bend these small grid cells significantly increase computational time additionally the grid will lead to a staircase representation along closed boundaries since the grid is not capable of following the smooth boundaries of the model domain kernkamp et al 2011 finally the grid is restrictive in representing a natural river system with different geometric features such as main channels junction points and wide floodplains due to the curvilinear shape of the grid cells lai 2010 due to the above mentioned shortcomings of a curvilinear grid a hybrid grid is used to solve the 2d shallow water equations in this study fig 2 the summer bed is discretized by curvilinear grid cells these cells are aligned with the flow direction the winter bed is discretized by triangular grid cells such that each triangular grid cell is connected to a single curvilinear grid cell as a result a smooth transition exists between the curvilinear and triangular grid cells fig 2 this hybrid grid overcomes the shortcomings of a curvilinear grid it also reduces the computational time while model accuracy stays sufficient bomers et al 2019 fig 3 shows the hybrid grid and a typical example of model output the open source software d flow flexible mesh fm is used to set up the 2d model deltares 2016 in each grid cell parameters such as water level and flow velocity can be computed for every time step a variable time step is used based on the maximum courant number as a result the model stays stable during the simulation 1 c u δ t δ x where u represents the velocity magnitude m s δ t the time step s and δ x the grid size in x direction m a maximum courant number of 0 95 is used and δ t is adapted accordingly d flow fm allows multiple roughness definitions to be implemented in a single model run e g a manning s value a nikuradse value or a van rijn predictor in general the land use classifications and hence the roughness classes are based on an input database a database provided by the dutch ministry of infrastructure and water management is used this database includes multiple roughness definitions that coincide with the land use classification of the studied area calibration of a 2d grid is required since each 2d grid has its own numerical friction caused by the resolution of the grid cells caviedes voullième et al 2012 a coarser grid results in a somewhat dampened discharge wave this effect can even become larger than those generated by physical friction caviedes voullième et al 2012 during calibration this numerical grid generated friction will be compensated such that reliable water levels are predicted hydraulic model calibration is most commonly done by changing the roughness of the summer bed until simulated water levels are close to measured water levels e g bomers et al 2019 and caviedes voullième et al 2012 in this study the same approach was used the calibration procedure was performed with the use of the open source software openda http www openda org the basic idea of the procedures of openda is to find the set of model parameters which minimizes the cost function measuring the distance between the measured water level and the model prediction the openda association 2016 the quadratic cost function is used in combination with the sparse dud does not use derivate algorithm for n calibration parameters in this study n 10 the algorithm requires n 1 set of parameter estimates the cost function based on the model predictions and measured data is used to get a new estimate if the cost function does not produce a better estimate the sparse dud algorithm will search in opposite direction and or decreases the searching step until a better estimate is found the openda association 2016 in this study the calibration procedure is stopped if the average rmse of each measurement station is smaller than 0 05 m for more information on the calibration procedure of openda see the openda association 2016 2 2 lower fidelity physically based surrogate model a hybrid 2d grid reduces computational time compared to a curvilinear grid however the computational time of simulating a discharge wave of approximately three weeks is still in the order of many hours for sensitivity analysis purposes many model runs 120 in this study have to be performed therefore a model with a computational time in the order of minutes is desirable for this reason a surrogate model based on the high detailed 2d model is developed this model is explained in more detail in the next sections 2 2 1 types of surrogate modelling surrogate models approximate the response pattern of a high detailed and computationally intensive simulation model razavi et al 2012a many methods to construct a surrogate model exist in literature these methods can be divided into two classes namely 1 response surface surrogates which are statistical or empirical data driven models emulating the original system and 2 lower fidelity physically based surrogates which are simplified models of the high detailed model razavi et al 2012b regardless of the type of response surface surrogates usually three steps are involved simpson et al 2001 1 choosing a design of experiment for generating the training data 2 choosing a statistical or empirical data driven model e g artificial neural network support vector machine gaussian progress regression model to represent the data and 3 fitting the surrogate model to the training data response surface surrogates are commonly used for automatic model calibration razavi et al 2012b to fit the response surface surrogate training data is required therefore the high fidelity model still needs to be run multiple times because of the relatively long simulation time of this model the methods based on response surface surrogates are not desirable for this reason the high fidelity model is simplified using method 2 creating a lower fidelity physically based surrogate model lower fidelity surrogate models are set up based on the original input data therefore for lower fidelity modelling only a single run with the high fidelity model is required for validation purposes moreover lower fidelity models are more reliable in predicting the output of the high fidelity model in unexplored regions of the input space since they predict model output based on the original input data razavi et al 2012b different methods exist to simplify the original model e g larger grid size less strict numerical convergence tolerances or ignoring or approximation physics of the original system razavi et al 2012b those methods were not sufficient to reduce the computational time of the high fidelity model significantly therefore it was decided to approximate several physical processes of the original system by 1 lowering the dimension of the model 2 increasing the computational time step and 3 simplifying the shallow water equations of the fully 2d model the set up of the lower fidelity model is explained in the next section 2 2 2 set up lower fidelity model the surrogate model developed represents a 1d 2d coupled model to combine the advantages of both a fully 2d and a fully 1d model 1d profiles give an accurate representation of flood wave propagation in case of in channel flows tayefi et al 2007 additionally the computational cost is relatively low compared to a fully 2d model however the use of 1d profiles may be insufficient for more complex flow patterns because of the simplified assumptions in the computational schemes in the embanked areas rapidly changes in flow velocity and direction may occur for this reason 1d profiles are solely used for the flow between the winter dikes i e the summer bed and winter bed the 1d profiles are coupled with 2d embanked areas that are possible to inundate the embanked areas refer to the areas protected by dikes and are therefore not part of the river system the embanked areas are discretized with a rectangular 2d grid flexible grid shapes are used along the boundaries of the model domain such that the 2d grid cells follow these boundaries the flexible grid cells along the boundaries can have a maximum of eight boundary edges fig 4 shows an example in which the 1d profiles of the rivers and the 2d embanked areas are given by yellow lines and grey areas respectively a close up of the 2d grid and its flexible grid shapes along the grid boundaries is also provided hec ras v 5 0 3 developed by the hydrologic engineering centre hec of the us army corps of engineers is used for the 1d 2d flood modelling hec ras is well known for its 1d flood modelling applications horritt and bates 2002 even showed that hec ras produces flood extents more accurately than the 2d models of lisflood fp and telemac 2d in cases of a confined and relatively narrow river in 2016 hec ras 5 0 was officially released with this version it is possible to perform 1d 2d coupled computations several studies have shown the applicability of 1d 2d flood modelling most software programs e g mike 11 hec ras that allow 1d 2d coupling are based on mass conservation the conservation of momentum is often neglected bladé et al 2012 argue that neglecting the momentum in the coupling of a 1d profile and the 2d grid cells affects flow patterns in the floodplains in most cases the more connected the river and the floodplains are e g in case of overland flows the more important momentum becomes since an increase in flow velocity results in an increase in momentum bladé et al 2012 conservation of momentum can only be neglected if the 1d profiles are coupled with 2d grid cells by a weir embankment since the hypothesis of the shallow water equations are not fulfilled for this specific case bladé et al 2012 with hec ras the weir equation can be used to compute the flow over the embankment using the results of the 1d and 2d solution algorithms on a time step by time step basis this allows for direct feedback at each time step between the 1d profiles and 2d grid cells brunner 2014 neglecting conservation of momentum is justified for this modelling purpose since the 1d profiles are coupled with the 2d grid cells by an embankment hence the 1d 2d coupling can be treated as a weir type connection 2 2 3 differences between the high fidelity and lower fidelity model a 1d 2d coupled model requires the same input data as a fully 2d model therefore we use the same input data of the high fidelity model to set up the 1d 2d coupled model the digital elevation model dem of the 2d model is used to establish the 1d profiles and 2d grid cells of the 1d 2d coupled model also the boundary conditions consisting of measured discharges and water levels as well as the land use classification for both models are identical therefore we can conclude that the differences in the representations of the input parameters of the high fidelity and the lower fidelity model are solely caused by the level of detail of the two models itself and the different settings of d flow fm and hec ras these differences are explained in more detail below and are summarized in table 1 firstly the 2d shallow water equations of the high fidelity model are simplified to the diffusive wave equations the diffusive wave equations are applicable if flow separation and turbulence eddies can be neglected this is the case if the inertial terms are much smaller than the gravity friction and pressure terms test runs showed that neglecting the inertial terms of the momentum equations did not result in a change in model results on the other hand the use of the diffusive wave equations resulted in a significant reduction of the computational time therefore the diffusive wave equations are used to compute the flow characteristics at each 1d profile and 2d grid cell the applicability of the diffusive wave equations for flood modelling purposes has also been shown by e g moya quiroga et al 2016 moussa and bocquillon 2009 and leandro et al 2014 secondly the computational time step of the lower fidelity model is increased compared to the fully 2d model to speed up computational time in a 2d model the river is usually the time step limiting factor since the depths and velocities in the main channel are larger than in the embanked areas bladé et al 2012 see equation 1 the high fidelity model had an average time step of 3 9 s based on the maximum courant number a fixed time step of five minutes can be used for the lower fidelity model this time step is based on a convergence argument reducing the time step further did not result in a reasonable improvement of the model accuracy the land use classification of the high fidelity model is used as input for the lower fidelity model d flow fm allows multiple roughness definitions to be implemented in a single model however hec ras only allows a manning s roughness coefficient for the various land use classes therefore the roughness classes as used in the high fidelity model were transformed towards manning s roughness values based on tables 5 6 values of the roughness coefficient n of chow 1959 we recall that it is necessary to calibrate the summer bed roughness of the high fidelity model since each 2d grid has its own numerical friction on the other hand it is decided to not calibrate the lower fidelity model as a result the summer bed roughness can be included in the sensitivity analysis as a random parameter this is justified since no inundations along the lower rhine occurred during the 1926 flood event therefore correct prediction of the water levels becomes irrelevant the lower fidelity model is set up to accurately predict maximum discharges at lobith during flood events instead during the simulation the entire discharge wave flows in downstream direction independent of simulated water levels since inundations are not possible to occur consequently it is expected that simulated maximum discharges of the uncalibrated surrogate model are close to those predicted by the calibrated high fidelity model however validation is recommended to study whether the lower fidelity model is capable of simulating the system behaviour sufficiently 2 2 4 validation lower fidelity model razavi et al 2012b argue that even though the lower fidelity model may be based on the same input parameters as the high fidelity model the response pattern can differ somewhat this was also shown by thokala and martins 2007 they neglected the fluid viscosity in the navier stokes equations to set up a lower fidelity model this resulted in less accurate results compared to the high fidelity model the discrepancies between the response patterns of the lower fidelity and high fidelity models mostly influence the local and global minimum and maximum of the system razavi et al 2012b since this study tries to predict maximum discharges during a historic flood event it is of high importance that the global maximum of the system is correctly modelled by the lower fidelity model if this is not the case the discrepancies between the lower fidelity and high fidelity model can be addressed with a correction function razavi et al 2012b these kind of functions correct the response of the lower fidelity model and align it with the response pattern of the high fidelity model it is thus of high importance to validate the lower fidelity model to study whether a correction function is required to tune the model results if the response pattern of the lower fidelity model is close to that of the high fidelity model the lower fidelity model can be treated as the high fidelity representation of the underlying system consequently the lower fidelity model can replace the sophisticated 2d model razavi et al 2012b the sensitivity analysis can then be safely performed with the lower fidelity model since the input parameters of the lower fidelity model are based on the input parameters of the high fidelity model 2 3 the 1926 casus the 1926 flood event of the rhine river is used to examine the methodology of developing a lower fidelity model for historic flood reconstruction the study area stretches from the areas downstream of andernach in germany to the three rhine river branches in the netherlands fig 5 in this paper the german part of the river is referred to as the lower rhine the river enters the netherlands at lobith where it bifurcates into the waal river and pannerdensch canal subsequently the pannerdensch canal bifurcates into the nederrijn and ijssel rivers only the summer bed its floodplains and two embanked areas that are connected by an inlet ooijpolder and rijnstrangen area fig 5 are captured in the model domain the term inlet is used for a dike section with a relatively low crest level due to this low crest level a part of the discharge wave will enter the lower lying area behind the inlet as soon as a certain water level is exceeded as a result the maximum discharge further downstream decreases the dikes represent the boundaries of the model domain and are assumed not to overflow 2 3 1 geographical situation to reconstruct a historical geometry the changes in the river system between the current geometry and the historical period of interest must be defined an existing data set representing the 1995 geometry is made available by the dutch ministry of infrastructure and water management this data set is used as starting point and is adapted such that it represents the historical geometry the following measures were taken to create the 1926 situation fig 5 increase summer bed level due to erosion measurements of the summer bed levels were available for the entire model domain the changes in summer bed level between the 1995 measurements and the oldest measurements available at each location were used to estimate the 1926 summer bed level by linear extrapolation decrease winter bed level due to sedimentation no measured sedimentation rates along the study area were available therefore the following sedimentation rates were used to predict the 1926 winter bed level 1 mm year along the ijssel river pannerdensch canal and lower rhine 3 mm year along the waal river and 0 5 mm year along the nederrijn river silva et al 2001 a linear decrease of the sedimentation rate in channel cross direction was assumed as a result the sedimentation near the summer bed equals the predicted sedimentation rates according to silva et al 2001 the sedimentation near the outside border of the floodplain equals zero dike relocation on the left side of the lower rhine close to the city of emmerich germany the floodplains of the river were much larger in 1926 than they are nowadays the 1926 dike locations and hence the 1926 winter bed were based on old maps dating back to 1895 fig 5 dike relocation provided by the german deichverband xanten kleve der oberdeichinspektor dusseldorf 1895 the current summer dikes along the pannerdensch canal close to the pannerdensche kop were the 1926 winter dikes therefore the present floodplains were not part of the 1926 river system the area outside the 1995 summer dikes were removed from the geometry fig 5 pannerdensche kop restoration of inlets in 1926 two retention areas were possible to inundate at high discharge stages as a result of inlets the spijke inlet caused inundation of the rijnstrangen area when the water level exceeded 15 m nap equal to the crest level of the inlet fig 5 rijnstrangen area in the ooijpolder three inlets were active the total length of the inlets was 150 m the ooijpolder started to inundate at a water level of 12 5 m nap equalling the height of the three inlets the location of the inlets was based on historical 1926 maps fig 5 ooijpolder restoration of meander cut offs in 1955 and 1969 two meanders near doesburg and rheden were cut off fig 5 meander cut offs due to these meander cut offs the total length of the ijssel river decreased with almost nine kilometres the location of the meander bends are based on historical 1926 maps 2 3 2 boundary conditions the 1926 flood event is simulated for a period of approximately three weeks starting on the 22nd of december 1925 till the 8th of january 1926 from the 26th of december onwards the weather conditions changed drastically high rainfall intensities occurred in almost the entire catchment area of the rhine river dutch ministry of infrastructure and the environment 1926 this resulted in a rapid rise of the discharge wave starting on the 27th of december fig 6 shows the discharge wave at andernach representing the upstream boundary condition data source german federal waterways and shipping administration wsv communicated by the german federal institute of hydrology bfg the downstream boundary conditions consist of h t relations based on daily measured water levels available at http waterinfo rws nl and provided by the dutch ministry of infrastructure and water management three streams enter the lower rhine namely the lippe ruhr and sieg rivers these streams were included in the model domain by source points discharge inflow figs 5 and 6 the presented boundary conditions and source points are used in both the high fidelity as well as the lower fidelity model to set up the models 3 methodology of sensitivity analysis in this study uncertainty and sensitivity analyses are performed an uncertainty analysis is executed to compute the maximum discharge at lobith with its standard deviation as a result of the uncertain input parameters next a sensitivity analysis is performed to study which parameter mostly influence the uncertainty of the model output the main objective of the sensitivity analysis is the so called factor prioritization with this prioritization it becomes clear on which parameter to focus during historical geometry reconstruction for flood modelling purposes in order to reduce the potential uncertainty in the model output during the analyses we only focus on the parameters that influence the maximum discharge at lobith a test run was performed in which all roughness parameters along the dutch river branches were increased with 20 in this run the roughness values are close to the upper bound of the truncated normal distributions the run showed that the increase in roughness resulted in only a minor decrease of the maximum discharge at lobith of approximately 0 2 from 12 402 to 12 373 m3 s this minor decrease suggests that the dutch river branches are sufficiently downstream such that the effects of different summer bed roughness on the maximum discharge are negligible therefore the study only focuses on the uncertainties of the input parameters in the most upstream part of the model domain the city of andernach until the location where the rhine river bifurcates into the waal river and pannderdench canal the dutch rhine river branches are seen as fixed boundary conditions of the model since they do not influence model response therefore they can be excluded from the global sensitivity analysis 3 1 input parameters the lower fidelity model is used to establish the uncertainty and sensitivity of the 1926 discharge at lobith only the input parameters that are based on an estimation i e those that are uncertain are included in the analysis in addition parameters that require the development of a new surrogate model when changed e g a planometric change are excluded from the analysis for pragmatic reasons the following parameters are considered during the sensitivity analysis 1 roughness parameters of the various types of land use classes and 2 the bed levels of the summer bed and winter bed in general two kinds of uncertainties exist the first uncertainty is as a result of the randomness of variations in nature inherent uncertainty the second uncertainty is caused by limited knowledge epistemic uncertainty warmink et al 2013 the uncertainty of the different roughness classes is mainly caused by inherent uncertainty since it depends amongst others on the season e g grass grows faster during summer periods resulting in a larger roughness as well as on maintenance e g the frequency of mowing grass fields the uncertainty of the summer bed and winter bed levels are caused by epistemic uncertainty no measured 1926 bed levels are present therefore the bed levels are based on extrapolation techniques and estimated sedimentation rates for all roughness parameters we link the value with the largest probability of occurrence as well as its minimum and maximum bounds to the tables of chow 1959 truncated normal distributions are used in this study since a normal distribution better fits the data if some information about the input parameters is available tails of the distribution and the expected value contrarily a uniform distribution assumes that there is no knowledge about the value with the largest probability of occurrence only a range of input values is known therefore we can conclude that for older historic events the distributions of the uncertain input parameters will shift towards uniform distributions since less and less information is available the roughness parameters are divided into five land use classes summer bed lakes grasslands forest and urban areas a smooth channel with no vegetation is assumed for the entire summer bed having a minimum mannings roughness of 0 025 a normal value of 0 028 which is used as the expected value and a maximum value of 0 033 chow 1959 these numbers are used to set up the truncated normal distribution the same method was used to define the truncated normal distributions of the other roughness classes table 2 a comparable method is used to set up the truncated normal distributions of the summer bed levels and winter bed levels the 1926 summer bed levels were computed based on extrapolation of measured bed level changes see section 2 3 the uncertainty ranges of the summer bed levels were based on these extrapolation values the minimum change in bed level corresponds to no change compared to the oldest measured bed value consequently the 1926 bed level equals the oldest measured bed level the maximum change in bed level equals the extrapolation of the trend between 1995 and the latest measured bed level multiplied with a factor two a factor of two is chosen to include a large uncertainty range the summer bed is divided into three classes 1 from the most upstream location andernach river km 614 until walsum river km 789 here almost no erosion has occurred between 1995 and 1926 additionally the bed level has been compensated for bed level decrease due to mining activities at several locations 2 from walsum until the german dutch border river km 857 here there is relatively much uncertainty in the amount of erosion since the oldest measured bed level dates back to only 1960 3 from the german dutch border till the first bifurcation point of the rhine river river km 867 here there is little uncertainty in the 1926 bed level since the oldest measurements date back to 1934 the winter bed level consists of just one class since no deviations in uncertainty along the lower rhine exist the estimated sedimentation rate of 1 mm year is used to define the ranges of the winter bed level in the lower rhine silva et al 2001 the minimum value equals no change in bed level compared to the 1995 situation the maximum range equals the sedimentation rate of 1 mm year multiplied with a factor of two again a factor of two is chosen to include a large uncertainty range since the 1 mm year sedimentation rate is relatively speculative since the summer bed and winter bed levels vary along the study area their truncated normal distributions and corresponding minimum and maximum values are given as change from its 1926 reference value table 2 these values will be referred to as standardized st bed levels from now on a value equal to zero correspond with the reconstructed 1926 geometry 3 2 design of experiment before a sensitivity analysis can be performed a design of experiment doe has to be defined does employ different space filling strategies to capture the behaviour of the underlying system over limited ranges of the input parameters razavi et al 2012b a doe results in a sample in which the boundary values of the input parameters are based on physical conditions this sample can be used in a monte carlo analysis most commonly used doe methods in literature appear to be full factorial design fractional factorial design central composite design and latin hypercube sampling lhs razavi et al 2012b in general a full factorial design a fractional factorial design and a central composite design require a relatively large number of simulations to generate all combinations to represent the corners of the input space razavi et al 2012b saltelli et al 2008 contrarily lhs can easily scale to different numbers of input parameters without the need for extra simulation runs razavi et al 2012b thus a stratified lhs sample has as advantage that less model runs are requried since a stratified sample achieves a better coverage of the sample space of the input parameters saltelli et al 2000 for this reason a lhs design is used in this study the nine input parameters are divided into eight levels each level has an equal probability of occurrence of 12 5 based on the determined truncated normal distributions in section 3 1 for each run each level is randomly selected constraining that if a level is already selected it cannot be selected again this results in a set of eight simulations in which all eight levels of the nine input parameters are present no clear guidelines exist concerning the minimal number of runs required in a monte carlo analysis this number depends on the number and range of the input parameters and on the shape of the response surface theses features are largely unknown in advance pappenberger et al 2005 in this study convergence of the uncertainty of the discharge at lobith expressed as standard deviation is used as stopping criteria following the method of pappenberger et al 2005 if an additional run results in a change of the standard deviation smaller than 0 05 m3 s it is assumed that the sample sufficiently represents the input space of the different input parameters this criteria resulted in 120 model runs corresponding with 15 latin hypercube sets to check whether the input space is sufficiently captured by the sample two additional model runs were performed with the most extreme situations these scenarios represent the limits of the probability distribution functions of the input parameters table 3 and fig 7 show the range of maximum discharges at lobith modelled in the 120 monte carlo runs and the range found with the two most extreme cases note that all runs are performed with the lower fidelity surrogate model the minimum and maximum values of the sample are close to the predicted values of the two most extreme runs therefore we can conclude that the input space is sufficiently captured by the sampling data set 3 3 stratified monte carlo analysis the results of the monte carlo analysis are used to determine the uncertainty in model predictions additionally the results are used to apportion this uncertainty to the contribution of the individual input parameters two sensitivity analysis methods are used namely multiple linear regression analysis and sobol indices explained in sections 3 3 1 and 3 3 2 respectively 3 3 1 multiple linear regression analysis if the number of simulations is much larger than the number of input parameters a lhs can be very effective in revealing the influence of each parameter using a regression analysis saltelli et al 2008 if the model does not contain any interactions between the input parameters i e the model is additive the linear regression function can be given as scheidt et al 2018 2 y β 0 i 1 n β i x i where y represents the model output in this study the maximum discharge at lobith and x i the different input parameters the coefficients β 0 and β i are determined by the least square computation based on the squared differences between the model output produced by the regression model and the actual model output produced by the surrogate model saltelli et al 2008 the coefficient β i is used to determine the importance of each parameter x i with respect to the model output if the input parameters are independent the absolute standardized regression coefficient β ˆ i can be used as a measure of sensitivity scheidt et al 2018 3 β ˆ i β i σ i σ y where β ˆ i represents the standardized regression coefficient and σ i and σ y represent the standard deviations for the input parameter x i and the model output respectively however the applicability of a linear regression analysis depends on the degree of linearity of the model saltelli et al 2008 a measure for linearity is expressed by saltelli et al 2008 4 r 2 i 1 n β ˆ i 2 where r 2 represents the model coefficient of determination this value is equal to the fraction of the variance of the original data that is explained by the regression model a value of r 2 equal to one indicates that the model is linear saltelli et al 2008 and that the multiple linear regression model is capable of expressing all variance of the original data 3 3 2 sobol indices if the model is not linear sobol indices can be used to determine the sensitivity of the input parameters sobol indices are widely used as global sensitivity analysis method in literature we are specifically interested in the first order indices i e the effect without interactions of input parameters since the sensitivity analysis is used for factor prioritization purposes saltelli et al 2008 li and mahadevan 2016 present an effective method to estimate the first order sobol indices analytically this method can be applied to any kind of data set and is not restricted to a specific sampling strategy furthermore the method can be applied to models with correlated input parameters li and mahadevan 2016 found that the method is highly efficient and that it is especially useful in ranking and identifying important parameters the formula used is as follow li and mahadevan 2016 5 s i 1 e xi v x i y x i v y where s i represents the sobol first order index v x i y x xi indicates the conditional variance of y caused by all input parameters other than x i e xi represents the expected value as a result of fixing input parameter x i and v y represents the variance of y the monte carlo sample has a relatively small size therefore the 95 confidence intervals of the sobol indices are computed based on a resampling strategy the matlab statistics toolbox is used to perform the computation the method to compute the 95 confidence intervals is based on the work of dubreuil et al 2014 in which a bootstrap resampling strategy is used computation of confidence intervals by bootstrap resampling is widely used in global sensitivity analysis and has been used in combination with surrogate models by gayton et al 2003 and janon et al 2011 bootstrap resampling aims at determining confidence intervals of a parameter of interest using only one design of experiment efron and tibshirani 1993 the method consists of the creation of new designs of experiment by drawing with replacement in the original design the method used is presented in fig 8 the lhs sample consisting of 120 model runs is resampled after which the confidence intervals of the first order sobol indices are computed if these confidence intervals have not reached a specific convergence criterion yet more bootstrap resamples are drawn the computation is repeated until the convergence criterion is met the criterion as suggested by dubreuil et al 2014 is used they suggested to stop the procedure at the iteration for which all confidence interval sizes have reached a range which is less than x percent of the maximum bootstrap mean of the sensitivity indices the choice of parameter x depends on the goal of the sensitivity analysis if the goal is only determining the most dominant input parameter a relatively large value of x in the order of 30 can be used however if the model has many variables of equal sensitivity indices it is better to look at the convergence graph at each bootstrap iteration and decide manually when to stop the procedure dubreuil et al 2014 the first convergence criteria 30 is used which will be evaluated by checking the convergence graphs of the sobol indices as suggested by dubreuil et al 2014 4 results 4 1 calibration high fidelity model the river branches lower rhine waal river and pannerdensch canal were calibrated with the use of measured water levels the discharge partitioning along the dutch river branches was based on the report of the dutch ministry of infrastructure and the environment 1952 during the calibration procedure this discharge partitioning had to be met the ijssel and nederrijn rivers were excluded from the calibration procedure since many inundations along the ijssel river have occurred during the 1926 flood event these inundations influence the water levels at both river branches even a very low summer bed roughness near the locations of the inundations did not result in the correct water levels for this study purpose it is accepted that the water levels along the ijssel and nederrijn rivers were not calibrated correctly these branches are located more than 15 km downstream of lobith such that backwater effects has vanished at lobith the ijssel and nederrijn rivers have thus no effect on the maximum discharge at this location in the data set only daily measured water levels are available hence the maximum measured water level may be lower than the occurred maximum water level therefore we calibrated on the three days with the highest water levels for each measurement station present along the river branches if the model is capable of predicting the correct shape and correct water levels at three moments in time near the peak discharge it is likely that also the correct maximum water level is predicted by the model the 1926 discharge wave was simulated maximum water levels at 10 measurement stations were validated after model calibration it was found that simulated maximum water levels only deviated 2 cm on average compared to the measurements therefore it can be concluded that the high fidelity model is capable of simulating maximum water levels with high accuracy after calibration of the summer bed roughness 4 2 validation and uncertainty of the lower fidelity model the model output was compared with the model output of the high fidelity model to study whether it is justified to use the lower fidelity model to perform the sensitivity analysis we found that the high fidelity model simulates a maximum discharge at lobith of 12 282 m3 s with the 1926 measured discharge wave at andernach as upstream boundary condition the lower fidelity model with all random input parameters set to their expected value predicts a maximum discharge of 12 402 m3 s this deviates less than 1 0 compared to the high fidelity model although correct prediction of the maximum discharge at lobith has the focus in this study it is also desirable that the lower fidelity predicts correct discharge stages at other locations table 4 shows that the lower fidelity model predicts maximum discharges along the lower rhine with high accuracy having a maximum deviation of 2 1 compared to the high fidelity model in addition the lower fidelity model is capable of accurately predicting the discharge partitioning along the dutch rhine river branches table 4 these values indicate that the surrogate model is capable of representing the system behaviour of the high fidelity model therefore no correction function is needed to tune the model results of the lower fidelity model we can thus conclude that the lower fidelity model can be treated as a high fidelity model from now on hence the sensitivity analysis can be performed with the 1d 2d coupled model the results of the uncertainty analysis show that the average maximum discharge at lobith as a result of the monte carlo sample equals 12 424 m3 s this value has a standard deviation of 49 m3 s caused by the uncertainty in the input parameters this relatively low standard deviation shows that uncertainties in the input parameters only have a limited effect on the maximum discharge at lobith during the 1926 flood event 4 3 sensitivity analysis 4 3 1 multiple linear regression analysis a multiple linear regression analysis was performed in which it was assumed that the model response as a result of the varying input parameters was linear this is not the case since the model coefficient of determination r 2 equation 4 equals 0 81 this value means that the regression model is capable of explaining 81 of the variance of the surrogate output the remaining 19 is ignored by the regression model however table 5 clearly shows that the roughness of grasslands highly influences the maximum discharge at lobith because of its high sensitivity measure β ˆ i equation 3 the high standardized regression coefficient of the roughness of grasslands can be explained by the fact that grassland is the most dominant land cover in the model domain with a surface area of 55 6 table 5 in addition the uncertainty within the class itself is relatively large table 2 since grasslands most often have a higher roughness during summer periods due to growing season compared to the winter periods only the roughness of forest has a larger uncertainty range however the surface area covered by forest is much less 6 4 4 3 2 sobol indices in the previous section it was shown that with the multiple linear regression analysis only 81 of the variance of the surrogate model output could be explained in order to check the results of the multiple linear regression analysis the sobol indices are computed these indices are independent of model linearity the results show that the roughness of grasslands is dominant with respect to influencing the uncertainty of the maximum discharge at lobith table 6 this is in line with the results of the multiple linear regression analysis if i 1 r s i 1 the variance of the model output is solely caused by the variance of the input parameters itself in that case there are no interactions between the different input parameters resulting in an increase in the variance of the model output in other words the model is additive the results show that the first order sobol indices are approximately 1 indicating that the model does not include any interactions of the input parameters in principle i 1 r s i cannot be larger than 1 in addition the first order sobol index computed for each uncertain input parameter cannot be lower than 0 saltelli et al 2008 in this study the computed i 1 r s i is slightly larger than 1 and the sobol index for the roughness of urban areas is smaller than 0 this is caused by the relatively little sample size of only 120 runs to overcome this problem we resampled the 120 runs as explained in section 3 3 with this resampled data set the 95 confidence intervals of the first order sobol indices are computed fig 9 fig 10 shows that the first order sobol indices have converged after approximately 700 bootstrap resamples this results in a data set of 700 120 model runs the outcomes then show that the roughness of grasslands remains the most dominant input parameter the lower bound of its confidence interval is under any condition larger than the sensitivity index of the other input parameters therefore we can conclude that for this specific case most attention must be paid to the roughness class with the largest surface area and which has a relatively large uncertainty range correct prediction of this parameter will result in a significant reduction of the output variance it must be noted that the uncertainty of the model output was small in this study in general the output variance depends on the probability distribution functions of the uncertain input parameters it can be expected that the output variance will increase for older historic events hence a significant reduction in model output variance can be reached if the most influential input parameter is correctly predicted this influential input parameter can be found by applying the method for factor prioritization as presented in this study 5 discussion in this study a methodology was developed to reconstruct historic flood events with the use of a lower fidelity model the maximum discharge is predicted as well as its uncertainty as a result of the uncertain input parameter general problems that arose were mostly related to the choice of the surrogate model type and the characteristics of the flood event therefore another historic event may ask for a different approach since the assumptions made for the 1926 event may not apply to put things into perspective an overview and discussion are presented of the problems that may arise during historic flood reconstruction and resulting sensitivity analysis 1 to predict a historic discharge an associated geometry should be reconstructed the geometry during the 1926 event was well known since maps of this time period are available however for events further in the past the geometry might be more uncertain these spatial uncertainties must be included in the analyses a major drawback is that for each uncertain geometric situation a separate model must be set up consequently for each model the sensitivity analysis must be performed separately this significantly increases the total number of simulations furthermore for older events the uncertainties in the input parameters may become larger hence the shape of their probability distributions may change we assumed that the uncertain input parameters of the 1926 flood event could be described by truncated normal distributions these distributions will shift towards uniform distributions for older events if less information is available 2 a lower fidelity based surrogate model was developed to reduce computational time many other methods exist to set up a surrogate model each with their own benefits and drawbacks a different study approach may lead to the need of another type of surrogate model in general a 1d 2d coupled model is capable of simulating any kind of flood event the 1d profiles enable correct prediction of discharge stages below bankfull conditions horritt and bates 2002 these 1d profiles can be coupled by 2d grid cells to include the possibility of simulating overland flows if the discharge exceeds the bankfull discharge referring to the situation in which the discharge is larger than the main channel and floodplain capacity therefore this type of lower fidelity model can be used to accurately simulate flood wave propagation for both discharges below as well as above bankfull conditions 3 the 1d 2d coupled model was not calibrated on maximum water levels the objective of the surrogate model was accurate prediction of maximum discharges at lobith however calibration on maximum water levels is required if dike breaches and or overtopping have evolved during the flood event for such a case correct prediction of maximum water levels becomes important since this value indicates whether overtopping occurs this influences the maximum discharge further downstream therefore it is recommended to use the summer bed roughness of the lower fidelity model as calibration parameter to correctly predict water levels in case of discharges exceeding bankfull conditions 4 to perform the sensitivity analysis a decision had to be made about the range of the truncated normal distributions of the input parameters the ranges of the roughness parameters were based on the tables of chow 1959 a smooth channel with no vegetation was assumed to determine the roughness of the summer bed this results in a relatively low expected mannings roughness value of 0 028 with a total range of between 0 023 and 0 033 it is expected that the dimensions of sand dunes during flood events are highly uncertain this uncertainty may influence summer bed roughness significantly the measured mannings roughness of the summer bed during the 1998 event with a maximum discharge of 9464 m3 s at lobith ranges of between 0 030 and 0 035 julien et al 2002 these values are higher than the values that we used paarlberg et al 2010 found a clear dependency between increase in the discharge and increase in the dune heights however it is still unclear to what extent dune heights increase during flood events some literature even suggest that the dunes are washed out under extreme conditions e g best 2005 and naqshband et al 2014 resulting in much lower values of the roughness parameter it is not the roughness value itself that influences the uncertainty of the maximum discharge but rather the uncertainty range of the summer bed roughness therefore the relatively broad roughness range for the summer bed used in this study is considered appropriate for the 1926 flood event 5 in this study only geometrical uncertainties in the input parameters are included in the sensitivity analyses these parameters are the bed levels of the summer bed and winter bed and the roughness of the various land use classes however much more uncertainties exist which can be related to the model structure model parameters and boundary conditions these inherent uncertainties can be considered in the sensitivity analysis by including them as random input parameters in the lhs this will result in more insight in the most dominant type of uncertainty i e uncertainty as a result of the input parameters model parameters or model set up this study is recommended for future work since here we only focused on the uncertainties of the geometrical input parameters to illustrate our method 6 conclusions the objective of this paper was to study whether a lower fidelity hydraulic model can be used for historic flood reconstruction in this paper a general framework is presented that shows which problems have to be tackled in order to enable historic flood reconstruction with the use of a surrogate model a 1d 2d coupled model was developed as lower fidelity model that is capable of simulating flood wave propagation with high accuracy it was found that model results predicted by the lower fidelity model were close to those predicted by the high fidelity model the lower fidelity model is thus capable of accurately predicting system behaviour in addition the proposed 1d 2d coupled model can be applied to any type of historic flood event this is because it is capable of accurately simulating flood wave propagation for both discharges below as above bankfull conditions however if the simulated discharges exceed the bankfull discharge model calibration is recommended since correct prediction of water levels becomes highly relevant for these cases a sensitivity analysis is required to determine the parameters that mostly influence the uncertainty in the model output the lower fidelity model could be used to perform this analysis this significantly decreased computational time compared to the use of a fully 2d model for future work we propose that a 1d 2d coupled model can be treated as a high fidelity model in general therefore setting up a sophisticated 2d model for validation will not be needed the proposed methodology was tested with the use of the 1926 flood event of the rhine river the lower fidelity model predicts a maximum discharge at lobith of 12 402 m3 s for this historic event deviating only 1 0 compared to the high fidelity model 12 282 m3 s the uncertainty of this maximum discharge at lobith equals 49 m3 s the uncertainty in model output is relatively small because a large amount of data of the 1926 flood event was available reconstruction of an older flood event will probably result in larger uncertainties of the input parameters since less information is available as a result the truncated normal distributions used to describe the uncertainty of the various input parameters will shift towards uniform distributions this will have a negative effect on the model output uncertainty the sensitivity analysis showed that the model output was most sensitive to the roughness class with the largest share in surface area in this case the roughness of the grassland areas moreover the location of the roughness class was important since areas close to the river have a relatively large impact on model results these two aspects in combination with the uncertainty range of the input parameter itself determined the influence on model response acknowledgement this research is supported by the netherlands organisation for scientific research nwo project 14506 which is partly funded by the ministry of economic affairs and climate policy furthermore the research is supported by the ministry of infrastructure and water management and deltares this research has benefited from cooperation within the network of the netherlands centre for river studies ncr www ncr web org the authors would like to thank the dutch ministry of infrastructure and water management and the german federal institute of hydrology for providing the data besides the authors would like to thank the following persons for their suggestions and valuable insights prof dr herget university of bonn dr aguilar lopez technical university delft dr lammersen dutch ministry of infrastructure and water management van doornik msc lievense cso berends msc university of twente and dr mara university of la réunion in addition the authors would like to thank the anonymous reviewer sheikholeslami msc unversity of saskatchewan and the associate editor dr razavi for their suggestions during the review process which greatly improved the quality of the paper 
26223,environmental timeseries data variety is exploding in the internet of things era making data reuse a very demanding task data acquisition and integration remains a laborious step of the environmental data lifecycle environmental data heterogeneity is a persistent issue as data are becoming available through different protocols and stored under diverse custom formats in this work we deal with syntactic heterogeneity in environmental timeseries data our approach is based on describing different dataset syntaxes using abstract representations called templates we designed and implemented edam environmental data acquisition module a template framework that facilitates timeseries data acquisition and integration edam templates are written using programming language agnostic semantics and can be reused both for input and output thus enabling data reuse via transformations across different formats we demonstrate edam generality in seven case studies which involve scraping online data extracting observations from a relational database or aggregating historical timeseries stored in local files case studies span different environmental sciences domains including meteorology agriculture urban air quality and hydrology we also demonstrate edam for data dissemination as instructed by output templates we identified several syntactic interoperability challenges though the case studies that include managing with differences in formatting observables temporal and spatial references and metadata documentation and addressed them with edam edam implementation has been released under an open source license keywords environmental timeseries internet of things syntactic interoperability data acquisition templates big data 1 introduction environmental data management that is acquisition processing storage and dissemination athanasiadis and mitkas 2004 mason et al 2014 is becoming more challenging in the era of big data bd and the internet of things iot in the contemporary data rich society a great variety of sensors and iot devices enable the collection of large observation volumes which can be further processed for enabling new knowledge insights at the same time this era is characterized as knowledge poor since universal data management and heterogeneous data integration remain still open challenges negru et al 2016 environmental data acquisition seems to be the most laborious step within the environmental data lifecycle terrizzano et al 2015 harth et al 2013 horsburgh et al 2009 this is attributed to the heterogeneity pertinent to environmental data sources environmental datasets are collected and stored under different data models in various forms mainly in files and relational databases horsburgh et al 2011 datasets which do not share common data formats and or communication protocols are difficult to be re used without human expert involvement syntactic heterogeneity is a factor which hinders the adoption of a universal strategy to acquire data originating from disparate information sources it also obstructs the environmental data science core objective to narrow the data to knowledge latency elag et al 2017 by supporting environmental data discovery and access and by enabling re usability horsburgh et al 2009 ames et al 2012 athanasiadis 2015 holzworth et al 2015 granell et al 2010 fair findable accessible interoperable reusable guiding principles for scientific data management and stewardship highlight the importance of scientific data reusability and reproducibility wilkinson et al 2016 long term archival and preservation of digital assets also implies the regular transformation of data between storage formats and media there are two approaches to tackle syntactic heterogeneity the first is to use adopt frameworks which were designed to facilitate environmental data discovery and accessibility such as the ogc sensor web enablement swe botts et al 2008 and cuahsi hydrologic information system his horsburgh et al 2009 such systems hide the underlying complexity of environmental data sources and expose them in a standardized manner through established data models e g o m cox 2011 sensorml botts and robin 2014 waterml 2 0 taylor 2014 etc the various datasets need to be stored in a common schema in order to be exposed through a data sharing framework this entails certain modifications which introduce overhead and commonly require a strong computer science background to implement andrae et al 2009 the second approach is to develop programming language scripts each one tailored to the custom data format eberle et al 2013 woodard 2016 these custom to data scripts usually transform a dataset into a common data schema porter et al 2014 which allows for further processing analysis or dissemination tasks boote et al 2015 these approaches have been used also for exchanging data between environmental models i e porter et al 2014 horita et al 2015 peckham and goodall 2013 jones et al 2015 both approaches rely upon computer programming skills that are not always available this contradicts the lowering e science barriers movement swain et al 2016 that envisions accessing data in an uncomplicated fashion so that e scientists can entirely focus on the domain of their expertise and not on side tasks such as curating datasets by the term e science we refer to a global collaboration in key areas of science hey and trefethen 2003 which promotes innovation in collaborative computationally or data intensive research across all disciplines throughout the research lifecycle international confer 2018 based on our experience transforming environmental datasets from different sources in order to fit as input to environmental models requires manual work which is hardly re usable for example different programming languages e g python van rossum and drake 2003 r ihaka and gentleman 1996 etc and data models e g o m waterml 2 0 etc are adopted for the scripting and environmental data management framework approaches respectively in this paper we outline the design and demonstrate an open source implementation of the environmental data acquisition module edam that addresses issues of syntactic data heterogeneity using templates an edam template is an abstract representation of a data file s contents using programming language agnostic semantics edam supports data acquisition integration and transformation from a variety of file types and syntaxes through templates specifically edam is applicable for environmental timeseries datasets stored in various data formats delimiter separated files flat files etc at various sources files folders databases websites and implementing different data models tables key value pairs edam employs a declarative approach to enable scientists to annotate their data by means of templates it automatically parses the data matches them with templates stores them and optionally exports them to a format described by an output template this allows for end users to query retrieve and transform environmental timeseries datasets into their own formats also edam supports interoperable data dissemination through standardized protocols e g ogc sos broering et al 2012 we also demonstrate its front end graphical user interface gui for creating maps we demonstrate edam in seven cases studies from various environmental domains including air quality meteorology agriculture and hydrology to the best of our knowledge this is the first time that structural templates are extensively used for environmental data management tasks i e acquisition integration and dissemination we started exploring this approach in papoutsoglou et al 2015 where we investigated a case study for collecting data from a smoky swiss railway station here we extend our work with six more real world cases scraping meteorological data from the public webpages of the bureau of meteorology bom in australia and the uk met office parsing hydrological timeseries data from the hydrological observatory of athens hoa extracting observations from an air quality archive from bom originally stored in a relational database aggregating historical timeseries data from all dutch weather stations provided by koninklijk nederlands meteorologisch instituut knmi transforming weather input data of apsim crop model holzworth et al 2014 into the agmip format porter et al 2014 the rest of the paper is structured as follows in section 2 we review contemporary approaches for environmental data acquisition and integration and introduce readers to environmental data management with web template frameworks section 3 presents the edam architecture specifically key requirements user types and use scenarios section 4 demonstrates edam the conducted experiments the used datasets and the addressed challenges finally in section 5 we discuss our research findings and lessons learned conclude the research summarizing key findings and future work 2 background and related work in the environmental data literature different terms are used for describing the process of obtaining a dataset and transforming it into another format specifically the terms harmonization porter et al 2014 mediation and conversion horsburgh et al 2011 management and publication jones et al 2015 integration beran et al 2009 acquisition and collation mason et al 2014 and wrangling terrizzano et al 2015 kandel et al 2011 are synonyms for data acquisition and integration in this work we focus on acquisition and integration of environmental timeseries data in general the acquisition process works as follows a station stationary or not houses one or more sensors a sensor measures one or more observable s producing observations an observation has a value expressed in some units and refers to a certain timestamp and possibly a location in this context environmental observations without a temporal dimension e g soil data are not considered timeseries and thus can not be processed by edam also note that edam can process location data when they are associated with a timeseries i e observations of latitude longitude angle etc at a certain timestamp location data are stored as regular timeseries and can be combined with other observations this is elaborated further in subsection 4 1 in the rest of this section we review approaches that cope with syntactic heterogeneity first we present environmental data management frameworks which by design account for syntactic interoperability environmental data management frameworks are typically used for preparing inputs required for executing scientific workflows decision support tools or environmental models however not all environmental datasets are offered through such frameworks second in subsection 2 2 we present the scripting approach which facilitates environmental data transformation to fit into a consistent data format last subsection 2 3 introduces web template frameworks and presents our previous experiences with them 2 1 environmental data management frameworks providing standardised discovery and access services for environmental data is a key requirement for an environmental data management framework horsburgh et al 2011 examples of such frameworks are the ogc sensor web enablement swe which supports timeseries dissemination through the sensor observation service sos broering et al 2012 and the cuahsi hydrologic information system his horsburgh et al 2009 both provide interoperable data access on two layers a communication b data representation communication is achieved by defining standardized ways to request environmental data e g getvalues for cuahsi his ames et al 2012 getobservation for ogc sos broering et al 2012 data representation deals with data dissemination through standardized information models which hide the underlying data complexity for example waterml 2 0 taylor 2014 is promoted by both frameworks in order to represent hydrological timeseries data environmental data management frameworks can provide interoperable access to raw data by transforming them to a common data model this common data model can be part of the framework or its implementation in the case of ogc sos there are different software implementations which use different data models mcferren et al 2009 on the other hand cuahsi his is founded around the observations data model horsburgh et al 2008 software tools were implemented to import data into an odm database horsburgh and tarboton 2007 document a data loader component which imports tabular timeseries into an odm instance mason et al 2014 present an environmental management framework which utilizes reusable data parsing templates to annotate tabular timeseries and import them into an odm instance 2 2 data integration through scripting several efforts are reported in the literature where scripts have been used for environmental timeseries acquisition and integration by the term script we refer to a small computer program which is intended to automate a task regardless of whether the programming language in which it was developed is considered a scripting language e g python or not e g java for example the ag analytics platform woodard 2016 demonstrates a data warehouse to retrieve data from heterogeneous data sources it extracts data through custom scripts written in python one for every data source in another example harth et al 2013 employ a linked data scripting language called data fu stadtmüller et al 2013 to integrate diverse data sources each data fu program comes with data source specific rules and queries in a third line of work porter et al in porter et al 2014 present a data harmonization workflow to promote model inter comparison and ensemble modelling data source specific translators were developed and used to integrate heterogeneous datasets into the agmip common data schema in order to facilitate data exchange between crop models 2 3 environmental data management with web template systems web template systems are designed to create dynamic content and are extensively used in web applications they are used for automatically generating custom content such as customer invoices search results data reports etc web template systems e g jinja2 ronacher 2008 mako bayer cheetah3 broytman and croy 2001 are intuitive to use and do not require advanced programming skills each one comes with a template language which is used to markup templates a template is a document which represents a data structure using variables geebelen et al 2008 dynamic views are rendered by feeding a template with data and template variables are substituted with values web template systems can support data output by design but not data input directly for example in samourkasidis and athanasiadis 2017 we employed jinja2 to create on the fly dynamic views for environmental data dissemination in a previous work papoutsoglou et al 2015 we also started experimenting with using template files as a markup for data input where we presented a platform which used templates to read from local files in a variety of formats 2 4 summary acquiring and integrating environmental timeseries in a consistent data format is a manual process which requires significant efforts this is because the vast majority of environmental datasets available in the environmental internet of things eiot are heterogeneous by nature hart and martinez 2015 universal data acquisition and integration can be achieved through the scripting approach nevertheless there is a trade off between generality and complexity this approach opposes the lowering e science barriers since it presumes a computer science background swain et al 2016 a web template framework language is much more simple compared to a traditional programming language in this work we investigate the use of templates in order to acquire and integrate environmental timeseries datasets and seek for a compromise in the trade off between complexity and generality 3 the edam framework 3 1 objectives there were three objectives in designing and developing edam the first was to lower e science barriers by embracing a programming language agnostic solution obtaining timeseries data by writing small computer programs scripts has already been investigated see subsection 2 2 thus we focused on solutions that involve as little as possible programming skills for its end users and examine the use of templates written with a simple programming language agnostic markup the second objective was to apply edam to a wide variety of case studies in order to tackle the intrinsic heterogeneity of environmental data sources this heterogeneity is related to a data source type which could be text files webpages databases web services b data formats i e comma separated values csv tab separated values tsv etc and c data models after which available environmental data are structured the third objective was to create custom views of timeseries data and disseminate them through standard interoperable protocols as ogc swe standards this enables users to transform data from one format to another promoting interoperability for environmental modelling and overcoming problems related to the diversity of data models it also copes with syntactic interoperability by exposing edam processed datasets via established information models such as o m and sensorml 3 2 abstract architectural design there are three key components involved in edam a input files b template files and c template engine fig 1 depicts the interaction between edam components for data input and output in all cases the data are extracted from their original source and stored in the edam database in a unified data model then they can be fetched and presented in a user defined way using a range of custom templates any kind of text based source can serve as an input inputs are stored in one or more files locally or remotely they may be stored in a local nested folder structure on a website or relational databases from which data could be extracted with sql queries an edam template is an abstract representation of data file contents each template file is bound to a specific data syntax which is comprised of a a timestamp which may come in different formats as we discuss below b a set of observables in a given order along with optional metadata annotating their semantics omitting observables changing their order of appearance and or changing timestamp representation results in a different data syntax i e requires a different template we envision that one template will be needed per sensor vendor or legacy data formats used for input output by environmental models templates can be used for specifying both input or output data file structures and are written using the edam template language the edam template engine and language are the core of the framework offering various processing capabilities the template language itself is founded on programming language agnostic semantics besides simple data parsing the edam template engine supports mathematical and statistical operations both the template engine and language are implemented after jinja2 ronacher 2008 this enables us to use the jinja2 mature framework for data dissemination purposes regarding data dissemination edam may offer acquired data as services on the web currently edam supports data dissemination through ogc sensor observation service and its own edam api the edam api enables the creation of custom data views since edam templates can be called dynamically edam template language artefacts keywords or user defined variables are located inside placeholders the edam template language has four restricted keywords station observable sensor timestamp which result from the edam underlying data model fig 2 depicts the edam unified data model along with the template language restricted keywords the data model was designed after our assumption of an environmental data source and it is tailored to the needs of the template language this is also the reason why we did not reuse any third party data model a third party data model involves a number of external dependencies via foreign keys that would affect the template language syntax rendering it complex and difficult to use user defined variables are used to annotate the observables found in a dataset their semantics are further specified in a metadata file a template may contain control statements e g if then else for loops to provide formatting and control functionality and set the logic which will be used for data retrieval next to the template file there is the metadata file it is drafted by users in order to further annotate data parsed from input files metadata include information commonly not stored directly in the original input files as for example units of measurement for observables or station locations such additional metadata which may include terms from ontologies are necessary for enriching the semantics of the original data how this works is further detailed in the following section 3 3 workflow the edam workflow operates in two phases data input and data output data input concerns data acquisition preprocessing and storage processes data output involves the discovery transformation and dissemination of information fig 3 depicts the workflow for data input and output accordingly we identify two user roles in the edam system data curators are interested in sharing data with edam added value services and import datasets into the system they draft input templates making new data sources available data consumers are e scientists i e modellers researchers decision makers who are interested in third party data stored in edam they use edam to a view available datasets b render graphs c apply filters on data and d download them in various formats i e csv txt etc they may create custom data views by editing template files e g change the order of columns omit columns etc software agents can also be considered as data consumers they interact with the system using ogc sos or the edam api a workflow to input data into edam is as follows a data curator drafts the input template documents all relevant metadata in a metadata file and finally provides a data source an input template has the original data source structure data curators may provide edam with metadata using the metadata files to augment the original information with additional details about the station the involved observables and their corresponding sensors and units of measurement while this step is optional it is critical towards data reusability and interoperability fig 4 shows a sample input file from the uk met office fig 4a and the corresponding template file fig 4b edam keyword station has been used for annotating all data relevant to the station name and location the keyword timestamp is used to parse the component of the date to which the observations correspond to in this case we used timestamp year and timestamp month to parse the year and the month respectively generic jinja2 keywords such as for are used to parse all data reported in the file user defined keywords are used as variable names to annotate observable values as tmax value tmin value af value rain value and sun value fig 5 depicts the corresponding metadata file which defines additional station and observable metadata data curators can specify the timezone station attribute which will be used to complement all the station related timestamps the value of the timezone attribute can be either the format as code keyword or a timezone from the tz database wikipedia contributors and l 2018 in case the format as code keyword is used edam automatically identifies the corresponding timezone from the station s location and assigns it to the related timeseries greenwich mean time gmt is the default timezone which is used when no location is provided or the timezone attribute in the metadata file is omitted data curators also use the metadata file to relate a user defined keyword e g tmax with a its corresponding observable name e g temperature maximum and b the unit it was reported e g celsius there is also a section to store metadata about the utilized sensors which in this example are unknown 3 4 implementation and modes of operation in table 1 we depict edam implemented functions organized by when they are utilized input functions are applied by edam during the process of data input processing functions concern statistical and conditional filters which are written in output templates and are applied during data output last but not least dissemination functions are added value services offered for edam imported datasets edam software has been developed in python and is available as open source software on github samourkasids et al 2018 under the gnu affero general public license version 3 it is also distributed as an autonomous python package through the python package index pip python software foundation 2018 and can be installed on a computer with python installed by typing pip install edam the pandas python library mckinney 2011 supports the edam input and processing functions the edam dissemination functions are implemented with the flask web framework ronacher 2010 and acquired timeseries are offered as ogc sos services through the python implementation reported in samourkasidis and athanasiadis 2017 the hardware requirements of edam are minimal we installed edam and tested its functionalities on a raspberry pi 2 model b mini computer raspberry pi foundation 2018 without any issues edam operates as a local standalone system this means that edam parsed datasets are stored and can be accessed locally on user s computer installation automatically creates a folder in the home directory in which the user should store templates and metadata files after installing edam two commands are available via the command line edam and viewer these commands reflect the two distinct modes of operation the command line mode and the graphical user interface mode in the command line mode data curators utilize the edam command to define the input arguments i e input template and metadata file in order to parse and store a dataset optionally they can define the output parameters i e template and metadata file in order to transform a dataset on the fly in the graphical user interface gui mode we assume that some datasets have already been imported in edam s database and the user wants to disseminate them via the edam web services the viewer command starts the edam web services which currently are the api ogc sos and the web front end human users can access these services on their browser and machines via the appropriate protocol note that the web front end includes information about the edam api how to access the stored datasets and the ogc sos instance 4 demonstration we demonstrate edam extended outreach by acquiring environmental timeseries data from diverse data sources in table 2 we name the seven sources we identified each of them poses a different challenge a timeseries with complex timestamp structures in custom formats and datasets which have essential metadata in their preamble apsim agmip b online datasets having a simple timeseries structure uk met office or a more complex one bom met c datasets stored in one file knmi or dispersed in multiple files within folders swiss tph and d abstract data models applied to text files hoa and relational databases bom air in subsection 4 1 we describe the case studies against which we evaluated edam we also highlight challenges associated with each dataset these challenges were addressed by employing edam input functions during the development of the input templates table 3 presents the exact functions used to cope with challenges for each case study besides timeseries data storing corresponding metadata is an essential requirement for edam the metadata curation i6 function was applied on every dataset for all case studies we developed edam templates as needed and successfully parsed the datasets using a single edam command the developed templates are available as supplementary material a and also on the edam github repository along with detailed instructions on how to repeat the experiments with the exception of swiss tph and bom as original data are not publicly available 4 1 test cases 4 1 1 agmip and apsim weather data files the agricultural model intercomparison and improvement project agmip porter et al 2014 have brought agricultural model data sharing into the spotlight within agmip various agricultural model data inputs and outputs such as the apsim keating et al 2003 were transformed into the common agmip data scheme here we worked only with the weather data files note that agmip and apsim data files use different timestamp formats apsim uses days of year and years while agmip timestamp is represented through year month date components we addressed the challenge of composing these into one universal timestamp with the timestamp assembly i8 function another challenge was related to metadata encoded in the preamble of apsim data files the apsim weather file includes station metadata above the timeseries data such as station name location and others we addressed this challenge of extracting metadata from the preamble with the metadata curation i6 function 4 1 2 uk meteorological office in the context of open data the uk meteorological office reports historical observations of 27 weather stations for every station monthly observations are stored in one text document new observations are appended every month and each weather station can be found on a certain web location they follow the pattern http www metoffice gov uk pub data weather uk climate stationdata station name data txt where station name is replaced with an actual station name data points reported have special markers for the quality of the reported values markers are weakly defined in each document preamble for example estimated data is marked with a after the value and missing values are represented through the notation such markers make it difficult to parse and reuse the data directly capturing such observation specific quality attributes is a further challenge that edam in its current version does not support we used the conditional filtering i7 function in order to filter out the missing values 4 1 3 australian bureau of meteorology meteorological datasets the bureau of meteorology bom in australia offers historical meteorological timeseries for a number of weather stations across australia they concern daily observations which are published every month as html and csv documents with the same structure users can access the timeseries by crafting urls which comprise information about the requested station id and month year for example the url for the meteorological data about adelaide station 5002 station id for october 2018 is http www bom gov au climate dwo 201810 text idcjdw5002 201810 csv the challenge in acquiring bom timeseries is in regard to their structure it is a common practice in delimiter separated files that every row corresponds to one observation for a given timestamp however each bom row reports two observations for the same daily timestamp these two observations report the same measured quantity at different times on the same day thus the sampling hour needs to complement the daily timestamp for the bi daily observations is included in the dataset s header we addressed this challenge with the timestamp assembly i8 function we also utilized custom jinja2 macros in order to support this type of tabular timeseries 4 1 4 koninklijk nederlands meteorologisch instituut knmi the royal netherlands meteorological institute knmi provides weather services for the netherlands they offer historical observations as text documents for our study we parsed historical observations from 37 dutch weather stations from 1901 to 2016 each weather station reports daily observations for 39 observables the dataset comes as a whole in a single text file of 158 mb which includes metadata in the preamble the challenge here was to separate the metadata from the timeseries using templates we addressed this challenge by utilizing metadata curation i6 and relationship establishment i9 functions 4 1 5 swiss tropical and public health institute the swiss tropical and public health institute tph monitors air quality in train stations among others both stationary and moving stations are used consisting of multiple sensing units each sensing instrument exports its measurements in a file in a sensor dependant format all station related files are stored in a folder structure additionally there are multiple data formats associated with a station as different sensor types are involved in the various studies for example the gps sensor exports its readings in a file with seven columns date time latitude longitude speed bearing altitude the challenge with the swiss datasets is related to the aforementioned folder tree structure not all file types are present in all folders so edam is challenged to match the various files found against several templates in order to extract observations we addressed this challenge by navigating folders with the folder exploration i5 function and matching files with the corresponding templates with the file parsing i3 function we associated the different datasets to the corresponding station with the timeseries merging i19 function the other challenge was to combine location data with the other observations into one output file specifically each sensor took observations at different time intervals edam automatically solves the issue by combining together observations sharing the same timestamp in a data fusion scenario output templates could be used for homogenising the reporting timestamps of the various sensors involved in a study 4 1 6 hydrological observatory of athens the hydrological observatory of athens hoa offers a service endpoint for hydrological timeseries several observed properties are reported for 23 stations each of them is offered separately on the web and every observed property dataset has a unique url timeseries are reported under the same abstract format consisting of a preamble with relevant metadata i e about the station observed property unit of measurement etc and the actual timeseries in the form of key value pairs the challenge in acquiring hoa timeseries concerns the abstract data format in non abstract data formats a given file column corresponds to a certain observable which is mentioned in the header in contrast the hoa abstract data format mentions the observed property at the preamble of each document we addressed this challenge by drafting an abstract input template the specific observable id was defined dynamically based on the metadata found in the document preamble again here each station reports several files one for each observable but all have exactly the same format instead of drafting as many templates as the available observed properties we use a generic template that includes the observable id in a data fusion scenario output templates can be used for linking together the various observables of the same station 4 1 7 australian bureau of meteorology air quality dataset bom developed a historical database that contains hourly air quality timeseries in several locations in australia we were given access to this postgresql database that contains data of common pollutants as so2 o3 co no2 and pm10 in 99 stations and corresponds to a period of 20 years 1988 2008 in total there are about 15 million records observations are stored in key value pairs with detailed metadata about the stations and observed quantities metadata are stored in a different relational database system i e oracle in total there are four tables in this implementation the challenge in acquiring these timeseries lies in the relational databases and the chosen structure data and metadata are stored in different tables across different database systems in the observations table each observation is associated with the corresponding station in the station metadata i e name location altitude table each station is referenced with the aforementioned identifier we addressed the challenge of realizing the external relationships so data are appropriately linked when harvested with the relationship establishment i9 function 4 2 demonstrating edam output with regard to dissemination services an example output of edam is shown in fig 6 we demonstrate edam acquisition and integration for all australian weather stations for july 2017 specifically edam utilizes uri generation i1 to discover 478 bom stations employing online parsing i2 and using one template for all stations edam acquired and stored approximately 210 000 data points above operations were realized through a single edam command that looks like edam input http www bom gov au climate dwo 201707 text idcjdw 7b2 8 7d0 7b01 82 7d 201707 csv template bom tmpl metadata bom yaml edam processing capabilities are statistical filters and conditional exports fig 7 exhibits them when applied to a uk met dataset specifically we aggregate daily into monthly observations with the resampling p1 function consequently we illustrate the conditional export p3 function exporting only those datapoints which satisfy a given condition processing functions are typed in the output template files the data transformation d3 function facilitates the dataset transformations from one format to another we demonstrate this feature with the agmip dataset and the webxtreme service klein et al 2017 the latter is a web service which given an input in a certain format calculates extreme weather indicators transforming an edam curated dataset requires the draft of a template file for the target format fig 8 demonstrates the creation of a custom data view by simply drafting a new template file 4 3 lessons learned while we aimed with this work to lower the barrier for e scientists we realized early that non standard data formats usually lead to complex templates this is due to the inherent complexities of environmental data domain and the poor design choices that often come with legacy formats the most complex data format we faced was bom met in all other cases each column represented a single observable however in the case of bom met the same observable was reported in two columns each column reported measurements taking place at different times in a day the exact time each measurement was taken was noted in the header using edam functions and jinja2 utility helpers we successfully acquired and integrated bom datasets another factor which leads to complex templates is when metadata are mixed with timeseries data this is the case with the knmi dataset where metadata about all stations and all their observables precede the observations parsing html tables using templates was rather cumbersome initially we tried to parse bom met weather stations in their html form however html comprises numerous tags which provide an aesthetic view to the page e g colors aligns fonts these tags hinder the draft of a reusable template file and render its composition a rather complex process thus in its current release edam cannot directly parse timeseries stored along with html tags rather these should be stripped out as a pre processing step there are also challenges in the way timestamps are represented in different datasets edam provides users with an intuitive mechanism to annotate different timestamp components among the edam case studies we successfully parsed all different timestamp representations in most of the datasets we experimented with timestamp components were spread in more than one column and they were not in an iso 8601 format for instance in apsim weather files the timestamp is as ordinal date comprised of two columns the first one for the year and the second for the day of the year julian date edam internally composes a universal timestamp object so data consumers during data output can transform a timestamp in as many components as they want timezone information is essential especially for spatially diverse datasets among all case studies the timezone of the reported observations was explicitly reported only in one hoa all other datasets contained timezone information neither on the dataset nor on the corresponding metadata files interestingly the bom online portal which serves observations for the whole australian continent does not state the timezone in which observations are reported edam is able to assign timezone information to datasets either using station level metadata or deriving it from the station geolocation in cases where no timezone information is declared the gmt timezone is used while this is not a performance study we measured some performance indicators the knmi dataset was the most voluminous dataset we parsed 158 mb edam parsed and stored over 24 million datapoints in less than 8 minutes on a pc with 16 gb ram nevertheless volume is not the only constraint in our attempt to discover bom met weather stations edam generated and requested 574 unique uris from the 574 generated stations 478 existed submitting the http get requests reading the responses and downloading the datasets took about 5 minutes station data were about 2 mb in total iterating through the 478 station timeseries and storing them took almost 11 minutes in another example the agmip dataset which consisted of one 1 mb file and more than 90 000 datapoints was parsed and stored in about 2 seconds 5 discussion and conclusions today environmental datasets are either available through interoperable environmental data management frameworks or can be found in raw non standardized formats both approaches require significant effort and usually a computer science background in order for data to be acquired integrated and re used in this work we present edam a template framework as a universal strategy of acquiring and integrating diverse environmental timeseries data edam copes with diversity in terms of data storage type i e files webpages databases and data format i e relational key value pairs the edam data acquisition and integration capabilities have been investigated in the light of several test cases using edam we acquired and integrated datasets with different characteristics demonstrating its generality the evaluation of the software against timeseries with simple and more complex structure provides insights about the system s extended outreach edam supports not only timeseries stored in files as the template parsing files introduced in mason et al 2014 but also from webpages and relational databases data transformation into consistent data formats and dissemination through standardized protocols is essential for syntactic interoperability in the iot era edam users can transform legacy environmental datasets between data formats by using edam templates in this way edam contributes towards a environmental model re usability by transforming data inputs outputs in scientific workflows granell et al 2010 and b environmental data fairness as it facilitates timeseries re usability and interoperability and enhances reproducibility wilkinson et al 2016 it also promotes further environmental data discovery and access through standardized dissemination protocols i e the ogc sensor observation service we consider that edam also contributes towards lowering the e science barriers swain et al 2016 in contrast with most methodologies for acquiring eiot datasets reported in the literature edam does not presuppose a strong computer science background we argue that templates offer a compromise between generality and complexity the system is founded around a template language which uses programming language agnostic semantics users are not required to have more programming skills than they already have in order to draft an edam template as we demonstrated in section 4 the templates drafted with edam language are reusable and can be used for both data input and output the edam design embraces the open source principles and allows for future extensions on the processing layer the system offers some pre implemented processing functions which can be called by end users these support the on the fly calculation of values which were not originally stored in the database and facilitate sensor data fusion and or aggregation external users more advanced with computer science background can extend the system by defining such processing functions 5 1 future work future work may focus on issues related to semantic interoperability edam supports metadata annotation of observables using ontologies while these annotations are stored in the system they are not fully utilized in its current version edam lacks a semantic layer to act upon datasets and templates in principle a dataset that was acquired through an input template can be transformed with another template only if both templates utilize the very same observable ids the observable ids are drafted by data curators and represent certain observables future work may investigate the use of a reasoner to resolve relations between the different observable ids in this way a certain data file format can be represented through a single template and by assigning synonym terms in an ontology we could enable automatic transformation into other formats another direction for future work is to support environmental timeseries datasets in other formats in this work we evaluated edam against text based documents and relational databases however environmental datasets are also available in data cubes and non relational databases edam could be extended to support such other sources 5 2 conclusions in this work we provided a proof of concept and a tested implementation of a template system that can be used for environmental timeseries acquisition and integration we demonstrated that the use of templates for data acquisition in the environmental internet of things provides a compromise between generality and complexity we designed and implemented an open source extensible template framework called edam to support environmental timeseries data acquisition integration and dissemination services without the prerequisite of a strong computer science background we enable users to extract datasets and create custom views out of them by defining the desired output format as a template in this way users can re use environmental timeseries data into scientific workflows edam also supports opening legacy datasets as services on the web through ogc sos currently edam supports data acquisition and integration of timeseries stored in relational databases files in folder structures and webpages the test cases we used to evaluate edam provided us with insights about its general purpose nature the novelty of this approach is that we are not trying to propose another standard but rather that we have developed a specific language for describing data file structures in a generic way using templates also such templates are programming language agnostic so that users of different computer literacy profiles could develop them software availability name of software edam environmental data acquisition module developers argyrios samourkasidis evangelia papoutsoglou ioannis n athanasiadis contact argysamo gmail com software required any operating system with python 3 program language python 3 license gnu general public license software availability released via python s pip package management system source code on https github com bigdatawur edam acknowledgements we would like to offer our special thanks to mr stavros foteinopoulos for his valuable and constructive suggestions during the design and implementation of the software we are grateful to dr robert argent and dr andre zerger from the australian bureau of meteorology for providing us with the historical air quality database we would also like to express our gratitude to dr ming yi tsai and dr mark davey from the swiss tropical and public health institute for providing us with the air quality dataset and case study ia was partially supported by the wageningen university and research strategic investment theme programme resilience finally we are grateful to dr s osinga and the three anonymous reviewers for their constructive feedback and comments appendix a supplementary data the following is the supplementary data to this article multimedia component multimedia component appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 009 
26223,environmental timeseries data variety is exploding in the internet of things era making data reuse a very demanding task data acquisition and integration remains a laborious step of the environmental data lifecycle environmental data heterogeneity is a persistent issue as data are becoming available through different protocols and stored under diverse custom formats in this work we deal with syntactic heterogeneity in environmental timeseries data our approach is based on describing different dataset syntaxes using abstract representations called templates we designed and implemented edam environmental data acquisition module a template framework that facilitates timeseries data acquisition and integration edam templates are written using programming language agnostic semantics and can be reused both for input and output thus enabling data reuse via transformations across different formats we demonstrate edam generality in seven case studies which involve scraping online data extracting observations from a relational database or aggregating historical timeseries stored in local files case studies span different environmental sciences domains including meteorology agriculture urban air quality and hydrology we also demonstrate edam for data dissemination as instructed by output templates we identified several syntactic interoperability challenges though the case studies that include managing with differences in formatting observables temporal and spatial references and metadata documentation and addressed them with edam edam implementation has been released under an open source license keywords environmental timeseries internet of things syntactic interoperability data acquisition templates big data 1 introduction environmental data management that is acquisition processing storage and dissemination athanasiadis and mitkas 2004 mason et al 2014 is becoming more challenging in the era of big data bd and the internet of things iot in the contemporary data rich society a great variety of sensors and iot devices enable the collection of large observation volumes which can be further processed for enabling new knowledge insights at the same time this era is characterized as knowledge poor since universal data management and heterogeneous data integration remain still open challenges negru et al 2016 environmental data acquisition seems to be the most laborious step within the environmental data lifecycle terrizzano et al 2015 harth et al 2013 horsburgh et al 2009 this is attributed to the heterogeneity pertinent to environmental data sources environmental datasets are collected and stored under different data models in various forms mainly in files and relational databases horsburgh et al 2011 datasets which do not share common data formats and or communication protocols are difficult to be re used without human expert involvement syntactic heterogeneity is a factor which hinders the adoption of a universal strategy to acquire data originating from disparate information sources it also obstructs the environmental data science core objective to narrow the data to knowledge latency elag et al 2017 by supporting environmental data discovery and access and by enabling re usability horsburgh et al 2009 ames et al 2012 athanasiadis 2015 holzworth et al 2015 granell et al 2010 fair findable accessible interoperable reusable guiding principles for scientific data management and stewardship highlight the importance of scientific data reusability and reproducibility wilkinson et al 2016 long term archival and preservation of digital assets also implies the regular transformation of data between storage formats and media there are two approaches to tackle syntactic heterogeneity the first is to use adopt frameworks which were designed to facilitate environmental data discovery and accessibility such as the ogc sensor web enablement swe botts et al 2008 and cuahsi hydrologic information system his horsburgh et al 2009 such systems hide the underlying complexity of environmental data sources and expose them in a standardized manner through established data models e g o m cox 2011 sensorml botts and robin 2014 waterml 2 0 taylor 2014 etc the various datasets need to be stored in a common schema in order to be exposed through a data sharing framework this entails certain modifications which introduce overhead and commonly require a strong computer science background to implement andrae et al 2009 the second approach is to develop programming language scripts each one tailored to the custom data format eberle et al 2013 woodard 2016 these custom to data scripts usually transform a dataset into a common data schema porter et al 2014 which allows for further processing analysis or dissemination tasks boote et al 2015 these approaches have been used also for exchanging data between environmental models i e porter et al 2014 horita et al 2015 peckham and goodall 2013 jones et al 2015 both approaches rely upon computer programming skills that are not always available this contradicts the lowering e science barriers movement swain et al 2016 that envisions accessing data in an uncomplicated fashion so that e scientists can entirely focus on the domain of their expertise and not on side tasks such as curating datasets by the term e science we refer to a global collaboration in key areas of science hey and trefethen 2003 which promotes innovation in collaborative computationally or data intensive research across all disciplines throughout the research lifecycle international confer 2018 based on our experience transforming environmental datasets from different sources in order to fit as input to environmental models requires manual work which is hardly re usable for example different programming languages e g python van rossum and drake 2003 r ihaka and gentleman 1996 etc and data models e g o m waterml 2 0 etc are adopted for the scripting and environmental data management framework approaches respectively in this paper we outline the design and demonstrate an open source implementation of the environmental data acquisition module edam that addresses issues of syntactic data heterogeneity using templates an edam template is an abstract representation of a data file s contents using programming language agnostic semantics edam supports data acquisition integration and transformation from a variety of file types and syntaxes through templates specifically edam is applicable for environmental timeseries datasets stored in various data formats delimiter separated files flat files etc at various sources files folders databases websites and implementing different data models tables key value pairs edam employs a declarative approach to enable scientists to annotate their data by means of templates it automatically parses the data matches them with templates stores them and optionally exports them to a format described by an output template this allows for end users to query retrieve and transform environmental timeseries datasets into their own formats also edam supports interoperable data dissemination through standardized protocols e g ogc sos broering et al 2012 we also demonstrate its front end graphical user interface gui for creating maps we demonstrate edam in seven cases studies from various environmental domains including air quality meteorology agriculture and hydrology to the best of our knowledge this is the first time that structural templates are extensively used for environmental data management tasks i e acquisition integration and dissemination we started exploring this approach in papoutsoglou et al 2015 where we investigated a case study for collecting data from a smoky swiss railway station here we extend our work with six more real world cases scraping meteorological data from the public webpages of the bureau of meteorology bom in australia and the uk met office parsing hydrological timeseries data from the hydrological observatory of athens hoa extracting observations from an air quality archive from bom originally stored in a relational database aggregating historical timeseries data from all dutch weather stations provided by koninklijk nederlands meteorologisch instituut knmi transforming weather input data of apsim crop model holzworth et al 2014 into the agmip format porter et al 2014 the rest of the paper is structured as follows in section 2 we review contemporary approaches for environmental data acquisition and integration and introduce readers to environmental data management with web template frameworks section 3 presents the edam architecture specifically key requirements user types and use scenarios section 4 demonstrates edam the conducted experiments the used datasets and the addressed challenges finally in section 5 we discuss our research findings and lessons learned conclude the research summarizing key findings and future work 2 background and related work in the environmental data literature different terms are used for describing the process of obtaining a dataset and transforming it into another format specifically the terms harmonization porter et al 2014 mediation and conversion horsburgh et al 2011 management and publication jones et al 2015 integration beran et al 2009 acquisition and collation mason et al 2014 and wrangling terrizzano et al 2015 kandel et al 2011 are synonyms for data acquisition and integration in this work we focus on acquisition and integration of environmental timeseries data in general the acquisition process works as follows a station stationary or not houses one or more sensors a sensor measures one or more observable s producing observations an observation has a value expressed in some units and refers to a certain timestamp and possibly a location in this context environmental observations without a temporal dimension e g soil data are not considered timeseries and thus can not be processed by edam also note that edam can process location data when they are associated with a timeseries i e observations of latitude longitude angle etc at a certain timestamp location data are stored as regular timeseries and can be combined with other observations this is elaborated further in subsection 4 1 in the rest of this section we review approaches that cope with syntactic heterogeneity first we present environmental data management frameworks which by design account for syntactic interoperability environmental data management frameworks are typically used for preparing inputs required for executing scientific workflows decision support tools or environmental models however not all environmental datasets are offered through such frameworks second in subsection 2 2 we present the scripting approach which facilitates environmental data transformation to fit into a consistent data format last subsection 2 3 introduces web template frameworks and presents our previous experiences with them 2 1 environmental data management frameworks providing standardised discovery and access services for environmental data is a key requirement for an environmental data management framework horsburgh et al 2011 examples of such frameworks are the ogc sensor web enablement swe which supports timeseries dissemination through the sensor observation service sos broering et al 2012 and the cuahsi hydrologic information system his horsburgh et al 2009 both provide interoperable data access on two layers a communication b data representation communication is achieved by defining standardized ways to request environmental data e g getvalues for cuahsi his ames et al 2012 getobservation for ogc sos broering et al 2012 data representation deals with data dissemination through standardized information models which hide the underlying data complexity for example waterml 2 0 taylor 2014 is promoted by both frameworks in order to represent hydrological timeseries data environmental data management frameworks can provide interoperable access to raw data by transforming them to a common data model this common data model can be part of the framework or its implementation in the case of ogc sos there are different software implementations which use different data models mcferren et al 2009 on the other hand cuahsi his is founded around the observations data model horsburgh et al 2008 software tools were implemented to import data into an odm database horsburgh and tarboton 2007 document a data loader component which imports tabular timeseries into an odm instance mason et al 2014 present an environmental management framework which utilizes reusable data parsing templates to annotate tabular timeseries and import them into an odm instance 2 2 data integration through scripting several efforts are reported in the literature where scripts have been used for environmental timeseries acquisition and integration by the term script we refer to a small computer program which is intended to automate a task regardless of whether the programming language in which it was developed is considered a scripting language e g python or not e g java for example the ag analytics platform woodard 2016 demonstrates a data warehouse to retrieve data from heterogeneous data sources it extracts data through custom scripts written in python one for every data source in another example harth et al 2013 employ a linked data scripting language called data fu stadtmüller et al 2013 to integrate diverse data sources each data fu program comes with data source specific rules and queries in a third line of work porter et al in porter et al 2014 present a data harmonization workflow to promote model inter comparison and ensemble modelling data source specific translators were developed and used to integrate heterogeneous datasets into the agmip common data schema in order to facilitate data exchange between crop models 2 3 environmental data management with web template systems web template systems are designed to create dynamic content and are extensively used in web applications they are used for automatically generating custom content such as customer invoices search results data reports etc web template systems e g jinja2 ronacher 2008 mako bayer cheetah3 broytman and croy 2001 are intuitive to use and do not require advanced programming skills each one comes with a template language which is used to markup templates a template is a document which represents a data structure using variables geebelen et al 2008 dynamic views are rendered by feeding a template with data and template variables are substituted with values web template systems can support data output by design but not data input directly for example in samourkasidis and athanasiadis 2017 we employed jinja2 to create on the fly dynamic views for environmental data dissemination in a previous work papoutsoglou et al 2015 we also started experimenting with using template files as a markup for data input where we presented a platform which used templates to read from local files in a variety of formats 2 4 summary acquiring and integrating environmental timeseries in a consistent data format is a manual process which requires significant efforts this is because the vast majority of environmental datasets available in the environmental internet of things eiot are heterogeneous by nature hart and martinez 2015 universal data acquisition and integration can be achieved through the scripting approach nevertheless there is a trade off between generality and complexity this approach opposes the lowering e science barriers since it presumes a computer science background swain et al 2016 a web template framework language is much more simple compared to a traditional programming language in this work we investigate the use of templates in order to acquire and integrate environmental timeseries datasets and seek for a compromise in the trade off between complexity and generality 3 the edam framework 3 1 objectives there were three objectives in designing and developing edam the first was to lower e science barriers by embracing a programming language agnostic solution obtaining timeseries data by writing small computer programs scripts has already been investigated see subsection 2 2 thus we focused on solutions that involve as little as possible programming skills for its end users and examine the use of templates written with a simple programming language agnostic markup the second objective was to apply edam to a wide variety of case studies in order to tackle the intrinsic heterogeneity of environmental data sources this heterogeneity is related to a data source type which could be text files webpages databases web services b data formats i e comma separated values csv tab separated values tsv etc and c data models after which available environmental data are structured the third objective was to create custom views of timeseries data and disseminate them through standard interoperable protocols as ogc swe standards this enables users to transform data from one format to another promoting interoperability for environmental modelling and overcoming problems related to the diversity of data models it also copes with syntactic interoperability by exposing edam processed datasets via established information models such as o m and sensorml 3 2 abstract architectural design there are three key components involved in edam a input files b template files and c template engine fig 1 depicts the interaction between edam components for data input and output in all cases the data are extracted from their original source and stored in the edam database in a unified data model then they can be fetched and presented in a user defined way using a range of custom templates any kind of text based source can serve as an input inputs are stored in one or more files locally or remotely they may be stored in a local nested folder structure on a website or relational databases from which data could be extracted with sql queries an edam template is an abstract representation of data file contents each template file is bound to a specific data syntax which is comprised of a a timestamp which may come in different formats as we discuss below b a set of observables in a given order along with optional metadata annotating their semantics omitting observables changing their order of appearance and or changing timestamp representation results in a different data syntax i e requires a different template we envision that one template will be needed per sensor vendor or legacy data formats used for input output by environmental models templates can be used for specifying both input or output data file structures and are written using the edam template language the edam template engine and language are the core of the framework offering various processing capabilities the template language itself is founded on programming language agnostic semantics besides simple data parsing the edam template engine supports mathematical and statistical operations both the template engine and language are implemented after jinja2 ronacher 2008 this enables us to use the jinja2 mature framework for data dissemination purposes regarding data dissemination edam may offer acquired data as services on the web currently edam supports data dissemination through ogc sensor observation service and its own edam api the edam api enables the creation of custom data views since edam templates can be called dynamically edam template language artefacts keywords or user defined variables are located inside placeholders the edam template language has four restricted keywords station observable sensor timestamp which result from the edam underlying data model fig 2 depicts the edam unified data model along with the template language restricted keywords the data model was designed after our assumption of an environmental data source and it is tailored to the needs of the template language this is also the reason why we did not reuse any third party data model a third party data model involves a number of external dependencies via foreign keys that would affect the template language syntax rendering it complex and difficult to use user defined variables are used to annotate the observables found in a dataset their semantics are further specified in a metadata file a template may contain control statements e g if then else for loops to provide formatting and control functionality and set the logic which will be used for data retrieval next to the template file there is the metadata file it is drafted by users in order to further annotate data parsed from input files metadata include information commonly not stored directly in the original input files as for example units of measurement for observables or station locations such additional metadata which may include terms from ontologies are necessary for enriching the semantics of the original data how this works is further detailed in the following section 3 3 workflow the edam workflow operates in two phases data input and data output data input concerns data acquisition preprocessing and storage processes data output involves the discovery transformation and dissemination of information fig 3 depicts the workflow for data input and output accordingly we identify two user roles in the edam system data curators are interested in sharing data with edam added value services and import datasets into the system they draft input templates making new data sources available data consumers are e scientists i e modellers researchers decision makers who are interested in third party data stored in edam they use edam to a view available datasets b render graphs c apply filters on data and d download them in various formats i e csv txt etc they may create custom data views by editing template files e g change the order of columns omit columns etc software agents can also be considered as data consumers they interact with the system using ogc sos or the edam api a workflow to input data into edam is as follows a data curator drafts the input template documents all relevant metadata in a metadata file and finally provides a data source an input template has the original data source structure data curators may provide edam with metadata using the metadata files to augment the original information with additional details about the station the involved observables and their corresponding sensors and units of measurement while this step is optional it is critical towards data reusability and interoperability fig 4 shows a sample input file from the uk met office fig 4a and the corresponding template file fig 4b edam keyword station has been used for annotating all data relevant to the station name and location the keyword timestamp is used to parse the component of the date to which the observations correspond to in this case we used timestamp year and timestamp month to parse the year and the month respectively generic jinja2 keywords such as for are used to parse all data reported in the file user defined keywords are used as variable names to annotate observable values as tmax value tmin value af value rain value and sun value fig 5 depicts the corresponding metadata file which defines additional station and observable metadata data curators can specify the timezone station attribute which will be used to complement all the station related timestamps the value of the timezone attribute can be either the format as code keyword or a timezone from the tz database wikipedia contributors and l 2018 in case the format as code keyword is used edam automatically identifies the corresponding timezone from the station s location and assigns it to the related timeseries greenwich mean time gmt is the default timezone which is used when no location is provided or the timezone attribute in the metadata file is omitted data curators also use the metadata file to relate a user defined keyword e g tmax with a its corresponding observable name e g temperature maximum and b the unit it was reported e g celsius there is also a section to store metadata about the utilized sensors which in this example are unknown 3 4 implementation and modes of operation in table 1 we depict edam implemented functions organized by when they are utilized input functions are applied by edam during the process of data input processing functions concern statistical and conditional filters which are written in output templates and are applied during data output last but not least dissemination functions are added value services offered for edam imported datasets edam software has been developed in python and is available as open source software on github samourkasids et al 2018 under the gnu affero general public license version 3 it is also distributed as an autonomous python package through the python package index pip python software foundation 2018 and can be installed on a computer with python installed by typing pip install edam the pandas python library mckinney 2011 supports the edam input and processing functions the edam dissemination functions are implemented with the flask web framework ronacher 2010 and acquired timeseries are offered as ogc sos services through the python implementation reported in samourkasidis and athanasiadis 2017 the hardware requirements of edam are minimal we installed edam and tested its functionalities on a raspberry pi 2 model b mini computer raspberry pi foundation 2018 without any issues edam operates as a local standalone system this means that edam parsed datasets are stored and can be accessed locally on user s computer installation automatically creates a folder in the home directory in which the user should store templates and metadata files after installing edam two commands are available via the command line edam and viewer these commands reflect the two distinct modes of operation the command line mode and the graphical user interface mode in the command line mode data curators utilize the edam command to define the input arguments i e input template and metadata file in order to parse and store a dataset optionally they can define the output parameters i e template and metadata file in order to transform a dataset on the fly in the graphical user interface gui mode we assume that some datasets have already been imported in edam s database and the user wants to disseminate them via the edam web services the viewer command starts the edam web services which currently are the api ogc sos and the web front end human users can access these services on their browser and machines via the appropriate protocol note that the web front end includes information about the edam api how to access the stored datasets and the ogc sos instance 4 demonstration we demonstrate edam extended outreach by acquiring environmental timeseries data from diverse data sources in table 2 we name the seven sources we identified each of them poses a different challenge a timeseries with complex timestamp structures in custom formats and datasets which have essential metadata in their preamble apsim agmip b online datasets having a simple timeseries structure uk met office or a more complex one bom met c datasets stored in one file knmi or dispersed in multiple files within folders swiss tph and d abstract data models applied to text files hoa and relational databases bom air in subsection 4 1 we describe the case studies against which we evaluated edam we also highlight challenges associated with each dataset these challenges were addressed by employing edam input functions during the development of the input templates table 3 presents the exact functions used to cope with challenges for each case study besides timeseries data storing corresponding metadata is an essential requirement for edam the metadata curation i6 function was applied on every dataset for all case studies we developed edam templates as needed and successfully parsed the datasets using a single edam command the developed templates are available as supplementary material a and also on the edam github repository along with detailed instructions on how to repeat the experiments with the exception of swiss tph and bom as original data are not publicly available 4 1 test cases 4 1 1 agmip and apsim weather data files the agricultural model intercomparison and improvement project agmip porter et al 2014 have brought agricultural model data sharing into the spotlight within agmip various agricultural model data inputs and outputs such as the apsim keating et al 2003 were transformed into the common agmip data scheme here we worked only with the weather data files note that agmip and apsim data files use different timestamp formats apsim uses days of year and years while agmip timestamp is represented through year month date components we addressed the challenge of composing these into one universal timestamp with the timestamp assembly i8 function another challenge was related to metadata encoded in the preamble of apsim data files the apsim weather file includes station metadata above the timeseries data such as station name location and others we addressed this challenge of extracting metadata from the preamble with the metadata curation i6 function 4 1 2 uk meteorological office in the context of open data the uk meteorological office reports historical observations of 27 weather stations for every station monthly observations are stored in one text document new observations are appended every month and each weather station can be found on a certain web location they follow the pattern http www metoffice gov uk pub data weather uk climate stationdata station name data txt where station name is replaced with an actual station name data points reported have special markers for the quality of the reported values markers are weakly defined in each document preamble for example estimated data is marked with a after the value and missing values are represented through the notation such markers make it difficult to parse and reuse the data directly capturing such observation specific quality attributes is a further challenge that edam in its current version does not support we used the conditional filtering i7 function in order to filter out the missing values 4 1 3 australian bureau of meteorology meteorological datasets the bureau of meteorology bom in australia offers historical meteorological timeseries for a number of weather stations across australia they concern daily observations which are published every month as html and csv documents with the same structure users can access the timeseries by crafting urls which comprise information about the requested station id and month year for example the url for the meteorological data about adelaide station 5002 station id for october 2018 is http www bom gov au climate dwo 201810 text idcjdw5002 201810 csv the challenge in acquiring bom timeseries is in regard to their structure it is a common practice in delimiter separated files that every row corresponds to one observation for a given timestamp however each bom row reports two observations for the same daily timestamp these two observations report the same measured quantity at different times on the same day thus the sampling hour needs to complement the daily timestamp for the bi daily observations is included in the dataset s header we addressed this challenge with the timestamp assembly i8 function we also utilized custom jinja2 macros in order to support this type of tabular timeseries 4 1 4 koninklijk nederlands meteorologisch instituut knmi the royal netherlands meteorological institute knmi provides weather services for the netherlands they offer historical observations as text documents for our study we parsed historical observations from 37 dutch weather stations from 1901 to 2016 each weather station reports daily observations for 39 observables the dataset comes as a whole in a single text file of 158 mb which includes metadata in the preamble the challenge here was to separate the metadata from the timeseries using templates we addressed this challenge by utilizing metadata curation i6 and relationship establishment i9 functions 4 1 5 swiss tropical and public health institute the swiss tropical and public health institute tph monitors air quality in train stations among others both stationary and moving stations are used consisting of multiple sensing units each sensing instrument exports its measurements in a file in a sensor dependant format all station related files are stored in a folder structure additionally there are multiple data formats associated with a station as different sensor types are involved in the various studies for example the gps sensor exports its readings in a file with seven columns date time latitude longitude speed bearing altitude the challenge with the swiss datasets is related to the aforementioned folder tree structure not all file types are present in all folders so edam is challenged to match the various files found against several templates in order to extract observations we addressed this challenge by navigating folders with the folder exploration i5 function and matching files with the corresponding templates with the file parsing i3 function we associated the different datasets to the corresponding station with the timeseries merging i19 function the other challenge was to combine location data with the other observations into one output file specifically each sensor took observations at different time intervals edam automatically solves the issue by combining together observations sharing the same timestamp in a data fusion scenario output templates could be used for homogenising the reporting timestamps of the various sensors involved in a study 4 1 6 hydrological observatory of athens the hydrological observatory of athens hoa offers a service endpoint for hydrological timeseries several observed properties are reported for 23 stations each of them is offered separately on the web and every observed property dataset has a unique url timeseries are reported under the same abstract format consisting of a preamble with relevant metadata i e about the station observed property unit of measurement etc and the actual timeseries in the form of key value pairs the challenge in acquiring hoa timeseries concerns the abstract data format in non abstract data formats a given file column corresponds to a certain observable which is mentioned in the header in contrast the hoa abstract data format mentions the observed property at the preamble of each document we addressed this challenge by drafting an abstract input template the specific observable id was defined dynamically based on the metadata found in the document preamble again here each station reports several files one for each observable but all have exactly the same format instead of drafting as many templates as the available observed properties we use a generic template that includes the observable id in a data fusion scenario output templates can be used for linking together the various observables of the same station 4 1 7 australian bureau of meteorology air quality dataset bom developed a historical database that contains hourly air quality timeseries in several locations in australia we were given access to this postgresql database that contains data of common pollutants as so2 o3 co no2 and pm10 in 99 stations and corresponds to a period of 20 years 1988 2008 in total there are about 15 million records observations are stored in key value pairs with detailed metadata about the stations and observed quantities metadata are stored in a different relational database system i e oracle in total there are four tables in this implementation the challenge in acquiring these timeseries lies in the relational databases and the chosen structure data and metadata are stored in different tables across different database systems in the observations table each observation is associated with the corresponding station in the station metadata i e name location altitude table each station is referenced with the aforementioned identifier we addressed the challenge of realizing the external relationships so data are appropriately linked when harvested with the relationship establishment i9 function 4 2 demonstrating edam output with regard to dissemination services an example output of edam is shown in fig 6 we demonstrate edam acquisition and integration for all australian weather stations for july 2017 specifically edam utilizes uri generation i1 to discover 478 bom stations employing online parsing i2 and using one template for all stations edam acquired and stored approximately 210 000 data points above operations were realized through a single edam command that looks like edam input http www bom gov au climate dwo 201707 text idcjdw 7b2 8 7d0 7b01 82 7d 201707 csv template bom tmpl metadata bom yaml edam processing capabilities are statistical filters and conditional exports fig 7 exhibits them when applied to a uk met dataset specifically we aggregate daily into monthly observations with the resampling p1 function consequently we illustrate the conditional export p3 function exporting only those datapoints which satisfy a given condition processing functions are typed in the output template files the data transformation d3 function facilitates the dataset transformations from one format to another we demonstrate this feature with the agmip dataset and the webxtreme service klein et al 2017 the latter is a web service which given an input in a certain format calculates extreme weather indicators transforming an edam curated dataset requires the draft of a template file for the target format fig 8 demonstrates the creation of a custom data view by simply drafting a new template file 4 3 lessons learned while we aimed with this work to lower the barrier for e scientists we realized early that non standard data formats usually lead to complex templates this is due to the inherent complexities of environmental data domain and the poor design choices that often come with legacy formats the most complex data format we faced was bom met in all other cases each column represented a single observable however in the case of bom met the same observable was reported in two columns each column reported measurements taking place at different times in a day the exact time each measurement was taken was noted in the header using edam functions and jinja2 utility helpers we successfully acquired and integrated bom datasets another factor which leads to complex templates is when metadata are mixed with timeseries data this is the case with the knmi dataset where metadata about all stations and all their observables precede the observations parsing html tables using templates was rather cumbersome initially we tried to parse bom met weather stations in their html form however html comprises numerous tags which provide an aesthetic view to the page e g colors aligns fonts these tags hinder the draft of a reusable template file and render its composition a rather complex process thus in its current release edam cannot directly parse timeseries stored along with html tags rather these should be stripped out as a pre processing step there are also challenges in the way timestamps are represented in different datasets edam provides users with an intuitive mechanism to annotate different timestamp components among the edam case studies we successfully parsed all different timestamp representations in most of the datasets we experimented with timestamp components were spread in more than one column and they were not in an iso 8601 format for instance in apsim weather files the timestamp is as ordinal date comprised of two columns the first one for the year and the second for the day of the year julian date edam internally composes a universal timestamp object so data consumers during data output can transform a timestamp in as many components as they want timezone information is essential especially for spatially diverse datasets among all case studies the timezone of the reported observations was explicitly reported only in one hoa all other datasets contained timezone information neither on the dataset nor on the corresponding metadata files interestingly the bom online portal which serves observations for the whole australian continent does not state the timezone in which observations are reported edam is able to assign timezone information to datasets either using station level metadata or deriving it from the station geolocation in cases where no timezone information is declared the gmt timezone is used while this is not a performance study we measured some performance indicators the knmi dataset was the most voluminous dataset we parsed 158 mb edam parsed and stored over 24 million datapoints in less than 8 minutes on a pc with 16 gb ram nevertheless volume is not the only constraint in our attempt to discover bom met weather stations edam generated and requested 574 unique uris from the 574 generated stations 478 existed submitting the http get requests reading the responses and downloading the datasets took about 5 minutes station data were about 2 mb in total iterating through the 478 station timeseries and storing them took almost 11 minutes in another example the agmip dataset which consisted of one 1 mb file and more than 90 000 datapoints was parsed and stored in about 2 seconds 5 discussion and conclusions today environmental datasets are either available through interoperable environmental data management frameworks or can be found in raw non standardized formats both approaches require significant effort and usually a computer science background in order for data to be acquired integrated and re used in this work we present edam a template framework as a universal strategy of acquiring and integrating diverse environmental timeseries data edam copes with diversity in terms of data storage type i e files webpages databases and data format i e relational key value pairs the edam data acquisition and integration capabilities have been investigated in the light of several test cases using edam we acquired and integrated datasets with different characteristics demonstrating its generality the evaluation of the software against timeseries with simple and more complex structure provides insights about the system s extended outreach edam supports not only timeseries stored in files as the template parsing files introduced in mason et al 2014 but also from webpages and relational databases data transformation into consistent data formats and dissemination through standardized protocols is essential for syntactic interoperability in the iot era edam users can transform legacy environmental datasets between data formats by using edam templates in this way edam contributes towards a environmental model re usability by transforming data inputs outputs in scientific workflows granell et al 2010 and b environmental data fairness as it facilitates timeseries re usability and interoperability and enhances reproducibility wilkinson et al 2016 it also promotes further environmental data discovery and access through standardized dissemination protocols i e the ogc sensor observation service we consider that edam also contributes towards lowering the e science barriers swain et al 2016 in contrast with most methodologies for acquiring eiot datasets reported in the literature edam does not presuppose a strong computer science background we argue that templates offer a compromise between generality and complexity the system is founded around a template language which uses programming language agnostic semantics users are not required to have more programming skills than they already have in order to draft an edam template as we demonstrated in section 4 the templates drafted with edam language are reusable and can be used for both data input and output the edam design embraces the open source principles and allows for future extensions on the processing layer the system offers some pre implemented processing functions which can be called by end users these support the on the fly calculation of values which were not originally stored in the database and facilitate sensor data fusion and or aggregation external users more advanced with computer science background can extend the system by defining such processing functions 5 1 future work future work may focus on issues related to semantic interoperability edam supports metadata annotation of observables using ontologies while these annotations are stored in the system they are not fully utilized in its current version edam lacks a semantic layer to act upon datasets and templates in principle a dataset that was acquired through an input template can be transformed with another template only if both templates utilize the very same observable ids the observable ids are drafted by data curators and represent certain observables future work may investigate the use of a reasoner to resolve relations between the different observable ids in this way a certain data file format can be represented through a single template and by assigning synonym terms in an ontology we could enable automatic transformation into other formats another direction for future work is to support environmental timeseries datasets in other formats in this work we evaluated edam against text based documents and relational databases however environmental datasets are also available in data cubes and non relational databases edam could be extended to support such other sources 5 2 conclusions in this work we provided a proof of concept and a tested implementation of a template system that can be used for environmental timeseries acquisition and integration we demonstrated that the use of templates for data acquisition in the environmental internet of things provides a compromise between generality and complexity we designed and implemented an open source extensible template framework called edam to support environmental timeseries data acquisition integration and dissemination services without the prerequisite of a strong computer science background we enable users to extract datasets and create custom views out of them by defining the desired output format as a template in this way users can re use environmental timeseries data into scientific workflows edam also supports opening legacy datasets as services on the web through ogc sos currently edam supports data acquisition and integration of timeseries stored in relational databases files in folder structures and webpages the test cases we used to evaluate edam provided us with insights about its general purpose nature the novelty of this approach is that we are not trying to propose another standard but rather that we have developed a specific language for describing data file structures in a generic way using templates also such templates are programming language agnostic so that users of different computer literacy profiles could develop them software availability name of software edam environmental data acquisition module developers argyrios samourkasidis evangelia papoutsoglou ioannis n athanasiadis contact argysamo gmail com software required any operating system with python 3 program language python 3 license gnu general public license software availability released via python s pip package management system source code on https github com bigdatawur edam acknowledgements we would like to offer our special thanks to mr stavros foteinopoulos for his valuable and constructive suggestions during the design and implementation of the software we are grateful to dr robert argent and dr andre zerger from the australian bureau of meteorology for providing us with the historical air quality database we would also like to express our gratitude to dr ming yi tsai and dr mark davey from the swiss tropical and public health institute for providing us with the air quality dataset and case study ia was partially supported by the wageningen university and research strategic investment theme programme resilience finally we are grateful to dr s osinga and the three anonymous reviewers for their constructive feedback and comments appendix a supplementary data the following is the supplementary data to this article multimedia component multimedia component appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 009 
26224,urban greenery such as trees can effectively reduce air pollution in a natural and eco friendly way however how to spatially locate and arrange greenery in an optimal way remains as a challenging task we developed an agent based model of air pollution dynamics to support the optimal allocation and configuration of tree clusters in a city the pareto optimal solutions for greenery in the city were computed using the suggested heuristic optimisation algorithm considering the complex absorptive diffusive interactions between agent trees tree clusters and air pollutants produced by agent enterprises factories and agent vehicles car clusters located in the city we applied and tested the model with empirical data in yerevan armenia and successfully found the optimal strategy under the budget constraint planting various types of trees around kindergartens and emission sources keywords air pollution dynamics agent based modelling evolutionary algorithms simulation of complex environmental systems greenery armenia 1 introduction as is known many countries and cities currently face the problem of increasing air pollution caused by industrial activities and the increasing number of vehicles previous research e g brunekreef and holgate 2002 has proven the strictly negative influence of air pollutants organic and inorganic dust carbon monoxide co sulphur dioxide so2 nitrogen oxides nox volatile organic compounds vocs heavy metals etc on human health air pollution mainly affects respiratory diseases such as asthma allergy chronical obstructive pulmonary diseases lung cancer cardiovascular diseases and skin diseases in addition infants and children are especially vulnerable to the influence of air pollution there are known approaches to reducing air pollution using natural based solutions nsb in particular urban greenery e g nesshöver et al 2017 among research concerning this topic can be highlighted a work fuiii et al 2005 which investigated air pollutant removal by plant absorption and adsorption in japan this research yielded the important conclusion that the efficiency of plant absorption depends on the photosynthesis actively performed in the spring and summer thus the contribution of nbs to air pollution removal can be significantly improved in countries having better climate characteristics e g a higher number of sunny days during the year as in the republic of armenia having approximately 300 sunny days per year in the city of yerevan thus the internal chemical processes of absorption and adsorption of different kinds of air pollutants by different kinds of plants have been very well investigated on the micro level e g fuiii et al 2005 omasa et al 2002 bell and treshow 2002 the results raise the flowing questions what is the positive impact of urban greenery in reducing air pollution how should the best kinds of trees be chosen and allocated in a city to protect the human population and which greenery strategies are better taking into account limited urban budgets solving such problems are impossible without modelling air pollution dynamics and forecasting the movement of air pollutants in the urban atmosphere there are three main groups of models that can be used to model air pollution dynamics the first is based on a statistical approach e g bolzern et al 1982 harnandez et al 1992 and intended mostly for analysis of historical data on pollutant concentrations and short term forecasting the second group consists of deterministic and deterministic statistical models e g lamb and seinfeld 1973 genikhovich 2004 pure statistical approaches to the investigation of air pollution dynamics are mainly simplified and their applications are limited the main reason for the limitation is that they ignore many factors with important influences on the dynamics of air pollution concentration such as plants located on paths of pollutant masses such plants are natural barriers implementing absorption effects some environmental factors e g changing wind directions and collusions between trees and landscape objects cannot be taken into account using statistical methods the last group of methods is based on modelling the dynamics of emission plumes using gaussian dispersion models e g turner 1994 beychok 2005 and are intended for the investigation of air diffusion processes taking into account different climate factors such models are the most realistic but they are characterised by significant computation complexity however such models are not always capable of representing the roles of different sources and sinks of air pollution such as cars factories and trees along with the multiple interactions between them specifically agent based models can provide a better understanding of the role of trees as sinks of air pollution than provided by diffusion reaction models and thus support the implementation of informed public policies to reduce pollution effects there are other well known and simpler models of air pollution dynamics such as lagrangian models eulerian models chemically reactive compound models and analytical models e g seinfeld 1975 zak 1983 seinfeld 1986 barbulescu and barbes 2017 ridley 2017 corani and scanagatta 2016 such models focus on forecasting trajectories of moving air pollutants without taking into account the impact of interaction with other landscape objects e g trees buildings nevertheless some studies have been devoted to researching air pollution dynamics taking into account some elements of the urban landscape e g turner 1964 finzi and tebaldi 1982 santiago et al 2017 in contrast the agent based simulation approach suggested here for modelling air pollution dynamics involving interaction with landscape objects particularly with tree clusters allows the dimensionality of the considered problem to be reduced significantly this ensures that the advantages of deterministic models are maintained the idea of combining computational air fluid dynamics abbot and basco 1989 and agent based modelling has already been suggested elsewhere epstein et al 2011 however the agent based model of air pollution dynamics using interactions with tree clusters having individual characteristics such as the geometry of planting e g simple circle arithmetic spiral double circle the type of tree e g popular oak maple and the distance between the nearest tree clusters is suggested herein for the first time this method aims to reduce the air pollution concentration in cities many studies have confirmed the positive impact of greenery in reducing air pollution in urban areas e g jim and chen 2008 wong et al 2009 however there are not any systems that determine the optimal number of tree clusters their location coordinates and planting geometry the best kind of trees and other parameters that could be computed to minimise the average daily concentration of air pollution if the urban greenery budget is limited this omission has been due to the overly high computational complexity needed for such tasks which involve large scale multi objective optimisation problems to achieve this goal we have combined computational air pollution dynamics with simulations of the ecological behaviour of different urban agents such as agent enterprises factories agent vehicles car clusters and agent trees tree clusters moreover we have used a special multi objective genetic algorithm akopov and hevencev 2013 aggregated with the developed simulation through objective functions to determine the pareto optimal solutions for greenery in a city there is a line of research to develop agent based models and multi objective systems to enhance the management of complex environmental systems hadka and reed 2015 sun et al 2016 tesfatsion et al 2017 west et al 2018 in the suggested model trees and air pollutants are considered interactional agents with individual characteristics this means that different tree clusters interacting with heterogeneous air pollutants have dissimilar absorptive diffusive characteristics and different influences on the daily air pollution concentration agent based modelling approaches in combination with other simulation methods have been described elsewhere papaleonidas and iliadis 2012 letcher et al 2013 zenonos et al 2015 vallejo et al 2015 ridley 2017 akopov et al 2017 these studies have identified certain associated advantages for ecological modelling the most important of which is the possibility of modelling the dynamics of agent interactions without the need to develop complex analytical models this study is focused on designing an agent based model to determine the best ecological trade offs for urban greenery the case study of the city of yerevan armenia will be presented the main purpose behind the development of the original decision making system was to analyse effective scenarios for greenery allocation under the budget constraint in the city of yerevan armenia to reduce the average daily pollution concentration the suggested system can be applied to identify optimal greenery strategies to reduce air pollution concentrations in other urban areas and regions if appropriate data are available in addition the developed approach improves the precision of the model by including additional characteristics such as the prevailing wind direction wind velocity air pollution intensities generated by agent enterprises and agent vehicles 2 data and method 2 1 study area the city of yerevan armenia has many features that should be taken into account for simulation development yerevan has an average height of 990 m 3248 03 ft with a minimum of 865 m 2837 93 ft and a maximum of 1390 m 4560 37 ft above sea level it is located on to the edge of the hrazdan river in the northeast of the ararat plain ararat valley the important aspect of the location of yerevan is that the city is surrounded by the caucasian mountains the mountains significantly restrict the possibility of simple relocation of enterprises that are the main stationary sources of air pollution outside the city another problem is the hard restriction of the budget that can be used for environmental activities caused by the difficult economic state of armenia these factors require the use of natural based solutions and relatively low cost methods for reducing the air pollution concentration such as the greenery allocation in the city the following types of agents are used in the model agent enterprises and agent vehicles car clusters which are sources of emissions in the city agent trees which consist of small groups of homogeneous closely located trees and agent emissions which are air pollutants produced by agent enterprises and agent vehicles in addition concepts of the personal space of an agent tree and the personal space of an agent emission are suggested the personal space of an agent tree is defined by the crones of plants forming the group fig 1 the personal space of agent trees consists of closely located separate homogenous trees several closely located agent trees with a specific planting geometry are considered a tree cluster one agent tree consists of 10 trees the crown of a plant refers to the totality of an individual plant s aboveground parts including stems leaves and reproductive structures a plant community canopy consists of one or more plant crowns growing in a given area therefore it can be considered as a tree cluster the personal space of an agent emission is defined by its radius length in the model all agent emissions are formed by masses of homogenous air pollutants and radiuses of agent emissions define areas having persistent concentrations of homogenous air pollutants to simplify the concentration of air pollutants belonging to any agent emission are assumed to be proportional to the agent emission radius length the radius length of the agent emission is reduced when the agent emission is touched by some agent tree fig 2 thus the absorption effect is implemented nevertheless agent emissions with radiuses significantly less than the radiuses of agent trees will pass through thus the diffusion effect is implemented the behaviour of such agents is based on natural observations taken in the city yerevan armenia all experiments were conducted in selected urban areas with agent trees consisting of separate trees with fixed distances between each other further emissions of different volumes of dust occurred and a special dust detector was used for estimation of the associated air pollution concentration in protected urban areas the ratio between the emitted dust volume which defines the initial radius of the agent emission and the size of the tree cluster which defines the radius of the agent tree when the agent trees do not protect from air pollutants was successfully identified such measurements were taken for different kinds of pollutants using different kinds of trees depending on the number of agent trees their radiuses the kinds of trees and other characteristics the rate of change in the air pollution concentration from the point of the emission source to the point of the protected urban area was found such absorption and diffusion effects have other known justifications e g fuiii et al 2005 omasa et al 2002 there are following agent emissions are considered in the model organic dust inorganic dust heavy metals carbon monoxide co sulphur dioxide so2 nitrogen oxides nox carbonates without volatile organic compounds volatile organic compounds vocs and other emissions the finite state of agent emissions caused by their life span is dissipation when the agent emission do not have a radius in the model the radiuses of undissipated agent emissions are summarised at the monitoring station or in protected urban areas and are used for estimating the average daily air pollution concentration the intersection of different agent emissions in air space is simulated without mixing their pollutants fig 2 this assumption allows for the modelling of the daily pollution concentrations of different air pollutants in the areas of appropriate agent emissions these areas are reduced quickly in the presence of multiple contacts with agent trees as well as slowly as a consequence of natural dissipation over time the important characteristic of the described model is the daily concentration of air pollution which can be estimated as the sum of the areas of the circles of all j i v t h s agent emissions located at the fixed zone of ecological monitoring the monitoring station with known coordinates x y and r is the radius of observation here j i v j i v is the set of indexes of agent emissions produced by the i t h emission source that contains the v t h air pollutant a homogenous mass the main justification for using such a simplified approach for estimation of the daily air pollution concentration is that the initial radiuses of agent emissions were chosen proportionately to appropriate volumes of pollutants produced by each agent enterprise and agent vehicle the current values of the radiuses of agent emissions located in the monitoring zone depend on many factors in particular all agent trees that they have previously touched thus the sum of the areas of the circles of agent emissions can be used for model estimation of air pollution as the most representative metric in this model the air pollution concentration of the j i v t h agent emission consisting of the v t h pollutant in the zone of the monitoring station with the radius r at the time t t t is 1 g j i v t π r j i v 2 t if x x j i v t 2 y y j i v t 2 r and s t j i v t 1 0 if x x j i v t 2 y y j i v t 2 r i i v v j i v j i v where t t 0 t 1 t t is the set of temporal intervals and the set is in days of a one year period t is 365 days i i 1 i 2 i i is the set of indexes of emission sources that produce air pollutants agent enterprises and agent vehicles located in the city i is the number of emission sources v v 1 v 2 v v is the set of indexes of air pollutants e g co2 organic dust inorganic dust heavy metals so2 nox v is the number of air pollutants j i v j 1 i v j 2 i v j j i v t is the set of indexes of agent emissions produced by the i t h emission source that contains the v t h air pollutant a homogenous mass j i v is the number of agent emissions at the moment t t x j i v t y j i v t are coordinates of agent emissions in the wgs 84 the world geodetic system coordinate system at the moment t t r j i v t is the radius of the j i v t h agent emission at the moment t t which changes as a result of interaction of the agent emission with any k ζ t h agent tree reflected in fig 2 and r j i v τ j i v 1 10 is the known initial radius of the j i v t h agent emission defined at the moments of air pollution production by different sources of emissions x y are known coordinates of the monitoring station in the city r is the radius of observation of the monitoring station h j i v t 0 u i t 0 2 u i t u i u i is the set of regular time moments of producing the j i v t h agent emission u i is the emissions intensity of the i t h source u i 7 for all agent enterprises and u i 2 for all agent vehicles and s t j i v t 1 0 is the state of the j i v t h agent emission wherein s t j i v t 1 if the j i v t h agent emission is active s t j i v t 0 if the j i v t h agent emission is dissipated and t j i v is the life time of the j i v t h agent emission 2 s t j i v t 1 if t τ j i v t j i v 0 if t τ j i v t j i v τ j i v h j i v i i v v j i v j i v the daily air pollution concentration summarised for all agent emissions and averaged for all associated pollutants that have reached the monitoring zone is 3 d c t 1 v i 1 i v 1 v κ v j i v 1 j i v t g j i v t i i v v j i v j i v t t here ϖ v is the weight coefficient reflecting the importance of the v t h pollutant for a decision maker v 1 v κ v 1 0 κ v using the weight coefficients κ v v v takes into account the importance for a decision maker of different air pollutants in the problem of air pollution minimisation the daily air pollution concentration should not exceed the maximum permissible concentration which is fixed individually for each air pollutant including dust heavy metals sulphur dioxide so2 nitrogen oxides nox carbonates and volatile organic compounds vocs the maximum permissible concentration is aggregated and denoted as d c in the model d c t d c the average daily air pollution concentration is 4 a d c 1 t t t 0 t d c t 2 2 data detailed data including datasets of agent enterprises agent vehicles and agent trees as well as locations of emission sources and protected kindergartens are presented in the data in brief paper akopov et al 2019 the number of agent emissions and their initial radiuses are directly proportional to the total air pollution produced by agent enterprises the main characteristics of agent trees are presented in tables 1 and 2 the value of the absorption coefficient is calculated by considering the estimated value of the maximum volume of absorption by one tree and the total number of trees included in one tree cluster the main characteristics of agent vehicles car clusters are presented in table 3 and in data in brief there are approximately 300000 cars in yerevan which form 1000 car clusters the total emissions produced per day by one agent vehicle consisting of 300 cars is approximately 3 8 tonnes one agent emission has an initial radius of 10 m deposits and 1 9 tonnes of total pollutants mainly co2 thus for one event occurring with a fixed intensity of 2 one time for two days taking into account parking a total of 3 8 2 1 9 4 agent emissions are produced for one event by one agent vehicle table 3 the initial radius of agent emissions produced by an agent vehicle was obtained from the results of air pollution measurements near roads and the measurement reflects areas with maximum concentrations of pollutants wind characteristics such as the prevailing wind direction and wind speed variation by month have an important influence on air pollution dynamics table 4 here an 8 wind compass rose is used with the four ordinal directions forming the bisecting angle of the cardinal winds north n west w northeast ne south s southeast se southwest sw northwest nw all data were obtained from the main weather station of yerevan armenia here the prevailing wind direction is used for correction of the movement directions of agent emissions when there are not any agent trees along the movement paths or there is a diffusion effect in the presence of obstacles such as agent trees the prevailing wind direction disappears and the movement directions of agent emissions are defined by interactions between agent emissions and agent trees there are different methods available for collecting air quality monitoring data as described in previous works papaleonidas and iliadis 2012 zenonos et al 2015 in this research the standard approach was applied based on reading real data using equipment such as gas analysers dust analysers sensors and spectroscopes and comparing the real data against the simulation results samples were collected and treated at the central analytic laboratory of the centre of ecological noosphere studies of the national academy of sciences of armenia cens accredited by iso iec 17025 the laboratory has modern equipment including a serinus 30 carbon monoxide analyser serinus 40 analyser serinus 40 nitrogen oxides analyser serinus 51 sulphur dioxide analyser voc72 m gas chromatography volatile organic compounds btex analyser and dust analyser opastop gp4000hd the collected samples were treated and analysed for air pollutant contents dust co nox so2 heavy metals vocs through the atomic absorption method aanalyst 800 perkin elmer us iso 8573 and iso 12500 air quality 2 3 method the developed simulation model employs the earlier developed agent based approach of modelling ecological economic systems suggested in a previous study akopov et al 2017 in this work agents are agent enterprises that produce air pollution and a method to find the best trade offs for ecological modernisation of enterprises towards pure ecological manufacturing is developed however such an approach assumes reduction of the production capacity of enterprises and appropriate outputs together with emissions in the atmosphere furthermore ecological modernisation requires significant expenditures in contrast developing urban greenery is an alternative and relatively inexpensive approach to reducing the air pollution concentration first the idea of using agent emissions instead of a more natural emissions plume should be explained as is known modelling the dynamics of an emissions plume requires more complex models based on differential equations with partial derivatives that describe appropriate chemical processes on the molecular level beychok 2005 turner 1994 the explicit advantage of such models is the possibility of investigating internal air dispersion and diffusion processes that depend on many factors such as wind direction temperature and air humidity therefore these models are the most realistic and accurate however such models are characterised by significant computation complexity therefore their application is justified for local and limited systems without taking into account complex interactions between trees and air pollutants on the scale of a whole city employing different strategies of urban greenery on the other hand emission plumes can be considered ensembles consisting of separate agent emissions controlled by external factors on the individual level e g the prevailing wind direction and interactions with plants if the resulting precision will be enough to model the air pollution dynamics because modelling atmospheric dispersion is not critical for investigating the influence of greenery on pollution removal using agent emissions instead of emission plumes is justified there is a known concept of agent states in agent based modelling each agent can have a set of individual characteristics that are called agent states e g the agent emission can be active or dissipated the agent enterprise can be non ecological and not surrounded by trees or ecological there are transitions between such states that can be controlled by special rules or can be under natural temporal control the scheme of possible states of the considered agents is presented in fig 3 the model starts with the initial states of agents which are highlighted in grey in fig 3 the initial conditions of the model are defined by the locations of the agent enterprises and agent vehicles in the city according to known coordinates for enterprises and stochastic coordinates of roads for vehicles the next step is the distribution of new agent trees with maximum initial radiuses of personal space and absorption characteristics within the city according to the defined optimal tree cluster configurations some states of agent trees agent vehicles and agent emissions are interconnected in particular new agent emissions with maximum initial radiuses are generated by agent enterprises and agent vehicles when they are in their second and third states of producing air pollution the production cycles of air pollution by agent enterprises and agent vehicles are defined by the emission intensities usually equal to seven days for agent enterprises and two days for agent vehicles further the abstract mathematical description of the suggested model is considered as well as multi criteria optimisation problems for rational greenery allocation in the city to minimise the air pollution concentration where z ζ 1 ζ 2 ζ z is the set of indexes of tree clusters where agent trees are located k ζ k 1 ζ k 2 ζ k k ζ is the set of indexes of agent trees that absorb air pollutants belonging to the ζ t h tree cluster s j i v t is the speed of movement of the j i v t h agent emission which equals the average wind speed at the moment t t α j i v t is the angle of movement of the j i v t h agent emission j i v j i v when there are not any agent tree on its path or if the diffusion effect takes place at the moment t t this angle takes into account the influence of the prevailing wind direction table 5 β j i v k ξ t is the angle of bypass of the j i v t h agent emission j i v j i v around the k ζ t h agent tree k ζ k ζ at the moment t t γ j i v k t is the angle of the rebound of the j i v t h agent emission j i v j i v from the k ζ t h agent tree k ζ k ζ at the moment t t с j i v t is the rebounding coefficient of the j i v t h agent emission from the k ζ t h agent tree с j i v t 10 for all j i v j i v r k ξ t is the radius of the k ζ t h agent tree depending on tree kind e g poplar oak maple d i s t j i v k ξ t is the euclidean distance between the j i v t h agent emission and the k ζ t h agent tree ξ j i v k ζ t is the event when the j i v t h agent emission touches the k ζ t h agent tree 5 ξ j i v k ζ t 1 if d i s t j i v k ζ t r j i v t r k ζ t 0 if d i s t j i v k ζ t r j i v t r k ζ t i i v v j i v j i v ζ ζ k ζ k ζ in the absence of a connection between the j i v t h agent emission j i v j i v i i v v and the k ζ t h agent tree k ζ k ζ ζ z the radius of the former will not be changed fig 4 an appropriate interaction can take place if any agent tree is located on the track of any agent emission and the radius of the latter is not much smaller than the radius of the agent tree that has been touched this case can be considered the absorption effect after an event in which an agent emission has been touched by some agent tree the radius of the former will be significantly reduced in this case the agent emission can pass through any agent tree this case can be considered the diffusion effect thus the radius of the j i v t h agent emission at moment t t t is defined by the same recursive procedure 6 r j i v t r j i v τ j i if i is true r j i v t 1 exp η j i v if ii is true r j i v t 1 exp η j i v μ k ζ j i v if iii is true 0 if iv is true where i t τ j i v τ j i v h j i v ii ξ j i v k t 0 or r j i v t r k ζ t ϖ and s t j i v t 1 iii ξ j i v k t 0 and r j i v t r k ζ t ϖ and s t j i v t 1 iv s t j i v t 0 i i v v j i v j i v ζ ζ k ζ k ζ where μ k ζ j i v is the fixed coefficient reflecting the absorption effect of the k ζ t h agent tree regarding the j i v t h agent emission containing the v t h pollutant μ k ζ j i v 1 η j i v is the fixed coefficient reflecting the exponential fading of the j i v t h agent emission η j i v 0 01 and ϖ is the fixed coefficient reflecting the diffusion effect ϖ 10 agent emissions move towards different directions under the influence of applied forces caused by the wind velocity and interactions with different landscape objects in particular agent trees such movement of masses of air pollutant types can be described by well known differential equations with variable structures thus the dynamics of the j i v t h agent emission for the time period of t r ψ t r 1 are described by the following system of differential equations at time 7 d x j i v ψ d ψ s j i v ψ cos α j i v ψ if v is true s j i v ψ cos β j i v k ζ ψ c j i v d i s t j i v k ζ cos γ j i v ψ if vi is true 0 if vii is true 8 d y j i v ψ d ψ s j i v ψ sin α j i v ψ if v is true s j i v ψ sin β j i v k ζ ψ c j i v d i s t j i v k ζ sin γ j i v ψ if vi is true 0 if vii is true i i v v j i v j i v ζ ζ k ζ k ζ where v d i s t j i v k ζ ψ r j i v ψ r k ζ ψ for all k ζ k ζ or r j i v ψ r k ζ ψ ϖ for the nearest k ζ k ζ and s t j i v ψ 1 vi d i s t j i v k ζ ψ r j i v ψ r k ζ ψ for the nearest k ζ k ζ and r j i v ψ r k ζ ψ ϖ for the nearest k ζ k ζ and s t j i v ψ 1 vii d i s t j i v k ζ ψ r j i v ψ r k ζ ψ and r j i v ψ r k ζ ψ ϖ for all k ζ k ζ or s t j i v ψ 0 the system of eqs 7 and 8 is similar in some elements to the model of crowd behaviour described in previous works akopov and beklaryan 2015 beklaryan and akopov 2016 this is due to agent emissions colliding with agent trees and taking on some characteristics of crowds such as chaotic movement and targeting to bypass landscape obstacles the main approach to minimisation of the air pollution concentration is the control of parameters related to tree cluster configurations on the individual level the control parameters for each ζ t h tree cluster ζ ζ consisting of the k ζ t h s agent trees k ζ k ζ are defined at the initial moment t 0 t as x ˆ ζ t 0 y ˆ ζ t 0 are the coordinates of the ζ t h tree cluster centre for simplification these coordinates are equal to the coordinates of agent enterprises and protected urban areas c ζ 0 1 4 is the set of possible configurations of the ζ t h tree cluster and c n f ζ t 0 c ζ is the selected configuration c n f ζ t 0 0 without agent trees c n f ζ t 0 1 simple circle c n f ζ t 0 2 arithmetic spiral with the fixed step c n f ζ t 0 3 double circle c n f ζ t 0 4 double circle with variable distances between the nearest agent trees considering the ζ t h tree cluster and c n f ζ t 0 c ζ s ζ 1 2 5 is the set of possible kinds of agent trees located in the tree cluster and t p ζ t 0 s ζ is the selected kind of agent trees t p ζ t 0 1 poplar tree cluster t p ζ t 0 2 oak tree cluster t p ζ t 0 3 maple tree cluster t p ζ t 0 4 pine tree cluster t p ζ t 0 5 ulmus tree cluster δ ζ t 0 is the distance between the nearest agent trees and the ζ t h tree cluster δ δ ζ t δ ζ ζ where δ δ are the lower and upper boundaries of the distance range and r ζ t 0 is the radius of the ζ t h tree cluster that defines the planting area r r ζ t 0 r ζ z where r r are the lower and upper boundaries of the radius range the target number of agent trees to be planted around agent enterprises for reducing the air pollution concentration can be found by solving the bi objective optimisation problem which will be considered later 9 k ζ t 0 f c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 ζ ζ t 0 t the needed greenery budget is 10 g b ζ 1 ζ k ζ k ζ c k ζ n ζ ζ ζ k ζ k ζ where c k ζ is the known cost of planting one tree of the k ζ t h agent tree of the ζ t h tree cluster and n ζ is the number of closely located trees to one agent tree of the ζ t h tree cluster n 10 for all ζ ζ here the cost of planting depends on the tree kind e g 900 usd for a poplar 700 usd for an oak and it includes all expenses related to the support of the tree life cycle during a year e g irrigation digging the soil the number of planted agent trees in all ζ t h s tree clusters ζ z is 11 n ζ 1 ζ k ζ problem a the need to minimise the average daily pollution concentration and greenery budget through the set of control parameters c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 12 min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 a d c min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 g b where a d c 1 t t t 0 t d c t g b ζ 1 ζ k ζ k ζ c k ζ n ζ d c t 1 v i 1 i v 1 v κ v j i v 1 j i v t g j i v t d c t d c v 1 v t κ v 1 0 κ v δ δ ζ t 0 δ r r ζ t 0 r c n f ζ t 0 c ζ t p ζ t 0 s ζ ζ ζ k ζ k ζ i i v v j i v j i v t 0 t under different other constraints further some modifications of the problem of minimising the average daily pollution concentration will be considered there is an important problem of protecting selected urban areas with a significant social relevance e g kindergartens and hospitals from harmful emissions produced by agent enterprises and agent vehicles a strategy considering this issues can be useful in conditions of greenery budget deficit and a lack of free areas acceptable for planting each kindergarten has its own personal space that should be protected from air pollutants the problem will be considered for the protection of kindergartens of the city yerevan armenia as an example here p p 1 p 2 p p is the set of indexes of kindergartens located in the city that should be protected and p is the total number of kindergartens x p y p are the coordinates of the p t h kindergarten and r ˆ p is the known radius of the p t h kindergarten defined by the personal space where the average daily pollution concentration should be minimized the air pollution concentration of the j i v t h agent emission consisting of the v t h pollutant in the p t h kindergarten with radius r ˆ p at time t t t is 13 g j i v p t π r j i v 2 t if x j i v t x p 2 y j i v t y p 2 r ˆ p and s t j i t 1 0 if x j i v t x p 2 y j i v t y p 2 r ˆ p i i v v j i v j i v p p the daily air pollution concentration summarised by all agent emissions and averaged by protected urban areas kindergartens is 14 d c t 1 p v p 1 p i 1 i v 1 v κ v j i v 1 j i v t g j i v p t the daily air pollution concentration should not exceed the maximum permissible concentration which is fixed individually for each air pollutant including dust heavy metals sulphur dioxide so2 nitrogen oxides nox carbonates and volatile organic compounds voc the maximum permissible concentration will be aggregated and denoted as d c in the model d c t d c the average daily air pollution concentration in protected kindergartens is 15 a d c 1 t t t 0 t d c t the number of planted agent trees needed for protection of the kindergartens is 16 n ζ 1 ζ k ζ ζ ζ where k ζ is the target number of agent trees to be planted around kindergartens the number of tree clusters planted around protected kindergartens k ζ can significantly differ from the number of tree clusters planted around agent enterprises k ζ due to differences in location the sizes of their areas remoteness from emissions sources and the accumulations of air pollutants produced by many agent enterprises and agent vehicles problem b the need to minimise the average daily pollution concentration and the greenery budget in protected urban areas kindergartens through the set of decision variables c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 17 min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 a d c min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 g b where a d c 1 t t t 0 t d c t g b ζ 1 ζ k ζ k ζ c k ζ n ζ d c t 1 p v p 1 p i 1 i v 1 v κ v j i v 1 j i v t g j i v p t d c t d c v 1 v κ v 1 0 κ v δ δ ζ t 0 δ r r ζ t 0 r c n f ζ t 0 c ζ t p ζ t 0 s ζ ζ ζ k ζ k ζ i i v v j i v j i v p p t 0 t the model eqs 1 17 was implemented in the well known anylogic simulation tool that supports agent based modelling methods with gis integration original java classes were developed for implementation of the individual behaviour rules of all agents including agent trees and agent emissions as a result over 12 000 replicated agents can be simulated in the system to calculate values of the objective functions eq 12 17 further the simulation was aggregated with suggested genetic algorithms through these objective functions to seek the pareto optimal solutions a line of the heuristic optimisation algorithms can be applied for the considered bi objective optimisation problem e g zitzler and thiele 1999 kim et al 2004 zhong et al 2004 among them the spea strength pareto evolutionary algorithms class proposed by zetzler zitzler and thiele 1999 and developed in previous works akopov and hevencev 2013 kim et al 2004 e zaenudin and kistijantoro 2016 can be highlighted using heuristic optimisation algorithms is effective for solving large scale optimisation problems of ecological economic systems e g akopov et al 2017 vallejo et al 2015 the magamo multi agent genetic algorithm for multi objective optimisation algorithm based on some aspects of previous work zhong et al 2004 and suggested in a previous study akopov and hevencev 2013 was used for solving the considered problem 17 the magamo paralleling of evolutionary processes has some advantages when objective functions are the result of agent based simulations the important feature of magamo is the possibility to organise the heuristic optimisation in the parallel mode where each process is responsible for the evolution of previously selected decision variables the decision variables in the set are separated into subsets associated with the corresponding genetic algorithms ga gas are exchanged by the best values of the decision variables within the evolutionary search for the considered optimisation problem this means that each evolving tree cluster consisting of agent trees and with its own characteristics will be associated with one independent genetic algorithm this approach allows reduction of the population size of each ga and improvement of the time efficiency and rate of convergence of the global heuristic search there are two optimisation functions for solving problem a and problem b the average daily pollution concentration a d c and a d c and the greenery budget g b and g b these criteria were chosen because they reflect the interests of potential decision makers aiming to seek appropriate trade offs in addition these objective functions are the most important for the city of yerevan having limited financial resources and problems with air quality in the developed model there are multiple interactions between agent emissions and agent trees that are complicating factors however using ordinary differential equations with a variable structure eq 7 8 for modelling the dynamics of air pollutants interconnected with agent trees allows significant simplification of the computation procedure and estimation of the objective functions eq 12 17 when using emission plumes instead of agent emissions the model can be more realistic but the computation process could be complicated because there is a need to solve more complex systems of differential equations describing appropriate dispersion processes for each tree cluster connected with air pollution over continuous time within the considered problem the computed configurations of tree clusters are described by the following set of decision variables c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 where the pair c n f ζ t 0 t p ζ t 0 are discrete variables and the pair δ ζ t 0 r ζ t 0 are continuous variables the genetic algorithm magamo is responsible for control of the tree cluster configurations on the individual level configuration samples of the suggested decision variables are presented in fig 5 the optimisation of tree cluster configurations e g the geometry of planting is based on the control of agent tree locations and their characteristics instead of separate trees to simplify the problem the values of the main parameters of the genetic optimisation algorithm are presented in table 6 in the model 83 tree clusters were created around the main sources of emissions agent enterprises located in yerevan to solve problem a solving the considered optimisation problems involves significant computational complexity due to the calculation time of the objective functions being significant over 10 min for each simulation run this is caused by the complex interaction mechanism between agent trees and agent emissions described by the system of differential equations with variable structures the main steps of the heuristic optimisation of tree cluster configurations are the following step 0 code chromosomes to represent the decision variable values with following distribution between all agent processes of the ga responsible for the evolution of personal tree clusters step 1 form the initial population of solutions with estimations of fitness functions which are the results of the simulation eqs 1 17 create the initial global archive of non dominated solutions for all tree clusters for each agent process the following evolutionary steps should be completed step 2 select parents from the archive of non dominated solutions with the highest pareto ranking and located sufficiently remote from each other to avoid inbreeding step 3 perform the crossover and mutation to generate offspring step 4 run the simulation eqs 1 17 to obtain values of fitness functions taking into account all constraints reject the solution without waiting for the end of the simulation process if any constraint has been exceeded step 5 send the best decisions for personal tree clusters from the local population to the global archive of non dominated solutions with an estimation of the pareto ranking retrieve the best decisions for other tree clusters from the global archive of non dominated solutions step 6 if the minimum convergence of ga and the minimum needed number of pareto optimal solutions in the global archive are not reached repeat steps 2 5 2 4 software the simulation model was developed using the anylogic tool aggregated with other components such as the database and genetic algorithm at the current time the software has not been shared for global users and it is implemented as the regional decision making system for yerevan armenia the model is available online at http www runmycode org companion view 3420 the software allows conduction of multiple optimisation experiments using real data for different scenarios of urban greenery for reducing the daily concentration of air pollution the model consists of two levels the first is the top level dashboard used for the setup of control parameters fig 6 as shown in fig 6 a decision maker can set up values of the control parameters such as the configuration of tree clusters both in a united configuration for all agent enterprises that should be surrounded by agent trees and in individual configurations of tree clusters previously computed using the genetic optimisation algorithm the magamo with selection of optimal scenarios on the pareto front an illustration of the air pollution dynamics in the city of yerevan during the first seven days after emissions is shown in fig 7 in more detail here agent emissions are denoted by black points and agent trees are denoted by yellow red and green circles depending on agent tree kind as can been seen from fig 7 tree clusters are natural barriers for agent emissions at least 13 546 agents are simulated in the system which include agent emissions agent trees agent enterprises and agent vehicles 3 results and model validation 3 1 results simulation experiments consisted of two parts the first block is related to solving problem a i e the minimisation of the average daily pollution concentration and the greenery budget through planting tree clusters around agent enterprises that are the main stationary sources of air pollutants the results of the simulation for the first problem are presented in figs 8 13 the pareto front computed with magamo for the considered optimisation problem a is shown in fig 8 at least 500 simulation runs with a thousand internal iterations of the magamo were completed to form the sustainable pareto front shown in fig 8 the quality of the pareto optimal decisions was estimated using different known methods such as the hypervolume metric and the space between decisions the area of the best decision was selected with which the decision maker can obtain acceptable reduction of the average daily pollution concentration in the range of 0 05 0 07 me m3 with the minimum greenery budget 2 95 6 32 musd equal values of weight coefficients reflecting the importance of each air pollutant for a decision maker were used the results obtained for problem a using the genetic algorithm show that over 20 pareto optimal solutions were obtained and 3 scenarios were selected as the final decisions scenario 1 expensive complex greenery this scenario assumed forming tree clusters planting around agent enterprises using mainly complex configurations such as regular circles double circles and double circles with variable distances between agent trees which require many trees and a significant greenery budget the considered trees are mainly poplar and oak additionally the configurations of tree clusters are different for each agent enterprise the maximum effect on reducing the daily pollution concentration is expected scenario 2 compromise greenery this scenario assumed using mainly individual configurations of tree clusters such as simple circles and arithmetic spirals which require an average number of agent trees and a more preferable size of the greenery budget the considered trees are mainly maple and oak for each agent enterprise appendix a an acceptable and sustainable reduction of the daily pollution concentration is expected scenario 3 inexpensive greenery this scenario assumed using mainly individual configurations of tree clusters such as arithmetic spirals which require the fewest number of agent trees and the minimum possible size of the greenery budget the considered trees are mainly poplar for each agent enterprise an unsustainable reduction of the daily pollution concentration to less than the maximum permissible level of the air pollution concentration is expected table 7 shows the numbers of agent trees and separate trees and the amount of greenery budget g b defined by solving problem a using the suggested ga here the distance between the nearest agent trees consisting of 10 closely located trees is 200 m and the radius of planting zones is 250 m for all tree cluster configurations e g simple circle double circle table 8 shows the simulation results obtained using different configurations of tree clusters consisting of different tree kinds for problem a as seen in table 8 the individual configurations of tree clusters obtained for scenario 2 are the most preferable in conditions of greenery budget deficit the configurations of tree cluster planting around agent enterprises obtained for these scenarios are illustrated in figs 9 11 using a map of yerevan armenia here poplar tree clusters are denoted by green circles oak tree clusters are denoted by yellow circles maple tree clusters are denoted by red circles pine tree clusters are denoted by blue circles ulmus tree clusters are denoted by aquamarine circles and agent emissions are denoted by black points for the scenarios considered the dynamics of the daily air pollution concentration were computed and are shown in fig 12 as shown in fig 12 scenario 1 gives the greatest reduction in the daily air pollution concentration while maintaining a significant reserve compared to the maximum permissible concentration 0 1 me m3 scenario 2 is characterised by a smaller reduction of the daily air pollution concentration though the daily air pollution is less than the maximum permissible concentration on any day scenario 3 is flawed because sometimes the daily air pollution concentration reaches the maximum permissible concentration however it can be considered if there is only a small budget available for greenery the dynamics of the average daily air pollution concentration a d c are presented in fig 13 as shown in fig 13 the average daily air pollution concentration exceeds the maximum permissible concentration if there are not any tree clusters using the pareto optimal solutions scenarios allows significant reduction of the average daily air pollution concentration the main reason why tree clusters allow for the reduction of the air pollution concentration is the interaction of agent trees with agent emissions to cause the absorptive diffusive effect in addition tree clusters separate streams of air pollutants change their directions and block them in locations with emission sources the second block of simulation experiments is related to solving problem b i e minimisation of the average daily pollution concentration and the greenery budget through planting tree clusters around selected urban areas that should be protected from air pollutants using a case study of kindergartens in the city yerevan armenia there were 2 scenarios obtained for problem b scenario 4 this scenario assumed planting agent trees around kindergartens without greenery near emissions sources additionally individual configurations of tree clusters would be implemented for each kindergarten as regular circles double circles and double circles with variable distances such tree clusters mainly consist of poplar oak and ulmus scenario 5 this scenario assumed planting agent trees around kindergartens and agent enterprises at the same time thus the greenery budget would be separated between planting around emissions sources and protected urban areas individual configurations of tree clusters would be implemented for each kindergarten such tree clusters consist trees of different kinds of trees dominated by maple and oak in fig 14 is presented the first optimisation scenario scenario 4 where agent trees are planted around protected urban areas kindergartens without greenery near emission sources here mainly simple circles and arithmetic spirals are used additionally poplar oak and ulmus tree clusters dominate the considered scenario provides significant reduction of the average daily pollution concentration to a value comparable to that achieved by planting around agent enterprises the other approach includes planting agent trees around kindergartens and agent enterprises simultaneously this is the balanced and more preferable optimisation scenario table 9 shows the numbers of agent trees tree kinds and the greenery budget g b defined by solving problem b using the suggested ga table 10 shows the simulation results obtained using different configurations of tree clusters consisting of different tree kinds for problem b here the base distance between the nearest agent trees consisting of closely located trees is 60 m and the radius of planting zones is 120 m for all tree cluster configurations e g simple circle double circle in comparison with planting agent trees around agent enterprises only scenarios 1 3 a much greater number of trees and a higher greenery budget are needed to minimise the average daily pollution concentration scenario 4 the main reason that the protection of kindergartens from air pollutants requires more agent trees is that in the absence of agent trees around agent enterprises appropriate pollutants reach areas of kindergartens faster without absorption additionally unlike the central monitoring station many kindergartens are located near sources of emissions thus the best strategy is planting agent trees around kindergartens and agent enterprises at the same time scenario 5 in comparison with the strategy of planting around sources of emissions only discussed earlier scenario 5 is a more expensive but better approach for protection of the human population from air pollution the comparison of all considered scenarios is represented in table 11 as it can be seen as from table 11 the scenario 5 is more preferable than scenarios 2 3 because it provides the lower level of the average daily pollution concentration 0 045 me m3 with keeping the acceptable greenery budget 7 41 musd in addition the scenario 5 is better than scenario 4 because it needs the less greenery budget it seems the effect of scenario 5 is similar with the results of the scenario 1 expensive complex greenery however the average daily air pollution concentration is minimized in the observation zone of the monitoring station only in scenarios 1 3 in contrast the daily air pollution concentration is minimized in multiple protected urban areas near kindergartens in scenarios 4 5 hence the last two scenarios can be considered as the more live safer and the reliability the model was validated using real data the validation procedure was based on comparative analysis of the average daily air pollution concentration obtained through the developed simulation with real data collected in the selected monitoring station in yerevan which records daily air quality readings 3 2 model validation the developed simulation was validated using historical data recorded both in the zone of the monitoring station located in the centre of yerevan armenia scenario 2 and in protected kindergartens scenario 5 the forecasted errors are based on the following estimation using the known method of ordinary least squares the model error estimated in the area of the motoring station is 18 a d c a d c a d c 2 n n n 2 χ 1 the model error estimated in protected urban areas kindergartens is 19 a d c a d c a d c 2 n n n 2 χ 2 where a d c a d c n n are simulated and actual values of the average daily air pollution concentration in the area of the monitoring station at the centre of the city and the number of agent trees respectively a d c a d c n n are simulated and actual values of the average daily air pollution concentration in protected urban areas kindergartens and the number of agent trees respectively and χ 1 χ 2 are the upper limits of the model error 10 and 5 14 respectively aggregated results of the model validation for selected scenarios 2 and 5 with using actual data for 2017 are presented in table 12 these scenarios were implemented in practice in yerevan armenia by 2017 4 discussion and conclusion in this paper a new approach to modelling the air pollution dynamics in a city with use of the agent based model and heuristic optimisation was suggested the main feature of the developed method involves the use of agent trees interacting with agent emissions which describe the dynamics using a system of differential equations that consider absorptive diffusive effects in real ecological systems the research aims to solve the problem of optimal allocation and configuration of tree clusters in the city of yerevan armenia to minimise the average daily pollution concentration observed in defined areas as was demonstrated in the work there are two important optimisation problems in the minimisation of air pollution the first involves reducing the air pollution concentration over a whole city through planting agent trees around emission sources problem a some effective scenarios were computed for solving this problem tables 7 8 and figs 9 13 the second optimisation problem involves reducing the air pollution concentration in selected urban areas e g planting agent trees around kindergartens problem b two additional scenarios were computed for solving this problem tables 9 and 10 and fig 14 and one of the scenarios is the most preferable this scenario assumed planting agent trees around kindergartens and agent enterprises at the same time thus the greenery budget would be separated between planting around emissions sources and protected urban areas such a greenery strategy is more effective because it requires a lower number of agent trees while achieving low values of the average daily pollution concentration the important result of the research is the calculated greater efficiency of using combined configurations of tree clusters consisting of different kinds of trees and different geometries of planting e g simple circle and arithmetic spiral in comparison with using homogeneous configurations tables 8 and 10 this allows solving considered optimisation problems through the pareto optimal solutions to provide the best decisions for reducing air pollution through agent trees which cannot be achieved by other means thus using the case study of the city of yerevan in armenia the agent based model allowed the pareto optimal solution to be determined using a genetic algorithm the system helped answer important questions about urban greenery including where plants should be located in a city which trees are better for reducing the air pollution concentration how many tree clusters are needed and what is the optimal greenery budget tables 7 10 and appendix a in comparison with other studies devoted to investigation of the influence of plants on air pollutant removal e g fuiii et al 2005 omasa et al 2002 bell and treshow 2002 this work focuses on the problem of interactions between air pollutants and greenery on a global scale including the whole city examination of the internal chemical processes of absorption of different air pollutants by different plants is outside the scope of the given work but optimisation of the greenery strategy and control of the allocation of different tree clusters in the city taking into account the dynamics of air pollutants is a core focus of the presented investigation such an approach allows for the simulation of the dynamics of daily air pollution concentration taking into account the combined influence of many other urban agents factories vehicles trees human populations to protect both selected urban areas and the whole city there is still much worthwhile work to be investigated in the future the most interesting and important research issues include 1 further specification of the model through including other agents particularly agent buildings which can be additional natural barrier for air pollutants 2 simulation of the strategy of vertical greening for tall buildings located near protected urban areas 3 further refinement of the methodology for estimation of the air pollution concentration through improving the model of interaction between different air pollutants and taking into account the wide set of climate effects e g temperature air humidity and wind turbulence and 4 further improving the optimisation problems through taking into account the population distribution and the remoteness of different population clusters from emission sources acknowledgements this work was supported by the russian foundation for basic research grant no 18 51 05004 real data provided by the cens http cens am were used for model validation appendix a optimal configurations of tree cluster planting around enterprises no of agent enterprises coordinates of tree cluster centres optimal configurations of tree clusters longitude latitude geometry planting of agent trees kind of trees distance between nearest agent trees metres radius of planting zone metres 1 44 452886 40 185255 arithmetic spiral maple 200 250 2 44 515191 40 186029 arithmetic spiral maple 200 250 3 44 452882 40 185183 arithmetic spiral maple 200 250 4 44 459556 40 183264 simple circle maple 200 250 5 44 446861 40 138059 arithmetic spiral maple 200 250 6 44 445827 40 174123 arithmetic spiral maple 200 250 7 44 566993 40 221453 arithmetic spiral maple 200 250 8 44 549951 40 203825 double circle maple 200 250 9 44 508014 40 211234 arithmetic spiral maple 200 250 10 44 521031 40 142562 arithmetic spiral maple 200 250 11 44 400036 40 152160 arithmetic spiral maple 200 250 12 44 518752 40 122132 double circle with variable distance oak 200 250 13 44 518006 40 120192 arithmetic spiral maple 200 250 14 44 485206 40 148622 arithmetic spiral maple 200 250 15 44 460888 40 144316 arithmetic spiral maple 200 250 16 44 427134 40 185669 arithmetic spiral maple 200 250 17 44 461560 40 162804 arithmetic spiral maple 200 250 18 44 489247 40 141717 arithmetic spiral maple 200 250 19 44 572290 40 164796 simple circle maple 200 250 20 44 532944 40 152099 simple circle maple 200 250 21 44 546347 40 195269 arithmetic spiral maple 200 250 22 44 500074 40 146946 arithmetic spiral maple 200 250 23 44 499244 40 144289 simple circle maple 200 250 24 44 507508 40 180254 simple circle maple 200 250 25 44 490130 40 179375 arithmetic spiral maple 200 250 26 44 523051 40 138597 arithmetic spiral maple 200 250 27 44 473250 40 150027 arithmetic spiral maple 200 250 28 44 429087 40 184458 arithmetic spiral maple 200 250 29 44 426235 40 187411 arithmetic spiral maple 200 250 30 44 507207 40 181125 arithmetic spiral maple 200 250 31 44 464907 40 191119 arithmetic spiral maple 200 250 32 44 529165 40 191523 arithmetic spiral maple 200 250 33 44 502422 40 173353 arithmetic spiral maple 200 250 34 44 493609 40 173447 arithmetic spiral maple 200 250 35 44 450710 40 138904 simple circle maple 200 250 36 44 525030 40 188061 simple circle maple 200 250 37 44 526432 40 181716 arithmetic spiral maple 200 250 38 44 502340 40 124714 double circle with variable distance oak 200 250 39 44 412927 40 197716 arithmetic spiral maple 200 250 40 44 593618 40 187313 arithmetic spiral maple 200 250 41 44 570344 40 190286 arithmetic spiral maple 200 250 42 44 549218 40 092105 arithmetic spiral maple 200 250 43 44 529402 40 210127 arithmetic spiral maple 200 250 44 44 523100 40 130684 arithmetic spiral maple 200 250 45 44 509941 40 125821 arithmetic spiral maple 200 250 46 44 498413 40 209968 arithmetic spiral maple 200 250 47 44 444196 40 186415 arithmetic spiral maple 200 250 48 44 481816 40 214362 arithmetic spiral maple 200 250 49 44 436104 40 138133 arithmetic spiral maple 200 250 50 44 533168 40 224208 arithmetic spiral maple 200 250 51 44 531171 40 152329 arithmetic spiral maple 200 250 52 44 536695 40 186394 arithmetic spiral maple 200 250 53 44 493192 40 143951 arithmetic spiral maple 200 250 54 44 558715 40 236147 double circle with variable distance oak 200 250 55 44 524415 40 169180 double circle with variable distance oak 200 250 56 44 569396 40 212193 arithmetic spiral maple 200 250 57 44 534373 40 145602 arithmetic spiral maple 200 250 58 44 466120 40 212868 arithmetic spiral maple 200 250 59 44 522173 40 190181 arithmetic spiral maple 200 250 60 44 572992 40 218547 arithmetic spiral maple 200 250 61 44 557955 40 174979 arithmetic spiral maple 200 250 62 44 465508 40 196313 arithmetic spiral maple 200 250 63 44 510599 40 133524 double circle with variable distance oak 200 250 64 44 516954 40 180059 simple circle maple 200 250 65 44 521633 40 198632 arithmetic spiral maple 200 250 66 44 494178 40 142460 simple circle maple 200 250 67 44 538600 40 226738 arithmetic spiral maple 200 250 68 44 562511 40 216550 arithmetic spiral maple 200 250 69 44 463694 40 143946 arithmetic spiral maple 200 250 70 44 498398 40 152903 arithmetic spiral maple 200 250 71 44 434629 40 212409 arithmetic spiral maple 200 250 72 44 464118 40 209714 arithmetic spiral maple 200 250 73 44 521437 40 171011 simple circle maple 200 250 74 44 513663 40 207475 simple circle maple 200 250 75 44 504072 40 164685 arithmetic spiral maple 200 250 76 44 521986 40 151798 arithmetic spiral maple 200 250 77 44 444853 40 142108 arithmetic spiral maple 200 250 78 44 616689 40 178025 simple circle maple 200 250 79 44 498021 40 144575 arithmetic spiral maple 200 250 80 44 510805 40 179524 arithmetic spiral maple 200 250 81 44 437445 40 171254 double circle with variable distance oak 200 250 82 44 497487 40 143960 arithmetic spiral maple 200 250 83 44 443283 40 163645 arithmetic spiral maple 200 250 appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 003 
26224,urban greenery such as trees can effectively reduce air pollution in a natural and eco friendly way however how to spatially locate and arrange greenery in an optimal way remains as a challenging task we developed an agent based model of air pollution dynamics to support the optimal allocation and configuration of tree clusters in a city the pareto optimal solutions for greenery in the city were computed using the suggested heuristic optimisation algorithm considering the complex absorptive diffusive interactions between agent trees tree clusters and air pollutants produced by agent enterprises factories and agent vehicles car clusters located in the city we applied and tested the model with empirical data in yerevan armenia and successfully found the optimal strategy under the budget constraint planting various types of trees around kindergartens and emission sources keywords air pollution dynamics agent based modelling evolutionary algorithms simulation of complex environmental systems greenery armenia 1 introduction as is known many countries and cities currently face the problem of increasing air pollution caused by industrial activities and the increasing number of vehicles previous research e g brunekreef and holgate 2002 has proven the strictly negative influence of air pollutants organic and inorganic dust carbon monoxide co sulphur dioxide so2 nitrogen oxides nox volatile organic compounds vocs heavy metals etc on human health air pollution mainly affects respiratory diseases such as asthma allergy chronical obstructive pulmonary diseases lung cancer cardiovascular diseases and skin diseases in addition infants and children are especially vulnerable to the influence of air pollution there are known approaches to reducing air pollution using natural based solutions nsb in particular urban greenery e g nesshöver et al 2017 among research concerning this topic can be highlighted a work fuiii et al 2005 which investigated air pollutant removal by plant absorption and adsorption in japan this research yielded the important conclusion that the efficiency of plant absorption depends on the photosynthesis actively performed in the spring and summer thus the contribution of nbs to air pollution removal can be significantly improved in countries having better climate characteristics e g a higher number of sunny days during the year as in the republic of armenia having approximately 300 sunny days per year in the city of yerevan thus the internal chemical processes of absorption and adsorption of different kinds of air pollutants by different kinds of plants have been very well investigated on the micro level e g fuiii et al 2005 omasa et al 2002 bell and treshow 2002 the results raise the flowing questions what is the positive impact of urban greenery in reducing air pollution how should the best kinds of trees be chosen and allocated in a city to protect the human population and which greenery strategies are better taking into account limited urban budgets solving such problems are impossible without modelling air pollution dynamics and forecasting the movement of air pollutants in the urban atmosphere there are three main groups of models that can be used to model air pollution dynamics the first is based on a statistical approach e g bolzern et al 1982 harnandez et al 1992 and intended mostly for analysis of historical data on pollutant concentrations and short term forecasting the second group consists of deterministic and deterministic statistical models e g lamb and seinfeld 1973 genikhovich 2004 pure statistical approaches to the investigation of air pollution dynamics are mainly simplified and their applications are limited the main reason for the limitation is that they ignore many factors with important influences on the dynamics of air pollution concentration such as plants located on paths of pollutant masses such plants are natural barriers implementing absorption effects some environmental factors e g changing wind directions and collusions between trees and landscape objects cannot be taken into account using statistical methods the last group of methods is based on modelling the dynamics of emission plumes using gaussian dispersion models e g turner 1994 beychok 2005 and are intended for the investigation of air diffusion processes taking into account different climate factors such models are the most realistic but they are characterised by significant computation complexity however such models are not always capable of representing the roles of different sources and sinks of air pollution such as cars factories and trees along with the multiple interactions between them specifically agent based models can provide a better understanding of the role of trees as sinks of air pollution than provided by diffusion reaction models and thus support the implementation of informed public policies to reduce pollution effects there are other well known and simpler models of air pollution dynamics such as lagrangian models eulerian models chemically reactive compound models and analytical models e g seinfeld 1975 zak 1983 seinfeld 1986 barbulescu and barbes 2017 ridley 2017 corani and scanagatta 2016 such models focus on forecasting trajectories of moving air pollutants without taking into account the impact of interaction with other landscape objects e g trees buildings nevertheless some studies have been devoted to researching air pollution dynamics taking into account some elements of the urban landscape e g turner 1964 finzi and tebaldi 1982 santiago et al 2017 in contrast the agent based simulation approach suggested here for modelling air pollution dynamics involving interaction with landscape objects particularly with tree clusters allows the dimensionality of the considered problem to be reduced significantly this ensures that the advantages of deterministic models are maintained the idea of combining computational air fluid dynamics abbot and basco 1989 and agent based modelling has already been suggested elsewhere epstein et al 2011 however the agent based model of air pollution dynamics using interactions with tree clusters having individual characteristics such as the geometry of planting e g simple circle arithmetic spiral double circle the type of tree e g popular oak maple and the distance between the nearest tree clusters is suggested herein for the first time this method aims to reduce the air pollution concentration in cities many studies have confirmed the positive impact of greenery in reducing air pollution in urban areas e g jim and chen 2008 wong et al 2009 however there are not any systems that determine the optimal number of tree clusters their location coordinates and planting geometry the best kind of trees and other parameters that could be computed to minimise the average daily concentration of air pollution if the urban greenery budget is limited this omission has been due to the overly high computational complexity needed for such tasks which involve large scale multi objective optimisation problems to achieve this goal we have combined computational air pollution dynamics with simulations of the ecological behaviour of different urban agents such as agent enterprises factories agent vehicles car clusters and agent trees tree clusters moreover we have used a special multi objective genetic algorithm akopov and hevencev 2013 aggregated with the developed simulation through objective functions to determine the pareto optimal solutions for greenery in a city there is a line of research to develop agent based models and multi objective systems to enhance the management of complex environmental systems hadka and reed 2015 sun et al 2016 tesfatsion et al 2017 west et al 2018 in the suggested model trees and air pollutants are considered interactional agents with individual characteristics this means that different tree clusters interacting with heterogeneous air pollutants have dissimilar absorptive diffusive characteristics and different influences on the daily air pollution concentration agent based modelling approaches in combination with other simulation methods have been described elsewhere papaleonidas and iliadis 2012 letcher et al 2013 zenonos et al 2015 vallejo et al 2015 ridley 2017 akopov et al 2017 these studies have identified certain associated advantages for ecological modelling the most important of which is the possibility of modelling the dynamics of agent interactions without the need to develop complex analytical models this study is focused on designing an agent based model to determine the best ecological trade offs for urban greenery the case study of the city of yerevan armenia will be presented the main purpose behind the development of the original decision making system was to analyse effective scenarios for greenery allocation under the budget constraint in the city of yerevan armenia to reduce the average daily pollution concentration the suggested system can be applied to identify optimal greenery strategies to reduce air pollution concentrations in other urban areas and regions if appropriate data are available in addition the developed approach improves the precision of the model by including additional characteristics such as the prevailing wind direction wind velocity air pollution intensities generated by agent enterprises and agent vehicles 2 data and method 2 1 study area the city of yerevan armenia has many features that should be taken into account for simulation development yerevan has an average height of 990 m 3248 03 ft with a minimum of 865 m 2837 93 ft and a maximum of 1390 m 4560 37 ft above sea level it is located on to the edge of the hrazdan river in the northeast of the ararat plain ararat valley the important aspect of the location of yerevan is that the city is surrounded by the caucasian mountains the mountains significantly restrict the possibility of simple relocation of enterprises that are the main stationary sources of air pollution outside the city another problem is the hard restriction of the budget that can be used for environmental activities caused by the difficult economic state of armenia these factors require the use of natural based solutions and relatively low cost methods for reducing the air pollution concentration such as the greenery allocation in the city the following types of agents are used in the model agent enterprises and agent vehicles car clusters which are sources of emissions in the city agent trees which consist of small groups of homogeneous closely located trees and agent emissions which are air pollutants produced by agent enterprises and agent vehicles in addition concepts of the personal space of an agent tree and the personal space of an agent emission are suggested the personal space of an agent tree is defined by the crones of plants forming the group fig 1 the personal space of agent trees consists of closely located separate homogenous trees several closely located agent trees with a specific planting geometry are considered a tree cluster one agent tree consists of 10 trees the crown of a plant refers to the totality of an individual plant s aboveground parts including stems leaves and reproductive structures a plant community canopy consists of one or more plant crowns growing in a given area therefore it can be considered as a tree cluster the personal space of an agent emission is defined by its radius length in the model all agent emissions are formed by masses of homogenous air pollutants and radiuses of agent emissions define areas having persistent concentrations of homogenous air pollutants to simplify the concentration of air pollutants belonging to any agent emission are assumed to be proportional to the agent emission radius length the radius length of the agent emission is reduced when the agent emission is touched by some agent tree fig 2 thus the absorption effect is implemented nevertheless agent emissions with radiuses significantly less than the radiuses of agent trees will pass through thus the diffusion effect is implemented the behaviour of such agents is based on natural observations taken in the city yerevan armenia all experiments were conducted in selected urban areas with agent trees consisting of separate trees with fixed distances between each other further emissions of different volumes of dust occurred and a special dust detector was used for estimation of the associated air pollution concentration in protected urban areas the ratio between the emitted dust volume which defines the initial radius of the agent emission and the size of the tree cluster which defines the radius of the agent tree when the agent trees do not protect from air pollutants was successfully identified such measurements were taken for different kinds of pollutants using different kinds of trees depending on the number of agent trees their radiuses the kinds of trees and other characteristics the rate of change in the air pollution concentration from the point of the emission source to the point of the protected urban area was found such absorption and diffusion effects have other known justifications e g fuiii et al 2005 omasa et al 2002 there are following agent emissions are considered in the model organic dust inorganic dust heavy metals carbon monoxide co sulphur dioxide so2 nitrogen oxides nox carbonates without volatile organic compounds volatile organic compounds vocs and other emissions the finite state of agent emissions caused by their life span is dissipation when the agent emission do not have a radius in the model the radiuses of undissipated agent emissions are summarised at the monitoring station or in protected urban areas and are used for estimating the average daily air pollution concentration the intersection of different agent emissions in air space is simulated without mixing their pollutants fig 2 this assumption allows for the modelling of the daily pollution concentrations of different air pollutants in the areas of appropriate agent emissions these areas are reduced quickly in the presence of multiple contacts with agent trees as well as slowly as a consequence of natural dissipation over time the important characteristic of the described model is the daily concentration of air pollution which can be estimated as the sum of the areas of the circles of all j i v t h s agent emissions located at the fixed zone of ecological monitoring the monitoring station with known coordinates x y and r is the radius of observation here j i v j i v is the set of indexes of agent emissions produced by the i t h emission source that contains the v t h air pollutant a homogenous mass the main justification for using such a simplified approach for estimation of the daily air pollution concentration is that the initial radiuses of agent emissions were chosen proportionately to appropriate volumes of pollutants produced by each agent enterprise and agent vehicle the current values of the radiuses of agent emissions located in the monitoring zone depend on many factors in particular all agent trees that they have previously touched thus the sum of the areas of the circles of agent emissions can be used for model estimation of air pollution as the most representative metric in this model the air pollution concentration of the j i v t h agent emission consisting of the v t h pollutant in the zone of the monitoring station with the radius r at the time t t t is 1 g j i v t π r j i v 2 t if x x j i v t 2 y y j i v t 2 r and s t j i v t 1 0 if x x j i v t 2 y y j i v t 2 r i i v v j i v j i v where t t 0 t 1 t t is the set of temporal intervals and the set is in days of a one year period t is 365 days i i 1 i 2 i i is the set of indexes of emission sources that produce air pollutants agent enterprises and agent vehicles located in the city i is the number of emission sources v v 1 v 2 v v is the set of indexes of air pollutants e g co2 organic dust inorganic dust heavy metals so2 nox v is the number of air pollutants j i v j 1 i v j 2 i v j j i v t is the set of indexes of agent emissions produced by the i t h emission source that contains the v t h air pollutant a homogenous mass j i v is the number of agent emissions at the moment t t x j i v t y j i v t are coordinates of agent emissions in the wgs 84 the world geodetic system coordinate system at the moment t t r j i v t is the radius of the j i v t h agent emission at the moment t t which changes as a result of interaction of the agent emission with any k ζ t h agent tree reflected in fig 2 and r j i v τ j i v 1 10 is the known initial radius of the j i v t h agent emission defined at the moments of air pollution production by different sources of emissions x y are known coordinates of the monitoring station in the city r is the radius of observation of the monitoring station h j i v t 0 u i t 0 2 u i t u i u i is the set of regular time moments of producing the j i v t h agent emission u i is the emissions intensity of the i t h source u i 7 for all agent enterprises and u i 2 for all agent vehicles and s t j i v t 1 0 is the state of the j i v t h agent emission wherein s t j i v t 1 if the j i v t h agent emission is active s t j i v t 0 if the j i v t h agent emission is dissipated and t j i v is the life time of the j i v t h agent emission 2 s t j i v t 1 if t τ j i v t j i v 0 if t τ j i v t j i v τ j i v h j i v i i v v j i v j i v the daily air pollution concentration summarised for all agent emissions and averaged for all associated pollutants that have reached the monitoring zone is 3 d c t 1 v i 1 i v 1 v κ v j i v 1 j i v t g j i v t i i v v j i v j i v t t here ϖ v is the weight coefficient reflecting the importance of the v t h pollutant for a decision maker v 1 v κ v 1 0 κ v using the weight coefficients κ v v v takes into account the importance for a decision maker of different air pollutants in the problem of air pollution minimisation the daily air pollution concentration should not exceed the maximum permissible concentration which is fixed individually for each air pollutant including dust heavy metals sulphur dioxide so2 nitrogen oxides nox carbonates and volatile organic compounds vocs the maximum permissible concentration is aggregated and denoted as d c in the model d c t d c the average daily air pollution concentration is 4 a d c 1 t t t 0 t d c t 2 2 data detailed data including datasets of agent enterprises agent vehicles and agent trees as well as locations of emission sources and protected kindergartens are presented in the data in brief paper akopov et al 2019 the number of agent emissions and their initial radiuses are directly proportional to the total air pollution produced by agent enterprises the main characteristics of agent trees are presented in tables 1 and 2 the value of the absorption coefficient is calculated by considering the estimated value of the maximum volume of absorption by one tree and the total number of trees included in one tree cluster the main characteristics of agent vehicles car clusters are presented in table 3 and in data in brief there are approximately 300000 cars in yerevan which form 1000 car clusters the total emissions produced per day by one agent vehicle consisting of 300 cars is approximately 3 8 tonnes one agent emission has an initial radius of 10 m deposits and 1 9 tonnes of total pollutants mainly co2 thus for one event occurring with a fixed intensity of 2 one time for two days taking into account parking a total of 3 8 2 1 9 4 agent emissions are produced for one event by one agent vehicle table 3 the initial radius of agent emissions produced by an agent vehicle was obtained from the results of air pollution measurements near roads and the measurement reflects areas with maximum concentrations of pollutants wind characteristics such as the prevailing wind direction and wind speed variation by month have an important influence on air pollution dynamics table 4 here an 8 wind compass rose is used with the four ordinal directions forming the bisecting angle of the cardinal winds north n west w northeast ne south s southeast se southwest sw northwest nw all data were obtained from the main weather station of yerevan armenia here the prevailing wind direction is used for correction of the movement directions of agent emissions when there are not any agent trees along the movement paths or there is a diffusion effect in the presence of obstacles such as agent trees the prevailing wind direction disappears and the movement directions of agent emissions are defined by interactions between agent emissions and agent trees there are different methods available for collecting air quality monitoring data as described in previous works papaleonidas and iliadis 2012 zenonos et al 2015 in this research the standard approach was applied based on reading real data using equipment such as gas analysers dust analysers sensors and spectroscopes and comparing the real data against the simulation results samples were collected and treated at the central analytic laboratory of the centre of ecological noosphere studies of the national academy of sciences of armenia cens accredited by iso iec 17025 the laboratory has modern equipment including a serinus 30 carbon monoxide analyser serinus 40 analyser serinus 40 nitrogen oxides analyser serinus 51 sulphur dioxide analyser voc72 m gas chromatography volatile organic compounds btex analyser and dust analyser opastop gp4000hd the collected samples were treated and analysed for air pollutant contents dust co nox so2 heavy metals vocs through the atomic absorption method aanalyst 800 perkin elmer us iso 8573 and iso 12500 air quality 2 3 method the developed simulation model employs the earlier developed agent based approach of modelling ecological economic systems suggested in a previous study akopov et al 2017 in this work agents are agent enterprises that produce air pollution and a method to find the best trade offs for ecological modernisation of enterprises towards pure ecological manufacturing is developed however such an approach assumes reduction of the production capacity of enterprises and appropriate outputs together with emissions in the atmosphere furthermore ecological modernisation requires significant expenditures in contrast developing urban greenery is an alternative and relatively inexpensive approach to reducing the air pollution concentration first the idea of using agent emissions instead of a more natural emissions plume should be explained as is known modelling the dynamics of an emissions plume requires more complex models based on differential equations with partial derivatives that describe appropriate chemical processes on the molecular level beychok 2005 turner 1994 the explicit advantage of such models is the possibility of investigating internal air dispersion and diffusion processes that depend on many factors such as wind direction temperature and air humidity therefore these models are the most realistic and accurate however such models are characterised by significant computation complexity therefore their application is justified for local and limited systems without taking into account complex interactions between trees and air pollutants on the scale of a whole city employing different strategies of urban greenery on the other hand emission plumes can be considered ensembles consisting of separate agent emissions controlled by external factors on the individual level e g the prevailing wind direction and interactions with plants if the resulting precision will be enough to model the air pollution dynamics because modelling atmospheric dispersion is not critical for investigating the influence of greenery on pollution removal using agent emissions instead of emission plumes is justified there is a known concept of agent states in agent based modelling each agent can have a set of individual characteristics that are called agent states e g the agent emission can be active or dissipated the agent enterprise can be non ecological and not surrounded by trees or ecological there are transitions between such states that can be controlled by special rules or can be under natural temporal control the scheme of possible states of the considered agents is presented in fig 3 the model starts with the initial states of agents which are highlighted in grey in fig 3 the initial conditions of the model are defined by the locations of the agent enterprises and agent vehicles in the city according to known coordinates for enterprises and stochastic coordinates of roads for vehicles the next step is the distribution of new agent trees with maximum initial radiuses of personal space and absorption characteristics within the city according to the defined optimal tree cluster configurations some states of agent trees agent vehicles and agent emissions are interconnected in particular new agent emissions with maximum initial radiuses are generated by agent enterprises and agent vehicles when they are in their second and third states of producing air pollution the production cycles of air pollution by agent enterprises and agent vehicles are defined by the emission intensities usually equal to seven days for agent enterprises and two days for agent vehicles further the abstract mathematical description of the suggested model is considered as well as multi criteria optimisation problems for rational greenery allocation in the city to minimise the air pollution concentration where z ζ 1 ζ 2 ζ z is the set of indexes of tree clusters where agent trees are located k ζ k 1 ζ k 2 ζ k k ζ is the set of indexes of agent trees that absorb air pollutants belonging to the ζ t h tree cluster s j i v t is the speed of movement of the j i v t h agent emission which equals the average wind speed at the moment t t α j i v t is the angle of movement of the j i v t h agent emission j i v j i v when there are not any agent tree on its path or if the diffusion effect takes place at the moment t t this angle takes into account the influence of the prevailing wind direction table 5 β j i v k ξ t is the angle of bypass of the j i v t h agent emission j i v j i v around the k ζ t h agent tree k ζ k ζ at the moment t t γ j i v k t is the angle of the rebound of the j i v t h agent emission j i v j i v from the k ζ t h agent tree k ζ k ζ at the moment t t с j i v t is the rebounding coefficient of the j i v t h agent emission from the k ζ t h agent tree с j i v t 10 for all j i v j i v r k ξ t is the radius of the k ζ t h agent tree depending on tree kind e g poplar oak maple d i s t j i v k ξ t is the euclidean distance between the j i v t h agent emission and the k ζ t h agent tree ξ j i v k ζ t is the event when the j i v t h agent emission touches the k ζ t h agent tree 5 ξ j i v k ζ t 1 if d i s t j i v k ζ t r j i v t r k ζ t 0 if d i s t j i v k ζ t r j i v t r k ζ t i i v v j i v j i v ζ ζ k ζ k ζ in the absence of a connection between the j i v t h agent emission j i v j i v i i v v and the k ζ t h agent tree k ζ k ζ ζ z the radius of the former will not be changed fig 4 an appropriate interaction can take place if any agent tree is located on the track of any agent emission and the radius of the latter is not much smaller than the radius of the agent tree that has been touched this case can be considered the absorption effect after an event in which an agent emission has been touched by some agent tree the radius of the former will be significantly reduced in this case the agent emission can pass through any agent tree this case can be considered the diffusion effect thus the radius of the j i v t h agent emission at moment t t t is defined by the same recursive procedure 6 r j i v t r j i v τ j i if i is true r j i v t 1 exp η j i v if ii is true r j i v t 1 exp η j i v μ k ζ j i v if iii is true 0 if iv is true where i t τ j i v τ j i v h j i v ii ξ j i v k t 0 or r j i v t r k ζ t ϖ and s t j i v t 1 iii ξ j i v k t 0 and r j i v t r k ζ t ϖ and s t j i v t 1 iv s t j i v t 0 i i v v j i v j i v ζ ζ k ζ k ζ where μ k ζ j i v is the fixed coefficient reflecting the absorption effect of the k ζ t h agent tree regarding the j i v t h agent emission containing the v t h pollutant μ k ζ j i v 1 η j i v is the fixed coefficient reflecting the exponential fading of the j i v t h agent emission η j i v 0 01 and ϖ is the fixed coefficient reflecting the diffusion effect ϖ 10 agent emissions move towards different directions under the influence of applied forces caused by the wind velocity and interactions with different landscape objects in particular agent trees such movement of masses of air pollutant types can be described by well known differential equations with variable structures thus the dynamics of the j i v t h agent emission for the time period of t r ψ t r 1 are described by the following system of differential equations at time 7 d x j i v ψ d ψ s j i v ψ cos α j i v ψ if v is true s j i v ψ cos β j i v k ζ ψ c j i v d i s t j i v k ζ cos γ j i v ψ if vi is true 0 if vii is true 8 d y j i v ψ d ψ s j i v ψ sin α j i v ψ if v is true s j i v ψ sin β j i v k ζ ψ c j i v d i s t j i v k ζ sin γ j i v ψ if vi is true 0 if vii is true i i v v j i v j i v ζ ζ k ζ k ζ where v d i s t j i v k ζ ψ r j i v ψ r k ζ ψ for all k ζ k ζ or r j i v ψ r k ζ ψ ϖ for the nearest k ζ k ζ and s t j i v ψ 1 vi d i s t j i v k ζ ψ r j i v ψ r k ζ ψ for the nearest k ζ k ζ and r j i v ψ r k ζ ψ ϖ for the nearest k ζ k ζ and s t j i v ψ 1 vii d i s t j i v k ζ ψ r j i v ψ r k ζ ψ and r j i v ψ r k ζ ψ ϖ for all k ζ k ζ or s t j i v ψ 0 the system of eqs 7 and 8 is similar in some elements to the model of crowd behaviour described in previous works akopov and beklaryan 2015 beklaryan and akopov 2016 this is due to agent emissions colliding with agent trees and taking on some characteristics of crowds such as chaotic movement and targeting to bypass landscape obstacles the main approach to minimisation of the air pollution concentration is the control of parameters related to tree cluster configurations on the individual level the control parameters for each ζ t h tree cluster ζ ζ consisting of the k ζ t h s agent trees k ζ k ζ are defined at the initial moment t 0 t as x ˆ ζ t 0 y ˆ ζ t 0 are the coordinates of the ζ t h tree cluster centre for simplification these coordinates are equal to the coordinates of agent enterprises and protected urban areas c ζ 0 1 4 is the set of possible configurations of the ζ t h tree cluster and c n f ζ t 0 c ζ is the selected configuration c n f ζ t 0 0 without agent trees c n f ζ t 0 1 simple circle c n f ζ t 0 2 arithmetic spiral with the fixed step c n f ζ t 0 3 double circle c n f ζ t 0 4 double circle with variable distances between the nearest agent trees considering the ζ t h tree cluster and c n f ζ t 0 c ζ s ζ 1 2 5 is the set of possible kinds of agent trees located in the tree cluster and t p ζ t 0 s ζ is the selected kind of agent trees t p ζ t 0 1 poplar tree cluster t p ζ t 0 2 oak tree cluster t p ζ t 0 3 maple tree cluster t p ζ t 0 4 pine tree cluster t p ζ t 0 5 ulmus tree cluster δ ζ t 0 is the distance between the nearest agent trees and the ζ t h tree cluster δ δ ζ t δ ζ ζ where δ δ are the lower and upper boundaries of the distance range and r ζ t 0 is the radius of the ζ t h tree cluster that defines the planting area r r ζ t 0 r ζ z where r r are the lower and upper boundaries of the radius range the target number of agent trees to be planted around agent enterprises for reducing the air pollution concentration can be found by solving the bi objective optimisation problem which will be considered later 9 k ζ t 0 f c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 ζ ζ t 0 t the needed greenery budget is 10 g b ζ 1 ζ k ζ k ζ c k ζ n ζ ζ ζ k ζ k ζ where c k ζ is the known cost of planting one tree of the k ζ t h agent tree of the ζ t h tree cluster and n ζ is the number of closely located trees to one agent tree of the ζ t h tree cluster n 10 for all ζ ζ here the cost of planting depends on the tree kind e g 900 usd for a poplar 700 usd for an oak and it includes all expenses related to the support of the tree life cycle during a year e g irrigation digging the soil the number of planted agent trees in all ζ t h s tree clusters ζ z is 11 n ζ 1 ζ k ζ problem a the need to minimise the average daily pollution concentration and greenery budget through the set of control parameters c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 12 min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 a d c min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 g b where a d c 1 t t t 0 t d c t g b ζ 1 ζ k ζ k ζ c k ζ n ζ d c t 1 v i 1 i v 1 v κ v j i v 1 j i v t g j i v t d c t d c v 1 v t κ v 1 0 κ v δ δ ζ t 0 δ r r ζ t 0 r c n f ζ t 0 c ζ t p ζ t 0 s ζ ζ ζ k ζ k ζ i i v v j i v j i v t 0 t under different other constraints further some modifications of the problem of minimising the average daily pollution concentration will be considered there is an important problem of protecting selected urban areas with a significant social relevance e g kindergartens and hospitals from harmful emissions produced by agent enterprises and agent vehicles a strategy considering this issues can be useful in conditions of greenery budget deficit and a lack of free areas acceptable for planting each kindergarten has its own personal space that should be protected from air pollutants the problem will be considered for the protection of kindergartens of the city yerevan armenia as an example here p p 1 p 2 p p is the set of indexes of kindergartens located in the city that should be protected and p is the total number of kindergartens x p y p are the coordinates of the p t h kindergarten and r ˆ p is the known radius of the p t h kindergarten defined by the personal space where the average daily pollution concentration should be minimized the air pollution concentration of the j i v t h agent emission consisting of the v t h pollutant in the p t h kindergarten with radius r ˆ p at time t t t is 13 g j i v p t π r j i v 2 t if x j i v t x p 2 y j i v t y p 2 r ˆ p and s t j i t 1 0 if x j i v t x p 2 y j i v t y p 2 r ˆ p i i v v j i v j i v p p the daily air pollution concentration summarised by all agent emissions and averaged by protected urban areas kindergartens is 14 d c t 1 p v p 1 p i 1 i v 1 v κ v j i v 1 j i v t g j i v p t the daily air pollution concentration should not exceed the maximum permissible concentration which is fixed individually for each air pollutant including dust heavy metals sulphur dioxide so2 nitrogen oxides nox carbonates and volatile organic compounds voc the maximum permissible concentration will be aggregated and denoted as d c in the model d c t d c the average daily air pollution concentration in protected kindergartens is 15 a d c 1 t t t 0 t d c t the number of planted agent trees needed for protection of the kindergartens is 16 n ζ 1 ζ k ζ ζ ζ where k ζ is the target number of agent trees to be planted around kindergartens the number of tree clusters planted around protected kindergartens k ζ can significantly differ from the number of tree clusters planted around agent enterprises k ζ due to differences in location the sizes of their areas remoteness from emissions sources and the accumulations of air pollutants produced by many agent enterprises and agent vehicles problem b the need to minimise the average daily pollution concentration and the greenery budget in protected urban areas kindergartens through the set of decision variables c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 17 min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 a d c min c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 g b where a d c 1 t t t 0 t d c t g b ζ 1 ζ k ζ k ζ c k ζ n ζ d c t 1 p v p 1 p i 1 i v 1 v κ v j i v 1 j i v t g j i v p t d c t d c v 1 v κ v 1 0 κ v δ δ ζ t 0 δ r r ζ t 0 r c n f ζ t 0 c ζ t p ζ t 0 s ζ ζ ζ k ζ k ζ i i v v j i v j i v p p t 0 t the model eqs 1 17 was implemented in the well known anylogic simulation tool that supports agent based modelling methods with gis integration original java classes were developed for implementation of the individual behaviour rules of all agents including agent trees and agent emissions as a result over 12 000 replicated agents can be simulated in the system to calculate values of the objective functions eq 12 17 further the simulation was aggregated with suggested genetic algorithms through these objective functions to seek the pareto optimal solutions a line of the heuristic optimisation algorithms can be applied for the considered bi objective optimisation problem e g zitzler and thiele 1999 kim et al 2004 zhong et al 2004 among them the spea strength pareto evolutionary algorithms class proposed by zetzler zitzler and thiele 1999 and developed in previous works akopov and hevencev 2013 kim et al 2004 e zaenudin and kistijantoro 2016 can be highlighted using heuristic optimisation algorithms is effective for solving large scale optimisation problems of ecological economic systems e g akopov et al 2017 vallejo et al 2015 the magamo multi agent genetic algorithm for multi objective optimisation algorithm based on some aspects of previous work zhong et al 2004 and suggested in a previous study akopov and hevencev 2013 was used for solving the considered problem 17 the magamo paralleling of evolutionary processes has some advantages when objective functions are the result of agent based simulations the important feature of magamo is the possibility to organise the heuristic optimisation in the parallel mode where each process is responsible for the evolution of previously selected decision variables the decision variables in the set are separated into subsets associated with the corresponding genetic algorithms ga gas are exchanged by the best values of the decision variables within the evolutionary search for the considered optimisation problem this means that each evolving tree cluster consisting of agent trees and with its own characteristics will be associated with one independent genetic algorithm this approach allows reduction of the population size of each ga and improvement of the time efficiency and rate of convergence of the global heuristic search there are two optimisation functions for solving problem a and problem b the average daily pollution concentration a d c and a d c and the greenery budget g b and g b these criteria were chosen because they reflect the interests of potential decision makers aiming to seek appropriate trade offs in addition these objective functions are the most important for the city of yerevan having limited financial resources and problems with air quality in the developed model there are multiple interactions between agent emissions and agent trees that are complicating factors however using ordinary differential equations with a variable structure eq 7 8 for modelling the dynamics of air pollutants interconnected with agent trees allows significant simplification of the computation procedure and estimation of the objective functions eq 12 17 when using emission plumes instead of agent emissions the model can be more realistic but the computation process could be complicated because there is a need to solve more complex systems of differential equations describing appropriate dispersion processes for each tree cluster connected with air pollution over continuous time within the considered problem the computed configurations of tree clusters are described by the following set of decision variables c n f ζ t 0 t p ζ t 0 δ ζ t 0 r ζ t 0 where the pair c n f ζ t 0 t p ζ t 0 are discrete variables and the pair δ ζ t 0 r ζ t 0 are continuous variables the genetic algorithm magamo is responsible for control of the tree cluster configurations on the individual level configuration samples of the suggested decision variables are presented in fig 5 the optimisation of tree cluster configurations e g the geometry of planting is based on the control of agent tree locations and their characteristics instead of separate trees to simplify the problem the values of the main parameters of the genetic optimisation algorithm are presented in table 6 in the model 83 tree clusters were created around the main sources of emissions agent enterprises located in yerevan to solve problem a solving the considered optimisation problems involves significant computational complexity due to the calculation time of the objective functions being significant over 10 min for each simulation run this is caused by the complex interaction mechanism between agent trees and agent emissions described by the system of differential equations with variable structures the main steps of the heuristic optimisation of tree cluster configurations are the following step 0 code chromosomes to represent the decision variable values with following distribution between all agent processes of the ga responsible for the evolution of personal tree clusters step 1 form the initial population of solutions with estimations of fitness functions which are the results of the simulation eqs 1 17 create the initial global archive of non dominated solutions for all tree clusters for each agent process the following evolutionary steps should be completed step 2 select parents from the archive of non dominated solutions with the highest pareto ranking and located sufficiently remote from each other to avoid inbreeding step 3 perform the crossover and mutation to generate offspring step 4 run the simulation eqs 1 17 to obtain values of fitness functions taking into account all constraints reject the solution without waiting for the end of the simulation process if any constraint has been exceeded step 5 send the best decisions for personal tree clusters from the local population to the global archive of non dominated solutions with an estimation of the pareto ranking retrieve the best decisions for other tree clusters from the global archive of non dominated solutions step 6 if the minimum convergence of ga and the minimum needed number of pareto optimal solutions in the global archive are not reached repeat steps 2 5 2 4 software the simulation model was developed using the anylogic tool aggregated with other components such as the database and genetic algorithm at the current time the software has not been shared for global users and it is implemented as the regional decision making system for yerevan armenia the model is available online at http www runmycode org companion view 3420 the software allows conduction of multiple optimisation experiments using real data for different scenarios of urban greenery for reducing the daily concentration of air pollution the model consists of two levels the first is the top level dashboard used for the setup of control parameters fig 6 as shown in fig 6 a decision maker can set up values of the control parameters such as the configuration of tree clusters both in a united configuration for all agent enterprises that should be surrounded by agent trees and in individual configurations of tree clusters previously computed using the genetic optimisation algorithm the magamo with selection of optimal scenarios on the pareto front an illustration of the air pollution dynamics in the city of yerevan during the first seven days after emissions is shown in fig 7 in more detail here agent emissions are denoted by black points and agent trees are denoted by yellow red and green circles depending on agent tree kind as can been seen from fig 7 tree clusters are natural barriers for agent emissions at least 13 546 agents are simulated in the system which include agent emissions agent trees agent enterprises and agent vehicles 3 results and model validation 3 1 results simulation experiments consisted of two parts the first block is related to solving problem a i e the minimisation of the average daily pollution concentration and the greenery budget through planting tree clusters around agent enterprises that are the main stationary sources of air pollutants the results of the simulation for the first problem are presented in figs 8 13 the pareto front computed with magamo for the considered optimisation problem a is shown in fig 8 at least 500 simulation runs with a thousand internal iterations of the magamo were completed to form the sustainable pareto front shown in fig 8 the quality of the pareto optimal decisions was estimated using different known methods such as the hypervolume metric and the space between decisions the area of the best decision was selected with which the decision maker can obtain acceptable reduction of the average daily pollution concentration in the range of 0 05 0 07 me m3 with the minimum greenery budget 2 95 6 32 musd equal values of weight coefficients reflecting the importance of each air pollutant for a decision maker were used the results obtained for problem a using the genetic algorithm show that over 20 pareto optimal solutions were obtained and 3 scenarios were selected as the final decisions scenario 1 expensive complex greenery this scenario assumed forming tree clusters planting around agent enterprises using mainly complex configurations such as regular circles double circles and double circles with variable distances between agent trees which require many trees and a significant greenery budget the considered trees are mainly poplar and oak additionally the configurations of tree clusters are different for each agent enterprise the maximum effect on reducing the daily pollution concentration is expected scenario 2 compromise greenery this scenario assumed using mainly individual configurations of tree clusters such as simple circles and arithmetic spirals which require an average number of agent trees and a more preferable size of the greenery budget the considered trees are mainly maple and oak for each agent enterprise appendix a an acceptable and sustainable reduction of the daily pollution concentration is expected scenario 3 inexpensive greenery this scenario assumed using mainly individual configurations of tree clusters such as arithmetic spirals which require the fewest number of agent trees and the minimum possible size of the greenery budget the considered trees are mainly poplar for each agent enterprise an unsustainable reduction of the daily pollution concentration to less than the maximum permissible level of the air pollution concentration is expected table 7 shows the numbers of agent trees and separate trees and the amount of greenery budget g b defined by solving problem a using the suggested ga here the distance between the nearest agent trees consisting of 10 closely located trees is 200 m and the radius of planting zones is 250 m for all tree cluster configurations e g simple circle double circle table 8 shows the simulation results obtained using different configurations of tree clusters consisting of different tree kinds for problem a as seen in table 8 the individual configurations of tree clusters obtained for scenario 2 are the most preferable in conditions of greenery budget deficit the configurations of tree cluster planting around agent enterprises obtained for these scenarios are illustrated in figs 9 11 using a map of yerevan armenia here poplar tree clusters are denoted by green circles oak tree clusters are denoted by yellow circles maple tree clusters are denoted by red circles pine tree clusters are denoted by blue circles ulmus tree clusters are denoted by aquamarine circles and agent emissions are denoted by black points for the scenarios considered the dynamics of the daily air pollution concentration were computed and are shown in fig 12 as shown in fig 12 scenario 1 gives the greatest reduction in the daily air pollution concentration while maintaining a significant reserve compared to the maximum permissible concentration 0 1 me m3 scenario 2 is characterised by a smaller reduction of the daily air pollution concentration though the daily air pollution is less than the maximum permissible concentration on any day scenario 3 is flawed because sometimes the daily air pollution concentration reaches the maximum permissible concentration however it can be considered if there is only a small budget available for greenery the dynamics of the average daily air pollution concentration a d c are presented in fig 13 as shown in fig 13 the average daily air pollution concentration exceeds the maximum permissible concentration if there are not any tree clusters using the pareto optimal solutions scenarios allows significant reduction of the average daily air pollution concentration the main reason why tree clusters allow for the reduction of the air pollution concentration is the interaction of agent trees with agent emissions to cause the absorptive diffusive effect in addition tree clusters separate streams of air pollutants change their directions and block them in locations with emission sources the second block of simulation experiments is related to solving problem b i e minimisation of the average daily pollution concentration and the greenery budget through planting tree clusters around selected urban areas that should be protected from air pollutants using a case study of kindergartens in the city yerevan armenia there were 2 scenarios obtained for problem b scenario 4 this scenario assumed planting agent trees around kindergartens without greenery near emissions sources additionally individual configurations of tree clusters would be implemented for each kindergarten as regular circles double circles and double circles with variable distances such tree clusters mainly consist of poplar oak and ulmus scenario 5 this scenario assumed planting agent trees around kindergartens and agent enterprises at the same time thus the greenery budget would be separated between planting around emissions sources and protected urban areas individual configurations of tree clusters would be implemented for each kindergarten such tree clusters consist trees of different kinds of trees dominated by maple and oak in fig 14 is presented the first optimisation scenario scenario 4 where agent trees are planted around protected urban areas kindergartens without greenery near emission sources here mainly simple circles and arithmetic spirals are used additionally poplar oak and ulmus tree clusters dominate the considered scenario provides significant reduction of the average daily pollution concentration to a value comparable to that achieved by planting around agent enterprises the other approach includes planting agent trees around kindergartens and agent enterprises simultaneously this is the balanced and more preferable optimisation scenario table 9 shows the numbers of agent trees tree kinds and the greenery budget g b defined by solving problem b using the suggested ga table 10 shows the simulation results obtained using different configurations of tree clusters consisting of different tree kinds for problem b here the base distance between the nearest agent trees consisting of closely located trees is 60 m and the radius of planting zones is 120 m for all tree cluster configurations e g simple circle double circle in comparison with planting agent trees around agent enterprises only scenarios 1 3 a much greater number of trees and a higher greenery budget are needed to minimise the average daily pollution concentration scenario 4 the main reason that the protection of kindergartens from air pollutants requires more agent trees is that in the absence of agent trees around agent enterprises appropriate pollutants reach areas of kindergartens faster without absorption additionally unlike the central monitoring station many kindergartens are located near sources of emissions thus the best strategy is planting agent trees around kindergartens and agent enterprises at the same time scenario 5 in comparison with the strategy of planting around sources of emissions only discussed earlier scenario 5 is a more expensive but better approach for protection of the human population from air pollution the comparison of all considered scenarios is represented in table 11 as it can be seen as from table 11 the scenario 5 is more preferable than scenarios 2 3 because it provides the lower level of the average daily pollution concentration 0 045 me m3 with keeping the acceptable greenery budget 7 41 musd in addition the scenario 5 is better than scenario 4 because it needs the less greenery budget it seems the effect of scenario 5 is similar with the results of the scenario 1 expensive complex greenery however the average daily air pollution concentration is minimized in the observation zone of the monitoring station only in scenarios 1 3 in contrast the daily air pollution concentration is minimized in multiple protected urban areas near kindergartens in scenarios 4 5 hence the last two scenarios can be considered as the more live safer and the reliability the model was validated using real data the validation procedure was based on comparative analysis of the average daily air pollution concentration obtained through the developed simulation with real data collected in the selected monitoring station in yerevan which records daily air quality readings 3 2 model validation the developed simulation was validated using historical data recorded both in the zone of the monitoring station located in the centre of yerevan armenia scenario 2 and in protected kindergartens scenario 5 the forecasted errors are based on the following estimation using the known method of ordinary least squares the model error estimated in the area of the motoring station is 18 a d c a d c a d c 2 n n n 2 χ 1 the model error estimated in protected urban areas kindergartens is 19 a d c a d c a d c 2 n n n 2 χ 2 where a d c a d c n n are simulated and actual values of the average daily air pollution concentration in the area of the monitoring station at the centre of the city and the number of agent trees respectively a d c a d c n n are simulated and actual values of the average daily air pollution concentration in protected urban areas kindergartens and the number of agent trees respectively and χ 1 χ 2 are the upper limits of the model error 10 and 5 14 respectively aggregated results of the model validation for selected scenarios 2 and 5 with using actual data for 2017 are presented in table 12 these scenarios were implemented in practice in yerevan armenia by 2017 4 discussion and conclusion in this paper a new approach to modelling the air pollution dynamics in a city with use of the agent based model and heuristic optimisation was suggested the main feature of the developed method involves the use of agent trees interacting with agent emissions which describe the dynamics using a system of differential equations that consider absorptive diffusive effects in real ecological systems the research aims to solve the problem of optimal allocation and configuration of tree clusters in the city of yerevan armenia to minimise the average daily pollution concentration observed in defined areas as was demonstrated in the work there are two important optimisation problems in the minimisation of air pollution the first involves reducing the air pollution concentration over a whole city through planting agent trees around emission sources problem a some effective scenarios were computed for solving this problem tables 7 8 and figs 9 13 the second optimisation problem involves reducing the air pollution concentration in selected urban areas e g planting agent trees around kindergartens problem b two additional scenarios were computed for solving this problem tables 9 and 10 and fig 14 and one of the scenarios is the most preferable this scenario assumed planting agent trees around kindergartens and agent enterprises at the same time thus the greenery budget would be separated between planting around emissions sources and protected urban areas such a greenery strategy is more effective because it requires a lower number of agent trees while achieving low values of the average daily pollution concentration the important result of the research is the calculated greater efficiency of using combined configurations of tree clusters consisting of different kinds of trees and different geometries of planting e g simple circle and arithmetic spiral in comparison with using homogeneous configurations tables 8 and 10 this allows solving considered optimisation problems through the pareto optimal solutions to provide the best decisions for reducing air pollution through agent trees which cannot be achieved by other means thus using the case study of the city of yerevan in armenia the agent based model allowed the pareto optimal solution to be determined using a genetic algorithm the system helped answer important questions about urban greenery including where plants should be located in a city which trees are better for reducing the air pollution concentration how many tree clusters are needed and what is the optimal greenery budget tables 7 10 and appendix a in comparison with other studies devoted to investigation of the influence of plants on air pollutant removal e g fuiii et al 2005 omasa et al 2002 bell and treshow 2002 this work focuses on the problem of interactions between air pollutants and greenery on a global scale including the whole city examination of the internal chemical processes of absorption of different air pollutants by different plants is outside the scope of the given work but optimisation of the greenery strategy and control of the allocation of different tree clusters in the city taking into account the dynamics of air pollutants is a core focus of the presented investigation such an approach allows for the simulation of the dynamics of daily air pollution concentration taking into account the combined influence of many other urban agents factories vehicles trees human populations to protect both selected urban areas and the whole city there is still much worthwhile work to be investigated in the future the most interesting and important research issues include 1 further specification of the model through including other agents particularly agent buildings which can be additional natural barrier for air pollutants 2 simulation of the strategy of vertical greening for tall buildings located near protected urban areas 3 further refinement of the methodology for estimation of the air pollution concentration through improving the model of interaction between different air pollutants and taking into account the wide set of climate effects e g temperature air humidity and wind turbulence and 4 further improving the optimisation problems through taking into account the population distribution and the remoteness of different population clusters from emission sources acknowledgements this work was supported by the russian foundation for basic research grant no 18 51 05004 real data provided by the cens http cens am were used for model validation appendix a optimal configurations of tree cluster planting around enterprises no of agent enterprises coordinates of tree cluster centres optimal configurations of tree clusters longitude latitude geometry planting of agent trees kind of trees distance between nearest agent trees metres radius of planting zone metres 1 44 452886 40 185255 arithmetic spiral maple 200 250 2 44 515191 40 186029 arithmetic spiral maple 200 250 3 44 452882 40 185183 arithmetic spiral maple 200 250 4 44 459556 40 183264 simple circle maple 200 250 5 44 446861 40 138059 arithmetic spiral maple 200 250 6 44 445827 40 174123 arithmetic spiral maple 200 250 7 44 566993 40 221453 arithmetic spiral maple 200 250 8 44 549951 40 203825 double circle maple 200 250 9 44 508014 40 211234 arithmetic spiral maple 200 250 10 44 521031 40 142562 arithmetic spiral maple 200 250 11 44 400036 40 152160 arithmetic spiral maple 200 250 12 44 518752 40 122132 double circle with variable distance oak 200 250 13 44 518006 40 120192 arithmetic spiral maple 200 250 14 44 485206 40 148622 arithmetic spiral maple 200 250 15 44 460888 40 144316 arithmetic spiral maple 200 250 16 44 427134 40 185669 arithmetic spiral maple 200 250 17 44 461560 40 162804 arithmetic spiral maple 200 250 18 44 489247 40 141717 arithmetic spiral maple 200 250 19 44 572290 40 164796 simple circle maple 200 250 20 44 532944 40 152099 simple circle maple 200 250 21 44 546347 40 195269 arithmetic spiral maple 200 250 22 44 500074 40 146946 arithmetic spiral maple 200 250 23 44 499244 40 144289 simple circle maple 200 250 24 44 507508 40 180254 simple circle maple 200 250 25 44 490130 40 179375 arithmetic spiral maple 200 250 26 44 523051 40 138597 arithmetic spiral maple 200 250 27 44 473250 40 150027 arithmetic spiral maple 200 250 28 44 429087 40 184458 arithmetic spiral maple 200 250 29 44 426235 40 187411 arithmetic spiral maple 200 250 30 44 507207 40 181125 arithmetic spiral maple 200 250 31 44 464907 40 191119 arithmetic spiral maple 200 250 32 44 529165 40 191523 arithmetic spiral maple 200 250 33 44 502422 40 173353 arithmetic spiral maple 200 250 34 44 493609 40 173447 arithmetic spiral maple 200 250 35 44 450710 40 138904 simple circle maple 200 250 36 44 525030 40 188061 simple circle maple 200 250 37 44 526432 40 181716 arithmetic spiral maple 200 250 38 44 502340 40 124714 double circle with variable distance oak 200 250 39 44 412927 40 197716 arithmetic spiral maple 200 250 40 44 593618 40 187313 arithmetic spiral maple 200 250 41 44 570344 40 190286 arithmetic spiral maple 200 250 42 44 549218 40 092105 arithmetic spiral maple 200 250 43 44 529402 40 210127 arithmetic spiral maple 200 250 44 44 523100 40 130684 arithmetic spiral maple 200 250 45 44 509941 40 125821 arithmetic spiral maple 200 250 46 44 498413 40 209968 arithmetic spiral maple 200 250 47 44 444196 40 186415 arithmetic spiral maple 200 250 48 44 481816 40 214362 arithmetic spiral maple 200 250 49 44 436104 40 138133 arithmetic spiral maple 200 250 50 44 533168 40 224208 arithmetic spiral maple 200 250 51 44 531171 40 152329 arithmetic spiral maple 200 250 52 44 536695 40 186394 arithmetic spiral maple 200 250 53 44 493192 40 143951 arithmetic spiral maple 200 250 54 44 558715 40 236147 double circle with variable distance oak 200 250 55 44 524415 40 169180 double circle with variable distance oak 200 250 56 44 569396 40 212193 arithmetic spiral maple 200 250 57 44 534373 40 145602 arithmetic spiral maple 200 250 58 44 466120 40 212868 arithmetic spiral maple 200 250 59 44 522173 40 190181 arithmetic spiral maple 200 250 60 44 572992 40 218547 arithmetic spiral maple 200 250 61 44 557955 40 174979 arithmetic spiral maple 200 250 62 44 465508 40 196313 arithmetic spiral maple 200 250 63 44 510599 40 133524 double circle with variable distance oak 200 250 64 44 516954 40 180059 simple circle maple 200 250 65 44 521633 40 198632 arithmetic spiral maple 200 250 66 44 494178 40 142460 simple circle maple 200 250 67 44 538600 40 226738 arithmetic spiral maple 200 250 68 44 562511 40 216550 arithmetic spiral maple 200 250 69 44 463694 40 143946 arithmetic spiral maple 200 250 70 44 498398 40 152903 arithmetic spiral maple 200 250 71 44 434629 40 212409 arithmetic spiral maple 200 250 72 44 464118 40 209714 arithmetic spiral maple 200 250 73 44 521437 40 171011 simple circle maple 200 250 74 44 513663 40 207475 simple circle maple 200 250 75 44 504072 40 164685 arithmetic spiral maple 200 250 76 44 521986 40 151798 arithmetic spiral maple 200 250 77 44 444853 40 142108 arithmetic spiral maple 200 250 78 44 616689 40 178025 simple circle maple 200 250 79 44 498021 40 144575 arithmetic spiral maple 200 250 80 44 510805 40 179524 arithmetic spiral maple 200 250 81 44 437445 40 171254 double circle with variable distance oak 200 250 82 44 497487 40 143960 arithmetic spiral maple 200 250 83 44 443283 40 163645 arithmetic spiral maple 200 250 appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 003 
