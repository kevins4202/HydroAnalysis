index,text
25640,active subspaces is a recently developed concept that identifies essential directions of the response surface of a model providing sensitivity metrics known as activity scores we compare activity scoring with the sobol and the morris global methods using a series of well known benchmark test functions with exactly computable sensitivities in the ranking context we analyse the accuracy efficiency impact of sampling method convergence rate and confidence interval estimation through both bootstrapping and replication heat maps that show both numerical rankings and underlying sensitivities with increasing sample size are introduced as a key visualization tool for such analysis convergence is also assessed through four previous measures activity scores match the total effect sensitivity index of sobol and the absolute mean of elementary effect of morris in most test cases activity scoring can be more computationally efficient its potential can be enhanced by expanding methods for approximating the gradient of the model function keywords global sensitivity analysis test function active subspace activity score variance based sobol method morris method 1 introduction global sensitivity analysis gsa has quickly gained attention in the past few years as a discipline coming of age for aiding model development and evaluation especially through its ability to identify and apportion relative sources of model factor influence on model outputs razavi et al 2021 saltelli et al 2021 this has led to development of a number of global sensitivity analysis methods constantine 2015 morris 1991 pianosi and wagener 2015 rakovec et al 2014 razavi and gupta 2016a saltelli et al 1999 sobol 1993 and associated software tools as covered in douglas smith et al 2020 the various methods tend to make different assumptions and or are based on different metrics for sensitivity leading to calls for the use of multiple methods in practice to inform conclusions about model sensitivity wagener and pianosi 2019 the active subspace concept constantine 2015 affords a recent global sensitivity analysis method that identifies the critical directions or the combination of model inputs in the parameter space with the help of eigenpairs in addition it can be utilized to help in constructing surrogate models e g cortesi et al 2020 vohra et al 2020 however due to its novelty active subspace methods of gsa have not been much applied indeed there is a limited number of papers that use them for ranking purposes e g diaz et al 2018 diaz 2016 leon et al 2019 vohra et al 2019 among all the studies of active subspace methods for gsa even fewer papers have investigated the efficiency and accuracy of the ranking provided by the active subspace method to render active subspace metrics somewhat comparable with other sensitivity metrics constantine et al constantine and diaz 2017 have transferred the results from the active subspace method into a sensitivity metric called the activity score as which mathematically relates to the sobol total effect sensitivity index the activity score has certain limitations regarding the requirement of a sufficiently distinct gap between eigenvalues for identifying the active subspace however it provides insight into sensitivity of model parameters and as will be seen it can efficiently provide a ranking of the parameters ranking in gsa aims to order the relative importance sensitivity of the model parameters in relation to their impact on the model output sarrazin et al 2016 this helps to identify significant parameters and reveal interactions saltelli et al 2008 sreedevi et al 2019 van werkhoven et al 2008 prioritise computational cost and efforts sin et al 2011 to obtain better estimates in the subsequent numerical or experimental process saltelli et al 2008 and support model development hartmann et al 2013 razavi and gupta 2016b have pointed out that model development relies heavily on robust and reliable ranking results and incorrect ranking results can lead to a poor representation of the subsequent reduced order model nossent and bauwens 2012 therefore ranking is the main objective of gsa considered in this paper and we compare how efficient the ranking provided by activity scoring compares to two other benchmark sensitivity analysis methods however we also introduce heat maps as a visualization tool that not only shows numerical rankings of parameters but that also portrays underlying sensitivities with increasing sample size as additional information in our study the popular quasi monte carlo sobol sequence antonov and saleev 1979 sobol 1967 is implemented to investigate the performance of the active subspace method and its activity score and that of the sobol metrics although guy et al 2020 used monte carlo sampling to generate activity scores they argued that quasi monte carlo sampling might be more computationally efficient the majority of active subspace studies reported either use monte carlo sampling abdo and abdel khalik 2016 cortesi et al 2020 loudon and pankavich 2017 ryken et al 2020 su et al 2021 vohra and mahadevan 2019 or largely make no mention of the sampling method used at all bittner et al 2020 bridges et al 2019 gilbert et al 2016 jefferson et al 2016 leon et al 2019 mola et al 2019 though there are a few studies that implement latin hypercube sampling comparison of the active subspace approach with others has rarely been reported although vohra et al vohra and mahadevan 2019 and godel et al gödel et al 2020 have compared the activity score with the total effect sensitivity index from the variance based sobol method sobol 1993 for a typical bottleneck scenario in crowd simulation with the optimal steps model concluding that these two sensitivity metrics coincide and confirm each other to investigate the performance of activity scoring more comprehensively we implement several test functions and compare the ranking obtained from active subspace sensitivity measures with the ones from the variance based sobol method and the morris method awad et al 2019 describe the morris method and sobol total effect sensitivity index as efficient in ranking parameters as they provide complementary information razavi and gupta 2016a have also pointed out that sobol and morris methods provide the most up to date rigorous approaches for global sensitivity analysis and hence these two methods should be used as benchmarks to demonstrate the performance of newly developed techniques on that account for the study of global sensitivity analysis the variance based sobol method and morris method are viewed here as the reference methods to demonstrate how the method in question compares with these two reference methods as emphasized by mai and tolson 2019 newly developed sensitivity analysis methods should be tested against analytical estimates of sensitivity measures from test functions accordingly we have chosen eight frequently used test functions from the literature to serve as benchmarking representing a useful range of possible model structures encountered in real life models moreover our work is inspired by the comparison study undertaken in mora et al 2019 where the pawn and sobol methods were compared with several test functions using a convergence analysis this paper is structured as follows section 2 provides the details of sensitivity analysis methods used for comparison section 3 presents the formulas of the test functions used section 4 introduces four previous measures to assess the convergence of ranking with sample size section 5 provides testing results using heat maps that visualize both sensitivity metrics and the numerical rankings of parameters for increasing sample size section 6 contains the discussion which includes confidence interval estimation of metrics by both bootstrapping and replicate sampling and section 7 gives the conclusions based on the testing results 2 sensitivity analysis methods as mentioned in the introduction we chose the variance based sobol method and the morris method as the reference sensitivity analysis methods to monitor the performance of activity scoring in this section we briefly introduce the three sensitivity analysis methods used in this paper 2 1 variance based sobol method assume that f x is a square integrable function defined in a unit hypercube 0 1 k the decomposition of f x can be written as 1 f x f 0 i 1 k f i x i i 1 k j i k f i j x i x j f 12 k x 1 x k where k is the number of parameters and f 0 is scalar defined as 2 f 0 0 1 k f x d x this decomposition of f x follows the analysis of variance anova if 3 0 1 f i 1 i s x i 1 x i s d x i q 0 for 1 i 1 i 2 i s k where x i q x i 1 x i s and all the terms are mutually orthogonal with respect to the integration by taking the variance of eq 1 the decomposition becomes 4 v f x 0 i 1 k v f i x i i 1 k j i k v f i j x i x j v f 12 k x 1 x k dividing both sides of eq 4 by the variance of function v f we obtain 5 1 i 1 k v f i v f i 1 k j i k v f i j v f v f 12 k x 1 x k v f i 1 k s i i 1 k j i k s i j s 12 k the term s i is known as the first order sensitivity index of the ith model parameter and it is defined as 6 s i v x i e x i f x i v f where e x is the expectation and x i refers to all model parameters except the ith parameter in addition to the first order sensitivity index the total effect sensitivity index s ti which measures the combination of the individual contribution of and the interactions with the ith model parameter is defined as 7 s t i e x i v x i f x i v f where 8 e x i v x i f x i 1 2 n j 1 n f a j f a b i j 2 is called the jansen1999 estimator jansen 1999 saltelli et al 2010 and f a j is the function value of the jth row of matrix a the variance v f is also approximated through the mean of n quasi monte carlo samples as 9 v f 1 n j 1 n f a j 2 1 n j 1 n f a j 2 where a b and a b i are the sample matrices generated based on the saltelli method saltelli 2002 and matrix a b i is a sample matrix where matrix a replaces the ith column with the ith column of matrix b the implementation of the variance based sobol method uses the version from the salib python library herman and usher 2017 for convenience we will use the notation s ti and st to represent total effect sensitivity index in the following text and figures correspondingly 2 2 morris method the basis of this method was proposed by morris 1991 and it measures the estimated mean of elementary effects of each model parameter through the parameter space by giving a baseline f x for a model parameter of interest x i this specific parameter is altered with a predefined perturbation δ and the elementary effect of this particular model parameter is defined as 10 e e i f x 1 x i δ i x k f x δ i the value that δ takes is p 2 p 1 where p is the number of the grid level the grid level number is reported to be 4 to 10 morris 1991 saltelli 1999 and p is set to 4 in several studies campolongo and saltelli 1997 campolongo et al 1999 naves et al 2020 however there is still an ongoing debate on the grid level number campolongo et al 2007 gan et al 2014 jaxa rozen and kwakkel 2018 sreedevi and eldho 2019 yang 2011 but this is out of the scope for this paper throughout the tests in this paper we keep the number of grid levels p as 4 with multiple baselines across the parameter space the estimated mean of the elementary effect μ can be calculated however the elementary effect may have opposite signs due to non monotonic model characteristics to avoid this compensation of opposite sign it has been proposed that the absolute mean of the estimated elementary effect be used instead campolongo et al 2007 which is given by 11 μ i 1 n j 1 n e e i j moreover it was shown in campolongo et al 2007 that the elementary effects could be compared with what the s ti of sobol provides and in saltelli et al 2008 that the μ is suitable for ranking for a better indication of the results of using μ we normalised μ with 12 μ n o r m i μ i i 1 k μ i 2 1 2 the implementation of the morris method also uses the salib python library herman and usher 2017 similar to the total effect sensitivity index we will use the notation μ and mu star to represent normalised absolute elementary effect μ n o r m i in the following text and figures correspondingly 2 3 active subspace method the active subspace method defines a k by k symmetric positive semi definite matrix c in which 13 c f x f x t ρ x d x w λ w t where w w 1 w k is the orthogonal matrix of eigenvectors and λ diag λ 1 λ k is the diagonal matrix of eigenvalues in decreasing order the eigenpairs satisfy the condition in which 14 λ i w i t c w i f x t w i 2 ρ x d x and λ i is zero if f x is constant along the direction of w i if a gap between eigenvalues can be identified such that λ n λ n 1 for some n k an n dimensional active subspace can be constructed with λ λ 1 λ 2 and w w 1 w 2 where λ1 are the first n eigenvalues and w 1 is the k by n eigenvector matrix this active subspace is a column span of the eigenvector matrix w 1 for application of active subspace analysis to computational models it can be rather difficult to obtain the exact form of the gradient of the model function f x thus the monte carlo method and various gradient approximation methods provide a way to estimate the matrix c as 15 c 1 n i 1 n f x i f x i t where x i is drawn independently according to ρ x for i 1 n in order to be comparable with other sensitivity analysis methods the activity score utilizes the obtained eigenvalues and eigenvectors and the activity score for the ith model parameter is defined as 16 α i j 1 n λ j w i j 2 for i 1 k for better visualization constantine et al constantine and diaz 2017 normalise the activity score to have a norm of 1 with 17 α n o r m i α i i 1 k α i 2 1 2 moreover it has been proven in constantine and diaz 2017 that the activity score gives an upper bound of the s ti 18 s t i 1 4 π 2 v f α i λ n 1 for the sake of simplicity we will use α i to represent α norm i in the remainder of the paper constantine et al provide a python library for active subspace analysis along with multiple methods to provide approximated gradients f x i to investigate the performance of each method of approximating the gradient we applied all three library methods in the experiments these approximation methods are the global linear model ols global quadratic model qphd and local linear model opg and for consistency we retain the same abbreviation of each model as defined by constantine the global linear model which is referred to as as ols in later sections estimates a one dimensional subspace to approximate the gradient f x i the global quadratic model which is referred to as as ophd has a similar concept as the approach used for generating principal hessian directions based on a global quadratic model li 1992 however the implemented version here uses the average outer product of the gradient of the quadratic model constantine et al 2016 the local linear model which is referred to as as opg is known as the outer product of the gradient method hristache et al 2001 and it builds a local linear model for a subset of neighbouring points of each pair of model input output more details of these approximation models can be found in constantine s code and book constantine et al 2016 constantine 2015 for all test functions implemented below except the modified sobol g function their gradients can be obtained analytically thus the analytical activity score will also be provided along with the results from each gradient approximation model to further investigate the performance of activity scoring we implemented a so called dummy parameter as an adjunct to all of the test function parameters just for the active subspace method several studies castaings et al 2012 khorashadi zadeh et al 2017 have implemented the dummy parameter approach to obtain the threshold of the approximation error for identifying if one model parameter is truly insensitive or not and godel et al gödel et al 2020 also used a dummy parameter referred to there as a control parameter to test the detection of an absence of an impact 3 test functions test functions have been purposely designed in sensitivity studies to simulate specific model characteristics that researchers could face when studying real world problems linearity or monotonicity being simple examples thus many studies have involved widely used test functions to show the efficiency and capability of particular sensitivity analysis methods we have assembled eight popular test functions from the literature most having several settings and configurations to modify the function structure and influence of parameters active subspace analysis requires the model parameters to be normalised to the range 1 1 so we will only focus on test functions using uniform parameter distributions with a known range for each of the test functions the model parameter vector x x 1 x 2 x k t consists of k model parameters where x i is the ith model parameter each test function is introduced in the following subsections specifying its form and ranges of model parameters 3 1 test function 1 ishigami homma function ishigami and homma introduced this function ishigami and homma 1990 due to its analytical tractability and non additive properties jaxa rozen and kwakkel 2018 it has been one of the most used test functions for the comparison of sensitivity analysis methods cuntz et al 2015 jaxa rozen and kwakkel 2018 mora et al 2019 razavi and gupta 2019 wang et al 2020 wei et al 2013 ziehn and tomlin 2009 its benchmark nonlinear and non monotonic features with the presence of interaction effects do and razavi 2020 also make it attractive for testing the function is defined as 19 f x sin x 1 a sin 2 x 2 b x 3 4 sin x 1 where x i is uniformly distributed within π π a 7 and b 0 1 3 2 test function 2 sobol g function davis and rabinowitz 1984 introduced the g function initially with coefficients a i 0 and later it was used with nonzero coefficients in the study of saltelli and sobol archer et al 1997 saltelli and sobol 1995 thus the nonzero coefficient version is henceforth called the sobol g function it has been used for testing sensitivity analysis methods in many studies glen and isaacs 2012 horiguchi et al 2021 mora et al 2019 razavi and gupta 2019 sun et al 2021 wang et al 2020 ziehn and tomlin 2009 its popularity is based at least partly on it being a product of univariate functions that encompass a broad spectrum of complexity cuntz et al 2015 the value of the coefficient a i determines the sensitivity of its corresponding model parameters the more sensitive model parameters having a lower value of a i the function is defined as 20 f x i 1 k 4 x i 2 a i 1 a i where x i is uniformly distributed within 0 1 k is the number of model parameters a 0 1 4 5 9 99 99 99 and a i is the ith value of a 3 3 test function 3 modified sobol g function saltelli also created the modified sobol g function to involve an extra layer of complexity using a so called floor function saltelli et al 2010 this modified sobol g function has been used in several papers for benchmarking different sensitivity analysis methods cuntz et al 2015 mai and tolson 2019 mora et al 2019 the function is defined as 21 f x i 1 k 1 α i 2 x i δ i x i δ i 1 i α a i 1 a i where is the floor function x i is uniformly distributed within 0 1 α and δ are the curvature and shift parameters respectively a i are coefficients and k is the number of model parameters the shift parameter δ is randomly chosen as it does not affect the propagation of uncertainties mora et al 2019 the floor function takes the integer part of the value inside the floor function brackets the inclusion of this floor function makes the modified sobol g function discontinuous and this prevents calculation of the analytical gradient along with the analytical activity score for the coefficients we follow saltelli et al 2010 and use the six sets of coefficients there with some modifications to the number of parameters g1 α 1 a 0 0 9 9 9 9 9 g2 α 1 a 0 0 1 0 2 0 3 0 4 0 8 1 g3 α 0 5 a 0 0 9 9 9 9 9 g4 α 0 5 a 0 0 1 0 2 0 3 0 4 0 8 1 g5 α 2 a 0 0 9 9 9 9 9 g6 α 2 a 0 0 1 0 2 0 3 0 4 0 8 1 3 4 test function 4 bratley function the bratley function also called the k function was introduced by bratley et al 1992 and is designed to have a higher degree of interaction with increasing index i of the model parameters horiguchi et al 2021 this function spans a range of parameter sensitivities in a convex manner which mimics real problems quite well cuntz et al 2015 thus it has been utilized in several studies cuntz et al 2015 mora et al 2019 the function is defined as 22 f x i 1 k 1 i j 1 i x j where x j is uniformly distributed within 0 1 and k is the number of model parameters 3 5 test function 5 morris function this function was presented by morris 1991 in 1991 and it has been used for testing in various studies cuntz et al 2015 sudret and mai 2015 later in 2006 morris et al modified the function morris et al 2006 and this modified version also used in horiguchi et al 2021 will be used in the paper the function is defined as 23 f x α i 1 k x i β i 1 k 1 x i j i 1 k x j where x i and x j are uniformly distributed within 0 1 α 12 6 0 1 k 1 β 12 10 k 1 and k is the number of model parameters 3 6 test function 6 friedman function the friedman function combines parameter interaction and non linearity horiguchi et al 2021 and thus poses a challenging problem to the testing of sensitivity analysis the function is defined as 24 f x 10 sin π x 1 x 2 20 x 3 0 5 2 10 x 4 5 x 5 x 6 0 1 x 7 where the x i are uniformly distributed within 0 1 3 7 test function 7 linear function this linear function was introduced by saltelli et al 2000 to provide basic testing it is defined as 25 f x i 1 k x i where x is uniformly distributed within x i σ i x i σ i and k is the number of model parameters the x i and σ i are defined as x i 3 i 1 σ i 0 5 x i 3 8 test function 8 sobol levitan function the sobol and levitan function was introduced in sobol and levitan 1999 similar to the sobol g function the magnitude of the coefficient impacts the sensitivity of the corresponding model parameters this test function is also presented in saltelli et al 2000 and is defined as 26 f x exp i 1 k b i x i i k where x is uniformly distributed within 0 1 b i is the coefficient and i k is defined as 27 i k i i k exp b i 1 b i two b i vector configurations for this function are considered k 7 and b 1 1 5 b 2 b 7 0 9 k 15 and b i 0 6 for 1 i 10 0 4 for 11 i 15 4 ranking measures in order to provide indicators regarding convergence aspects of the ranking results for these test functions we implemented four ranking measures from the literature it is worth noting that these four ranking measures have been applied only rarely in sensitivity analysis studies other than in the papers that first introduced them in searching through available python libraries implementation of the ranking measures was seen to be currently lacking and so we coded a python version of these measures which is available through github https github com xifus saconvergenceanalysis we also test the efficiency and accuracy of each ranking measurement for our test functions in this paper the ranking is in decreasing order with rank 1 being the most sensitive parameter 4 1 the ranking measure from sarrazin et al in order to address critical choices and gaps in convergence studies of sensitivity analysis sarrazin et al 2016 proposed criteria to quantify the convergence of ranking results from sensitivity analysis methods their ranking statistic modifies the spearman s rank correlation coefficient spearman 1904 to include a better weighted version than the one by dancelli et al 2013 and it can be written as follows 28 ρ s j m i 1 k p i j p i m max j m s i j s i m 2 i 1 k max j m s i j s i m 2 where p i j and p i m are the positions ranks of the ith model parameter using jth and mth bootstrap replicate resamples and s i j and s i m are the sensitivity measures respectively sarrazin et al calculate the ρ of every possible pair of resamples and summarize the statistics into an indicator stat ranking q 0 95 ρ s j m to quantify the convergence of ranking results the ranking result is considered to be converged when stat ranking 1 if stat ranking 1 the difference in the resamples for the most sensitive parameters is less than or equal to one position on average in this paper we will use the term sarrazin ranking stat ranking or simply stat ranking for convenience 4 2 position factor the concept of position factor was first introduced by ruano et al 2012 and was later modified by cosenza et al 2013 to take the absolute value of the difference in the ranks the modified version is defined as 29 position factor i 1 k p i j p i m μ p i j p i m where p i j and p i m are the positions ranks of the ith model parameter obtained by using samples j and samples m μ p i j p i m is the average of p i j and p i m and k is the number of model parameters if the value of the position factor is zero the ranks provided by the two different samples are the same 4 3 top down coefficient of concordance the top down coefficient of concordance tdcc measures the level of agreement among multiple rankings derived either by different sensitivity analysis methods or different resamples within each model run iman and conover 1987 it emphasizes the agreement on the most important model parameters whilst de emphasizing less important model parameters helton et al 2005 yang 2011 iman and conover 1987 employed tdcc with a so called savage score which is used in this paper and is given by 30 t d c c i 1 k j 1 r s s s i m 2 r 2 k r 2 k i 1 k 1 i the savage score ss is defined as 31 s s s i m i p i m k 1 i where s i m is the sensitivity measure of the ith model parameter obtained by the mth bootstrap replicate resample p i m is the corresponding rank r is the number of bootstrap replicate resamples and k is the number of model parameters the experiment has a high reproducibility if tdcc is close to 1 otherwise this experiment is unlikely to obtain similar results under the same settings if tdcc decreases away from 1 another interpretation is that a high tdcc score represents a high agreement on the ranking of important parameters yang 2011 in the results and discussion sections we will consider reproducibility in terms of rankings derived by different resamples 4 4 reliability incorporating a bootstrap method razavi and gupta 2016b generated estimates of the reliability of the inferred input rankings for each of the model parameters this reliability provides a percentage of how many resamples give the same rank for one specific model parameter as the original sample in extending this measurement so as to work for factor groupings sheikholeslami et al 2019 pointed out that this estimation measure should be considered as a check on robustness rather than reliability here we keep the nomenclature of reliability for this measure so as to be consistent with razavi and gupta 2016b the reliability of the factor ranking for the ith model parameter is given by 32 r e l i j 1 n b ψ p i p i j n b where n b is the number of bootstrap resamples and p i and p i j are the positions ranks of the ith model parameter obtained by the original sample and the jth bootstrap resample respectively the function ψ is defined as ψ a b 0 a b 1 a b the ith model parameter is fully reliable with respect to the sample variability when rel i 100 5 results in order to assess the performance of activity scoring in ranking we adopt the sequential approach of gradually increasing sample size and observe how the sensitivity measures change with differing numbers of model evaluations according to owen 2020 the proper convergence rate of a quasi monte carlo sequence specifically a sobol sequence may not be maintained if the sample size is not a power of 2 and similarly the number of samples that are skipped should be a power of 2 for a fair comparison in terms of computational effort between the sensitivity analysis methods we keep the number of model evaluations the same across all methods where the number of model evaluations m 256 1024 4096 16 384 65 536 the activity score implemented here takes the same number of samples n as model evaluations with m n since the three implemented gradients are calculated externally with existing model outputs without needing to access the model again however the s ti and the μ take the number of model evaluations to be m n k 1 where n is the number of samples trajectories and k is the number of parameters therefore we modified the number of model parameters for most of the test functions so that the number of model evaluations m and the number of samples n can both be a power of 2 and this retains the proper convergence rate of the quasi monte carlo sequence in the following the ranking results for the two methods generating s ti μ and for activity scores with the three different gradient approximation models ols qphd opg are shown and discussed for each test function through the form of heat maps that are grouped by the five methods however the supplementary material contains results in the form of tables from applying the four ranking measures presented in section 4 also for each test function it contains additional results of computing confidence intervals for those measures and their convergence with sample size on the basis of bootstrapping and replication the presentation in this section is therefore quite detailed and some readers may prefer to skip to the discussion section for a distillation of the main points and then return to this section for specifics the heat maps in the main text indicate both sensitivity and numerical ranking according to the sensitivity indices as opposed to the four ranking measures the last column of each heat map shows the analytical sensitivity measures for s ti μ and the activity score as thus the analytical s ti and as measures are manually calculated by definition while the analytical μ is obtained through a large m 1 0 7 model runs the heat maps have several other features the horizontal x axis is labeled with each of the sensitivity analysis methods each method has a grouping of six results where the first five columns are the computational results against increasing sample size and the sixth column shows the exact result each row of the vertical y axis corresponds to a model parameter with x 1 from the bottom to the dummy parameter at the very top the top x axis annotates each column with the number of model evaluations m and a specific column indicates the parameter ranking result obtained for that m thus each of the model parameters in each cell is annotated with a number from 1 to k showing the rank in terms of sensitivity importance for this model parameter where rank 1 is the most important or most sensitive parameter the colour of each cell in the heat maps represents the magnitude of the sensitivity measures index value or the value of activity score and this is explained by the colour bar at the right of each heat map where black ish colour for instance shows the sensitivity measure to be of value 0 8 or higher and yellow ish colour shows the sensitivity measure to be near 0 to better present the results of μ in the heat maps we normalised the values of μ to have a maximum value of 1 for s ti and μ the rank of the dummy parameter is shown as a dash rather than a number as the dummy parameter is not applicable for these methods calculation of the sarrazin ranking stat ranking tdcc with the savage score and reliability is made possible with 1000 bootstrap resamples and full tables of results from the ranking measures can be found in sections 4 7 of the supplementary file due to the length of this paper we will describe some key values from the different ranking measures in words rather than showing the full table here furthermore generation of the samples uses the sobol sequence generator implemented in the salib python library herman and usher 2017 for both the variance based sobol method and active subspaces the morris method uses a specific sampling strategy so it uses its own sample generator which is also implemented in the salib python library this section is split into several subsections to exhibit the different sensitivity results obtained for each test function 5 1 test function 1 ishigami homma function for the ishigami homma function the s ti measure gives correct ranking results that are stable from 256 model evaluations onwards fig 1 the slight change of colours indicates change in the index value but this does not affect the ranking results in the supplementary materials both the sarrazin ranking stat ranking and position factor ranking measures show agreement in the ranking between resamples for the s ti from 256 model evaluations onwards the stat ranking is 0 959 and position factor indicates that the ranking did not change from 256 to 1024 model runs the tdcc scores are 0 748 and 0 777 for 256 and 1024 model runs correspondingly and this indicates a better than average 0 5 reproducibility with 4096 model evaluations tdcc has a score of 0 993 which suggests a high reproducibility and these match fig 1 where the colour stabilizes from 4096 model evaluations the reliability of all three model parameters is above 80 from 256 model evaluations the μ of the morris method does not achieve a stable and correct ranking result until reaching 1024 model evaluations the un converged ranking result at 256 model evaluations is also identified by the ranking measures with the sarrazin ranking stat ranking at 1 369 and position factor at 1 333 and these values indicate a disagreement in the ranking between resamples the value of the reproducibility for the morris method is at around 0 5 until 16 384 model evaluations but the ranking has already stabilized at 1024 model evaluations with the confirmation of the sarrazin ranking stat ranking and position factor the reliability of x 1 and x 2 is slightly lower than 60 at 256 model evaluations but increases right after and this matches the observation of the sarrazin ranking stat ranking however μ ranks x 2 to be more important than x 1 different from that provided by the s ti as seen from fig 1 the activity score calculated from the global linear model as ols ranks x 1 to be the most critical parameter and other model parameters are all close to zero with yellow ish colour this activity score ranks the dummy parameter as the second most important parameter but this is likely due to the approximation error within activity scores as the sensitivity measures for x 2 and x 3 are too small judging from fig 1 the ranking results do not change with increase in the m however the sarrazin ranking stat ranking picks up the disagreement between resamples with a value of 1 448 for 256 model evaluations and the stat ranking value drops to 0 195 at 1024 model evaluations similarly for the reliability x 1 has less than 80 reliability at 256 model evaluations but the percentage rises to 100 with increasing model runs on the other hand position factor does not find a difference between the ranking results for 256 versus 1024 model evaluations as also indicated by the heat map tdcc gives a value larger than 0 5 reproducibility at 256 model evaluations and improves quickly as the number of evaluations increases the global linear model identifies a one dimensional active subspace for the ishigami homma function the first eigenvalue indicates that the most important direction in the parameter space is orthogonal to both x 2 and x 3 which returns a near zero activity score for these two parameters similarly the activity score calculated from the global quadratic model qphd ranks x 1 as the most important parameter the difference is that the active subspace with a global quadratic model identifies a two dimensional active subspace and this ranks an extra model parameter as more important than other parameters with low model evaluations 256 and 1026 specifically as qphd ranks the dummy parameter as the second most sensitive parameter indicating insufficient samples have been taken from 4096 model evaluations onwards this activity score starts to provide converged ranking results with x 1 x 2 other model parameters on the other hand the sarrazin ranking stat ranking is 0 628 for this activity score and this indicates converged ranking results it is obvious that this is false confidence as the dummy parameter should not be ranked more important than the actual model parameters with an increase of model evaluations sarrazin ranking stat ranking gives a value of more than 1 from 1024 to 65 536 model evaluations and this shows a strong disagreement between resamples even with large m moreover the reliability of all the model parameters is larger than 80 only at 65 536 model runs the position factor only picks up the different ranking between 1024 model evaluations and 4096 model evaluations in terms of reproducibility as qphd has the highest value of 0 784 at 256 model evaluations and the lowest value of 0 496 at 4096 model evaluations the activity score as opg ranks the dummy parameter as the most crucial parameter at 256 model evaluations and x 2 is ranked first with a higher sample size by approximating the gradients with the local linear model at 65 536 model evaluations x 3 and x 1 are higher with more extensive sensitivity measures than previous results this ranking result is different from that of either as ols or as qphd but the most critical parameter is the same as the morris method despite the shifting of the ranks stat ranking indicates converged ranking results for any number of our model evaluations the most prominent stat ranking is 0 944 at 65 536 model evaluations and this is still less than 1 on the other hand position factor has a high value from 256 to 1024 model evaluations and from 16 384 to 65 536 model evaluations at all numbers of model evaluations tdcc gives values above 0 85 and thus shows a high reproducibility the reliability of x 2 is near 100 from 1024 model runs and this value is higher than other model parameters for the analytical exact activity scores the model parameter x 2 is the most important parameter and the activity scores for the other two model parameters are zero 5 2 test function 2 sobol g function for this test function fig 2 shows that the s ti measure is able to provide stable and correct ranking results for parameters with high sensitivity at 256 model evaluations with x 1 x 2 x 3 x 4 moreover the index values for x 5 to x 7 are close to zero and the small fluctuations due to the approximation error cause the ranks of x 5 to x 7 to change at different numbers of model evaluations even with the small fluctuations the sarrazin ranking stat ranking is zero at 256 model evaluations this stat ranking is zero consistently with increase of sample size thus showing agreement between resamples similarly to the position factor the reproducibility is also extremely high with a value of 0 985 from tdcc as with the s ti the μ from the morris method also correctly ranks x 1 x 2 x 3 x 4 from 256 model evaluations onwards this ranking result is consistent with the coefficients of the sobol g function where a small coefficient indicates a larger influence on the model output the sarrazin ranking and position factor also demonstrate a converged ranking at any m with high reproducibility the activity score with the global linear model as ols acts interestingly for the sobol g function as it ranks x 7 as the most important parameter for all model evaluation numbers except m 16 384 moreover the colour which shows the magnitude of the activity score is inconsistent with change in the m thus it is hard to discern the status of the convergence of the ranking from fig 2 the stat ranking has a value of nearly 5 2 across all model evaluations portraying a strong disagreement in the ranking between resamples the position factor also indicates divisions between different numbers of model evaluations and the reproducibility metric is near 0 indicating an extremely low reproducibility for any m activity scoring with either the qphd or opg setting ranks x 1 as the most important model parameter and this is the same ranking obtained for both the s ti and μ the ranks however constantly shift for model parameters other than x 1 and the magnitudes of the activity scores are close to zero the shift in rankings is due to the slight fluctuations of the approximation errors the sarrazin ranking stat ranking values are near zero for both as qphd and as opg and this represents a stable ranking between the resamples at each m however the position factor is highly impacted by these small fluctuations with a value greater than 2 5 in most cases the reproducibility is also only slightly larger than 0 5 for as qphd and as opg activity scoring with the local linear model identifies x 2 as the second most important parameter with the activity score being slightly higher than x 3 for the dummy parameter all of the model parameters of as ols have a reliability around 20 and this explains the high sarrazin ranking stat ranking for other methods the reliability of x 1 to x 4 for s ti and μ is 100 from 256 model runs onwards and this is the same for x 1 of as qphd and as opg the analytical activity score is similar to the previous two activity scores where x 1 is ranked as most important whilst other model parameters have activity scores of zero 5 3 test function 3 modified sobol g function the modified sobol g function has six different coefficient settings controlled by the value of α and δ observing the ranking results provided by each of our sensitivity analysis methods g1 g3 and g5 can be classified into the same group and g2 g4 and g6 into another group for results presented in this subsection only g5 and g6 are chosen to act as representative ones whereas figures from other coefficient settings are included in section 1 of the supplementary file this test function has a floor function component resulting in the test function being discontinuous and so the gradient cannot be obtained analytically for determining the exact activity score the ranking results for the modified sobol g function with the g5 coefficient setting are shown in fig 3 as with g1 and g3 x 1 and x 2 are the two most important model parameters with other model parameters much less sensitive the s ti μ and the activity score using the global quadratic model as qphd successfully pinpoint x 1 and x 2 to be the essential model parameters from 256 model evaluations onwards the activity scores with a global linear model as ols and local linear model as opg also identify x 1 and x 2 as the most important parameters at 256 model evaluations however the ranks change as model evaluations increase to 1024 indicating a fortuitous result for these two activity scores at 256 model evaluations there is a constant 10 difference between x 1 and x 2 in their s ti up until 16 384 model evaluations whereas x 1 and x 2 should have the same sensitivity for the morris method μ does not show the first two model parameters to be at a similar sensitivity until 65 536 model evaluations is reached at 4096 model evaluations μ assesses these two model parameters to be close in importance but the difference is enlarged at 16 384 model evaluations activity scoring with the global quadratic model as qphd assesses the same importance for x 1 and x 2 at 1024 model evaluations with only a 0 1 difference in terms of activity score and this difference shrinks as m increases in regard to the least sensitive parameters as qphd recognizes these with near zero sensitivity where their activity scores are less than or equal to 0 001 from 1024 model evaluations onwards whilst total effect sensitivity indices are approximately 0 01 for these parameters at the same m as the sensitivity measures are small the ranks of the least sensitive parameters are quickly impacted by the approximation errors and this is reflected in the heat map of fig 3 the sarrazin rankings stat ranking for s ti μ and as qphd are approximately equal to but not less than 1 at all numbers of model evaluations except μ at 16 384 model evaluations where the stat ranking is 0 025 this phenomenon is likely due to the rank shifting of x 1 and x 2 between the resamples for as ols and as opg the stat ranking is almost 1 at 65 536 model evaluations and 4096 model evaluations respectively but the stat ranking does not drop below a value of 1 this behaviour of stat ranking can also be explained by the reliability measure as the stat ranking is close to 1 when the reliability of the two most important parameters x 1 and x 2 is equal to or greater than 60 for the position factor as qphd has a score of 0 667 at 16 384 model evaluations but in every other case has a value larger than 1 with respect to reproducibility as ols and as opg have low reproducibility at specific model evaluations but every other sensitivity analysis method has a reproducibility higher than 0 75 the sensitivity properties of the modified sobol g function change accordingly with its coefficients for g2 g4 and g6 the coefficients are set to have importance of model parameters in the order of x 1 x 2 x 3 x 7 we use the g6 setting as the representative here and fig 4 shows the ranking results when the g6 coefficient setting is applied to this test function the s ti index does not yield a converged ranking result even at 65 536 model evaluations the ranking from s ti is persistently changing with a different m however by comparing with the exact analytically derived result s ti correctly recognizes that x 1 to x 3 are the most important model parameters from 1024 model evaluations onwards for the morris method μ gives the correct ranking at 65 536 model evaluations however unlike s ti μ does not pinpoint the first three model parameters as essential parameters early in the model runs as with s ti as ols and as qphd identify the first three model parameters as more sensitive than others at 1024 model evaluations moreover as ols gives the correct ranking at 16 384 model evaluations whereas as qphd gives a stable ranking result at the lower number of 1024 model evaluations the activity score with the local linear model as opg on the other hand does not provide a sensible ranking as it even identifies the dummy parameter as the most crucial parameter at 16 384 model evaluations for the sarrazin ranking stat ranking only as ols and as qphd have scores lower than 1 at 16 384 model evaluations but stat ranking is much higher than 1 before this point the percentage of reliability results also confirm the observations of stat ranking with more than 80 for as ols and as qphd from 16 384 model runs onwards across all model parameters with an increase to 65 536 model evaluations stat ranking for μ is less than 1 and as ols and as qphd have stat ranking s of 0 which indicates the agreement of ranking results between resamples the reliability of all model parameters using the μ measure is only above 80 at 65 536 model runs which is the same as that provided by stat ranking the position factor has a value of 0 for as qphd only at 4096 model evaluations and the value is much higher than 1 for other sensitivity analysis methods also as qphd is the only method with a consistent high reproducibility scoring higher than 0 75 from 1024 model evaluations 5 4 test function 4 7 bratley k function and linear function for the bratley function fig 5 all of the ranking results from activity scoring yield the same rank for the highly sensitive model parameters as s ti and μ and the ranks stabilize from 256 model evaluations these ranking results agrees with the analytical results for important parameters from fig 5 s ti μ and as opg still exhibit change in values for some model parameters as shown by the change of colours but this does not impact the ranking for less sensitive model parameters x 5 to x 7 x 5 stays as the 5th important parameter at all times but x 6 and x 7 change ranks constantly for all of the sensitivity analysis methods other than the sobol method both as ols and as opg rank the dummy parameter to be more important than either x 6 or x 7 early on and this is likely due to the near zero sensitivity for these two model parameters both stat ranking and the position factor indicate a converged ranking for all of the sensitivity analysis methods from 256 model evaluations with high reproducibility tdcc 0 97 for the reliability measure it appears that there is a reliability of above 60 for relatively important model parameters according to a converged value for the sarrazin ranking measures stat ranking for the linear function fig 6 similar to the bratley function fig 5 all of the sensitivity analysis methods rank the model parameters the same from 256 model evaluations onwards the activity scores with different gradient approximation methods yield the same ranking results as the analytical activity score the sarrazin ranking stat ranking and the position factor all are ideally zero the reproducibility is 1 for tdcc and the reliability is 100 for all methods and model parameters 5 5 test function 5 morris function judging from fig 7 all of the sensitivity analysis methods show that the 7 model parameters have similar sensitivities and the analytical results confirm this the s ti has clashing colours for some of the model parameters until 4096 model evaluations and it is not until 65 536 model evaluations is reached for μ to have consistent colour for all of the model parameters the activity score as opg does not have consistent colours through all model parameters with the local linear model even at 65 536 model evaluations on the other hand both as ols and as qphd already provide consistent colours for all model parameters at either 256 or 1024 model evaluations in addition all of the activity scores can identify the dummy parameter due to the frequent switching of the ranks between all of the model parameters the sarrazin ranking stat ranking and the position factor are unable to conclude there is convergence of the rankings for any of the sensitivity analysis methods also the reproducibility is relatively low for any of the sensitivity analysis methods as well the reliability of all model parameters is also bouncing around 20 5 6 test function 6 friedman function the ranking results for the friedman function can be seen from fig 8 in accord with the exact rankings both s ti and μ rank x 4 as the most important parameters with x 2 and x 3 second and the remaining parameters are in the order of x 3 x 5 x 6 x 7 the s ti identifies x 1 and x 2 to be of similar sensitivity cells with the same colour from 1024 model evaluations onwards but μ achieves this at 256 model evaluations even though the ranking shifts for x 1 and x 2 for activity scoring both the global linear model version as ols and global quadratic model as qphd rank x 4 x 1 and x 2 of the same importance as s ti and μ however as ols and as qphd rank x 3 as the least important model parameter moreover the dummy parameter appears to have a higher rank than x 3 indicating a near zero sensitivity for x 3 for as qphd x 1 and x 2 have disagreement in colour at 256 model evaluations using the local linear model the activity score cannot provide a consistent ranking at any m as opg can identify that x 1 x 2 and x 4 are the most important model parameters but it cannot differentiate the exact ranking between these three model parameters as opg also ranks x 3 as less sensitive the analytical activity score ranks both x 1 and x 2 as the most important model parameters with x 4 next and this ranking result is different compared to any of the previous ranking results similarly the analytical activity score classifies x 3 as non important the sarrazin ranking indicates that the s ti reaches a converged ranking result from 4096 model evaluations onwards and both μ and as qphd reach converged ranking results at 1024 model evaluations as observed from fig 8 as ols achieves convergence at 256 model evaluations but as opg has a stat ranking value higher than 1 even at 65 536 model evaluations in terms of reliability using s ti x 4 reaches 80 and x 3 and x 5 reach 60 at 1024 model runs also for μ x 1 and x 2 have reliability more than 60 and that of x 3 is higher than 80 from 1024 model runs confirming the change in stat ranking for the activity scores it is hard to conclude anything from the reliability of each model parameter the position factor shows a disagreement for as ols and as qphd at 1024 to 4096 model evaluations and 4096 to 16 384 model evaluations respectively due to the rank shifts between the two important model parameters x 1 and x 2 on the other hand the position factor gives a value higher than 2 for as opg from 4096 to 65 536 model evaluations surprisingly the reproducibility is relatively high for the activity score approximated by the local linear model with a value more than 0 8 and the value is even higher than 0 9 for 1024 and 65 536 model evaluations despite the fact that the ranking is unstable 5 7 test function 8 sobol levitan function the sobol levitan function has two configurations which change the number of model parameters and the coefficients the ranking results of the first configuration can be seen from fig 9 and the second configuration from fig 10 for the first configuration fig 9 x 1 has a coefficient value of 1 5 higher than the coefficient value 0 9 of other model parameters and this makes x 1 to be the most important parameter followed by other parameters with the same sensitivity from fig 9 all of the sensitivity analysis methods can correctly rank x 1 as the most important model parameter for x 2 to x 7 total effect sensitivity analysis and the μ of morris yield similar sensitivity for these parameters but not until 16 384 model evaluations the sensitivity indices of the sobol method have the largest difference of 60 between x 2 and x 4 at 1024 model evaluations and this difference drops to 17 5 at 4096 model evaluations where the difference uses the smallest sensitivity index among x 2 to x 7 as the base for μ the difference is 14 5 at 1024 model evaluations and 12 5 at 4096 model evaluations between x 7 and x 5 and between x 5 and x 7 correspondingly both as ols and as qphd provide similar sensitivities for model parameters x 2 to x 7 at 1024 model evaluations the largest gap is for as ols at 4 04 for 1024 model evaluations between x 3 and x 6 and it is 6 64 for as qphd between x 3 and x 5 at the same m the activity score with the local linear model is unable to provide a stable ranking for this test function as well where x 4 and x 6 appear to have higher sensitivities than other model parameters even at 65 536 model evaluations activity scores other than as opg match the ranking results with the analytical activity score in terms of convergence the sarrazin ranking using stat ranking is unable to conclude convergence which is similar to the morris test function as the stat ranking is higher than 1 other than for as opg at 65 536 model evaluations the position factor is also unable to show any sign of convergence with x 2 to x 7 being of similar sensitivity for tdcc reproducibility is higher than average for any sensitivity analysis methods at any m for the reliability activity scores have 100 reliability for x 1 from 256 model evaluations onwards yet s ti and μ require at least 1024 model evaluations to reach 100 for other model parameters their reliability bounces between 20 and 40 and this low reliability matches the results of the sarrazin ranking stat ranking the first 10 model parameters have the same coefficient in the second configuration fig 10 whereas the other 5 model parameters have a different though the same value the difference in coefficients makes the first 10 parameters more sensitive than the other 5 parameters judging from fig 10 both μ and activity scores point out that x 11 to x 15 are the least important parameters at 256 model evaluations and the activity scores successfully discover the dummy parameter the s ti locates the least sensitive group from 1024 model evaluations onwards as it ranks x 12 to be more important than x 3 however some of the model parameters are still wrongly classified into the least sensitive group by some sensitivity analysis methods for s ti the index value of x 5 is 0 102 x 2 is 0 068 and x 12 is 0 038 at 1024 model evaluations these indices give an impression of three different sensitive groups rather than two at 4096 model evaluations the largest difference for the sensitive group is x 2 and x 3 with a value of 0 029 which is much better than the case at 1024 model evaluations likewise μ provides a consistent sensitive grouping at 4096 model evaluations since x 4 has a value of 0 229 x 9 is 0 308 and x 11 is at 0 196 for 1024 model evaluations for activity scoring with the global linear model the as of the sensitive group ranges from 0 282 to 0 313 whilst the insensitive group ranges from 0 122 to 0 142 at 1024 model evaluations for as qphd these ranges are 0 287 0 314 and 0 123 to 0 151 respectively therefore it suffices to say that as ols and as qphd behave better than s ti and μ for the activity score with a local linear model the sensitive group ranges from 0 280 to 0 321 the less sensitive group ranges from 0 130 to 0 150 comparable with the results from other activity scores the activity scores with three gradient approximation methods correctly pinpoint the dummy parameter matching ranking results with the analytical activity score identically to the case of the first configuration the sarrazin ranking stat ranking shows a significant disagreement between resamples for all of the sensitivity analysis methods and the position factor also fails to conclude anything due to the shifting of ranks between the different m for the reproducibility it is slightly above 0 5 on average for all of the methods the reliability of all model parameters fluctuates between 10 and 60 but less than 60 and this agrees with the high stat ranking 6 discussion for the more novel gsa method of activity scoring it is both intuitive and clear from the test function results that the method used for calculating the gradient can have a significant impact on rankings obtained the ishigami homma and friedman functions are the two test functions where all the three estimated activity scores provide different ranking results on specific model parameters compared to the analytical exact activity scores the differences in the gradients calculated affect the matrix c and the eigenpairs which ultimately causes the activity scores to be distinct for the ishigami homma function fig 1 the difference in the ranking between the estimated activity scores and the analytical activity score is mainly caused by the term x 3 4 sin x 1 if this term is changed to either x 3 3 sin x 1 or x 3 2 sin x 1 the estimated activity scores and the analytical activity score would provide the same ranking thus the global linear model and global quadratic model are not able to approximate such a high order response surface well it is this limitation that causes the approximated activity scores to rank parameters differently for the friedman test function fig 8 the main concern is the importance of x 1 x 2 and x 4 this function is an additive one for which we can split the two terms 10 sin πx 1 x 2 and 10x 4 out as another single function simplified as 33 f x sin π x 1 x 2 x 3 where the x i are uniformly distributed within 0 1 for eq 33 both as ols and as qphd would rank x 3 x 1 x 2 yet the analytical activity score would rank x 1 x 2 x 3 as in fig 8 there are two tipping points for this function to make the estimated activity scores and the analytical exact scores different in terms of ranking one is the π inside the sine function both the estimated and analytical activity scores provide the same ranking with x 3 x 1 x 2 if the constant π is changed to a smaller number such as 1 with π the sine function is sufficiently non linear to cause the gradient approximation methods to fail in identifying the model structure with smaller constant the sine function is more linear for the gradient approximation methods to be sufficiently accurate the other tipping point is the parameter range as long as the parameter range exceeds 0 44 0 89 the ranking from the estimated activity scores and the analytical score would remain different if the parameter range of each model parameter shrinks from 0 44 0 89 for example the lower bound is higher than 0 44 or the upper bound is lower than 0 89 the ranking from both estimated and analytical activity scores would agree with each other to be more specific the ranking from the estimated activity score remains the same but the analytical activity score changes the ranking with the change of parameter range this change in parameter range causes the number of dimensions of the active subspace to be different as the largest gap between the eigenvalues is between first and second eigenvalues in decreasing order but the shrinkage in parameter range makes the largest gap between the eigenvalues to be between the second and third eigenvalues in decreasing order with one less dimension in the active subspace the calculation of the activity score is highly impacted the active subspace method identifies the most important direction in the parameter space the activity score shows how much each model parameter contributes to the direction defined by the first n eigenvectors where n is the dimension of the active subspace for the friedman function activity scores consider x 3 to be non important unlike s ti or μ because the important direction identified by active subspaces is orthogonal to x 3 thus x 3 is not considered as an active variable from an integration point of view the gradient of the x 3 term is 40 x 3 0 5 and the integral of the outer product of this term with others after the input normalisation is 0 so x 3 has zero influence on any other directions in the parameter space similarly the directions of every model parameter of the sobol g function are orthogonal to each other with the setting of coefficients x 1 is designed to be the most influential parameter resulting in the largest eigenvalue gap being between x 1 and x 2 moreover the first eigenvector is in the form 100 0 t and this induces the activity scores of model parameters x 2 to x 7 along with the dummy parameter to be zero for the case of the sobol g function activity scoring using the global linear model identifies x 7 as the most important parameter and this is different from the other activity scoring methods the reason being that the approximated matrix c by the global linear model has numbers with very small magnitude in the entries of its first column c 1 3 72 e 8 1 963 e 7 1 465 e 7 2 63 e 8 3 03 e 8 2 97 e 8 7 81 e 7 5 6 e 9 t nevertheless the entries in the 7th row which corresponds to x 7 are slightly larger than other entries due to approximation error and this causes the 7th entry of the first eigenvector to be the largest thus x 7 becomes the most sensitive parameter there are certain cases where the activity score with the local linear gradient approximation behaves wrongly especially for the case of the modified sobol g function with g6 coefficient see fig 4 this behaviour is caused by the limitation of local approximation in exploring the parameter space when the response surface is too complex as will be covered in section 6 1 in a replication implementation in other words where more samples from different locations are taken the ranking provided by the activity score with the local linear model is more reasonable being based on different fundamental principles than either the variance based sobol method or the morris method the active subspace approach can yield activity scores that do not provide the same ranking as that from s ti or μ on the other hand results for the ishigami homma function fig 1 sobol g function fig 2 and friedman function fig 8 have characteristics that show active subspace methods yield a different perspective than other sensitivity analysis methods but aside from these three test functions activity scores compare well with what the s ti and μ provide indeed activity scoring can achieve this with a much smaller m for the modified sobol g function in the g6 fig 4 coefficient settings the activity scoring with the global quadratic model qphd can reach a stable ranking with as few as 1024 model evaluations compared to more than 65 536 for sobol and to 65 536 for morris methods furthermore as qphd can identify important model parameters that have similar sensitivity earlier than other methods in the model runs and this is shown from the results for the morris function fig 7 with 256 model runs and for the sobol levitan function with two configurations figs 9 and 10 with less than 4096 model runs for as qphd for the bratley and linear functions all activity scores perform as well as the benchmark methods in terms of achieving stable ranking at the same m 6 1 the performance of ranking measures in terms of the four ranking measures in the literature several conclusions emanate from the test function results described in previous sections and given in the supplementary files first the sarrazin ranking stat ranking highly depends on the quality of bootstrap resamples whereby poor quality of resamples may provide a false indicator of ranks for example as opg yields stat ranking 0 216 1 for the ishigami homma function at 256 model evaluations see fig 1 however as opg wrongly ranks the dummy parameter as the most important parameter the sarrazin ranking weights important model parameters more highly than less sensitive model parameters for the sobol g function the ranks constantly shift for x 2 to x 7 for the activity scores but stat ranking yields values less than 0 02 at any m however the shifting in rank of important model parameters with similar sensitivity renders stat ranking to be useless such as in the case of the morris function or sobol levitan function that have several important parameters with the same sensitivity analytically for the most important parameters the threshold of reliability for stat ranking to be less than 1 is about 60 and this percentage can rise to 80 for most cases the sarrazin ranking stat ranking agrees with the results of reliability with a large number of model parameters in a single model the sarrazin ranking stat ranking can provide a general view of the agreement between resamples and the reliability provides the ability to analyse model parameters of interest more closely thus these two measures could be used in complementary fashion the position factor measure unlike the sarrazin ranking stat ranking weights the change of ranks for insensitive parameters and sensitive parameters equally important and this can be observed from the results of any of the sa methods for the sobol g function as qphd ranks x 1 as the most important parameter from 256 model evaluations onwards but the insensitive parameters x 2 and the dummy parameter still change ranks with increases in m due to near zero activity scores and approximation error however the position factor gives values of more than 2 5 meaning ranking has not converged given the shifting ranks of insensitive parameters in terms of information gain the position factor measure does not provide additional information if one can visualize ranking results with something like the heat map design of this paper moreover just as with the sarrazin ranking stat ranking the position factor is unable to provide any valuable information when sensitive parameters have similar sensitivities furthermore the position factor only compares how ranking changed from the previous m to the current one to a certain degree however if the number of model parameters inputs is extremely large position factor could be a simple solution to avoid drawing a huge figure for visual inspection of ranking changes the tdcc measure with the savage score provides less information in terms of the meaning of reproducibility compared with other ranking measures for our cases 0 8 or more seems to be the threshold to indicate a stability in the ranking results but a value of more than 0 85 may be sufficient for the other ranking measures to also give a signal of converged ranking results again model parameters with similar sensitives are also something that tdcc fails to identify 6 2 confidence interval estimates of parameter sensitivity bootstrap versus replication for the ranking measures in section 4 all convergence calculations are based on bootstrap resampling with replacement 1000 times this number of resampled sets has been employed or recommended in several studies baroni and tarantola 2014 brunetti et al 2016 nossent et al 2011 sheikholeslami et al 2017 the bootstrap technique efron 1979 is widely used in the study of sensitivity analysis methods baroni et al 2018 khorashadi zadeh et al 2017 wang and solomatine 2019 as it does not require additional model runs to provide an estimate of the confidence interval here we discuss results from employing replication with our test functions to measure the comparative efficiency and accuracy of parameter confidence intervals provided by the bootstrap technique the replication technique is however a much more expensive way of generating confidence intervals for metrics of interest as it requires the experiment to be repeated with multiple sets of different samples known as replicates the usage of replication has been discussed in several studies sun et al 2021 tarantola et al 2012 yang 2011 though it appears to be much less popular than bootstrapping due to high computational cost standard deviation can be used here as a concise indication of confidence interval width under the assumption of a normal distribution the 95 confidence bounds can be obtained by calculating them as 7 84 times the standard deviation we checked the relevant histograms from both 1000 bootstrap resamples and 100 replications to confirm they follow a normal distribution moreover standard deviation is also a useful way to estimate confidence intervals when data are from a small population as occurs for the 100 replicates in our case we found for all test functions that standard deviations of parameter sensitivity from the bootstrap do not decrease as much as those from replication as an example fig 11 gives results for the sobol levitan function with the g6 configuration it shows the standard deviation across the parameter sensitivity measures obtained from both resampling and replication for each model parameter with every subplot on a log scale the first row of subplots provides the results from the bootstrap analysis and the second row from replication 100 times all the gsa methods rank x 1 as the most important parameter for this test function the other model parameters being of similar sensitivities excepting the dummy parameter for the activity scores for s ti and μ it appears that the standard deviation between resamples replicates for the most important parameter is slightly larger than the other model parameters it is less obvious to see the difference in standard deviation between bootstrap and replication for s ti and μ however the standard deviation from as ols and as qphd suggests that the bootstrap converges with a rate of approximately 1 n while the replication converges with a rate of approximately 1 n the bootstrap resamples do not use as much of the information provided by the sobol sequence as replication does as its convergence rate is less than expected on the other hand as opg appears to behave disorderly even at a large number of model runs this behaviour is due to the local linear gradient approximation model not making efficient use of increases in the sample size n moreover bootstrapping is less reliable as an assessment of standard deviation at small sample size notice for example the standard deviation results for s ti and μ at 256 model runs with increase in m the line of standard deviation becomes smoother indeed the bootstrap technique should be used with care as it relies on the smoothness and symmetry of the bootstrap distribution efron 1987 and the bootstrap resamples may hinder the convergence rate using the sobol sequence with the resamples no longer being a sobol sequence whenever computational budget is available researchers would ideally consider implementing replication to check the reliability of confidence intervals provided by the bootstrap technique similar plots to fig 11 for our other test functions can be found in section 3 of the supplementary file along with the heat map of ranking results from replication in section 2 of the supplementary file these figures accord generally with the conclusions above 6 3 the impact of sampling method on ranking by activity scoring random sampling vs sobol sequence among studies of the active subspace approach and activity scoring monte carlo sampling also called random sampling has generally been used as was indicated in the introduction it has however been noted e g guy et al 2020 that quasi monte carlo sampling might improve the performance of activity scoring over that of random sampling and our results confirm this statement and quantify the performance benefit for the modified sobol g function with g6 coefficient test function fig 12 illustrates the ranking results from activity scoring with the three different gradient approximation methods obtained from random sampling these are to be compared to the results obtained from sobol sequence sampling in fig 4 clearly activity scoring with the sobol sequence can achieve correct and stable ranking results with much fewer model runs than random sampling does it is not until reaching 16 384 model evaluations and only for the as qphd approximation that random sampling obtains the correct ranking yet using quasi monte carlo as qphd obtains the accurate ranking results at 1024 model runs and is stable from thereon with increasing sample size the performance of as ols with random sampling is even worse as it cannot rank the model parameters in the correct order with 65 536 model runs yet as ols achieves the correct ranking at 16 384 with sobol sequence sampling as opg does poorly with either random sampling or sobol sequence the advantage of the quasi monte carlo sampling method has been tested and approved for other sensitivity analysis methods in several studies niederreiter 1978 qian and mahdi 2020 sobol 1967 and multiple researchers have emphasized the importance of the sampling method for producing sound sensitivity measures andres 1997 castaings et al 2012 janssen 2013 from our experiments the importance of the sampling method also holds for activity scoring the ranking results for other test functions obtained by activity scoring using the random sampling method can be found in section 7 of the supplementary file in general they are consistent with the illustrative results for the modified sobol g function confirming and quantifying the benefit of quasi monte carlo sampling at least for our test functions the impact of the sampling method on the performance of both the variance based sobol method and the morris method is also a significant point of discussion in the literature and there are several studies that provide substantially more detail on the differences and development of sampling methods for either variance based sobol method or the morris method campolongo et al 2007 2011 qian and mahdi 2020 reusser et al 2011 sheikholeslami and razavi 2017 zhan and zhang 2013 7 conclusions the paper has investigated the performance of the novel method of activity scoring in comparison to two benchmark methods of global sensitivity analysis using several well known test functions that encompass overall a wide range of function behaviour heat maps that show both parameter sensitivities and numerical rankings with increasing sample size are introduced as a key visualization tool although the focus of the comparison is on ranking sensitivities our heat maps also show parameter sensitivity in order to provide additional information to the numerical rankings through testing against analytical solutions activity scores match the benchmark methods total effect sensitivity index of sobol and the absolute mean of elementary effect index of morris in most but not all test cases moreover activity scoring can be more computationally efficient than the benchmark methods in some instances in the case of the ishigami homma function sobol g function and friedman function where the activity score relies highly on the important directions obtained from the corresponding eigenvector analysis activity scoring does not provide as stable ranking results with increasing sample size as the total effect sensitivity index s ti or morris μ in the other cases activity scoring performs as well as s ti and μ and it behaves even better with the more appropriate selection of gradient approximation methods the performance of activity scoring depends on the accuracy of the gradient approximation model that is employed as gradient quality affects the detection of the number of active subspace dimensions for the ishigami homma function and friedman function the activity score based on different gradient approximation models provide different rankings even in comparison to the analytical exact activity score the analytical activity scoring results can be viewed as results that would be obtained with the perfect gradient approximation method thereby indicating the strong impact of better gradient approximation method for the performance of activity scoring of the three gradient approximation models employed in this paper the global quadratic model as qphd performs the best overall indeed it is more efficient than both s ti and μ in some instances such as for the modified sobol g function morris function and sobol levitan function moreover as qphd has an advantage over other methods in determining the most important parameters with similar sensitivities as is the case for the morris function and sobol levitan function in regard to the convergence of the four ranking measures both the sarrazin ranking stat ranking and reliability are to be preferred to the other two ranking measures as these two provide the most accurate information in addition stat ranking and reliability utilize all of the bootstrap resamples to provide indicators comparable with the traditional confidence interval of a given percentage say 95 this utility is not feasible for the current setting of the position factor or tdcc the advantage of stat ranking is that it provides a scalar as an indicator of convergence unlike reliability that evaluates the model parameter individually however reliability can explain certain changes in stat ranking with more detail nevertheless it is to be noted that usage of the sarrazin ranking stat ranking and reliability requires access to resamples and thereby is highly reliant on the quality of the resamples in addition the sarrazin ranking stat ranking and reliability perform poorly when multiple sensitive parameters have similar sensitivity or when these parameters should have the same sensitivity analytically a way of addressing the poor performance in such a situation is to rank with a chosen tolerance whereby model parameters with differences in sensitivity inside this tolerance are given the same rank however this tolerance should be chosen with care as it could falsely classify model parameters with different sensitivity analytically the same due to approximation error whenever using any of these four ranking measures it is still recommended that visualization of the ranking results is employed such as the heat mapping in this paper more examples of useful visualization tools can be found in pianosi et al 2016 in approximating confidence intervals for ranking purposes the bootstrap is naturally much cheaper than replication but it may suffer a loss in the expected convergence rate when using a sobol sequence as the bootstrap resamples may not comply with the optimal structure of the original sobol sequence with respect to the sampling methods the quasi monte carlo sobol sequence has a significant computational advantage in achieving ranking results compared to the results from random sampling as the sobol sequence requires much fewer model runs to achieve stability in ranking there is much opportunity for improving the applicability of activity scoring by addressing additional ways for approximating the function gradient one avenue is building surrogate models such as with sparse grid polynomial chaos or gaussian process emulation for high dimensional models with considerable non linearity a reliable and accurate gradient approximation model is crucial to active subspace performance and its activity scoring on ranking several studies have proposed the morris method as the first step of screening for important parameters and then implementing other sensitivity analysis methods on these selected important parameters activity scoring could serve as one of the sensitivity analysis methods for the second step the reuse of samples from the morris method can also be considered and tested for efficiency lastly the active subspace approach represents a different way of looking at sensitivity in contrast to the popular variance based derivative based or variogram based sensitivity analysis methods multiple perspectives and methods are invaluable to understand model behaviour and activity scoring has an obvious role to play in that arsenal software data availability library name saconvergenceanalysis contact person xifu sun e mail address xifu sun anu edu au year first available 2021 software required python 3 availability https github com xifus saconvergenceanalysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement xifu sun s research was funded by a scholarship provided by the mathematical sciences institute of the australian national university appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105310 
25640,active subspaces is a recently developed concept that identifies essential directions of the response surface of a model providing sensitivity metrics known as activity scores we compare activity scoring with the sobol and the morris global methods using a series of well known benchmark test functions with exactly computable sensitivities in the ranking context we analyse the accuracy efficiency impact of sampling method convergence rate and confidence interval estimation through both bootstrapping and replication heat maps that show both numerical rankings and underlying sensitivities with increasing sample size are introduced as a key visualization tool for such analysis convergence is also assessed through four previous measures activity scores match the total effect sensitivity index of sobol and the absolute mean of elementary effect of morris in most test cases activity scoring can be more computationally efficient its potential can be enhanced by expanding methods for approximating the gradient of the model function keywords global sensitivity analysis test function active subspace activity score variance based sobol method morris method 1 introduction global sensitivity analysis gsa has quickly gained attention in the past few years as a discipline coming of age for aiding model development and evaluation especially through its ability to identify and apportion relative sources of model factor influence on model outputs razavi et al 2021 saltelli et al 2021 this has led to development of a number of global sensitivity analysis methods constantine 2015 morris 1991 pianosi and wagener 2015 rakovec et al 2014 razavi and gupta 2016a saltelli et al 1999 sobol 1993 and associated software tools as covered in douglas smith et al 2020 the various methods tend to make different assumptions and or are based on different metrics for sensitivity leading to calls for the use of multiple methods in practice to inform conclusions about model sensitivity wagener and pianosi 2019 the active subspace concept constantine 2015 affords a recent global sensitivity analysis method that identifies the critical directions or the combination of model inputs in the parameter space with the help of eigenpairs in addition it can be utilized to help in constructing surrogate models e g cortesi et al 2020 vohra et al 2020 however due to its novelty active subspace methods of gsa have not been much applied indeed there is a limited number of papers that use them for ranking purposes e g diaz et al 2018 diaz 2016 leon et al 2019 vohra et al 2019 among all the studies of active subspace methods for gsa even fewer papers have investigated the efficiency and accuracy of the ranking provided by the active subspace method to render active subspace metrics somewhat comparable with other sensitivity metrics constantine et al constantine and diaz 2017 have transferred the results from the active subspace method into a sensitivity metric called the activity score as which mathematically relates to the sobol total effect sensitivity index the activity score has certain limitations regarding the requirement of a sufficiently distinct gap between eigenvalues for identifying the active subspace however it provides insight into sensitivity of model parameters and as will be seen it can efficiently provide a ranking of the parameters ranking in gsa aims to order the relative importance sensitivity of the model parameters in relation to their impact on the model output sarrazin et al 2016 this helps to identify significant parameters and reveal interactions saltelli et al 2008 sreedevi et al 2019 van werkhoven et al 2008 prioritise computational cost and efforts sin et al 2011 to obtain better estimates in the subsequent numerical or experimental process saltelli et al 2008 and support model development hartmann et al 2013 razavi and gupta 2016b have pointed out that model development relies heavily on robust and reliable ranking results and incorrect ranking results can lead to a poor representation of the subsequent reduced order model nossent and bauwens 2012 therefore ranking is the main objective of gsa considered in this paper and we compare how efficient the ranking provided by activity scoring compares to two other benchmark sensitivity analysis methods however we also introduce heat maps as a visualization tool that not only shows numerical rankings of parameters but that also portrays underlying sensitivities with increasing sample size as additional information in our study the popular quasi monte carlo sobol sequence antonov and saleev 1979 sobol 1967 is implemented to investigate the performance of the active subspace method and its activity score and that of the sobol metrics although guy et al 2020 used monte carlo sampling to generate activity scores they argued that quasi monte carlo sampling might be more computationally efficient the majority of active subspace studies reported either use monte carlo sampling abdo and abdel khalik 2016 cortesi et al 2020 loudon and pankavich 2017 ryken et al 2020 su et al 2021 vohra and mahadevan 2019 or largely make no mention of the sampling method used at all bittner et al 2020 bridges et al 2019 gilbert et al 2016 jefferson et al 2016 leon et al 2019 mola et al 2019 though there are a few studies that implement latin hypercube sampling comparison of the active subspace approach with others has rarely been reported although vohra et al vohra and mahadevan 2019 and godel et al gödel et al 2020 have compared the activity score with the total effect sensitivity index from the variance based sobol method sobol 1993 for a typical bottleneck scenario in crowd simulation with the optimal steps model concluding that these two sensitivity metrics coincide and confirm each other to investigate the performance of activity scoring more comprehensively we implement several test functions and compare the ranking obtained from active subspace sensitivity measures with the ones from the variance based sobol method and the morris method awad et al 2019 describe the morris method and sobol total effect sensitivity index as efficient in ranking parameters as they provide complementary information razavi and gupta 2016a have also pointed out that sobol and morris methods provide the most up to date rigorous approaches for global sensitivity analysis and hence these two methods should be used as benchmarks to demonstrate the performance of newly developed techniques on that account for the study of global sensitivity analysis the variance based sobol method and morris method are viewed here as the reference methods to demonstrate how the method in question compares with these two reference methods as emphasized by mai and tolson 2019 newly developed sensitivity analysis methods should be tested against analytical estimates of sensitivity measures from test functions accordingly we have chosen eight frequently used test functions from the literature to serve as benchmarking representing a useful range of possible model structures encountered in real life models moreover our work is inspired by the comparison study undertaken in mora et al 2019 where the pawn and sobol methods were compared with several test functions using a convergence analysis this paper is structured as follows section 2 provides the details of sensitivity analysis methods used for comparison section 3 presents the formulas of the test functions used section 4 introduces four previous measures to assess the convergence of ranking with sample size section 5 provides testing results using heat maps that visualize both sensitivity metrics and the numerical rankings of parameters for increasing sample size section 6 contains the discussion which includes confidence interval estimation of metrics by both bootstrapping and replicate sampling and section 7 gives the conclusions based on the testing results 2 sensitivity analysis methods as mentioned in the introduction we chose the variance based sobol method and the morris method as the reference sensitivity analysis methods to monitor the performance of activity scoring in this section we briefly introduce the three sensitivity analysis methods used in this paper 2 1 variance based sobol method assume that f x is a square integrable function defined in a unit hypercube 0 1 k the decomposition of f x can be written as 1 f x f 0 i 1 k f i x i i 1 k j i k f i j x i x j f 12 k x 1 x k where k is the number of parameters and f 0 is scalar defined as 2 f 0 0 1 k f x d x this decomposition of f x follows the analysis of variance anova if 3 0 1 f i 1 i s x i 1 x i s d x i q 0 for 1 i 1 i 2 i s k where x i q x i 1 x i s and all the terms are mutually orthogonal with respect to the integration by taking the variance of eq 1 the decomposition becomes 4 v f x 0 i 1 k v f i x i i 1 k j i k v f i j x i x j v f 12 k x 1 x k dividing both sides of eq 4 by the variance of function v f we obtain 5 1 i 1 k v f i v f i 1 k j i k v f i j v f v f 12 k x 1 x k v f i 1 k s i i 1 k j i k s i j s 12 k the term s i is known as the first order sensitivity index of the ith model parameter and it is defined as 6 s i v x i e x i f x i v f where e x is the expectation and x i refers to all model parameters except the ith parameter in addition to the first order sensitivity index the total effect sensitivity index s ti which measures the combination of the individual contribution of and the interactions with the ith model parameter is defined as 7 s t i e x i v x i f x i v f where 8 e x i v x i f x i 1 2 n j 1 n f a j f a b i j 2 is called the jansen1999 estimator jansen 1999 saltelli et al 2010 and f a j is the function value of the jth row of matrix a the variance v f is also approximated through the mean of n quasi monte carlo samples as 9 v f 1 n j 1 n f a j 2 1 n j 1 n f a j 2 where a b and a b i are the sample matrices generated based on the saltelli method saltelli 2002 and matrix a b i is a sample matrix where matrix a replaces the ith column with the ith column of matrix b the implementation of the variance based sobol method uses the version from the salib python library herman and usher 2017 for convenience we will use the notation s ti and st to represent total effect sensitivity index in the following text and figures correspondingly 2 2 morris method the basis of this method was proposed by morris 1991 and it measures the estimated mean of elementary effects of each model parameter through the parameter space by giving a baseline f x for a model parameter of interest x i this specific parameter is altered with a predefined perturbation δ and the elementary effect of this particular model parameter is defined as 10 e e i f x 1 x i δ i x k f x δ i the value that δ takes is p 2 p 1 where p is the number of the grid level the grid level number is reported to be 4 to 10 morris 1991 saltelli 1999 and p is set to 4 in several studies campolongo and saltelli 1997 campolongo et al 1999 naves et al 2020 however there is still an ongoing debate on the grid level number campolongo et al 2007 gan et al 2014 jaxa rozen and kwakkel 2018 sreedevi and eldho 2019 yang 2011 but this is out of the scope for this paper throughout the tests in this paper we keep the number of grid levels p as 4 with multiple baselines across the parameter space the estimated mean of the elementary effect μ can be calculated however the elementary effect may have opposite signs due to non monotonic model characteristics to avoid this compensation of opposite sign it has been proposed that the absolute mean of the estimated elementary effect be used instead campolongo et al 2007 which is given by 11 μ i 1 n j 1 n e e i j moreover it was shown in campolongo et al 2007 that the elementary effects could be compared with what the s ti of sobol provides and in saltelli et al 2008 that the μ is suitable for ranking for a better indication of the results of using μ we normalised μ with 12 μ n o r m i μ i i 1 k μ i 2 1 2 the implementation of the morris method also uses the salib python library herman and usher 2017 similar to the total effect sensitivity index we will use the notation μ and mu star to represent normalised absolute elementary effect μ n o r m i in the following text and figures correspondingly 2 3 active subspace method the active subspace method defines a k by k symmetric positive semi definite matrix c in which 13 c f x f x t ρ x d x w λ w t where w w 1 w k is the orthogonal matrix of eigenvectors and λ diag λ 1 λ k is the diagonal matrix of eigenvalues in decreasing order the eigenpairs satisfy the condition in which 14 λ i w i t c w i f x t w i 2 ρ x d x and λ i is zero if f x is constant along the direction of w i if a gap between eigenvalues can be identified such that λ n λ n 1 for some n k an n dimensional active subspace can be constructed with λ λ 1 λ 2 and w w 1 w 2 where λ1 are the first n eigenvalues and w 1 is the k by n eigenvector matrix this active subspace is a column span of the eigenvector matrix w 1 for application of active subspace analysis to computational models it can be rather difficult to obtain the exact form of the gradient of the model function f x thus the monte carlo method and various gradient approximation methods provide a way to estimate the matrix c as 15 c 1 n i 1 n f x i f x i t where x i is drawn independently according to ρ x for i 1 n in order to be comparable with other sensitivity analysis methods the activity score utilizes the obtained eigenvalues and eigenvectors and the activity score for the ith model parameter is defined as 16 α i j 1 n λ j w i j 2 for i 1 k for better visualization constantine et al constantine and diaz 2017 normalise the activity score to have a norm of 1 with 17 α n o r m i α i i 1 k α i 2 1 2 moreover it has been proven in constantine and diaz 2017 that the activity score gives an upper bound of the s ti 18 s t i 1 4 π 2 v f α i λ n 1 for the sake of simplicity we will use α i to represent α norm i in the remainder of the paper constantine et al provide a python library for active subspace analysis along with multiple methods to provide approximated gradients f x i to investigate the performance of each method of approximating the gradient we applied all three library methods in the experiments these approximation methods are the global linear model ols global quadratic model qphd and local linear model opg and for consistency we retain the same abbreviation of each model as defined by constantine the global linear model which is referred to as as ols in later sections estimates a one dimensional subspace to approximate the gradient f x i the global quadratic model which is referred to as as ophd has a similar concept as the approach used for generating principal hessian directions based on a global quadratic model li 1992 however the implemented version here uses the average outer product of the gradient of the quadratic model constantine et al 2016 the local linear model which is referred to as as opg is known as the outer product of the gradient method hristache et al 2001 and it builds a local linear model for a subset of neighbouring points of each pair of model input output more details of these approximation models can be found in constantine s code and book constantine et al 2016 constantine 2015 for all test functions implemented below except the modified sobol g function their gradients can be obtained analytically thus the analytical activity score will also be provided along with the results from each gradient approximation model to further investigate the performance of activity scoring we implemented a so called dummy parameter as an adjunct to all of the test function parameters just for the active subspace method several studies castaings et al 2012 khorashadi zadeh et al 2017 have implemented the dummy parameter approach to obtain the threshold of the approximation error for identifying if one model parameter is truly insensitive or not and godel et al gödel et al 2020 also used a dummy parameter referred to there as a control parameter to test the detection of an absence of an impact 3 test functions test functions have been purposely designed in sensitivity studies to simulate specific model characteristics that researchers could face when studying real world problems linearity or monotonicity being simple examples thus many studies have involved widely used test functions to show the efficiency and capability of particular sensitivity analysis methods we have assembled eight popular test functions from the literature most having several settings and configurations to modify the function structure and influence of parameters active subspace analysis requires the model parameters to be normalised to the range 1 1 so we will only focus on test functions using uniform parameter distributions with a known range for each of the test functions the model parameter vector x x 1 x 2 x k t consists of k model parameters where x i is the ith model parameter each test function is introduced in the following subsections specifying its form and ranges of model parameters 3 1 test function 1 ishigami homma function ishigami and homma introduced this function ishigami and homma 1990 due to its analytical tractability and non additive properties jaxa rozen and kwakkel 2018 it has been one of the most used test functions for the comparison of sensitivity analysis methods cuntz et al 2015 jaxa rozen and kwakkel 2018 mora et al 2019 razavi and gupta 2019 wang et al 2020 wei et al 2013 ziehn and tomlin 2009 its benchmark nonlinear and non monotonic features with the presence of interaction effects do and razavi 2020 also make it attractive for testing the function is defined as 19 f x sin x 1 a sin 2 x 2 b x 3 4 sin x 1 where x i is uniformly distributed within π π a 7 and b 0 1 3 2 test function 2 sobol g function davis and rabinowitz 1984 introduced the g function initially with coefficients a i 0 and later it was used with nonzero coefficients in the study of saltelli and sobol archer et al 1997 saltelli and sobol 1995 thus the nonzero coefficient version is henceforth called the sobol g function it has been used for testing sensitivity analysis methods in many studies glen and isaacs 2012 horiguchi et al 2021 mora et al 2019 razavi and gupta 2019 sun et al 2021 wang et al 2020 ziehn and tomlin 2009 its popularity is based at least partly on it being a product of univariate functions that encompass a broad spectrum of complexity cuntz et al 2015 the value of the coefficient a i determines the sensitivity of its corresponding model parameters the more sensitive model parameters having a lower value of a i the function is defined as 20 f x i 1 k 4 x i 2 a i 1 a i where x i is uniformly distributed within 0 1 k is the number of model parameters a 0 1 4 5 9 99 99 99 and a i is the ith value of a 3 3 test function 3 modified sobol g function saltelli also created the modified sobol g function to involve an extra layer of complexity using a so called floor function saltelli et al 2010 this modified sobol g function has been used in several papers for benchmarking different sensitivity analysis methods cuntz et al 2015 mai and tolson 2019 mora et al 2019 the function is defined as 21 f x i 1 k 1 α i 2 x i δ i x i δ i 1 i α a i 1 a i where is the floor function x i is uniformly distributed within 0 1 α and δ are the curvature and shift parameters respectively a i are coefficients and k is the number of model parameters the shift parameter δ is randomly chosen as it does not affect the propagation of uncertainties mora et al 2019 the floor function takes the integer part of the value inside the floor function brackets the inclusion of this floor function makes the modified sobol g function discontinuous and this prevents calculation of the analytical gradient along with the analytical activity score for the coefficients we follow saltelli et al 2010 and use the six sets of coefficients there with some modifications to the number of parameters g1 α 1 a 0 0 9 9 9 9 9 g2 α 1 a 0 0 1 0 2 0 3 0 4 0 8 1 g3 α 0 5 a 0 0 9 9 9 9 9 g4 α 0 5 a 0 0 1 0 2 0 3 0 4 0 8 1 g5 α 2 a 0 0 9 9 9 9 9 g6 α 2 a 0 0 1 0 2 0 3 0 4 0 8 1 3 4 test function 4 bratley function the bratley function also called the k function was introduced by bratley et al 1992 and is designed to have a higher degree of interaction with increasing index i of the model parameters horiguchi et al 2021 this function spans a range of parameter sensitivities in a convex manner which mimics real problems quite well cuntz et al 2015 thus it has been utilized in several studies cuntz et al 2015 mora et al 2019 the function is defined as 22 f x i 1 k 1 i j 1 i x j where x j is uniformly distributed within 0 1 and k is the number of model parameters 3 5 test function 5 morris function this function was presented by morris 1991 in 1991 and it has been used for testing in various studies cuntz et al 2015 sudret and mai 2015 later in 2006 morris et al modified the function morris et al 2006 and this modified version also used in horiguchi et al 2021 will be used in the paper the function is defined as 23 f x α i 1 k x i β i 1 k 1 x i j i 1 k x j where x i and x j are uniformly distributed within 0 1 α 12 6 0 1 k 1 β 12 10 k 1 and k is the number of model parameters 3 6 test function 6 friedman function the friedman function combines parameter interaction and non linearity horiguchi et al 2021 and thus poses a challenging problem to the testing of sensitivity analysis the function is defined as 24 f x 10 sin π x 1 x 2 20 x 3 0 5 2 10 x 4 5 x 5 x 6 0 1 x 7 where the x i are uniformly distributed within 0 1 3 7 test function 7 linear function this linear function was introduced by saltelli et al 2000 to provide basic testing it is defined as 25 f x i 1 k x i where x is uniformly distributed within x i σ i x i σ i and k is the number of model parameters the x i and σ i are defined as x i 3 i 1 σ i 0 5 x i 3 8 test function 8 sobol levitan function the sobol and levitan function was introduced in sobol and levitan 1999 similar to the sobol g function the magnitude of the coefficient impacts the sensitivity of the corresponding model parameters this test function is also presented in saltelli et al 2000 and is defined as 26 f x exp i 1 k b i x i i k where x is uniformly distributed within 0 1 b i is the coefficient and i k is defined as 27 i k i i k exp b i 1 b i two b i vector configurations for this function are considered k 7 and b 1 1 5 b 2 b 7 0 9 k 15 and b i 0 6 for 1 i 10 0 4 for 11 i 15 4 ranking measures in order to provide indicators regarding convergence aspects of the ranking results for these test functions we implemented four ranking measures from the literature it is worth noting that these four ranking measures have been applied only rarely in sensitivity analysis studies other than in the papers that first introduced them in searching through available python libraries implementation of the ranking measures was seen to be currently lacking and so we coded a python version of these measures which is available through github https github com xifus saconvergenceanalysis we also test the efficiency and accuracy of each ranking measurement for our test functions in this paper the ranking is in decreasing order with rank 1 being the most sensitive parameter 4 1 the ranking measure from sarrazin et al in order to address critical choices and gaps in convergence studies of sensitivity analysis sarrazin et al 2016 proposed criteria to quantify the convergence of ranking results from sensitivity analysis methods their ranking statistic modifies the spearman s rank correlation coefficient spearman 1904 to include a better weighted version than the one by dancelli et al 2013 and it can be written as follows 28 ρ s j m i 1 k p i j p i m max j m s i j s i m 2 i 1 k max j m s i j s i m 2 where p i j and p i m are the positions ranks of the ith model parameter using jth and mth bootstrap replicate resamples and s i j and s i m are the sensitivity measures respectively sarrazin et al calculate the ρ of every possible pair of resamples and summarize the statistics into an indicator stat ranking q 0 95 ρ s j m to quantify the convergence of ranking results the ranking result is considered to be converged when stat ranking 1 if stat ranking 1 the difference in the resamples for the most sensitive parameters is less than or equal to one position on average in this paper we will use the term sarrazin ranking stat ranking or simply stat ranking for convenience 4 2 position factor the concept of position factor was first introduced by ruano et al 2012 and was later modified by cosenza et al 2013 to take the absolute value of the difference in the ranks the modified version is defined as 29 position factor i 1 k p i j p i m μ p i j p i m where p i j and p i m are the positions ranks of the ith model parameter obtained by using samples j and samples m μ p i j p i m is the average of p i j and p i m and k is the number of model parameters if the value of the position factor is zero the ranks provided by the two different samples are the same 4 3 top down coefficient of concordance the top down coefficient of concordance tdcc measures the level of agreement among multiple rankings derived either by different sensitivity analysis methods or different resamples within each model run iman and conover 1987 it emphasizes the agreement on the most important model parameters whilst de emphasizing less important model parameters helton et al 2005 yang 2011 iman and conover 1987 employed tdcc with a so called savage score which is used in this paper and is given by 30 t d c c i 1 k j 1 r s s s i m 2 r 2 k r 2 k i 1 k 1 i the savage score ss is defined as 31 s s s i m i p i m k 1 i where s i m is the sensitivity measure of the ith model parameter obtained by the mth bootstrap replicate resample p i m is the corresponding rank r is the number of bootstrap replicate resamples and k is the number of model parameters the experiment has a high reproducibility if tdcc is close to 1 otherwise this experiment is unlikely to obtain similar results under the same settings if tdcc decreases away from 1 another interpretation is that a high tdcc score represents a high agreement on the ranking of important parameters yang 2011 in the results and discussion sections we will consider reproducibility in terms of rankings derived by different resamples 4 4 reliability incorporating a bootstrap method razavi and gupta 2016b generated estimates of the reliability of the inferred input rankings for each of the model parameters this reliability provides a percentage of how many resamples give the same rank for one specific model parameter as the original sample in extending this measurement so as to work for factor groupings sheikholeslami et al 2019 pointed out that this estimation measure should be considered as a check on robustness rather than reliability here we keep the nomenclature of reliability for this measure so as to be consistent with razavi and gupta 2016b the reliability of the factor ranking for the ith model parameter is given by 32 r e l i j 1 n b ψ p i p i j n b where n b is the number of bootstrap resamples and p i and p i j are the positions ranks of the ith model parameter obtained by the original sample and the jth bootstrap resample respectively the function ψ is defined as ψ a b 0 a b 1 a b the ith model parameter is fully reliable with respect to the sample variability when rel i 100 5 results in order to assess the performance of activity scoring in ranking we adopt the sequential approach of gradually increasing sample size and observe how the sensitivity measures change with differing numbers of model evaluations according to owen 2020 the proper convergence rate of a quasi monte carlo sequence specifically a sobol sequence may not be maintained if the sample size is not a power of 2 and similarly the number of samples that are skipped should be a power of 2 for a fair comparison in terms of computational effort between the sensitivity analysis methods we keep the number of model evaluations the same across all methods where the number of model evaluations m 256 1024 4096 16 384 65 536 the activity score implemented here takes the same number of samples n as model evaluations with m n since the three implemented gradients are calculated externally with existing model outputs without needing to access the model again however the s ti and the μ take the number of model evaluations to be m n k 1 where n is the number of samples trajectories and k is the number of parameters therefore we modified the number of model parameters for most of the test functions so that the number of model evaluations m and the number of samples n can both be a power of 2 and this retains the proper convergence rate of the quasi monte carlo sequence in the following the ranking results for the two methods generating s ti μ and for activity scores with the three different gradient approximation models ols qphd opg are shown and discussed for each test function through the form of heat maps that are grouped by the five methods however the supplementary material contains results in the form of tables from applying the four ranking measures presented in section 4 also for each test function it contains additional results of computing confidence intervals for those measures and their convergence with sample size on the basis of bootstrapping and replication the presentation in this section is therefore quite detailed and some readers may prefer to skip to the discussion section for a distillation of the main points and then return to this section for specifics the heat maps in the main text indicate both sensitivity and numerical ranking according to the sensitivity indices as opposed to the four ranking measures the last column of each heat map shows the analytical sensitivity measures for s ti μ and the activity score as thus the analytical s ti and as measures are manually calculated by definition while the analytical μ is obtained through a large m 1 0 7 model runs the heat maps have several other features the horizontal x axis is labeled with each of the sensitivity analysis methods each method has a grouping of six results where the first five columns are the computational results against increasing sample size and the sixth column shows the exact result each row of the vertical y axis corresponds to a model parameter with x 1 from the bottom to the dummy parameter at the very top the top x axis annotates each column with the number of model evaluations m and a specific column indicates the parameter ranking result obtained for that m thus each of the model parameters in each cell is annotated with a number from 1 to k showing the rank in terms of sensitivity importance for this model parameter where rank 1 is the most important or most sensitive parameter the colour of each cell in the heat maps represents the magnitude of the sensitivity measures index value or the value of activity score and this is explained by the colour bar at the right of each heat map where black ish colour for instance shows the sensitivity measure to be of value 0 8 or higher and yellow ish colour shows the sensitivity measure to be near 0 to better present the results of μ in the heat maps we normalised the values of μ to have a maximum value of 1 for s ti and μ the rank of the dummy parameter is shown as a dash rather than a number as the dummy parameter is not applicable for these methods calculation of the sarrazin ranking stat ranking tdcc with the savage score and reliability is made possible with 1000 bootstrap resamples and full tables of results from the ranking measures can be found in sections 4 7 of the supplementary file due to the length of this paper we will describe some key values from the different ranking measures in words rather than showing the full table here furthermore generation of the samples uses the sobol sequence generator implemented in the salib python library herman and usher 2017 for both the variance based sobol method and active subspaces the morris method uses a specific sampling strategy so it uses its own sample generator which is also implemented in the salib python library this section is split into several subsections to exhibit the different sensitivity results obtained for each test function 5 1 test function 1 ishigami homma function for the ishigami homma function the s ti measure gives correct ranking results that are stable from 256 model evaluations onwards fig 1 the slight change of colours indicates change in the index value but this does not affect the ranking results in the supplementary materials both the sarrazin ranking stat ranking and position factor ranking measures show agreement in the ranking between resamples for the s ti from 256 model evaluations onwards the stat ranking is 0 959 and position factor indicates that the ranking did not change from 256 to 1024 model runs the tdcc scores are 0 748 and 0 777 for 256 and 1024 model runs correspondingly and this indicates a better than average 0 5 reproducibility with 4096 model evaluations tdcc has a score of 0 993 which suggests a high reproducibility and these match fig 1 where the colour stabilizes from 4096 model evaluations the reliability of all three model parameters is above 80 from 256 model evaluations the μ of the morris method does not achieve a stable and correct ranking result until reaching 1024 model evaluations the un converged ranking result at 256 model evaluations is also identified by the ranking measures with the sarrazin ranking stat ranking at 1 369 and position factor at 1 333 and these values indicate a disagreement in the ranking between resamples the value of the reproducibility for the morris method is at around 0 5 until 16 384 model evaluations but the ranking has already stabilized at 1024 model evaluations with the confirmation of the sarrazin ranking stat ranking and position factor the reliability of x 1 and x 2 is slightly lower than 60 at 256 model evaluations but increases right after and this matches the observation of the sarrazin ranking stat ranking however μ ranks x 2 to be more important than x 1 different from that provided by the s ti as seen from fig 1 the activity score calculated from the global linear model as ols ranks x 1 to be the most critical parameter and other model parameters are all close to zero with yellow ish colour this activity score ranks the dummy parameter as the second most important parameter but this is likely due to the approximation error within activity scores as the sensitivity measures for x 2 and x 3 are too small judging from fig 1 the ranking results do not change with increase in the m however the sarrazin ranking stat ranking picks up the disagreement between resamples with a value of 1 448 for 256 model evaluations and the stat ranking value drops to 0 195 at 1024 model evaluations similarly for the reliability x 1 has less than 80 reliability at 256 model evaluations but the percentage rises to 100 with increasing model runs on the other hand position factor does not find a difference between the ranking results for 256 versus 1024 model evaluations as also indicated by the heat map tdcc gives a value larger than 0 5 reproducibility at 256 model evaluations and improves quickly as the number of evaluations increases the global linear model identifies a one dimensional active subspace for the ishigami homma function the first eigenvalue indicates that the most important direction in the parameter space is orthogonal to both x 2 and x 3 which returns a near zero activity score for these two parameters similarly the activity score calculated from the global quadratic model qphd ranks x 1 as the most important parameter the difference is that the active subspace with a global quadratic model identifies a two dimensional active subspace and this ranks an extra model parameter as more important than other parameters with low model evaluations 256 and 1026 specifically as qphd ranks the dummy parameter as the second most sensitive parameter indicating insufficient samples have been taken from 4096 model evaluations onwards this activity score starts to provide converged ranking results with x 1 x 2 other model parameters on the other hand the sarrazin ranking stat ranking is 0 628 for this activity score and this indicates converged ranking results it is obvious that this is false confidence as the dummy parameter should not be ranked more important than the actual model parameters with an increase of model evaluations sarrazin ranking stat ranking gives a value of more than 1 from 1024 to 65 536 model evaluations and this shows a strong disagreement between resamples even with large m moreover the reliability of all the model parameters is larger than 80 only at 65 536 model runs the position factor only picks up the different ranking between 1024 model evaluations and 4096 model evaluations in terms of reproducibility as qphd has the highest value of 0 784 at 256 model evaluations and the lowest value of 0 496 at 4096 model evaluations the activity score as opg ranks the dummy parameter as the most crucial parameter at 256 model evaluations and x 2 is ranked first with a higher sample size by approximating the gradients with the local linear model at 65 536 model evaluations x 3 and x 1 are higher with more extensive sensitivity measures than previous results this ranking result is different from that of either as ols or as qphd but the most critical parameter is the same as the morris method despite the shifting of the ranks stat ranking indicates converged ranking results for any number of our model evaluations the most prominent stat ranking is 0 944 at 65 536 model evaluations and this is still less than 1 on the other hand position factor has a high value from 256 to 1024 model evaluations and from 16 384 to 65 536 model evaluations at all numbers of model evaluations tdcc gives values above 0 85 and thus shows a high reproducibility the reliability of x 2 is near 100 from 1024 model runs and this value is higher than other model parameters for the analytical exact activity scores the model parameter x 2 is the most important parameter and the activity scores for the other two model parameters are zero 5 2 test function 2 sobol g function for this test function fig 2 shows that the s ti measure is able to provide stable and correct ranking results for parameters with high sensitivity at 256 model evaluations with x 1 x 2 x 3 x 4 moreover the index values for x 5 to x 7 are close to zero and the small fluctuations due to the approximation error cause the ranks of x 5 to x 7 to change at different numbers of model evaluations even with the small fluctuations the sarrazin ranking stat ranking is zero at 256 model evaluations this stat ranking is zero consistently with increase of sample size thus showing agreement between resamples similarly to the position factor the reproducibility is also extremely high with a value of 0 985 from tdcc as with the s ti the μ from the morris method also correctly ranks x 1 x 2 x 3 x 4 from 256 model evaluations onwards this ranking result is consistent with the coefficients of the sobol g function where a small coefficient indicates a larger influence on the model output the sarrazin ranking and position factor also demonstrate a converged ranking at any m with high reproducibility the activity score with the global linear model as ols acts interestingly for the sobol g function as it ranks x 7 as the most important parameter for all model evaluation numbers except m 16 384 moreover the colour which shows the magnitude of the activity score is inconsistent with change in the m thus it is hard to discern the status of the convergence of the ranking from fig 2 the stat ranking has a value of nearly 5 2 across all model evaluations portraying a strong disagreement in the ranking between resamples the position factor also indicates divisions between different numbers of model evaluations and the reproducibility metric is near 0 indicating an extremely low reproducibility for any m activity scoring with either the qphd or opg setting ranks x 1 as the most important model parameter and this is the same ranking obtained for both the s ti and μ the ranks however constantly shift for model parameters other than x 1 and the magnitudes of the activity scores are close to zero the shift in rankings is due to the slight fluctuations of the approximation errors the sarrazin ranking stat ranking values are near zero for both as qphd and as opg and this represents a stable ranking between the resamples at each m however the position factor is highly impacted by these small fluctuations with a value greater than 2 5 in most cases the reproducibility is also only slightly larger than 0 5 for as qphd and as opg activity scoring with the local linear model identifies x 2 as the second most important parameter with the activity score being slightly higher than x 3 for the dummy parameter all of the model parameters of as ols have a reliability around 20 and this explains the high sarrazin ranking stat ranking for other methods the reliability of x 1 to x 4 for s ti and μ is 100 from 256 model runs onwards and this is the same for x 1 of as qphd and as opg the analytical activity score is similar to the previous two activity scores where x 1 is ranked as most important whilst other model parameters have activity scores of zero 5 3 test function 3 modified sobol g function the modified sobol g function has six different coefficient settings controlled by the value of α and δ observing the ranking results provided by each of our sensitivity analysis methods g1 g3 and g5 can be classified into the same group and g2 g4 and g6 into another group for results presented in this subsection only g5 and g6 are chosen to act as representative ones whereas figures from other coefficient settings are included in section 1 of the supplementary file this test function has a floor function component resulting in the test function being discontinuous and so the gradient cannot be obtained analytically for determining the exact activity score the ranking results for the modified sobol g function with the g5 coefficient setting are shown in fig 3 as with g1 and g3 x 1 and x 2 are the two most important model parameters with other model parameters much less sensitive the s ti μ and the activity score using the global quadratic model as qphd successfully pinpoint x 1 and x 2 to be the essential model parameters from 256 model evaluations onwards the activity scores with a global linear model as ols and local linear model as opg also identify x 1 and x 2 as the most important parameters at 256 model evaluations however the ranks change as model evaluations increase to 1024 indicating a fortuitous result for these two activity scores at 256 model evaluations there is a constant 10 difference between x 1 and x 2 in their s ti up until 16 384 model evaluations whereas x 1 and x 2 should have the same sensitivity for the morris method μ does not show the first two model parameters to be at a similar sensitivity until 65 536 model evaluations is reached at 4096 model evaluations μ assesses these two model parameters to be close in importance but the difference is enlarged at 16 384 model evaluations activity scoring with the global quadratic model as qphd assesses the same importance for x 1 and x 2 at 1024 model evaluations with only a 0 1 difference in terms of activity score and this difference shrinks as m increases in regard to the least sensitive parameters as qphd recognizes these with near zero sensitivity where their activity scores are less than or equal to 0 001 from 1024 model evaluations onwards whilst total effect sensitivity indices are approximately 0 01 for these parameters at the same m as the sensitivity measures are small the ranks of the least sensitive parameters are quickly impacted by the approximation errors and this is reflected in the heat map of fig 3 the sarrazin rankings stat ranking for s ti μ and as qphd are approximately equal to but not less than 1 at all numbers of model evaluations except μ at 16 384 model evaluations where the stat ranking is 0 025 this phenomenon is likely due to the rank shifting of x 1 and x 2 between the resamples for as ols and as opg the stat ranking is almost 1 at 65 536 model evaluations and 4096 model evaluations respectively but the stat ranking does not drop below a value of 1 this behaviour of stat ranking can also be explained by the reliability measure as the stat ranking is close to 1 when the reliability of the two most important parameters x 1 and x 2 is equal to or greater than 60 for the position factor as qphd has a score of 0 667 at 16 384 model evaluations but in every other case has a value larger than 1 with respect to reproducibility as ols and as opg have low reproducibility at specific model evaluations but every other sensitivity analysis method has a reproducibility higher than 0 75 the sensitivity properties of the modified sobol g function change accordingly with its coefficients for g2 g4 and g6 the coefficients are set to have importance of model parameters in the order of x 1 x 2 x 3 x 7 we use the g6 setting as the representative here and fig 4 shows the ranking results when the g6 coefficient setting is applied to this test function the s ti index does not yield a converged ranking result even at 65 536 model evaluations the ranking from s ti is persistently changing with a different m however by comparing with the exact analytically derived result s ti correctly recognizes that x 1 to x 3 are the most important model parameters from 1024 model evaluations onwards for the morris method μ gives the correct ranking at 65 536 model evaluations however unlike s ti μ does not pinpoint the first three model parameters as essential parameters early in the model runs as with s ti as ols and as qphd identify the first three model parameters as more sensitive than others at 1024 model evaluations moreover as ols gives the correct ranking at 16 384 model evaluations whereas as qphd gives a stable ranking result at the lower number of 1024 model evaluations the activity score with the local linear model as opg on the other hand does not provide a sensible ranking as it even identifies the dummy parameter as the most crucial parameter at 16 384 model evaluations for the sarrazin ranking stat ranking only as ols and as qphd have scores lower than 1 at 16 384 model evaluations but stat ranking is much higher than 1 before this point the percentage of reliability results also confirm the observations of stat ranking with more than 80 for as ols and as qphd from 16 384 model runs onwards across all model parameters with an increase to 65 536 model evaluations stat ranking for μ is less than 1 and as ols and as qphd have stat ranking s of 0 which indicates the agreement of ranking results between resamples the reliability of all model parameters using the μ measure is only above 80 at 65 536 model runs which is the same as that provided by stat ranking the position factor has a value of 0 for as qphd only at 4096 model evaluations and the value is much higher than 1 for other sensitivity analysis methods also as qphd is the only method with a consistent high reproducibility scoring higher than 0 75 from 1024 model evaluations 5 4 test function 4 7 bratley k function and linear function for the bratley function fig 5 all of the ranking results from activity scoring yield the same rank for the highly sensitive model parameters as s ti and μ and the ranks stabilize from 256 model evaluations these ranking results agrees with the analytical results for important parameters from fig 5 s ti μ and as opg still exhibit change in values for some model parameters as shown by the change of colours but this does not impact the ranking for less sensitive model parameters x 5 to x 7 x 5 stays as the 5th important parameter at all times but x 6 and x 7 change ranks constantly for all of the sensitivity analysis methods other than the sobol method both as ols and as opg rank the dummy parameter to be more important than either x 6 or x 7 early on and this is likely due to the near zero sensitivity for these two model parameters both stat ranking and the position factor indicate a converged ranking for all of the sensitivity analysis methods from 256 model evaluations with high reproducibility tdcc 0 97 for the reliability measure it appears that there is a reliability of above 60 for relatively important model parameters according to a converged value for the sarrazin ranking measures stat ranking for the linear function fig 6 similar to the bratley function fig 5 all of the sensitivity analysis methods rank the model parameters the same from 256 model evaluations onwards the activity scores with different gradient approximation methods yield the same ranking results as the analytical activity score the sarrazin ranking stat ranking and the position factor all are ideally zero the reproducibility is 1 for tdcc and the reliability is 100 for all methods and model parameters 5 5 test function 5 morris function judging from fig 7 all of the sensitivity analysis methods show that the 7 model parameters have similar sensitivities and the analytical results confirm this the s ti has clashing colours for some of the model parameters until 4096 model evaluations and it is not until 65 536 model evaluations is reached for μ to have consistent colour for all of the model parameters the activity score as opg does not have consistent colours through all model parameters with the local linear model even at 65 536 model evaluations on the other hand both as ols and as qphd already provide consistent colours for all model parameters at either 256 or 1024 model evaluations in addition all of the activity scores can identify the dummy parameter due to the frequent switching of the ranks between all of the model parameters the sarrazin ranking stat ranking and the position factor are unable to conclude there is convergence of the rankings for any of the sensitivity analysis methods also the reproducibility is relatively low for any of the sensitivity analysis methods as well the reliability of all model parameters is also bouncing around 20 5 6 test function 6 friedman function the ranking results for the friedman function can be seen from fig 8 in accord with the exact rankings both s ti and μ rank x 4 as the most important parameters with x 2 and x 3 second and the remaining parameters are in the order of x 3 x 5 x 6 x 7 the s ti identifies x 1 and x 2 to be of similar sensitivity cells with the same colour from 1024 model evaluations onwards but μ achieves this at 256 model evaluations even though the ranking shifts for x 1 and x 2 for activity scoring both the global linear model version as ols and global quadratic model as qphd rank x 4 x 1 and x 2 of the same importance as s ti and μ however as ols and as qphd rank x 3 as the least important model parameter moreover the dummy parameter appears to have a higher rank than x 3 indicating a near zero sensitivity for x 3 for as qphd x 1 and x 2 have disagreement in colour at 256 model evaluations using the local linear model the activity score cannot provide a consistent ranking at any m as opg can identify that x 1 x 2 and x 4 are the most important model parameters but it cannot differentiate the exact ranking between these three model parameters as opg also ranks x 3 as less sensitive the analytical activity score ranks both x 1 and x 2 as the most important model parameters with x 4 next and this ranking result is different compared to any of the previous ranking results similarly the analytical activity score classifies x 3 as non important the sarrazin ranking indicates that the s ti reaches a converged ranking result from 4096 model evaluations onwards and both μ and as qphd reach converged ranking results at 1024 model evaluations as observed from fig 8 as ols achieves convergence at 256 model evaluations but as opg has a stat ranking value higher than 1 even at 65 536 model evaluations in terms of reliability using s ti x 4 reaches 80 and x 3 and x 5 reach 60 at 1024 model runs also for μ x 1 and x 2 have reliability more than 60 and that of x 3 is higher than 80 from 1024 model runs confirming the change in stat ranking for the activity scores it is hard to conclude anything from the reliability of each model parameter the position factor shows a disagreement for as ols and as qphd at 1024 to 4096 model evaluations and 4096 to 16 384 model evaluations respectively due to the rank shifts between the two important model parameters x 1 and x 2 on the other hand the position factor gives a value higher than 2 for as opg from 4096 to 65 536 model evaluations surprisingly the reproducibility is relatively high for the activity score approximated by the local linear model with a value more than 0 8 and the value is even higher than 0 9 for 1024 and 65 536 model evaluations despite the fact that the ranking is unstable 5 7 test function 8 sobol levitan function the sobol levitan function has two configurations which change the number of model parameters and the coefficients the ranking results of the first configuration can be seen from fig 9 and the second configuration from fig 10 for the first configuration fig 9 x 1 has a coefficient value of 1 5 higher than the coefficient value 0 9 of other model parameters and this makes x 1 to be the most important parameter followed by other parameters with the same sensitivity from fig 9 all of the sensitivity analysis methods can correctly rank x 1 as the most important model parameter for x 2 to x 7 total effect sensitivity analysis and the μ of morris yield similar sensitivity for these parameters but not until 16 384 model evaluations the sensitivity indices of the sobol method have the largest difference of 60 between x 2 and x 4 at 1024 model evaluations and this difference drops to 17 5 at 4096 model evaluations where the difference uses the smallest sensitivity index among x 2 to x 7 as the base for μ the difference is 14 5 at 1024 model evaluations and 12 5 at 4096 model evaluations between x 7 and x 5 and between x 5 and x 7 correspondingly both as ols and as qphd provide similar sensitivities for model parameters x 2 to x 7 at 1024 model evaluations the largest gap is for as ols at 4 04 for 1024 model evaluations between x 3 and x 6 and it is 6 64 for as qphd between x 3 and x 5 at the same m the activity score with the local linear model is unable to provide a stable ranking for this test function as well where x 4 and x 6 appear to have higher sensitivities than other model parameters even at 65 536 model evaluations activity scores other than as opg match the ranking results with the analytical activity score in terms of convergence the sarrazin ranking using stat ranking is unable to conclude convergence which is similar to the morris test function as the stat ranking is higher than 1 other than for as opg at 65 536 model evaluations the position factor is also unable to show any sign of convergence with x 2 to x 7 being of similar sensitivity for tdcc reproducibility is higher than average for any sensitivity analysis methods at any m for the reliability activity scores have 100 reliability for x 1 from 256 model evaluations onwards yet s ti and μ require at least 1024 model evaluations to reach 100 for other model parameters their reliability bounces between 20 and 40 and this low reliability matches the results of the sarrazin ranking stat ranking the first 10 model parameters have the same coefficient in the second configuration fig 10 whereas the other 5 model parameters have a different though the same value the difference in coefficients makes the first 10 parameters more sensitive than the other 5 parameters judging from fig 10 both μ and activity scores point out that x 11 to x 15 are the least important parameters at 256 model evaluations and the activity scores successfully discover the dummy parameter the s ti locates the least sensitive group from 1024 model evaluations onwards as it ranks x 12 to be more important than x 3 however some of the model parameters are still wrongly classified into the least sensitive group by some sensitivity analysis methods for s ti the index value of x 5 is 0 102 x 2 is 0 068 and x 12 is 0 038 at 1024 model evaluations these indices give an impression of three different sensitive groups rather than two at 4096 model evaluations the largest difference for the sensitive group is x 2 and x 3 with a value of 0 029 which is much better than the case at 1024 model evaluations likewise μ provides a consistent sensitive grouping at 4096 model evaluations since x 4 has a value of 0 229 x 9 is 0 308 and x 11 is at 0 196 for 1024 model evaluations for activity scoring with the global linear model the as of the sensitive group ranges from 0 282 to 0 313 whilst the insensitive group ranges from 0 122 to 0 142 at 1024 model evaluations for as qphd these ranges are 0 287 0 314 and 0 123 to 0 151 respectively therefore it suffices to say that as ols and as qphd behave better than s ti and μ for the activity score with a local linear model the sensitive group ranges from 0 280 to 0 321 the less sensitive group ranges from 0 130 to 0 150 comparable with the results from other activity scores the activity scores with three gradient approximation methods correctly pinpoint the dummy parameter matching ranking results with the analytical activity score identically to the case of the first configuration the sarrazin ranking stat ranking shows a significant disagreement between resamples for all of the sensitivity analysis methods and the position factor also fails to conclude anything due to the shifting of ranks between the different m for the reproducibility it is slightly above 0 5 on average for all of the methods the reliability of all model parameters fluctuates between 10 and 60 but less than 60 and this agrees with the high stat ranking 6 discussion for the more novel gsa method of activity scoring it is both intuitive and clear from the test function results that the method used for calculating the gradient can have a significant impact on rankings obtained the ishigami homma and friedman functions are the two test functions where all the three estimated activity scores provide different ranking results on specific model parameters compared to the analytical exact activity scores the differences in the gradients calculated affect the matrix c and the eigenpairs which ultimately causes the activity scores to be distinct for the ishigami homma function fig 1 the difference in the ranking between the estimated activity scores and the analytical activity score is mainly caused by the term x 3 4 sin x 1 if this term is changed to either x 3 3 sin x 1 or x 3 2 sin x 1 the estimated activity scores and the analytical activity score would provide the same ranking thus the global linear model and global quadratic model are not able to approximate such a high order response surface well it is this limitation that causes the approximated activity scores to rank parameters differently for the friedman test function fig 8 the main concern is the importance of x 1 x 2 and x 4 this function is an additive one for which we can split the two terms 10 sin πx 1 x 2 and 10x 4 out as another single function simplified as 33 f x sin π x 1 x 2 x 3 where the x i are uniformly distributed within 0 1 for eq 33 both as ols and as qphd would rank x 3 x 1 x 2 yet the analytical activity score would rank x 1 x 2 x 3 as in fig 8 there are two tipping points for this function to make the estimated activity scores and the analytical exact scores different in terms of ranking one is the π inside the sine function both the estimated and analytical activity scores provide the same ranking with x 3 x 1 x 2 if the constant π is changed to a smaller number such as 1 with π the sine function is sufficiently non linear to cause the gradient approximation methods to fail in identifying the model structure with smaller constant the sine function is more linear for the gradient approximation methods to be sufficiently accurate the other tipping point is the parameter range as long as the parameter range exceeds 0 44 0 89 the ranking from the estimated activity scores and the analytical score would remain different if the parameter range of each model parameter shrinks from 0 44 0 89 for example the lower bound is higher than 0 44 or the upper bound is lower than 0 89 the ranking from both estimated and analytical activity scores would agree with each other to be more specific the ranking from the estimated activity score remains the same but the analytical activity score changes the ranking with the change of parameter range this change in parameter range causes the number of dimensions of the active subspace to be different as the largest gap between the eigenvalues is between first and second eigenvalues in decreasing order but the shrinkage in parameter range makes the largest gap between the eigenvalues to be between the second and third eigenvalues in decreasing order with one less dimension in the active subspace the calculation of the activity score is highly impacted the active subspace method identifies the most important direction in the parameter space the activity score shows how much each model parameter contributes to the direction defined by the first n eigenvectors where n is the dimension of the active subspace for the friedman function activity scores consider x 3 to be non important unlike s ti or μ because the important direction identified by active subspaces is orthogonal to x 3 thus x 3 is not considered as an active variable from an integration point of view the gradient of the x 3 term is 40 x 3 0 5 and the integral of the outer product of this term with others after the input normalisation is 0 so x 3 has zero influence on any other directions in the parameter space similarly the directions of every model parameter of the sobol g function are orthogonal to each other with the setting of coefficients x 1 is designed to be the most influential parameter resulting in the largest eigenvalue gap being between x 1 and x 2 moreover the first eigenvector is in the form 100 0 t and this induces the activity scores of model parameters x 2 to x 7 along with the dummy parameter to be zero for the case of the sobol g function activity scoring using the global linear model identifies x 7 as the most important parameter and this is different from the other activity scoring methods the reason being that the approximated matrix c by the global linear model has numbers with very small magnitude in the entries of its first column c 1 3 72 e 8 1 963 e 7 1 465 e 7 2 63 e 8 3 03 e 8 2 97 e 8 7 81 e 7 5 6 e 9 t nevertheless the entries in the 7th row which corresponds to x 7 are slightly larger than other entries due to approximation error and this causes the 7th entry of the first eigenvector to be the largest thus x 7 becomes the most sensitive parameter there are certain cases where the activity score with the local linear gradient approximation behaves wrongly especially for the case of the modified sobol g function with g6 coefficient see fig 4 this behaviour is caused by the limitation of local approximation in exploring the parameter space when the response surface is too complex as will be covered in section 6 1 in a replication implementation in other words where more samples from different locations are taken the ranking provided by the activity score with the local linear model is more reasonable being based on different fundamental principles than either the variance based sobol method or the morris method the active subspace approach can yield activity scores that do not provide the same ranking as that from s ti or μ on the other hand results for the ishigami homma function fig 1 sobol g function fig 2 and friedman function fig 8 have characteristics that show active subspace methods yield a different perspective than other sensitivity analysis methods but aside from these three test functions activity scores compare well with what the s ti and μ provide indeed activity scoring can achieve this with a much smaller m for the modified sobol g function in the g6 fig 4 coefficient settings the activity scoring with the global quadratic model qphd can reach a stable ranking with as few as 1024 model evaluations compared to more than 65 536 for sobol and to 65 536 for morris methods furthermore as qphd can identify important model parameters that have similar sensitivity earlier than other methods in the model runs and this is shown from the results for the morris function fig 7 with 256 model runs and for the sobol levitan function with two configurations figs 9 and 10 with less than 4096 model runs for as qphd for the bratley and linear functions all activity scores perform as well as the benchmark methods in terms of achieving stable ranking at the same m 6 1 the performance of ranking measures in terms of the four ranking measures in the literature several conclusions emanate from the test function results described in previous sections and given in the supplementary files first the sarrazin ranking stat ranking highly depends on the quality of bootstrap resamples whereby poor quality of resamples may provide a false indicator of ranks for example as opg yields stat ranking 0 216 1 for the ishigami homma function at 256 model evaluations see fig 1 however as opg wrongly ranks the dummy parameter as the most important parameter the sarrazin ranking weights important model parameters more highly than less sensitive model parameters for the sobol g function the ranks constantly shift for x 2 to x 7 for the activity scores but stat ranking yields values less than 0 02 at any m however the shifting in rank of important model parameters with similar sensitivity renders stat ranking to be useless such as in the case of the morris function or sobol levitan function that have several important parameters with the same sensitivity analytically for the most important parameters the threshold of reliability for stat ranking to be less than 1 is about 60 and this percentage can rise to 80 for most cases the sarrazin ranking stat ranking agrees with the results of reliability with a large number of model parameters in a single model the sarrazin ranking stat ranking can provide a general view of the agreement between resamples and the reliability provides the ability to analyse model parameters of interest more closely thus these two measures could be used in complementary fashion the position factor measure unlike the sarrazin ranking stat ranking weights the change of ranks for insensitive parameters and sensitive parameters equally important and this can be observed from the results of any of the sa methods for the sobol g function as qphd ranks x 1 as the most important parameter from 256 model evaluations onwards but the insensitive parameters x 2 and the dummy parameter still change ranks with increases in m due to near zero activity scores and approximation error however the position factor gives values of more than 2 5 meaning ranking has not converged given the shifting ranks of insensitive parameters in terms of information gain the position factor measure does not provide additional information if one can visualize ranking results with something like the heat map design of this paper moreover just as with the sarrazin ranking stat ranking the position factor is unable to provide any valuable information when sensitive parameters have similar sensitivities furthermore the position factor only compares how ranking changed from the previous m to the current one to a certain degree however if the number of model parameters inputs is extremely large position factor could be a simple solution to avoid drawing a huge figure for visual inspection of ranking changes the tdcc measure with the savage score provides less information in terms of the meaning of reproducibility compared with other ranking measures for our cases 0 8 or more seems to be the threshold to indicate a stability in the ranking results but a value of more than 0 85 may be sufficient for the other ranking measures to also give a signal of converged ranking results again model parameters with similar sensitives are also something that tdcc fails to identify 6 2 confidence interval estimates of parameter sensitivity bootstrap versus replication for the ranking measures in section 4 all convergence calculations are based on bootstrap resampling with replacement 1000 times this number of resampled sets has been employed or recommended in several studies baroni and tarantola 2014 brunetti et al 2016 nossent et al 2011 sheikholeslami et al 2017 the bootstrap technique efron 1979 is widely used in the study of sensitivity analysis methods baroni et al 2018 khorashadi zadeh et al 2017 wang and solomatine 2019 as it does not require additional model runs to provide an estimate of the confidence interval here we discuss results from employing replication with our test functions to measure the comparative efficiency and accuracy of parameter confidence intervals provided by the bootstrap technique the replication technique is however a much more expensive way of generating confidence intervals for metrics of interest as it requires the experiment to be repeated with multiple sets of different samples known as replicates the usage of replication has been discussed in several studies sun et al 2021 tarantola et al 2012 yang 2011 though it appears to be much less popular than bootstrapping due to high computational cost standard deviation can be used here as a concise indication of confidence interval width under the assumption of a normal distribution the 95 confidence bounds can be obtained by calculating them as 7 84 times the standard deviation we checked the relevant histograms from both 1000 bootstrap resamples and 100 replications to confirm they follow a normal distribution moreover standard deviation is also a useful way to estimate confidence intervals when data are from a small population as occurs for the 100 replicates in our case we found for all test functions that standard deviations of parameter sensitivity from the bootstrap do not decrease as much as those from replication as an example fig 11 gives results for the sobol levitan function with the g6 configuration it shows the standard deviation across the parameter sensitivity measures obtained from both resampling and replication for each model parameter with every subplot on a log scale the first row of subplots provides the results from the bootstrap analysis and the second row from replication 100 times all the gsa methods rank x 1 as the most important parameter for this test function the other model parameters being of similar sensitivities excepting the dummy parameter for the activity scores for s ti and μ it appears that the standard deviation between resamples replicates for the most important parameter is slightly larger than the other model parameters it is less obvious to see the difference in standard deviation between bootstrap and replication for s ti and μ however the standard deviation from as ols and as qphd suggests that the bootstrap converges with a rate of approximately 1 n while the replication converges with a rate of approximately 1 n the bootstrap resamples do not use as much of the information provided by the sobol sequence as replication does as its convergence rate is less than expected on the other hand as opg appears to behave disorderly even at a large number of model runs this behaviour is due to the local linear gradient approximation model not making efficient use of increases in the sample size n moreover bootstrapping is less reliable as an assessment of standard deviation at small sample size notice for example the standard deviation results for s ti and μ at 256 model runs with increase in m the line of standard deviation becomes smoother indeed the bootstrap technique should be used with care as it relies on the smoothness and symmetry of the bootstrap distribution efron 1987 and the bootstrap resamples may hinder the convergence rate using the sobol sequence with the resamples no longer being a sobol sequence whenever computational budget is available researchers would ideally consider implementing replication to check the reliability of confidence intervals provided by the bootstrap technique similar plots to fig 11 for our other test functions can be found in section 3 of the supplementary file along with the heat map of ranking results from replication in section 2 of the supplementary file these figures accord generally with the conclusions above 6 3 the impact of sampling method on ranking by activity scoring random sampling vs sobol sequence among studies of the active subspace approach and activity scoring monte carlo sampling also called random sampling has generally been used as was indicated in the introduction it has however been noted e g guy et al 2020 that quasi monte carlo sampling might improve the performance of activity scoring over that of random sampling and our results confirm this statement and quantify the performance benefit for the modified sobol g function with g6 coefficient test function fig 12 illustrates the ranking results from activity scoring with the three different gradient approximation methods obtained from random sampling these are to be compared to the results obtained from sobol sequence sampling in fig 4 clearly activity scoring with the sobol sequence can achieve correct and stable ranking results with much fewer model runs than random sampling does it is not until reaching 16 384 model evaluations and only for the as qphd approximation that random sampling obtains the correct ranking yet using quasi monte carlo as qphd obtains the accurate ranking results at 1024 model runs and is stable from thereon with increasing sample size the performance of as ols with random sampling is even worse as it cannot rank the model parameters in the correct order with 65 536 model runs yet as ols achieves the correct ranking at 16 384 with sobol sequence sampling as opg does poorly with either random sampling or sobol sequence the advantage of the quasi monte carlo sampling method has been tested and approved for other sensitivity analysis methods in several studies niederreiter 1978 qian and mahdi 2020 sobol 1967 and multiple researchers have emphasized the importance of the sampling method for producing sound sensitivity measures andres 1997 castaings et al 2012 janssen 2013 from our experiments the importance of the sampling method also holds for activity scoring the ranking results for other test functions obtained by activity scoring using the random sampling method can be found in section 7 of the supplementary file in general they are consistent with the illustrative results for the modified sobol g function confirming and quantifying the benefit of quasi monte carlo sampling at least for our test functions the impact of the sampling method on the performance of both the variance based sobol method and the morris method is also a significant point of discussion in the literature and there are several studies that provide substantially more detail on the differences and development of sampling methods for either variance based sobol method or the morris method campolongo et al 2007 2011 qian and mahdi 2020 reusser et al 2011 sheikholeslami and razavi 2017 zhan and zhang 2013 7 conclusions the paper has investigated the performance of the novel method of activity scoring in comparison to two benchmark methods of global sensitivity analysis using several well known test functions that encompass overall a wide range of function behaviour heat maps that show both parameter sensitivities and numerical rankings with increasing sample size are introduced as a key visualization tool although the focus of the comparison is on ranking sensitivities our heat maps also show parameter sensitivity in order to provide additional information to the numerical rankings through testing against analytical solutions activity scores match the benchmark methods total effect sensitivity index of sobol and the absolute mean of elementary effect index of morris in most but not all test cases moreover activity scoring can be more computationally efficient than the benchmark methods in some instances in the case of the ishigami homma function sobol g function and friedman function where the activity score relies highly on the important directions obtained from the corresponding eigenvector analysis activity scoring does not provide as stable ranking results with increasing sample size as the total effect sensitivity index s ti or morris μ in the other cases activity scoring performs as well as s ti and μ and it behaves even better with the more appropriate selection of gradient approximation methods the performance of activity scoring depends on the accuracy of the gradient approximation model that is employed as gradient quality affects the detection of the number of active subspace dimensions for the ishigami homma function and friedman function the activity score based on different gradient approximation models provide different rankings even in comparison to the analytical exact activity score the analytical activity scoring results can be viewed as results that would be obtained with the perfect gradient approximation method thereby indicating the strong impact of better gradient approximation method for the performance of activity scoring of the three gradient approximation models employed in this paper the global quadratic model as qphd performs the best overall indeed it is more efficient than both s ti and μ in some instances such as for the modified sobol g function morris function and sobol levitan function moreover as qphd has an advantage over other methods in determining the most important parameters with similar sensitivities as is the case for the morris function and sobol levitan function in regard to the convergence of the four ranking measures both the sarrazin ranking stat ranking and reliability are to be preferred to the other two ranking measures as these two provide the most accurate information in addition stat ranking and reliability utilize all of the bootstrap resamples to provide indicators comparable with the traditional confidence interval of a given percentage say 95 this utility is not feasible for the current setting of the position factor or tdcc the advantage of stat ranking is that it provides a scalar as an indicator of convergence unlike reliability that evaluates the model parameter individually however reliability can explain certain changes in stat ranking with more detail nevertheless it is to be noted that usage of the sarrazin ranking stat ranking and reliability requires access to resamples and thereby is highly reliant on the quality of the resamples in addition the sarrazin ranking stat ranking and reliability perform poorly when multiple sensitive parameters have similar sensitivity or when these parameters should have the same sensitivity analytically a way of addressing the poor performance in such a situation is to rank with a chosen tolerance whereby model parameters with differences in sensitivity inside this tolerance are given the same rank however this tolerance should be chosen with care as it could falsely classify model parameters with different sensitivity analytically the same due to approximation error whenever using any of these four ranking measures it is still recommended that visualization of the ranking results is employed such as the heat mapping in this paper more examples of useful visualization tools can be found in pianosi et al 2016 in approximating confidence intervals for ranking purposes the bootstrap is naturally much cheaper than replication but it may suffer a loss in the expected convergence rate when using a sobol sequence as the bootstrap resamples may not comply with the optimal structure of the original sobol sequence with respect to the sampling methods the quasi monte carlo sobol sequence has a significant computational advantage in achieving ranking results compared to the results from random sampling as the sobol sequence requires much fewer model runs to achieve stability in ranking there is much opportunity for improving the applicability of activity scoring by addressing additional ways for approximating the function gradient one avenue is building surrogate models such as with sparse grid polynomial chaos or gaussian process emulation for high dimensional models with considerable non linearity a reliable and accurate gradient approximation model is crucial to active subspace performance and its activity scoring on ranking several studies have proposed the morris method as the first step of screening for important parameters and then implementing other sensitivity analysis methods on these selected important parameters activity scoring could serve as one of the sensitivity analysis methods for the second step the reuse of samples from the morris method can also be considered and tested for efficiency lastly the active subspace approach represents a different way of looking at sensitivity in contrast to the popular variance based derivative based or variogram based sensitivity analysis methods multiple perspectives and methods are invaluable to understand model behaviour and activity scoring has an obvious role to play in that arsenal software data availability library name saconvergenceanalysis contact person xifu sun e mail address xifu sun anu edu au year first available 2021 software required python 3 availability https github com xifus saconvergenceanalysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement xifu sun s research was funded by a scholarship provided by the mathematical sciences institute of the australian national university appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105310 
25641,biophysical models simulate dispersal and connectivity in marine environments by combining numerical models that represent water circulation with biological parameters that define the attributes of species the effects of parameters such as the number of particles released to simulate the trajectories of individual organisms is potentially large but rarely tested we present a framework to measure the optimal number of particles required to capture variability in dispersal and connectivity of the marine plants seagrasses we found that the number of optimal release particles per element or grid cell for dispersal estimates varied with seagrass habitat type season and physical parameters of the modelled propagules i e wind drag connectivity metrics were comparatively much less sensitive requiring lower particle numbers to achieve stable results we provide guidance on important factors to consider when determining the optimal number of particles required to robustly predict dispersal and connectivity in biophysical models of marine plants keywords biophysical model connectivity dispersal lagrangian particles individual based models seagrass data availability the data seagrass habitats and nodes shapefile slim hydrodynamic model processing and run files and mesh python sensitivity analysis csv and xls and their run files python and connectivity matrices csv and their run files python that support the findings of this study are openly available via research data jcu at https doi org 10 25903 3pe4 9n65 additionally the raw model outputs are stored on the qris cloud qcif collection q3871 https doi org 10 25903 1vje 6c38 1 introduction dispersal and connectivity are fundamental processes that shape the distribution structure and resilience of coastal and marine ecosystems cowen and sponaugle 2009 mcmahon et al 2014 the accumulated forces of ocean waves and currents are efficient vectors for long distance dispersal and influence metapopulation dynamics and the replenishment and recovery of populations after disturbance events nathan et al 2008 treml et al 2008 information on dispersal and connectivity is essential for the management of coastal and marine ecosystems as it underpins broad scale conservation planning and the design of marine reserve networks treml and halpin 2012 gaines et al 2010 furthermore dispersal and connectivity patterns set the spatial and temporal scales for effective interventions to counter the spread of nuisance species hock et al 2016 and for assessing environmental impact swearer et al 2019 empirical measures of connectivity are limited to a handful of species because of the cost and logistical constraints of assessing marine dispersal abesamis et al 2016 kool et al 2013 instead biophysical models are often used to simulate dispersal by combining numerical models that represent water circulation with biological parameters that define the attributes of a species swearer et al 2019 outputs of biophysical models can be exported to connectivity matrices to enable connectivity analysis using techniques such as network or graph theory e g thomas et al 2014 samsing et al 2017 grech et al 2018 simulating the trajectories and dispersal of an ensemble of individual organisms in biophysical models is facilitated by individual based models ibms that integrate multiple biological parameters such as larval and propagule duration and mortality pre competency period spawning or reproductive window and behaviour treml et al 2015a ibms include a random diffusion vector that is used to represent horizontal turbulent mixing at sub grid scales paris et al 2002 the random diffusion generates variability in dispersal trajectories meaning that multiple particles released from the same location at the same time and with the same biological parameters do not necessarily follow the same trajectory the amount of variability in dispersal trajectories can be adjusted by the modeler but generally increases as the number of released particles and duration of simulations increases the number of particles released in biophysical models therefore needs to be tested relative to the duration of simulations so that the particle number parameter is sufficient to capture variability in dispersal trajectories and to produce robust dispersal predictions north et al 2009 simons et al 2013 in practice the number of particles released per element or grid cell is more often viewed as a trade off between computational time and precision the effect of particle number on dispersal estimates is rarely assessed in a sensitivity analysis prior to the implementation of biophysical models swearer et al 2019 the goal of the present study is to resolve a methodological question critical to the implementation of robust biophysical models what is the optimal number of particles required to capture variability in dispersal and connectivity we focus on seagrasses because 1 previous studies on the sensitivity of dispersal predictions to particle tracking parameters use marine larvae rather than marine plants as a model taxon e g simons et al 2013 jones et al 2016 and 2 the complex life history and morphology of seagrass species necessitates a more refined approach to measuring the effect of particle number on dispersal seagrasses are capable of dispersing long distances grech et al 2016 kendrick et al 2017 mcmahon et al 2014 via the waterborne transport of viable propagules i e vegetative fragments fruits and plant fragments with attached fruits and seeds berkovic et al 2014 the maximum dispersal distances recorded in the literature are generally less than 100 km except during extreme weather events when dispersal has been recorded over distances of up to 400 km lacap et al 2002 buoyancy and survival times for seagrass propagules may be as long as 85 days in temperate species thomson et al 2014 but varies for subtropical species weatherall et al 2016 the reproductive period of seagrass species also occurs over many months and the complex morphology of fragments and fruits results in highly variable wind drag and decay rates lai et al 2018 2020 previous seagrass biophysical models use a wide range of particle release numbers to represent seagrass dispersal from 15 particles per location per day mari et al 2020 to almost 2000 jahnke et al 2017 but no studies have tested the sensitivity of dispersal estimates to particle number instead the decision behind the number of particles deployed in seagrass biophysical models is to optimize the total number of particles to ensure a reasonable computational load within the constraints of the modelling domain grech et al 2016 melia et al 2016 more often no justification is provided e g jahnke et al 2017 2020 and occasionally the total number of particles goes unstated e g sinclair et al 2018 cucco et al 2020 we advance the application of biophysical models by developing a procedure to determine optimal particle release numbers for species with long dispersal times and reproductive periods and with variable wind drags and decay rates we use a numerical approach to model the hydrodynamics and simulate the dispersal of virtual seagrass propagules in our study region torres strait australia fig 1 torres strait is an ideal case study because its complex hydrodynamics and diversity of species propagules and habitat types support the testing of model parameters across a range of biophysical settings the biophysical model s outputs were analysed as particle density distributions pdds to 1 assess the number of virtual particles required to robustly simulate the dispersal of seagrass propagules and 2 measure the effect of variable wind drag decay rate habitat type and season on particle number determining the optimal number of particles to release in biophysical models also requires consideration of the question that the model s outputs are intended to inform for example biophysical model outputs used to inform understanding of broad scale patterns of species connectivity e g mari et al 2020 require less accuracy than complex assessments of connectivity for precision pest management e g hock et al 2016 samsing et al 2019 despite these differences the effect of particle tracking parameters on connectivity estimates has not previously been assessed in this study we use the results from nearly 1000 simulations of a seagrass biophysical model to measure the sensitivity of changes in particle number on commonly used connectivity measures e g degree closeness centrality and local retention in network analysis we conclude by providing guidance on important factors to consider when determining the optimal number of particles required to robustly predict dispersal and connectivity in biophysical models 2 methods 2 1 study region and species torres strait between mainland australia and papua new guinea covers an area of approximately 48 000 km2 it is comprised of shallow seas and over 150 islands 18 of them inhabited fig 1 torres strait islanders maintain a strong affiliation with their land and sea and native title determinations have been made for the sea region and most of the islands the marine and coastal region of torres strait encompasses coral reefs seagrass meadows mangroves and sandy rocky and muddy beaches pitcher et al 2007 the extensive seagrass habitats of torres strait support commercial and indigenous fisheries and globally significant populations of dugongs and green turtles marsh et al 2015 torres strait has over 17 000 km2 of known seagrass meadows poiner and peterkin 1996 carter and rasheed 2016 with more areas being added as surveys are completed twelve species are recorded from 3 families encompassing nearly 20 of the world s known species meadows are typically abundant in near shore waters on reef platforms and in the south west region of the strait but can be found in most locations where suitable bottom sediment are present spatial geographic information system gis layers of seagrass distribution in torres strait were sourced from carter et al 2014 and carter and rasheed 2016 these layers were supplemented with information on the potential distribution of seagrass in the unmapped area between badu and boigu islands fig 1a derived from a model of dugong distribution and relative abundance marsh et al 2015 dugongs are specialist feeders on seagrass and their distribution is correlated with seagrass presence in queensland waters grech et al 2011 we extracted areas of very high dugong density between badu and boigu islands from marsh et al 2015 and combined it with the carter et al 2014 and carter and rasheed 2016 layers to generate the final spatial layer of torres strait seagrass distribution fig 1a torres strait has a complex hydrodynamic environment saint cast 2008 wolanski et al 2013 the eastern side of the strait is connected to the great barrier reef gbr and the coral sea and the western side is connected to the gulf of carpentaria and the arafura sea flows through the strait are barotropic generated by the wind driven difference in the mean sea level between the coral sea and the gulf of carpentaria the net flow through torres strait runs east to west and is relatively weak 0 04 0 13 sv saint cast 2008 wolanski et al 2013 at daily time scales the distinct tidal regimes from the coral sea and the arafura sea propagate into torres strait generating highly complex tides within the strait saint cast 2008 that are then further distorted by shallow water effects leblond 1991 wolanski et al 1988 the tidal currents generate fast flows peaks from 1 5 to 3 0 m s 1 saint cast 2008 within the strait as they interact with the complex bathymetry shoals reefs and islands south easterly trade winds which typically occur from march until september also generate a wind driven current that flows north into torres strait from queensland s east coast 2 2 biophysical model 2 2 1 hydrodynamic model torres strait currents were simulated using the depth averaged i e two dimensional version of the second generation louvain la neuve ice ocean model slim lambrechts et al 2008 a full description of slim a justification for the use of a depth averaged model in torres strait and a description of the model setup and validation are provided in the supporting information in brief slim solves the shallow water equations discretised in space by a second order discontinuous galerkin finite element method on an unstructured triangular mesh and in time with a second order implicit runge kutta method lambrechts et al 2008 our model domain matched the domain used by wolanksi et al 2013 however we generated a custom grid and forced our model differently the eastern side was open to the gbr and the coral sea and the western side was open to the gulf of carpentaria and the arafura sea fig 1 the longitudinal coverage of the domain was sufficient to capture the sea level difference between the eastern and western sides of torres strait that generates the barotropic flow through the strait the variable resolution mesh of slim was made finer with proximity to land and coral reefs to effectively capture the horizontal shear generated as currents flow in the complex bathymetry topography fig 1c the mesh was also made finer with proximity to seagrass meadows to increase the resolution of our analyses specifically the resolution ranged from 8 km in open water to 650 m near land reefs and the boundaries of seagrass meadows the model was forced by wind over the entire domain and by a combination of currents tides and sea surface elevation gradients at the open boundaries the hydrodynamic simulation ran from september 2011 the start of the peak reproductive period of seagrasses to february 2012 the end of the window of viability the window of viability was calculated as two months after the peak reproductive period december 2011 given the maximum probable survival time of seagrass propagules in the tropics is 8 weeks 56 days harwell and orth 2002 lacap et al 2002 hall et al 2006 källström et al 2008 kendrik et al 2012 thomson et al 2014 the 2011 2012 season was chosen because it had average physical conditions specifically the winds in this season matched the long term trends measured at the australian bureau of meteorology s coconut island poruma island id 027054 weather station in central torres strait figs s1 and s2 the wind conditions changed markedly over the simulation period fig s1 the relatively weak 5 m s 1 south easterly winds persisted from september to november 2011 the wind from december 2011 onwards was more variable in direction and speed there was a prevailing northerly wind of variable strength in december 2011 and january 2012 strong north westerlies 5 m s 1 and weaker south easterlies 4 m s 1 were common in february 2012 the hydrodynamic fields were simulated every 5 min and saved every 30 min the slim outputs were validated against field data and the outputs of an alternate model however our options for validation were limited by the remoteness of torres strait and the scarcity of hydrodynamic data available for the region specifically the modelled sea surface elevation was validated against a tide gauge at thursday island in southern torres strait and against a tide gauge at booby island located in the west of the strait figs s3 and s4 the slim velocity field was compared to the depth averaged month averaged current fields from the eight year 1997 2004 simulation of geoscience australia s torres strait circulation model fig s5 saint cast and condie 2006 overall the slim output was in good agreement with the available validation data the normalised root mean square error nrmse of the comparisons between the tide simulated by slim and the measured tides were 0 02 0 06 and the major monthly flow features delineated in the interannual simulations of geoscience australia s model were also simulated by slim given the match of the slim outputs with the validation data we have relative confidence in the simulate hydrodynamic fields despite the limited opportunity for validation 2 2 2 propagule dispersal the dispersal of passive i e particles unable to direct their own motion virtual seagrass propagules was simulated by coupling a lagrangian particle tracker model with the hydrodynamic model the hydrodynamic fields were re interpolated to match the particle tracker model time step of 5 min the particle tracker model was developed following thomas et al 2014 and grech et al 2016 in the particle tracker a random walk formulation of the 2d advection diffusion equation was parameterised with an okubo scheme okubo 1971 as outlined in spagnol et al 2001 and hunter et al 1993 the equations were 1 x n 1 x n v n δ t r n r 2 k h δ t 2 v n u c w u w k h h h k h x n 3 k h α l 1 15 x n v n δ t where the position of a seagrass propagule at time n 1 x n 1 was determined by its position at time n x n the advection velocity v n a random component r n is a horizontal vector of zero mean random numbers of variance r the horizontal diffusivity from sub mesh scale turbulent mixing k h and the time interval between iterations δt v n was determined by the depth average horizontal water velocity u the wind velocity at 10 m above the water surface u w the wind drag coefficient c w k h and the water column depth h k h was modelled as a function of the local mesh size l and the value of the coefficient α was set to 2 10 4 m0 85 s 1 okubo 1971 the propagules were modelled as either floating at the surface or suspended in the water column table 1 to partially account for the variety of structurally diverse forms of seagrass propagules i e vegetative fragments fruits and plant fragments with attached fruits and seeds the dispersal of the floating propagules was programmed to be influenced by the wind where the magnitude of the influence was set by the value of c w i e wind drag coefficient physically c w is dependent on a propagule s mass shape and buoyancy and by the angle it faces in relation to the wind direction two c w s were modelled for the virtual seagrass propagules in this study again to account for the structural diversity of propagules the c w was set to either 1 or 2 based on the results of 1 lai et al 2020 who performed flume tank experiments on vegetative fragments from tropical seagrasses and calculated that the average c w approached 1 and 2 grech et al 2016 who performed a biophysical modelling sensitivity analysis and determined that a c w of 2 was conservative for the propagules of tropical seagrass species to simulate the dispersal of propagules below the water surface c w was set to 0 so v n was solely determined by the water currents on each simulation start date staggered hourly releases were performed over a 24 h period so the propagules were released in all states of the tide grech et al 2016 virtual propagules were released during the peak reproductive period of seagrass in torres strait september 2011 to december 2011 and all simulations were run for 8 weeks 56 days the maximum probable survival time of seagrass propagules in the tropics table 1 post release the propagules could settle via two mechanisms beaching and sinking beaching was modelled following previous studies of passive particles e g hock et al 2016 2017 whereby propagules that came within 100 m of land were assumed to experience stokes drift net drift shoreward in the direction of wave propagation stokes 1847 and beach for settlement via sinking the floating trajectories of propagules were modelled to decay through time according to a first order decay function erftemeijer et al 2008 grech et al 2016 4 c t c 0 e k t where c t the propagule concentration in the domain at time t was modelled as a function of the initial particle concentration c 0 and the decay rate k given the uncertainty in k three different rates were simulated k was set to 0 075 day 1 following erftemeijer et al 2008 half 0 075 ½ k 0 0375 day 1 or double 0 075 2k 0 15 day 1 representing the sinking dynamics of seagrass propagules with a depth independent decay rate units day 1 is a physical simplification of sinking dynamics in our model representation when propagules become negatively buoyant and their floating trajectories start to decay they instantaneously sink to the bottom in reality given the energetic environment of torres strait the propagules would probably be dispersed through the water column by turbulent mixing until they were caught in the bottom boundary layer where they could sink to the bottom in biophysical models this is captured with the inclusion of a sinking rate units m day 1 e g condie and bormans 1997 despite the physical simplification the choice to use a decay rate in this study instead of a sinking rate was justified for two reasons firstly biophysical modelling studies of seagrass dispersal regularly use decay rate s erftemeijer et al 2008 making them highly applicable in this sensitivity analysis which considers common model parameters secondly seagrass propagules can remain positively or neutrally buoyant for up to 85 days before they start to sink thomson et al 2014 and torres strait is a relatively shallow system 20 m in the central strait fig 1b in combination these factors suggest that the time propagules take to sink following the onset of negative buoyancy is likely short compared to the prolonged period of positive or neutral buoyancy 2 3 sensitivity analysis the biophysical model was applied in a two phase sensitivity analysis to determine the optimal number of virtual particles required to 1 capture variability in dispersal trajectories of seagrass propagules and 2 measure the connectivity of seagrass meadows in torres strait 2 3 1 dispersal we followed a modified process of simons et al 2013 to determine the optimum number of virtual particles required to sufficiently capture the dispersal of seagrass propagules ten release locations were selected to reflect the range of hydrodynamic conditions of torres strait and four seagrass habitat types offshore and deep od n 3 inshore and shallow is n 3 offshore and shallow os n 2 and reef top rt n 2 fig 1a particles were released from the centroids of the elements or grids that intersected the ten locations fig 1a and c the areas of the elements ranged from 0 37 to 16 18 km2 due to the finite element method used in the hydrodynamic model the sensitivity of propagule dispersal to changes in wind drag c w decay rate k month of release and release location were examined in 36 simulations virtual particles with all nine combinations of k ½k k 2k and c w 0 1 2 were released on a waning gibbous moon near the beginning of each of the four months in the 2011 peak reproductive season 8 september 8 october 7 november and 7 december table s1 in each of the 36 simulations 100 008 particles were released per location making a total of just over 1 million particles per simulation following simons et al 2013 the distributions of settled particles from each model simulation release location combination were quantified as particle density distributions pdd and the particle numbers required to robustly simulate the distributions were determined through comparing reference and subsample pdds in python3 version 3 6 5 each reference pdd was calculated from the distribution of all the virtual particles that settled within the model domain during the simulation period while each subsample pdd was calculated from a random subsample of the settled particles the reference pdd was compared to 500 subsample pdds per evaluated subsample size subsample size range 3 to 5000 virtual particles for each comparison the reference pdd and a subsample pdd were sorted into vectors with the same element order and correlated via pearson s correlation the fraction of unexplained variation fuv between the two pdds was then calculated from the correlation coefficient r as follows 5 f u v 1 r 2 then for each of the evaluated subsample sizes an fuv distribution was generated from the 500 fuvs and the number which 95 of the distribution lay below was taken as the fuv upper bound the fuv upper bounds were plotted against subsample size finally the number of particles required to generate what we define as a robust pdd was determined as the number at which the fuv upper bound intersected a 0 05 cut off comparable to a 95 confidence level fig s8 the procedure simons et al 2013 used to calculate pdds was developed for a hydrodynamic model with a mesh in a cartesian grid format we therefore adapted their procedure to account for the variable resolution mesh used in this study in addition to the steps outlined by simons et al 2013 our element wise counts were standardized by area given the different areas of the mesh elements further simons et al 2013 reduced the amount of noise and detail in their pdds by smoothing them with an isotropic gaussian filter this was done for practicality as it reduced the number of particles required to achieve a robust pdd by an order of magnitude substantially reducing the computing costs we used weighted averages rather than an isotropic gaussian filter to smooth our pdds as it worked better with our variable resolution mesh the averages were weighted by the distances between an element s centroid and the centroids of its surrounding elements the weighted averaging was implemented in two stages in the first stage weighted averages were performed on all elements that contained particles and the elements directly adjacent to them without particles in the second stage the set of elements averaged included all elements from the first stage and also the empty elements adjacent to them the smoothing therefore extended the pdd 2 elements beyond the initial distribution of settled particles fig s9 importantly the smoothing of our pdds with weighted averages reduced the number of particles required to achieve a robust pdd by an order of magnitude similar to the effect of the spatial filter applied by simons et al 2013 the influence of the release habitat inshore shallow is offshore deep od offshore shallow os reef top rt on the relationship between the number of particles np required to robustly represent a pdd nppdd and the area covered by the pdd area pdd was tested a power regression was performed as the data were heteroscedastic and so both the np pdd and area pdd data had to be log10 transformed the power regression involved first modelling the transformed data using a linear regression table s3 note the transformed data satisfied the regression assumptions of normality and homoscedasticity fig s10 habitat specific equations describing how log 10 np pdd was a linear function of log 10 area pdd were fitted with the parameter estimates from the linear regression table s4 both sides of the equations were then raised to the power of e to get the habitat specific equations describing how nppdd was a function of the area pdd raised to a power table s4 the initial linear regression model had the form 6 log 10 n p p d d β 0 β 1 log 10 a r e a p d d β 2 i i s β 3 i o d β 4 i o s β 5 log 10 a r e a p d d i i s β 6 log 10 a r e a p d d i o d β 7 log 10 a r e a p d d i o s where rt was the reference habitat and i is i od and i os were dummy variables indicating the habitats os od and os respectively an analysis of co variance ancova was run on the linear model to test if there was an interacting effect of log 10 area pdd and release habitat on log 10 np pdd table s5 the statsmodels version 0 12 2 seabold and perktold 2010 package in python3 was used to conduct the linear regression and the ancova the effect of propagule properties c w and k and different environmental conditions release month and release habitat type on the calculated virtual particle requirements for robust pdds np pdd was tested with a four way anova in r v 3 4 3 r core team 2020 table s6 all of the factors in the model were set as fixed the particle number data were log10 transformed to satisfy the assumptions of normality and homoscedasticity fig s11 the astronomical forcing of tides in torres strait is distorted by shallow water effects leblond 1991 wolanski et al 1988 and consequently tidal ranges from 1 7 m to 2 9 m at thursday island fig s3 were measured across the simulation start dates even though the dates fell on the same lunar phase however the projected particle number requirements were found to be insignificantly effected by variations in the tidal range present at the start of model simulations figs s6 and s7 table s2 the results of the present sensitivity analysis were therefore unlikely to be influenced by the differences in tidal ranges on the simulation start dates 2 3 2 connectivity after determining how many virtual particles were needed to robustly simulate the dispersal of seagrass propagules we tested if estimates of seagrass connectivity had similar requirements we ran a series of simulations where virtual particles were released from the elements that intersected the mapped seagrass habitats fig 1 with release numbers per element differing across 9 treatments from 25 to 2500 virtual particles treatments 25 100 250 500 750 1 000 1 250 1 500 and 2500 the minimum 25 and maximum 2500 values were chosen because 1 the results of the dispersal sensitivity analysis indicated that 1500 particles are needed to confidently simulate the dispersal of structurally diverse seagrass propagules under most conditions and over 2500 particles are needed to simulate dispersal under all conditions see results and 2 in previous studies 25 virtual particles have been released per site e g ruiz montoya et al 2015 the total number of particles released in the simulations ranged from 196 725 25 particles per element to 19 672 500 2500 particles per element the simulation sets for each of the nine treatments consisted of 108 simulations total 972 simulations specifically for each treatment propagules with each of the nine combinations of k ½k k 2k and c w 0 1 2 were released on one quarter one between and one new or full moon in each of the four months in the peak reproductive period in 2011 5 8 and 12 september 4 8 and 12 october 3 7 and 11 november 3 7 and 11 december table s1 virtual particles were released from the centroids of the 7869 elements that intersected the mapped seagrass habitats fig 1 the elements that intersected mapped seagrass habitats had a combined area of 13 070 km2 range 0 16 19 0 km2 following grech et al 2018 outputs of the individual simulation sets were pooled resulting in one dataset per release number treatment seagrass habitat patches were generated by dividing the spatial layer into 195 units range 0 46 650 22 km2 mean 67 0 km2 fig 1a large meadows were subdivided while smaller meadows of reef top and inshore seagrass in close proximity to each other were grouped we calculated the cumulative settlement of virtual particles on seagrass habitat patches in python3 and imported the results into weighted and directed connectivity matrices differences in spatial patterns of connectivity across the nine connectivity matrices were analysed using network analysis the networks referred to as habitat graphs were a set of nodes that represent discrete seagrass habitat patches n 195 and edges or links between nodes indicate functional connections edges were weighted by the number of virtual particles from every habitat patch i to every other habitat patch j we did not use a connectivity strength threshold to remove weak connections andrello et al 2017 as this would have disproportionally influenced the habitat graphs of treatments with smaller particle numbers we used gephi 0 9 2 to calculate a range of network and node level metrics to define the nine habitat graphs derived from the connectivity matrices of the nine particle number treatments we used a combination of weighted and unweighted i e independent of particle number network and node level metrics to enable comparisons between the different treatments modularity and the louvain algorithm blondel et al 2008 were used to detect communities or clusters of nodes that were densely connected to each other and weakly connected to other nodes in the network thomas et al 2014 grech et al 2018 we used linear mixed effect models package lme4 bates et al 2015 in r v 3 6 3 with node as a random factor to test for the effects of the nine treatments on the following node level metrics 1 degree number of unweighted edges incident to a node 2 closeness centrality average shortest path length between a node and all other nodes 3 local retention proportion of particles that remained within the spatial extent of the node and 4 pagerank weighted measure that is used to identify important source meadows taking into account the full topology of the habitat graph allesina and pascual 2009 smaller elements had a higher number of released particles per unit area than larger elements however this should not impact on the interpretation of our results as most of the connectivity metrics that we used in the analysis were unweighted 3 results 3 1 dispersal the seagrass habitat type timing of simulation and wind drag coefficients c w had interacting effects on the number of virtual particles required to robustly simulate the dispersal of seagrass propagules broadly and among all release habitats the number of particles required increased in proportion to the area settled by the particles raised to a power fig 2 table s4 however the rate of increase differed significantly by habitat ancova interaction term f 3 352 22 78 p 0 0001 table s5 in particular the particle requirements increased substantially faster with relative increases in area for the reef top habitat compared to the other habitats fig 2 when tested explicitly the release habitat type had a significant effect on the number of virtual particles required to robustly simulate dispersal four way anova f 3 216 468 85 p 0 0001 fig 3 table s6 for example a vast majority of the propagules released from the inshore shallow seagrass habitats were quickly advected toward the coast and beached table 2 relatively small numbers of virtual particles were required to capture the dispersal of propagules released from these habitats 191 in all simulations fig 3 in contrast hundreds more propagules were required to capture the dispersal from the offshore deep locations from which large numbers of propagules were advected westward and lost from the model domain mean range 49 71 the highest release numbers were required to simulate dispersal from locations where propagules were commonly advected into the open waters of torres strait and away from the coast i e offshore shallow locations 8 and 9 and the reef top location 10 fig 1a moderate numbers of these propagules were retained within the model domain less than 42 lost on average they settled by sinking rather than beaching and therefore settled over large areas while the maximum number of particles required to robustly simulate dispersal was less than 1500 for nine of the ten particle release locations it jumped to 2761 for one of the reef top locations the release month and release habitat type had a large and significant interactive effect on the particle number requirements four way anova f 9 216 7 49 p 0 0001 table s6 fig 3 the wind and circulation patterns dispersed propagules westward at the beginning of the reproductive season september october in 2011 figs s12 s15 later in the season november december the wind changed from being predominantly south easterly to predominantly northerly and the coral sea coastal current developed and impinged into the eastern sector of the model fig s5 these changes in wind and circulation particularly affected the settlement distributions and resulting particle number requirements for particles released from locations in eastern torres strait e g reef top id 10 fig 1a the effect of wind drag on particle number was statistically significant when considered independently four way anova f 2 216 8 08 p 0 001 and also interacted with the release habitat four way anova f 6 216 8 03 p 0 0001 and month four way anova f 6 216 3 81 p 0 01 to have significant effects on the particle number requirements table s6 fig 3 trends in the particle number requirements were projected with increases in the wind drag depending on how the wind effected the fates of particles i e beaching sinking or leaving the model domain the decay rate had a variable but non significant effect on the number of particles required to robustly simulate dispersal 3 2 connectivity the nine habitat graphs shared similar network measures table 3 however there was a difference in the graph structure for simulations with 100 release particles fig s16 the number of communities ranged from 6 to 8 and all graphs featured only one component and had short diameters 6 7 relative to network order 195 network density ranged from 0 21 to 0 37 and exhibited the same diminishing growth phase as network size between 500 and 1500 particles the node degree distributions fig s17 of the nine habitat graphs all showed properties of a random graph with degree distribution similar to a gaussian bell shaped curve fig 4 shows the estimated marginal means and 95 credible intervals for the linear mixed effect models used to measure the effect of particle number on the node measures degree closeness centrality local retention and pagerank a pairwise comparison with a tukey s test showed a significant effect of particle number on degree 1000 particles closeness centrality 750 particles but limited to no effect on the weighted measures pagerank 250 particles and local retention 4 discussion biophysical models of species dispersal in marine systems are often implemented without testing for the sensitivity of modelled outputs to input parameters with potentially large consequences on model inference and rigor of subsequent analyses in this study we present a framework to identify the optimal number of particles required to robustly simulate dispersal in a biophysical model of marine plants seagrasses with complex life histories advection and sinking settlement modes we also assessed the sensitivity of connectivity metrics to particle number the number of optimal release particles per element or grid for dispersal estimates varied with seagrass habitat type release month and wind drag of the modelled propagules connectivity metrics were comparatively much less sensitive requiring lower particle numbers to achieve stable results this dichotomy highlights the need to tailor sensitivity analyses in biophysical models and the resulting particle number requirements to the research question other marine plants such as mangroves and macroalgae share similar complex life histories and morphology as seagrasses and the approach presented here could be applied to biophysical models in these ecosystems 4 1 sensitivity of dispersal to particle number spatial release habitat type temporal release month and morphological wind drag coefficient c w factors effected the number of particles required to robustly capture the potential spread of seagrass propagules we found the particle number requirements depended on the spread and complexity of the settlement distributions this aligned with the results of simons et al 2013 who found that the number of particles required to robustly capture the distribution of passive animal larvae increased with area and underlying complexity of the larval distribution in the present study with seagrass propagules as the model organism the spread and complexity of the distributions were determined by the relative proportions of particles settling via beaching versus sinking and by the loss of particles from the domain the release context i e release habitat and timing of release and the propensity of particles to catch the wind i e the wind drag were directly linked to these settlement and loss dynamics these results suggest that effectively capturing the nuances of dispersal trajectories for marine plants will require releasing enough particles to robustly represent dispersal across the full range of habitats and wind drag coefficients and over the entire reproductive window in contrast to the other parameters changing the decay rate in the biophysical model did not significantly influence the particle number requirements in some instances the expected effect was observed where increasing the sinking i e the rate of decay through time of the floating trajectories made more particles settle faster and over a smaller area so less particles were needed for robust simulations this trend was commonly measured at the offshore shallow site however in other instances the particle requirements increased with increases in the decay rate or changing the decay rate had no measurable effect these counter intuitive results can be explained by interactions between decay rates and settlement dynamics for example where a high proportion of particles settled via beaching an increase in the decay rate would mean more particles sunk before they beached resulting in a more complex settlement distribution and requiring more particle for a robust representation given the biological importance of the sinking of viable marine plant fragments and their subsequent establishment sherman et al 2016 thomson et al 2014 and the potential for the decay rate to effect dispersal estimates the importance of this parameter should not be discounted in future biophysical models our results broadly indicate that less particles are required where dispersal is more strongly constrained by geography and or species biology for example the inshore shallow release locations were all adjacent to land barriers which facilitated the settlement of particles via the beaching mechanism due to this geographic biological constriction of dispersal fewer particles were required to capture the spread of particles from the inshore shallow locations in contrast dispersal from the reef top locations was much less constrained geographically and the particle number requirements for these locations were greater a divergent biological control was recorded for dispersal from reef tops where greater wind drags constrained dispersal when particles were blown toward land or toward the model boundary and facilitated dispersal when particles were blown into open water a reduction in the spread of particles with the geographic biological constraint of dispersal has been documented in other biophysical modelling studies e g critchell and lambrechts 2016 treml et al 2015a treml et al 2015b schlaefer et al 2018 but the associated reduction in particle number requirements has scarcely been documented however simons et al 2013 reported a reduction in particle requirements with a biological control on dispersal where less particles were needed to confidently simulate dispersal over shorter time periods i e shorter larval phases the dependence of particle requirements on the geographic biological restriction of dispersal could have substantial implications for example lesser requirements when modelling the dispersal of species inhabiting semi enclosed estuarine bays versus more open coastal habitats 4 2 sensitivity of connectivity to particle number we used a series of unweighted network measures to assess how changes in the number of release particles affected the detection of edges or links between nodes we found that increasing the number of release particles 500 per element had a diminishing return on the detection of new edges indicated by network size table 3 as well as other network measures the nine habitat graphs representing the outputs of simulation sets with 25 100 250 500 750 1 000 1 250 1 500 and 2500 particles released per element were densely connected and featured only one component with a short network diameter relative to network order the number of communities were also not affected by particle number our results indicate that simulations that use relatively low numbers of particles are able to effectively predict network measures when systems exhibit high levels of connectivity between patches we found that seagrasses in the torres strait form a highly connected system in alignment with the biophysical modelling of wolanski 2017 who projected that floating seagrass propagules in the strait could disperse tens to over one hundred kilometres high connectivity of seagrasses in torres strait is facilitated by long survival times of fragments and seeds variable wind drag and long reproductive periods intersecting with fluctuating hydrodynamic environments similar properties are likely to be exhibited in other marine plant ecosystems along open coastlines at a node level we found that degree and closeness centrality were significantly affected by particle number indicating that greater precision is required to capture variability in connectivity measures at the node level however local retention was not significantly affected by particle number because it is a relative measure of the proportion of particles that remain within a source patch in cases of systems with lower connectivity e g large coral reef mosaics with short reproductive periods hock et al 2017 further sensitivity analyses are needed to assess the effect of particle number on connectivity measures at network and node levels 4 3 contrasting dispersal and connectivity sensitivities we found that the number of released particles per element required to effectively capture dispersal of seagrass propagules in torres strait ranged over three orders of magnitude from 5 to 2761 consequently over 2700 particles would need to be released per element to confidently simulate dispersal of seagrasses in the strait in every month in the peak reproductive season and for the range of propagule wind drags in contrast there was limited benefit to increasing particles beyond 1000 particles per element for commonly used network and node level connectivity measures our finding of greater particle requirements for dispersal versus connectivity metrics may hold for other systems and highlights the need to tailor sensitivity analyses to specific research questions 4 4 considerations for biophysical modelling in other systems this study focused on the particle requirements for quantifying dispersal and connectivity in seagrasses similar studies based in other biophysical systems need to consider the effects of system specific factors that were not addressed in this study for example for seagrass in torres strait it was appropriate to use a depth averaged model because the strait waters are shallow and vertically well mixed wolanski et al 1988 lambrechts et al 2008 and the biological component included representations of the different positions that seagrass propagules could occupy within the water column i e at the surface and suspended in the water column three dimensional models are necessary in systems where the waters are deep and or vertically stratified and or the species being modelled perform complex vertical behaviours such as ontogenetic vertical migration e g allain et al 2001 paris et al 2007 greater numbers of particles would be required to capture the range of possible dispersal trajectories and connectivity networks in these more complex systems compared to the requirements presented here the horizontal swimming of marine animal larvae capable of influencing their horizontal dispersal trajectories in the later stages of their pelagic phases e g fish larvae fisher et al 2000 invertebrate larvae young 1995 would also add complexity requiring more particles to capture variability in dispersal trajectories and connectivity furthermore marine plant propagules such as mangroves di nitto et al 2013 van der stocken et al 2019 and the larvae of most marine animals must spend some time in the pelagic before they are competent to settle the reproductive or competency period and pelagic larval duration of marine plants and animals varies from minutes to months di nitto et al 2013 shanks 2009 van der stocken et al 2019 a higher proportion of individuals in species with obligatory pelagic larval phases may travel farther relative to species that are immediately viable although larval duration is not related to dispersal distance in some circumstances shanks 2009 in the context of biophysical modelling more release particles per element could be needed to robustly simulate the dispersal and connectivity of species with pre competency periods and obligate pelagic larval phases finally for studies on connectivity an important consideration is that the spatial and temporal coverage needs to match the space and time required to adequately characterise the connectivity which is system specific e g condie et al 2018 we recommend that any study of marine dispersal or connectivity should analyse the sensitivities of particle number requirements taking into account all particle parameters such as vertical and horizontal swimming and pelagic larval duration 4 5 model limitations uncertainty in the parameterisation of the biological component of biophysical models including a lack of knowledge about release locations and the timing of release translates to uncertainty in the model outputs swearer et al 2019 in our analysis we incorporated this uncertainty by using a range of biological parameters i e wind drags and decay rates over an entire reproductive season and with the maximum number of release locations we also attempted to minimise uncertainty by basing our study in torres strait where knowledge of seagrass habitats is advanced carter et al 2014 carter and rasheed 2016 marsh et al 2015 however placing the study in the remote torres strait did add a small level of uncertainty in the simulated hydrodynamic fields given the limited data available for model validation over the simulation period although the simulated fields did strongly match the available validation data the small level of uncertainty in the simulated hydrodynamics carried over into some minor and unavoidable uncertainty in the modelled dispersal and connectivity another area of uncertainty that we could not account for was the loss of particles from the model open boundary particles that were lost from the boundary were assumed to be unable to re enter the area and this assumption is common in biophysical modelling studies over hourly time scales some particles may have been erroneously lost when advected over the boundary by tidal currents that would have transported them back into the domain with the change of the tide however these particles had to be close to the boundary within the tidal cycle that expatriated them and so most of them would have ultimately been expatriated by the net currents flowing over longer time scales for example a high proportion of the particles released from locations near the western boundary were lost the inability of these particles to travel against the net east to west current that runs through torres strait and re enter the strait is physically realistic 4 6 conclusions the difference in input parameter requirements to robustly capture dispersal versus connectivity in biophysical models highlights the importance of sensitivity analysis and matching model accuracy to the research question we demonstrate that variability in life history morphology and habitat type of marine plants must be accounted for when measuring the sensitivity of the number of release particles per element our torres strait case study provided an ideal opportunity to test the effect of interactions of complex hydrodynamics with a broad range of habitat types season and wind drag parameters we found the number of particles required to robustly predict dispersal in complex systems is higher than previous studies in similar systems e g grech et al 2016 other systems with for example unidirectional flow and lower species diversity may not require as many particles for robust simulations e g evans et al 2021 sinclair et al 2018 ruiz montoya et al 2015 we also show that connectivity measures require less release particles than dispersal estimates however the number is much higher than what has also been used in previous studies of seagrass dispersal and connectivity e g grech et al 2018 in future modelling studies efforts should be made to align particle release numbers with the numbers required to robustly answer the research questions our study presents a systematic framework for testing model sensitivity that should be implemented in other studies of marine plants prior to biophysical modelling applications author contributions js sc and ag conceived the ideas js jl kc ag rc st ac mr and sc designed the methodology ac rc st and mr collected the seagrass data js jl kc and ag conducted the analysis js and ag led the writing of the manuscript all authors contributed critically to the drafts and gave final approval for publication declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests alana grech reports financial support was provided by australian research council centre of excellence for coral reef studies alana grech reports financial support was provided by ian potter foundation samantha tol reports financial support was provided by seaworld research and rescue foundation acknowledgements funding for this research was provided by the australian research council centre of excellence for coral reef studies james cook university the ian potter foundation and the seaworld research and rescue foundation seagrass data collection was funded by the torres strait regional authority tsra ports north crc torres strait and national environmental science programme nesp tropical water quality hub we thank the torres strait islanders for access to their sea country and thank the many tsra land and sea management unit rangers fisheries queensland and tropwater staff for their valuable assistance in the field appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105313 
25641,biophysical models simulate dispersal and connectivity in marine environments by combining numerical models that represent water circulation with biological parameters that define the attributes of species the effects of parameters such as the number of particles released to simulate the trajectories of individual organisms is potentially large but rarely tested we present a framework to measure the optimal number of particles required to capture variability in dispersal and connectivity of the marine plants seagrasses we found that the number of optimal release particles per element or grid cell for dispersal estimates varied with seagrass habitat type season and physical parameters of the modelled propagules i e wind drag connectivity metrics were comparatively much less sensitive requiring lower particle numbers to achieve stable results we provide guidance on important factors to consider when determining the optimal number of particles required to robustly predict dispersal and connectivity in biophysical models of marine plants keywords biophysical model connectivity dispersal lagrangian particles individual based models seagrass data availability the data seagrass habitats and nodes shapefile slim hydrodynamic model processing and run files and mesh python sensitivity analysis csv and xls and their run files python and connectivity matrices csv and their run files python that support the findings of this study are openly available via research data jcu at https doi org 10 25903 3pe4 9n65 additionally the raw model outputs are stored on the qris cloud qcif collection q3871 https doi org 10 25903 1vje 6c38 1 introduction dispersal and connectivity are fundamental processes that shape the distribution structure and resilience of coastal and marine ecosystems cowen and sponaugle 2009 mcmahon et al 2014 the accumulated forces of ocean waves and currents are efficient vectors for long distance dispersal and influence metapopulation dynamics and the replenishment and recovery of populations after disturbance events nathan et al 2008 treml et al 2008 information on dispersal and connectivity is essential for the management of coastal and marine ecosystems as it underpins broad scale conservation planning and the design of marine reserve networks treml and halpin 2012 gaines et al 2010 furthermore dispersal and connectivity patterns set the spatial and temporal scales for effective interventions to counter the spread of nuisance species hock et al 2016 and for assessing environmental impact swearer et al 2019 empirical measures of connectivity are limited to a handful of species because of the cost and logistical constraints of assessing marine dispersal abesamis et al 2016 kool et al 2013 instead biophysical models are often used to simulate dispersal by combining numerical models that represent water circulation with biological parameters that define the attributes of a species swearer et al 2019 outputs of biophysical models can be exported to connectivity matrices to enable connectivity analysis using techniques such as network or graph theory e g thomas et al 2014 samsing et al 2017 grech et al 2018 simulating the trajectories and dispersal of an ensemble of individual organisms in biophysical models is facilitated by individual based models ibms that integrate multiple biological parameters such as larval and propagule duration and mortality pre competency period spawning or reproductive window and behaviour treml et al 2015a ibms include a random diffusion vector that is used to represent horizontal turbulent mixing at sub grid scales paris et al 2002 the random diffusion generates variability in dispersal trajectories meaning that multiple particles released from the same location at the same time and with the same biological parameters do not necessarily follow the same trajectory the amount of variability in dispersal trajectories can be adjusted by the modeler but generally increases as the number of released particles and duration of simulations increases the number of particles released in biophysical models therefore needs to be tested relative to the duration of simulations so that the particle number parameter is sufficient to capture variability in dispersal trajectories and to produce robust dispersal predictions north et al 2009 simons et al 2013 in practice the number of particles released per element or grid cell is more often viewed as a trade off between computational time and precision the effect of particle number on dispersal estimates is rarely assessed in a sensitivity analysis prior to the implementation of biophysical models swearer et al 2019 the goal of the present study is to resolve a methodological question critical to the implementation of robust biophysical models what is the optimal number of particles required to capture variability in dispersal and connectivity we focus on seagrasses because 1 previous studies on the sensitivity of dispersal predictions to particle tracking parameters use marine larvae rather than marine plants as a model taxon e g simons et al 2013 jones et al 2016 and 2 the complex life history and morphology of seagrass species necessitates a more refined approach to measuring the effect of particle number on dispersal seagrasses are capable of dispersing long distances grech et al 2016 kendrick et al 2017 mcmahon et al 2014 via the waterborne transport of viable propagules i e vegetative fragments fruits and plant fragments with attached fruits and seeds berkovic et al 2014 the maximum dispersal distances recorded in the literature are generally less than 100 km except during extreme weather events when dispersal has been recorded over distances of up to 400 km lacap et al 2002 buoyancy and survival times for seagrass propagules may be as long as 85 days in temperate species thomson et al 2014 but varies for subtropical species weatherall et al 2016 the reproductive period of seagrass species also occurs over many months and the complex morphology of fragments and fruits results in highly variable wind drag and decay rates lai et al 2018 2020 previous seagrass biophysical models use a wide range of particle release numbers to represent seagrass dispersal from 15 particles per location per day mari et al 2020 to almost 2000 jahnke et al 2017 but no studies have tested the sensitivity of dispersal estimates to particle number instead the decision behind the number of particles deployed in seagrass biophysical models is to optimize the total number of particles to ensure a reasonable computational load within the constraints of the modelling domain grech et al 2016 melia et al 2016 more often no justification is provided e g jahnke et al 2017 2020 and occasionally the total number of particles goes unstated e g sinclair et al 2018 cucco et al 2020 we advance the application of biophysical models by developing a procedure to determine optimal particle release numbers for species with long dispersal times and reproductive periods and with variable wind drags and decay rates we use a numerical approach to model the hydrodynamics and simulate the dispersal of virtual seagrass propagules in our study region torres strait australia fig 1 torres strait is an ideal case study because its complex hydrodynamics and diversity of species propagules and habitat types support the testing of model parameters across a range of biophysical settings the biophysical model s outputs were analysed as particle density distributions pdds to 1 assess the number of virtual particles required to robustly simulate the dispersal of seagrass propagules and 2 measure the effect of variable wind drag decay rate habitat type and season on particle number determining the optimal number of particles to release in biophysical models also requires consideration of the question that the model s outputs are intended to inform for example biophysical model outputs used to inform understanding of broad scale patterns of species connectivity e g mari et al 2020 require less accuracy than complex assessments of connectivity for precision pest management e g hock et al 2016 samsing et al 2019 despite these differences the effect of particle tracking parameters on connectivity estimates has not previously been assessed in this study we use the results from nearly 1000 simulations of a seagrass biophysical model to measure the sensitivity of changes in particle number on commonly used connectivity measures e g degree closeness centrality and local retention in network analysis we conclude by providing guidance on important factors to consider when determining the optimal number of particles required to robustly predict dispersal and connectivity in biophysical models 2 methods 2 1 study region and species torres strait between mainland australia and papua new guinea covers an area of approximately 48 000 km2 it is comprised of shallow seas and over 150 islands 18 of them inhabited fig 1 torres strait islanders maintain a strong affiliation with their land and sea and native title determinations have been made for the sea region and most of the islands the marine and coastal region of torres strait encompasses coral reefs seagrass meadows mangroves and sandy rocky and muddy beaches pitcher et al 2007 the extensive seagrass habitats of torres strait support commercial and indigenous fisheries and globally significant populations of dugongs and green turtles marsh et al 2015 torres strait has over 17 000 km2 of known seagrass meadows poiner and peterkin 1996 carter and rasheed 2016 with more areas being added as surveys are completed twelve species are recorded from 3 families encompassing nearly 20 of the world s known species meadows are typically abundant in near shore waters on reef platforms and in the south west region of the strait but can be found in most locations where suitable bottom sediment are present spatial geographic information system gis layers of seagrass distribution in torres strait were sourced from carter et al 2014 and carter and rasheed 2016 these layers were supplemented with information on the potential distribution of seagrass in the unmapped area between badu and boigu islands fig 1a derived from a model of dugong distribution and relative abundance marsh et al 2015 dugongs are specialist feeders on seagrass and their distribution is correlated with seagrass presence in queensland waters grech et al 2011 we extracted areas of very high dugong density between badu and boigu islands from marsh et al 2015 and combined it with the carter et al 2014 and carter and rasheed 2016 layers to generate the final spatial layer of torres strait seagrass distribution fig 1a torres strait has a complex hydrodynamic environment saint cast 2008 wolanski et al 2013 the eastern side of the strait is connected to the great barrier reef gbr and the coral sea and the western side is connected to the gulf of carpentaria and the arafura sea flows through the strait are barotropic generated by the wind driven difference in the mean sea level between the coral sea and the gulf of carpentaria the net flow through torres strait runs east to west and is relatively weak 0 04 0 13 sv saint cast 2008 wolanski et al 2013 at daily time scales the distinct tidal regimes from the coral sea and the arafura sea propagate into torres strait generating highly complex tides within the strait saint cast 2008 that are then further distorted by shallow water effects leblond 1991 wolanski et al 1988 the tidal currents generate fast flows peaks from 1 5 to 3 0 m s 1 saint cast 2008 within the strait as they interact with the complex bathymetry shoals reefs and islands south easterly trade winds which typically occur from march until september also generate a wind driven current that flows north into torres strait from queensland s east coast 2 2 biophysical model 2 2 1 hydrodynamic model torres strait currents were simulated using the depth averaged i e two dimensional version of the second generation louvain la neuve ice ocean model slim lambrechts et al 2008 a full description of slim a justification for the use of a depth averaged model in torres strait and a description of the model setup and validation are provided in the supporting information in brief slim solves the shallow water equations discretised in space by a second order discontinuous galerkin finite element method on an unstructured triangular mesh and in time with a second order implicit runge kutta method lambrechts et al 2008 our model domain matched the domain used by wolanksi et al 2013 however we generated a custom grid and forced our model differently the eastern side was open to the gbr and the coral sea and the western side was open to the gulf of carpentaria and the arafura sea fig 1 the longitudinal coverage of the domain was sufficient to capture the sea level difference between the eastern and western sides of torres strait that generates the barotropic flow through the strait the variable resolution mesh of slim was made finer with proximity to land and coral reefs to effectively capture the horizontal shear generated as currents flow in the complex bathymetry topography fig 1c the mesh was also made finer with proximity to seagrass meadows to increase the resolution of our analyses specifically the resolution ranged from 8 km in open water to 650 m near land reefs and the boundaries of seagrass meadows the model was forced by wind over the entire domain and by a combination of currents tides and sea surface elevation gradients at the open boundaries the hydrodynamic simulation ran from september 2011 the start of the peak reproductive period of seagrasses to february 2012 the end of the window of viability the window of viability was calculated as two months after the peak reproductive period december 2011 given the maximum probable survival time of seagrass propagules in the tropics is 8 weeks 56 days harwell and orth 2002 lacap et al 2002 hall et al 2006 källström et al 2008 kendrik et al 2012 thomson et al 2014 the 2011 2012 season was chosen because it had average physical conditions specifically the winds in this season matched the long term trends measured at the australian bureau of meteorology s coconut island poruma island id 027054 weather station in central torres strait figs s1 and s2 the wind conditions changed markedly over the simulation period fig s1 the relatively weak 5 m s 1 south easterly winds persisted from september to november 2011 the wind from december 2011 onwards was more variable in direction and speed there was a prevailing northerly wind of variable strength in december 2011 and january 2012 strong north westerlies 5 m s 1 and weaker south easterlies 4 m s 1 were common in february 2012 the hydrodynamic fields were simulated every 5 min and saved every 30 min the slim outputs were validated against field data and the outputs of an alternate model however our options for validation were limited by the remoteness of torres strait and the scarcity of hydrodynamic data available for the region specifically the modelled sea surface elevation was validated against a tide gauge at thursday island in southern torres strait and against a tide gauge at booby island located in the west of the strait figs s3 and s4 the slim velocity field was compared to the depth averaged month averaged current fields from the eight year 1997 2004 simulation of geoscience australia s torres strait circulation model fig s5 saint cast and condie 2006 overall the slim output was in good agreement with the available validation data the normalised root mean square error nrmse of the comparisons between the tide simulated by slim and the measured tides were 0 02 0 06 and the major monthly flow features delineated in the interannual simulations of geoscience australia s model were also simulated by slim given the match of the slim outputs with the validation data we have relative confidence in the simulate hydrodynamic fields despite the limited opportunity for validation 2 2 2 propagule dispersal the dispersal of passive i e particles unable to direct their own motion virtual seagrass propagules was simulated by coupling a lagrangian particle tracker model with the hydrodynamic model the hydrodynamic fields were re interpolated to match the particle tracker model time step of 5 min the particle tracker model was developed following thomas et al 2014 and grech et al 2016 in the particle tracker a random walk formulation of the 2d advection diffusion equation was parameterised with an okubo scheme okubo 1971 as outlined in spagnol et al 2001 and hunter et al 1993 the equations were 1 x n 1 x n v n δ t r n r 2 k h δ t 2 v n u c w u w k h h h k h x n 3 k h α l 1 15 x n v n δ t where the position of a seagrass propagule at time n 1 x n 1 was determined by its position at time n x n the advection velocity v n a random component r n is a horizontal vector of zero mean random numbers of variance r the horizontal diffusivity from sub mesh scale turbulent mixing k h and the time interval between iterations δt v n was determined by the depth average horizontal water velocity u the wind velocity at 10 m above the water surface u w the wind drag coefficient c w k h and the water column depth h k h was modelled as a function of the local mesh size l and the value of the coefficient α was set to 2 10 4 m0 85 s 1 okubo 1971 the propagules were modelled as either floating at the surface or suspended in the water column table 1 to partially account for the variety of structurally diverse forms of seagrass propagules i e vegetative fragments fruits and plant fragments with attached fruits and seeds the dispersal of the floating propagules was programmed to be influenced by the wind where the magnitude of the influence was set by the value of c w i e wind drag coefficient physically c w is dependent on a propagule s mass shape and buoyancy and by the angle it faces in relation to the wind direction two c w s were modelled for the virtual seagrass propagules in this study again to account for the structural diversity of propagules the c w was set to either 1 or 2 based on the results of 1 lai et al 2020 who performed flume tank experiments on vegetative fragments from tropical seagrasses and calculated that the average c w approached 1 and 2 grech et al 2016 who performed a biophysical modelling sensitivity analysis and determined that a c w of 2 was conservative for the propagules of tropical seagrass species to simulate the dispersal of propagules below the water surface c w was set to 0 so v n was solely determined by the water currents on each simulation start date staggered hourly releases were performed over a 24 h period so the propagules were released in all states of the tide grech et al 2016 virtual propagules were released during the peak reproductive period of seagrass in torres strait september 2011 to december 2011 and all simulations were run for 8 weeks 56 days the maximum probable survival time of seagrass propagules in the tropics table 1 post release the propagules could settle via two mechanisms beaching and sinking beaching was modelled following previous studies of passive particles e g hock et al 2016 2017 whereby propagules that came within 100 m of land were assumed to experience stokes drift net drift shoreward in the direction of wave propagation stokes 1847 and beach for settlement via sinking the floating trajectories of propagules were modelled to decay through time according to a first order decay function erftemeijer et al 2008 grech et al 2016 4 c t c 0 e k t where c t the propagule concentration in the domain at time t was modelled as a function of the initial particle concentration c 0 and the decay rate k given the uncertainty in k three different rates were simulated k was set to 0 075 day 1 following erftemeijer et al 2008 half 0 075 ½ k 0 0375 day 1 or double 0 075 2k 0 15 day 1 representing the sinking dynamics of seagrass propagules with a depth independent decay rate units day 1 is a physical simplification of sinking dynamics in our model representation when propagules become negatively buoyant and their floating trajectories start to decay they instantaneously sink to the bottom in reality given the energetic environment of torres strait the propagules would probably be dispersed through the water column by turbulent mixing until they were caught in the bottom boundary layer where they could sink to the bottom in biophysical models this is captured with the inclusion of a sinking rate units m day 1 e g condie and bormans 1997 despite the physical simplification the choice to use a decay rate in this study instead of a sinking rate was justified for two reasons firstly biophysical modelling studies of seagrass dispersal regularly use decay rate s erftemeijer et al 2008 making them highly applicable in this sensitivity analysis which considers common model parameters secondly seagrass propagules can remain positively or neutrally buoyant for up to 85 days before they start to sink thomson et al 2014 and torres strait is a relatively shallow system 20 m in the central strait fig 1b in combination these factors suggest that the time propagules take to sink following the onset of negative buoyancy is likely short compared to the prolonged period of positive or neutral buoyancy 2 3 sensitivity analysis the biophysical model was applied in a two phase sensitivity analysis to determine the optimal number of virtual particles required to 1 capture variability in dispersal trajectories of seagrass propagules and 2 measure the connectivity of seagrass meadows in torres strait 2 3 1 dispersal we followed a modified process of simons et al 2013 to determine the optimum number of virtual particles required to sufficiently capture the dispersal of seagrass propagules ten release locations were selected to reflect the range of hydrodynamic conditions of torres strait and four seagrass habitat types offshore and deep od n 3 inshore and shallow is n 3 offshore and shallow os n 2 and reef top rt n 2 fig 1a particles were released from the centroids of the elements or grids that intersected the ten locations fig 1a and c the areas of the elements ranged from 0 37 to 16 18 km2 due to the finite element method used in the hydrodynamic model the sensitivity of propagule dispersal to changes in wind drag c w decay rate k month of release and release location were examined in 36 simulations virtual particles with all nine combinations of k ½k k 2k and c w 0 1 2 were released on a waning gibbous moon near the beginning of each of the four months in the 2011 peak reproductive season 8 september 8 october 7 november and 7 december table s1 in each of the 36 simulations 100 008 particles were released per location making a total of just over 1 million particles per simulation following simons et al 2013 the distributions of settled particles from each model simulation release location combination were quantified as particle density distributions pdd and the particle numbers required to robustly simulate the distributions were determined through comparing reference and subsample pdds in python3 version 3 6 5 each reference pdd was calculated from the distribution of all the virtual particles that settled within the model domain during the simulation period while each subsample pdd was calculated from a random subsample of the settled particles the reference pdd was compared to 500 subsample pdds per evaluated subsample size subsample size range 3 to 5000 virtual particles for each comparison the reference pdd and a subsample pdd were sorted into vectors with the same element order and correlated via pearson s correlation the fraction of unexplained variation fuv between the two pdds was then calculated from the correlation coefficient r as follows 5 f u v 1 r 2 then for each of the evaluated subsample sizes an fuv distribution was generated from the 500 fuvs and the number which 95 of the distribution lay below was taken as the fuv upper bound the fuv upper bounds were plotted against subsample size finally the number of particles required to generate what we define as a robust pdd was determined as the number at which the fuv upper bound intersected a 0 05 cut off comparable to a 95 confidence level fig s8 the procedure simons et al 2013 used to calculate pdds was developed for a hydrodynamic model with a mesh in a cartesian grid format we therefore adapted their procedure to account for the variable resolution mesh used in this study in addition to the steps outlined by simons et al 2013 our element wise counts were standardized by area given the different areas of the mesh elements further simons et al 2013 reduced the amount of noise and detail in their pdds by smoothing them with an isotropic gaussian filter this was done for practicality as it reduced the number of particles required to achieve a robust pdd by an order of magnitude substantially reducing the computing costs we used weighted averages rather than an isotropic gaussian filter to smooth our pdds as it worked better with our variable resolution mesh the averages were weighted by the distances between an element s centroid and the centroids of its surrounding elements the weighted averaging was implemented in two stages in the first stage weighted averages were performed on all elements that contained particles and the elements directly adjacent to them without particles in the second stage the set of elements averaged included all elements from the first stage and also the empty elements adjacent to them the smoothing therefore extended the pdd 2 elements beyond the initial distribution of settled particles fig s9 importantly the smoothing of our pdds with weighted averages reduced the number of particles required to achieve a robust pdd by an order of magnitude similar to the effect of the spatial filter applied by simons et al 2013 the influence of the release habitat inshore shallow is offshore deep od offshore shallow os reef top rt on the relationship between the number of particles np required to robustly represent a pdd nppdd and the area covered by the pdd area pdd was tested a power regression was performed as the data were heteroscedastic and so both the np pdd and area pdd data had to be log10 transformed the power regression involved first modelling the transformed data using a linear regression table s3 note the transformed data satisfied the regression assumptions of normality and homoscedasticity fig s10 habitat specific equations describing how log 10 np pdd was a linear function of log 10 area pdd were fitted with the parameter estimates from the linear regression table s4 both sides of the equations were then raised to the power of e to get the habitat specific equations describing how nppdd was a function of the area pdd raised to a power table s4 the initial linear regression model had the form 6 log 10 n p p d d β 0 β 1 log 10 a r e a p d d β 2 i i s β 3 i o d β 4 i o s β 5 log 10 a r e a p d d i i s β 6 log 10 a r e a p d d i o d β 7 log 10 a r e a p d d i o s where rt was the reference habitat and i is i od and i os were dummy variables indicating the habitats os od and os respectively an analysis of co variance ancova was run on the linear model to test if there was an interacting effect of log 10 area pdd and release habitat on log 10 np pdd table s5 the statsmodels version 0 12 2 seabold and perktold 2010 package in python3 was used to conduct the linear regression and the ancova the effect of propagule properties c w and k and different environmental conditions release month and release habitat type on the calculated virtual particle requirements for robust pdds np pdd was tested with a four way anova in r v 3 4 3 r core team 2020 table s6 all of the factors in the model were set as fixed the particle number data were log10 transformed to satisfy the assumptions of normality and homoscedasticity fig s11 the astronomical forcing of tides in torres strait is distorted by shallow water effects leblond 1991 wolanski et al 1988 and consequently tidal ranges from 1 7 m to 2 9 m at thursday island fig s3 were measured across the simulation start dates even though the dates fell on the same lunar phase however the projected particle number requirements were found to be insignificantly effected by variations in the tidal range present at the start of model simulations figs s6 and s7 table s2 the results of the present sensitivity analysis were therefore unlikely to be influenced by the differences in tidal ranges on the simulation start dates 2 3 2 connectivity after determining how many virtual particles were needed to robustly simulate the dispersal of seagrass propagules we tested if estimates of seagrass connectivity had similar requirements we ran a series of simulations where virtual particles were released from the elements that intersected the mapped seagrass habitats fig 1 with release numbers per element differing across 9 treatments from 25 to 2500 virtual particles treatments 25 100 250 500 750 1 000 1 250 1 500 and 2500 the minimum 25 and maximum 2500 values were chosen because 1 the results of the dispersal sensitivity analysis indicated that 1500 particles are needed to confidently simulate the dispersal of structurally diverse seagrass propagules under most conditions and over 2500 particles are needed to simulate dispersal under all conditions see results and 2 in previous studies 25 virtual particles have been released per site e g ruiz montoya et al 2015 the total number of particles released in the simulations ranged from 196 725 25 particles per element to 19 672 500 2500 particles per element the simulation sets for each of the nine treatments consisted of 108 simulations total 972 simulations specifically for each treatment propagules with each of the nine combinations of k ½k k 2k and c w 0 1 2 were released on one quarter one between and one new or full moon in each of the four months in the peak reproductive period in 2011 5 8 and 12 september 4 8 and 12 october 3 7 and 11 november 3 7 and 11 december table s1 virtual particles were released from the centroids of the 7869 elements that intersected the mapped seagrass habitats fig 1 the elements that intersected mapped seagrass habitats had a combined area of 13 070 km2 range 0 16 19 0 km2 following grech et al 2018 outputs of the individual simulation sets were pooled resulting in one dataset per release number treatment seagrass habitat patches were generated by dividing the spatial layer into 195 units range 0 46 650 22 km2 mean 67 0 km2 fig 1a large meadows were subdivided while smaller meadows of reef top and inshore seagrass in close proximity to each other were grouped we calculated the cumulative settlement of virtual particles on seagrass habitat patches in python3 and imported the results into weighted and directed connectivity matrices differences in spatial patterns of connectivity across the nine connectivity matrices were analysed using network analysis the networks referred to as habitat graphs were a set of nodes that represent discrete seagrass habitat patches n 195 and edges or links between nodes indicate functional connections edges were weighted by the number of virtual particles from every habitat patch i to every other habitat patch j we did not use a connectivity strength threshold to remove weak connections andrello et al 2017 as this would have disproportionally influenced the habitat graphs of treatments with smaller particle numbers we used gephi 0 9 2 to calculate a range of network and node level metrics to define the nine habitat graphs derived from the connectivity matrices of the nine particle number treatments we used a combination of weighted and unweighted i e independent of particle number network and node level metrics to enable comparisons between the different treatments modularity and the louvain algorithm blondel et al 2008 were used to detect communities or clusters of nodes that were densely connected to each other and weakly connected to other nodes in the network thomas et al 2014 grech et al 2018 we used linear mixed effect models package lme4 bates et al 2015 in r v 3 6 3 with node as a random factor to test for the effects of the nine treatments on the following node level metrics 1 degree number of unweighted edges incident to a node 2 closeness centrality average shortest path length between a node and all other nodes 3 local retention proportion of particles that remained within the spatial extent of the node and 4 pagerank weighted measure that is used to identify important source meadows taking into account the full topology of the habitat graph allesina and pascual 2009 smaller elements had a higher number of released particles per unit area than larger elements however this should not impact on the interpretation of our results as most of the connectivity metrics that we used in the analysis were unweighted 3 results 3 1 dispersal the seagrass habitat type timing of simulation and wind drag coefficients c w had interacting effects on the number of virtual particles required to robustly simulate the dispersal of seagrass propagules broadly and among all release habitats the number of particles required increased in proportion to the area settled by the particles raised to a power fig 2 table s4 however the rate of increase differed significantly by habitat ancova interaction term f 3 352 22 78 p 0 0001 table s5 in particular the particle requirements increased substantially faster with relative increases in area for the reef top habitat compared to the other habitats fig 2 when tested explicitly the release habitat type had a significant effect on the number of virtual particles required to robustly simulate dispersal four way anova f 3 216 468 85 p 0 0001 fig 3 table s6 for example a vast majority of the propagules released from the inshore shallow seagrass habitats were quickly advected toward the coast and beached table 2 relatively small numbers of virtual particles were required to capture the dispersal of propagules released from these habitats 191 in all simulations fig 3 in contrast hundreds more propagules were required to capture the dispersal from the offshore deep locations from which large numbers of propagules were advected westward and lost from the model domain mean range 49 71 the highest release numbers were required to simulate dispersal from locations where propagules were commonly advected into the open waters of torres strait and away from the coast i e offshore shallow locations 8 and 9 and the reef top location 10 fig 1a moderate numbers of these propagules were retained within the model domain less than 42 lost on average they settled by sinking rather than beaching and therefore settled over large areas while the maximum number of particles required to robustly simulate dispersal was less than 1500 for nine of the ten particle release locations it jumped to 2761 for one of the reef top locations the release month and release habitat type had a large and significant interactive effect on the particle number requirements four way anova f 9 216 7 49 p 0 0001 table s6 fig 3 the wind and circulation patterns dispersed propagules westward at the beginning of the reproductive season september october in 2011 figs s12 s15 later in the season november december the wind changed from being predominantly south easterly to predominantly northerly and the coral sea coastal current developed and impinged into the eastern sector of the model fig s5 these changes in wind and circulation particularly affected the settlement distributions and resulting particle number requirements for particles released from locations in eastern torres strait e g reef top id 10 fig 1a the effect of wind drag on particle number was statistically significant when considered independently four way anova f 2 216 8 08 p 0 001 and also interacted with the release habitat four way anova f 6 216 8 03 p 0 0001 and month four way anova f 6 216 3 81 p 0 01 to have significant effects on the particle number requirements table s6 fig 3 trends in the particle number requirements were projected with increases in the wind drag depending on how the wind effected the fates of particles i e beaching sinking or leaving the model domain the decay rate had a variable but non significant effect on the number of particles required to robustly simulate dispersal 3 2 connectivity the nine habitat graphs shared similar network measures table 3 however there was a difference in the graph structure for simulations with 100 release particles fig s16 the number of communities ranged from 6 to 8 and all graphs featured only one component and had short diameters 6 7 relative to network order 195 network density ranged from 0 21 to 0 37 and exhibited the same diminishing growth phase as network size between 500 and 1500 particles the node degree distributions fig s17 of the nine habitat graphs all showed properties of a random graph with degree distribution similar to a gaussian bell shaped curve fig 4 shows the estimated marginal means and 95 credible intervals for the linear mixed effect models used to measure the effect of particle number on the node measures degree closeness centrality local retention and pagerank a pairwise comparison with a tukey s test showed a significant effect of particle number on degree 1000 particles closeness centrality 750 particles but limited to no effect on the weighted measures pagerank 250 particles and local retention 4 discussion biophysical models of species dispersal in marine systems are often implemented without testing for the sensitivity of modelled outputs to input parameters with potentially large consequences on model inference and rigor of subsequent analyses in this study we present a framework to identify the optimal number of particles required to robustly simulate dispersal in a biophysical model of marine plants seagrasses with complex life histories advection and sinking settlement modes we also assessed the sensitivity of connectivity metrics to particle number the number of optimal release particles per element or grid for dispersal estimates varied with seagrass habitat type release month and wind drag of the modelled propagules connectivity metrics were comparatively much less sensitive requiring lower particle numbers to achieve stable results this dichotomy highlights the need to tailor sensitivity analyses in biophysical models and the resulting particle number requirements to the research question other marine plants such as mangroves and macroalgae share similar complex life histories and morphology as seagrasses and the approach presented here could be applied to biophysical models in these ecosystems 4 1 sensitivity of dispersal to particle number spatial release habitat type temporal release month and morphological wind drag coefficient c w factors effected the number of particles required to robustly capture the potential spread of seagrass propagules we found the particle number requirements depended on the spread and complexity of the settlement distributions this aligned with the results of simons et al 2013 who found that the number of particles required to robustly capture the distribution of passive animal larvae increased with area and underlying complexity of the larval distribution in the present study with seagrass propagules as the model organism the spread and complexity of the distributions were determined by the relative proportions of particles settling via beaching versus sinking and by the loss of particles from the domain the release context i e release habitat and timing of release and the propensity of particles to catch the wind i e the wind drag were directly linked to these settlement and loss dynamics these results suggest that effectively capturing the nuances of dispersal trajectories for marine plants will require releasing enough particles to robustly represent dispersal across the full range of habitats and wind drag coefficients and over the entire reproductive window in contrast to the other parameters changing the decay rate in the biophysical model did not significantly influence the particle number requirements in some instances the expected effect was observed where increasing the sinking i e the rate of decay through time of the floating trajectories made more particles settle faster and over a smaller area so less particles were needed for robust simulations this trend was commonly measured at the offshore shallow site however in other instances the particle requirements increased with increases in the decay rate or changing the decay rate had no measurable effect these counter intuitive results can be explained by interactions between decay rates and settlement dynamics for example where a high proportion of particles settled via beaching an increase in the decay rate would mean more particles sunk before they beached resulting in a more complex settlement distribution and requiring more particle for a robust representation given the biological importance of the sinking of viable marine plant fragments and their subsequent establishment sherman et al 2016 thomson et al 2014 and the potential for the decay rate to effect dispersal estimates the importance of this parameter should not be discounted in future biophysical models our results broadly indicate that less particles are required where dispersal is more strongly constrained by geography and or species biology for example the inshore shallow release locations were all adjacent to land barriers which facilitated the settlement of particles via the beaching mechanism due to this geographic biological constriction of dispersal fewer particles were required to capture the spread of particles from the inshore shallow locations in contrast dispersal from the reef top locations was much less constrained geographically and the particle number requirements for these locations were greater a divergent biological control was recorded for dispersal from reef tops where greater wind drags constrained dispersal when particles were blown toward land or toward the model boundary and facilitated dispersal when particles were blown into open water a reduction in the spread of particles with the geographic biological constraint of dispersal has been documented in other biophysical modelling studies e g critchell and lambrechts 2016 treml et al 2015a treml et al 2015b schlaefer et al 2018 but the associated reduction in particle number requirements has scarcely been documented however simons et al 2013 reported a reduction in particle requirements with a biological control on dispersal where less particles were needed to confidently simulate dispersal over shorter time periods i e shorter larval phases the dependence of particle requirements on the geographic biological restriction of dispersal could have substantial implications for example lesser requirements when modelling the dispersal of species inhabiting semi enclosed estuarine bays versus more open coastal habitats 4 2 sensitivity of connectivity to particle number we used a series of unweighted network measures to assess how changes in the number of release particles affected the detection of edges or links between nodes we found that increasing the number of release particles 500 per element had a diminishing return on the detection of new edges indicated by network size table 3 as well as other network measures the nine habitat graphs representing the outputs of simulation sets with 25 100 250 500 750 1 000 1 250 1 500 and 2500 particles released per element were densely connected and featured only one component with a short network diameter relative to network order the number of communities were also not affected by particle number our results indicate that simulations that use relatively low numbers of particles are able to effectively predict network measures when systems exhibit high levels of connectivity between patches we found that seagrasses in the torres strait form a highly connected system in alignment with the biophysical modelling of wolanski 2017 who projected that floating seagrass propagules in the strait could disperse tens to over one hundred kilometres high connectivity of seagrasses in torres strait is facilitated by long survival times of fragments and seeds variable wind drag and long reproductive periods intersecting with fluctuating hydrodynamic environments similar properties are likely to be exhibited in other marine plant ecosystems along open coastlines at a node level we found that degree and closeness centrality were significantly affected by particle number indicating that greater precision is required to capture variability in connectivity measures at the node level however local retention was not significantly affected by particle number because it is a relative measure of the proportion of particles that remain within a source patch in cases of systems with lower connectivity e g large coral reef mosaics with short reproductive periods hock et al 2017 further sensitivity analyses are needed to assess the effect of particle number on connectivity measures at network and node levels 4 3 contrasting dispersal and connectivity sensitivities we found that the number of released particles per element required to effectively capture dispersal of seagrass propagules in torres strait ranged over three orders of magnitude from 5 to 2761 consequently over 2700 particles would need to be released per element to confidently simulate dispersal of seagrasses in the strait in every month in the peak reproductive season and for the range of propagule wind drags in contrast there was limited benefit to increasing particles beyond 1000 particles per element for commonly used network and node level connectivity measures our finding of greater particle requirements for dispersal versus connectivity metrics may hold for other systems and highlights the need to tailor sensitivity analyses to specific research questions 4 4 considerations for biophysical modelling in other systems this study focused on the particle requirements for quantifying dispersal and connectivity in seagrasses similar studies based in other biophysical systems need to consider the effects of system specific factors that were not addressed in this study for example for seagrass in torres strait it was appropriate to use a depth averaged model because the strait waters are shallow and vertically well mixed wolanski et al 1988 lambrechts et al 2008 and the biological component included representations of the different positions that seagrass propagules could occupy within the water column i e at the surface and suspended in the water column three dimensional models are necessary in systems where the waters are deep and or vertically stratified and or the species being modelled perform complex vertical behaviours such as ontogenetic vertical migration e g allain et al 2001 paris et al 2007 greater numbers of particles would be required to capture the range of possible dispersal trajectories and connectivity networks in these more complex systems compared to the requirements presented here the horizontal swimming of marine animal larvae capable of influencing their horizontal dispersal trajectories in the later stages of their pelagic phases e g fish larvae fisher et al 2000 invertebrate larvae young 1995 would also add complexity requiring more particles to capture variability in dispersal trajectories and connectivity furthermore marine plant propagules such as mangroves di nitto et al 2013 van der stocken et al 2019 and the larvae of most marine animals must spend some time in the pelagic before they are competent to settle the reproductive or competency period and pelagic larval duration of marine plants and animals varies from minutes to months di nitto et al 2013 shanks 2009 van der stocken et al 2019 a higher proportion of individuals in species with obligatory pelagic larval phases may travel farther relative to species that are immediately viable although larval duration is not related to dispersal distance in some circumstances shanks 2009 in the context of biophysical modelling more release particles per element could be needed to robustly simulate the dispersal and connectivity of species with pre competency periods and obligate pelagic larval phases finally for studies on connectivity an important consideration is that the spatial and temporal coverage needs to match the space and time required to adequately characterise the connectivity which is system specific e g condie et al 2018 we recommend that any study of marine dispersal or connectivity should analyse the sensitivities of particle number requirements taking into account all particle parameters such as vertical and horizontal swimming and pelagic larval duration 4 5 model limitations uncertainty in the parameterisation of the biological component of biophysical models including a lack of knowledge about release locations and the timing of release translates to uncertainty in the model outputs swearer et al 2019 in our analysis we incorporated this uncertainty by using a range of biological parameters i e wind drags and decay rates over an entire reproductive season and with the maximum number of release locations we also attempted to minimise uncertainty by basing our study in torres strait where knowledge of seagrass habitats is advanced carter et al 2014 carter and rasheed 2016 marsh et al 2015 however placing the study in the remote torres strait did add a small level of uncertainty in the simulated hydrodynamic fields given the limited data available for model validation over the simulation period although the simulated fields did strongly match the available validation data the small level of uncertainty in the simulated hydrodynamics carried over into some minor and unavoidable uncertainty in the modelled dispersal and connectivity another area of uncertainty that we could not account for was the loss of particles from the model open boundary particles that were lost from the boundary were assumed to be unable to re enter the area and this assumption is common in biophysical modelling studies over hourly time scales some particles may have been erroneously lost when advected over the boundary by tidal currents that would have transported them back into the domain with the change of the tide however these particles had to be close to the boundary within the tidal cycle that expatriated them and so most of them would have ultimately been expatriated by the net currents flowing over longer time scales for example a high proportion of the particles released from locations near the western boundary were lost the inability of these particles to travel against the net east to west current that runs through torres strait and re enter the strait is physically realistic 4 6 conclusions the difference in input parameter requirements to robustly capture dispersal versus connectivity in biophysical models highlights the importance of sensitivity analysis and matching model accuracy to the research question we demonstrate that variability in life history morphology and habitat type of marine plants must be accounted for when measuring the sensitivity of the number of release particles per element our torres strait case study provided an ideal opportunity to test the effect of interactions of complex hydrodynamics with a broad range of habitat types season and wind drag parameters we found the number of particles required to robustly predict dispersal in complex systems is higher than previous studies in similar systems e g grech et al 2016 other systems with for example unidirectional flow and lower species diversity may not require as many particles for robust simulations e g evans et al 2021 sinclair et al 2018 ruiz montoya et al 2015 we also show that connectivity measures require less release particles than dispersal estimates however the number is much higher than what has also been used in previous studies of seagrass dispersal and connectivity e g grech et al 2018 in future modelling studies efforts should be made to align particle release numbers with the numbers required to robustly answer the research questions our study presents a systematic framework for testing model sensitivity that should be implemented in other studies of marine plants prior to biophysical modelling applications author contributions js sc and ag conceived the ideas js jl kc ag rc st ac mr and sc designed the methodology ac rc st and mr collected the seagrass data js jl kc and ag conducted the analysis js and ag led the writing of the manuscript all authors contributed critically to the drafts and gave final approval for publication declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests alana grech reports financial support was provided by australian research council centre of excellence for coral reef studies alana grech reports financial support was provided by ian potter foundation samantha tol reports financial support was provided by seaworld research and rescue foundation acknowledgements funding for this research was provided by the australian research council centre of excellence for coral reef studies james cook university the ian potter foundation and the seaworld research and rescue foundation seagrass data collection was funded by the torres strait regional authority tsra ports north crc torres strait and national environmental science programme nesp tropical water quality hub we thank the torres strait islanders for access to their sea country and thank the many tsra land and sea management unit rangers fisheries queensland and tropwater staff for their valuable assistance in the field appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105313 
25642,the la miel river watershed is an area of high hydrological interest in colombia due to its abundant water resources high annual precipitation and hydroelectric power generation this work proposes a sequential modeling framework to quantify the spatially and temporally variable groundwater recharge and to analyze its impact on piezometric fluctuations in the study area where the groundwater flow is affected by geological faults water modeling framework wmf and hydrogeosphere hgs models have been used to calculate groundwater recharge and distribution of hydraulic heads respectively recharge computed with wmf is used as input to hgs to compare groundwater flow 1 with uniform and spatially variable recharge 2 with and without discrete fractures and 3 during dry and wet conditions the results generate valuable knowledge for water resource management and highlight the importance of groundwater recharge estimation and a proper representation of fractured aquifers keywords aquifer recharge wmf hydrogeosphere rainfall scenarios 1 introduction it is estimated that more than half of the world s population depends on groundwater as a source of drinking water oki and kanae 2006 however groundwater resources are often not managed to ensure its long term sustainability sophocleous et al 2009 and their depletion and pollution prevail in both developed and developing countries konikow and kendy 2005 rodell et al 2009 sustainable management of water resources environmental impacts assessment of water exploitation activities flood and drought risk and soil and vegetation management can be evaluated through the development of surface and subsurface numerical models rébori et al 2009 chaney et al 2015 yu et al 2019 velasquez et al 2020 the modeling of surface and groundwater resources has increased in the last decades lerner and kumar 1991 crawford and linsley 1966 simunek et al 1998 beven 2001 izbicki et al 2004 harou et al 2009 chandio et al 2012 anderson et al 2015 singh 2018 surface and subsurface flow models are helpful to understand the hydrological processes and their interactions at the watershed scale which is fundamental knowledge for the sustainable management of water resources publications in integrated surface subsurface water modeling can be found in jayakrishnan et al 2005 kalbus et al 2006 jhorar et al 2009 partington et al 2011 levy and xu 2012 lamontagne et al 2014 hassan et al 2014 barthel and banzhaf 2015 wei et al 2019 cardoso de salis et al 2019 yu et al 2019 wijayarathne and coulibaly 2020 other modeling techniques are related to machine learning approaches although they have already been applied jimeno sáez et al 2017 their use in hydrological science is still the object of detailed analysis nearing et al 2021 moreover lumped and distributed hydrological approaches can also be employed perez sanchez et al 2019 bournas and baltas 2021 in colombia surface water models have been implemented for different applications such as flood risk analysis water resources management the impact of glacier melt on river flow and analysis of satellite based rainfall data on hydrological modeling among others marulanda aguirre et al 2016 elgamal et al 2017 rodríguez et al 2019 velasquez et al 2020 in comparison the interest in subsurface water models has only recently started historically the focus has been on surface water resources which are abundant in the country and the professional expertise in hydrogeology and the amount and quality of data required to build groundwater models were limited however groundwater is an essential resource in several regions of the country such as in the valle del cauca la guajira and sucre departments as well as in the bogotá savanna and urabá regions lobo guerrero and gilboa 1987 mavdt 2010 ossa valencia and betancur vargas 2018 in these regions rural populations agricultural irrigation and drinking water supply depend mainly on groundwater resources there is an urgent need to address water resource management in colombia in a more integrated way such that the interactions between surface and subsurface water can be better understood as a result of the first evaluation and classification of colombian hydrogeological systems in 2013 it was estimated that at least 75 of the colombian territory has a great storage potential for groundwater resources with a total of 5848 km3 in estimated reserves ideam 2013 despite the efforts that have been made the management of these reserves is still limited however guidelines for formulating aquifer environmental management plans and a groundwater national program were defined mads 2014 in 2019 an updated version of the hydrogeological zones was published it indicates that only 30 8 of the aquifer systems are identified with sufficient knowledge for groundwater management and that groundwater recharge remains a topic that requires detailed investigations at different scales ideam 2019 the uppermost part of the la miel river watershed belongs to the areas without detailed hydrogeological studies it is chosen here as a case study to compensate for the lack of knowledge in the region and to analyze the impact of a proper conceptual model for the fractured aquifer located in the central and western portion of the study area where two regional faults and a lineament are identified in fact one can rarely model flow in a fractured rock considering it as a continuous uniform isotropic unit neuman 2005 and the numerical results depend on the uncertainty associated to the conceptualization of the movement of groundwater in fractured environments selroos 2002 although much research has been conducted it is challenging to accurately determine the permeability of fractured media generating modeling challenges that are not present in standard porous media zhang 2013 berre et al 2019 in colombia studies on the hydrogeology of fractured rocks are few piña et al 2019 such that this case study provides valuable knowledge for groundwater modeling a sequential modeling procedure is proposed here where groundwater recharge calculated by the water modeling framework wmf surface model is used as an inflow boundary condition to the groundwater model built with the hydrogeosphere hgs software aquanty 2013 sequential modeling to study surface and subsurface water interaction has been applied in some studies such as in pulido velazquez et al 2018 and okkonen and kløve 2011 however the physical aspects of those studies and their implementation are more straightforward than the work conducted and analyzed here our work has two main objectives first assess the impact of spatially variable recharge and faults associated with the fractured aquifer on the simulated steady state hydraulic heads second analyze the amplitude of hydraulic head variations and the groundwater level response to recharge the time lag for transient flow considering a spatiotemporally variable groundwater recharge these goals are relevant since fractured media are often simplified to equivalent porous media with few attempts to quantify and incorporate spatiotemporal recharge variability into groundwater modeling jyrkama et al 2002 batelaan and smedt 2007 moeck et al 2018 besides recharge relies on vegetation precipitation climate topography geology and soil type which are also variable making it a challenging parameter to quantify dripps and bradbury 2010 moreover recharge is one of the most poorly constrained hydrological parameters in almost all groundwater flow models and the least understood zomlot et al 2015 therefore recharge variability is often neglected in groundwater models and a single value is estimated as a percentage of the precipitation or as a model fitted parameter determined by calibration dripps and bradbury 2010 although the role of recharge conceptualization in groundwater modeling has been generally recognized in the literature few concrete examples include and quantify this parameter ehtiat et al 2016 manna et al 2019 this work presents the first study based on sequential modeling using wmf and hgs while wmf offers robust hydrologic simulation and analysis solutions hgs is widely known as a hydrogeological simulator both models are combined to develop a new straightforward modeling methodology to capture the spatiotemporal variations of aquifer recharge this study contributes to the understanding and expands the complex topic of surface subsurface water modeling 2 data and methods 2 1 description of the study area the study area is located in the upper part of the la miel river watershed at the eastern slope of the central andes mountain range in the municipalities of samaná pensilvania victoria marquetalia and manzanares in the department of caldas colombia fig 1 the la miel river begins in cuchilla la picota in the municipality of pensilvania which is located in the western part of the watershed at approximately 3600 m a s l it flows towards the northeast and ends by flowing into the left bank of the magdalena river at 146 m a s l the studied area has an approximate area of 475 km2 which represents 40 of the entire watershed elevation ranges from 482 to 3680 m a s l the average slope and watershed length are estimated to be 38 and 36 km respectively the main river has a slope and length of 5 and 48 km respectively with a strahler horton order of 7 the primary vegetation is cropland and pasture which covers 63 of the watershed it is followed by forest with 30 in total area mixed rangeland with 6 and the remaining 1 corresponds to open water and urban areas the upper la miel river watershed contains a protected area a portion of the florencia national natural park and other smaller preservation areas such as el popal la linda el guayabo and la gaviota the geological context which plays a key role in constructing the hydrogeological model is described in detail in a separate section according to the classification of hydrographic and hydrogeological units of colombia ideam 2013 the study area is located in the magdalena cauca hydrographic region and the mariquita dorada salgar aquifer system which belongs to the hydrogeological province of the magdalena medio valley the aquifer systems in this province are composed of alluvial sediments terraces and sandstone sequences paleogene neogene conglomerates and metamorphic rocks of the cajamarca complex with secondary permeability ideam 2013 the yearly precipitation regime in the study area is bimodal fig 2 with two rainy march may and september november and two dry seasons december february and june august the average annual precipitation ranges from 2500 mm to 7500 mm approximately with the lowest values observed in the west upstream region and the highest values observed in the east downstream part of the watershed due to the high water availability and the favorable topography water is stored in a reservoir for hydroelectric power generation the la miel moro and santa barbara rivers have been dammed at norcasia municipality patángoras dam to form the amaní reservoir which has an area of 12 2 km2 and a storage capacity of 571 mm3 the dam is a roller compacted concrete of 188 m in height located 47 km upstream of the magdalena river its installed capacity is 396 mw and it started operating in 2002 generating approximately 1460 gwh of electricity https isagen com co es nuestro negocio generamos energia central hidroelectrica miel i the outlet of the study area corresponds to the streamflow gauge located in la miel river upstream of the amaní reservoir near its tailwater 2 2 geological context the geology of the study area is composed mainly of metamorphic rocks influenced by a high grade of tectonism the oldest rocks are of the paleozoic era such as the cajamarca complex which constitutes the basement of the central range of the andes mountains which are divided into three ranges in the colombian territory the cajamarca complex covers 90 of the study area it is subdivided into two metamorphic lithological units the first one is located in the western part of the area in the pensilvania and marquetalia municipalities and it consists of quartzite and biotite feldspathic quartzite while the latter is located in the east and consists of quartz sericite schists tres which are usually found as graphitic schists with good foliation igneous and sedimentary rocks represent cretaceous lithologies one of the most representative is the samaná igneous complex composed of a series of intrusive rocks mainly diorites quartz diorites tonalites granodiorites and hornblende pyroxene gabbros the alaskite lithology of the samaná unit forms a crown on the eastern edge of the samaná igneous complex where quartz plagioclase and biotite are found in greater quantity corpocaldas et al 2016 the el hatillo stock tcdh is located to the southeast of the study area between the municipalities of samaná and marquetalia and is composed of quartz dioritic rocks from the tertiary period in some locations this unit is observed as a saprolitic deposit of igneous rocks with clay texture and red coloration where its mineralogical composition cannot be differentiated the guadalupe porphyry tadg unit is located between the municipalities of manzanares and pensilvania it is composed of porphyritic rocks where hornblendic andesite predominates finally the mesa formation tsm unit is located in the municipality of victoria it has a good stratification composed of volcanic material with intercalations interspersed of coarse grained sandstones with agglomerates corpocaldas et al 2016 two regional faults are observed in the study area the palestina and the el eden faults the palestina fault was defined as a dextral fault with a length of 350 km that crosses the central mountain range and presents a morphological expression characterized by remarkable depressions with elongated and alienated valleys gonzález 2001 the segment of the fault that lies within the study area affects especially the aluminum silicate schist unit the samaná igneous complex kds samaná alaskite unit kas and the superficial pyroclastic and glaciofluvial deposits qto the el eden strike slip fault is located to the southwest of the study area between the municipalities of marulanda and pensilvania in addition a lineament has been identified in the middle of the study area fig 1b according to photointerpretation changes in the river direction and the interpretation of structural and lithological changes as indicated in the la miel river watershed planning and management plan pomca corpocaldas et al 2016 2 3 data collected and fieldwork the data required for the surface and subsurface flow modeling are topography daily rainfall data daily discharge and piezometric data levels the digital elevation model dem was obtained from alos palsar with a 12 5 m resolution resampled to a 60 m resolution to reduce model computing time daily precipitation time series are obtained for the period 2003 2017 within the study area the analysis period is chosen based on the length and completeness of data records seven rain gauges fig 1 from the instituto de hidrología meteorología y estudios ambientales ideam are selected to represent precipitation over the study area daily discharge values for the period 2003 2015 are obtained from the isagen electricity generation company in colombia gauge puente samaná located in the outlet of the study area water levels measured in six piezometers at the end of the first rainy season march may were obtained from la miel watershed management plan pomca the spatial land cover and soil texture data required by the surface model are obtained from geographic institute agustin codazzi igac 2014 each soil profile is a group of horizons with organic matter and textural characteristics using the soil profiles and the soil plant air water program spaw the wmf hydraulic conductivity k s and the static storage h u are estimated for each horizon and then averaged over the vertical profile fieldwork activities have been conducted to collect rock samples and observe the thickness of weathered profiles to improve the geological characterization required to build the 3d subsurface model satellite imagery has been used to identify the points suitable for rock sampling and observation of soil horizons considering that they should be located close to roads and in areas with evidence of past slope movements bedrock outcrops or close to creeks and streams most of the rock samples collected correspond to pq quartzite and tres schist lithological units belonging to the cajamarca complex table 1 which is the most prominent geological formation in the study area fig 1b soils color textural class and weathering profile were analyzed to build vertical sequences that are spatially correlated to identify the lateral relationships required to define the stratigraphy of the study area vera torres 1994 a stratigraphic column is constructed for each lithological unit such as those shown in fig 3 for the metamorphic cajamarca complex the samaná igneous complex and the mesa sedimentary formation 2 4 surface and subsurface flow modeling according to the national map recently published ideam 2019 the la miel river watershed falls within the region of low groundwater recharge however high recharge areas may exist but they did not show up because the ideam map has been built at the country scale therefore the modeling work conducted here focuses on quantifying the spatial and temporal variation of recharge with the surface modeling tool wmf and its successive use to assess the groundwater flow dynamics with the subsurface modeling tool hgs both surface and subsurface flow models cover the study area of 475 km2 shown in fig 1 three scenarios are defined the first uses rainfall observations while the other two are synthetic based on dry and wet conditions using wmf and the scenarios the spatiotemporal variable recharge is calculated each recharge scenario is then used as a flow boundary condition in hgs to assess the impact of higher and lower recharge on the groundwater flow response in the study area the years 2003 2015 are used for the surface model calibration nevertheless only march may 2016 is simulated with the subsurface model since groundwater level records are only available for those months 2 4 1 watershed modeling framework wmf wmf is a fortran90 python numerical package developed to produce geomorphological analysis and distributed hydrological simulations for the hydrological simulations the package uses wmf t which is a modification of the tetis model vélez 2001 francés 2007 the modifications include saturated runoff production and non linear lateral flow equations conceptually the model divides the watershed into cells that contain 4 to 5 storages or tanks the tanks are aligned vertically at the cell location with tank 1 being the uppermost one fig 4 tanks 1 to 4 represent the capillary runoff sub surface and subterranean storages the 5th tank represents channel storage and its existence depends on a pre defined upstream area threshold of 0 1 km2 specified by the observed channel at each cell the model solves vertical flow from tanks 1 to 4 then it solves horizontal flow that goes to the tanks of the downstream cell or if it exists to the channel tank a detailed description of the model equations and parameters is provided by velásquez 2020 wmf t model setup requires a dem distributed maps of land use soil texture and geology as described in the previous section those maps were used to estimate the saturated hydraulic conductivity ks mm s 1 and percolation kp mm s 1 as shown in fig 5 a and b respectively three rainfall scenarios base dry and wet were defined and considered as inputs the base scenario corresponds to the observed daily fields interpolated through the idw inverse distance weighted method using the rainfall gauges of the study region fig 1 this method has been used by several authors hohmann et al 2021 kurtzman et al 2009 besides velasquez et al 2011 found that it provides a robust interpolation for the region wmf model was manually calibrated for the period 2003 2015 with observed streamflow records at puente samaná gauge then the dry and wet scenarios were defined for the first months of 2016 since they include the period calibrated by the subsurface model these scenarios will be explained in section 2 5 table 2 presents the calibrated scalar values ri the aquifer recharge estimation corresponds to the vertical flux d 3 4 equation 1 between tanks 3 and 4 representing the sub surface and subterranean storages respectively after calibrating the model the simulated vertical flux for each cell between march and june of 2016 was recorded 1 d 3 4 min d 2 3 k p δ t 2 5 rainfall scenarios description the dry and wet scenarios correspond to temporal modifications of the base scenario using a bootstrap approach in the process the rainfall field of the watershed was shifted at any random day d i during the study period the day d i becomes a receiver day d r if it falls within the receiver daily total rainfall range d r m i n d i d r m a x the donor rainfall d d corresponds to a random day that falls inside the donor daily total rainfall range d d m i n d j d d m a x finally the receiver day d r was replaced with the rainfall field of d d with a given probability p r d 0 2 and varying the receiver and donor ranges for the wet and dry cases in the dry case some days with total rainfall between 10 and 40 mm were shifted d r m i n and d r m a x respectively using random days with rainfall between 0 and 0 1 mm d d m i n and d d m a x respectively in the wet case 20 of the days with rainfall between 0 and 10 mm were changed using days with total rainfall between 10 and 30 mm a total rainfall of 2600 mm for the dry scenario and 3700 mm for the wet one was obtained compared with the base scenario 3100 mm variations of 16 and 21 for the dry and wet cases were obtained respectively see fig 6 2 6 groundwater modeling hydrogeosphere hgs is a control volume finite element model developed to simulate integrated surface and subsurface water flow aquanty 2013 and used to analyze a wide range of processes brunner and simmons 2012 this software has been selected to model the study area where comprehensive hydrogeological modeling is not publicly available yet because of its capabilities to integrate all hydrologic cycle processes however there are currently little data available to build a fully integrated surface subsurface water flow model as a first modeling step it is valuable to build a regional subsurface model to analyze groundwater recharge in one of the regions with the largest precipitation rates in colombia this model can be updated in the future as soon as field data are available according to several authors long term piezometric data and recharge modeling are scarce in the tropics scientific literature jasechko and taylor 2015 in the colombian case this lack of data is caused by restricted access to remote areas and safety issues which have limited fieldwork in the past decades furthermore the abundance of surface water resources has limited the development of hydrogeological studies the 3d model built has a thickness of 100 m ensuring a proper element size and the representation of the rock basement as a no flow bottom boundary the patángoras dam mentioned previously is located downstream and outside the study area it is therefore assumed that the simulation domain is not influenced by the dam s operation which is not represented in the model a 2d triangular mesh is generated with algomesh hydroalgorithmics 2016 to discretize ground surface it is refined around the drainage network to ensure a proper resolution for numerical purposes with element size varying from 20 m to 857 m then in hgs a 3d prismatic mesh is built by stacking multiple 2d triangular meshes at a variable vertical distance to represent the basement the weathered rock and the soil corresponding to each lithology with an average thickness of 90 m 7 m and 0 3 m respectively according to field observation the resulting 3d mesh is made of 445 620 nodes which constitute the vertices of 860 161 prismatic elements the two regional faults shown in fig 1 edén and palestina are included in the numerical model and are represented as discrete fractures with constant aperture following the parallel plate conceptual model implemented in hgs aquanty 2013 fractures and piezometers are discretized by the faces 2d planar finite elements and segments 1d linear finite elements of the 3d prisms respectively such that they share mesh nodes according to the common node approach aquanty 2013 170 849 2d elements and 97 1d elements are designed respectively as faults and piezometers the average hydraulic conductivity values of the hydrogeological units considered in the model are listed in table 3 specific storage has been assumed from literature values since no field data are available a value of 1 97 10 2 m 1 is assigned to the weathered rock and 4 3 10 5 m 1 to the basement after the manual calibration process dirichlet or first type boundary condition is assigned to the rivers and neuman or second type condition to the top of the model to assign the groundwater recharge that is externally calculated by wmf a no flow boundary is assigned to the lateral and bottom boundaries of the model two groundwater recharge cases are considered spatially uniform and variable the purpose is to quantify the variation of hydraulic heads at the observation points represented by the piezometers and to demonstrate the importance of considering a spatially variable recharge if rainfall shows significant changes over the study area the groundwater flow model is calibrated in steady state flow regime using the six water levels measured at the end of the first wet season of 2016 corpocaldas et al 2016 since there are no publicly available monitoring data to calibrate the model under transient flow conditions for the steady state simulation the spatially variable recharge is calculated as time average of the 94 days considered from march to may 2016 a second simulation is then executed with a uniform recharge calculated as the mean value 4 7 mm day of the spatially variable distribution 3 results 3 1 aquifer recharge fig 7 shows the simulated and the observed flow for the calibration period 2003 2015 with the adjustment process time to reach an optimal state warm up period highlighted by the light blue background to the left of the graphic as expected this warm up period is characterized by low model performance kge 0 33 and nse 0 06 on the contrary the performance improves in the rest of the period reaching the values of kge 0 62 and nse 0 56 the model performance for the whole period is described by kge 0 57 and nse 0 45 model calibration was focused on the hydrograph recession curves and baseflow conditions since the model is used to estimate the aquifer recharge the model performance for low flows was nselog 0 68 the observed and simulated flow duration curves also ratified a better match for the hydrograph recession and low flows an acceptable representation of the streamflow variability at the watershed outlet is achieved fig 8 for the coupled modeling period the model performance for the base scenario has a kge equal to 0 83 nse of 0 75 pbias equal to 7 2 and an msle of 16 despite the limited number of stations moreover the model has a good representation of the base flows and recessions while some of the peak flows are not accurately represented the dry scenario maintains base flow and reduces streamflow during wet months such as march april and may the wet scenario increases streamflow during days with high rainfall accumulation and also during recession periods the three rainfall scenarios produce significant differences in aquifer storage fig 9 a the base and dry scenarios are equal until mid february while the wet scenario has more storage from the starting point of the simulations the mean aquifer storages are 150 170 and 200 mm for the dry base and wet scenarios respectively those aquifer storage values correspond to differences of 11 and 18 for the dry and wet scenarios with respect to the base scenario the described differences decrease for the days between may 1st and may 15th corresponding to a period of more significant rainfall accumulation the differences in aquifer storage also translate into differences in simulated aquifer recharge fig 9b at the end of the simulation period the accumulated recharge values are 648 915 and 503 mm for the base dry and wet scenarios respectively the described values correspond to differences with respect to the base scenario of 22 and 41 for the dry and wet scenarios respectively the differences for recharge are greater than the ones obtained for total rainfall 16 and 22 and aquifer storage 11 and 18 the rainfall recharge and aquifer storage differences are attributed to the non linear processes in the watershed and are represented in the wmf t model however more research is required to truly understand the influence of the temporal rainfall patterns on the aquifer dynamics additionally the spatial distribution of the simulated recharge was analyzed with respect to total rainfall fig 10 according to fig 10a there is a significant rainfall gradient between the east downstream region and west upstream of the la miel river watershed the total rainfall accumulation oscillates between 2000 and 3000 mm upstream and between 4000 and 5000 mm downstream the base scenario recharge fig 10a follows to some extent the spatial distribution of parameters k s and k p fig 5a and b respectively as expected since the hydraulic conductivity k s depends on soil texture and the percolation rate k p depends on geology opposite to the rainfall pattern fig 10a a significant portion of the simulated recharge occurs upstream additionally fig 10c shows the percentage of rainfall that becomes recharge with the highest percentage occurring upstream these results highlight the control of the type of soil on the aquifer recharge dynamics 3 2 groundwater flow table 4 shows the results of the simulations conducted with spatially variable and uniform recharge the use of the uniform recharge shows a significant change in the simulated hydraulic heads the largest variation is observed in piezometer p5 followed by p7 and p6 which are located in the area with a recharge lower than the mean value as shown in fig 9b for year 2016 in contrast p3 p15 and p2 show a decrease in hydraulic heads however this variation is much limited about 27 m than the increase observed in the other zone because of the smaller difference with the average recharge value these three piezometers are located in an area where the recharge is about 5 mm day only 6 larger than the average value these results confirm that the assumption of a spatially uniform recharge to the saturated zone is questionable seibert 1997 it is worth quantifying the impact of using a spatially uniform recharge to ensure a suitable implementation of a water management plan that should include the assessment of groundwater resources the impact of the two regional faults is shown in fig 11 the spatial distribution of hydraulic heads generally follows the topography and hydrographic network fig 1a and they are much larger in the western and central part of the watershed when the faults are not included the medium is in fact less permeable without the two faults which allow local recharge and cannot be neglected for a proper analysis of regional groundwater flow this result is coherent with the fractured aquifer identified in the study area by corpocaldas et al 2016 this regional study provides new and useful insights for the country and for tropical regions characterized by a similar context with few field data and a growing interest in groundwater jasechko and taylor 2015 transient simulation results for the base scenario are shown in fig 12 the hydraulic head variation δh which is calculated with respect to the initial condition corresponding to the steady state regime shows different behaviors depending on the piezometer location the largest δh is observed in p15 which is the northernmost piezometer and it is located in a zone where a locally high recharge has been calculated on the other hand p5 p6 and p7 are located approximately at the same longitude in the eastern portion of the study area characterized by a low recharge finally although p2 is the only piezometer located on the cajamarca quartzites it shows a behavior similar to p3 since they are located in an area with the same recharge the hydraulic head fluctuations can thus be grouped into three main trends that are associated to the piezometer location and recharge however geology and soil texture have been considered in the wmf software to calculate recharge such that geological characteristics also have an influence on the hydraulic head variation fig 12 also shows the delayed response of piezometric levels for example the largest δh in p15 occurs at day 72 2016 05 12 while the recharge peak is observed at day 68 4 days before as soon as recharge stops such as between days 73 and 88 the piezometric levels decrease toward their initial value or eventually below it for p7 and p6 the dry and wet scenarios characterized by an average recharge of 3 4 mm day and 5 75 mm day respectively are analyzed to assess their impact on piezometric fluctuations fig 13 shows the δh simulated at piezometers p7 and p15 which have been selected as they show the minimum and maximum range of variation δh respectively when precipitation intensity increases during the rainy season between april and may the increase in recharge determines larger δh since the different precipitation scenarios are randomly generated the peaks observed in fig 13 do not occur at the same time the purpose of comparing a wet and dry scenario is to quantify the maximum variations that can be expected in piezometric levels which are useful for groundwater management looking at p15 during the dry scenario at mid may its level is higher than p7 while the opposite behavior is observed at the beginning of april with differences of the order of few tens of centimeters in contrast looking at the wet scenario a more considerable difference is observed reaching almost 1 m after the rainy period of april and may 4 discussion and conclusion this paper describes sequential surface subsurface flow modeling applied to a hydrological system in a tropical climate the study area is the upper part of the la miel river watershed at the eastern slope of the central andes mountain range in the department of caldas colombia the water modeling framework wmf and hydrogeosphere hgs software have been used for surface and subsurface water flow modeling a fully distributed hydrological model that considers spatial variability of the runoff generation infiltration and percolation processes is built with wmf using rainfall observations and two alternative synthetic scenarios based on dry and wet conditions three aquifer recharge cases are generated showing that the changes in the rainfall induce complex variations in the recharge the use of rainfall scenarios along with simulation tools provide a framework to obtain more information of this complexity a 3d saturated groundwater flow model is built with hgs to quantify the impact of the spatiotemporally variable recharge calculated by wmf applied as top flow boundary condition to the subsurface model on the simulated hydraulic heads at six piezometers considered as the observation points this 3d finite element model includes the main geological units identified in the study area the samaná igneous complex cajamarca complex quartzite and schists superficial pyroclastic and glaciofluvial deposits samaná alaskite el hatillo stock guadalupe porphyry and mesa formation the main results can be summarized as follows 1 the comparison between uniform and spatially variable recharge highlights the importance of its proper assessment to simulate groundwater dynamics 2 the selection of a suitable conceptual model for the fractured aquifer is also evidenced by the relevant variation of simulated hydraulic heads considering or neglecting the discrete fractures representing the two fault zones identified in the study area and 3 dry and wet scenarios are shown to be an appropriate tool to quantify the impact of climatic variability on groundwater recharge the results shown here provide new valuable knowledge for managing water resources in the region and highlight the importance of adequately estimating groundwater recharge and properly representing fractured aquifers these two topics still require detailed investigations in colombia due to the limited number of hydrogeological studies conducted the main contribution of this work is to create a modeling framework that can be updated and used in future studies and to present a new methodology for sequential modeling based on wmf and hgs using a hydrological distributed model provides an advantage for the integrated management of water resources and the study of hydrological processes however they are data intensive and require parameters calibration which can be time consuming nevertheless the computing time of a fully integrated surface subsurface simulation with hgs would be even longer because the subsurface flow in a variably saturated porous medium should be considered together with the diffusion wave approximation of the saint venant equation for surface water flow aquanty 2013 furthermore building this kind of model requires a larger dataset that is currently not available although the simulation of groundwater dynamics considering saturated conditions is a limitation of the subsurface modeling component of this work it is considered appropriate to describe the interaction between surface and subsurface physical processes in the study area considering the available data among other limitations a higher number of streamflow gages will allow more robustness in the results moreover the calibration period for the subsurface water model could be longer to obtain more reliable results this requirement demonstrates that is important to improve hydrogeological studies in the country and install monitoring network for piezometric levels although the sequential modeling is fine it could be improved with online fully coupling between wmf and hgs to have surface and subsurface variable feedback among both models about the differences between the two models wmf provides a detailed and explicit calculation of groundwater recharge which is an often used parameter for managing groundwater resources in comparison recharge is neither an input nor an output parameter in fully coupled surface subsurface flow models such as hgs and it must be internally calculated after a simulation using fractured media conceptual models and variable aquifer recharge are common challenges in subsurface water flow modeling explicit discretization of fractures increases the finite element mesh and as a consequence computing time on the other hand recharge varies widely in space and time and its direct measurement is difficult this study provides a practical framework based on sequential modeling to obtain a first and quick estimation of the impact of variable groundwater recharge on subsurface flow dynamics future work should be focused on installing a monitoring network to provide groundwater level records that can be used to calibrate the hydrogeological model under a transient regime it is worth mentioning that it is a need in several country areas not only in the la miel river watershed considering the general lack of hydrogeological information in colombia this study can therefore be helpful to promote field characterization and hydrological groundwater modeling exercises in other areas of the country contributing to integrated water management which is a national goal mavdt 2010 a fully coupled surface subsurface water flow model can also be built in the future to provide a better understanding of the interaction between precipitations infiltration runoff recharge and groundwater flow future work may also include model based predicted rainfall scenarios more information is required to achieve these goals such as the characterization of the vadose zone water retention curve and a larger dataset to describe the subsurface water system declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been conducted in the context of a research project funded by the universidad de medellín medellín colombia in collaboration with université laval in québec québec canadá and the university of iowa usa 
25642,the la miel river watershed is an area of high hydrological interest in colombia due to its abundant water resources high annual precipitation and hydroelectric power generation this work proposes a sequential modeling framework to quantify the spatially and temporally variable groundwater recharge and to analyze its impact on piezometric fluctuations in the study area where the groundwater flow is affected by geological faults water modeling framework wmf and hydrogeosphere hgs models have been used to calculate groundwater recharge and distribution of hydraulic heads respectively recharge computed with wmf is used as input to hgs to compare groundwater flow 1 with uniform and spatially variable recharge 2 with and without discrete fractures and 3 during dry and wet conditions the results generate valuable knowledge for water resource management and highlight the importance of groundwater recharge estimation and a proper representation of fractured aquifers keywords aquifer recharge wmf hydrogeosphere rainfall scenarios 1 introduction it is estimated that more than half of the world s population depends on groundwater as a source of drinking water oki and kanae 2006 however groundwater resources are often not managed to ensure its long term sustainability sophocleous et al 2009 and their depletion and pollution prevail in both developed and developing countries konikow and kendy 2005 rodell et al 2009 sustainable management of water resources environmental impacts assessment of water exploitation activities flood and drought risk and soil and vegetation management can be evaluated through the development of surface and subsurface numerical models rébori et al 2009 chaney et al 2015 yu et al 2019 velasquez et al 2020 the modeling of surface and groundwater resources has increased in the last decades lerner and kumar 1991 crawford and linsley 1966 simunek et al 1998 beven 2001 izbicki et al 2004 harou et al 2009 chandio et al 2012 anderson et al 2015 singh 2018 surface and subsurface flow models are helpful to understand the hydrological processes and their interactions at the watershed scale which is fundamental knowledge for the sustainable management of water resources publications in integrated surface subsurface water modeling can be found in jayakrishnan et al 2005 kalbus et al 2006 jhorar et al 2009 partington et al 2011 levy and xu 2012 lamontagne et al 2014 hassan et al 2014 barthel and banzhaf 2015 wei et al 2019 cardoso de salis et al 2019 yu et al 2019 wijayarathne and coulibaly 2020 other modeling techniques are related to machine learning approaches although they have already been applied jimeno sáez et al 2017 their use in hydrological science is still the object of detailed analysis nearing et al 2021 moreover lumped and distributed hydrological approaches can also be employed perez sanchez et al 2019 bournas and baltas 2021 in colombia surface water models have been implemented for different applications such as flood risk analysis water resources management the impact of glacier melt on river flow and analysis of satellite based rainfall data on hydrological modeling among others marulanda aguirre et al 2016 elgamal et al 2017 rodríguez et al 2019 velasquez et al 2020 in comparison the interest in subsurface water models has only recently started historically the focus has been on surface water resources which are abundant in the country and the professional expertise in hydrogeology and the amount and quality of data required to build groundwater models were limited however groundwater is an essential resource in several regions of the country such as in the valle del cauca la guajira and sucre departments as well as in the bogotá savanna and urabá regions lobo guerrero and gilboa 1987 mavdt 2010 ossa valencia and betancur vargas 2018 in these regions rural populations agricultural irrigation and drinking water supply depend mainly on groundwater resources there is an urgent need to address water resource management in colombia in a more integrated way such that the interactions between surface and subsurface water can be better understood as a result of the first evaluation and classification of colombian hydrogeological systems in 2013 it was estimated that at least 75 of the colombian territory has a great storage potential for groundwater resources with a total of 5848 km3 in estimated reserves ideam 2013 despite the efforts that have been made the management of these reserves is still limited however guidelines for formulating aquifer environmental management plans and a groundwater national program were defined mads 2014 in 2019 an updated version of the hydrogeological zones was published it indicates that only 30 8 of the aquifer systems are identified with sufficient knowledge for groundwater management and that groundwater recharge remains a topic that requires detailed investigations at different scales ideam 2019 the uppermost part of the la miel river watershed belongs to the areas without detailed hydrogeological studies it is chosen here as a case study to compensate for the lack of knowledge in the region and to analyze the impact of a proper conceptual model for the fractured aquifer located in the central and western portion of the study area where two regional faults and a lineament are identified in fact one can rarely model flow in a fractured rock considering it as a continuous uniform isotropic unit neuman 2005 and the numerical results depend on the uncertainty associated to the conceptualization of the movement of groundwater in fractured environments selroos 2002 although much research has been conducted it is challenging to accurately determine the permeability of fractured media generating modeling challenges that are not present in standard porous media zhang 2013 berre et al 2019 in colombia studies on the hydrogeology of fractured rocks are few piña et al 2019 such that this case study provides valuable knowledge for groundwater modeling a sequential modeling procedure is proposed here where groundwater recharge calculated by the water modeling framework wmf surface model is used as an inflow boundary condition to the groundwater model built with the hydrogeosphere hgs software aquanty 2013 sequential modeling to study surface and subsurface water interaction has been applied in some studies such as in pulido velazquez et al 2018 and okkonen and kløve 2011 however the physical aspects of those studies and their implementation are more straightforward than the work conducted and analyzed here our work has two main objectives first assess the impact of spatially variable recharge and faults associated with the fractured aquifer on the simulated steady state hydraulic heads second analyze the amplitude of hydraulic head variations and the groundwater level response to recharge the time lag for transient flow considering a spatiotemporally variable groundwater recharge these goals are relevant since fractured media are often simplified to equivalent porous media with few attempts to quantify and incorporate spatiotemporal recharge variability into groundwater modeling jyrkama et al 2002 batelaan and smedt 2007 moeck et al 2018 besides recharge relies on vegetation precipitation climate topography geology and soil type which are also variable making it a challenging parameter to quantify dripps and bradbury 2010 moreover recharge is one of the most poorly constrained hydrological parameters in almost all groundwater flow models and the least understood zomlot et al 2015 therefore recharge variability is often neglected in groundwater models and a single value is estimated as a percentage of the precipitation or as a model fitted parameter determined by calibration dripps and bradbury 2010 although the role of recharge conceptualization in groundwater modeling has been generally recognized in the literature few concrete examples include and quantify this parameter ehtiat et al 2016 manna et al 2019 this work presents the first study based on sequential modeling using wmf and hgs while wmf offers robust hydrologic simulation and analysis solutions hgs is widely known as a hydrogeological simulator both models are combined to develop a new straightforward modeling methodology to capture the spatiotemporal variations of aquifer recharge this study contributes to the understanding and expands the complex topic of surface subsurface water modeling 2 data and methods 2 1 description of the study area the study area is located in the upper part of the la miel river watershed at the eastern slope of the central andes mountain range in the municipalities of samaná pensilvania victoria marquetalia and manzanares in the department of caldas colombia fig 1 the la miel river begins in cuchilla la picota in the municipality of pensilvania which is located in the western part of the watershed at approximately 3600 m a s l it flows towards the northeast and ends by flowing into the left bank of the magdalena river at 146 m a s l the studied area has an approximate area of 475 km2 which represents 40 of the entire watershed elevation ranges from 482 to 3680 m a s l the average slope and watershed length are estimated to be 38 and 36 km respectively the main river has a slope and length of 5 and 48 km respectively with a strahler horton order of 7 the primary vegetation is cropland and pasture which covers 63 of the watershed it is followed by forest with 30 in total area mixed rangeland with 6 and the remaining 1 corresponds to open water and urban areas the upper la miel river watershed contains a protected area a portion of the florencia national natural park and other smaller preservation areas such as el popal la linda el guayabo and la gaviota the geological context which plays a key role in constructing the hydrogeological model is described in detail in a separate section according to the classification of hydrographic and hydrogeological units of colombia ideam 2013 the study area is located in the magdalena cauca hydrographic region and the mariquita dorada salgar aquifer system which belongs to the hydrogeological province of the magdalena medio valley the aquifer systems in this province are composed of alluvial sediments terraces and sandstone sequences paleogene neogene conglomerates and metamorphic rocks of the cajamarca complex with secondary permeability ideam 2013 the yearly precipitation regime in the study area is bimodal fig 2 with two rainy march may and september november and two dry seasons december february and june august the average annual precipitation ranges from 2500 mm to 7500 mm approximately with the lowest values observed in the west upstream region and the highest values observed in the east downstream part of the watershed due to the high water availability and the favorable topography water is stored in a reservoir for hydroelectric power generation the la miel moro and santa barbara rivers have been dammed at norcasia municipality patángoras dam to form the amaní reservoir which has an area of 12 2 km2 and a storage capacity of 571 mm3 the dam is a roller compacted concrete of 188 m in height located 47 km upstream of the magdalena river its installed capacity is 396 mw and it started operating in 2002 generating approximately 1460 gwh of electricity https isagen com co es nuestro negocio generamos energia central hidroelectrica miel i the outlet of the study area corresponds to the streamflow gauge located in la miel river upstream of the amaní reservoir near its tailwater 2 2 geological context the geology of the study area is composed mainly of metamorphic rocks influenced by a high grade of tectonism the oldest rocks are of the paleozoic era such as the cajamarca complex which constitutes the basement of the central range of the andes mountains which are divided into three ranges in the colombian territory the cajamarca complex covers 90 of the study area it is subdivided into two metamorphic lithological units the first one is located in the western part of the area in the pensilvania and marquetalia municipalities and it consists of quartzite and biotite feldspathic quartzite while the latter is located in the east and consists of quartz sericite schists tres which are usually found as graphitic schists with good foliation igneous and sedimentary rocks represent cretaceous lithologies one of the most representative is the samaná igneous complex composed of a series of intrusive rocks mainly diorites quartz diorites tonalites granodiorites and hornblende pyroxene gabbros the alaskite lithology of the samaná unit forms a crown on the eastern edge of the samaná igneous complex where quartz plagioclase and biotite are found in greater quantity corpocaldas et al 2016 the el hatillo stock tcdh is located to the southeast of the study area between the municipalities of samaná and marquetalia and is composed of quartz dioritic rocks from the tertiary period in some locations this unit is observed as a saprolitic deposit of igneous rocks with clay texture and red coloration where its mineralogical composition cannot be differentiated the guadalupe porphyry tadg unit is located between the municipalities of manzanares and pensilvania it is composed of porphyritic rocks where hornblendic andesite predominates finally the mesa formation tsm unit is located in the municipality of victoria it has a good stratification composed of volcanic material with intercalations interspersed of coarse grained sandstones with agglomerates corpocaldas et al 2016 two regional faults are observed in the study area the palestina and the el eden faults the palestina fault was defined as a dextral fault with a length of 350 km that crosses the central mountain range and presents a morphological expression characterized by remarkable depressions with elongated and alienated valleys gonzález 2001 the segment of the fault that lies within the study area affects especially the aluminum silicate schist unit the samaná igneous complex kds samaná alaskite unit kas and the superficial pyroclastic and glaciofluvial deposits qto the el eden strike slip fault is located to the southwest of the study area between the municipalities of marulanda and pensilvania in addition a lineament has been identified in the middle of the study area fig 1b according to photointerpretation changes in the river direction and the interpretation of structural and lithological changes as indicated in the la miel river watershed planning and management plan pomca corpocaldas et al 2016 2 3 data collected and fieldwork the data required for the surface and subsurface flow modeling are topography daily rainfall data daily discharge and piezometric data levels the digital elevation model dem was obtained from alos palsar with a 12 5 m resolution resampled to a 60 m resolution to reduce model computing time daily precipitation time series are obtained for the period 2003 2017 within the study area the analysis period is chosen based on the length and completeness of data records seven rain gauges fig 1 from the instituto de hidrología meteorología y estudios ambientales ideam are selected to represent precipitation over the study area daily discharge values for the period 2003 2015 are obtained from the isagen electricity generation company in colombia gauge puente samaná located in the outlet of the study area water levels measured in six piezometers at the end of the first rainy season march may were obtained from la miel watershed management plan pomca the spatial land cover and soil texture data required by the surface model are obtained from geographic institute agustin codazzi igac 2014 each soil profile is a group of horizons with organic matter and textural characteristics using the soil profiles and the soil plant air water program spaw the wmf hydraulic conductivity k s and the static storage h u are estimated for each horizon and then averaged over the vertical profile fieldwork activities have been conducted to collect rock samples and observe the thickness of weathered profiles to improve the geological characterization required to build the 3d subsurface model satellite imagery has been used to identify the points suitable for rock sampling and observation of soil horizons considering that they should be located close to roads and in areas with evidence of past slope movements bedrock outcrops or close to creeks and streams most of the rock samples collected correspond to pq quartzite and tres schist lithological units belonging to the cajamarca complex table 1 which is the most prominent geological formation in the study area fig 1b soils color textural class and weathering profile were analyzed to build vertical sequences that are spatially correlated to identify the lateral relationships required to define the stratigraphy of the study area vera torres 1994 a stratigraphic column is constructed for each lithological unit such as those shown in fig 3 for the metamorphic cajamarca complex the samaná igneous complex and the mesa sedimentary formation 2 4 surface and subsurface flow modeling according to the national map recently published ideam 2019 the la miel river watershed falls within the region of low groundwater recharge however high recharge areas may exist but they did not show up because the ideam map has been built at the country scale therefore the modeling work conducted here focuses on quantifying the spatial and temporal variation of recharge with the surface modeling tool wmf and its successive use to assess the groundwater flow dynamics with the subsurface modeling tool hgs both surface and subsurface flow models cover the study area of 475 km2 shown in fig 1 three scenarios are defined the first uses rainfall observations while the other two are synthetic based on dry and wet conditions using wmf and the scenarios the spatiotemporal variable recharge is calculated each recharge scenario is then used as a flow boundary condition in hgs to assess the impact of higher and lower recharge on the groundwater flow response in the study area the years 2003 2015 are used for the surface model calibration nevertheless only march may 2016 is simulated with the subsurface model since groundwater level records are only available for those months 2 4 1 watershed modeling framework wmf wmf is a fortran90 python numerical package developed to produce geomorphological analysis and distributed hydrological simulations for the hydrological simulations the package uses wmf t which is a modification of the tetis model vélez 2001 francés 2007 the modifications include saturated runoff production and non linear lateral flow equations conceptually the model divides the watershed into cells that contain 4 to 5 storages or tanks the tanks are aligned vertically at the cell location with tank 1 being the uppermost one fig 4 tanks 1 to 4 represent the capillary runoff sub surface and subterranean storages the 5th tank represents channel storage and its existence depends on a pre defined upstream area threshold of 0 1 km2 specified by the observed channel at each cell the model solves vertical flow from tanks 1 to 4 then it solves horizontal flow that goes to the tanks of the downstream cell or if it exists to the channel tank a detailed description of the model equations and parameters is provided by velásquez 2020 wmf t model setup requires a dem distributed maps of land use soil texture and geology as described in the previous section those maps were used to estimate the saturated hydraulic conductivity ks mm s 1 and percolation kp mm s 1 as shown in fig 5 a and b respectively three rainfall scenarios base dry and wet were defined and considered as inputs the base scenario corresponds to the observed daily fields interpolated through the idw inverse distance weighted method using the rainfall gauges of the study region fig 1 this method has been used by several authors hohmann et al 2021 kurtzman et al 2009 besides velasquez et al 2011 found that it provides a robust interpolation for the region wmf model was manually calibrated for the period 2003 2015 with observed streamflow records at puente samaná gauge then the dry and wet scenarios were defined for the first months of 2016 since they include the period calibrated by the subsurface model these scenarios will be explained in section 2 5 table 2 presents the calibrated scalar values ri the aquifer recharge estimation corresponds to the vertical flux d 3 4 equation 1 between tanks 3 and 4 representing the sub surface and subterranean storages respectively after calibrating the model the simulated vertical flux for each cell between march and june of 2016 was recorded 1 d 3 4 min d 2 3 k p δ t 2 5 rainfall scenarios description the dry and wet scenarios correspond to temporal modifications of the base scenario using a bootstrap approach in the process the rainfall field of the watershed was shifted at any random day d i during the study period the day d i becomes a receiver day d r if it falls within the receiver daily total rainfall range d r m i n d i d r m a x the donor rainfall d d corresponds to a random day that falls inside the donor daily total rainfall range d d m i n d j d d m a x finally the receiver day d r was replaced with the rainfall field of d d with a given probability p r d 0 2 and varying the receiver and donor ranges for the wet and dry cases in the dry case some days with total rainfall between 10 and 40 mm were shifted d r m i n and d r m a x respectively using random days with rainfall between 0 and 0 1 mm d d m i n and d d m a x respectively in the wet case 20 of the days with rainfall between 0 and 10 mm were changed using days with total rainfall between 10 and 30 mm a total rainfall of 2600 mm for the dry scenario and 3700 mm for the wet one was obtained compared with the base scenario 3100 mm variations of 16 and 21 for the dry and wet cases were obtained respectively see fig 6 2 6 groundwater modeling hydrogeosphere hgs is a control volume finite element model developed to simulate integrated surface and subsurface water flow aquanty 2013 and used to analyze a wide range of processes brunner and simmons 2012 this software has been selected to model the study area where comprehensive hydrogeological modeling is not publicly available yet because of its capabilities to integrate all hydrologic cycle processes however there are currently little data available to build a fully integrated surface subsurface water flow model as a first modeling step it is valuable to build a regional subsurface model to analyze groundwater recharge in one of the regions with the largest precipitation rates in colombia this model can be updated in the future as soon as field data are available according to several authors long term piezometric data and recharge modeling are scarce in the tropics scientific literature jasechko and taylor 2015 in the colombian case this lack of data is caused by restricted access to remote areas and safety issues which have limited fieldwork in the past decades furthermore the abundance of surface water resources has limited the development of hydrogeological studies the 3d model built has a thickness of 100 m ensuring a proper element size and the representation of the rock basement as a no flow bottom boundary the patángoras dam mentioned previously is located downstream and outside the study area it is therefore assumed that the simulation domain is not influenced by the dam s operation which is not represented in the model a 2d triangular mesh is generated with algomesh hydroalgorithmics 2016 to discretize ground surface it is refined around the drainage network to ensure a proper resolution for numerical purposes with element size varying from 20 m to 857 m then in hgs a 3d prismatic mesh is built by stacking multiple 2d triangular meshes at a variable vertical distance to represent the basement the weathered rock and the soil corresponding to each lithology with an average thickness of 90 m 7 m and 0 3 m respectively according to field observation the resulting 3d mesh is made of 445 620 nodes which constitute the vertices of 860 161 prismatic elements the two regional faults shown in fig 1 edén and palestina are included in the numerical model and are represented as discrete fractures with constant aperture following the parallel plate conceptual model implemented in hgs aquanty 2013 fractures and piezometers are discretized by the faces 2d planar finite elements and segments 1d linear finite elements of the 3d prisms respectively such that they share mesh nodes according to the common node approach aquanty 2013 170 849 2d elements and 97 1d elements are designed respectively as faults and piezometers the average hydraulic conductivity values of the hydrogeological units considered in the model are listed in table 3 specific storage has been assumed from literature values since no field data are available a value of 1 97 10 2 m 1 is assigned to the weathered rock and 4 3 10 5 m 1 to the basement after the manual calibration process dirichlet or first type boundary condition is assigned to the rivers and neuman or second type condition to the top of the model to assign the groundwater recharge that is externally calculated by wmf a no flow boundary is assigned to the lateral and bottom boundaries of the model two groundwater recharge cases are considered spatially uniform and variable the purpose is to quantify the variation of hydraulic heads at the observation points represented by the piezometers and to demonstrate the importance of considering a spatially variable recharge if rainfall shows significant changes over the study area the groundwater flow model is calibrated in steady state flow regime using the six water levels measured at the end of the first wet season of 2016 corpocaldas et al 2016 since there are no publicly available monitoring data to calibrate the model under transient flow conditions for the steady state simulation the spatially variable recharge is calculated as time average of the 94 days considered from march to may 2016 a second simulation is then executed with a uniform recharge calculated as the mean value 4 7 mm day of the spatially variable distribution 3 results 3 1 aquifer recharge fig 7 shows the simulated and the observed flow for the calibration period 2003 2015 with the adjustment process time to reach an optimal state warm up period highlighted by the light blue background to the left of the graphic as expected this warm up period is characterized by low model performance kge 0 33 and nse 0 06 on the contrary the performance improves in the rest of the period reaching the values of kge 0 62 and nse 0 56 the model performance for the whole period is described by kge 0 57 and nse 0 45 model calibration was focused on the hydrograph recession curves and baseflow conditions since the model is used to estimate the aquifer recharge the model performance for low flows was nselog 0 68 the observed and simulated flow duration curves also ratified a better match for the hydrograph recession and low flows an acceptable representation of the streamflow variability at the watershed outlet is achieved fig 8 for the coupled modeling period the model performance for the base scenario has a kge equal to 0 83 nse of 0 75 pbias equal to 7 2 and an msle of 16 despite the limited number of stations moreover the model has a good representation of the base flows and recessions while some of the peak flows are not accurately represented the dry scenario maintains base flow and reduces streamflow during wet months such as march april and may the wet scenario increases streamflow during days with high rainfall accumulation and also during recession periods the three rainfall scenarios produce significant differences in aquifer storage fig 9 a the base and dry scenarios are equal until mid february while the wet scenario has more storage from the starting point of the simulations the mean aquifer storages are 150 170 and 200 mm for the dry base and wet scenarios respectively those aquifer storage values correspond to differences of 11 and 18 for the dry and wet scenarios with respect to the base scenario the described differences decrease for the days between may 1st and may 15th corresponding to a period of more significant rainfall accumulation the differences in aquifer storage also translate into differences in simulated aquifer recharge fig 9b at the end of the simulation period the accumulated recharge values are 648 915 and 503 mm for the base dry and wet scenarios respectively the described values correspond to differences with respect to the base scenario of 22 and 41 for the dry and wet scenarios respectively the differences for recharge are greater than the ones obtained for total rainfall 16 and 22 and aquifer storage 11 and 18 the rainfall recharge and aquifer storage differences are attributed to the non linear processes in the watershed and are represented in the wmf t model however more research is required to truly understand the influence of the temporal rainfall patterns on the aquifer dynamics additionally the spatial distribution of the simulated recharge was analyzed with respect to total rainfall fig 10 according to fig 10a there is a significant rainfall gradient between the east downstream region and west upstream of the la miel river watershed the total rainfall accumulation oscillates between 2000 and 3000 mm upstream and between 4000 and 5000 mm downstream the base scenario recharge fig 10a follows to some extent the spatial distribution of parameters k s and k p fig 5a and b respectively as expected since the hydraulic conductivity k s depends on soil texture and the percolation rate k p depends on geology opposite to the rainfall pattern fig 10a a significant portion of the simulated recharge occurs upstream additionally fig 10c shows the percentage of rainfall that becomes recharge with the highest percentage occurring upstream these results highlight the control of the type of soil on the aquifer recharge dynamics 3 2 groundwater flow table 4 shows the results of the simulations conducted with spatially variable and uniform recharge the use of the uniform recharge shows a significant change in the simulated hydraulic heads the largest variation is observed in piezometer p5 followed by p7 and p6 which are located in the area with a recharge lower than the mean value as shown in fig 9b for year 2016 in contrast p3 p15 and p2 show a decrease in hydraulic heads however this variation is much limited about 27 m than the increase observed in the other zone because of the smaller difference with the average recharge value these three piezometers are located in an area where the recharge is about 5 mm day only 6 larger than the average value these results confirm that the assumption of a spatially uniform recharge to the saturated zone is questionable seibert 1997 it is worth quantifying the impact of using a spatially uniform recharge to ensure a suitable implementation of a water management plan that should include the assessment of groundwater resources the impact of the two regional faults is shown in fig 11 the spatial distribution of hydraulic heads generally follows the topography and hydrographic network fig 1a and they are much larger in the western and central part of the watershed when the faults are not included the medium is in fact less permeable without the two faults which allow local recharge and cannot be neglected for a proper analysis of regional groundwater flow this result is coherent with the fractured aquifer identified in the study area by corpocaldas et al 2016 this regional study provides new and useful insights for the country and for tropical regions characterized by a similar context with few field data and a growing interest in groundwater jasechko and taylor 2015 transient simulation results for the base scenario are shown in fig 12 the hydraulic head variation δh which is calculated with respect to the initial condition corresponding to the steady state regime shows different behaviors depending on the piezometer location the largest δh is observed in p15 which is the northernmost piezometer and it is located in a zone where a locally high recharge has been calculated on the other hand p5 p6 and p7 are located approximately at the same longitude in the eastern portion of the study area characterized by a low recharge finally although p2 is the only piezometer located on the cajamarca quartzites it shows a behavior similar to p3 since they are located in an area with the same recharge the hydraulic head fluctuations can thus be grouped into three main trends that are associated to the piezometer location and recharge however geology and soil texture have been considered in the wmf software to calculate recharge such that geological characteristics also have an influence on the hydraulic head variation fig 12 also shows the delayed response of piezometric levels for example the largest δh in p15 occurs at day 72 2016 05 12 while the recharge peak is observed at day 68 4 days before as soon as recharge stops such as between days 73 and 88 the piezometric levels decrease toward their initial value or eventually below it for p7 and p6 the dry and wet scenarios characterized by an average recharge of 3 4 mm day and 5 75 mm day respectively are analyzed to assess their impact on piezometric fluctuations fig 13 shows the δh simulated at piezometers p7 and p15 which have been selected as they show the minimum and maximum range of variation δh respectively when precipitation intensity increases during the rainy season between april and may the increase in recharge determines larger δh since the different precipitation scenarios are randomly generated the peaks observed in fig 13 do not occur at the same time the purpose of comparing a wet and dry scenario is to quantify the maximum variations that can be expected in piezometric levels which are useful for groundwater management looking at p15 during the dry scenario at mid may its level is higher than p7 while the opposite behavior is observed at the beginning of april with differences of the order of few tens of centimeters in contrast looking at the wet scenario a more considerable difference is observed reaching almost 1 m after the rainy period of april and may 4 discussion and conclusion this paper describes sequential surface subsurface flow modeling applied to a hydrological system in a tropical climate the study area is the upper part of the la miel river watershed at the eastern slope of the central andes mountain range in the department of caldas colombia the water modeling framework wmf and hydrogeosphere hgs software have been used for surface and subsurface water flow modeling a fully distributed hydrological model that considers spatial variability of the runoff generation infiltration and percolation processes is built with wmf using rainfall observations and two alternative synthetic scenarios based on dry and wet conditions three aquifer recharge cases are generated showing that the changes in the rainfall induce complex variations in the recharge the use of rainfall scenarios along with simulation tools provide a framework to obtain more information of this complexity a 3d saturated groundwater flow model is built with hgs to quantify the impact of the spatiotemporally variable recharge calculated by wmf applied as top flow boundary condition to the subsurface model on the simulated hydraulic heads at six piezometers considered as the observation points this 3d finite element model includes the main geological units identified in the study area the samaná igneous complex cajamarca complex quartzite and schists superficial pyroclastic and glaciofluvial deposits samaná alaskite el hatillo stock guadalupe porphyry and mesa formation the main results can be summarized as follows 1 the comparison between uniform and spatially variable recharge highlights the importance of its proper assessment to simulate groundwater dynamics 2 the selection of a suitable conceptual model for the fractured aquifer is also evidenced by the relevant variation of simulated hydraulic heads considering or neglecting the discrete fractures representing the two fault zones identified in the study area and 3 dry and wet scenarios are shown to be an appropriate tool to quantify the impact of climatic variability on groundwater recharge the results shown here provide new valuable knowledge for managing water resources in the region and highlight the importance of adequately estimating groundwater recharge and properly representing fractured aquifers these two topics still require detailed investigations in colombia due to the limited number of hydrogeological studies conducted the main contribution of this work is to create a modeling framework that can be updated and used in future studies and to present a new methodology for sequential modeling based on wmf and hgs using a hydrological distributed model provides an advantage for the integrated management of water resources and the study of hydrological processes however they are data intensive and require parameters calibration which can be time consuming nevertheless the computing time of a fully integrated surface subsurface simulation with hgs would be even longer because the subsurface flow in a variably saturated porous medium should be considered together with the diffusion wave approximation of the saint venant equation for surface water flow aquanty 2013 furthermore building this kind of model requires a larger dataset that is currently not available although the simulation of groundwater dynamics considering saturated conditions is a limitation of the subsurface modeling component of this work it is considered appropriate to describe the interaction between surface and subsurface physical processes in the study area considering the available data among other limitations a higher number of streamflow gages will allow more robustness in the results moreover the calibration period for the subsurface water model could be longer to obtain more reliable results this requirement demonstrates that is important to improve hydrogeological studies in the country and install monitoring network for piezometric levels although the sequential modeling is fine it could be improved with online fully coupling between wmf and hgs to have surface and subsurface variable feedback among both models about the differences between the two models wmf provides a detailed and explicit calculation of groundwater recharge which is an often used parameter for managing groundwater resources in comparison recharge is neither an input nor an output parameter in fully coupled surface subsurface flow models such as hgs and it must be internally calculated after a simulation using fractured media conceptual models and variable aquifer recharge are common challenges in subsurface water flow modeling explicit discretization of fractures increases the finite element mesh and as a consequence computing time on the other hand recharge varies widely in space and time and its direct measurement is difficult this study provides a practical framework based on sequential modeling to obtain a first and quick estimation of the impact of variable groundwater recharge on subsurface flow dynamics future work should be focused on installing a monitoring network to provide groundwater level records that can be used to calibrate the hydrogeological model under a transient regime it is worth mentioning that it is a need in several country areas not only in the la miel river watershed considering the general lack of hydrogeological information in colombia this study can therefore be helpful to promote field characterization and hydrological groundwater modeling exercises in other areas of the country contributing to integrated water management which is a national goal mavdt 2010 a fully coupled surface subsurface water flow model can also be built in the future to provide a better understanding of the interaction between precipitations infiltration runoff recharge and groundwater flow future work may also include model based predicted rainfall scenarios more information is required to achieve these goals such as the characterization of the vadose zone water retention curve and a larger dataset to describe the subsurface water system declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been conducted in the context of a research project funded by the universidad de medellín medellín colombia in collaboration with université laval in québec québec canadá and the university of iowa usa 
25643,information concerning water availability in a basin can be key to trustworthy and robust decisions and reduce disputes over water among its multiple users the cerrado region however lacks basic hydrological information concerning water availability in many of its basins in this study two regionalization frameworks based on the donation of parameter sets from a hydrological model calibrated in gauged catchments were assessed these approaches were evaluated using a leave one out cross validation for the gauged catchments parameters donation by spatial proximity led to kge and rnse of 0 58 and 0 56 while by attributes proximity led to kge and rnse of 0 56 and 0 45 respectively a regional sample data set for the cerrado hydrocerrado was made available with information compiled for the 411 gauged catchments used in this study and runoff time series simulated for 4531 level 5 ottobasins by donating parameter sets by spatial proximity keywords regionalization gr5j tropical watersheds savanna availability of data and material the data set compiled in this study hydrocerrado is available at the following online repository https github com daniel althoff hydrocerrado 1 introduction with an extension of approximately 200 million hectares and occupying 24 of the brazilian territory the cerrado is brazil s second largest biome despite its acidic soils which are poor in organic matter and nutrients klink 2014 rada 2013 the cerrado was fundamental for the ascension of the brazilian agriculture in recent decades cremaq 2010 rada 2013 the region presents highly technified farms with potential for high productivity as seen in its latest agriculture frontier the matopiba an acronym for the states maranhão tocantins piauí and bahia one of the reasons for the agriculture success in the cerrado is its long wet season which lasts from october to april enabling double cropping within the same season spangler et al 2017 in addition to climate water availability plays a key factor in the region the cerrado has a high water yield contributing to the availability of surface water in many of the main hydrographic regions of brazil it encompasses lima 2011 and with a high potential for the expansion of irrigated areas althoff and rodrigues 2019 fealq 2014 however over the last few years there has been an increasing number of conflicts over water use in parts of the cerrado where irrigated agriculture expanded with poor planning and lack of hydrological information pousa et al 2019 irrigation represents alone 67 of all water consumption in the country ana 2017 besides studies considering different projections of climate change have highlighted several risks for sustainable socio economic development in the region for instance chou et al 2014 predicted an increase in temperature and decrease in rainfall while others predict an increase in the length of the dry season pires et al 2016 and a decrease in river discharge oliveira et al 2017 to guarantee social well being and sustainable development in the region it is crucial to have enough technical information to support the planning and management of water resources water allocation among multiple users usually comes with conflicting interests rodrigues et al 2014 requiring knowledge of the historic and current water availability and water demand within a basin pousa et al 2019 suggested that the knowledge of water availability in a basin which serves as a management unit can be key to trustworthy and robust decisions and can contribute to the reduction of conflicts during the dry season the cerrado region however lacks basic hydrological information that can be used to develop reliable estimates of water availability in specific regions of its basins there are many ways to determine water availability in hydrographic basins where streamflow measurements or other hydrometric data are lacking for example the complex interaction between physiographic factors such as climate land use and land cover topography and geology make the application of mathematical and computational models the best alternative to estimate target variables in ungauged basins such models can be valuable tools in the planning and management of water resources because they can simulate the impact of different factors on the basin s water availability a large number of models have already been developed to assess water availability beck et al 2017 devia et al 2015 douglas mankin et al 2010 perrin et al 2003 steenhuis et al 2009 these models should preferably be simple and easy to implement making use of recent advances of different data sources such as satellite or remote sensing derived products including evapotranspiration net radiation snow cover or soil moisture data chen and wang 2018 jiang and wang 2019 tran et al 2018 the use of gridded products has excelled in hydroclimatic modeling in the past 20 years for both conceptual lumped bucket style and physically based distributed models especially in regions with data scarcity jiang and wang 2019 tran et al 2018 however runoff simulations and predictions in ungauged basins pub remain a crucial challenge in water resource planning and management guo et al 2021 qi et al 2020 pub has been addressed mainly by regionalization approaches these approaches usually consist of donating hydrological model parameters calibrated at gauged catchments to predict runoff time series or hydrological signatures in ungauged catchments guo et al 2021 in a large sample study qi et al 2020 assessed several regionalization approaches for 2277 gauged catchments worldwide and found that donating hydrological parameters based on spatial proximity generally outperformed donation based on physical similarity attributes proximity whereas both outperformed regression techniques considering that previous efforts to regionalize hydrologic information in parts of the cerrado have mainly been addressed by regression techniques and for hydrologic signatures lopes et al 2017 morais et al 2020 pruski et al 2013 exploring the donation of hydrological parameters can be considered an advance since obtaining runoff series allows for more dynamic analyses to be carried out based on the current state of hydrological data availability in the cerrado and its importance for brazil s agriculture there is a need to improve information on water availability in the many unmonitored parts of the biome to help stakeholders make decisions based on scientific knowledge pousa et al 2019 thus the objectives of the present study were to i assess two frameworks for regionalization of hydrological model s parameters and ii to develop a runoff time series data set for all 4531 of the cerrado level 5 ottobasins ottobasins are basins and interbasins hierarchically classified according to the topological system proposed by otto pfafstetter see furnans and olivera 2001 the development of this data set enables the analysis of different hydrological signatures and water availability assessment across the cerrado biome 2 material and methods 2 1 study area the cerrado biome comprises the entire federal district and the state of goiás and encompasses areas of the states of tocantins maranhão piauí bahia minas gerais são paulo mato grosso do sul mato grosso and in smaller proportions the states of rondônia pará and paraná fig 1 ribeiro and walter 1998 also documented disjoint areas of the cerrado biome in the northern region of the states amapá amazonas pará and roraima the climate in the cerrado biome is predominantly classified as a tropical savanna climate aw on the köppen climate classification alvares et al 2013 fig 1b the aw climate is characterized by mean air temperature above 18 c in every month of the year where the precipitation of the driest month is less than 60 mm the tropical savanna climate is also known as a wet and dry climate because it has two well defined seasons in the cerrado about 90 of the total annual precipitation falls in the wet season which typically lasts from october to april while the dry season lasts from may to september klink 2014 rodrigues et al 2012 the cerrado is one of the main producers of surface water for brazil s major hydrographic regions for example the cerrado accounts for most of the surface water production for the paraná tocantins araguaia são francisco parnaíba and paraguai hydrographic regions lima 2011 besides the largest hydroelectric facilities in the country are placed along rivers in the cerrado playing a key role in brazil s energy matrix oliveira et al 2014 2 2 hydrological model the model chosen for the case study is the gr5j model le moine 2008 a five parameter daily lumped rainfall runoff hydrological model fig 2 the model estimates the water balance of a basin using a soil moisture accounting reservoir and a conceptual water exchange function f the routing structure consists of two unit hydrographs and a non linear store providing two flow components the water exchange function f allows simulating the exchange of water with deep aquifers or surrounding reservoirs pushpalatha et al 2011 there are five free parameters in the gr5j model that can be optimized x1 and x3 are the production and routing store capacity x2 and x5 regulate the groundwater exchange function and x4 is the time base of the unit hydrograph the only physical inputs required by the model are potential evaporation and precipitation the model is coded using the r environment and programming language r core team 2020 and the airgr package coron et al 2017 2020 2 3 data sources this section describes the data sources used for the development of the hydrological models in the cerrado the study period of 14 5 years starting june 1 2000 and ending december 31 2014 was chosen based on the best available temporal coverage of all data sets streamflow only presents consisted data i e data derived from calibrated rating curves up to december 31 2014 while potential evaporation and rainfall data are available from june 1 2000 2 3 1 observed streamflow and catchment selection the initial database consisted of 819 stations within the cerrado biome and a buffer area of 1 5 166 km around it daily streamflow data for the gauges were downloaded from the hidroweb portal http www snirh gov br hidroweb ana 2020 hidroweb is part of the national system of water resources information in portuguese sistema nacional de informações sobre recursos hídricos snirh the snirh offers access to the database containing information collected by the national hydrometeorological network in portuguese rede hidrometeorológica nacional rhn catchment boundaries were derived using the taudem toolset tarboton 2008 and the srtm void filled 3 arc seconds 90 m spatial resolution digital elevation model dem data earth resources observation and science center 2017 unsuitable stations catchments were discarded using the following criteria first catchment boundaries that did not intersect the cerrado biome were discarded second stations or periods with potentially erroneous streamflow or routing effects due to large dams identified through visual screening of the stations streamflow time series were discarded third catchments with less than 5 years of observed streamflow data during the calibration and validation period 2003 2014 see section 3 4 were discarded the final databased used in this study comprised 411 gauge stations catchment boundaries fig 3 2 3 2 meteorological forcing to adequately represent precipitation and potential evaporation across the watersheds gridded data products at daily resolution were used daily precipitation data from the integrated multi satellite retrievals for the global precipitation measurement gpm mission imerg huffman et al 2019a 2019b were used while the eto brazil product althoff et al 2020a 2020b was used to represent the potential evaporation over the watersheds both imerg and eto brazil have a spatial resolution of 0 10 0 10 10 km 10 km imerg data has been widely tested and is considered one of the most reliable gridded precipitation products available for brazil gadelha et al 2019 the eto brazil on the other hand is a gridded database developed for the penman monteith allen et al 1998 evapotranspiration using machine learning techniques and matching the imerg spatial and temporal resolution in addition both data sets cover areas within the cerrado where low weather station density limits data availability 2 4 model calibration and optimization algorithm the full record data set was split into three periods the first 2 5 years were used for the hydrological model warm up 2000 06 01 to 2002 12 31 the following 9 years 2003 01 01 to 2011 12 31 were used for the calibration of model parameters and the latest 3 years 2012 01 01 to 2014 12 31 for model validation despite the model run beginning in a period of lower soil moisture for its initial conditions june 2 5 years is a long enough warm up period kim et al 2018 a period of 9 years is also considered enough for the calibration of conceptual hydrological models ayzel and heistermann 2021 bai et al 2021 for gauge stations with an incomplete set of observations the first 75 of the available observations from 2003 to 2014 were used for the calibration of model parameters and the remaining 25 for the model validation the optimization of the model s parameters was carried out using two approaches the first is a single objective optimization for the kling gupta efficiency kge gupta et al 2009 and the second is a multi objective optimization for the kge and the relative nash sutcliffe efficiency rnse krause et al 2005 nash and sutcliffe 1970 these criteria are calculated as follows 1 r n s e 1 i 1 n x ˆ i x i x i 2 i 1 n x i x x 2 r n s e 1 2 k g e 1 e d k g e 1 with e d r 1 2 α 1 2 β 1 2 where x i and x ˆ i are observed and predicted streamflow at time step i respectively x is the mean observed streamflow n is the number of observations ed is the euclidian distance r is the linear correlation coefficient between x ˆ i and x i α is the variability ratio or ratio between the standard deviation of predicted values and standard deviation of observed values σ x ˆ σ x and β is the ratio between the mean predicted and mean observed values μ x ˆ μ x the kge works as a multi objective function that is based on the decomposition of errors in terms of the mean flow flow variability and flow dynamics gupta et al 2009 the rnse was used in the multi objective optimization to evaluate if a robust trade off that emphasizes low flow conditions can be found the rnse criterium was chosen considering the importance of the simulations during the dry season for assessing water availability in the cerrado the best parameter set was chosen from a pareto front by considering which set minimized the euclidean distance to the ideal point kge 1 rnse 1 eq 3 3 b e s t s e t m i n k g e 1 2 r n s e 1 2 where the best set is the parameter set that minimizes the distance to the ideal point kge 1 rnse 1 the model was optimized in r r core team 2020 using the particle swarm optimization pso technique clerc 2012 parsopoulos and vrahatis 2002 raquel and naval 2005 the multi objective pso was implemented using the mopsocd package in r naval 2013 2 5 regionalization approaches streamflow series were simulated at ungauged basins using regionalization of the parameters of the gr5j hydrological model calibrated at gauged basins the regionalization was based on the best performing method among the two regionalization methods assessed both methods were based on donor catchments i e gauged catchments for which the gr5j hydrological model was calibrated selected by different definitions of homogeneous regions in this study homogeneous regions are defined either as i regions with spatially close catchments or ii regions with catchments having similar attributes in land use soil type climate or catchment area pool et al 2019 the first regionalization method was based on five donor catchments from spatially close catchments m1 the second method was also based on five donor catchments but from catchments having similar attributes m2 according to qi et al 2020 the optimal number of donor catchments lies within 4 and 6 gauged catchments and the optimal threshold to qualify a gauged as an information donor is kge 0 50 however since the difference between setting a threshold of kge 0 50 and kge 0 70 was minimal for the study by the authors we adopted stations that scored over the kge threshold of 0 70 during the calibration period spatial proximity was defined as the euclidean distance between the coordinates of the centroids of given catchments while attribute similarity was defined as the euclidean distance in attribute space the attribute space consisted of fourteen selected catchment characteristics based on commonly used attributes in other regionalization studies addor et al 2018 beck et al 2020 kratzert et al 2019 pool et al 2019 razavi and coulibaly 2013 the attributes include the log transformed catchment area mean annual precipitation precipitation seasonality index mean annual reference evapotranspiration aridity index percentage of soil clay content percentage of soil sand content soil organic carbon content soil bulk density percentage of forest area percentage of crop area percentage of grasslands and pasture area mean elevation and mean slope all attributes were standardized before computing the attribute similarity the catchment attributes description range values and sources are described in table 1 for each regionalization method each donor catchment provided a parameter set that was used to compute a hydrograph at the ungauged catchment the 5 streamflow simulations were aggregated into an ensemble mean hydrograph eq 8 8 q i 1 n d 1 n q i d where q i is the mean streamflow simulation at time step i q i d is the streamflow simulation for the parameter set d and at time step i n is the total number of parameter sets from donor catchments d refers to each of the total n parameter sets the regionalization was evaluated for the validation period using a leave one out cross validation i e each gauged basin was treated as an ungauged catchment at a time and its streamflow simulated using the information from donor catchments the performance of the regionalization approaches was evaluated during the validation period using kge and rnse scores the scores of the regionalization approaches were also compared to an upper limit which are the scores achieved by the local calibration for the validation period and a lower limit which are the scores from benchmark series for the validation period the benchmark series consisted of the mean monthly streamflow from data observed during the calibration period and are similar to making naïve predictions for highly seasonal watersheds as are tropical ones the mean monthly streamflow from a known period is a reasonable baseline lower limit for the regionalization to show improvements on althoff et al 2021 2 6 runoff time series simulation at ungauged basins the total runoff streamflow series were simulated at all 4531 level 5 ottobasins considering the regionalization approach that performed best the ottoclassification is adopted in brazil by the snirh for the hierarchical classification of hydrologic units consisting of basins and interbasins furnans and olivera 2001 this classification system allows the identification of topologically related watersheds and can be used anywhere in the world although some ottobasins are only adjacent interbasins simulating streamflow series is a good measure of their runoff contribution in the watersheds they belong to 2 7 data availability a regional sample data set the hydrocerrado was made available at an online repository https github com daniel althoff hydrocerrado containing information compiled for the 411 catchments used in this study and the 4531 level 5 ottobasins 3 results and discussion 3 1 catchment attributes fig 4 presents an overview of the catchment attributes used in the study concerning topographic characteristics watersheds in the southeastern regions of the cerrado are in general smaller in area higher in altitude and present higher slopes for climate indices annual rainfall is higher in western cerrado and lower in eastern cerrado potential evaporation is higher in the eastern and northern cerrado leading to higher aridity indexes in these regions most of cerrado present similar cycles of rainfall and evapotranspiration with an exception to northern cerrado which has opposite cycles for land cover characteristics higher forest fractions are seen in northern cerrado while higher crop and grassland fractions concentrate in the central southern cerrado for soil characteristics the eastern cerrado have watersheds with higher clay content organic carbon concentration and bulk density the catchments attributes were considered static in time despite some may considerably change over time for example the average land cover fraction of the entire period was used although there has been an increase decrease of more than 40 in land cover fractions for some catchments fig 5 crops have replaced both grasslands and forests in the western region of the cerrado while primarily grasslands in the southern pastures replaced forests mostly in northern cerrado as substantial as these changes are they may be the reason for non stationarity and a possible lack of power of the hydrological models in simulating streamflow during both the calibration and validation period gupta et al 2009 for example the regionalization study by beck et al 2016 discarded catchments with forest gain or loss 20 based on the review of bosch and hewlett 1982 however considering the cerrado current scenario we would end up with only a few catchments and poor spatial coverage by discarding catchments with significant land cover changes in contrast catchments with significant land cover changes can present a degradation in performance during the validation period nevertheless since most of the cerrado has faced severe land cover changes in recent years it is better to assess the validation performance for such conditions than to deny their occurrence therefore all catchments were considered regardless 3 2 regionalization performance the performance of the regionalization approaches is shown in fig 6 along with the upper and lower performance limits i e local calibration and benchmark series the calibration function using both kge and rnse as objective function led to robust runoff simulations as seen by equivalent kge values for the local calibration and regionalization by spatial and attributes proximity but median rnse increase of 0 09 0 04 and 0 03 respectively when compared to using only kge as the objective function this highlights the potential of multi objective functions for the calibration of hydrological models when specific goals are well established in this case the rnse was chosen to improve simulations under low flow conditions and all further discussion considers only the outcome from using kge rnse as the objective function among the regionalization approaches donating parameter sets by spatial proximity resulted in median kge and rnse of 0 58 and 0 56 respectively which are 0 02 and 0 11 higher than median scores obtained when donating parameter sets by attributes proximity the substantially higher rnse obtained by spatial proximity regionalization is an important matter for the cerrado region this is due to the importance of adequately predicting water availability under low flow conditions for sustainable socio economic development in many regions the lower performance achieved by donors with attributes proximity may come from giving all catchment attributes the same weight when computing their physical similarity despite physically similar catchments showing close ranges for the attributes it is unclear which attributes are more meaningful and should be given more weight there is significant heterogeneity of the land surface conditions soil profiles and space time variability of climatic forcing which is lumped for the catchments and makes it difficult to unveil such relationships guo et al 2021 sivapalan 2003 as for the regionalization based on spatial proximity other aspects also deserve attention which is the number of catchments used as donors and their distance to target catchments in this study we used the threshold of kge 0 70 and only 5 donor catchments despite following the recommendations by qi et al 2020 better results may be found using different numbers of donor catchments either based on the maximum allowed distance or different kge thresholds the trade off of using higher kge thresholds is that a lesser number of donor catchments will be available or only in longer distances resulting in lower performance for predictions in ungauged catchments or even parameter discontinuity for some regions the kge threshold of 0 70 seemed reasonable compared to the results obtained by qi et al 2020 this threshold resulted in the availability of 82 n 339 of the stations as donor catchments and the average distance between the donor and target catchment of 71 km median 60 km max 359 km only the regionalization approach based on spatial proximity was used for further analysis since it performed better than the regionalization based on attributes proximity qi et al 2020 showed in a large scale study that regionalization by spatial proximity is generally better than regionalization by physical similarity however this may not be the case for every region arsenault and brissette 2016 guo et al 2021 in comparison to the upper limit the local calibration kge 0 70 the spatial proximity presented a decrease in median kge score of 0 12 and approximately 13 of the catchments presented substantially low kge scores 0 as opposed to 6 when local calibration is used in comparison to the lower limit the benchmark series kge 0 34 donating parameters by spatial proximity showed an increase in median kge score of 0 24 fig 7 a and b presents the scatterplots comparing kge scores obtained for the catchments using parameter sets donated by spatial proximity to scores obtained using the local calibration and benchmark series although the local calibration can be considered an upper limit the regionalization based on spatial proximity showed robustness with improvements in approximately 34 of the catchments this is because neighbor catchments can be hydrologically similar and therefore calibrate equivalent parameter sets ensembles obtained with multiple equivalent parameter sets can function as multiple calibration runs and be advantageous for the runoff predictions fig 7c e 7d show maps of the kge scores obtained for the catchments using local calibration and parameter sets donated by spatial proximity respectively the performance at ungauged catchments deteriorated more in the eastern and northern regions of the cerrado which matches with regions showing higher aridity indexes fig 4 a review study by guo et al 2021 highlights that many authors have also found lower performance scores for regionalization methods when aridity increases for example the global regionalization studies by beck et al 2020 and qi et al 2020 show lower performance for validation catchments in regions with low humidity index inverse of aridity index and in arid regions respectively 3 3 runoff time series in ottobasins the parameter sets from donor catchments were used to estimate runoff time series for all 4531 level 5 ottobasins median size 185 km2 for the period from 2003 to 2014 12 years it is important to highlight that the threshold of kge 0 70 resulted in distances between donor catchments and ottobasins similar to observed during the cross validation mean 97 km median 90 km max 410 km this means that the threshold allowed several donor catchments with similar distances to replicate the experimental conditions of the cross validation for a brief overview of the data set generated for the ottobasins two important hydrological signatures were computed the long term mean runoff termed here mean runoff q fig 8 b and minimum runoff with an exceedance probability of 95 q95 fig 8d q provides us some information on the water balance and availability in the catchments or ottobasins while q95 is the reference used by the brazilian national water agency for granting water withdrawal permits maximum allowed 70 of q95 there is a good spatial agreement between hydrological signatures computed for observed and simulated runoff series fig 8 overall there is remarkably higher water availability both represented by q and q95 at the transition between the cerrado and the amazon biome western cerrado where the tropical climate registers higher rainfall rates fig 4 there is also considerable water availability in southern cerrado which is closer to regions with a temperate climate the eastern and northern cerrado are marked by lower water availability due to their proximity to the transition to semiarid regions the correlation between the catchment attributes and hydrological signatures reinforces some already known relationships and unveils other aspects regarding the hydrological processes for example precipitation correlates to higher water availability correlation to q ρ 0 78 and q95 ρ 0 42 whereas higher aridity correlates with lower water availability q ρ 0 75 and q95 ρ 0 41 the mean slope shows a negative correlation with q95 ρ 0 20 which is likely due to better drainage and or increased direct runoff in the wet season the soil characteristics presented a lower correlation with the hydrological signatures in contrast lower water availability q is seen in watersheds with higher forest fractions q ρ 0 50 in comparison to grasslands and crops forests help intercept rainfall and increase perspiration which can reduce runoff in relation to base flow the effect of forests is context dependent although they can improve infiltration increasing recharge and therefore the base flow during the dry season the increase in perspiration for water limited regions may aggravate the water availability issue lele 2009 in this study higher forest fractions also negatively correlate to q95 ρ 0 32 the overall negative correlation between forest fractions and these hydrological signatures can also be attributed to higher forest fractions being seen in the northern cerrado where the climate is more arid fig 4 it may also be the case that crop first expanded over forested areas in the regions with higher water availability i e higher chances of success for the agribusiness activity thus the effects of increasing or decreasing forest fractions would be better assessed along with the trends in base flow for these areas these results are important and practical giving water managers more confidence in water resource planning and management for instance let us consider that we were able to compile only 411 observed streamflow series for the cerrado median catchment area 4280 km2 thus hydrological signatures are generally obtained for large catchments and water availability can only be assumed for smaller river segments under high uncertainty furthermore many areas do not fall within larger catchments and are completely unmonitored fig 3 the simulated runoff series data set comprises 4531 ottobasins median area 185 km2 of which roughly 50 represent independent river segments and not interbasins despite the cross validation being used to approximate the regionalization performance the average area of ottobasins is generally lower than that of the gauged basins thus there may exist biases in predicting hydrological responses for ottobasins which are difficult to measure in conclusion despite imperfect the application and assessment of regionalization techniques provide us with valuable information at an expected performance cross validation finally the data set for the ottobasins is also provided with some uncertainty quantification i e the prediction intervals consisting of quantiles 5 and 95 derived from the hydrographs simulated using donated parameters 4 conclusions hydrological models calibrated considering both the kling gupta efficiency kge and relative nash sutcliffe efficiency rnse in a multi objective function resulted in better performance for the regionalization as opposed to using the kge in an objective function alone the multi objective function resulted in a median rnse increase of 0 10 for the local calibration and 0 03 and 0 04 for regionalization by attributes proximity and spatial proximity respectively while maintaining equivalent kge scores the regionalization by spatial proximity outperformed that by attributes proximity physical similarity especially concerning rnse scores spatial proximity led to kge and rnse of 0 58 and 0 56 while attributes proximity led to kge and rnse of 0 56 and 0 45 respectively when compared to an upper performance limit which is the performance achieved with local calibration the regionalization by spatial proximity showed a decrease in median kge by 0 12 but showed robustness and was able to improve the performance on approximately 34 of the catchments concerning the benchmark series mean monthly streamflow from calibration period the regionalization by spatial proximity showed an increase in median kge by 0 24 funding this study was financed in part by the coordenação de aperfeiçoamento de pessoal de nível superior capes in english coordination of improvement of higher education personnel finance code 001 and by the conselho nacional de desenvolvimento científico e tecnológico cnpq in english national council for scientific and technological development grant number 142273 2019 8 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financed in part by the coordenação de aperfeiçoamento de pessoal de nível superior capes in english coordination of improvement of higher education personnel finance code 001 and by the conselho nacional de desenvolvimento científico e tecnológico cnpq in english national council for scientific and technological development grant number 142273 2019 8 
25643,information concerning water availability in a basin can be key to trustworthy and robust decisions and reduce disputes over water among its multiple users the cerrado region however lacks basic hydrological information concerning water availability in many of its basins in this study two regionalization frameworks based on the donation of parameter sets from a hydrological model calibrated in gauged catchments were assessed these approaches were evaluated using a leave one out cross validation for the gauged catchments parameters donation by spatial proximity led to kge and rnse of 0 58 and 0 56 while by attributes proximity led to kge and rnse of 0 56 and 0 45 respectively a regional sample data set for the cerrado hydrocerrado was made available with information compiled for the 411 gauged catchments used in this study and runoff time series simulated for 4531 level 5 ottobasins by donating parameter sets by spatial proximity keywords regionalization gr5j tropical watersheds savanna availability of data and material the data set compiled in this study hydrocerrado is available at the following online repository https github com daniel althoff hydrocerrado 1 introduction with an extension of approximately 200 million hectares and occupying 24 of the brazilian territory the cerrado is brazil s second largest biome despite its acidic soils which are poor in organic matter and nutrients klink 2014 rada 2013 the cerrado was fundamental for the ascension of the brazilian agriculture in recent decades cremaq 2010 rada 2013 the region presents highly technified farms with potential for high productivity as seen in its latest agriculture frontier the matopiba an acronym for the states maranhão tocantins piauí and bahia one of the reasons for the agriculture success in the cerrado is its long wet season which lasts from october to april enabling double cropping within the same season spangler et al 2017 in addition to climate water availability plays a key factor in the region the cerrado has a high water yield contributing to the availability of surface water in many of the main hydrographic regions of brazil it encompasses lima 2011 and with a high potential for the expansion of irrigated areas althoff and rodrigues 2019 fealq 2014 however over the last few years there has been an increasing number of conflicts over water use in parts of the cerrado where irrigated agriculture expanded with poor planning and lack of hydrological information pousa et al 2019 irrigation represents alone 67 of all water consumption in the country ana 2017 besides studies considering different projections of climate change have highlighted several risks for sustainable socio economic development in the region for instance chou et al 2014 predicted an increase in temperature and decrease in rainfall while others predict an increase in the length of the dry season pires et al 2016 and a decrease in river discharge oliveira et al 2017 to guarantee social well being and sustainable development in the region it is crucial to have enough technical information to support the planning and management of water resources water allocation among multiple users usually comes with conflicting interests rodrigues et al 2014 requiring knowledge of the historic and current water availability and water demand within a basin pousa et al 2019 suggested that the knowledge of water availability in a basin which serves as a management unit can be key to trustworthy and robust decisions and can contribute to the reduction of conflicts during the dry season the cerrado region however lacks basic hydrological information that can be used to develop reliable estimates of water availability in specific regions of its basins there are many ways to determine water availability in hydrographic basins where streamflow measurements or other hydrometric data are lacking for example the complex interaction between physiographic factors such as climate land use and land cover topography and geology make the application of mathematical and computational models the best alternative to estimate target variables in ungauged basins such models can be valuable tools in the planning and management of water resources because they can simulate the impact of different factors on the basin s water availability a large number of models have already been developed to assess water availability beck et al 2017 devia et al 2015 douglas mankin et al 2010 perrin et al 2003 steenhuis et al 2009 these models should preferably be simple and easy to implement making use of recent advances of different data sources such as satellite or remote sensing derived products including evapotranspiration net radiation snow cover or soil moisture data chen and wang 2018 jiang and wang 2019 tran et al 2018 the use of gridded products has excelled in hydroclimatic modeling in the past 20 years for both conceptual lumped bucket style and physically based distributed models especially in regions with data scarcity jiang and wang 2019 tran et al 2018 however runoff simulations and predictions in ungauged basins pub remain a crucial challenge in water resource planning and management guo et al 2021 qi et al 2020 pub has been addressed mainly by regionalization approaches these approaches usually consist of donating hydrological model parameters calibrated at gauged catchments to predict runoff time series or hydrological signatures in ungauged catchments guo et al 2021 in a large sample study qi et al 2020 assessed several regionalization approaches for 2277 gauged catchments worldwide and found that donating hydrological parameters based on spatial proximity generally outperformed donation based on physical similarity attributes proximity whereas both outperformed regression techniques considering that previous efforts to regionalize hydrologic information in parts of the cerrado have mainly been addressed by regression techniques and for hydrologic signatures lopes et al 2017 morais et al 2020 pruski et al 2013 exploring the donation of hydrological parameters can be considered an advance since obtaining runoff series allows for more dynamic analyses to be carried out based on the current state of hydrological data availability in the cerrado and its importance for brazil s agriculture there is a need to improve information on water availability in the many unmonitored parts of the biome to help stakeholders make decisions based on scientific knowledge pousa et al 2019 thus the objectives of the present study were to i assess two frameworks for regionalization of hydrological model s parameters and ii to develop a runoff time series data set for all 4531 of the cerrado level 5 ottobasins ottobasins are basins and interbasins hierarchically classified according to the topological system proposed by otto pfafstetter see furnans and olivera 2001 the development of this data set enables the analysis of different hydrological signatures and water availability assessment across the cerrado biome 2 material and methods 2 1 study area the cerrado biome comprises the entire federal district and the state of goiás and encompasses areas of the states of tocantins maranhão piauí bahia minas gerais são paulo mato grosso do sul mato grosso and in smaller proportions the states of rondônia pará and paraná fig 1 ribeiro and walter 1998 also documented disjoint areas of the cerrado biome in the northern region of the states amapá amazonas pará and roraima the climate in the cerrado biome is predominantly classified as a tropical savanna climate aw on the köppen climate classification alvares et al 2013 fig 1b the aw climate is characterized by mean air temperature above 18 c in every month of the year where the precipitation of the driest month is less than 60 mm the tropical savanna climate is also known as a wet and dry climate because it has two well defined seasons in the cerrado about 90 of the total annual precipitation falls in the wet season which typically lasts from october to april while the dry season lasts from may to september klink 2014 rodrigues et al 2012 the cerrado is one of the main producers of surface water for brazil s major hydrographic regions for example the cerrado accounts for most of the surface water production for the paraná tocantins araguaia são francisco parnaíba and paraguai hydrographic regions lima 2011 besides the largest hydroelectric facilities in the country are placed along rivers in the cerrado playing a key role in brazil s energy matrix oliveira et al 2014 2 2 hydrological model the model chosen for the case study is the gr5j model le moine 2008 a five parameter daily lumped rainfall runoff hydrological model fig 2 the model estimates the water balance of a basin using a soil moisture accounting reservoir and a conceptual water exchange function f the routing structure consists of two unit hydrographs and a non linear store providing two flow components the water exchange function f allows simulating the exchange of water with deep aquifers or surrounding reservoirs pushpalatha et al 2011 there are five free parameters in the gr5j model that can be optimized x1 and x3 are the production and routing store capacity x2 and x5 regulate the groundwater exchange function and x4 is the time base of the unit hydrograph the only physical inputs required by the model are potential evaporation and precipitation the model is coded using the r environment and programming language r core team 2020 and the airgr package coron et al 2017 2020 2 3 data sources this section describes the data sources used for the development of the hydrological models in the cerrado the study period of 14 5 years starting june 1 2000 and ending december 31 2014 was chosen based on the best available temporal coverage of all data sets streamflow only presents consisted data i e data derived from calibrated rating curves up to december 31 2014 while potential evaporation and rainfall data are available from june 1 2000 2 3 1 observed streamflow and catchment selection the initial database consisted of 819 stations within the cerrado biome and a buffer area of 1 5 166 km around it daily streamflow data for the gauges were downloaded from the hidroweb portal http www snirh gov br hidroweb ana 2020 hidroweb is part of the national system of water resources information in portuguese sistema nacional de informações sobre recursos hídricos snirh the snirh offers access to the database containing information collected by the national hydrometeorological network in portuguese rede hidrometeorológica nacional rhn catchment boundaries were derived using the taudem toolset tarboton 2008 and the srtm void filled 3 arc seconds 90 m spatial resolution digital elevation model dem data earth resources observation and science center 2017 unsuitable stations catchments were discarded using the following criteria first catchment boundaries that did not intersect the cerrado biome were discarded second stations or periods with potentially erroneous streamflow or routing effects due to large dams identified through visual screening of the stations streamflow time series were discarded third catchments with less than 5 years of observed streamflow data during the calibration and validation period 2003 2014 see section 3 4 were discarded the final databased used in this study comprised 411 gauge stations catchment boundaries fig 3 2 3 2 meteorological forcing to adequately represent precipitation and potential evaporation across the watersheds gridded data products at daily resolution were used daily precipitation data from the integrated multi satellite retrievals for the global precipitation measurement gpm mission imerg huffman et al 2019a 2019b were used while the eto brazil product althoff et al 2020a 2020b was used to represent the potential evaporation over the watersheds both imerg and eto brazil have a spatial resolution of 0 10 0 10 10 km 10 km imerg data has been widely tested and is considered one of the most reliable gridded precipitation products available for brazil gadelha et al 2019 the eto brazil on the other hand is a gridded database developed for the penman monteith allen et al 1998 evapotranspiration using machine learning techniques and matching the imerg spatial and temporal resolution in addition both data sets cover areas within the cerrado where low weather station density limits data availability 2 4 model calibration and optimization algorithm the full record data set was split into three periods the first 2 5 years were used for the hydrological model warm up 2000 06 01 to 2002 12 31 the following 9 years 2003 01 01 to 2011 12 31 were used for the calibration of model parameters and the latest 3 years 2012 01 01 to 2014 12 31 for model validation despite the model run beginning in a period of lower soil moisture for its initial conditions june 2 5 years is a long enough warm up period kim et al 2018 a period of 9 years is also considered enough for the calibration of conceptual hydrological models ayzel and heistermann 2021 bai et al 2021 for gauge stations with an incomplete set of observations the first 75 of the available observations from 2003 to 2014 were used for the calibration of model parameters and the remaining 25 for the model validation the optimization of the model s parameters was carried out using two approaches the first is a single objective optimization for the kling gupta efficiency kge gupta et al 2009 and the second is a multi objective optimization for the kge and the relative nash sutcliffe efficiency rnse krause et al 2005 nash and sutcliffe 1970 these criteria are calculated as follows 1 r n s e 1 i 1 n x ˆ i x i x i 2 i 1 n x i x x 2 r n s e 1 2 k g e 1 e d k g e 1 with e d r 1 2 α 1 2 β 1 2 where x i and x ˆ i are observed and predicted streamflow at time step i respectively x is the mean observed streamflow n is the number of observations ed is the euclidian distance r is the linear correlation coefficient between x ˆ i and x i α is the variability ratio or ratio between the standard deviation of predicted values and standard deviation of observed values σ x ˆ σ x and β is the ratio between the mean predicted and mean observed values μ x ˆ μ x the kge works as a multi objective function that is based on the decomposition of errors in terms of the mean flow flow variability and flow dynamics gupta et al 2009 the rnse was used in the multi objective optimization to evaluate if a robust trade off that emphasizes low flow conditions can be found the rnse criterium was chosen considering the importance of the simulations during the dry season for assessing water availability in the cerrado the best parameter set was chosen from a pareto front by considering which set minimized the euclidean distance to the ideal point kge 1 rnse 1 eq 3 3 b e s t s e t m i n k g e 1 2 r n s e 1 2 where the best set is the parameter set that minimizes the distance to the ideal point kge 1 rnse 1 the model was optimized in r r core team 2020 using the particle swarm optimization pso technique clerc 2012 parsopoulos and vrahatis 2002 raquel and naval 2005 the multi objective pso was implemented using the mopsocd package in r naval 2013 2 5 regionalization approaches streamflow series were simulated at ungauged basins using regionalization of the parameters of the gr5j hydrological model calibrated at gauged basins the regionalization was based on the best performing method among the two regionalization methods assessed both methods were based on donor catchments i e gauged catchments for which the gr5j hydrological model was calibrated selected by different definitions of homogeneous regions in this study homogeneous regions are defined either as i regions with spatially close catchments or ii regions with catchments having similar attributes in land use soil type climate or catchment area pool et al 2019 the first regionalization method was based on five donor catchments from spatially close catchments m1 the second method was also based on five donor catchments but from catchments having similar attributes m2 according to qi et al 2020 the optimal number of donor catchments lies within 4 and 6 gauged catchments and the optimal threshold to qualify a gauged as an information donor is kge 0 50 however since the difference between setting a threshold of kge 0 50 and kge 0 70 was minimal for the study by the authors we adopted stations that scored over the kge threshold of 0 70 during the calibration period spatial proximity was defined as the euclidean distance between the coordinates of the centroids of given catchments while attribute similarity was defined as the euclidean distance in attribute space the attribute space consisted of fourteen selected catchment characteristics based on commonly used attributes in other regionalization studies addor et al 2018 beck et al 2020 kratzert et al 2019 pool et al 2019 razavi and coulibaly 2013 the attributes include the log transformed catchment area mean annual precipitation precipitation seasonality index mean annual reference evapotranspiration aridity index percentage of soil clay content percentage of soil sand content soil organic carbon content soil bulk density percentage of forest area percentage of crop area percentage of grasslands and pasture area mean elevation and mean slope all attributes were standardized before computing the attribute similarity the catchment attributes description range values and sources are described in table 1 for each regionalization method each donor catchment provided a parameter set that was used to compute a hydrograph at the ungauged catchment the 5 streamflow simulations were aggregated into an ensemble mean hydrograph eq 8 8 q i 1 n d 1 n q i d where q i is the mean streamflow simulation at time step i q i d is the streamflow simulation for the parameter set d and at time step i n is the total number of parameter sets from donor catchments d refers to each of the total n parameter sets the regionalization was evaluated for the validation period using a leave one out cross validation i e each gauged basin was treated as an ungauged catchment at a time and its streamflow simulated using the information from donor catchments the performance of the regionalization approaches was evaluated during the validation period using kge and rnse scores the scores of the regionalization approaches were also compared to an upper limit which are the scores achieved by the local calibration for the validation period and a lower limit which are the scores from benchmark series for the validation period the benchmark series consisted of the mean monthly streamflow from data observed during the calibration period and are similar to making naïve predictions for highly seasonal watersheds as are tropical ones the mean monthly streamflow from a known period is a reasonable baseline lower limit for the regionalization to show improvements on althoff et al 2021 2 6 runoff time series simulation at ungauged basins the total runoff streamflow series were simulated at all 4531 level 5 ottobasins considering the regionalization approach that performed best the ottoclassification is adopted in brazil by the snirh for the hierarchical classification of hydrologic units consisting of basins and interbasins furnans and olivera 2001 this classification system allows the identification of topologically related watersheds and can be used anywhere in the world although some ottobasins are only adjacent interbasins simulating streamflow series is a good measure of their runoff contribution in the watersheds they belong to 2 7 data availability a regional sample data set the hydrocerrado was made available at an online repository https github com daniel althoff hydrocerrado containing information compiled for the 411 catchments used in this study and the 4531 level 5 ottobasins 3 results and discussion 3 1 catchment attributes fig 4 presents an overview of the catchment attributes used in the study concerning topographic characteristics watersheds in the southeastern regions of the cerrado are in general smaller in area higher in altitude and present higher slopes for climate indices annual rainfall is higher in western cerrado and lower in eastern cerrado potential evaporation is higher in the eastern and northern cerrado leading to higher aridity indexes in these regions most of cerrado present similar cycles of rainfall and evapotranspiration with an exception to northern cerrado which has opposite cycles for land cover characteristics higher forest fractions are seen in northern cerrado while higher crop and grassland fractions concentrate in the central southern cerrado for soil characteristics the eastern cerrado have watersheds with higher clay content organic carbon concentration and bulk density the catchments attributes were considered static in time despite some may considerably change over time for example the average land cover fraction of the entire period was used although there has been an increase decrease of more than 40 in land cover fractions for some catchments fig 5 crops have replaced both grasslands and forests in the western region of the cerrado while primarily grasslands in the southern pastures replaced forests mostly in northern cerrado as substantial as these changes are they may be the reason for non stationarity and a possible lack of power of the hydrological models in simulating streamflow during both the calibration and validation period gupta et al 2009 for example the regionalization study by beck et al 2016 discarded catchments with forest gain or loss 20 based on the review of bosch and hewlett 1982 however considering the cerrado current scenario we would end up with only a few catchments and poor spatial coverage by discarding catchments with significant land cover changes in contrast catchments with significant land cover changes can present a degradation in performance during the validation period nevertheless since most of the cerrado has faced severe land cover changes in recent years it is better to assess the validation performance for such conditions than to deny their occurrence therefore all catchments were considered regardless 3 2 regionalization performance the performance of the regionalization approaches is shown in fig 6 along with the upper and lower performance limits i e local calibration and benchmark series the calibration function using both kge and rnse as objective function led to robust runoff simulations as seen by equivalent kge values for the local calibration and regionalization by spatial and attributes proximity but median rnse increase of 0 09 0 04 and 0 03 respectively when compared to using only kge as the objective function this highlights the potential of multi objective functions for the calibration of hydrological models when specific goals are well established in this case the rnse was chosen to improve simulations under low flow conditions and all further discussion considers only the outcome from using kge rnse as the objective function among the regionalization approaches donating parameter sets by spatial proximity resulted in median kge and rnse of 0 58 and 0 56 respectively which are 0 02 and 0 11 higher than median scores obtained when donating parameter sets by attributes proximity the substantially higher rnse obtained by spatial proximity regionalization is an important matter for the cerrado region this is due to the importance of adequately predicting water availability under low flow conditions for sustainable socio economic development in many regions the lower performance achieved by donors with attributes proximity may come from giving all catchment attributes the same weight when computing their physical similarity despite physically similar catchments showing close ranges for the attributes it is unclear which attributes are more meaningful and should be given more weight there is significant heterogeneity of the land surface conditions soil profiles and space time variability of climatic forcing which is lumped for the catchments and makes it difficult to unveil such relationships guo et al 2021 sivapalan 2003 as for the regionalization based on spatial proximity other aspects also deserve attention which is the number of catchments used as donors and their distance to target catchments in this study we used the threshold of kge 0 70 and only 5 donor catchments despite following the recommendations by qi et al 2020 better results may be found using different numbers of donor catchments either based on the maximum allowed distance or different kge thresholds the trade off of using higher kge thresholds is that a lesser number of donor catchments will be available or only in longer distances resulting in lower performance for predictions in ungauged catchments or even parameter discontinuity for some regions the kge threshold of 0 70 seemed reasonable compared to the results obtained by qi et al 2020 this threshold resulted in the availability of 82 n 339 of the stations as donor catchments and the average distance between the donor and target catchment of 71 km median 60 km max 359 km only the regionalization approach based on spatial proximity was used for further analysis since it performed better than the regionalization based on attributes proximity qi et al 2020 showed in a large scale study that regionalization by spatial proximity is generally better than regionalization by physical similarity however this may not be the case for every region arsenault and brissette 2016 guo et al 2021 in comparison to the upper limit the local calibration kge 0 70 the spatial proximity presented a decrease in median kge score of 0 12 and approximately 13 of the catchments presented substantially low kge scores 0 as opposed to 6 when local calibration is used in comparison to the lower limit the benchmark series kge 0 34 donating parameters by spatial proximity showed an increase in median kge score of 0 24 fig 7 a and b presents the scatterplots comparing kge scores obtained for the catchments using parameter sets donated by spatial proximity to scores obtained using the local calibration and benchmark series although the local calibration can be considered an upper limit the regionalization based on spatial proximity showed robustness with improvements in approximately 34 of the catchments this is because neighbor catchments can be hydrologically similar and therefore calibrate equivalent parameter sets ensembles obtained with multiple equivalent parameter sets can function as multiple calibration runs and be advantageous for the runoff predictions fig 7c e 7d show maps of the kge scores obtained for the catchments using local calibration and parameter sets donated by spatial proximity respectively the performance at ungauged catchments deteriorated more in the eastern and northern regions of the cerrado which matches with regions showing higher aridity indexes fig 4 a review study by guo et al 2021 highlights that many authors have also found lower performance scores for regionalization methods when aridity increases for example the global regionalization studies by beck et al 2020 and qi et al 2020 show lower performance for validation catchments in regions with low humidity index inverse of aridity index and in arid regions respectively 3 3 runoff time series in ottobasins the parameter sets from donor catchments were used to estimate runoff time series for all 4531 level 5 ottobasins median size 185 km2 for the period from 2003 to 2014 12 years it is important to highlight that the threshold of kge 0 70 resulted in distances between donor catchments and ottobasins similar to observed during the cross validation mean 97 km median 90 km max 410 km this means that the threshold allowed several donor catchments with similar distances to replicate the experimental conditions of the cross validation for a brief overview of the data set generated for the ottobasins two important hydrological signatures were computed the long term mean runoff termed here mean runoff q fig 8 b and minimum runoff with an exceedance probability of 95 q95 fig 8d q provides us some information on the water balance and availability in the catchments or ottobasins while q95 is the reference used by the brazilian national water agency for granting water withdrawal permits maximum allowed 70 of q95 there is a good spatial agreement between hydrological signatures computed for observed and simulated runoff series fig 8 overall there is remarkably higher water availability both represented by q and q95 at the transition between the cerrado and the amazon biome western cerrado where the tropical climate registers higher rainfall rates fig 4 there is also considerable water availability in southern cerrado which is closer to regions with a temperate climate the eastern and northern cerrado are marked by lower water availability due to their proximity to the transition to semiarid regions the correlation between the catchment attributes and hydrological signatures reinforces some already known relationships and unveils other aspects regarding the hydrological processes for example precipitation correlates to higher water availability correlation to q ρ 0 78 and q95 ρ 0 42 whereas higher aridity correlates with lower water availability q ρ 0 75 and q95 ρ 0 41 the mean slope shows a negative correlation with q95 ρ 0 20 which is likely due to better drainage and or increased direct runoff in the wet season the soil characteristics presented a lower correlation with the hydrological signatures in contrast lower water availability q is seen in watersheds with higher forest fractions q ρ 0 50 in comparison to grasslands and crops forests help intercept rainfall and increase perspiration which can reduce runoff in relation to base flow the effect of forests is context dependent although they can improve infiltration increasing recharge and therefore the base flow during the dry season the increase in perspiration for water limited regions may aggravate the water availability issue lele 2009 in this study higher forest fractions also negatively correlate to q95 ρ 0 32 the overall negative correlation between forest fractions and these hydrological signatures can also be attributed to higher forest fractions being seen in the northern cerrado where the climate is more arid fig 4 it may also be the case that crop first expanded over forested areas in the regions with higher water availability i e higher chances of success for the agribusiness activity thus the effects of increasing or decreasing forest fractions would be better assessed along with the trends in base flow for these areas these results are important and practical giving water managers more confidence in water resource planning and management for instance let us consider that we were able to compile only 411 observed streamflow series for the cerrado median catchment area 4280 km2 thus hydrological signatures are generally obtained for large catchments and water availability can only be assumed for smaller river segments under high uncertainty furthermore many areas do not fall within larger catchments and are completely unmonitored fig 3 the simulated runoff series data set comprises 4531 ottobasins median area 185 km2 of which roughly 50 represent independent river segments and not interbasins despite the cross validation being used to approximate the regionalization performance the average area of ottobasins is generally lower than that of the gauged basins thus there may exist biases in predicting hydrological responses for ottobasins which are difficult to measure in conclusion despite imperfect the application and assessment of regionalization techniques provide us with valuable information at an expected performance cross validation finally the data set for the ottobasins is also provided with some uncertainty quantification i e the prediction intervals consisting of quantiles 5 and 95 derived from the hydrographs simulated using donated parameters 4 conclusions hydrological models calibrated considering both the kling gupta efficiency kge and relative nash sutcliffe efficiency rnse in a multi objective function resulted in better performance for the regionalization as opposed to using the kge in an objective function alone the multi objective function resulted in a median rnse increase of 0 10 for the local calibration and 0 03 and 0 04 for regionalization by attributes proximity and spatial proximity respectively while maintaining equivalent kge scores the regionalization by spatial proximity outperformed that by attributes proximity physical similarity especially concerning rnse scores spatial proximity led to kge and rnse of 0 58 and 0 56 while attributes proximity led to kge and rnse of 0 56 and 0 45 respectively when compared to an upper performance limit which is the performance achieved with local calibration the regionalization by spatial proximity showed a decrease in median kge by 0 12 but showed robustness and was able to improve the performance on approximately 34 of the catchments concerning the benchmark series mean monthly streamflow from calibration period the regionalization by spatial proximity showed an increase in median kge by 0 24 funding this study was financed in part by the coordenação de aperfeiçoamento de pessoal de nível superior capes in english coordination of improvement of higher education personnel finance code 001 and by the conselho nacional de desenvolvimento científico e tecnológico cnpq in english national council for scientific and technological development grant number 142273 2019 8 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financed in part by the coordenação de aperfeiçoamento de pessoal de nível superior capes in english coordination of improvement of higher education personnel finance code 001 and by the conselho nacional de desenvolvimento científico e tecnológico cnpq in english national council for scientific and technological development grant number 142273 2019 8 
25644,a simulation optimization framework requires a substantial number of model simulations which are computationally intensive and may be impractical when the model simulations are extremely time consuming this paper presents an improved hadoop based cloud framework to alleviate the computational burden of optimization the framework parallelizes conventional sequential model based optimization techniques by concurrently orchestrating multiple model computations within hadoop mapreduce it guarantees the reliability of simulation optimization tasks by handling node failures without affecting the ongoing simulation a case study using bayesian optimization to calibrate a swat model achieved a speedup of nearly 55 58 when using 100 cores demonstrating the efficiency of parallelizing the bayesian optimization algorithm on the hadoop based cloud experiments in which computing nodes were dynamically increased or decreased demonstrated that the framework can automatically rebalance the workload across the remaining nodes the framework is readily adaptable to other complex model applications that perform sequential model based optimizations or large scale simulations keywords hadoop based cloud sequential model parallel computing partial failure simulation optimization swat 1 introduction computer models that simulate physical processes are increasingly contributing to the understanding discovery and design of environmental systems models of hydrologic processes have been increasing in complexity and scope however efforts to include high resolution input data parameters and functions increase the computational resources required yang et al 2018 this requirement is even greater when common modeling routines such as calibration and uncertainty analysis are involved yalew et al 2013 such modeling routines may require thousands of model evaluations or more adding significantly to the computational burden for example calibration and uncertainty analysis of a complex over parameterized environmental model such as the soil and water assessment tool swat requires thousands of simulation runs and multiple calibration iterations therefore the effective use of computationally expensive simulations remains a challenge for many applications that involve environmental modeling khu et al 2004 one method of optimization based calibration is to adjust model parameters in an attempt to minimize differences between observed data and the corresponding model outputs another method is to conduct uncertainty based calibration that repeatedly samples model parameter configurations to develop a calibrated probability distribution for the parameters both methods have limited applicability because of their heavy computational demands for such computationally expensive models despite the availability of high speed computers it is seldom feasible to perform a large number of simulations to find the optimal solution mugunthan and shoemaker 2006 thus limiting the successful application of the models to address this challenge we usually resort to optimization algorithms to reduce the number of simulations optimization can be described as the systematic discovery of the best available input values of some objective function to be maximized or minimized given a defined domain under a set of constraints in practice simulation optimization frameworks that couple the model simulations with the optimization algorithm still entail a large number of model simulations which are computationally intensive and may be impractical when the model simulations are extremely time consuming many optimization algorithms have been presented in the literature tayfur 2017 for instance duan et al 1992 proposed shuffled complex evolution sce ua to calibrate the conceptual rainfall runoff model abbaspour et al 2004 proposed sequential uncertainty fitting sufi 2 to calibrate complex swat models and recently sufi 2 was integrated into the commonly used swat cup tool tolson and shoemaker 2007 proposed the dynamically dimensioned search dds method to calibrate complex and high dimensional hydrologic and watershed models vrugt et al 2008 proposed a markov chain monte carlo mcmc sampler based differential evolution adaptive metropolis dream algorithm han and zheng 2016 proposed the multiple response bayesian calibration mrbc framework which can quantify input uncertainty and parameter uncertainty during calibration given the inherent multi objective nature of watershed model calibration gupta et al 1998 madsen 2000 kollat and reed 2006 numerous multi objective algorithms moea have also been proposed and used for watershed model calibration and uncertainty quantification some notable methods are the non dominated sorting genetic algorithm ii nsga ii bekele and nicklow 2007 confesor and whittaker 2007 ercan and goodall 2016 parasol van griensven and meixner 2007 the borg multiobjective evolutionary algorithm borg moea hadka and reed 2013 2015 and pareto archived dynamically dimensioned search pa dds asadzadeh and tolson 2013 however most optimization algorithms cannot dramatically reduce the computational cost and in some cases they increase execution time such costs are further exacerbated by automated calibration which can require thousands of model evaluations zamani et al 2021 fortunately the advent of parallel computing technology provides a promising way to solve the complex computationally expensive problems in hydrological modeling ahmadisharaf et al 2019 the purpose of the parallel computing technology is to decompose the modeling routines into multiple subtasks that can be executed simultaneously to reduce the elapsed computation time therefore the use of parallel computing in the form of distributed gridded and cloud computing has become increasingly prevalent gorgan et al 2012 standalone machine and cluster gridded networks are two main parallel computing categories defined according to the level at which the hardware supports parallelism zhang et al 2016 the former parallelization technique distributes tasks to different processors within a standalone machine typical examples include swat calibration and uncertainty procedures swat cup rouholahnejad et al 2012 and dream joseph and guillaume 2013 which both greatly improve the efficiency of model calibration however the scalability of these systems is poor because of the limited performance of the standalone machine in some cases it is not feasible for these systems to automatically calibrate and analyze large scale watershed models with complex physical domains and multiple water resource issues such as water quality droughts and floods abbaspour et al 2007 gupta et al 2012 the latter parallelization technique executes tasks on a cluster or gridded network such networks are easy to scale up by adding more machines to the computing cluster or gridded network and they therefore have a greater potential for speeding up model simulations notable examples include a generic tool for enabling the swat model application to be executed on a computer grid yalew et al 2013 and a python based parallel computing package pp swat zhang et al 2013 both of these have achieved significant speedups depending on model complexity however because these methods usually use a shared file system there is a potential problem when cluster gridded nodes need to access larger data volumes because the network bandwidth becomes a bottleneck white 2009 because most parallel computing paradigms evolved from conventional supercomputing resources their application may be limited by the time and monetary cost required to access such resources in addition to the technical knowledge and skills required to achieve high performance cloud computing technology has some advantages over traditional technologies including excellent scalability elasticity and its elegant and automatic handling of partial failure for this reason cloud computing may be a viable solution for performing complex model calibration and simulations because it can be used to parallelize model simulations and handle large post simulation results notable examples include a calibration system for swat based on the microsoft azure and dds method humphrey et al 2012 which obtained a significant speedup for swat calibration when a large number of cloud cores were used subsequently ercan et al 2014 further tested this system across a range of typical watershed sizes and simulation durations hu et al 2015a 2015b used hadoop a common open source software framework for creating a cloud computing environment on clusters of commodity hardware with large amounts of data storage to address computational intensity and model accessibility for a coupled human and natural system zhang et al 2016 moved swat model calibration and uncertainty analysis to a hadoop based cloud to achieve a speedup of 21 7 26 6 depending on model complexity recently zamani et al 2021 presented a cloud based calibration and uncertainty analysis system called lcc swat in which two optimization techniques sufi 2 and dds were implemented experimental results on swat models of different complexities showed that lcc swat could reduce execution time significantly these previous studies together demonstrated the applicability of cloud computing technology to complex model simulations and optimizations however many prior studies failed to adequately address two aspects of parallel model simulation optimization i the compatibility of implementations of the optimization algorithm with the cloud platform and ii the impact of node failures on model simulation optimization in general in the optimization algorithms commonly used in the above studies parallelism relies on the property that the next candidate points suggested are mutually independent therefore these techniques cannot be extended to standard sequential model based optimization algorithms such as bayesian optimization bo in which the next candidate points suggested depend on all the previous candidate points bo is an important technique in computer science that has been reported to achieve state of the art performance in many domains and is critical to the practical global optimization of time consuming complex models shahriari et al 2016 therefore we chose bo as the target algorithm to investigate the feasibility of parallelizing a conventional sequential model based optimization algorithm on cloud computing technology in addition because few studies have considered the problem of node failure in the optimization process we adopted hadoop a well established cloud platform to explore the impact of node failures on model simulation optimization the main objective of this study was to investigate the parallelization of the sequential model based optimization algorithm on the hadoop based cloud another objective was to investigate the capability of the hadoop based cloud to handle partial node failures during the simulation optimization process because the swat model is widely used we took the calibration of swat as an example to conduct the above exploration the main contribution of our work is the reliable parallelization of standard sequential model based optimization algorithms on cloud computing technology to achieve state of the art performance in complex model simulation optimization the paper is organized as follows in section 2 the hadoop based cloud framework for parallelizing traditional sequential model based optimization algorithms is introduced its two improvements and its major components including the physical swat model sequential model based optimization algorithm and hadoop based cloud are described in section 3 we apply the approach to calibrate the swat model in a case study and we examine the effectiveness and efficiency of the method in section 4 and 5 we present and discuss the optimization results respectively finally we provide some conclusions in section 6 2 hadoop based cloud framework for parallelizing traditional sequential model based optimization algorithms the key principle of using the hadoop based cloud framework to parallelize traditional sequential model based optimization algorithms is to group multiple simulations into multiple sequential mapreduce jobs on hadoop each mapreduce job involves multiple tasks that can be executed in parallel in the mapreduce framework in this manner multiple simulations within a sequential optimization algorithm such as bo can be orchestrated to execute in parallel the rationale for this approach is that the cost of evaluating the acquisition function of bo is small or even negligible compared with the cost of evaluating the complex model therefore the sequentially suggested candidate model simulations can be executed in a parallel manner in addition the efficiency of a single simulation is further improved by invoking a single model computation within a mapper container only without using the reducer procedure this technique avoids the need for the shuffle procedure a well known performance bottleneck of the mapreduce framework the proposed framework was implemented mainly in python in a linux environment by coupling the swat model with the bo library scikit optimize on the java supported hadoop based cloud scikit optimize is a simple and efficient library for minimizing the use of expensive and noisy black box functions which implements several methods for sequential model based optimization the library is built on top of numpy scipy and scikit learn and is intended to be accessible and easy to use in many optimization contexts similarly to other optimization libraries scikit optimize uses the suggestion evaluation observation step to locate the optima in our framework the computationally expensive swat simulations performed during the iteration of the suggestion evaluation observation loop are executed using the hadoop based cloud tornado and springboot were used to construct two web servers for data exchange via http requests and responses between the swat model and bo the former hosts the scikit optimize library and the latter serves as a transformer responsible for first receiving parameter sets pss from the tornado framework then parsing pss to generate hadoop mapreduce jobs and finally submitting these jobs to the hadoop cluster for model execution the fortran based swat model was first compiled in a linux environment and then wrapped as a mapreduce task within a hadoop cluster for model evaluation the methodology of the framework involves three steps model setup to establish the goal of the optimization setup of the hadoop based cloud to decompose a large number of model simulations and distribute them to multiple computing nodes and setup of the sequential optimization algorithm to guide and locate optima two improvements were made parallelizing the traditional sequential model based algorithm bo and simplifying the coupling between the complex model and the mapreduce framework to avoid using the reducer procedure the main components of the framework are elaborated in the following subsections the core tasks are explained below and illustrated in fig 1 i model setup first a complex model needs to be constructed to serve as the target of the optimization as an example we describe the construction process of the swat model the swat model requires explicit information regarding weather topography soil properties vegetation and land management practices to simulate processes such as surface and subsurface stream flow sediment transport nutrient cycling and crop growth franco et al 2020 we used arcswat to set up the swat model because it provides a user friendly interface and presents the results as intuitive and informative maps during the setup process swat is fed by multiple data sources including digital elevation models dems streamlines soil data and land use data after the swat model setup is completed the swat project configuration files and the swat executable program compiled for the linux operating system are compressed and stored as a distributedcache the next step of initialization comprises the following tasks first eight hydrogeological parameters are initially chosen according to a previous study ghaith and li 2020 the parameter ranges are carefully determined according to available field data second the eight parameters are sampled n times using the latin hypercube sampling method mckay et al 1979 where n is the total number of processors employed to perform model simulations in the hadoop based cloud each sample drawn from the eight dimensional space is an eight vector and the n samples represent complete possible combinations of the eight parameters which are used as the inputs to the n simulations of the swat model n plain text files are then constructed from these n input samples and uploaded to the hadoop based cloud in the form of a hadoop distributed file system hdfs file finally an initialization job is created and submitted to the hadoop based cloud ii hadoop based cloud on receiving the initialization job the hadoop based cloud takes over to locate the positions of n sample files typically the compute nodes and storage nodes are the same that is the mapreduce task and the hdfs are executed on the same set of nodes this design allows the framework to effectively schedule tasks on nodes where the data are already present resulting in very high aggregate bandwidth across the cluster subsequently n mapreduce tasks are created and distributed to the n nodes where the n files are located the sample file is parsed on the node to update the eight parameters and evaluate the model at the same time the swat project configuration files and the swat executable program are retrieved from the distributedcache and uncompressed to a local directory therefore the swat project configuration files can be edited and updated according to the content of the sample file the swat executable program is then invoked to execute at the local path and keep the user informed of the status of the model during execution because of the complexity of the calculation process both the raw and parsed simulation results are stored the raw results are directly stored in the hdfs and the parsed results are exported to a persistent storage database such as hbase or postgresql hbase was employed in our study because of its compatibility with the mapreduce computing framework thus keeping the different technology solutions used within the same hadoop ecosystem to provide training data for the optimization algorithm the parsed simulation results are further analyzed with respect to the objective function required by the optimization algorithm and then converted to json format and sent to the optimization algorithm using the http protocol thus there are n responses to the optimization algorithm from the hadoop based cloud improvement 1 simplifying the traditional mapreduce programming paradigm without using the reducer procedure the main idea of mapreduce is the divide and conquer principle whereby a large problem is divided into many small problems which are then executed on each node in the cluster after the map process is complete it is followed by an intricate and intractable reduce process which gathers shuffles sorts and reduces the results of all the map phases notably a reduce process cannot complete until all map processes are complete so that it can fetch all the data that it needs to process this is ideal for processing very large amounts of data e g multi terabyte datasets in parallel however it is not compatible with complex models because it greatly exacerbates the complexity of program design there is no need to gather and analyze the model results by using the reduce process because these tasks are the responsibility of the optimization algorithm therefore the mapreduce programming paradigm needs to be improved to deal with complex models instead of using the standard mapreduce programming paradigm in which both map and reduce processes are implemented single simulation efficiency is improved by invoking a single model computation in the mapper container only without using the reducer procedure this improvement has two advantages it avoids the potential performance bottleneck of the shuffle procedure a well known problem that may occur in the reduce process and greatly simplifies the program design process iii sequential optimization algorithm on receiving the first n responses from the hadoop based cloud the sequential model based optimization algorithm a form of bo begins to initialize itself first the streamflow outputs of the n representative swat simulations are used to calculate the bo objective function which was the nash sutcliffe efficiency coefficient nse in this study to be minimized between the simulated and measured streamflow histories the nse objective function is used as the response variable of the surrogate model to the eight input variables second the bo object is instantiated the n pairs of eight vector inputs and the corresponding objective function are used to instantiate a bo object using random exploration from n responses from hadoop this exploration can help by diversifying the exploration space as shown in fig 1 the major routine following the suggestion evaluation observation loop is then launched after the construction of the bo object three steps are executed in a loop first bo sequentially creates n new suggestions from the acquisition function of bo one suggestion represents a potential eight vector input balanced between exploration and exploitation for locating the next potential optima second n suggested eight vector inputs are packaged and sent to the hadoop based cloud for feeding into the swat model to evaluate the objective function the combination of the eight vector inputs and the responding objective function values are then treated as n new observations to update the bo object the loop continues until a specified criterion is satisfied for example when the maximum number of iterations has been performed or a specific objective function value has been obtained finally the eight optimal parameters are determined the above suggestion evaluation observation loop forms the core of the bo based calibration framework as the loop continues the bo balances its needs for exploration and exploitation taking into account its knowledge of the target function at each step a gaussian process gp is fitted to the known observations the points previously explored and the posterior distribution combined with an exploration strategy such as the upper confidence bound ucb or expected improvement ei is used to determine the next suggestion that should be explored the final loop results in the optimal values of the eight input parameters see fig 2 improvement 2 parallelizing the traditional sequential optimization algorithm on the hadoop based cloud the sequential model based optimization method is an application of bayesian reasoning which is also a form of bo that updates the probability model sequentially every evaluation of the objective function with a set of values updates the model so that the model will eventually represent the true objective function the standard sequential model based optimization algorithm takes into account the previous results to try more promising values but apparently cannot be parallelized to improve efficiency however it differs from random search or grid search in that it constructs a probability model of the objective function also called a surrogate function which is much easier to optimize than the actual objective function this function can be evaluated and optimized very quickly to identify the next evaluation point in summary the hadoop based cloud can enable the parallelization of multiple simulations and sequential optimization can identify the next candidate point very quickly combining these two points we propose an improvement for parallelizing the traditional sequential model based optimization algorithm on the hadoop based cloud by an elegant orchestration when the bo object receives the first n pairs of eight vector inputs and the corresponding objective function values it instantiates itself uses those n pairs as observations to update the probability model of the objective function suggests n new eight vector inputs and sends them to the hadoop based cloud in the next iteration when the bo object receives the second n pairs of eight vector inputs and the corresponding objective function values from hadoop it updates the probability model of the objective function suggests n eight vector inputs and sends them to the hadoop based cloud the loop continues until a specified criterion is satisfied for example when the maximum number of iterations has been performed or a specific objective function value has been obtained this improvement differs from the standard sequential optimization method as shown in fig 3 because it suggests n eight vector inputs at a time rather than one pair at a time such an orchestration is beneficial for taking advantage of the hadoop based cloud because the n eight vector inputs are identified at approximately the same time allowing n swat models to be executed concurrently 2 1 swat model the swat hydrological model was adopted in this study to simulate streamflow hydrographs this is a semi distributed continuous time watershed model that is capable of running in daily and sub daily time steps gassman et al 2007 originally developed by arnold et al 1998 to understand the impact of management scenarios and non point source pollution on water supplies at a watershed scale it has been used widely in a variety of watershed studies including simulations of both water quantity and water quality li et al 2010 lee et al 2010 liu et al 2013 the watershed is first divided into several sub basins depending on the outlets which are usually defined by subdividing the dem and performing manual adjustment each sub basin is further divided into hydrological response units hrus with specific attributes by setting different thresholds for soil land use and slope classes the swat model uses the concept of hru to represent variability within the sub basins of a watershed the hrus are unique representations of land cover soil and management characteristics within a single sub basin and are used for water balance calculations in the model the hrus are not spatially contiguous therefore they are often composed of many disjoint parcels of land in a watershed therefore swat takes the spatial heterogeneity of the watershed into account by dividing the watershed into different sub basins that belong to the same range of hydrological factors this makes parameter calibration very difficult 2 2 hadoop based cloud hadoop is an open source cloud computing framework that implements the google mapreduce algorithm lam 2010 white 2009 it consists of two main components the hdfs and the distributed computation framework dcf which is a framework that implements the mapreduce algorithm hdfs is a highly fault tolerant distributed file system which divides large files into small data blocks filesplit and distributes these blocks among the datanodes of the cluster as a result it is able to read and write data in parallel across a large number of machines and can achieve much higher throughput than traditional technologies dcf uses the mapreduce algorithm to achieve parallel processing across the cluster it helps by moving the computation part of the algorithm to the same location as the data the mapreduce algorithm is designed to compute with large volumes of data by splitting the data into blocks and processing different blocks in parallel mapreduce comprises two major phases the map phase and the reduce phase in the map phase mapreduce splits data into elements inputsplit and feeds each data element to the mappers each mapper maps input key value pairs to intermediate key value pairs that is the mapper is used to transform the input pairs to output pairs which the reducer can further process in the reduce phase the input key value pairs i e the key value pairs output by the mapper with the same key are aggregated to form a new small set of key value pairs by implementing these two interfaces users can easily establish robust scalable and distributed programs 2 3 bayesian optimization bo is an efficient framework for the global optimization of expensive objective functions which is especially useful for optimizing black box functions whose exact form is unknown auer 2002 the core of bo employs a probabilistic model to capture the unknown function this differentiates it from commonly used random search or grid search algorithms rather than just selecting from a grid uninformed by past objective function evaluations bayesian methods take into account the previous results to try more promising values the probability model is much easier to optimize than the actual objective function and therefore the optimization process is much faster after each evaluation of the objective function the algorithm updates the probability model a gp specified by its mean and covariance functions is widely used as such a probabilistic model the next stage of bo uses surrogate utility functions known as acquisition functions these surrogate functions are constructed to trade off the need to explore regions where the epistemic uncertainty about the objective function is high and the need to exploit regions where the predicted values are high these acquisition functions can be optimized to extract the next candidate point for evaluating the objective function in bo as in other optimization algorithms the objective function should be selected first the objective function is the statistic that is maximized or minimized in the optimization process the nse nash and sutcliffe 1970 which represents the percentage of model fitting variance to total variance is one of the most commonly used model evaluation indicators bennett et al 2013 bae and seo 2018 therefore the nse was selected as the model evaluation index in this study it is defined as follows 1 n s e 1 i 1 n o i s i 2 i 1 n o i o 2 where oi is the observed value and si is the simulated value when nse is 1 there is a perfect fit between the simulated values and the observed values 3 case study 3 1 swat model setup the yiwu river basin 29 00ʹ n 29 40ʹ n 119 50ʹ e 120 40ʹ e is mainly located in yiwu city zhejiang province as shown in fig 4 the main stream is the yiwu river and the nanjiang river is its main tributary the drainage area is 2050 47 km2 the average annual precipitation in the basin is 1447 mm april september is the wet season and november december is the dry season the precipitation accounts for 59 and 8 of the annual precipitation in the wet season and dry season respectively the basic data required by the swat model include a dem a river network and data on land use and cover soil type and its physical and chemical properties plants and plant growth fertilizer hydrology and water quality meteorology livestock husbandry and population emissions neitsch et al 2004 lai et al 2020 the geographic coordinate system of all spatial data in this model is gcs wgs 1984 and the projection coordinate system adopts albers projection with the central meridian 105 e and two standard latitudes of 25 n and 47 n the elevation data are taken from the geospatial data cloud with 30 m accuracy the land use data are obtained by processing remote sensing image data from landsat for 2018 with 30 m precision from the geospatial data cloud the soil data come from the 1 1 000 000 soil data provided by the china soil database which adopts the world soil database hwsd fao90 classification standard using the dem and the imported real river network the yiwu river basin is finally divided into 18 sub basins and 319 hrus by reclassification of the dem soil data and land use data data from two meteorological stations 89 181 and 89 182 were obtained from cmads the china meteorological assimilation driving datasets for the swat model meng et al 2018 including data on temperature precipitation humidity wind speed and solar radiation observed monthly streamflows for the watershed outlets doyang and fotang were acquired from the chinese hydrological data yearbook for the years 2010 2017 the first 15 of the 27 parameters recommended in the literature ghaith and li 2020 were selected as the target for streamflow calibration these parameters are cn2 ch w2 ch l2 ch k2 sol awc sol bd alpha bf esco gw delay sol k ch d tlaps timp gwqmn and smfmx their names upper and lower limits and meanings can be found in neitsch et al 2005 because ch w2 ch l2 ch k2 and ch d describe channel properties they can be measured directly through the stream and watershed survey and were therefore not considered to be calibration parameters because timp gwqmn and smfmx are less sensitive compared with other 12 parameters and were also not considered to be calibration parameters therefore the remaining eight parameters cn2 sol awc sol bd alpha bf esco gw delay sol k and tlaps became the final target for calibration 3 2 hadoop based cluster setup the model evaluations numbering several hundred were distributed to a hadoop cluster comprising seven nodes each equipped with an intel 8 core i7 3770 processor and 32 gb ram and another four nodes each equipped with an intel 32 core xeon r silver 421 processor and 96 gb ram these physical machines are depicted in fig 5 after establishing the main structure of the hadoop cluster many other installation and configuration tasks are needed to run hadoop these include the installation of the operating system centos 7 java environment jdk 1 8 and hadoop software version 2 7 5 as well as the creation of a hadoop user and configuration of ssh and hadoop a detailed description of these steps is beyond the scope of this paper but the reader can refer to white 2009 for more information to solve the problem of the single point of failure which is encountered in a hadoop cluster two high availability improvements have been made the first is the inclusion of the resourcemanager which is responsible for tracking the resources in a cluster and scheduling applications e g mapreduce jobs prior to hadoop 2 4 the resourcemanager was the single point of failure in a yarn cluster the high availability feature adds redundancy in the form of an active standby resourcemanager pair to remove this single point of failure yarn proposes the separation of the two major functions of the jobtracker resource management and job scheduling and monitoring the second high availability improvement is the hdfs oriani and garcia 2012 which improves the availability of hdfs by obviating failure on the namenode machine which makes the entire hdfs cluster unavailable the hdfs feature provides the option of running two redundant namenodes in the same cluster in an active standby configuration with a hot standby the procedure for performing swat model evaluations on a hadoop cluster is to call the fortran based swat model from the java based mapreduce framework because the fortran code of the swat model is compiled to an executable program for the linux environment it can easily be executed in the mapreduce framework finally http requests are used to transfer variables bidirectionally between swat and bo 3 3 implementation of bayesian optimization for the swat model on the improved hadoop based cloud because of its flexibility tractability analyzability and good substitutability for linear and nonlinear relationships gp was chosen as the probabilistic surrogate model in our bo algorithm in practice it is very difficult to specify the prior information for the mean function of the gp therefore for the sake of simplicity we made the usual assumption that the prior mean function is 0 which has little effect on the accuracy of the posterior distribution the covariance specifies the smoothing line and amplitude of the unknown objective function and represents the similarity between the two calculation points for simplicity we adopted the matérn covariance function with a smoothing parameter value of 2 5 and a scale parameter value of 1 another component of bo is the acquisition function ei was chosen because it incorporates both the improvement probability and the improvement amount the main implementation process for the bayesian method was as follows 1 tornado initializes the parameter space of the swat model randomly implements various sampling iterations uses these samples to create n batch model pss and sends the n pss to springboot 2 springboot parses the n pss to create n batch model configuration files and distributes these configuration files to the hadoop cluster in hdfs format 3 the hadoop cluster performs the model calculation concurrently on the node where these files are located and returns the nse objective function value to tornado in an http response when the model execution has finished 4 tornado sequentially collects n nse objective function values until all n model calculations on the hadoop cluster are finished 5 tornado determines whether the calibration requirement is satisfied according to one of the n nse values returned when the calibration requirement is not satisfied the bo s built in acquisition function automatically and sequentially recommends the next set of n parameters to continue the iterative calculation otherwise the cluster operation is terminated 6 tornado returns the calibration result when the calibration criterion is satisfied 4 results 4 1 improved parallelized optimization algorithm for hadoop based cloud framework to evaluate the speed improvement houstis et al 1997 resulting from parallelizing the traditional sequential model based bo algorithm on the hadoop based cloud two experiments were conducted to perform swat calibration with and without the reduce procedure overall 1200 swat calculations were performed with each experiment involving up to 600 iterations to ensure the convergence of bo fig 6 plots the speedup with different numbers of cores ranging from 10 to 100 although the speedup was not proportional to the number of allocated cores a speedup of about 55 58 was still achieved when 100 cores were allocated this demonstrated the benefit of parallelizing the sequential model based optimization algorithm on the hadoop based cloud the actual speedup of both the map task and the reduce task indicated that as the number of cores increased the speedup gradually deviated from the ideal linear speedup this implies that performance may deteriorate when more cores are used 4 2 simplified coupling between complex model and mapreduce framework without using reducer procedure traditionally a typical hadoop mapreduce application implements both map and reduce tasks at the same time to further improve the efficiency of swat model evaluation the speedups achieved with and without the reducer procedure were compared contrary to our expectations the speedup of the map task slightly exceeded the speedup of the reduce task however this improvement was not sufficiently pronounced as shown in fig 6 when more than 30 cores were used the map task began to show a noticeable performance improvement a speedup gain of between 2 and 4 over the reduce task notwithstanding this such a slight improvement both facilitates the coupling between the complex model and the mapreduce framework and circumvents the shuffle procedure which is a notorious performance bottleneck of the mapreduce framework 4 3 effect of the suggestion procedure of the optimization algorithm on parallelization efficiency as the number of iterations increases the process of suggesting candidate points in the optimization algorithm may take more time to quantify the time loss caused by the suggestion process and eliminate its impact on the speedup which may increase the total time for suggesting n batch model pss two experiments to calibrate the swat model were conducted fig 7 plots the suggestion time taken by bo when calibrating eight and 15 parameters the first experiment calibration of eight parameters showed a steady trend with a suggestion time of less than 500 ms which is negligible in comparison with the execution time of the swat model in contrast when calibrating 15 parameters the suggestion time gradually increased as the number of iterations increased in particular when the number of iterations was 600 the suggestion time reached as high as 40 s these experiments highlighted a potentially serious shortcoming of the sequential optimization algorithm 4 4 handling of partial failures on the improved hadoop based cloud framework failures frequently occur during model execution particularly when a large number of iterations are performed to demonstrate the capability of the framework to handle failures during the optimization procedure experiments were conducted to dynamically increase or decrease the number of computing nodes to eliminate the influence of the difference in computing node hardware on the experiment seven nodes with almost the same hardware intel 8 core i7 3770 processor and 32 gb ram were used each node used seven cores for model execution and one core to support the operating system five experiments were designed of which the first and last were used as baselines and the other three were designed to test the resilience of the model to abnormal operation these three experiments involved shutting down one node 1 7 cores three nodes 3 7 cores and seven nodes 7 7 cores each experiment was repeated nine times to calculate the average so a total of 45 experiments were conducted fig 8 shows an inverse relation between the cost time and the number of active nodes the average execution time for one iteration increased from 198 s to 406 s when the number of nodes was reduced from seven to one and then decreased again to 198 s when all seven nodes became active this demonstrates that even if some nodes fail during the optimization procedure hadoop automatically rebalances the workload on the remaining nodes and the simulation continues without affecting the overall optimization process 4 5 case study calibration results to quantify the calibration effect of the model the monthly streamflow simulations were compared with the observed data at doyang and fotang hydrological stations the average nse values of the two stations both reached as high as 0 84 during the calibration period 2014 2015 and verification period 2016 2017 this indicates that the calibrated parameters captured the monthly runoff variation accurately and thus can be used for future simulations of monthly runoff however a detailed description of the results is not the main purpose of this study 5 discussion the improved hadoop based cloud can parallelize conventional sequential model based optimization algorithms simulation optimizations have always suffered from a high computational burden usually requiring substantial numbers of model simulations such costs are further exacerbated when the model simulation is extremely computationally intensive razavi et al 2010 in response to this numerious parallel optimization algorithms have been developed due to the fact that these algorithms have the ability to be implemented in a parallel manner however for sequential optimization algorithms such as bo because the next candidate point depends on previous ones parallelizing the algorithm seems impractical we have proposed a novel framework for parallelizing a sequential optimization algorithm on the hadoop based cloud because the time taken by the optimization algorithm s suggestion is negligible compared with that taken by the complex model calculation our method suggests multiple sets of candidate points sequentially instead of 1 ps at a time and then distributes the candidate points to the hadoop cluster for concurrent processing in this manner a sequential algorithm can be elegantly converted to execute in parallel the case study using bo to calibrating the swat model achieved a speedup of nearly 55 58 when using 100 cores demonstrating the feasibility of parallelizing the bo algorithm on the hadoop based cloud the improved hadoop based cloud can guarantee the reliability of simulation optimization tasks it handles node failures without affecting the ongoing simulation by redistributing the failed tasks to working hadoop cluster our framework adopts a modular architecture that separates the optimization algorithm from model calculations the latter are wrapped in hadoop so that complex models such as swat can be executed on independent nodes and a failed model calculation task can be recovered by spawning a new process to execute it possibly using an idle process that has completed its map task or reduce task hadoop satisfies the nonfunctional requirements of constructing well established system software such as fault tolerance load balancing and scalability sethia and karlapalem 2011 therefore it automatically rebalances the workload on remaining nodes and the simulation continues when partial failures occur which allows users to focus on their optimization without getting involved in ensuring fault tolerance furthermore hadoop facilitates dynamic addition of new nodes to an executing simulation because it is developed on top of hadoop our framework inherits hadoop s advantages making it a promising option with fault tolerance and failure recovery capability our framework is the first of its kind to parallelize sequential optimization techniques on cloud based architectures to reduce calibration execution time commonly used optimization techniques particularly population based algorithms such as nasg ii deb et al 2002 shuffled complex evolution metropolis scem ua vrugt et al 2003 dds tolson and shoemaker 2007 particle swarm optimization afshar et al 2011 and borg hadka and reed 2013 all of which are parallel techniques can use parallel computing resources because these techniques are inherently parallel however this is not the case for sequential model based optimization techniques whose next candidate point depends on the previous ones to date studies on extending parallel computing to sequential optimization algorithms have not been reported our study proposed a novel framework built on top of the hadoop based cloud to add parallelism to conventional sequential optimization algorithms to facilitate broader applications of the existing algorithms in addition few studies have addressed the possibility of node failures occurring during model simulation sethia and karlapalem 2011 our hadoop based framework solves this problem by automatically rebalancing the workload on remaining nodes and continuing the simulation which provides an elegant solution that enhances the reliability of the optimization effort moreover compared with traditional applications of hadoop which typically employ both map and reduce tasks at the same time hu et al 2015a 2015b zhang et al 2016 our framework is the first of its kind to use only map tasks to further improve the efficiency of the model simulation although no obvious improvement was observed only a speedup gain of 2 5 was achieved when more than 30 cores were used to clearly prove that the approach using only map tasks outperforms the traditional method this approach does greatly simplify the programming process of wrapping a complex model into mapreduce whch may provide an insight on addressing the compatibility of complex models with cloud platform simulation optimization framework methods that couple the model simulations with the optimization algorithm usually entail a large number of model simulations such frameworks may be problematic when single model simulation is computationally intensive an ideal framework would therefore separate the optimization algorithm from model simulation to both achieve high performance computing and ensure that the execution of the optimization algorithm is not affected by a failure of the model simulation our improved hadoop based cloud framework for complex model simulation optimization was indeed designed and implemented in such a manner the case study clearly demonstrated its ability to both efficiently decompose a large number of model simulations and effectively handle partial failure because it inherits the dynamic load balancing and failure recovery capabilities of hadoop this framework is particularly suitable for complex model optimization in addition it is a generic framework for solving diverse optimization problems and is not specifically intended for the calibration of hydrological models however there are two possible drawbacks associated with this framework the speedup performance of the framework may not be proportional to the number of allocated cores this drawback was also observed in other studies ercan et al 2014 zhang et al 2015 bacu et al 2017 zamani et al 2021 fig 6 indicates that as the number of allocated cores increases the speedup gradually deviates from the ideal speedup exhibiting a nonlinear scaling of execution time performance this implies that performance may deteriorate when more cores are used similar results have been reported by others for instance ercan et al 2014 showed with an experiment involving 256 cores that the use of 64 cores was the most desirable from the economic point of view one plausible reason for this deterioration is the expected network latency and i o operations required for communication between the nodes of the hadoop cluster zamani et al 2021 another reason is the unexpected network latency required to transfer variables between tornado and springboot the other possible drawback of this framework arises from the assumption that the time taken by the optimization algorithm s suggestion is negligible in comparison with that of model simulation this is not often the case fig 7 plots the difference in suggestion time of bo between different numbers of calibration parameters of the swat model when the number of iterations is 600 the suggestion time can reach 40 s which is nearly one third of the execution time of the swat model this may degrade the efficiency of parallelism this can be explained by the use of bo to calibrate the swat model in our case study bo uses the gp mockus et al 1978 shahriari et al 2016 metamodel to approximate the relationships between several explanatory variables typically the simulation model parameters and the variables affecting model inputs and one or more model response variables generally the computation time required for refitting a metamodel mostly nonlinearly increases with an increase in either the size of the variables known as the curse of dimensionality or the number of iterations therefore care should be taken to check the suggestion time of the optimization algorithm when the number of iterations is large 6 conclusions this paper presented an improved hadoop based cloud framework for alleviating the substantial computation burden of complex model simulation optimization the framework is the first of its kind to parallelize sequential optimization techniques on cloud based architectures and thereby reduce the execution time of model simulation and is the first to further simplify the coupling between the complex model and the mapreduce framework without using the reducer procedure the framework is also capable of guaranteeing the reliability of simulation optimization tasks by handling node failures without affecting the ongoing simulation the case study using bo to calibrate the swat model achieved a speedup of nearly 55 58 when using 100 cores demonstrating the feasibility of parallelizing the bo algorithm on the hadoop based cloud the experiments to dynamically increase and decrease computing nodes demonstrated that even if some nodes fail during the optimization procedure the framework can automatically rebalance the workload on the remaining nodes and the simulation can continue without affecting the overall optimization process the results clearly demonstrate the framework s ability to both reduce the execution time of model simulation and effectively handle partial failure the framework is particularly suitable for complex model optimization but it is also a generic framework for solving diverse optimization problems it may provide a new perspective for the parallelization of traditional sequential optimization algorithms and is expected to be an ideal method to alleviate the substantial computational burden of the optimization process first avaliable december 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to the funding support from the national natural science foundation of china 41925005 and the national key r d program of china 2019yfd0901105 
25644,a simulation optimization framework requires a substantial number of model simulations which are computationally intensive and may be impractical when the model simulations are extremely time consuming this paper presents an improved hadoop based cloud framework to alleviate the computational burden of optimization the framework parallelizes conventional sequential model based optimization techniques by concurrently orchestrating multiple model computations within hadoop mapreduce it guarantees the reliability of simulation optimization tasks by handling node failures without affecting the ongoing simulation a case study using bayesian optimization to calibrate a swat model achieved a speedup of nearly 55 58 when using 100 cores demonstrating the efficiency of parallelizing the bayesian optimization algorithm on the hadoop based cloud experiments in which computing nodes were dynamically increased or decreased demonstrated that the framework can automatically rebalance the workload across the remaining nodes the framework is readily adaptable to other complex model applications that perform sequential model based optimizations or large scale simulations keywords hadoop based cloud sequential model parallel computing partial failure simulation optimization swat 1 introduction computer models that simulate physical processes are increasingly contributing to the understanding discovery and design of environmental systems models of hydrologic processes have been increasing in complexity and scope however efforts to include high resolution input data parameters and functions increase the computational resources required yang et al 2018 this requirement is even greater when common modeling routines such as calibration and uncertainty analysis are involved yalew et al 2013 such modeling routines may require thousands of model evaluations or more adding significantly to the computational burden for example calibration and uncertainty analysis of a complex over parameterized environmental model such as the soil and water assessment tool swat requires thousands of simulation runs and multiple calibration iterations therefore the effective use of computationally expensive simulations remains a challenge for many applications that involve environmental modeling khu et al 2004 one method of optimization based calibration is to adjust model parameters in an attempt to minimize differences between observed data and the corresponding model outputs another method is to conduct uncertainty based calibration that repeatedly samples model parameter configurations to develop a calibrated probability distribution for the parameters both methods have limited applicability because of their heavy computational demands for such computationally expensive models despite the availability of high speed computers it is seldom feasible to perform a large number of simulations to find the optimal solution mugunthan and shoemaker 2006 thus limiting the successful application of the models to address this challenge we usually resort to optimization algorithms to reduce the number of simulations optimization can be described as the systematic discovery of the best available input values of some objective function to be maximized or minimized given a defined domain under a set of constraints in practice simulation optimization frameworks that couple the model simulations with the optimization algorithm still entail a large number of model simulations which are computationally intensive and may be impractical when the model simulations are extremely time consuming many optimization algorithms have been presented in the literature tayfur 2017 for instance duan et al 1992 proposed shuffled complex evolution sce ua to calibrate the conceptual rainfall runoff model abbaspour et al 2004 proposed sequential uncertainty fitting sufi 2 to calibrate complex swat models and recently sufi 2 was integrated into the commonly used swat cup tool tolson and shoemaker 2007 proposed the dynamically dimensioned search dds method to calibrate complex and high dimensional hydrologic and watershed models vrugt et al 2008 proposed a markov chain monte carlo mcmc sampler based differential evolution adaptive metropolis dream algorithm han and zheng 2016 proposed the multiple response bayesian calibration mrbc framework which can quantify input uncertainty and parameter uncertainty during calibration given the inherent multi objective nature of watershed model calibration gupta et al 1998 madsen 2000 kollat and reed 2006 numerous multi objective algorithms moea have also been proposed and used for watershed model calibration and uncertainty quantification some notable methods are the non dominated sorting genetic algorithm ii nsga ii bekele and nicklow 2007 confesor and whittaker 2007 ercan and goodall 2016 parasol van griensven and meixner 2007 the borg multiobjective evolutionary algorithm borg moea hadka and reed 2013 2015 and pareto archived dynamically dimensioned search pa dds asadzadeh and tolson 2013 however most optimization algorithms cannot dramatically reduce the computational cost and in some cases they increase execution time such costs are further exacerbated by automated calibration which can require thousands of model evaluations zamani et al 2021 fortunately the advent of parallel computing technology provides a promising way to solve the complex computationally expensive problems in hydrological modeling ahmadisharaf et al 2019 the purpose of the parallel computing technology is to decompose the modeling routines into multiple subtasks that can be executed simultaneously to reduce the elapsed computation time therefore the use of parallel computing in the form of distributed gridded and cloud computing has become increasingly prevalent gorgan et al 2012 standalone machine and cluster gridded networks are two main parallel computing categories defined according to the level at which the hardware supports parallelism zhang et al 2016 the former parallelization technique distributes tasks to different processors within a standalone machine typical examples include swat calibration and uncertainty procedures swat cup rouholahnejad et al 2012 and dream joseph and guillaume 2013 which both greatly improve the efficiency of model calibration however the scalability of these systems is poor because of the limited performance of the standalone machine in some cases it is not feasible for these systems to automatically calibrate and analyze large scale watershed models with complex physical domains and multiple water resource issues such as water quality droughts and floods abbaspour et al 2007 gupta et al 2012 the latter parallelization technique executes tasks on a cluster or gridded network such networks are easy to scale up by adding more machines to the computing cluster or gridded network and they therefore have a greater potential for speeding up model simulations notable examples include a generic tool for enabling the swat model application to be executed on a computer grid yalew et al 2013 and a python based parallel computing package pp swat zhang et al 2013 both of these have achieved significant speedups depending on model complexity however because these methods usually use a shared file system there is a potential problem when cluster gridded nodes need to access larger data volumes because the network bandwidth becomes a bottleneck white 2009 because most parallel computing paradigms evolved from conventional supercomputing resources their application may be limited by the time and monetary cost required to access such resources in addition to the technical knowledge and skills required to achieve high performance cloud computing technology has some advantages over traditional technologies including excellent scalability elasticity and its elegant and automatic handling of partial failure for this reason cloud computing may be a viable solution for performing complex model calibration and simulations because it can be used to parallelize model simulations and handle large post simulation results notable examples include a calibration system for swat based on the microsoft azure and dds method humphrey et al 2012 which obtained a significant speedup for swat calibration when a large number of cloud cores were used subsequently ercan et al 2014 further tested this system across a range of typical watershed sizes and simulation durations hu et al 2015a 2015b used hadoop a common open source software framework for creating a cloud computing environment on clusters of commodity hardware with large amounts of data storage to address computational intensity and model accessibility for a coupled human and natural system zhang et al 2016 moved swat model calibration and uncertainty analysis to a hadoop based cloud to achieve a speedup of 21 7 26 6 depending on model complexity recently zamani et al 2021 presented a cloud based calibration and uncertainty analysis system called lcc swat in which two optimization techniques sufi 2 and dds were implemented experimental results on swat models of different complexities showed that lcc swat could reduce execution time significantly these previous studies together demonstrated the applicability of cloud computing technology to complex model simulations and optimizations however many prior studies failed to adequately address two aspects of parallel model simulation optimization i the compatibility of implementations of the optimization algorithm with the cloud platform and ii the impact of node failures on model simulation optimization in general in the optimization algorithms commonly used in the above studies parallelism relies on the property that the next candidate points suggested are mutually independent therefore these techniques cannot be extended to standard sequential model based optimization algorithms such as bayesian optimization bo in which the next candidate points suggested depend on all the previous candidate points bo is an important technique in computer science that has been reported to achieve state of the art performance in many domains and is critical to the practical global optimization of time consuming complex models shahriari et al 2016 therefore we chose bo as the target algorithm to investigate the feasibility of parallelizing a conventional sequential model based optimization algorithm on cloud computing technology in addition because few studies have considered the problem of node failure in the optimization process we adopted hadoop a well established cloud platform to explore the impact of node failures on model simulation optimization the main objective of this study was to investigate the parallelization of the sequential model based optimization algorithm on the hadoop based cloud another objective was to investigate the capability of the hadoop based cloud to handle partial node failures during the simulation optimization process because the swat model is widely used we took the calibration of swat as an example to conduct the above exploration the main contribution of our work is the reliable parallelization of standard sequential model based optimization algorithms on cloud computing technology to achieve state of the art performance in complex model simulation optimization the paper is organized as follows in section 2 the hadoop based cloud framework for parallelizing traditional sequential model based optimization algorithms is introduced its two improvements and its major components including the physical swat model sequential model based optimization algorithm and hadoop based cloud are described in section 3 we apply the approach to calibrate the swat model in a case study and we examine the effectiveness and efficiency of the method in section 4 and 5 we present and discuss the optimization results respectively finally we provide some conclusions in section 6 2 hadoop based cloud framework for parallelizing traditional sequential model based optimization algorithms the key principle of using the hadoop based cloud framework to parallelize traditional sequential model based optimization algorithms is to group multiple simulations into multiple sequential mapreduce jobs on hadoop each mapreduce job involves multiple tasks that can be executed in parallel in the mapreduce framework in this manner multiple simulations within a sequential optimization algorithm such as bo can be orchestrated to execute in parallel the rationale for this approach is that the cost of evaluating the acquisition function of bo is small or even negligible compared with the cost of evaluating the complex model therefore the sequentially suggested candidate model simulations can be executed in a parallel manner in addition the efficiency of a single simulation is further improved by invoking a single model computation within a mapper container only without using the reducer procedure this technique avoids the need for the shuffle procedure a well known performance bottleneck of the mapreduce framework the proposed framework was implemented mainly in python in a linux environment by coupling the swat model with the bo library scikit optimize on the java supported hadoop based cloud scikit optimize is a simple and efficient library for minimizing the use of expensive and noisy black box functions which implements several methods for sequential model based optimization the library is built on top of numpy scipy and scikit learn and is intended to be accessible and easy to use in many optimization contexts similarly to other optimization libraries scikit optimize uses the suggestion evaluation observation step to locate the optima in our framework the computationally expensive swat simulations performed during the iteration of the suggestion evaluation observation loop are executed using the hadoop based cloud tornado and springboot were used to construct two web servers for data exchange via http requests and responses between the swat model and bo the former hosts the scikit optimize library and the latter serves as a transformer responsible for first receiving parameter sets pss from the tornado framework then parsing pss to generate hadoop mapreduce jobs and finally submitting these jobs to the hadoop cluster for model execution the fortran based swat model was first compiled in a linux environment and then wrapped as a mapreduce task within a hadoop cluster for model evaluation the methodology of the framework involves three steps model setup to establish the goal of the optimization setup of the hadoop based cloud to decompose a large number of model simulations and distribute them to multiple computing nodes and setup of the sequential optimization algorithm to guide and locate optima two improvements were made parallelizing the traditional sequential model based algorithm bo and simplifying the coupling between the complex model and the mapreduce framework to avoid using the reducer procedure the main components of the framework are elaborated in the following subsections the core tasks are explained below and illustrated in fig 1 i model setup first a complex model needs to be constructed to serve as the target of the optimization as an example we describe the construction process of the swat model the swat model requires explicit information regarding weather topography soil properties vegetation and land management practices to simulate processes such as surface and subsurface stream flow sediment transport nutrient cycling and crop growth franco et al 2020 we used arcswat to set up the swat model because it provides a user friendly interface and presents the results as intuitive and informative maps during the setup process swat is fed by multiple data sources including digital elevation models dems streamlines soil data and land use data after the swat model setup is completed the swat project configuration files and the swat executable program compiled for the linux operating system are compressed and stored as a distributedcache the next step of initialization comprises the following tasks first eight hydrogeological parameters are initially chosen according to a previous study ghaith and li 2020 the parameter ranges are carefully determined according to available field data second the eight parameters are sampled n times using the latin hypercube sampling method mckay et al 1979 where n is the total number of processors employed to perform model simulations in the hadoop based cloud each sample drawn from the eight dimensional space is an eight vector and the n samples represent complete possible combinations of the eight parameters which are used as the inputs to the n simulations of the swat model n plain text files are then constructed from these n input samples and uploaded to the hadoop based cloud in the form of a hadoop distributed file system hdfs file finally an initialization job is created and submitted to the hadoop based cloud ii hadoop based cloud on receiving the initialization job the hadoop based cloud takes over to locate the positions of n sample files typically the compute nodes and storage nodes are the same that is the mapreduce task and the hdfs are executed on the same set of nodes this design allows the framework to effectively schedule tasks on nodes where the data are already present resulting in very high aggregate bandwidth across the cluster subsequently n mapreduce tasks are created and distributed to the n nodes where the n files are located the sample file is parsed on the node to update the eight parameters and evaluate the model at the same time the swat project configuration files and the swat executable program are retrieved from the distributedcache and uncompressed to a local directory therefore the swat project configuration files can be edited and updated according to the content of the sample file the swat executable program is then invoked to execute at the local path and keep the user informed of the status of the model during execution because of the complexity of the calculation process both the raw and parsed simulation results are stored the raw results are directly stored in the hdfs and the parsed results are exported to a persistent storage database such as hbase or postgresql hbase was employed in our study because of its compatibility with the mapreduce computing framework thus keeping the different technology solutions used within the same hadoop ecosystem to provide training data for the optimization algorithm the parsed simulation results are further analyzed with respect to the objective function required by the optimization algorithm and then converted to json format and sent to the optimization algorithm using the http protocol thus there are n responses to the optimization algorithm from the hadoop based cloud improvement 1 simplifying the traditional mapreduce programming paradigm without using the reducer procedure the main idea of mapreduce is the divide and conquer principle whereby a large problem is divided into many small problems which are then executed on each node in the cluster after the map process is complete it is followed by an intricate and intractable reduce process which gathers shuffles sorts and reduces the results of all the map phases notably a reduce process cannot complete until all map processes are complete so that it can fetch all the data that it needs to process this is ideal for processing very large amounts of data e g multi terabyte datasets in parallel however it is not compatible with complex models because it greatly exacerbates the complexity of program design there is no need to gather and analyze the model results by using the reduce process because these tasks are the responsibility of the optimization algorithm therefore the mapreduce programming paradigm needs to be improved to deal with complex models instead of using the standard mapreduce programming paradigm in which both map and reduce processes are implemented single simulation efficiency is improved by invoking a single model computation in the mapper container only without using the reducer procedure this improvement has two advantages it avoids the potential performance bottleneck of the shuffle procedure a well known problem that may occur in the reduce process and greatly simplifies the program design process iii sequential optimization algorithm on receiving the first n responses from the hadoop based cloud the sequential model based optimization algorithm a form of bo begins to initialize itself first the streamflow outputs of the n representative swat simulations are used to calculate the bo objective function which was the nash sutcliffe efficiency coefficient nse in this study to be minimized between the simulated and measured streamflow histories the nse objective function is used as the response variable of the surrogate model to the eight input variables second the bo object is instantiated the n pairs of eight vector inputs and the corresponding objective function are used to instantiate a bo object using random exploration from n responses from hadoop this exploration can help by diversifying the exploration space as shown in fig 1 the major routine following the suggestion evaluation observation loop is then launched after the construction of the bo object three steps are executed in a loop first bo sequentially creates n new suggestions from the acquisition function of bo one suggestion represents a potential eight vector input balanced between exploration and exploitation for locating the next potential optima second n suggested eight vector inputs are packaged and sent to the hadoop based cloud for feeding into the swat model to evaluate the objective function the combination of the eight vector inputs and the responding objective function values are then treated as n new observations to update the bo object the loop continues until a specified criterion is satisfied for example when the maximum number of iterations has been performed or a specific objective function value has been obtained finally the eight optimal parameters are determined the above suggestion evaluation observation loop forms the core of the bo based calibration framework as the loop continues the bo balances its needs for exploration and exploitation taking into account its knowledge of the target function at each step a gaussian process gp is fitted to the known observations the points previously explored and the posterior distribution combined with an exploration strategy such as the upper confidence bound ucb or expected improvement ei is used to determine the next suggestion that should be explored the final loop results in the optimal values of the eight input parameters see fig 2 improvement 2 parallelizing the traditional sequential optimization algorithm on the hadoop based cloud the sequential model based optimization method is an application of bayesian reasoning which is also a form of bo that updates the probability model sequentially every evaluation of the objective function with a set of values updates the model so that the model will eventually represent the true objective function the standard sequential model based optimization algorithm takes into account the previous results to try more promising values but apparently cannot be parallelized to improve efficiency however it differs from random search or grid search in that it constructs a probability model of the objective function also called a surrogate function which is much easier to optimize than the actual objective function this function can be evaluated and optimized very quickly to identify the next evaluation point in summary the hadoop based cloud can enable the parallelization of multiple simulations and sequential optimization can identify the next candidate point very quickly combining these two points we propose an improvement for parallelizing the traditional sequential model based optimization algorithm on the hadoop based cloud by an elegant orchestration when the bo object receives the first n pairs of eight vector inputs and the corresponding objective function values it instantiates itself uses those n pairs as observations to update the probability model of the objective function suggests n new eight vector inputs and sends them to the hadoop based cloud in the next iteration when the bo object receives the second n pairs of eight vector inputs and the corresponding objective function values from hadoop it updates the probability model of the objective function suggests n eight vector inputs and sends them to the hadoop based cloud the loop continues until a specified criterion is satisfied for example when the maximum number of iterations has been performed or a specific objective function value has been obtained this improvement differs from the standard sequential optimization method as shown in fig 3 because it suggests n eight vector inputs at a time rather than one pair at a time such an orchestration is beneficial for taking advantage of the hadoop based cloud because the n eight vector inputs are identified at approximately the same time allowing n swat models to be executed concurrently 2 1 swat model the swat hydrological model was adopted in this study to simulate streamflow hydrographs this is a semi distributed continuous time watershed model that is capable of running in daily and sub daily time steps gassman et al 2007 originally developed by arnold et al 1998 to understand the impact of management scenarios and non point source pollution on water supplies at a watershed scale it has been used widely in a variety of watershed studies including simulations of both water quantity and water quality li et al 2010 lee et al 2010 liu et al 2013 the watershed is first divided into several sub basins depending on the outlets which are usually defined by subdividing the dem and performing manual adjustment each sub basin is further divided into hydrological response units hrus with specific attributes by setting different thresholds for soil land use and slope classes the swat model uses the concept of hru to represent variability within the sub basins of a watershed the hrus are unique representations of land cover soil and management characteristics within a single sub basin and are used for water balance calculations in the model the hrus are not spatially contiguous therefore they are often composed of many disjoint parcels of land in a watershed therefore swat takes the spatial heterogeneity of the watershed into account by dividing the watershed into different sub basins that belong to the same range of hydrological factors this makes parameter calibration very difficult 2 2 hadoop based cloud hadoop is an open source cloud computing framework that implements the google mapreduce algorithm lam 2010 white 2009 it consists of two main components the hdfs and the distributed computation framework dcf which is a framework that implements the mapreduce algorithm hdfs is a highly fault tolerant distributed file system which divides large files into small data blocks filesplit and distributes these blocks among the datanodes of the cluster as a result it is able to read and write data in parallel across a large number of machines and can achieve much higher throughput than traditional technologies dcf uses the mapreduce algorithm to achieve parallel processing across the cluster it helps by moving the computation part of the algorithm to the same location as the data the mapreduce algorithm is designed to compute with large volumes of data by splitting the data into blocks and processing different blocks in parallel mapreduce comprises two major phases the map phase and the reduce phase in the map phase mapreduce splits data into elements inputsplit and feeds each data element to the mappers each mapper maps input key value pairs to intermediate key value pairs that is the mapper is used to transform the input pairs to output pairs which the reducer can further process in the reduce phase the input key value pairs i e the key value pairs output by the mapper with the same key are aggregated to form a new small set of key value pairs by implementing these two interfaces users can easily establish robust scalable and distributed programs 2 3 bayesian optimization bo is an efficient framework for the global optimization of expensive objective functions which is especially useful for optimizing black box functions whose exact form is unknown auer 2002 the core of bo employs a probabilistic model to capture the unknown function this differentiates it from commonly used random search or grid search algorithms rather than just selecting from a grid uninformed by past objective function evaluations bayesian methods take into account the previous results to try more promising values the probability model is much easier to optimize than the actual objective function and therefore the optimization process is much faster after each evaluation of the objective function the algorithm updates the probability model a gp specified by its mean and covariance functions is widely used as such a probabilistic model the next stage of bo uses surrogate utility functions known as acquisition functions these surrogate functions are constructed to trade off the need to explore regions where the epistemic uncertainty about the objective function is high and the need to exploit regions where the predicted values are high these acquisition functions can be optimized to extract the next candidate point for evaluating the objective function in bo as in other optimization algorithms the objective function should be selected first the objective function is the statistic that is maximized or minimized in the optimization process the nse nash and sutcliffe 1970 which represents the percentage of model fitting variance to total variance is one of the most commonly used model evaluation indicators bennett et al 2013 bae and seo 2018 therefore the nse was selected as the model evaluation index in this study it is defined as follows 1 n s e 1 i 1 n o i s i 2 i 1 n o i o 2 where oi is the observed value and si is the simulated value when nse is 1 there is a perfect fit between the simulated values and the observed values 3 case study 3 1 swat model setup the yiwu river basin 29 00ʹ n 29 40ʹ n 119 50ʹ e 120 40ʹ e is mainly located in yiwu city zhejiang province as shown in fig 4 the main stream is the yiwu river and the nanjiang river is its main tributary the drainage area is 2050 47 km2 the average annual precipitation in the basin is 1447 mm april september is the wet season and november december is the dry season the precipitation accounts for 59 and 8 of the annual precipitation in the wet season and dry season respectively the basic data required by the swat model include a dem a river network and data on land use and cover soil type and its physical and chemical properties plants and plant growth fertilizer hydrology and water quality meteorology livestock husbandry and population emissions neitsch et al 2004 lai et al 2020 the geographic coordinate system of all spatial data in this model is gcs wgs 1984 and the projection coordinate system adopts albers projection with the central meridian 105 e and two standard latitudes of 25 n and 47 n the elevation data are taken from the geospatial data cloud with 30 m accuracy the land use data are obtained by processing remote sensing image data from landsat for 2018 with 30 m precision from the geospatial data cloud the soil data come from the 1 1 000 000 soil data provided by the china soil database which adopts the world soil database hwsd fao90 classification standard using the dem and the imported real river network the yiwu river basin is finally divided into 18 sub basins and 319 hrus by reclassification of the dem soil data and land use data data from two meteorological stations 89 181 and 89 182 were obtained from cmads the china meteorological assimilation driving datasets for the swat model meng et al 2018 including data on temperature precipitation humidity wind speed and solar radiation observed monthly streamflows for the watershed outlets doyang and fotang were acquired from the chinese hydrological data yearbook for the years 2010 2017 the first 15 of the 27 parameters recommended in the literature ghaith and li 2020 were selected as the target for streamflow calibration these parameters are cn2 ch w2 ch l2 ch k2 sol awc sol bd alpha bf esco gw delay sol k ch d tlaps timp gwqmn and smfmx their names upper and lower limits and meanings can be found in neitsch et al 2005 because ch w2 ch l2 ch k2 and ch d describe channel properties they can be measured directly through the stream and watershed survey and were therefore not considered to be calibration parameters because timp gwqmn and smfmx are less sensitive compared with other 12 parameters and were also not considered to be calibration parameters therefore the remaining eight parameters cn2 sol awc sol bd alpha bf esco gw delay sol k and tlaps became the final target for calibration 3 2 hadoop based cluster setup the model evaluations numbering several hundred were distributed to a hadoop cluster comprising seven nodes each equipped with an intel 8 core i7 3770 processor and 32 gb ram and another four nodes each equipped with an intel 32 core xeon r silver 421 processor and 96 gb ram these physical machines are depicted in fig 5 after establishing the main structure of the hadoop cluster many other installation and configuration tasks are needed to run hadoop these include the installation of the operating system centos 7 java environment jdk 1 8 and hadoop software version 2 7 5 as well as the creation of a hadoop user and configuration of ssh and hadoop a detailed description of these steps is beyond the scope of this paper but the reader can refer to white 2009 for more information to solve the problem of the single point of failure which is encountered in a hadoop cluster two high availability improvements have been made the first is the inclusion of the resourcemanager which is responsible for tracking the resources in a cluster and scheduling applications e g mapreduce jobs prior to hadoop 2 4 the resourcemanager was the single point of failure in a yarn cluster the high availability feature adds redundancy in the form of an active standby resourcemanager pair to remove this single point of failure yarn proposes the separation of the two major functions of the jobtracker resource management and job scheduling and monitoring the second high availability improvement is the hdfs oriani and garcia 2012 which improves the availability of hdfs by obviating failure on the namenode machine which makes the entire hdfs cluster unavailable the hdfs feature provides the option of running two redundant namenodes in the same cluster in an active standby configuration with a hot standby the procedure for performing swat model evaluations on a hadoop cluster is to call the fortran based swat model from the java based mapreduce framework because the fortran code of the swat model is compiled to an executable program for the linux environment it can easily be executed in the mapreduce framework finally http requests are used to transfer variables bidirectionally between swat and bo 3 3 implementation of bayesian optimization for the swat model on the improved hadoop based cloud because of its flexibility tractability analyzability and good substitutability for linear and nonlinear relationships gp was chosen as the probabilistic surrogate model in our bo algorithm in practice it is very difficult to specify the prior information for the mean function of the gp therefore for the sake of simplicity we made the usual assumption that the prior mean function is 0 which has little effect on the accuracy of the posterior distribution the covariance specifies the smoothing line and amplitude of the unknown objective function and represents the similarity between the two calculation points for simplicity we adopted the matérn covariance function with a smoothing parameter value of 2 5 and a scale parameter value of 1 another component of bo is the acquisition function ei was chosen because it incorporates both the improvement probability and the improvement amount the main implementation process for the bayesian method was as follows 1 tornado initializes the parameter space of the swat model randomly implements various sampling iterations uses these samples to create n batch model pss and sends the n pss to springboot 2 springboot parses the n pss to create n batch model configuration files and distributes these configuration files to the hadoop cluster in hdfs format 3 the hadoop cluster performs the model calculation concurrently on the node where these files are located and returns the nse objective function value to tornado in an http response when the model execution has finished 4 tornado sequentially collects n nse objective function values until all n model calculations on the hadoop cluster are finished 5 tornado determines whether the calibration requirement is satisfied according to one of the n nse values returned when the calibration requirement is not satisfied the bo s built in acquisition function automatically and sequentially recommends the next set of n parameters to continue the iterative calculation otherwise the cluster operation is terminated 6 tornado returns the calibration result when the calibration criterion is satisfied 4 results 4 1 improved parallelized optimization algorithm for hadoop based cloud framework to evaluate the speed improvement houstis et al 1997 resulting from parallelizing the traditional sequential model based bo algorithm on the hadoop based cloud two experiments were conducted to perform swat calibration with and without the reduce procedure overall 1200 swat calculations were performed with each experiment involving up to 600 iterations to ensure the convergence of bo fig 6 plots the speedup with different numbers of cores ranging from 10 to 100 although the speedup was not proportional to the number of allocated cores a speedup of about 55 58 was still achieved when 100 cores were allocated this demonstrated the benefit of parallelizing the sequential model based optimization algorithm on the hadoop based cloud the actual speedup of both the map task and the reduce task indicated that as the number of cores increased the speedup gradually deviated from the ideal linear speedup this implies that performance may deteriorate when more cores are used 4 2 simplified coupling between complex model and mapreduce framework without using reducer procedure traditionally a typical hadoop mapreduce application implements both map and reduce tasks at the same time to further improve the efficiency of swat model evaluation the speedups achieved with and without the reducer procedure were compared contrary to our expectations the speedup of the map task slightly exceeded the speedup of the reduce task however this improvement was not sufficiently pronounced as shown in fig 6 when more than 30 cores were used the map task began to show a noticeable performance improvement a speedup gain of between 2 and 4 over the reduce task notwithstanding this such a slight improvement both facilitates the coupling between the complex model and the mapreduce framework and circumvents the shuffle procedure which is a notorious performance bottleneck of the mapreduce framework 4 3 effect of the suggestion procedure of the optimization algorithm on parallelization efficiency as the number of iterations increases the process of suggesting candidate points in the optimization algorithm may take more time to quantify the time loss caused by the suggestion process and eliminate its impact on the speedup which may increase the total time for suggesting n batch model pss two experiments to calibrate the swat model were conducted fig 7 plots the suggestion time taken by bo when calibrating eight and 15 parameters the first experiment calibration of eight parameters showed a steady trend with a suggestion time of less than 500 ms which is negligible in comparison with the execution time of the swat model in contrast when calibrating 15 parameters the suggestion time gradually increased as the number of iterations increased in particular when the number of iterations was 600 the suggestion time reached as high as 40 s these experiments highlighted a potentially serious shortcoming of the sequential optimization algorithm 4 4 handling of partial failures on the improved hadoop based cloud framework failures frequently occur during model execution particularly when a large number of iterations are performed to demonstrate the capability of the framework to handle failures during the optimization procedure experiments were conducted to dynamically increase or decrease the number of computing nodes to eliminate the influence of the difference in computing node hardware on the experiment seven nodes with almost the same hardware intel 8 core i7 3770 processor and 32 gb ram were used each node used seven cores for model execution and one core to support the operating system five experiments were designed of which the first and last were used as baselines and the other three were designed to test the resilience of the model to abnormal operation these three experiments involved shutting down one node 1 7 cores three nodes 3 7 cores and seven nodes 7 7 cores each experiment was repeated nine times to calculate the average so a total of 45 experiments were conducted fig 8 shows an inverse relation between the cost time and the number of active nodes the average execution time for one iteration increased from 198 s to 406 s when the number of nodes was reduced from seven to one and then decreased again to 198 s when all seven nodes became active this demonstrates that even if some nodes fail during the optimization procedure hadoop automatically rebalances the workload on the remaining nodes and the simulation continues without affecting the overall optimization process 4 5 case study calibration results to quantify the calibration effect of the model the monthly streamflow simulations were compared with the observed data at doyang and fotang hydrological stations the average nse values of the two stations both reached as high as 0 84 during the calibration period 2014 2015 and verification period 2016 2017 this indicates that the calibrated parameters captured the monthly runoff variation accurately and thus can be used for future simulations of monthly runoff however a detailed description of the results is not the main purpose of this study 5 discussion the improved hadoop based cloud can parallelize conventional sequential model based optimization algorithms simulation optimizations have always suffered from a high computational burden usually requiring substantial numbers of model simulations such costs are further exacerbated when the model simulation is extremely computationally intensive razavi et al 2010 in response to this numerious parallel optimization algorithms have been developed due to the fact that these algorithms have the ability to be implemented in a parallel manner however for sequential optimization algorithms such as bo because the next candidate point depends on previous ones parallelizing the algorithm seems impractical we have proposed a novel framework for parallelizing a sequential optimization algorithm on the hadoop based cloud because the time taken by the optimization algorithm s suggestion is negligible compared with that taken by the complex model calculation our method suggests multiple sets of candidate points sequentially instead of 1 ps at a time and then distributes the candidate points to the hadoop cluster for concurrent processing in this manner a sequential algorithm can be elegantly converted to execute in parallel the case study using bo to calibrating the swat model achieved a speedup of nearly 55 58 when using 100 cores demonstrating the feasibility of parallelizing the bo algorithm on the hadoop based cloud the improved hadoop based cloud can guarantee the reliability of simulation optimization tasks it handles node failures without affecting the ongoing simulation by redistributing the failed tasks to working hadoop cluster our framework adopts a modular architecture that separates the optimization algorithm from model calculations the latter are wrapped in hadoop so that complex models such as swat can be executed on independent nodes and a failed model calculation task can be recovered by spawning a new process to execute it possibly using an idle process that has completed its map task or reduce task hadoop satisfies the nonfunctional requirements of constructing well established system software such as fault tolerance load balancing and scalability sethia and karlapalem 2011 therefore it automatically rebalances the workload on remaining nodes and the simulation continues when partial failures occur which allows users to focus on their optimization without getting involved in ensuring fault tolerance furthermore hadoop facilitates dynamic addition of new nodes to an executing simulation because it is developed on top of hadoop our framework inherits hadoop s advantages making it a promising option with fault tolerance and failure recovery capability our framework is the first of its kind to parallelize sequential optimization techniques on cloud based architectures to reduce calibration execution time commonly used optimization techniques particularly population based algorithms such as nasg ii deb et al 2002 shuffled complex evolution metropolis scem ua vrugt et al 2003 dds tolson and shoemaker 2007 particle swarm optimization afshar et al 2011 and borg hadka and reed 2013 all of which are parallel techniques can use parallel computing resources because these techniques are inherently parallel however this is not the case for sequential model based optimization techniques whose next candidate point depends on the previous ones to date studies on extending parallel computing to sequential optimization algorithms have not been reported our study proposed a novel framework built on top of the hadoop based cloud to add parallelism to conventional sequential optimization algorithms to facilitate broader applications of the existing algorithms in addition few studies have addressed the possibility of node failures occurring during model simulation sethia and karlapalem 2011 our hadoop based framework solves this problem by automatically rebalancing the workload on remaining nodes and continuing the simulation which provides an elegant solution that enhances the reliability of the optimization effort moreover compared with traditional applications of hadoop which typically employ both map and reduce tasks at the same time hu et al 2015a 2015b zhang et al 2016 our framework is the first of its kind to use only map tasks to further improve the efficiency of the model simulation although no obvious improvement was observed only a speedup gain of 2 5 was achieved when more than 30 cores were used to clearly prove that the approach using only map tasks outperforms the traditional method this approach does greatly simplify the programming process of wrapping a complex model into mapreduce whch may provide an insight on addressing the compatibility of complex models with cloud platform simulation optimization framework methods that couple the model simulations with the optimization algorithm usually entail a large number of model simulations such frameworks may be problematic when single model simulation is computationally intensive an ideal framework would therefore separate the optimization algorithm from model simulation to both achieve high performance computing and ensure that the execution of the optimization algorithm is not affected by a failure of the model simulation our improved hadoop based cloud framework for complex model simulation optimization was indeed designed and implemented in such a manner the case study clearly demonstrated its ability to both efficiently decompose a large number of model simulations and effectively handle partial failure because it inherits the dynamic load balancing and failure recovery capabilities of hadoop this framework is particularly suitable for complex model optimization in addition it is a generic framework for solving diverse optimization problems and is not specifically intended for the calibration of hydrological models however there are two possible drawbacks associated with this framework the speedup performance of the framework may not be proportional to the number of allocated cores this drawback was also observed in other studies ercan et al 2014 zhang et al 2015 bacu et al 2017 zamani et al 2021 fig 6 indicates that as the number of allocated cores increases the speedup gradually deviates from the ideal speedup exhibiting a nonlinear scaling of execution time performance this implies that performance may deteriorate when more cores are used similar results have been reported by others for instance ercan et al 2014 showed with an experiment involving 256 cores that the use of 64 cores was the most desirable from the economic point of view one plausible reason for this deterioration is the expected network latency and i o operations required for communication between the nodes of the hadoop cluster zamani et al 2021 another reason is the unexpected network latency required to transfer variables between tornado and springboot the other possible drawback of this framework arises from the assumption that the time taken by the optimization algorithm s suggestion is negligible in comparison with that of model simulation this is not often the case fig 7 plots the difference in suggestion time of bo between different numbers of calibration parameters of the swat model when the number of iterations is 600 the suggestion time can reach 40 s which is nearly one third of the execution time of the swat model this may degrade the efficiency of parallelism this can be explained by the use of bo to calibrate the swat model in our case study bo uses the gp mockus et al 1978 shahriari et al 2016 metamodel to approximate the relationships between several explanatory variables typically the simulation model parameters and the variables affecting model inputs and one or more model response variables generally the computation time required for refitting a metamodel mostly nonlinearly increases with an increase in either the size of the variables known as the curse of dimensionality or the number of iterations therefore care should be taken to check the suggestion time of the optimization algorithm when the number of iterations is large 6 conclusions this paper presented an improved hadoop based cloud framework for alleviating the substantial computation burden of complex model simulation optimization the framework is the first of its kind to parallelize sequential optimization techniques on cloud based architectures and thereby reduce the execution time of model simulation and is the first to further simplify the coupling between the complex model and the mapreduce framework without using the reducer procedure the framework is also capable of guaranteeing the reliability of simulation optimization tasks by handling node failures without affecting the ongoing simulation the case study using bo to calibrate the swat model achieved a speedup of nearly 55 58 when using 100 cores demonstrating the feasibility of parallelizing the bo algorithm on the hadoop based cloud the experiments to dynamically increase and decrease computing nodes demonstrated that even if some nodes fail during the optimization procedure the framework can automatically rebalance the workload on the remaining nodes and the simulation can continue without affecting the overall optimization process the results clearly demonstrate the framework s ability to both reduce the execution time of model simulation and effectively handle partial failure the framework is particularly suitable for complex model optimization but it is also a generic framework for solving diverse optimization problems it may provide a new perspective for the parallelization of traditional sequential optimization algorithms and is expected to be an ideal method to alleviate the substantial computational burden of the optimization process first avaliable december 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to the funding support from the national natural science foundation of china 41925005 and the national key r d program of china 2019yfd0901105 
