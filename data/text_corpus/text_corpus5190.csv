index,text
25950,a new software application called urunme was developed in c net for integrated environmental modeling urunme provides functionality for model coupling data storage analysis visualization scenario management and decision support systems a graphical user interface gui based on drag and drop flowchart type diagrams allow modelers to set up a model application once and then focus on running and analyzing results without needing to further address housekeeping details data are stored in the embedded sqlite database interactive dashboards can be created using multiple screens and visual elements the built in scenario management feature provides the capability of running multiple scenarios of a model using different input parameters initial and boundary conditions this paper presents urunme s architecture and describes how it can be utilized for creating automated integrated modeling applications by domain experts a demonstration of capabilities is shown in a series of case studies of increasing complexity in the water and wastewater applications keywords integrated modeling multi model coupling decision support system data analysis data visualization scenario management 1 introduction integrated solutions and higher order systems thinking are required to deal with increasingly complex real world environmental problems laniak et al 2013 integrated environmental modeling iem provides a comprehensive approach to environmental management by evaluating different components of a system in a holistic manner according to johnston 2010 integrated modeling encompasses a broad range of approaches and configurations of models data and assessment methods to describe and analyze complex environmental problems often in a multi media and multi disciplinary manner over the years the interest in iem and more specifically integrated water resource modeling has increased significantly kalbacher et al 2012 molina et al 2010 these iem applications can be used as an effective tool for design planning risk assessment decision making and enhancing the transparency and collaboration between stakeholders johnston et al 2011 peach et al 2011 knapen et al 2013 noted five approaches for the model integration 1 soft linking 2 scripts 3 proprietary one large monolithic model 4 proprietary loosely linked models and 5 standard modeling framework however we broadly categorized the integrated modeling applications into 1 built in integrated models and 2 coupled integrated models 1 1 built in integrated models the first category provides built in integration to simulate a complex environmental system in this method the model integration is generally achieved by using two different approaches 1 models for the different sub systems are contained within the principal software 2 various models developed by the same organization are coupled based on some proprietary linking mechanism one example of this approach is software called weap water evaluation and planning which was developed by the stockholm environment institute s us center sei us to assist policymakers in evaluating water supply policies and suitable water resources plans yates et al 2005 another such example is basins better assessment science integrating point and nonpoint sources that was developed by the u s environmental protection agency usepa 2015 it is designed to model meteorological condition streamflow and pollutant transport across watersheds by using embedded hydrologic and pollutant fate and transport models basins contains several useful components that allow users to develop integrated models and perform watershed analysis and data compilation within the software 1 2 coupled integrated models coupled applications on the other hand are based on linking different standalone models to simulate various components of a targeted environmental system these models are linked together based on two different approaches 1 loose coupling or 2 framework based coupling 1 2 1 loose coupling loose or soft coupling is a sequential approach where data flow paths are unidirectional and configured in a tree like cascade structure rauch et al 2002 the individual models are linked together such that the output of one model is used as an input to the other with perhaps some unit conversions performed as an intermediate step the models used in such applications provide readable input and output in an open data format baran et al 2018 malard et al 2017 xu et al 2007 the chesapeake bay watershed model is one such example in the water resources domain where various applications of hydrological simulation program fortran hspf of sub watersheds are linked together to simulate the entire watershed hopkins et al 2000 1 2 2 framework based coupling in contrast tight coupling generally involves linking different standalone models using standardized data transfer protocols based on some commonly accepted standard framework common frameworks used in environmental modeling are generally categorized into either 1 component based modeling or 2 service oriented architecture soa based modeling belete et al 2017 in component based modeling each model is run as an independent component and data exchange between different models is achieved using a standard interface goodall and peckham 2016 over the past years a number of component based frameworks and interface standards have been formulated and applied in various domains including water resources buahin and horsburgh 2018 for instance earth systems modeling framework esmf developed with the focus on weather climate and related earth science applications hill et al 2004 the community surface dynamics modeling system csdms is another example of a component based framework which was developed with the goal of modeling earth s surface processes and dynamics peckham et al 2013 another such framework is the object modeling system oms with currently more than 200 model components that mostly focuses on agro environmental models david et al 2013 in the water resources domain a more well known component based framework is the open modeling interface openmi which enables the dynamic linking of models at runtime moore and tindall 2005 in soa based frameworks models are turned into web services and linked using standard text based protocols creating plug and play autonomous components python network simulation pynsim framework is an example of this knox et al 2018 other examples of soa based frameworks include weezard for geoanalysis which is a web service for debris flow hazard assessment and visualization zhang et al 2018 and emeli web that applies a basic model interface bmi for modeling a set of spatially distributed hydrologic models jiang et al 2017 1 3 need for generic iem software in the past few years many advancements in iem were made new component and web based frameworks have been introduced and the existing ones were improved belete et al 2017 lloyd et al 2011 whelan et al 2014 while these efforts are vital for addressing the emerging challenges in environmental modeling they are mostly directed towards advanced applications specific to different science domains belete et al 2017 on the other hand there is still a need within the modeling community for software programs which focuses beyond the various coupling processes and frameworks and provide advanced pre and post processing and other capabilities to manipulate the data inputs and generated outputs and visualizations from the coupled models even today there is a lack of dedicated tools to handle data i o input output e g reading and writing databases text files binary data etc and transformation e g data quality assurance quality control qa qc timeseries aggregation de aggregation infilling of missing data using standard or user defined techniques joining of datasets unit conversions etc david et al 2013 whelan et al 2014 additionally modelers have to use a number of different software programs for data storage downloads analysis statistics visualization etc in many cases where models are required to run multiple scenarios using different initial and boundary conditions manual steps update of input model run and post processing of output for example are required for each run belete et al 2017 buahin and horsburgh 2018 knapen et al 2013 performing all these tasks manually is a tedious process which requires a significant amount of time and effort bergez et al 2013 for instance there has been a lot of interest recently in developing decision support systems based on integrated modeling of natural and engineered environmental systems e g watershed reservoir treatment plants etc lodhi et al 2019 most of these applications require constant updates of input data obtained from different dynamic sources e g flow measuring stations weather stations weather forecasting models water quality monitoring labs online probes etc to run the models on a periodic basis data updates are often hourly for continuous monitoring and daily often in the overnight hours for updates of long term weather forecasts for example routinely performing the data update and model run steps manually is not a viable option although coding scripting can be used to automate these tasks it requires a considerable level of programming skills and in depth knowledge of different model structure and data formats which is not very common buahin and horsburgh 2018 iem applications based on loose sequential coupling are less complex and often do not require advanced modeling protocols however even such an application requires a large number of different pre and post operations to run the integrated models software which can provide an extensive built in data processing functionality for such a loose coupling arrangement in a single package has not been available in the iem software domain framework based coupling also presents similar challenges many frameworks lack advanced software support to perform the necessary pre and post processing and runtime operations for instance most of the software programs developed to run openmi compliant models e g openmi configuration editor pipistrelle etc do not provide any capabilities for the software to read or write data directly from a model component at runtime e g to show a model output on a chart while existing frameworks provide one or two of these capabilities none provide all of them gui data storage web data access visualization analytics scenario management scheduling capabilities for automated model runs and tools for creating interactive dashboards having all these capabilities in one software package would allow modelers to create applications that consolidate all these functions into one application therefore there is a need for software that can handle in a flexible way both loose and framework based coupling and provide all the necessary functionality required for integrated environmental modeling the capability of being able to develop fully automated modeling applications which can be operated by the click of a button dynamically creating inputs from different data sources managing scenarios running the models reading outputs analyzing data making statistical inferences and ultimately showing results in a meaningful and interactive way laniak et al 2013 morsy et al 2017 would be of value to both the ordinary user and the modeler in essence it should be able to facilitate a one stop solution for automated iem by a modeler urunme you run models easily was developed with the above mentioned integrated modeling functionality in mind we feel it is mature enough to be of use to researchers academics students and the like and is available for free for these constituencies this paper will explain the software architecture section 2 and the features and capabilities appendix a we will discuss some real world examples section 3 to demonstrate how it would work in a range of applications of environmental systems modeling focusing primarily on water resources from low flow analyses to a complete decision support system which links a wastewater treatment plant model with a multi application watershed model in this paper we illustrate how the software we have developed eliminates the burden of doing routine tasks such as statistical analyses data visualization for calibration checking forecasting optimization including calibration and goal seeking and other useful functions while urunme can be used for modeling other systems our emphasis here is on environmental systems and specifically water resources systems a summary of different features provided in urunme and how that can help the modeler is provided in table 1 below 2 software architecture urunme is programmed in c using net framework it is developed using a three tiered architecture arranged by presentation business and data fig 1 the gui is developed using various third party libraries table a1 which provides a comprehensive suite of various visual components in addition several helper classes are written to wrap the third party controls for added functionality which is otherwise not available by default the business side of the software is programmed as a standalone dynamic link library dll which is accessed by the presentation layer using a strict one way association no commercial component is used to develop this dll allowing an open source option in the future if desired the business tier is further decomposed into multiple layers each responsible for implementing different aspects of the logic the core layer of the business tier is called nodes which contains all the classes that are visually exposed to the users in a project the remaining classes are contained in the helper layer which provides additional functionality classes in the data access layer are essentially wrappers on net and other third party data providers to access different types of databases including sqlite mysql sql server in addition the data access layer also contains custom classes to read and write different file formats such as text comma space tab or custom delimiter and fixed width excel netcdf grib wdm this ability was considered essential due to multiple data sources often being used for a model application and also to reformat the input or output of one model for use by another for instance most weather current and forecasted data available on the noaa servers are either in grib2 or netcdf format similarly some models use fixed or comma separated csv files for inputs and outputs the user customizes the application for the appropriate format and also determines the association of the values read in with the appropriate parameter and its format the user may also use the built in functions to change formats for example a decimal julian date may be changed to one of the standard date formats selected by the user by implementation of a simple calculation process step this is essentially the conversion of dates from one unit to another much as the software allows for conversions between english and si units 2 1 graphical user interface the default gui consists of a ribbon type main menu on the top a project explorer and toolbox on the left engine and scheduler windows on the bottom and a workspace in the remaining area containing scenarios and views fig 2 we now present some of the main class diagrams that are used in applications note that this paper does not contain an exhaustive list of all such class diagrams a project in urunme consists of visual nodes which are shown in the project explorer as a tree list the node abstract class provides the logic for the hierarchical parent child structure fig 3 each node in a project is derived from the abstract node class and requires a parent with any number of children a project is shown as the root node in the project explorer and is implemented using a singleton project class it provides a one point access to the presentation tier by exposing a number of properties and methods a project is stored as a binary file with a urm extension using data contract serialization in net a scenario class is used to implement all the scenarios in a project they are stacked on the top of the workspace so that only one can be selected at a time deemed the active scenario a view class represents a standalone window within the workspace that contains visual elements called panels e g diagrams charts etc the views are stacked as tabs on the left side of the workspace so that only one is visible at a time deemed the active view a panel is a window inside a view containing a visual control e g diagram chart grid etc all panels derive directly from the node class and implement the ipanel interface this interface provides the logic for storing the layout and visual attributes of the panels in a view 2 2 process and functions one of the core components in urunme is the diagram panel which is implemented by the process class as shown in fig 4 the term diagram and process are used interchangeably throughout the manuscript a process can also be called a network or a graph this component works as a canvas to create visual objects and implement all the diagram settings including properties for grid scroll size layers etc these visual objects are implemented by the symbol base class which exposes properties and methods for visual manipulation some concrete classes that are inherited from the symbol base class e g textbox label image shape etc are used as visualization objects in a diagram the function abstract class on the other hand provides the base for implementing all tasks e g data reading writing analytics etc each class that is derived from the function base implements its own business logic by overriding the run method which is called by the engine when the user commits a run link and port classes are used for creating any type of connector between two symbols in a diagram the process class is also inherited from the function class but also implements the ipanel interface therefore it is shown both as a symbol in a diagram and a drawing panel in a view a process does not have any specific business logic of its own and only runs its child functions when its run method is called processes can be either standalone or sub process as depicted in the hierarchical structure fig 4 a standalone process like process 1 has no parent however it can contain many other sub processes and functions a sub process on the other hand can have its own child sub processes and functions for instance in fig 4 when process 1 is run it first runs process 2 which essentially runs all members of process 3 before running its own functions and then returns the execution back to its standalone process this structure allows the organization of a large project into smaller manageable processes which can be run independently or as a part of the parent process 2 3 open modeling interface urunme provides built in functionality for running open modeling interface omi v1 4 compliant models the standard was seamlessly integrated into urunme by creating new classes derived from existing process and function classes figure a1 the coupling logic is implemented by reusing the code provided in the standard development kit sdk of the omi framework https www openmi org the model setup is similar to the default model configuration editor provided with the omi with sdk the coupled models are configured using a sub process called omiprocess located in the toolbox under openmi group the omiprocess is inherited from the process class and implements the diagram editor to drag and drop any number of omi models from the toolbox an omimodel is essentially a function that runs an omi model based on the omi file provided as an input omitrigger function provides the trigger to run the models omilink contains the settings for data linkage and is derived from the link class to represent the model connections in addition two more functions omiinput and omioutput are provided to exchange data between urunme and different omi model components both these function implement openmi s oatc openmi sdk wrapper iengine interface and therefore essentially expose linkable components any existing variable can be dragged and dropped onto an omiinput function to create an output exchange item based on variable name and quantity this output exchange item can then be linked to any model to provide input data at runtime similarly omioutput function can be used to create new variables which can be linked to any model as input exchange items these variables can then be used to read output data from a model component at runtime this feature provides a method for reading and writing data directly from different model components at runtime which can be extremely useful for any pre and post processing operation 2 4 parameters and variables the parameter class is the building block of the scenario management logic in urunme figure a2 it is used as a custom data type in other classes to specify the user defined attributes for example in a read excel function class attributes like file path name of the worksheet cell address number of rows to be read etc are all defined as parameters to run multiple scenarios in a project a parameter should be able to store a different value of an attribute for each scenario if required for instance an excel read function might be required to access two separate files in two scenarios this is implemented by providing an option of storing a value in both global and local contexts within the parameter class the global value is essentially the default value of a parameter value property which is used in all of the scenarios a local value on the other hand is scenario specific and defined by a generic directory values field in the parameter class say a user selects a particular scenario called s1 from the tabs at the top of the workspace and makes a parameter local in a function consequently whenever the function is run in the selected scenario s1 it will only use the local value of the parameter in all other scenarios where the parameter is not localized the function will continue to use the global value of the said parameter this logic allows any parameter to be made local for any number of scenarios to enter different values for an attribute all the data in a project are represented by scalar and vector variables shown as visual nodes in the project explorer variable class is inherited from the node to represent the data associated with a function data transfer in the software is carried out by a simple drag and drop of variables onto different functions or panels as inputs each variable contains a field class which defines all its attributes including variable type vector or scalar data type numeric text datetime time quantity length area volume flow flux etc unit format etc for example in order to read tabular data from an excel file the user is required to enter the attributes for each column once defined the function automatically creates its variables corresponding to the number of columns which are displayed in the project explorer however in some function child variables are created automatically where each output variable derives its attributes from the input variable for example the datainfilling function provides different built in methods for generating missing data and creates an output vector variable for each input vector variable the output variables generated by this function have the same attributes as the associated input variable including the variable type data type quantity and units therefore the output variable keeps a reference of the input variable in its field class when many different functions are created in a project the logic creates a chain between one variable derived from another and so on this provides a very useful mechanism to keep track of changes happening in different chains of connected variables in each scenario the state of each variable is defined by the state property fig 3 using different colors and is shown as bullet symbols alongside each node in the project explorer for instance when a single function is run in a particular scenario the state of its child variables changes to green or red to indicate success or error respectively consequently all the variables down the data chain are set to orange to indicate that a source has been changed and an update is required also when any variable changes its state all the parents up the tree in the project explorer will show the same state it must be noted that the state of any variable is scenario specific and may vary across scenarios depending upon the status of the data the state color coding is summarized in table 2 in fig 2 all the variables in the variables highlighted portion inside the project explorer section shown are green as is function 1 process 1 etc all the way up the tree in the scenario section scenario 1 is shown as green as it is a complete successful run whereas all the other scenarios are shown as gray not run yet in the engine section function 2 is showing as disabled and hence in the project explorer section the symbol beside function 2 is black 2 5 embedded database urunme packages sqlite as its default embedded database a new project automatically creates a single db file in addition to the urm for every function created by the user a new table is added to the embedded database using a function id with the tb prefix e g tb100 where 100 is the built in id of the function by default the new table only contains two integer columns id as the primary key and scenario id to identify data stored for different scenarios the remaining columns are created at runtime by a function one for each variable using its associated field name all scalar variables in a project however are stored in a single table called tbscalar created by default for every project the main benefits of sqlite are portability and speed compared to a conventional server database with an on disk format data are stored as a single ordinary disk file as opposed to a server database which requires dedicated server and administration this database can be easily shared among different urunme projects or external software an important requirement for urunme is to sift and sort lots of data in diverse ways before permanently storing the essential information into the database this is achieved by using the in memory temporary database provided by sqlite each function provides the option to either store the data directly in the project s permanent database file or to keep it in the temporary database for analysis this is helpful in cases where a large amount of data must be queried prior to storage in the database file the temporary database in sqlite remains in memory until it comes under memory limitations and must flush some data to the disk as a temporary file one of the downsides of using sqlite is its maximum allowable size theoretically the sqlite database can store up to 140 tb of data but in practice this upper storage limit is untested https www sqlite org limits html in addition the sqlite database cannot be used for concurrent writing by multiple users though simultaneous reading is allowed the advantages and disadvantages were considered during the design stage of the software ultimately the decision was made to choose simplicity speed and portability of sqlite over the larger data limit of a server database most typical environmental models will not reach the 140 tb limit except in cases where there are many model runs stored in the database many large models such as climate prediction models have their own built in run processes and are highly unlikely to need to use software such as urunme 2 6 unit converter another feature of urunme is the built in unit converter there are a number of different quantities defined in the software e g length area volume speed flow flux radiation etc using quantity enum figure a3 for each quantity a quantitydef class contains a number of units e g m ft etc for length quantity implemented by the unit class which specifies the relevant conversion factors numeric variables contain user defined quantity and unit parameters and call the associated quantitydef in the unitconverter class for unit conversion a detailed explanation of different key features and capacities of urunme is provided in appendix b 3 case studies this section describes water resources case studies for urunme ranging from simple data analysis to advanced model coupling and interactive dashboards the examples discussed here are from real world projects involving different applications of integrated models to different types of watershed analysis such as forecasting in potable reuse systems and climate change project files for some of the case studies are available for download at https www urunme com resources the first case is described in more detail to illustrate how different processes are set up in a urunme project the remaining project examples are described more concisely and include only those details that are not shown or used in the previous examples 3 1 data analysis urunme can be used for in depth data analysis owing to the embedded sql and r language support and other built in functions both ordinary data units conversion data read from websites and specialized statistics tests the current release of urunme contains most functions that are likely to be used by water resource professionals a case in point is the low flow frequency analysis carried out on a stream called st10 in the occoquan watershed located in northern virginia usa in an attempt to calculate drought conditions under different return periods in this method first annual minima are derived from daily flow by selecting the lowest flow every year using different averaging durations e g 1 7 10 etc days an n day moving average is progressed through the daily data to calculate the timeseries for each duration and then yearly minima are calculated hisdal and gustard 2008 a cumulative distribution function cdf is developed using the yearly minimum flows low flow for any return period t can then be calculated by selecting the desired exceedance probability p on the cdf where t 1 p for instance a commonly used standard called 7q10 represents the minimum 7 day flow expected to occur every 10 years the following example illustrates how a different set of features in urunme can be used to calculate the low flows in real time using online flow data from a stream station fig 5 shows the process for low flow frequency analysis for the duration of 7 15 30 60 90 and 180 days over a return period of 2 5 and 10 years p 0 5 0 25 and 0 1 respectively this demonstrates that once low flows are calculated for any given return period different scenarios can be created to calculate other return periods by simply localizing one variable a formula function called global is used to create one variable called p which is kept local for each scenario to define the return period flow data are obtained from three different sources the historical function reads a text file obtained from a usgs united states geological survey website containing the daily flows for this location from 1968 to 1980 read st 25 30 reads daily flows from a visual foxpro database for stream stations st25 and st30 located upstream of st10 for a time period between 1980 and 2003 before st10 was commissioned the fill 25 30 function uses interpolation to fill in any missing data in the timeseries the compute st10 function contains a formula to extrapolate missing st10 data between 1980 and 2003 using flows from st25 and st30 based on the drainage area of each station st10 st25 2 166 st30 the st10 function reads the data for the remaining time period from the database for the online stream station st10 the qaqc quality assurance quality control function contains an sql statement to remove any duplicates and also sort the data by date in case there is any discrepancy the daily flow function aggregates the data from st10 to daily flows by using a time weighted average fill st10 infills any missing data using interpolation all flow contains a sql statement using union to add the three different sources of data and create one single timeseries from 1968 to the present date figure a4 the moving avg formula function creates moving averages from the final timeseries for each time duration figure a5 yearly min aggregates the moving averages for each duration to get yearly minima finally low flow uses an r script to create a cdf for each duration and returns low flow for a given p defined in global using inverse cdf figure a6 once this process is created and tested any number of scenarios can be created to get values from different return periods by changing the value of variable p in each scenario run all which is an execute function is used to conveniently run all the scenarios sequentially figure a7 a button can be added in the diagram to run the execute function if a more dashboard type application is desired the two charts shown in fig 5 shows how scenario management is implemented in urunme the low flow function produces two vector variables x for return periods and xqy for low flows if the variable xqy is attached to the chart using an active attribute each time the user changes the scenario the chart will update itself using the values from the active scenario left chart in fig 5 showing results from the active scenario xq2 to compare values from all three scenarios at the same time the variable xqy should be dragged and dropped three times on the right chart from the project explorer each with its own scenario name attribute figure a8 this will create three different timeseries each for one scenario right chart in fig 5 this particular chart will not update every time the active scenario is changed by the user unless the underlying data or processes are changed and the scenarios run again this case study shows a very simple example of data analysis and how different scenarios can be used in urunme more sophisticated analytics can be performed using similar techniques 3 2 web data download urunme can be used to download data from different websites using various file transfer protocols on a periodic basis for real time modeling analysis and visualization the following example is adopted from a futurecasting project where forecasted streamflow data are downloaded from the national water model nwm https water noaa gov about nwm ftp file transfer protocol server on a daily basis fig 6 nwm is a hydrological model that forecasts stream flows across the continental united states and has been in operation since aug 16 2016 on the national oceanographic and atmospheric administration noaa supercomputing system a long range 16 member ensemble forecast is produced every day going out 30 days in the future with 4 ensemble members in each cycle at 00 06 12 and 18 utc fig 6 shows the process created in urunme used to download and query the 4 ensemble members produced at 1800 h each day a new directory is created in the nwm ftp url ftp ftpprd ncep noaa gov pub data nccf com nwm prod named based on the production date with a nwm prefix e g nwm 20180805 a formula function is used to create the url and local paths shown using the following expressions in table 3 also in figure a9 now formula gets the current date and sub datetime timespan subtracts a day from it text datetime format converts the date to text using the given format the concatenate text1 text2 function is then used to create the remote path ftp url and local directories where each ensemble is stored the downloader function is used to download each ensemble in netcdf format and store them into their respective directories using the parameters shown in figure a10 a total of 120 netcdf files are downloaded for each ensemble the parameters remotepath and localpath in the downloader function are set to use variables and the corresponding variables from the function paths are used as inputs simple masks are used to include or exclude files or folders from the download four read netcdf functions one for each ensemble are used to read the downloaded files the read netcdf function has the option of reading either a single file or an entire directory of multiple files and then combining the data to create a single timeseries flow data for the required stream are extracted from these netcdf files by providing the stream identification numbers as defined by nwm once the data are read into the temporary database formula functions are used to calculate min max and means of the four ensembles before storing that information into the embedded database using the write database function a scheduler is used to run this process automatically on a daily basis 3 3 iterative approach for calibration of models urunme can be used to run models recursively to calibrate them to user defined criteria although complex multi step calibration objectives can be used a single test single objective example is shown here from a reservoir modeling project to demonstrate the basic concept ce qual w2 is a two dimensional hydrodynamic and water quantity model developed by the u s army corps of engineers to simulate the water quantity and quality in a waterbody cole and wells 2011 the simulated water elevation from the model is often not well calibrated the model does not account for water loss due to seepage etc and requires external calculations to create artificial tributary inflows outflows for water balance a water balance utility is provided in the ce qual w2 package which requires a text file el obs npt containing observed water elevation as an input for the simulated time period the model is required to be run multiple times and each iteration must be followed by running the utility to calibrate the water elevation a looped process is created in urunme to run the model and water balance utility successively fig 7 until a desired performance is achieved for example the absolute value of the difference between the model computed water elevation and the observed value is less than a specified value for each day of the simulation the process starts with a formula function to initialize a variable called count to keep track of the number of times the model is run observed water elevation is read from the visual foxpro file pool db for the desired time period by a read database function and is then written to the el obs npt file by a write text file function in the desired format the ce qual w2 model is run by execute figure a11 and then followed by calibration to calculate different calibration performance indices as scalar variables such as r squared rsq rsq variable is used by the calculate formula function to evaluate the following two expressions count count 1 stop if count 10 or rsq 0 9 1 0 the count is incremented and an if statement is used to calculate a scalar variable stop the condition shown above is just for demonstration purposes any logical statement can be used instead to be used by the decision function to break the loop and go to the end this is achieved by dragging and dropping the stop variable in the decision properties form when variable is 1 the loop breaks in case the criterion is not met the process goes into the loop and executes the water balance utility a similar technique can be used for calibrating models by replacing the water balance utility with any other optimization routine or objective function the next section describes a case of a more complex model and calibration process 3 4 loose model coupling the occoquan model is a set of computer models linked together to simulate the watershed s hydrology and water quality xu et al 2007 the occoquan watershed is located in northern virginia near washington dc the model consists of seven watershed models hspf and two reservoir models ce qual w2 the coupling of these models is achieved by using the output from the upstream model as input for the downstream model in the past the entire modeling process had a number of manual steps particularly where data outputs from one model had to converted from english to si and vice versa units to provide input for the next model and having to perform the appropriate statistical tests to calibrate a particular model before moving on to the next one mainly this was due to a failure to develop modeling tools to aid the modeler the result was a huge amount of time and effort to carry out the simulation process which took days if not weeks to complete although some parts of the process had been made easier to run using macros a more complete solution was desired urunme development was started and in 2017 the entire occoquan model was migrated to urunme a dashboard was created for visualization of different inputs and outputs using multiple views and panels for the multi step coupled model each task is accomplished through a dedicated process there are two main standalone processes in the project observed and models the observed process contains multiple processes that use hundreds of functions for reading data from different sources database text excel wdm etc these data sources include meteorological data from 14 weather stations in the watershed flow and water quality data from stream stations water elevation and water quality data from reservoir stations water quality data from a water reuse plant oxygen flow rates from the hypolimnetic oxygenation system and water withdrawals from a water treatment plant various data analyses and manipulation operations are performed in these processes to develop the required input for each model these include qaqc of data infilling missing data timeseries aggregation unit conversions load calculations etc to keep the database size small most functions use the temporary database and only processed information is stored in the embedded database the model process is used to run all nine models sequentially as shown in fig 8 each sub process in the diagram runs one model and consists of multiple other sub processes and functions the data created in the observed process is written to a different input text file of models before running them sequentially output data from one model is transferred and output units converted as needed to the downstream model and saved in the embedded database to create outputs for the dashboard the entire simulation process is run with the click of a button and takes less than 2 h to complete prior to using urunme when each model was run individually and unit conversions performed by importing the data into excel and then re exporting it in the format for the next downstream model even a fast worker would take the equivalent of 24 h to do a complete run moreover the potential for error was greater as both hspf and ce qual w2 expect data in particular columns so a one column shift in exporting the data for example would result in a bad run and have to be repeated sometimes these human errors were caught much later than one would want this is not counting the time it took to make the various charts and plots to determine how the models were performing in the calibration application project each model is run recursively until the objective function is satisfied before urunme proceeds to the next model in the sequence the objective function for the ce qual w2 models is the same as for the case described in the previous section a matching within tolerance limits of the water surface elevations both modeled and observed for the hspf models objective functions could be obtaining a particular goodness of fit using whatever method the user wishes for one or more variables for example flow and concentration of a constituent 3 5 openmi based coupling this case study is presented primarily to illustrate the implementation of openmi framework within urunme application it also explains the mechanism for direct data exchange with the openmi component for example inputting and outputting data directly to a model component at runtime openmi standard is a software component interface which facilitates the data exchange between different models on timestep by timestep basis which is especially useful for simulating interacting environmental processes this case study is based on the simple river model provided with openmi v1 4 user manual and shown in fig 9 the left panel on the screen shows the parent process which has two functions to read and write data from two separate excel files the omi process sub process is expanded in the right panel which shows the openmi model composition the omimodel function simply requires the path of simple river model s omi file to load the linkable engine dll figure a12 the two variables date and inflows read by the readexcel function are dragged and dropped on the omiinput function that automatically creates an output data exchange item inflows within omiinput function figure a13 which can then be linked to branch inflow item of the model component using a link figure a14 similarly in the omioutput function a variable called outflow is created using output tab in the properties form which essentially works as an input data exchange item this variable is then linked to the required branch flow item of model components figure a14 the lists of available input and output data exchange items for a component are shown in the properties form of the function figure a12 the data exchange linkages between different omi functions can be created in the properties form of the visual link figure a15 at runtime the simple river component directly reads inflows from the omiinput function and omiouput pulls outflows from the model component this example shows how the openmi framework is seamlessly embedded in urunme as a sub process in future similar sub processes can be created for other frameworks and can be executed with any pre and post processing operations this architecture even allows different openmi sub processes to be run and exchange data sequentially if required 3 6 decision support system this case study presents in summary form a decision support system developed in urunme for the upper occoquan service authority uosa water reclamation plant wrf in northern virginia a complete treatment of the dss is described in lodhi et al 2019 the wrf discharges its reclaimed water into a tributary of the occoquan reservoir fig 10 fairfax water s griffith water treatment plant wtp withdraws water from the tail end of the reservoir at the dam the contribution of the wrf to the reservoir s inflow varies and can reach as high as 80 of the reservoir inflow during extremely dry months one of the key challenges for uosa is to regulate the nitrate concentration in reclaimed water throughout the year during thermal stratification and extremely low hypolimnetic oxygen concentrations in the summer the supply of nitrate from uosa actually benefits the reservoir by providing nitrate as an alternate electron accepter to oxygen with bacterial activity thus reducing the nitrate to nitrogen gas and thus preventing the reservoir bottom from becoming anaerobic and preventing the release of phosphorus iron manganese and ammonia therefore the occoquan policy virginia state water control board 1971 coupled with uosa s operating permit allows for high nitrate concentration in the reclaimed water during summers by limiting uosa to a total mass rather than concentration of all nitrogen species during a calendar year without a discharge limit on nitrate during the summer months of stratification provided the nitrate nitrogen concentration at the occoquan dam does not exceed 5 mg l in which case uosa must start denitrifying the excess nitrate this is achieved at the plant by minimizing the denitrification process in summer however during winters the reservoir loses its denitrification capacity due to the lack of stratification and the presence of dissolved oxygen from top to bottom waters and the plant has to significantly reduce nitrates in the discharge by increasing in plant denitrification past experience has shown that uosa has sometimes had difficulty in maintaining low effluent nitrate concentrations during winter mostly due to operational constraints including limited organic carbon and low water temperatures which hinders the denitrification process in some cases methanol is required to be added to improve the denitrification process resulting in a substantial increase in the operational cost the effect of higher discharged load from the wrf on the nitrate concentrations at the dam depends on many factors including the pool elevation volume and quality temperature background nitrate concentration etc of stream inflows withdrawal by the water treatment plant and weather conditions therefore it would be extremely beneficial for the wrf to identify strategies to manage the wrf performance dynamically in response to future reservoir conditions lodhi et al 2019 the uosa dss was developed to capture the dynamic nature of nitrate loadings to the reservoir both from natural streams and the wrf withdrawals by the wtp wrf effluent flows wrf plant operations and the denitrification capacity of the reservoir and help manage the nutrient levels in the reservoir it is based on an integrated reservoir model ce qual w2 that uses data from a real time climate forecast system cfs nwm and observed data to predict future reservoir conditions the resulting application provides valuable feedback to plant operators to correctly target the effluent nitrates using a biochemical operation and optimization model called iviewops sen et al 2018 each week the dss moves forward in time by predicting the reservoir water quality based on the most recent initial and future boundary conditions when the forecasted water quality starts reaching the 5 mg l nitrate n threshold at the dam 20 miles downstream from the uosa discharge location the dss automatically runs an optimization routine on iviewops to determine the most suitable operational strategy to reduce the nutrient discharge from the plant by changes in configurational and operational setpoints for each run urunme reads data from various data sources dynamically data being continuously updated in different databases and performs extensive analysis and manipulation before feeding it to the model as inputs the overall data flow path to operate the integrated model is shown in fig 11 multiple future scenarios based on different combinations of natural stream inflows plant effluent flows and nitrate concentrations and water withdrawal by the wtp are simulated the future weather conditions are created using the forecasts obtained from the cfs a 30 day forecast predicts the reservoir conditions based on the nwm stream flow forecast going out 30 days a 90 day forecast on the other hand is used as a what if analysis to simulate the effects of any possible future winter drought the model is run multiple times to simulate each scenario first the base scenario is run for the last three years to evaluate the current model calibration using the last three years of observed data then data for each of the forecast scenarios are created and the model is run sequentially using the present day boundary conditions from the base scenario as the starting point the forecasts can be critical to wrf operations because the switchover from a nitrifying to a denitrifying mode can take up to two weeks the nitrate profiles shown in fig 12 are extracted from the dss output generated on october 10 2018 an artificial dry condition simulated by creating an artificial drawdown of 2 m from full pool level for a 90 day forecast the model run showed no significant change in the nitrate during the 30 days forecast shown with green background as nwm predicted relatively higher stream flow during that period which reduced the uosa contribution to the total inflows causing higher dilution however for the 90 day drought forecast it was predicted by the dss that an effluent nitrate n concentration of 12 5 mg l as n green line by the wrf would push the nitrates to the 5 mg l as n limit in the forecasted period for both the w2 scenarios therefore a lower effluent nitrate concentration should be targeted the above results demonstrate how the dynamic nature of the system can affect the reservoir based on different boundary conditions different conditions have significant effects on the outcome for instance colder than normal air temperatures in fall or winter can increase the nitrate concentration in the reservoir by reducing the denitrification capacity of the wrf on the other hand any significant rain event can washout the entire reservoir thus diluting the nitrate concentrations to much lower levels fig 13 shows one of the many views created in a dashboard application for the dss two main sets of buttons on the left are used to open a specific view from the multiple views available another view from the same application shows the output from the process simulator fig 14 iviewops reads online data directly from scada supervisory control and data acquisition system and the wrf laboratory to optimize the plant using different predefined combinations of process configuration tanks in service mixed liquor recycle dissolved oxygen wasting supplemental carbon etc while specifying certain constraints on the effluent quality in fig 14 observed data from the wrf laboratory are shown as red field sensors as green and the optimized results from the biochemical simulator as blue 4 summary commonly the term integrated modeling only refers to the coupling mechanism used to automate the data exchange between different linked models however most such applications require extensive pre and post processing for developing an automated integrated model urunme was developed with the goals of facilitating model coupling and to automate the entire simulation process creating inputs from different data sources reading outputs analyzing data making statistical inferences and ultimately showing results in a meaningful and interactive way it serves as an intelligent layer between the users models and data sources practically hiding the underlying mechanics of the complex simulation process we have described much of the core functionality of the software above and in the appendix our objective was to build a framework that would be able to integrate models data from a variety of sources including web databases the ability to do statistical and other analyses and provide tools for the user to generate appropriate data visualizations although urunme was developed to help automate these functions for the occoquan model from the start our efforts were to not particularize the software for that system but to create a framework which would run a variety of environmental models and indeed could be used for other types of models too there are many models in a modeler s toolbox some are widely used such as hspf and ce qual w2 that we use in our occoquan model and other are less so including research models that are still under development many public domain models again such as hspf and ce qual w2 but also many others such as the hec series swat wasp etc are well accepted but often lack the post processing tools that a modeler needs these tools include statistical analyses goodness of fit tests and many different types of visualizations there are commercial packages available that include these public domain models and provide some pre and post processing tools but they are often limited to one software there are commercial models that provide some excellent tools but they are often expensive packages and because the code is not available cannot be modified if needed for a particular application moreover environmental models come in different scales and types the iviewops package mentioned in this paper is used for simulating the functioning of a wastewater treatment plant and can be used to optimize the treatment process towards an objective trying to couple such models with watershed scale models to create a decision support system for indirect potable use which is the system we have used in the dss case study in order to predict water quality some miles downstream can be a challenge similarly keeping track of downscaled global climate model data to use in a watershed model under various scenarios can take up a lot of a modeler s time just keeping track of the data add to that that the output of many models may require unit conversions before it can be used in other models and the software framework described herein we hope will be of use to modelers finally our efforts were also particularly focused on academic research as modelers in academia we often cannot afford to purchase expensive software to do everything we would like students often have to use models both big and small in their classes and then have to import the output into something like excel or matlab in order to draw charts and figures not everybody has free access to such software we wished to remove the drudgery that we had at one time or another experienced with modeling and to allow the modeler to focus on the application and its results rather than on the manipulation of the data necessary to perform a statistical analysis or visualization urunme was partially prompted by this need too and is free for academic use more functionality and features will be added to urunme based on user feedback support for several new data formats implementation of other modeling frameworks upgdrade to openmi 2 0 and integration of geographic information system gis mapping features are also part of the future software development plan declaration of competing interest no conflict of interest acknowledgments the authors acknowledge the support of the owml occoquan watershed monitoring laboratory family including the field and lab staff students and faculty members we would also like to extend our appreciation to the late dr thomas j grizzard for his unwavering commitment in the establishment and operation of the occoquan program and the owml for over 40 years this work was started during his leadership of the owml and much of the data used for the case studies were gathered during his tenure the principal author lodhi was supported during his phd work by funds from the occoquan modeling project locally funded by jurisdictions in the watershed fairfax county fauquier county loudoun county prince william county ciry of manassas and city of manassas park and fairfax water urunme development was however not funded by any particular source but evolved out of a need appendix a table a1 list of third party libraries used in the development of urunme table a1 no name purpose link 1 dotnet framework 4 6 general https www microsoft com en us download details aspx id 53344 2 devexpress winforms gui https www devexpress com 3 northwood software godiagram diagrams https www nwoods com products godiagram 4 extreme optimization math statistics https www extremeoptimization com 5 sqlite embedded database https www sqlite org index html 6 ms sql server supported database system data sqlclient net 7 mysql supported database mysql data oracle https dev mysql com downloads 8 oracle supported database oracle manageddataacess by oracle https www nuget org packages oracle manageddataaccess 9 r r language https www r project org 10 openmi v1 4 openmi https www openmi org 11 ncalc expression parser https archive codeplex com p ncalc 12 winscp file upload downloads https winscp net eng index php 13 quartz scheduler https www quartz scheduler net fig a1 class diagram for a process which works as a canvas to create visual objects including symbols functions and other sub processes fig a1 fig a2 class diagram of parameter and variable fig a2 fig a3 class diagram for classes used in unit conversion in urunme fig a3 fig a4 sql function properties form showing a query tab fig a4 fig a5 formula function properties form showing expression tab fig a5 fig a6 r function properties form showing script tab fig a6 fig a7 execute function properties form showing the process low flows selected to run in three different scenarios in batch mode fig a7 fig a8 chart input tab showing different variables shown in the chart fig a8 fig a9 formula function showing the variables defined for downloading nwm forecast fig a9 fig a10 downloader function showing parameters defined to download nwm forecast fig a10 fig a11 execute function properties form showing the parameters required to run an external model fig a11 fig a12 omimodel function properties form showing output and input exchange items for simple river model fig a12 fig a13 omiinput function properties form showing output item automatically generated for the input variable inflows fig a13 fig a14 omilink properties form showing an openmi link between omioutput function and simple river model component for outflows fig a14 fig a15 omilink properties form showing an openmi link between omiinput function and simple river model component for inflows fig a15 appendix b features and capabilities to create a new project in urunme the user has to specify the project directory the purpose is to ensure that all the data used in a project are contained in one directory and its sub directories for easier exchange between different computers a user can override this restriction folders in a project can be created using the toolbar in the project explorer to group different types of nodes a new view or scenario can be added by the insert group in the main menu and is shown in the project explorer under the views and scenarios folders respectively any view or scenario can be made invisible to the workspace by using show hide button in the associated context menu two types of views are provided in the software tabbed and widget tabbed view displays panels as tabs and only one panel can be shown at a time in a view similar to worksheets in ms excel widget view on the other hand allows the panels to be docked on the screen in any layout which may be more suitable for dashboard like presentations diagram process panel a new standalone process can be added from the main menu and shown in the project explorer under the active view sub processes can be added to a process diagram using the general group in the toolbox and are shown as a child of parent processes in the project explorer to view the process panel the show hide button can be used in its context menu symbols the main menu contains a number of items for visual manipulation of symbols in an active diagram which are divided into different groups layers format edit and size different layers can be used to group different symbols in a diagram which can then be made either invisible or locked some objects derived from the symbol class are used purely for visualization purposes in a diagram and are available in the miscellaneous group in the toolbox the label symbol is used to write text on a diagram the shape symbol provides hundreds of built in figures which can be used to create different shapes the button symbol is used to create interactive applications as explained in section 3 4 the image symbol is used to insert an image in a diagram any number of ports of various shapes and sizes can be created for the image symbols by using the port button in its context menu the textbox symbol is similar to a label but used for scalar variables it can be created by dragging and dropping a scalar variable from the project explorer to a diagram and automatically updates itself if the input variable changes its state functions urunme uses simple flowchart type diagrams to set up a process every function has a context menu that can be displayed by either right clicking on it or its corresponding node in the project explorer basic functions such as start end converge and decision are used to define the execution path the decision function for instance is used to run desired functions in a loop until designated criteria are met advanced functions are used to perform more dedicated tasks and are available in different associated groups in the toolbox links can be used to connect functions to create an execution flow path for the engine by default the ports on a new function are hidden and every process runs its children sequentially based on their order in the project explorer the inputs for any function can be specified in the properties form shown by clicking the properties button in the context menu of the function as previously explained each input parameter in a function can have different values for different scenarios a parameter can be made local by right clicking on its associated editor e g textbox checkbox combobox etc and click use local value in its context menu this will change the backcolor of the editor to blue and an icon image 16 will appear right next to it figure b1 now any value entered by the user will only be applicable for that specific scenario while the rest of the scenarios will continue to use the global values a variable can be used as an input to a parameter by right clicking on its editor and selecting use variable in the context menu it will show anicon image 17 next to the editor indicating that this parameter is now attached to a variable figure b2 any scalar variable can then be dragged and dropped onto the editor from the project explorer it must be noted that the default value of a variable is always based on the active scenario hence when a variable is attached to a parameter a combobox or dropbox appears next to the parameter editor and sets active as the default scenario figure b3 however if the variable is desired to have a fixed value from a scenario specific pesrpective regardless of whichever scenario is active that scenario should be selected from the drop down instead combobox contains the list of all scenarios in the project fig b1 read excel function properties form showing general tab with localized file parameter fig b1 fig b2 read excel function properties form showing file parameter ready to be attached to a variable fig b2 fig b3 read excel function properties form showing file parameter attached to a variable with a dropbox to select the scenario fig b3 for all functions that create user defined variables the output tab in the properties form of the function is used to define the required attributes figure b4 for example to read an excel file a user is required to define the name variable type data type and cell reference in the excel sheet for each variable to be read these attributes may vary from function to function quantity and unit are also required in cases where the variable is numeric fig b4 read excel function properties form showing the output tab fig b4 in functions e g write excel file or panels e g charts where variables need to be added as inputs a user is required to simply drag and drop the desired variable on the input tab in the properties form figure b5 and then set the name unit if required and scenario as explained in the above paragraph attributes each function also has a settings tab in its properties form which contains a parameter to disable the function so that it cannot be run by the engine in addition it contains two more parameters for the functions that handle data the cache parameter is used to set whether the data are kept in the temporary or file database and the insert mode parameter defines how data are added to a table for insert mode selecting replace deletes all the existing rows for the given scenario before adding new data while append just adds new data rows after existing data fig b5 write excel function properties form showing the input tab fig b5 three language functions sql https www sqlite org r https www r project org and formula are provided under the editors group in the toolbox the sql function is used to query the embedded database external databases can also be directly queried using the read database function described in table 1 for example select from tb100 will query the table created for a function having an id 100 queried columns can be used to create new variables in the output tab by using the same names in the name attribute as used in the query each time the sql function is run it executes the query and updates its variables by default every query is automatically run only on the active scenario however the user can uncheck the use active scenario parameter in the general tab of the sql function to query the entire table in addition to querying values the user can also write non queries queries that do not return any data and performs an action e g deleting some rows by checking execute non query one of the features in the sql function is the option of using other scalar variables in the query text for instance any scalar variable say a date variable named x can be added to a sql function by using the input tab if there is a date column in a table say tb100 it can be queried using select from tb100 where date x to get all the rows having a date greater than x this feature essentially makes a query dynamic in nature and every time it runs the data are queried based on the current values of the variables embedded in the query text the r function has an embedded r engine which is a widely used statistical language with a host of third party libraries and functions r combined with sql provide useful data analysis and statistical capabilities in urunme similar to sql any variable can be used in the r script by adding it to the input tab output variables are defined to use the return values from the r engine with the same names as defined in the r script in addition a separate formula function is provided to write mathematical and logical expressions on the variables this function incorporates a large number of excel style formulas e g abs cos sin etc the reason for including this function in urunme is to provide additional built in formulas which are not readily available in sql or r furthermore due to internal processing these formulas are generally faster in performance compared to sql and r which require external engines to run more formulas can be added to this function in the future based on user feedback the execute function is used to run the external applications e g models utilities software etc the path of an executable file is required as an input including any command line arguments accepted by the file a check box is used to tell the function if it should wait until the external application exists before handing back the control to the urunme engine a maximum runtime parameter is also provided to terminate the execution if the external application is taking more time than specified in case the external application has stopped working due to error execute can also be used to run different scenarios of a project in batch mode by setting the scope parameter to internal in the general tab of the function any number of processes and functions can then be dragged and dropped within execute and the run scenario can then be specified for each of them using the associated drop down menu by initiating execute these functions are run one by one in the specified scenarios in addition to the above urunme contains a number of other functions to perform various tasks pertaining to data read and write file i o timeseries manipulation statistics etc a brief description of each function is provided in table b1 more functions will be added in the future based on requirements and user feedback table b1 brief summary of various functions provided in urunme table b1 no function description 1 read database read write data from different popular database types including foxpro sql server mysql oracle sqlite etc 2 write database 3 read text file read write data from text files comma separated values space delimited tab delimited custom delimited and fixed width 4 write text file 5 read spreadsheet read write data from an excel spreadsheet 6 write spreadsheet 7 read wdm read write timeseries from watershed data management wdm format used in hspf 8 write wdm 9 read netcdf read netcdf a multidimensional array oriented binary format generally used for meteorological data 10 read grib2 read grib2 a concise binary data format generally used for meteorological data 11 read blob read blobs binary large objects e g images media files etc 12 write template write scalar variables to different text files using tags this feature is used to automatically change input parameter for models which use text files as input 13 execute run any external executable file or internal processes under different scenarios 14 sql to write queries on the embedded sqlite database using sql 15 r to write r code using the embedded r engine 16 formula to write mathematical and logical expressions using built in formulas 17 file io provides file io functionality e g creating copying and deleting files and folders 18 downloader provides a range of features for downloading files from web servers 19 uploader provides a range of features for uploading files to web servers 20 emailer provides a simple way for emailing 21 data infilling fill missing data using different methods and or de aggregate timeseries 22 aggregate aggregate timeseries using average sum time weighted average and time weighted sum 23 match match two different timeseries 24 omimodel run an openmi compliant model 25 omitrigger provides an openmi trigger 26 omiinput to input data directly to an openmi model component at runtime 27 omioutput to output data directly from an openmi model component at runtime run engine the engine is responsible for running different functions and displaying the run status in the window at the bottom of the gui any process or function can be run by selecting the run button in the engine group in the main menu or from its context menu in the diagram or project explorer moreover model runs can be configured for a given date and time or on a periodic basis using scheduler which can be added to a project from the engine group in the main menu details for next scheduled run and remaining time for each scheduler are constantly updated in the scheduler window located next to the engine tab the engine runs different functions of a process sequentially however different functions can be run in parallel on separate threads if more than one execution path is available by checking the allow parallel processing parameter in the general tab of the parent process properties form panels different panels can be added to a view by using panel group in the main menu a variety of charts are provided with extensive customization options figure b2 including the option for creating multiple series panels built in panels in a chart legends axis annotations point labels etc in addition charts provide various built in data analysis tools including linear regression error bars summaries moving averages etc data can be added to the chart by dragging and dropping any number of vector variables to the chart itself or the input tab in its properties form one of the features of the chart is its ability to dynamically update its different text properties e g titles axes legends etc using scalar variables as input for example a text variable can be used as a chart title using the annotation tab in its property form each time the variable state is changed the chart title will be updated automatically grid panel is used to show data in tabular form it provides a rich set of built in sorting grouping filtering and data searching tools in addition to a simple grid pivot grid panel is also provided to create pivot tables for multidimensional data analysis all of these panels update automatically if there is any change in the underlying data fig b6 chart wizard in urunme fig b6 gauges can be used to show scalar variables on the screen for a higher level of data visualization different types of customizable elements are provided to imitate real gauges including circular angular linear thermometers digital etc figure b7 gauges are very useful for creating dashboard type applications to mimic screens used in supervisory control systems fig b7 a sample of available gauges in urunme fig b7 image panel is used to display an image on the screen urunme provides the option of reading and storing blobs binary large object e g images media files etc which can be read using the read blob function these blobs are similar to variables and can be dragged and dropped onto an image panel or as symbols in a diagram any time the read blob function is run the associated images are updated in the database and the panel or symbol automatically updates its image this functionality can be really useful in applications where externally created outputs e g charts and diagrams from different models are shown as images on a dashboard in urunme when a large amount of data are processed in a simulation it is very difficult to monitor the changes in different variables the event manager panel provides a very simple mechanism of data monitoring and can display any message warning or alarm any number of events can be specified in the event manager where each event contains a scalar variable user defined message and a logical expression as a trigger figure b8 the logical expression is evaluated every time its associated variable changes its state and if true the message is displayed on the event manager screen figure b9 for example this feature can be used for indicating a need for model calibration if certain performance indices are above or below a threshold fig b8 event manager properties form showing how an event is defined fig b8 fig b9 event manager panel showing a triggered event display when the evaluation expression becomes true fig b9 commands a useful feature of urunme is its ability to create interactive dashboards where different actions can be defined in response to user initiated triggers e g mouse click key press etc this is accomplished by adding different commands to any visual element in the software panels symbols functions etc commands can be configured by using the commands button in the context menu and specifying the different attributes figure b10 any predefined action e g run a function show or hide a view or panel change size move or rotate an image create some animation etc can be defined in a command in response to triggers e g mouse click key press etc for a target visual element in addition a logical expression can be specified in a command using a scalar variable to override the trigger if specific criteria are not met fig b10 commands properties form for a button showing definition of a run command fig b10 
25950,a new software application called urunme was developed in c net for integrated environmental modeling urunme provides functionality for model coupling data storage analysis visualization scenario management and decision support systems a graphical user interface gui based on drag and drop flowchart type diagrams allow modelers to set up a model application once and then focus on running and analyzing results without needing to further address housekeeping details data are stored in the embedded sqlite database interactive dashboards can be created using multiple screens and visual elements the built in scenario management feature provides the capability of running multiple scenarios of a model using different input parameters initial and boundary conditions this paper presents urunme s architecture and describes how it can be utilized for creating automated integrated modeling applications by domain experts a demonstration of capabilities is shown in a series of case studies of increasing complexity in the water and wastewater applications keywords integrated modeling multi model coupling decision support system data analysis data visualization scenario management 1 introduction integrated solutions and higher order systems thinking are required to deal with increasingly complex real world environmental problems laniak et al 2013 integrated environmental modeling iem provides a comprehensive approach to environmental management by evaluating different components of a system in a holistic manner according to johnston 2010 integrated modeling encompasses a broad range of approaches and configurations of models data and assessment methods to describe and analyze complex environmental problems often in a multi media and multi disciplinary manner over the years the interest in iem and more specifically integrated water resource modeling has increased significantly kalbacher et al 2012 molina et al 2010 these iem applications can be used as an effective tool for design planning risk assessment decision making and enhancing the transparency and collaboration between stakeholders johnston et al 2011 peach et al 2011 knapen et al 2013 noted five approaches for the model integration 1 soft linking 2 scripts 3 proprietary one large monolithic model 4 proprietary loosely linked models and 5 standard modeling framework however we broadly categorized the integrated modeling applications into 1 built in integrated models and 2 coupled integrated models 1 1 built in integrated models the first category provides built in integration to simulate a complex environmental system in this method the model integration is generally achieved by using two different approaches 1 models for the different sub systems are contained within the principal software 2 various models developed by the same organization are coupled based on some proprietary linking mechanism one example of this approach is software called weap water evaluation and planning which was developed by the stockholm environment institute s us center sei us to assist policymakers in evaluating water supply policies and suitable water resources plans yates et al 2005 another such example is basins better assessment science integrating point and nonpoint sources that was developed by the u s environmental protection agency usepa 2015 it is designed to model meteorological condition streamflow and pollutant transport across watersheds by using embedded hydrologic and pollutant fate and transport models basins contains several useful components that allow users to develop integrated models and perform watershed analysis and data compilation within the software 1 2 coupled integrated models coupled applications on the other hand are based on linking different standalone models to simulate various components of a targeted environmental system these models are linked together based on two different approaches 1 loose coupling or 2 framework based coupling 1 2 1 loose coupling loose or soft coupling is a sequential approach where data flow paths are unidirectional and configured in a tree like cascade structure rauch et al 2002 the individual models are linked together such that the output of one model is used as an input to the other with perhaps some unit conversions performed as an intermediate step the models used in such applications provide readable input and output in an open data format baran et al 2018 malard et al 2017 xu et al 2007 the chesapeake bay watershed model is one such example in the water resources domain where various applications of hydrological simulation program fortran hspf of sub watersheds are linked together to simulate the entire watershed hopkins et al 2000 1 2 2 framework based coupling in contrast tight coupling generally involves linking different standalone models using standardized data transfer protocols based on some commonly accepted standard framework common frameworks used in environmental modeling are generally categorized into either 1 component based modeling or 2 service oriented architecture soa based modeling belete et al 2017 in component based modeling each model is run as an independent component and data exchange between different models is achieved using a standard interface goodall and peckham 2016 over the past years a number of component based frameworks and interface standards have been formulated and applied in various domains including water resources buahin and horsburgh 2018 for instance earth systems modeling framework esmf developed with the focus on weather climate and related earth science applications hill et al 2004 the community surface dynamics modeling system csdms is another example of a component based framework which was developed with the goal of modeling earth s surface processes and dynamics peckham et al 2013 another such framework is the object modeling system oms with currently more than 200 model components that mostly focuses on agro environmental models david et al 2013 in the water resources domain a more well known component based framework is the open modeling interface openmi which enables the dynamic linking of models at runtime moore and tindall 2005 in soa based frameworks models are turned into web services and linked using standard text based protocols creating plug and play autonomous components python network simulation pynsim framework is an example of this knox et al 2018 other examples of soa based frameworks include weezard for geoanalysis which is a web service for debris flow hazard assessment and visualization zhang et al 2018 and emeli web that applies a basic model interface bmi for modeling a set of spatially distributed hydrologic models jiang et al 2017 1 3 need for generic iem software in the past few years many advancements in iem were made new component and web based frameworks have been introduced and the existing ones were improved belete et al 2017 lloyd et al 2011 whelan et al 2014 while these efforts are vital for addressing the emerging challenges in environmental modeling they are mostly directed towards advanced applications specific to different science domains belete et al 2017 on the other hand there is still a need within the modeling community for software programs which focuses beyond the various coupling processes and frameworks and provide advanced pre and post processing and other capabilities to manipulate the data inputs and generated outputs and visualizations from the coupled models even today there is a lack of dedicated tools to handle data i o input output e g reading and writing databases text files binary data etc and transformation e g data quality assurance quality control qa qc timeseries aggregation de aggregation infilling of missing data using standard or user defined techniques joining of datasets unit conversions etc david et al 2013 whelan et al 2014 additionally modelers have to use a number of different software programs for data storage downloads analysis statistics visualization etc in many cases where models are required to run multiple scenarios using different initial and boundary conditions manual steps update of input model run and post processing of output for example are required for each run belete et al 2017 buahin and horsburgh 2018 knapen et al 2013 performing all these tasks manually is a tedious process which requires a significant amount of time and effort bergez et al 2013 for instance there has been a lot of interest recently in developing decision support systems based on integrated modeling of natural and engineered environmental systems e g watershed reservoir treatment plants etc lodhi et al 2019 most of these applications require constant updates of input data obtained from different dynamic sources e g flow measuring stations weather stations weather forecasting models water quality monitoring labs online probes etc to run the models on a periodic basis data updates are often hourly for continuous monitoring and daily often in the overnight hours for updates of long term weather forecasts for example routinely performing the data update and model run steps manually is not a viable option although coding scripting can be used to automate these tasks it requires a considerable level of programming skills and in depth knowledge of different model structure and data formats which is not very common buahin and horsburgh 2018 iem applications based on loose sequential coupling are less complex and often do not require advanced modeling protocols however even such an application requires a large number of different pre and post operations to run the integrated models software which can provide an extensive built in data processing functionality for such a loose coupling arrangement in a single package has not been available in the iem software domain framework based coupling also presents similar challenges many frameworks lack advanced software support to perform the necessary pre and post processing and runtime operations for instance most of the software programs developed to run openmi compliant models e g openmi configuration editor pipistrelle etc do not provide any capabilities for the software to read or write data directly from a model component at runtime e g to show a model output on a chart while existing frameworks provide one or two of these capabilities none provide all of them gui data storage web data access visualization analytics scenario management scheduling capabilities for automated model runs and tools for creating interactive dashboards having all these capabilities in one software package would allow modelers to create applications that consolidate all these functions into one application therefore there is a need for software that can handle in a flexible way both loose and framework based coupling and provide all the necessary functionality required for integrated environmental modeling the capability of being able to develop fully automated modeling applications which can be operated by the click of a button dynamically creating inputs from different data sources managing scenarios running the models reading outputs analyzing data making statistical inferences and ultimately showing results in a meaningful and interactive way laniak et al 2013 morsy et al 2017 would be of value to both the ordinary user and the modeler in essence it should be able to facilitate a one stop solution for automated iem by a modeler urunme you run models easily was developed with the above mentioned integrated modeling functionality in mind we feel it is mature enough to be of use to researchers academics students and the like and is available for free for these constituencies this paper will explain the software architecture section 2 and the features and capabilities appendix a we will discuss some real world examples section 3 to demonstrate how it would work in a range of applications of environmental systems modeling focusing primarily on water resources from low flow analyses to a complete decision support system which links a wastewater treatment plant model with a multi application watershed model in this paper we illustrate how the software we have developed eliminates the burden of doing routine tasks such as statistical analyses data visualization for calibration checking forecasting optimization including calibration and goal seeking and other useful functions while urunme can be used for modeling other systems our emphasis here is on environmental systems and specifically water resources systems a summary of different features provided in urunme and how that can help the modeler is provided in table 1 below 2 software architecture urunme is programmed in c using net framework it is developed using a three tiered architecture arranged by presentation business and data fig 1 the gui is developed using various third party libraries table a1 which provides a comprehensive suite of various visual components in addition several helper classes are written to wrap the third party controls for added functionality which is otherwise not available by default the business side of the software is programmed as a standalone dynamic link library dll which is accessed by the presentation layer using a strict one way association no commercial component is used to develop this dll allowing an open source option in the future if desired the business tier is further decomposed into multiple layers each responsible for implementing different aspects of the logic the core layer of the business tier is called nodes which contains all the classes that are visually exposed to the users in a project the remaining classes are contained in the helper layer which provides additional functionality classes in the data access layer are essentially wrappers on net and other third party data providers to access different types of databases including sqlite mysql sql server in addition the data access layer also contains custom classes to read and write different file formats such as text comma space tab or custom delimiter and fixed width excel netcdf grib wdm this ability was considered essential due to multiple data sources often being used for a model application and also to reformat the input or output of one model for use by another for instance most weather current and forecasted data available on the noaa servers are either in grib2 or netcdf format similarly some models use fixed or comma separated csv files for inputs and outputs the user customizes the application for the appropriate format and also determines the association of the values read in with the appropriate parameter and its format the user may also use the built in functions to change formats for example a decimal julian date may be changed to one of the standard date formats selected by the user by implementation of a simple calculation process step this is essentially the conversion of dates from one unit to another much as the software allows for conversions between english and si units 2 1 graphical user interface the default gui consists of a ribbon type main menu on the top a project explorer and toolbox on the left engine and scheduler windows on the bottom and a workspace in the remaining area containing scenarios and views fig 2 we now present some of the main class diagrams that are used in applications note that this paper does not contain an exhaustive list of all such class diagrams a project in urunme consists of visual nodes which are shown in the project explorer as a tree list the node abstract class provides the logic for the hierarchical parent child structure fig 3 each node in a project is derived from the abstract node class and requires a parent with any number of children a project is shown as the root node in the project explorer and is implemented using a singleton project class it provides a one point access to the presentation tier by exposing a number of properties and methods a project is stored as a binary file with a urm extension using data contract serialization in net a scenario class is used to implement all the scenarios in a project they are stacked on the top of the workspace so that only one can be selected at a time deemed the active scenario a view class represents a standalone window within the workspace that contains visual elements called panels e g diagrams charts etc the views are stacked as tabs on the left side of the workspace so that only one is visible at a time deemed the active view a panel is a window inside a view containing a visual control e g diagram chart grid etc all panels derive directly from the node class and implement the ipanel interface this interface provides the logic for storing the layout and visual attributes of the panels in a view 2 2 process and functions one of the core components in urunme is the diagram panel which is implemented by the process class as shown in fig 4 the term diagram and process are used interchangeably throughout the manuscript a process can also be called a network or a graph this component works as a canvas to create visual objects and implement all the diagram settings including properties for grid scroll size layers etc these visual objects are implemented by the symbol base class which exposes properties and methods for visual manipulation some concrete classes that are inherited from the symbol base class e g textbox label image shape etc are used as visualization objects in a diagram the function abstract class on the other hand provides the base for implementing all tasks e g data reading writing analytics etc each class that is derived from the function base implements its own business logic by overriding the run method which is called by the engine when the user commits a run link and port classes are used for creating any type of connector between two symbols in a diagram the process class is also inherited from the function class but also implements the ipanel interface therefore it is shown both as a symbol in a diagram and a drawing panel in a view a process does not have any specific business logic of its own and only runs its child functions when its run method is called processes can be either standalone or sub process as depicted in the hierarchical structure fig 4 a standalone process like process 1 has no parent however it can contain many other sub processes and functions a sub process on the other hand can have its own child sub processes and functions for instance in fig 4 when process 1 is run it first runs process 2 which essentially runs all members of process 3 before running its own functions and then returns the execution back to its standalone process this structure allows the organization of a large project into smaller manageable processes which can be run independently or as a part of the parent process 2 3 open modeling interface urunme provides built in functionality for running open modeling interface omi v1 4 compliant models the standard was seamlessly integrated into urunme by creating new classes derived from existing process and function classes figure a1 the coupling logic is implemented by reusing the code provided in the standard development kit sdk of the omi framework https www openmi org the model setup is similar to the default model configuration editor provided with the omi with sdk the coupled models are configured using a sub process called omiprocess located in the toolbox under openmi group the omiprocess is inherited from the process class and implements the diagram editor to drag and drop any number of omi models from the toolbox an omimodel is essentially a function that runs an omi model based on the omi file provided as an input omitrigger function provides the trigger to run the models omilink contains the settings for data linkage and is derived from the link class to represent the model connections in addition two more functions omiinput and omioutput are provided to exchange data between urunme and different omi model components both these function implement openmi s oatc openmi sdk wrapper iengine interface and therefore essentially expose linkable components any existing variable can be dragged and dropped onto an omiinput function to create an output exchange item based on variable name and quantity this output exchange item can then be linked to any model to provide input data at runtime similarly omioutput function can be used to create new variables which can be linked to any model as input exchange items these variables can then be used to read output data from a model component at runtime this feature provides a method for reading and writing data directly from different model components at runtime which can be extremely useful for any pre and post processing operation 2 4 parameters and variables the parameter class is the building block of the scenario management logic in urunme figure a2 it is used as a custom data type in other classes to specify the user defined attributes for example in a read excel function class attributes like file path name of the worksheet cell address number of rows to be read etc are all defined as parameters to run multiple scenarios in a project a parameter should be able to store a different value of an attribute for each scenario if required for instance an excel read function might be required to access two separate files in two scenarios this is implemented by providing an option of storing a value in both global and local contexts within the parameter class the global value is essentially the default value of a parameter value property which is used in all of the scenarios a local value on the other hand is scenario specific and defined by a generic directory values field in the parameter class say a user selects a particular scenario called s1 from the tabs at the top of the workspace and makes a parameter local in a function consequently whenever the function is run in the selected scenario s1 it will only use the local value of the parameter in all other scenarios where the parameter is not localized the function will continue to use the global value of the said parameter this logic allows any parameter to be made local for any number of scenarios to enter different values for an attribute all the data in a project are represented by scalar and vector variables shown as visual nodes in the project explorer variable class is inherited from the node to represent the data associated with a function data transfer in the software is carried out by a simple drag and drop of variables onto different functions or panels as inputs each variable contains a field class which defines all its attributes including variable type vector or scalar data type numeric text datetime time quantity length area volume flow flux etc unit format etc for example in order to read tabular data from an excel file the user is required to enter the attributes for each column once defined the function automatically creates its variables corresponding to the number of columns which are displayed in the project explorer however in some function child variables are created automatically where each output variable derives its attributes from the input variable for example the datainfilling function provides different built in methods for generating missing data and creates an output vector variable for each input vector variable the output variables generated by this function have the same attributes as the associated input variable including the variable type data type quantity and units therefore the output variable keeps a reference of the input variable in its field class when many different functions are created in a project the logic creates a chain between one variable derived from another and so on this provides a very useful mechanism to keep track of changes happening in different chains of connected variables in each scenario the state of each variable is defined by the state property fig 3 using different colors and is shown as bullet symbols alongside each node in the project explorer for instance when a single function is run in a particular scenario the state of its child variables changes to green or red to indicate success or error respectively consequently all the variables down the data chain are set to orange to indicate that a source has been changed and an update is required also when any variable changes its state all the parents up the tree in the project explorer will show the same state it must be noted that the state of any variable is scenario specific and may vary across scenarios depending upon the status of the data the state color coding is summarized in table 2 in fig 2 all the variables in the variables highlighted portion inside the project explorer section shown are green as is function 1 process 1 etc all the way up the tree in the scenario section scenario 1 is shown as green as it is a complete successful run whereas all the other scenarios are shown as gray not run yet in the engine section function 2 is showing as disabled and hence in the project explorer section the symbol beside function 2 is black 2 5 embedded database urunme packages sqlite as its default embedded database a new project automatically creates a single db file in addition to the urm for every function created by the user a new table is added to the embedded database using a function id with the tb prefix e g tb100 where 100 is the built in id of the function by default the new table only contains two integer columns id as the primary key and scenario id to identify data stored for different scenarios the remaining columns are created at runtime by a function one for each variable using its associated field name all scalar variables in a project however are stored in a single table called tbscalar created by default for every project the main benefits of sqlite are portability and speed compared to a conventional server database with an on disk format data are stored as a single ordinary disk file as opposed to a server database which requires dedicated server and administration this database can be easily shared among different urunme projects or external software an important requirement for urunme is to sift and sort lots of data in diverse ways before permanently storing the essential information into the database this is achieved by using the in memory temporary database provided by sqlite each function provides the option to either store the data directly in the project s permanent database file or to keep it in the temporary database for analysis this is helpful in cases where a large amount of data must be queried prior to storage in the database file the temporary database in sqlite remains in memory until it comes under memory limitations and must flush some data to the disk as a temporary file one of the downsides of using sqlite is its maximum allowable size theoretically the sqlite database can store up to 140 tb of data but in practice this upper storage limit is untested https www sqlite org limits html in addition the sqlite database cannot be used for concurrent writing by multiple users though simultaneous reading is allowed the advantages and disadvantages were considered during the design stage of the software ultimately the decision was made to choose simplicity speed and portability of sqlite over the larger data limit of a server database most typical environmental models will not reach the 140 tb limit except in cases where there are many model runs stored in the database many large models such as climate prediction models have their own built in run processes and are highly unlikely to need to use software such as urunme 2 6 unit converter another feature of urunme is the built in unit converter there are a number of different quantities defined in the software e g length area volume speed flow flux radiation etc using quantity enum figure a3 for each quantity a quantitydef class contains a number of units e g m ft etc for length quantity implemented by the unit class which specifies the relevant conversion factors numeric variables contain user defined quantity and unit parameters and call the associated quantitydef in the unitconverter class for unit conversion a detailed explanation of different key features and capacities of urunme is provided in appendix b 3 case studies this section describes water resources case studies for urunme ranging from simple data analysis to advanced model coupling and interactive dashboards the examples discussed here are from real world projects involving different applications of integrated models to different types of watershed analysis such as forecasting in potable reuse systems and climate change project files for some of the case studies are available for download at https www urunme com resources the first case is described in more detail to illustrate how different processes are set up in a urunme project the remaining project examples are described more concisely and include only those details that are not shown or used in the previous examples 3 1 data analysis urunme can be used for in depth data analysis owing to the embedded sql and r language support and other built in functions both ordinary data units conversion data read from websites and specialized statistics tests the current release of urunme contains most functions that are likely to be used by water resource professionals a case in point is the low flow frequency analysis carried out on a stream called st10 in the occoquan watershed located in northern virginia usa in an attempt to calculate drought conditions under different return periods in this method first annual minima are derived from daily flow by selecting the lowest flow every year using different averaging durations e g 1 7 10 etc days an n day moving average is progressed through the daily data to calculate the timeseries for each duration and then yearly minima are calculated hisdal and gustard 2008 a cumulative distribution function cdf is developed using the yearly minimum flows low flow for any return period t can then be calculated by selecting the desired exceedance probability p on the cdf where t 1 p for instance a commonly used standard called 7q10 represents the minimum 7 day flow expected to occur every 10 years the following example illustrates how a different set of features in urunme can be used to calculate the low flows in real time using online flow data from a stream station fig 5 shows the process for low flow frequency analysis for the duration of 7 15 30 60 90 and 180 days over a return period of 2 5 and 10 years p 0 5 0 25 and 0 1 respectively this demonstrates that once low flows are calculated for any given return period different scenarios can be created to calculate other return periods by simply localizing one variable a formula function called global is used to create one variable called p which is kept local for each scenario to define the return period flow data are obtained from three different sources the historical function reads a text file obtained from a usgs united states geological survey website containing the daily flows for this location from 1968 to 1980 read st 25 30 reads daily flows from a visual foxpro database for stream stations st25 and st30 located upstream of st10 for a time period between 1980 and 2003 before st10 was commissioned the fill 25 30 function uses interpolation to fill in any missing data in the timeseries the compute st10 function contains a formula to extrapolate missing st10 data between 1980 and 2003 using flows from st25 and st30 based on the drainage area of each station st10 st25 2 166 st30 the st10 function reads the data for the remaining time period from the database for the online stream station st10 the qaqc quality assurance quality control function contains an sql statement to remove any duplicates and also sort the data by date in case there is any discrepancy the daily flow function aggregates the data from st10 to daily flows by using a time weighted average fill st10 infills any missing data using interpolation all flow contains a sql statement using union to add the three different sources of data and create one single timeseries from 1968 to the present date figure a4 the moving avg formula function creates moving averages from the final timeseries for each time duration figure a5 yearly min aggregates the moving averages for each duration to get yearly minima finally low flow uses an r script to create a cdf for each duration and returns low flow for a given p defined in global using inverse cdf figure a6 once this process is created and tested any number of scenarios can be created to get values from different return periods by changing the value of variable p in each scenario run all which is an execute function is used to conveniently run all the scenarios sequentially figure a7 a button can be added in the diagram to run the execute function if a more dashboard type application is desired the two charts shown in fig 5 shows how scenario management is implemented in urunme the low flow function produces two vector variables x for return periods and xqy for low flows if the variable xqy is attached to the chart using an active attribute each time the user changes the scenario the chart will update itself using the values from the active scenario left chart in fig 5 showing results from the active scenario xq2 to compare values from all three scenarios at the same time the variable xqy should be dragged and dropped three times on the right chart from the project explorer each with its own scenario name attribute figure a8 this will create three different timeseries each for one scenario right chart in fig 5 this particular chart will not update every time the active scenario is changed by the user unless the underlying data or processes are changed and the scenarios run again this case study shows a very simple example of data analysis and how different scenarios can be used in urunme more sophisticated analytics can be performed using similar techniques 3 2 web data download urunme can be used to download data from different websites using various file transfer protocols on a periodic basis for real time modeling analysis and visualization the following example is adopted from a futurecasting project where forecasted streamflow data are downloaded from the national water model nwm https water noaa gov about nwm ftp file transfer protocol server on a daily basis fig 6 nwm is a hydrological model that forecasts stream flows across the continental united states and has been in operation since aug 16 2016 on the national oceanographic and atmospheric administration noaa supercomputing system a long range 16 member ensemble forecast is produced every day going out 30 days in the future with 4 ensemble members in each cycle at 00 06 12 and 18 utc fig 6 shows the process created in urunme used to download and query the 4 ensemble members produced at 1800 h each day a new directory is created in the nwm ftp url ftp ftpprd ncep noaa gov pub data nccf com nwm prod named based on the production date with a nwm prefix e g nwm 20180805 a formula function is used to create the url and local paths shown using the following expressions in table 3 also in figure a9 now formula gets the current date and sub datetime timespan subtracts a day from it text datetime format converts the date to text using the given format the concatenate text1 text2 function is then used to create the remote path ftp url and local directories where each ensemble is stored the downloader function is used to download each ensemble in netcdf format and store them into their respective directories using the parameters shown in figure a10 a total of 120 netcdf files are downloaded for each ensemble the parameters remotepath and localpath in the downloader function are set to use variables and the corresponding variables from the function paths are used as inputs simple masks are used to include or exclude files or folders from the download four read netcdf functions one for each ensemble are used to read the downloaded files the read netcdf function has the option of reading either a single file or an entire directory of multiple files and then combining the data to create a single timeseries flow data for the required stream are extracted from these netcdf files by providing the stream identification numbers as defined by nwm once the data are read into the temporary database formula functions are used to calculate min max and means of the four ensembles before storing that information into the embedded database using the write database function a scheduler is used to run this process automatically on a daily basis 3 3 iterative approach for calibration of models urunme can be used to run models recursively to calibrate them to user defined criteria although complex multi step calibration objectives can be used a single test single objective example is shown here from a reservoir modeling project to demonstrate the basic concept ce qual w2 is a two dimensional hydrodynamic and water quantity model developed by the u s army corps of engineers to simulate the water quantity and quality in a waterbody cole and wells 2011 the simulated water elevation from the model is often not well calibrated the model does not account for water loss due to seepage etc and requires external calculations to create artificial tributary inflows outflows for water balance a water balance utility is provided in the ce qual w2 package which requires a text file el obs npt containing observed water elevation as an input for the simulated time period the model is required to be run multiple times and each iteration must be followed by running the utility to calibrate the water elevation a looped process is created in urunme to run the model and water balance utility successively fig 7 until a desired performance is achieved for example the absolute value of the difference between the model computed water elevation and the observed value is less than a specified value for each day of the simulation the process starts with a formula function to initialize a variable called count to keep track of the number of times the model is run observed water elevation is read from the visual foxpro file pool db for the desired time period by a read database function and is then written to the el obs npt file by a write text file function in the desired format the ce qual w2 model is run by execute figure a11 and then followed by calibration to calculate different calibration performance indices as scalar variables such as r squared rsq rsq variable is used by the calculate formula function to evaluate the following two expressions count count 1 stop if count 10 or rsq 0 9 1 0 the count is incremented and an if statement is used to calculate a scalar variable stop the condition shown above is just for demonstration purposes any logical statement can be used instead to be used by the decision function to break the loop and go to the end this is achieved by dragging and dropping the stop variable in the decision properties form when variable is 1 the loop breaks in case the criterion is not met the process goes into the loop and executes the water balance utility a similar technique can be used for calibrating models by replacing the water balance utility with any other optimization routine or objective function the next section describes a case of a more complex model and calibration process 3 4 loose model coupling the occoquan model is a set of computer models linked together to simulate the watershed s hydrology and water quality xu et al 2007 the occoquan watershed is located in northern virginia near washington dc the model consists of seven watershed models hspf and two reservoir models ce qual w2 the coupling of these models is achieved by using the output from the upstream model as input for the downstream model in the past the entire modeling process had a number of manual steps particularly where data outputs from one model had to converted from english to si and vice versa units to provide input for the next model and having to perform the appropriate statistical tests to calibrate a particular model before moving on to the next one mainly this was due to a failure to develop modeling tools to aid the modeler the result was a huge amount of time and effort to carry out the simulation process which took days if not weeks to complete although some parts of the process had been made easier to run using macros a more complete solution was desired urunme development was started and in 2017 the entire occoquan model was migrated to urunme a dashboard was created for visualization of different inputs and outputs using multiple views and panels for the multi step coupled model each task is accomplished through a dedicated process there are two main standalone processes in the project observed and models the observed process contains multiple processes that use hundreds of functions for reading data from different sources database text excel wdm etc these data sources include meteorological data from 14 weather stations in the watershed flow and water quality data from stream stations water elevation and water quality data from reservoir stations water quality data from a water reuse plant oxygen flow rates from the hypolimnetic oxygenation system and water withdrawals from a water treatment plant various data analyses and manipulation operations are performed in these processes to develop the required input for each model these include qaqc of data infilling missing data timeseries aggregation unit conversions load calculations etc to keep the database size small most functions use the temporary database and only processed information is stored in the embedded database the model process is used to run all nine models sequentially as shown in fig 8 each sub process in the diagram runs one model and consists of multiple other sub processes and functions the data created in the observed process is written to a different input text file of models before running them sequentially output data from one model is transferred and output units converted as needed to the downstream model and saved in the embedded database to create outputs for the dashboard the entire simulation process is run with the click of a button and takes less than 2 h to complete prior to using urunme when each model was run individually and unit conversions performed by importing the data into excel and then re exporting it in the format for the next downstream model even a fast worker would take the equivalent of 24 h to do a complete run moreover the potential for error was greater as both hspf and ce qual w2 expect data in particular columns so a one column shift in exporting the data for example would result in a bad run and have to be repeated sometimes these human errors were caught much later than one would want this is not counting the time it took to make the various charts and plots to determine how the models were performing in the calibration application project each model is run recursively until the objective function is satisfied before urunme proceeds to the next model in the sequence the objective function for the ce qual w2 models is the same as for the case described in the previous section a matching within tolerance limits of the water surface elevations both modeled and observed for the hspf models objective functions could be obtaining a particular goodness of fit using whatever method the user wishes for one or more variables for example flow and concentration of a constituent 3 5 openmi based coupling this case study is presented primarily to illustrate the implementation of openmi framework within urunme application it also explains the mechanism for direct data exchange with the openmi component for example inputting and outputting data directly to a model component at runtime openmi standard is a software component interface which facilitates the data exchange between different models on timestep by timestep basis which is especially useful for simulating interacting environmental processes this case study is based on the simple river model provided with openmi v1 4 user manual and shown in fig 9 the left panel on the screen shows the parent process which has two functions to read and write data from two separate excel files the omi process sub process is expanded in the right panel which shows the openmi model composition the omimodel function simply requires the path of simple river model s omi file to load the linkable engine dll figure a12 the two variables date and inflows read by the readexcel function are dragged and dropped on the omiinput function that automatically creates an output data exchange item inflows within omiinput function figure a13 which can then be linked to branch inflow item of the model component using a link figure a14 similarly in the omioutput function a variable called outflow is created using output tab in the properties form which essentially works as an input data exchange item this variable is then linked to the required branch flow item of model components figure a14 the lists of available input and output data exchange items for a component are shown in the properties form of the function figure a12 the data exchange linkages between different omi functions can be created in the properties form of the visual link figure a15 at runtime the simple river component directly reads inflows from the omiinput function and omiouput pulls outflows from the model component this example shows how the openmi framework is seamlessly embedded in urunme as a sub process in future similar sub processes can be created for other frameworks and can be executed with any pre and post processing operations this architecture even allows different openmi sub processes to be run and exchange data sequentially if required 3 6 decision support system this case study presents in summary form a decision support system developed in urunme for the upper occoquan service authority uosa water reclamation plant wrf in northern virginia a complete treatment of the dss is described in lodhi et al 2019 the wrf discharges its reclaimed water into a tributary of the occoquan reservoir fig 10 fairfax water s griffith water treatment plant wtp withdraws water from the tail end of the reservoir at the dam the contribution of the wrf to the reservoir s inflow varies and can reach as high as 80 of the reservoir inflow during extremely dry months one of the key challenges for uosa is to regulate the nitrate concentration in reclaimed water throughout the year during thermal stratification and extremely low hypolimnetic oxygen concentrations in the summer the supply of nitrate from uosa actually benefits the reservoir by providing nitrate as an alternate electron accepter to oxygen with bacterial activity thus reducing the nitrate to nitrogen gas and thus preventing the reservoir bottom from becoming anaerobic and preventing the release of phosphorus iron manganese and ammonia therefore the occoquan policy virginia state water control board 1971 coupled with uosa s operating permit allows for high nitrate concentration in the reclaimed water during summers by limiting uosa to a total mass rather than concentration of all nitrogen species during a calendar year without a discharge limit on nitrate during the summer months of stratification provided the nitrate nitrogen concentration at the occoquan dam does not exceed 5 mg l in which case uosa must start denitrifying the excess nitrate this is achieved at the plant by minimizing the denitrification process in summer however during winters the reservoir loses its denitrification capacity due to the lack of stratification and the presence of dissolved oxygen from top to bottom waters and the plant has to significantly reduce nitrates in the discharge by increasing in plant denitrification past experience has shown that uosa has sometimes had difficulty in maintaining low effluent nitrate concentrations during winter mostly due to operational constraints including limited organic carbon and low water temperatures which hinders the denitrification process in some cases methanol is required to be added to improve the denitrification process resulting in a substantial increase in the operational cost the effect of higher discharged load from the wrf on the nitrate concentrations at the dam depends on many factors including the pool elevation volume and quality temperature background nitrate concentration etc of stream inflows withdrawal by the water treatment plant and weather conditions therefore it would be extremely beneficial for the wrf to identify strategies to manage the wrf performance dynamically in response to future reservoir conditions lodhi et al 2019 the uosa dss was developed to capture the dynamic nature of nitrate loadings to the reservoir both from natural streams and the wrf withdrawals by the wtp wrf effluent flows wrf plant operations and the denitrification capacity of the reservoir and help manage the nutrient levels in the reservoir it is based on an integrated reservoir model ce qual w2 that uses data from a real time climate forecast system cfs nwm and observed data to predict future reservoir conditions the resulting application provides valuable feedback to plant operators to correctly target the effluent nitrates using a biochemical operation and optimization model called iviewops sen et al 2018 each week the dss moves forward in time by predicting the reservoir water quality based on the most recent initial and future boundary conditions when the forecasted water quality starts reaching the 5 mg l nitrate n threshold at the dam 20 miles downstream from the uosa discharge location the dss automatically runs an optimization routine on iviewops to determine the most suitable operational strategy to reduce the nutrient discharge from the plant by changes in configurational and operational setpoints for each run urunme reads data from various data sources dynamically data being continuously updated in different databases and performs extensive analysis and manipulation before feeding it to the model as inputs the overall data flow path to operate the integrated model is shown in fig 11 multiple future scenarios based on different combinations of natural stream inflows plant effluent flows and nitrate concentrations and water withdrawal by the wtp are simulated the future weather conditions are created using the forecasts obtained from the cfs a 30 day forecast predicts the reservoir conditions based on the nwm stream flow forecast going out 30 days a 90 day forecast on the other hand is used as a what if analysis to simulate the effects of any possible future winter drought the model is run multiple times to simulate each scenario first the base scenario is run for the last three years to evaluate the current model calibration using the last three years of observed data then data for each of the forecast scenarios are created and the model is run sequentially using the present day boundary conditions from the base scenario as the starting point the forecasts can be critical to wrf operations because the switchover from a nitrifying to a denitrifying mode can take up to two weeks the nitrate profiles shown in fig 12 are extracted from the dss output generated on october 10 2018 an artificial dry condition simulated by creating an artificial drawdown of 2 m from full pool level for a 90 day forecast the model run showed no significant change in the nitrate during the 30 days forecast shown with green background as nwm predicted relatively higher stream flow during that period which reduced the uosa contribution to the total inflows causing higher dilution however for the 90 day drought forecast it was predicted by the dss that an effluent nitrate n concentration of 12 5 mg l as n green line by the wrf would push the nitrates to the 5 mg l as n limit in the forecasted period for both the w2 scenarios therefore a lower effluent nitrate concentration should be targeted the above results demonstrate how the dynamic nature of the system can affect the reservoir based on different boundary conditions different conditions have significant effects on the outcome for instance colder than normal air temperatures in fall or winter can increase the nitrate concentration in the reservoir by reducing the denitrification capacity of the wrf on the other hand any significant rain event can washout the entire reservoir thus diluting the nitrate concentrations to much lower levels fig 13 shows one of the many views created in a dashboard application for the dss two main sets of buttons on the left are used to open a specific view from the multiple views available another view from the same application shows the output from the process simulator fig 14 iviewops reads online data directly from scada supervisory control and data acquisition system and the wrf laboratory to optimize the plant using different predefined combinations of process configuration tanks in service mixed liquor recycle dissolved oxygen wasting supplemental carbon etc while specifying certain constraints on the effluent quality in fig 14 observed data from the wrf laboratory are shown as red field sensors as green and the optimized results from the biochemical simulator as blue 4 summary commonly the term integrated modeling only refers to the coupling mechanism used to automate the data exchange between different linked models however most such applications require extensive pre and post processing for developing an automated integrated model urunme was developed with the goals of facilitating model coupling and to automate the entire simulation process creating inputs from different data sources reading outputs analyzing data making statistical inferences and ultimately showing results in a meaningful and interactive way it serves as an intelligent layer between the users models and data sources practically hiding the underlying mechanics of the complex simulation process we have described much of the core functionality of the software above and in the appendix our objective was to build a framework that would be able to integrate models data from a variety of sources including web databases the ability to do statistical and other analyses and provide tools for the user to generate appropriate data visualizations although urunme was developed to help automate these functions for the occoquan model from the start our efforts were to not particularize the software for that system but to create a framework which would run a variety of environmental models and indeed could be used for other types of models too there are many models in a modeler s toolbox some are widely used such as hspf and ce qual w2 that we use in our occoquan model and other are less so including research models that are still under development many public domain models again such as hspf and ce qual w2 but also many others such as the hec series swat wasp etc are well accepted but often lack the post processing tools that a modeler needs these tools include statistical analyses goodness of fit tests and many different types of visualizations there are commercial packages available that include these public domain models and provide some pre and post processing tools but they are often limited to one software there are commercial models that provide some excellent tools but they are often expensive packages and because the code is not available cannot be modified if needed for a particular application moreover environmental models come in different scales and types the iviewops package mentioned in this paper is used for simulating the functioning of a wastewater treatment plant and can be used to optimize the treatment process towards an objective trying to couple such models with watershed scale models to create a decision support system for indirect potable use which is the system we have used in the dss case study in order to predict water quality some miles downstream can be a challenge similarly keeping track of downscaled global climate model data to use in a watershed model under various scenarios can take up a lot of a modeler s time just keeping track of the data add to that that the output of many models may require unit conversions before it can be used in other models and the software framework described herein we hope will be of use to modelers finally our efforts were also particularly focused on academic research as modelers in academia we often cannot afford to purchase expensive software to do everything we would like students often have to use models both big and small in their classes and then have to import the output into something like excel or matlab in order to draw charts and figures not everybody has free access to such software we wished to remove the drudgery that we had at one time or another experienced with modeling and to allow the modeler to focus on the application and its results rather than on the manipulation of the data necessary to perform a statistical analysis or visualization urunme was partially prompted by this need too and is free for academic use more functionality and features will be added to urunme based on user feedback support for several new data formats implementation of other modeling frameworks upgdrade to openmi 2 0 and integration of geographic information system gis mapping features are also part of the future software development plan declaration of competing interest no conflict of interest acknowledgments the authors acknowledge the support of the owml occoquan watershed monitoring laboratory family including the field and lab staff students and faculty members we would also like to extend our appreciation to the late dr thomas j grizzard for his unwavering commitment in the establishment and operation of the occoquan program and the owml for over 40 years this work was started during his leadership of the owml and much of the data used for the case studies were gathered during his tenure the principal author lodhi was supported during his phd work by funds from the occoquan modeling project locally funded by jurisdictions in the watershed fairfax county fauquier county loudoun county prince william county ciry of manassas and city of manassas park and fairfax water urunme development was however not funded by any particular source but evolved out of a need appendix a table a1 list of third party libraries used in the development of urunme table a1 no name purpose link 1 dotnet framework 4 6 general https www microsoft com en us download details aspx id 53344 2 devexpress winforms gui https www devexpress com 3 northwood software godiagram diagrams https www nwoods com products godiagram 4 extreme optimization math statistics https www extremeoptimization com 5 sqlite embedded database https www sqlite org index html 6 ms sql server supported database system data sqlclient net 7 mysql supported database mysql data oracle https dev mysql com downloads 8 oracle supported database oracle manageddataacess by oracle https www nuget org packages oracle manageddataaccess 9 r r language https www r project org 10 openmi v1 4 openmi https www openmi org 11 ncalc expression parser https archive codeplex com p ncalc 12 winscp file upload downloads https winscp net eng index php 13 quartz scheduler https www quartz scheduler net fig a1 class diagram for a process which works as a canvas to create visual objects including symbols functions and other sub processes fig a1 fig a2 class diagram of parameter and variable fig a2 fig a3 class diagram for classes used in unit conversion in urunme fig a3 fig a4 sql function properties form showing a query tab fig a4 fig a5 formula function properties form showing expression tab fig a5 fig a6 r function properties form showing script tab fig a6 fig a7 execute function properties form showing the process low flows selected to run in three different scenarios in batch mode fig a7 fig a8 chart input tab showing different variables shown in the chart fig a8 fig a9 formula function showing the variables defined for downloading nwm forecast fig a9 fig a10 downloader function showing parameters defined to download nwm forecast fig a10 fig a11 execute function properties form showing the parameters required to run an external model fig a11 fig a12 omimodel function properties form showing output and input exchange items for simple river model fig a12 fig a13 omiinput function properties form showing output item automatically generated for the input variable inflows fig a13 fig a14 omilink properties form showing an openmi link between omioutput function and simple river model component for outflows fig a14 fig a15 omilink properties form showing an openmi link between omiinput function and simple river model component for inflows fig a15 appendix b features and capabilities to create a new project in urunme the user has to specify the project directory the purpose is to ensure that all the data used in a project are contained in one directory and its sub directories for easier exchange between different computers a user can override this restriction folders in a project can be created using the toolbar in the project explorer to group different types of nodes a new view or scenario can be added by the insert group in the main menu and is shown in the project explorer under the views and scenarios folders respectively any view or scenario can be made invisible to the workspace by using show hide button in the associated context menu two types of views are provided in the software tabbed and widget tabbed view displays panels as tabs and only one panel can be shown at a time in a view similar to worksheets in ms excel widget view on the other hand allows the panels to be docked on the screen in any layout which may be more suitable for dashboard like presentations diagram process panel a new standalone process can be added from the main menu and shown in the project explorer under the active view sub processes can be added to a process diagram using the general group in the toolbox and are shown as a child of parent processes in the project explorer to view the process panel the show hide button can be used in its context menu symbols the main menu contains a number of items for visual manipulation of symbols in an active diagram which are divided into different groups layers format edit and size different layers can be used to group different symbols in a diagram which can then be made either invisible or locked some objects derived from the symbol class are used purely for visualization purposes in a diagram and are available in the miscellaneous group in the toolbox the label symbol is used to write text on a diagram the shape symbol provides hundreds of built in figures which can be used to create different shapes the button symbol is used to create interactive applications as explained in section 3 4 the image symbol is used to insert an image in a diagram any number of ports of various shapes and sizes can be created for the image symbols by using the port button in its context menu the textbox symbol is similar to a label but used for scalar variables it can be created by dragging and dropping a scalar variable from the project explorer to a diagram and automatically updates itself if the input variable changes its state functions urunme uses simple flowchart type diagrams to set up a process every function has a context menu that can be displayed by either right clicking on it or its corresponding node in the project explorer basic functions such as start end converge and decision are used to define the execution path the decision function for instance is used to run desired functions in a loop until designated criteria are met advanced functions are used to perform more dedicated tasks and are available in different associated groups in the toolbox links can be used to connect functions to create an execution flow path for the engine by default the ports on a new function are hidden and every process runs its children sequentially based on their order in the project explorer the inputs for any function can be specified in the properties form shown by clicking the properties button in the context menu of the function as previously explained each input parameter in a function can have different values for different scenarios a parameter can be made local by right clicking on its associated editor e g textbox checkbox combobox etc and click use local value in its context menu this will change the backcolor of the editor to blue and an icon image 16 will appear right next to it figure b1 now any value entered by the user will only be applicable for that specific scenario while the rest of the scenarios will continue to use the global values a variable can be used as an input to a parameter by right clicking on its editor and selecting use variable in the context menu it will show anicon image 17 next to the editor indicating that this parameter is now attached to a variable figure b2 any scalar variable can then be dragged and dropped onto the editor from the project explorer it must be noted that the default value of a variable is always based on the active scenario hence when a variable is attached to a parameter a combobox or dropbox appears next to the parameter editor and sets active as the default scenario figure b3 however if the variable is desired to have a fixed value from a scenario specific pesrpective regardless of whichever scenario is active that scenario should be selected from the drop down instead combobox contains the list of all scenarios in the project fig b1 read excel function properties form showing general tab with localized file parameter fig b1 fig b2 read excel function properties form showing file parameter ready to be attached to a variable fig b2 fig b3 read excel function properties form showing file parameter attached to a variable with a dropbox to select the scenario fig b3 for all functions that create user defined variables the output tab in the properties form of the function is used to define the required attributes figure b4 for example to read an excel file a user is required to define the name variable type data type and cell reference in the excel sheet for each variable to be read these attributes may vary from function to function quantity and unit are also required in cases where the variable is numeric fig b4 read excel function properties form showing the output tab fig b4 in functions e g write excel file or panels e g charts where variables need to be added as inputs a user is required to simply drag and drop the desired variable on the input tab in the properties form figure b5 and then set the name unit if required and scenario as explained in the above paragraph attributes each function also has a settings tab in its properties form which contains a parameter to disable the function so that it cannot be run by the engine in addition it contains two more parameters for the functions that handle data the cache parameter is used to set whether the data are kept in the temporary or file database and the insert mode parameter defines how data are added to a table for insert mode selecting replace deletes all the existing rows for the given scenario before adding new data while append just adds new data rows after existing data fig b5 write excel function properties form showing the input tab fig b5 three language functions sql https www sqlite org r https www r project org and formula are provided under the editors group in the toolbox the sql function is used to query the embedded database external databases can also be directly queried using the read database function described in table 1 for example select from tb100 will query the table created for a function having an id 100 queried columns can be used to create new variables in the output tab by using the same names in the name attribute as used in the query each time the sql function is run it executes the query and updates its variables by default every query is automatically run only on the active scenario however the user can uncheck the use active scenario parameter in the general tab of the sql function to query the entire table in addition to querying values the user can also write non queries queries that do not return any data and performs an action e g deleting some rows by checking execute non query one of the features in the sql function is the option of using other scalar variables in the query text for instance any scalar variable say a date variable named x can be added to a sql function by using the input tab if there is a date column in a table say tb100 it can be queried using select from tb100 where date x to get all the rows having a date greater than x this feature essentially makes a query dynamic in nature and every time it runs the data are queried based on the current values of the variables embedded in the query text the r function has an embedded r engine which is a widely used statistical language with a host of third party libraries and functions r combined with sql provide useful data analysis and statistical capabilities in urunme similar to sql any variable can be used in the r script by adding it to the input tab output variables are defined to use the return values from the r engine with the same names as defined in the r script in addition a separate formula function is provided to write mathematical and logical expressions on the variables this function incorporates a large number of excel style formulas e g abs cos sin etc the reason for including this function in urunme is to provide additional built in formulas which are not readily available in sql or r furthermore due to internal processing these formulas are generally faster in performance compared to sql and r which require external engines to run more formulas can be added to this function in the future based on user feedback the execute function is used to run the external applications e g models utilities software etc the path of an executable file is required as an input including any command line arguments accepted by the file a check box is used to tell the function if it should wait until the external application exists before handing back the control to the urunme engine a maximum runtime parameter is also provided to terminate the execution if the external application is taking more time than specified in case the external application has stopped working due to error execute can also be used to run different scenarios of a project in batch mode by setting the scope parameter to internal in the general tab of the function any number of processes and functions can then be dragged and dropped within execute and the run scenario can then be specified for each of them using the associated drop down menu by initiating execute these functions are run one by one in the specified scenarios in addition to the above urunme contains a number of other functions to perform various tasks pertaining to data read and write file i o timeseries manipulation statistics etc a brief description of each function is provided in table b1 more functions will be added in the future based on requirements and user feedback table b1 brief summary of various functions provided in urunme table b1 no function description 1 read database read write data from different popular database types including foxpro sql server mysql oracle sqlite etc 2 write database 3 read text file read write data from text files comma separated values space delimited tab delimited custom delimited and fixed width 4 write text file 5 read spreadsheet read write data from an excel spreadsheet 6 write spreadsheet 7 read wdm read write timeseries from watershed data management wdm format used in hspf 8 write wdm 9 read netcdf read netcdf a multidimensional array oriented binary format generally used for meteorological data 10 read grib2 read grib2 a concise binary data format generally used for meteorological data 11 read blob read blobs binary large objects e g images media files etc 12 write template write scalar variables to different text files using tags this feature is used to automatically change input parameter for models which use text files as input 13 execute run any external executable file or internal processes under different scenarios 14 sql to write queries on the embedded sqlite database using sql 15 r to write r code using the embedded r engine 16 formula to write mathematical and logical expressions using built in formulas 17 file io provides file io functionality e g creating copying and deleting files and folders 18 downloader provides a range of features for downloading files from web servers 19 uploader provides a range of features for uploading files to web servers 20 emailer provides a simple way for emailing 21 data infilling fill missing data using different methods and or de aggregate timeseries 22 aggregate aggregate timeseries using average sum time weighted average and time weighted sum 23 match match two different timeseries 24 omimodel run an openmi compliant model 25 omitrigger provides an openmi trigger 26 omiinput to input data directly to an openmi model component at runtime 27 omioutput to output data directly from an openmi model component at runtime run engine the engine is responsible for running different functions and displaying the run status in the window at the bottom of the gui any process or function can be run by selecting the run button in the engine group in the main menu or from its context menu in the diagram or project explorer moreover model runs can be configured for a given date and time or on a periodic basis using scheduler which can be added to a project from the engine group in the main menu details for next scheduled run and remaining time for each scheduler are constantly updated in the scheduler window located next to the engine tab the engine runs different functions of a process sequentially however different functions can be run in parallel on separate threads if more than one execution path is available by checking the allow parallel processing parameter in the general tab of the parent process properties form panels different panels can be added to a view by using panel group in the main menu a variety of charts are provided with extensive customization options figure b2 including the option for creating multiple series panels built in panels in a chart legends axis annotations point labels etc in addition charts provide various built in data analysis tools including linear regression error bars summaries moving averages etc data can be added to the chart by dragging and dropping any number of vector variables to the chart itself or the input tab in its properties form one of the features of the chart is its ability to dynamically update its different text properties e g titles axes legends etc using scalar variables as input for example a text variable can be used as a chart title using the annotation tab in its property form each time the variable state is changed the chart title will be updated automatically grid panel is used to show data in tabular form it provides a rich set of built in sorting grouping filtering and data searching tools in addition to a simple grid pivot grid panel is also provided to create pivot tables for multidimensional data analysis all of these panels update automatically if there is any change in the underlying data fig b6 chart wizard in urunme fig b6 gauges can be used to show scalar variables on the screen for a higher level of data visualization different types of customizable elements are provided to imitate real gauges including circular angular linear thermometers digital etc figure b7 gauges are very useful for creating dashboard type applications to mimic screens used in supervisory control systems fig b7 a sample of available gauges in urunme fig b7 image panel is used to display an image on the screen urunme provides the option of reading and storing blobs binary large object e g images media files etc which can be read using the read blob function these blobs are similar to variables and can be dragged and dropped onto an image panel or as symbols in a diagram any time the read blob function is run the associated images are updated in the database and the panel or symbol automatically updates its image this functionality can be really useful in applications where externally created outputs e g charts and diagrams from different models are shown as images on a dashboard in urunme when a large amount of data are processed in a simulation it is very difficult to monitor the changes in different variables the event manager panel provides a very simple mechanism of data monitoring and can display any message warning or alarm any number of events can be specified in the event manager where each event contains a scalar variable user defined message and a logical expression as a trigger figure b8 the logical expression is evaluated every time its associated variable changes its state and if true the message is displayed on the event manager screen figure b9 for example this feature can be used for indicating a need for model calibration if certain performance indices are above or below a threshold fig b8 event manager properties form showing how an event is defined fig b8 fig b9 event manager panel showing a triggered event display when the evaluation expression becomes true fig b9 commands a useful feature of urunme is its ability to create interactive dashboards where different actions can be defined in response to user initiated triggers e g mouse click key press etc this is accomplished by adding different commands to any visual element in the software panels symbols functions etc commands can be configured by using the commands button in the context menu and specifying the different attributes figure b10 any predefined action e g run a function show or hide a view or panel change size move or rotate an image create some animation etc can be defined in a command in response to triggers e g mouse click key press etc for a target visual element in addition a logical expression can be specified in a command using a scalar variable to override the trigger if specific criteria are not met fig b10 commands properties form for a button showing definition of a run command fig b10 
25951,modeling agriculture impacted hydrological processes remains a challenge as information on irrigation activities is often insufficient this study developed modules for emulating high resolution irrigation activities and representing water management policies in integrated hydrologic modeling the new modeling approach was implemented in china s second largest endorheic river basin with intensive irrigation and exemplary environmental flow regulation pumping management is found to impact the hydrological regime at the basin scale and modulate the tradeoff between ecological services and agriculture limiting the groundwater level decline within 1 m would lead to a 3 9 decrease in cropland evapotranspiration in the middle basin and a 1 5 increase in the environmental flow towards the lower basin environmental flow regulation and pumping management have both synergistic and offset effects reflecting a complex water agriculture ecosystem nexus in this basin this modeling study suggests heterogeneous and dynamic groundwater management policies in water limited areas and unravels threshold effects in the policies keywords integrated hydrological model environmental flow endorheic river basin irrigation groundwater management water resources 1 introduction irrigated agriculture is an important aspect of food production b li et al 2019 but it consumes a large amount of water resources as well it is estimated that irrigation accounts for 70 of the water diverted from rivers globally hoekstra and mekonnen 2012 arid and semiarid areas cover approximately 31 of the land surface on earth and support over 1 3 of the global human population pravalie 2016 where water resource scarcity is the pivot of human nature conflicts due to the rapid population growth and expansion of irrigated agriculture ecological degradation has become widespread in arid and semiarid areas barnett et al 2008 a well known ecological tragedy is the aral sea which was formerly the fourth largest lake in the world the aral sea has been steadily shrinking since the 1960s due to the diversion of the rivers feeding the lake for the irrigation projects of the former soviet union cretaux et al 2013 varis 2014 the desiccation of the aral sea caused notable desertification and soil salinization and greatly increased the number of dust and salt storms in that region bosch et al 2007 similar ecological disasters have occurred in other endorheic areas such as lake urmia in iran stone 2015 the tarim river basin bao et al 2017 and the heihe river basin hrb in northwestern china cheng et al 2014 in arid and semiarid areas integrated water resources management iwrm is important for alleviating the competition between agriculture and other water consumers including the industrial domestic and ecological sectors the conjunctive use of surface water sw and groundwater gw is a common practice in iwrm mani et al 2016 wu et al 2014 2015 to develop sustainable water management strategies in such areas the interactions among hydrological agricultural and ecological systems have to be adequately addressed in a quantitative way to this end various models have been developed to support iwrm and physically based and operational models are two major categories with regard to which part of the hydrological cycle is addressed physically based hydrological models can be further grouped into surface water sw models groundwater gw models and integrated sw gw models classic sw models such as swat arnold et al 1998 and gw flow models such as modflow harbaugh 2005 are widely used to study hydrological processes impacted by water management swat comprehensively considers agricultural practices e g planting irrigation fertilization harvest and tillage and has been applied successfully in agricultural watersheds for long term simulations zhang et al 2018a b c however subsurface flow processes are extremely simplified in swat modflow includes many packages to simulate water management practices such as modflow gwm ahlfeld et al 2005 modflow fmp schmid and hanson 2009 and modsim modflow morway et al 2016 but these packages have limited consideration of the surface hydrology and its interaction with gw a common limitation of these classic hydrological models in addressing irrigation in arid and semiarid areas is that the water losses in irrigation canals are often ignored or inadequately accounted for due to the complexity of the canal network and unknown operation rules in fact the seepage and evaporation in irrigation canals can account for 30 50 of the total transported water volume mohammadi et al 2019 in addition the impacts of ecological constraints such as environmental flow protection and gw conservation on irrigation have rarely been addressed by these classic models physically based integrated sw gw models simulate the complete terrestrial hydrological cycle including complex sw gw feedback with sufficient spatial and temporal detail representative models include parflow maxwell 2013 hydrogeosphere brunner and simmons 2012 pihm kumar et al 2010 and gsflow markstrom et al 2008 which have been successfully applied to study various water resource problems while such models represent the trend of hydrological modeling few can internally simulate the conjunctive use of sw and gw for irrigation under significant ecological constraints efforts have been made to enhance the representation of water management practices in physically based models for example condon and maxwell 2013 implemented a linear optimization water allocation algorithm in parflow tian et al 2015 embedded an advanced hydraulic module in gsflow to consider agricultural water management in representing irrigation processes however these improved models still require external data on sw diversion and distribution as well as gw pumping if gw is used unfortunately historical sw diversion and gw pumping records are usually difficult to collect in reality or of poor quality e g low spatial and temporal resolutions substantial errors etc if any exist more importantly the models by themselves are not able to depict irrigation activities under hypothetical scenarios e g hypothetical extreme events such as droughts and floods and regulation scenarios such as limiting sw diversion and or gw pumping and future climate conditions niswonger 2020 recently developed an agricultural water use ag package for gslfow and modflow which enables the simulation of dynamic water use in agriculture however similar to classic hydrological models the impact of ecological constraints on irrigation is not adequately addressed by existing integrated sw gw models operational models which are also commonly referred to as planning optimization or allocation models employ optimization algorithms to allocate water among multiple users with the purpose of maximizing the satisfaction of the water demand or the economic benefits under specific rules and constraints popular operational models include riverware zagona et al 2001 weap yates et al 2005 waterware jamieson and fedra 1996 and modsim berhe et al 2013 operational models are usually less computationally intensive and require less data input due to their simplification of hydrological processes making it easier for end users to compare operating policies or adjust parameters ecological constraints can also be readily addressed nevertheless this simplification may limit the capability of an operational model to simulate the interconnections within complex heterogeneous conjunctively managed systems rassam 2011 for example due to the time delays in the gw responses to sw it is difficult for operational models to approximate the finite and temporally varying gw resources and sw gw interactions as a result the effects of alternative gw management decisions may not be properly evaluated this study develops a water resource allocation wra module for integrated sw gw modeling within the context of the conjunctive use of sw and gw in ecologically vulnerable regions the module is coupled with an improved version of gsflow tian et al 2015 and the new integrated model is applied to the hrb which is the second largest endorheic river basin in china from the 1970s to the 2000s the overexploitation of water resources in the irrigation districts in the middle hrb mhrb reduced the streamflow to the lower hrb lhrb causing serious ecological issues li et al 2018 wang et al 2013 after the implementation of an ecological water diversion project ewdp in 2000 the ecosystems in the lhrb have been considerably restored m li et al 2019 zhou et al 2018 the agriculture ecosystem water conflict in the hrb is a common challenge encountered in endorheic river basins globally and its evolution under human intervention has notable management implications while a number of hydrological modeling studies liu et al 2018 sun et al 2018 yao et al 2018b zhao et al 2016 have been performed in this basin the interrelationship between the ewdp and the irrigation process has not been systematically addressed the main objectives of the study are 1 to develop advanced modules for an integrated hydrological model gsflow in this study to internally and explicitly simulate the irrigation water supply at high spatial and temporal resolutions under ecological constraints and 2 to unravel the complex nexus of water agriculture and ecosystems in arid and semiarid areas and investigate the impacts of typical water management policies on the nexus overall this study provides a powerful scientific tool and insights into the processes that support iwrm in endorheic river basins around the world 2 the integrated modeling approach 2 1 gsflow gsflow coupled groundwater surfacewater flow model is a physically based integrated sw gw model that has been applied to various problems such as climate change and water resource management feng et al 2018 hassan et al 2014 surfleet et al 2012 wu et al 2016 gsflow integrates the sw model prms with the gw flow model modflow nwt to simulate the surface hydrology top of the plant canopy to the soil zone base and three dimensional 3d gw flow base of the soil zone to the base of the aquifers in addition the sfr2 and lak packages in modflow are applied to simulate streams and lakes respectively the water exchange beneath the bottom of the soil zone is simulated by the modflow uzf1 package the land surface and soil zone in the prms is discretized into hydrologic response units hrus whose shape can be either irregular polygons or regular cells the subsurface zone is discretized with a finite difference grid representing computational cells streams are discretized into segments and reaches a reach is a stream section while a segment is a group of reaches lakes are represented by both the hrus in prms and a group of finite difference cells in modflow more details about the model are provided in markstrom et al 2008 the sfr2 package and well package contained in modflow can account for streamflow diversion and groundwater pumping respectively but have significant limitations water diversion in the sfr2 package niswonger and prudic 2005 needs to be predefined and its amount is constant within a stress period pumping rates in the well package also need to be predefined and their values are constant within a stress period as well furthermore the distribution of irrigation water from canals to fields was not accounted for by gsflow an improved version of gsflow called gsflow swmm was developed by embedding an advanced hydraulic engine provided by the storm water management model swmm into gsflow tian et al 2015 gsflow swmm enables the representation of an irrigation system with both sw diversion and gw pumping but it requires daily water diversion and pumping data as external model inputs this study develops a new module to simulate the demand based diversion and pumping rates driven by meteorological conditions subject to ecological constraints the module as well as its coupling with the improved version of gsflow is introduced in the next section 2 2 the water resource allocation module to estimate the crop irrigation requirements and appropriately partition them into sw from river diversion and gw from pumping fractions the wra module was developed particularly for integrated sw gw modeling at the large basin scale the wra module performs calculations on the irrigation water budget whose components include the crop water demand water supplies from sw and gw sources and water losses in canals the budget components are calculated based on spatially distributed hydrologic variables such as the soil moisture and gw head and streamflow as simulated by an integrated sw gw model gsflow in this study both gsflow and wra are run at a daily time step object oriented programming oop was adopted to develop the wra module which considers two unique types of objects i e the irrigation district and irrigation hru ihru objects the irrigation district object is adopted for simulating water management operations such as prioritizing sw and gw sources and restricting the gw level drawdown the ihru object refers to the hru with irrigation an irrigation district can contain a number of ihrus the ihrus within the same irrigation district adopt the same water management operations and divert sw from the same stream reach many agricultural areas have a complicated canal system it is difficult or time consuming to simulate the detailed water delivery processes in canals using traditional channel flow dynamics methods at the large basin scale at least thousands to tens of thousands of square kilometers on the other hand detailed pumping rate data from individual wells are usually unavailable and even if they are available it is technically infeasible to represent all individual wells in basin scale modeling as the number of wells can be very large therefore adequately representing the water losses from canals and internally estimating the gw pumpage are two challenges instead of routing water in the real canal system the wra module assumes that each ihru has a virtual canal and the water losses water evaporation and canal seepage are estimated for each virtual canal similarly the wra module defines a virtual pumping well in each ihru to represent gw exploitation a common management policy to prevent overpumping is to set a tolerable drawdown of the gw level to represent this policy option the wra module introduces a policy parameter the maximum groundwater level drawdown gld as the constraint for pumping if the drawdown with regard to a reference level defined by the manager at a virtual well exceeds the gld pumping is stopped and the final pumpage is set to zero more technical details on the wra module are provided below note that a recent study developed an ag package niswonger 2020 for gsflow to simulate the conjunctive use of sw and gw for irrigation our wra is different from this package in several aspects first the ag package utilizes the modflow sfr package to divert and route sw to fields and detailed information about the canal network and its physical properties e g cross sectional geometry hydraulic conductivity of the canal bed etc is required when modeling a large area with a complex canal network canal information is often unavailable or time consuming to collect and process in contrast the wra module in this study represents water diversion and routing in a simpler way using the concept of virtual canals and is therefore much easier to parameterize in large scale applications second in the current version of the ag package sw is assumed to be exploited first while the wra module allows the prioritization of different water sources 2 2 1 crop water demand the wra module calculates the crop water demand by either the scheduled or automatic irrigation method these two methods have been adopted in classic models such as swat and modflow but have rarely been considered with integrated sw gw models the scheduled irrigation method requires predefined timing and quota levels e g the depth of water irrigated during a certain time period for irrigation the crop water demand d i h r u i j t m3 d 1 for the i th i h r u in the j th irrigation district at time step t can be calculated as 1 d i h r u i j t a i h r u i j i q k t i 1 n h r u j j 1 n d f o r k 1 n c r o p where a i h r u i j m2 is the cropland area of the i th i h r u i q k t m d 1 is the net daily irrigation quota for crop type k at time step t n h r u j is the number of ihrus in the j th irrigation district n d is the total number of irrigation districts and n c r o p is the total number of crop types the crop types and their respective irrigation quotas are predefined by the modeler and can be either fixed throughout the entire simulation period or changed annually as part of the crop water demand can be fulfilled by natural precipitation a further adjustment is made 2 p d i h r u i j t e p a i h r u i j p i h r u i j t 3 a d i h r u i j t 0 i f p d i h r u i j t d i h r u i j t d i h r u i j t p d i h r u i j t i f p d i h r u i j t d i h r u i j t where p i h r u i j t m d 1 is the precipitation rate in terms of the water depth p d i h r u i j t m3 d 1 is the volumetric precipitation rate actually reaching the ground e p is the effective precipitation coefficient which represents the overall effect of canopy interception and a d i h r u i j t m3 d 1 is the adjusted crop water demand in the automatic irrigation method irrigation is triggered when the deficit between the soil water content and field capacity exceeds a certain threshold value 4 s m d i h r u i j t f c i h r u i j s m i h r u i j t 5 d i h r u i j t s f s m d i h r u i j t a i h r u i j i f s m d i h r u i j t s m t i h r u i j 0 i f s m d i h r u i j t s m t i h r u i j where s m i h r u i j t mm is the soil moisture content of the i th i h r u in the j th irrigation district at time step t f c i h r u i j mm is the field capacity s m d i h r u i j t mm and s m t i h r u i j mm are the soil moisture deficit and deficit threshold respectively and s f is a unit conversion factor that converts millimeters to meters whose value is equal to 0 001 similar to the scheduled irrigation method the crop water demand is adjusted using eq 3 when considering the contribution of natural precipitation 2 2 2 allocation of water sources the crop water demand can be satisfied by diversion from a river reach and the gw pumped from an underlying aquifer to determine the appropriate source allocation the wra module adopts a rule based approach that requires the preferences for different water sources to be specified in advance fig 1 shows the allocation procedure in an ihru which starts with the crop water demand the wra module allows irrigation districts to have specific preferences for water sources if sw is preferred the calculation procedure proceeds as described below first calculate the volumetric rate of the sw demand at the diversion point where river water is delivered from the p th reach to the j th irrigation district s w d d i s t r i c t p j t m3 d 1 as 6 s w d d i s t r i c t p j t i 1 n h r u j a d i h r u i j t c e i h r u i j where c e i h r u i j dimensionless is the canal efficiency of the i th ihru in the j th irrigation district the canal efficiency is defined as the ratio of the water amount reaching the fields to the water amount drawn at the diversion point which represents the efficiency of water transport in the virtual canal second apply any ecological constraints to adjust the sw demand according to 7 s w a r e a c h p j t e f r e a c h p j t s w d r e a c h p j t where s w a r e a c h p j t m3 d 1 is the adjusted sw demand and e f r e a c h p j t is an adjusting factor between 0 and 1 to represent the impact of the ecological constraints e f r e a c h p j t needs to be determined externally and separately by the modeler in this study an environment flow constraint module is further developed to calculate e f r e a c h p j t as introduced in section 2 3 if no ecological constraints are applied e f r e a c h p j t is set to 1 third estimate the streamflow potentially available at each diversion point using simple kinematic wave routing to alleviate the computational burden the routing calculation is isolated from the gsflow iteration process and uses the hydrological conditions of the previous day determined by the gsflow model a preliminary estimate of the sw diversion from the p th reach to the j th irrigation district at time step t denoted as d i v r e a c h p j t m3 d 1 can be calculated as 8 q a r e a c h p t m i n q r e a c h p t m d r e a c h p 9 d i v r e a c h p j t m i n s w a d i s t r i c t p j t q a r e a c h p t where q a r e a c h p t m3 d 1 is the volumetric rate of the water available for diversion from the p th reach at time step t q r e a c h p t m3 d 1 is the volumetric water rate flowing into the p th reach which is calculated by isolated stream routing and m d r e a c h p m3 d 1 is the maximum daily diversion capability from the p th reach which depends on the hydraulic facilities fourth estimate the volumetric water rate reaching the fields which is denoted as s w c i h r u i j t m3 d 1 by calculating the water balance in the virtual canal as 10 s w c i h r u i j t d i v r e a c h p j t a r i h r u i j c p i h r u i j t c e i h r u i j t c l i h r u i j t where c p i h r u i j t m3 d 1 is the volumetric precipitation rate in the virtual canal c e i h r u i j t m3 d 1 is the volumetric evaporation rate from the canal c l i h r u i j t m3 d 1 is the volumetric water leakage rate and a r i h r u i j dimensionless is the ratio of the water allocated to the i th ihru to the total water d i v r e a c h p t which is proportional to the ihru area because the flow velocity in the canals is high the storage change in the virtual canals is ignored fifth if the crop water demand is satisfied by sw no gw pumping is needed otherwise estimate the daily pumping rate g w i h r u i j t m3 d 1 as 11 g w i h r u i j t m i n a d i h r u i j t s w c i h r u i j t p m i h r u i j where p m i h r u i j m3 d 1 is the maximum daily pumping capability in the ihru which mainly depends on the pump power and aquifer properties estimating daily pumping rates internally by eq 11 is a unique feature of the wra module sixth enter the preliminarily estimated d i v r e a c h p j t and g w i h r u i j t into the gsflow simulation to obtain the final diversion and pumpage estimates gsflow has an internal iteration loop to adopt the preliminary diversion and pumping estimates as sink terms and generate the respective final estimates the extended gsflow model allows imposing a gw level drawdown constraint if the constraint is activated once the gw head decline compared to the head on the reference day specified by the modeler in an ihru exceeds the maximum daily allowed value the pumpage is set to zero on that day finally update the canal water balance to obtain the final estimate of the diverted water reaching the fields the final diversion and pumpage are distributed to each ihru in the next time step in cases where sw is limited e g far away from the river or when sw diversion is costly and gw is preferred the calculation proceeds as follows see fig 1 first estimate the daily gw pumping rate in each ihru as 12 g w i h r u i j t m i n a d i h r u i j t p m i h r u i j second if the crop water demand can be satisfied by gw alone no sw diversion is needed otherwise calculate the sw demand at a diversion point as 13 s w d d i s t r i c t p j t i 1 n h r u a d i h r u i j t g w i h r u i j t c e i h r u i j third derive the sw diversion amount using eqs 6 9 as described above finally enter the preliminarily estimated sw diversionand gw pumping in the gsflow simulation to obtain the final diversion and pumpage estimates which are distributed to the ihru in the next time step as described above the wra module employs a rule based framework that can flexibly emulate different partitioning schemes of surface water and groundwater in the irrigation water supply varying from surface water only to groundwater only for example if the maximum daily pumping capability in eq 11 is set to zero the irrigation district will use only surface water for irrigation in contrast if the adjusting factor in eq 7 is set to zero the irrigation district will use only groundwater 2 2 3 canal water losses the water losses from the virtual canals are calculated based on eq 10 the surface runoff interflow gw discharge to the canal and storage change in the canal are ignored in the equation which is consistent with previous studies alam and bhutta 2004 kinzli et al 2010 meredith and blais 2019 zhang et al 2017 in arid regions shallow gw is often meters beneath the ground thus it is unusual that the gw table exceeds the canal bottom and gw discharges into the canals additionally according to our field investigations canals have water storage when water diversion and irrigation are operated but are nearly empty beyond the operational period the wra module assigns a virtual canal to each ihru at this small spatial scale it is reasonable to assume that water diversion and irrigation are operated for a part of an irrigation day and therefore the water storage in a virtual canal would not undergo a significant storage change at the daily time scale the precipitation in the canal is calculated as follows 14 c p i h r u i j t p i h r u i j t a h r u i j r c i h r u i j where r c i h r u i j dimensionless is the ratio of the canal surface area to the ihru area the daily canal evaporation c e i h r u i j t m3 d 1 is calculated as follows 15 c i i h r u i j t d i v r e a c h p j t a r i h r u i j c p i h r u i j t 16 c p e i h r u i j t p e t i h r u i j t a h r u i j r c i h r u i j 17 c e i h r u i j t c p e i h r u i j t w h e n c i i h r u i j t c p e i h r u i j t c i i h r u i j t w h e n c i i h r u i j t c p e i h r u i j t where c i i h r u i j t m3 d 1 is the volumetric water rate flowing into the canal p e t i h r u i j t m d 1 is the potential evapotranspiration et flux across the i th ihru and c p e i h r u i j t m3 d 1 is the volumetric rate of the potential et attributed to the virtual canal the maximum volumetric rate of the water available for downward leakage c l i h r u i j t m3 d 1 is calculated as follows 18 c l i h r u i j t c i i h r u i j t c e i h r u i j t 19 c l f i h r u i j t c l i h r u i j t a h r u i j where c l f i h r u i j t m d 1 is the flux of c l i h r u i j t when calculating the mass balance for a virtual canal only downward leakage is considered mohammadi et al 2019 the downward leakage flux is directly applied to the unsaturated zone in the ihru and the net gravity leakage is set to the potential gravity leakage flux c l f i h r u i j t calculated above if the net gravity leakage flux c l f i h r u i j t exceeds the vertical hydraulic conductivity of the unsaturated zone k s i j it is set as k s i j and the amount in excess of k s i j is returned to the canal if c l i h r u i j t equals zero the downward leakage is also set to zero 2 2 4 coupling with gsflow the wra module is integrated into gsflow at the iteration level the computational sequence of the integrated simulation is illustrated in fig 2 the major steps are introduced below i run the gsflow declaration initialization allocation and read procedures upon gsflow initialization the wra module is also initialized the wra objects including the irrigation districts and ihrus are loaded from corresponding input files ii begin the daily time step loop and if the loop is at time step 1 and the initial stress period is at the steady state execute the modflow module in gsflow and exclude the prms i e the surface hydrological module in gsflow and wra computations from the initial steady state stress period otherwise execute the modflow prms and wra procedures iii simulate the land surface and soil zone hydrologic processes in the prms and obtain the preliminary stream diversion and gw pumping estimates when simulating the soil zone processes the final diversion and pumpage estimates after the previous time step are combined with the net precipitation reaching the soil surface at the present step iv begin the iteration loop the preliminary stream diversion and gw pumping estimates by the wra module see fig 1 are added as sink terms to the modflow modules in gsflow the interdependent equations within gsflow are solved using iteration loops such as the gw head and head dependent flow during the solution process the gw level drawdown constraint is assessed once the iteration is stopped the final stream diversion and gw pumpage estimates are obtained v compute the water budget in each zone the whole basin and all irrigation districts write the values of the state and flux variables in the surface and subsurface zones and irrigation districts to output files vi repeat the time loop steps iii v until the end of the simulation period when the simulation ends close all files and clear the computer memory 2 3 the environmental flow constraint module environmental flow regulation is a common water resource management practice that has been implemented in many river basins such as the murray darling basin in australia kirby et al 2014 the ebro river basin in spain almazán gómez et al 2018 the colorado river in the united states kendy et al 2017 and the tarim river basin in china xue et al 2017 environmental flow regulation is needed in these river basins to address some common challenges such as land degradation poff et al 2017 deterioration in water dependent ecosystems yao et al 2018a and intensive human nature competition for limited water resources environmental flow regulation inevitably reshapes the regional irrigation behavior and the approach used to account for the impact in integrated hydrological models at the basin scale remains a challenge as mentioned in the introduction an ewdp has been implemented in the hrb in 2000 which represents a typical environmental flow regulation scheme the hrb is the second largest endorheic basin in china fig 3 the heihe river originates in the qilian mountains located along the margin of the qinghai tibetan plateau and then flows northward into the terminal lake east juyan lake in the gobi desert yingluoxia and zhengyixia are two dividing points defining the upper middle and lower reaches of the main heihe river fig 3 the ewdp in the hrb specifies five minimum required outflows at zhengyxia the dividing point between the middle and lower reaches under different inflow conditions at yingluoxia the dividing point between the upper and middle reaches based on these five control points a flow regulation curve frc with a linear shape can be drawn fig 4 a the frc imposes critical constraints on the water consumption in the mhrb and represents the rule used to allocate the runoff between the mhrb and lhrb however the frc only specifies the total annual environmental flow amount without any explicit advice on how to achieve this goal in real management practice the frc is implemented by the simultaneous shutdown approach which means that all the diversion points along the midstream of the heihe river are mandatorily closed at the same time during particular time windows of a year a shutdown window typically lasts for several days the decision regarding the number length and timing of the yearly shutdown periods is made by the basin water resources agency in an ad hoc manner the decision making approach is largely empirical without formal and rigorous calculations considering a number of factors including historical and recent meteorological hydrological and socioeconomic conditions in fact from 2002 to 2015 the required flow volumes per frc were never fully met and the average gap was 0 17 109 m3 a in this study an environmental flow constraint efc module is developed as a formal approach to represent regulation decisions this efc module provides the ecological constraint factor e f r e a c h p j t in eq 7 to the wra module the efc module includes an algorithm to manipulate the cumulative streamflow at zhengyixia on a daily basis considering its target annual value per the frc fig 4b illustrates the principle of the manipulation algorithm let tan α denote the ratio of the annual outflow at zhengyixia to the inflow at yingluoxia tan α 0 represent the condition strictly following the frc and tan α m a x and tan α min represent the minimum and maximum allowed water diversion conditions respectively at each time step the algorithm reduces the difference between the target outflow r t and actual outflow r t based on two principles first less more water is diverted at the next time step if r t is smaller larger than the target outflow r t at the present time step and second a higher adjustment is needed when there is a larger discrepancy between r t and r t let tan β t 1 denote the ratio of the outflow δ r t 1 p r e d to the inflow δ i t 1 p r e d at time step t 1 i e tan β t 1 δ r t 1 p r e d δ i t 1 p r e d the abovementioned two rules can be modeled as eqs 20 and 21 respectively 20 β t 1 λ t α m a x 1 λ t α 0 i f r t r t λ t α m i n 1 λ t α 0 i f r t r t 21 λ t k r t r t r t i f r t r t k r t r t r t i f r t r t where k 0 is a behavioral parameter measuring the extent to which the water manager seeks to reduce the discrepancy between r t 1 and r t 1 a larger k represents a more aggressive adjustment the allowable water diversion amount which is denoted as δ w t 1 at time step t 1 can be approximated as 22 δ w t 1 δ i t 1 p r e d 1 tan β t 1 then the ecological constraint factor e f r e a c h p j t in the wra module see eq 9 can be calculated as 23 e f r e a c h p j t 1 1 i f δ w t 1 d i v r e a c h p j t δ w t 1 d i v r e a c h p j t i f δ w t 1 d i v r e a c h p j t through the adaptive adjustment of e f r e a c h p t the derived cumulative streamflow at zhengyixia approaches its target value per the frc it is worth emphasizing that eqs 20 23 are not necessarily applied in the real decision making process but they capture the major behavioral features of decision makers and appropriately reflect the effect of the ecological flow constraint i e the frc 3 study area and management scenarios 3 1 study area and data the hrb can be divided into upper middle and lower reaches fig 3 from the upper stream to the lower stream the elevation of the hrb changes from 5500 900 m the landscape changes from alpine vegetation in the upper stream to agricultural ecosystems in the middle stream and to riparian ecosystems and gobi deserts in the lower stream the annual precipitation changes from 407 mm y in the upper stream to 189 mm y in the middle stream and to 52 mm y in the lower stream li et al 2018 the major land cover types in the mhrb and lhrb are croplands forests grasslands wetlands built up areas the gobi desert and vacant lands intensive irrigated agriculture has long been implemented in the mhrb the dominant crop is maize followed by wheat sunflowers melons beans and others the irrigation season starts from late march and ends in late november there are 20 major irrigation districts in the mhrb fig 5 and the total irrigated cropland area exceeded 233 000 ha as of 2007 the water sources used for irrigation in the mhrb consist of sw from the main heihe river and tributaries 70 gw 23 and precipitation 7 liu et al 2010 sw is delivered through a complex canal network the total length exceeds 12 430 km towards the croplands gw is pumped from more than 10 000 wells in our previous study tian et al 2018 an improved version of gsflow was calibrated and validated for the entire mhrb and lhrb from 2000 to 2012 this existing model is equipped with the wra module and the efc module developed in this study with the calibrated prms and modflow parameters unchanged and the modeling period extended to 2015 16 years in total the meteorological datasets produced by xiong and yan 2013 were collected for the period of 2000 2015 and used as the driving forces the daily streamflow observations at zhengyixia and gaoya see figs 3 and 5 from 2000 to 2015 and the monthly gw levels measured at 80 monitoring wells from 2000 to 2012 were used to evaluate the performance of the extended model geospatial data e g land use and canal networks and statistical reports were collected and used to parameterize the wra module the cropland area i e a i h r u i j in each ihru was determined based on the land use data the gis layers of the canal network in 2007 were adopted to estimate the ratio of the canal surface area to the ihru area i e r c i h r u i j in the case shown the scheduled irrigation method is adopted to calculate the crop water demand a typical irrigation schedule is presented in fig s1 in the supplementary materials which was determined based on the two major crops in the study area maize and wheat different irrigation districts share the same temporal pattern of irrigation schedules with different total water quotas the annual average canal efficiency and the daily scale irrigation schedule were derived for each irrigation district based on the annual water resource reports 2000 2015 provided by the local government the total irrigation quota for an individual district equals the sum of the scheduled daily irrigation water all ihrus in the same irrigation district share the same canal efficiency and irrigation schedule in the previous model tian et al 2018 land cover datasets in three different periods were used to configure the time variant land use in the extended model three wra input files were generated corresponding to the land uses in 2000 2006 2007 2010 and 2011 2015 in each of the three simulation segments the wra parameters remain fixed table 1 lists the values of key wra parameters in the 2000 2006 simulation segment and the parameter values in the other two simulation segments are summarized in tables s1 and s2 in the supplementary materials 3 2 water management scenarios to investigate the impacts of different water management strategies on the hydrological cycle in the hrb two types of management scenarios in addition to the baseline scenario i e the actual conditions are hypothesized as summarized in table 2 sw is preferred in all scenarios while in the baseline scenario environmental flow regulation is implemented as the simultaneous shutdown operation in the model this operation is represented by setting e f r e a c h p j t see eq 7 to zero on the shutdown days per the historical records maintained by the heihe river bureau in the baseline scenario both the impacts of the frc and the maximum groundwater level drawdown gld constraint are not explicitly simulated but are implicitly reflected by historical shutdown operations simulation of the baseline scenario requires the wra module with the gw constraint deactivated but not the efc module the type a scenarios are hypothesized to investigate the impact of gw management policies on the hydrological cycle in which the gw constraint is activated and different maximum gld values are specified the heads computed at the end of the first year are used as the reference heads for computing the gld gw pumping has been largely unregulated in the study area only in very recent years has the local government started to meter the gw pumpage in certain irrigation districts it is expected that gw pumping will be strictly regulated by the local government in the future ideally every pumping well can be monitored in real time and remotely controlled once the gld at a well exceeds the predefined maximum value pumping would be mandatorily stopped the type a scenarios reflect this expected regulation design the maximum gld values are set to 1 2 3 4 and 5 m and are tested in scenarios a1 to a5 see table 2 the stopped pumping ratio spr is further defined as 26 s p r d a y s t o p p e d d a y t o t a l where d a y s t o p p e d is the number of days on which gw pumping is stopped because the gld value exceeds the maximum allowed value and d a y t o t a l is the total number of days when gw is needed for irrigation spr is between 0 and 1 the type b scenarios are hypothesized to investigate the effects of the efc in scenario b1 the efc module is implemented but no gld constraint is applied in scenarios b2 and b3 both environmental flow and gld constraints are applied but the maximum gld values are 5 and 1 m respectively 4 results and discussion 4 1 high resolution simulated diversion and pumping the extended gsflow model is run from 2000 to 2015 at a daily time step under the baseline condition with simultaneous shutdown operations and no gld constraints table 2 the first year i e 2000 is the warm up period during which to establish the initial soil zone and unsaturated zone storage levels huntington and niswonger 2012 but is excluded from the following analysis fig 6 a and b compare the mean annual sw diversion and gw pumpage levels simulated by the model respectively against the statistics derived from the historical data of the 20 irrigation districts the simulation results match the data very well at the irrigation district scale as indicated by the high r2 values more importantly the wra module provides a systematic way to derive high resolution diversion and pumping data when only annual or monthly statistics are available which is a common situation in reality fig 6c illustrates the simulated daily time series of the diversion and pumping levels in the baseline scenario the capacity of the wra module to derive high resolution diversion and pumping data is also of great value for evaluating future scenarios fig 7 a further illustrates the simulated source composition of the irrigation water at the irrigation district level the spatial heterogeneity is notable some districts mainly depend on sw while others rely more on gw for example sw accounts for 88 and 89 of the total water supply in districts 1 and 4 respectively while gw accounts for 83 of the total water supply in district 15 irrigation districts 1 and 4 are located in the upper reaches of the heihe river and their diversion demands are prioritized in contrast irrigation district 15 is located in the lower reaches of the heihe river and far away from the river and the available streamflow amount for diversion is limited thus more gw is pumped to meet the irrigation demand overall sw gw and precipitation account for 67 25 and 8 of the total water supply in the 20 irrigation districts respectively table s3 in the supplementary materials provides more details on the water demand supply and losses in the 20 irrigation districts these estimated ratios are consistent with a previous study zhang et al 2018a b c fig 7b shows the spatial pattern of the water losses in the canals at the irrigation district scale in all irrigation districts except districts 6 8 and 15 canal drainages account for the majority of the canal water loss in general canal drainage accounts for 75 of the canal water loss the magnitude and spatial pattern of the simulated water supply components are consistent with previous studies li et al 2018 liu et al 2016 in fact the wra module can also provide simulation results similar to fig 7 at the hru level 9858 ihrus in total within the modeling domain fig s5 in the supplementary materials provides an example which is an advantage of this module because such detailed data are usually unavailable in reality 4 2 shift in the hydrological regime under groundwater management as mentioned previously the regulation of gw pumping has been largely overlooked in the hrb but has recently gained increasing management attention to examine the potential impact of pumping regulations on the hydrological cycle and irrigation behavior the type a scenarios were simulated with the extended model under the simultaneous shutdown operation table 2 scenario a1 represents the strictest regulation while scenario a5 represents the least strict regulation fig 8 shows the key water budget components and changes in gw storage in the mhrb based on the model simulations demonstrating the considerable impact of the constraint as expected gw pumpage is reduced in all four scenarios fig 10a gld based gw management is beneficial for gw storage recovery fig 8b in comparison with the baseline scenario gw storage in scenario a1 is greatly recovered however the reduction in gw pumpage results in a reduction in et fig 8c because less water is supplied for irrigation and the streamflow at the zhengyixia gauging station is increased fig 8d as indicated by scenario a1 limiting the groundwater level drawdown compared to the historical condition within 1 m would lead to a 3 9 decrease in cropland et in the middle basin and a 1 5 increase in the environmental flow towards the lower basin overall fig 8 shows that the hydrological system in the mhrb responds to gw regulation and tuning the regulation would effectively alter the complex nexus between the agricultural production in the mhrb as represented by the et the gw resources in the mhrb and the sw accessible to the ecosystems in the lhrb an important implication is that the gld constraint needs to be carefully determined on a scientific basis to address the water conflict between agriculture and ecosystems fig 8a also compares the averaged spr in the five type a scenarios it is evident that the spr increases nonlinearly with an accelerating tendency with the strengthening of the gw pumping restriction the loss of agriculture would increase sharply as the gw regulation is tightened however gw would be better recovered with tighter regulations which is beneficial for the wetlands and other ecosystems in the mhrb therefore it is necessary to balance the ecological services and agricultural production in the mhrb when designing gw management policies fig 9 depicts the substantial spatial heterogeneity in the spr in scenarios a1 and a5 as shown in fig 9a under the most rigid regulation i e scenario a1 most of the ihrus in irrigation districts 6 9 13 18 and 20 have a zero spr indicating no impact from the gld constraint according to previous studies hu et al 2016 intensive river gw interactions occur in the stream segments flowing through these districts which lead to quick replenishment of the shallow aquifer beneath these districts thus diminishing the impact of the gld constraint in contrast a large portion of the ihrus in districts 8 11 12 and 15 have sprs over 70 these districts are away from the main river and have much weaker sw gw interactions tian et al 2018 thus these districts are more sensitive to gw regulations fig 9b shows the spatial distribution of the spr in scenario a5 in which no districts are notably impacted by the regulation except district 15 fig 9 reveals two important management implications first gw management policies need to be spatially heterogeneous to cope with the heterogeneity in hydrogeological conditions second there exists a gw management policy threshold below which the policy will have a trivial effect and therefore a rigorous assessment such as the one performed here is critical for identifying this threshold fig 10 further illustrates the temporal pattern of the spr in scenario a1 the color of an ihru indicates a month during which the ihru attains a monthly spr peak value fig 10 shows that the monthly spr peaks during the flood season june july and august in most ihrus except those with spr 0 gray color in the figure from june to august the irrigation demand in the study area peaks and gw pumping is intensified to bridge the gap between the demand and available streamflow however gw recharge is not able to fully compensate for the exploitation during the flood season because the velocity of the infiltrated water traversing the vadose zone is much lower than the pumping rate as a result the gw level continues to decrease until the gld reaches its limit and pumping is stopped it is also noted that the monthly spr is the highest in october in several irrigation districts this timing is related to the custom irrigation practices in the hrb in late october or early november while no crops are planted croplands are still irrigated to maintain the soil moisture level which is beneficial for the production next year fig 10 implies that a gw management policy with temporal variability may be desirable during critical periods in the study area such as june to august and october gw regulation could be loosened to avoid agricultural production damage due to water deficits and in other periods a tighter constraint could be applied 4 3 water conflicts influenced by management policies as mentioned previously the present diversion management under the simultaneous shutdown operation is subject to an ad hoc decision process conditioned upon historical and recent meteorological hydrological and socioeconomic conditions in fact the frc has never been fully met in the past and the gap was even larger in wet years fig 11 the reasoning behind this result is threefold first a systematic approach to planning diversions under environmental flow regulation is lacking second the regulation is not equipped with effective incentive and or penalty mechanisms and therefore the midstream irrigation districts commonly tend to overuse the streamflow scenario b1 emulates the situation in which the midstream districts are wholly motivated to achieve the designed environmental flow goals and the approach i e the efc module proposed in section 2 3 is applied fig 11 shows that the efc module works well in reconfiguring the midstream diversion towards attaining the regulation goals table 4 compares scenario b1 against the baseline scenario reflecting the actual conditions with regard to key hydrological variables to meet the regulation goals the mhrb would have to reduce its diversion 21 85 and increase its pumpage for compensation 7 04 but the reduction in the total irrigation water supply would still be remarkable 15 94 the change in the water supply is beneficial to the ecosystems in the lhrb as the environmental flows both through zhengyixia and into the end lake are notably increased 22 47 and 51 92 respectively and gw storage in the lhrb is increased 15 28 however enhanced ecological security is attained at the cost of losing agricultural production in the mhrb as indicated by the cropland et 8 57 and the accelerated depletion of gw storage in the mhrb 43 37 scenarios b2 and b3 are simulated to further evaluate the consequences of adding gw management to the environmental flow regulation and their comparison against the baseline scenario and scenario b1 is also summarized in table 4 scenario b2 represents light gw control and scenario b3 represents tight gw control table 4 indicates that implementing the gw level drawdown constraint would inhibit gw overpumping but would further exacerbate the reduction in the total irrigation water supply see the shaded numbers in the table gw storage and cropland et in the mhrb are sensitive to such changes with regard to both the absolute and relative differences while the other variables are not in the extreme case with a tight pumping limit scenario b3 the cropland et in the mhrb would be reduced by 13 04 while compared to the baseline scenario the decrease in gw storage in the mhrb would be slowed by 56 63 this comparison indicates that the impact of gw regulation is largely confined within the mhrb and has a much smaller transboundary effect than that of environmental flow regulation overall the above analysis demonstrates the complex nexus among water agriculture and ecosystems in the mhrb and lhrb as well as the water conflict between the two subbasins and how it would evolve under conjunctive sw and gw management there are several important associated water resource management implications for arid areas similar to the mhrb and lhrb first management policies need to maintain an appropriate balance between water security e g gw storage socioeconomic development e g agriculture and ecological services e g vegetation and wetlands to this end an appropriate approach to weighting competing goals is critical but remains a challenge in the hrb whether the frc is appropriate and how much inflow into the terminal lake is needed have long been disputed second in water limited areas it may be very difficult if not impossible to satisfy all competing systems or subareas simply through water allocation and specific fundamental shifts are necessary such as introducing water saving technologies for irrigation sun et al 2018 and converting agriculture into a more water efficient industry with the belt and road initiative bri of china inland china as well as vast central asia is anticipated to experience more infrastructure construction and faster urbanization which may provide an opportunity for these areas to alleviate their current water conflicts 5 conclusions this study developed modules for emulating high resolution daily resolution in time and down to the model grid size in space irrigation activities and representing water management policies in an integrated hydrologic model the improved gsflow model with the new modules addresses the challenge of hydrological simulations and predictions in areas with intensive agricultural irrigation but limited data on irrigation practices and enables the systematic assessment of the complex nexus of water agriculture and ecosystems the improved gsflow model also offers powerful support for iwrm in arid and semiarid areas as a case study the model is applied in the hrb which is the second largest endorheic river basin in china with typical water conflicts between agricultural development and ecological conservation the major study findings include the following first the model attains a good performance in reconstructing high resolution streamflow diversion and gw pumping data from coarse information as well as in emulating the procedure of environmental flow regulation second the hydrological regime in the mhrb is sensitive to gw management in terms of the gld limit imposing a tight limit would be beneficial for the ecological services in the mhrb due to gw storage recovery but would cause a significant loss in agricultural production due to the irrigation water supply deficit which is a critical tradeoff to be balanced via management third sw management i e environmental flow regulation and gw management i e gld constraints have both synergistic e g in reducing the irrigation water supply and offset e g in maintaining gw storage effects reflecting the complex nexus of water agriculture and ecosystems in the mhrb and lhrb this study has important water resource management implications for arid and semiarid areas first gw management policies with spatial heterogeneity and temporal variability are highly desirable particularly in areas where the hydrogeological conditions vary notably in space second it is critical to identify the threshold effects of policies in the hrb for instance there exists a gw management policy threshold below which the policy will have a trivial effect third management policies need to maintain an appropriate balance between water security e g gw storage socioeconomic development e g agriculture and ecological services e g vegetation and wetlands finally in water limited areas it may be very difficult if not impossible to satisfy all competing systems or subareas simply through water allocation and certain fundamental shifts may be necessary such as introducing water saving technologies for irrigation and converting agriculture into a more water efficient industry the current model can be further improved to better represent complex human nature interactions and support the decision making process for example the water allocation method in the model is currently rule based and optimization methods can be employed in future studies modeling capability for the operation of reservoirs weirs and other hydraulic structures is also desirable for a better understanding of the hydrological cycle in endorheic river basins influenced by considerable human intervention and climate change declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the strategic priority research program of the chinese academy of sciences xda20100104 and the national natural science foundation of china no 42071244 no 51961125203 no 41807164 and no 41861124003 the improved gsflow model with the new modules can be freely downloaded from https github com deephydro wra appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104874 
25951,modeling agriculture impacted hydrological processes remains a challenge as information on irrigation activities is often insufficient this study developed modules for emulating high resolution irrigation activities and representing water management policies in integrated hydrologic modeling the new modeling approach was implemented in china s second largest endorheic river basin with intensive irrigation and exemplary environmental flow regulation pumping management is found to impact the hydrological regime at the basin scale and modulate the tradeoff between ecological services and agriculture limiting the groundwater level decline within 1 m would lead to a 3 9 decrease in cropland evapotranspiration in the middle basin and a 1 5 increase in the environmental flow towards the lower basin environmental flow regulation and pumping management have both synergistic and offset effects reflecting a complex water agriculture ecosystem nexus in this basin this modeling study suggests heterogeneous and dynamic groundwater management policies in water limited areas and unravels threshold effects in the policies keywords integrated hydrological model environmental flow endorheic river basin irrigation groundwater management water resources 1 introduction irrigated agriculture is an important aspect of food production b li et al 2019 but it consumes a large amount of water resources as well it is estimated that irrigation accounts for 70 of the water diverted from rivers globally hoekstra and mekonnen 2012 arid and semiarid areas cover approximately 31 of the land surface on earth and support over 1 3 of the global human population pravalie 2016 where water resource scarcity is the pivot of human nature conflicts due to the rapid population growth and expansion of irrigated agriculture ecological degradation has become widespread in arid and semiarid areas barnett et al 2008 a well known ecological tragedy is the aral sea which was formerly the fourth largest lake in the world the aral sea has been steadily shrinking since the 1960s due to the diversion of the rivers feeding the lake for the irrigation projects of the former soviet union cretaux et al 2013 varis 2014 the desiccation of the aral sea caused notable desertification and soil salinization and greatly increased the number of dust and salt storms in that region bosch et al 2007 similar ecological disasters have occurred in other endorheic areas such as lake urmia in iran stone 2015 the tarim river basin bao et al 2017 and the heihe river basin hrb in northwestern china cheng et al 2014 in arid and semiarid areas integrated water resources management iwrm is important for alleviating the competition between agriculture and other water consumers including the industrial domestic and ecological sectors the conjunctive use of surface water sw and groundwater gw is a common practice in iwrm mani et al 2016 wu et al 2014 2015 to develop sustainable water management strategies in such areas the interactions among hydrological agricultural and ecological systems have to be adequately addressed in a quantitative way to this end various models have been developed to support iwrm and physically based and operational models are two major categories with regard to which part of the hydrological cycle is addressed physically based hydrological models can be further grouped into surface water sw models groundwater gw models and integrated sw gw models classic sw models such as swat arnold et al 1998 and gw flow models such as modflow harbaugh 2005 are widely used to study hydrological processes impacted by water management swat comprehensively considers agricultural practices e g planting irrigation fertilization harvest and tillage and has been applied successfully in agricultural watersheds for long term simulations zhang et al 2018a b c however subsurface flow processes are extremely simplified in swat modflow includes many packages to simulate water management practices such as modflow gwm ahlfeld et al 2005 modflow fmp schmid and hanson 2009 and modsim modflow morway et al 2016 but these packages have limited consideration of the surface hydrology and its interaction with gw a common limitation of these classic hydrological models in addressing irrigation in arid and semiarid areas is that the water losses in irrigation canals are often ignored or inadequately accounted for due to the complexity of the canal network and unknown operation rules in fact the seepage and evaporation in irrigation canals can account for 30 50 of the total transported water volume mohammadi et al 2019 in addition the impacts of ecological constraints such as environmental flow protection and gw conservation on irrigation have rarely been addressed by these classic models physically based integrated sw gw models simulate the complete terrestrial hydrological cycle including complex sw gw feedback with sufficient spatial and temporal detail representative models include parflow maxwell 2013 hydrogeosphere brunner and simmons 2012 pihm kumar et al 2010 and gsflow markstrom et al 2008 which have been successfully applied to study various water resource problems while such models represent the trend of hydrological modeling few can internally simulate the conjunctive use of sw and gw for irrigation under significant ecological constraints efforts have been made to enhance the representation of water management practices in physically based models for example condon and maxwell 2013 implemented a linear optimization water allocation algorithm in parflow tian et al 2015 embedded an advanced hydraulic module in gsflow to consider agricultural water management in representing irrigation processes however these improved models still require external data on sw diversion and distribution as well as gw pumping if gw is used unfortunately historical sw diversion and gw pumping records are usually difficult to collect in reality or of poor quality e g low spatial and temporal resolutions substantial errors etc if any exist more importantly the models by themselves are not able to depict irrigation activities under hypothetical scenarios e g hypothetical extreme events such as droughts and floods and regulation scenarios such as limiting sw diversion and or gw pumping and future climate conditions niswonger 2020 recently developed an agricultural water use ag package for gslfow and modflow which enables the simulation of dynamic water use in agriculture however similar to classic hydrological models the impact of ecological constraints on irrigation is not adequately addressed by existing integrated sw gw models operational models which are also commonly referred to as planning optimization or allocation models employ optimization algorithms to allocate water among multiple users with the purpose of maximizing the satisfaction of the water demand or the economic benefits under specific rules and constraints popular operational models include riverware zagona et al 2001 weap yates et al 2005 waterware jamieson and fedra 1996 and modsim berhe et al 2013 operational models are usually less computationally intensive and require less data input due to their simplification of hydrological processes making it easier for end users to compare operating policies or adjust parameters ecological constraints can also be readily addressed nevertheless this simplification may limit the capability of an operational model to simulate the interconnections within complex heterogeneous conjunctively managed systems rassam 2011 for example due to the time delays in the gw responses to sw it is difficult for operational models to approximate the finite and temporally varying gw resources and sw gw interactions as a result the effects of alternative gw management decisions may not be properly evaluated this study develops a water resource allocation wra module for integrated sw gw modeling within the context of the conjunctive use of sw and gw in ecologically vulnerable regions the module is coupled with an improved version of gsflow tian et al 2015 and the new integrated model is applied to the hrb which is the second largest endorheic river basin in china from the 1970s to the 2000s the overexploitation of water resources in the irrigation districts in the middle hrb mhrb reduced the streamflow to the lower hrb lhrb causing serious ecological issues li et al 2018 wang et al 2013 after the implementation of an ecological water diversion project ewdp in 2000 the ecosystems in the lhrb have been considerably restored m li et al 2019 zhou et al 2018 the agriculture ecosystem water conflict in the hrb is a common challenge encountered in endorheic river basins globally and its evolution under human intervention has notable management implications while a number of hydrological modeling studies liu et al 2018 sun et al 2018 yao et al 2018b zhao et al 2016 have been performed in this basin the interrelationship between the ewdp and the irrigation process has not been systematically addressed the main objectives of the study are 1 to develop advanced modules for an integrated hydrological model gsflow in this study to internally and explicitly simulate the irrigation water supply at high spatial and temporal resolutions under ecological constraints and 2 to unravel the complex nexus of water agriculture and ecosystems in arid and semiarid areas and investigate the impacts of typical water management policies on the nexus overall this study provides a powerful scientific tool and insights into the processes that support iwrm in endorheic river basins around the world 2 the integrated modeling approach 2 1 gsflow gsflow coupled groundwater surfacewater flow model is a physically based integrated sw gw model that has been applied to various problems such as climate change and water resource management feng et al 2018 hassan et al 2014 surfleet et al 2012 wu et al 2016 gsflow integrates the sw model prms with the gw flow model modflow nwt to simulate the surface hydrology top of the plant canopy to the soil zone base and three dimensional 3d gw flow base of the soil zone to the base of the aquifers in addition the sfr2 and lak packages in modflow are applied to simulate streams and lakes respectively the water exchange beneath the bottom of the soil zone is simulated by the modflow uzf1 package the land surface and soil zone in the prms is discretized into hydrologic response units hrus whose shape can be either irregular polygons or regular cells the subsurface zone is discretized with a finite difference grid representing computational cells streams are discretized into segments and reaches a reach is a stream section while a segment is a group of reaches lakes are represented by both the hrus in prms and a group of finite difference cells in modflow more details about the model are provided in markstrom et al 2008 the sfr2 package and well package contained in modflow can account for streamflow diversion and groundwater pumping respectively but have significant limitations water diversion in the sfr2 package niswonger and prudic 2005 needs to be predefined and its amount is constant within a stress period pumping rates in the well package also need to be predefined and their values are constant within a stress period as well furthermore the distribution of irrigation water from canals to fields was not accounted for by gsflow an improved version of gsflow called gsflow swmm was developed by embedding an advanced hydraulic engine provided by the storm water management model swmm into gsflow tian et al 2015 gsflow swmm enables the representation of an irrigation system with both sw diversion and gw pumping but it requires daily water diversion and pumping data as external model inputs this study develops a new module to simulate the demand based diversion and pumping rates driven by meteorological conditions subject to ecological constraints the module as well as its coupling with the improved version of gsflow is introduced in the next section 2 2 the water resource allocation module to estimate the crop irrigation requirements and appropriately partition them into sw from river diversion and gw from pumping fractions the wra module was developed particularly for integrated sw gw modeling at the large basin scale the wra module performs calculations on the irrigation water budget whose components include the crop water demand water supplies from sw and gw sources and water losses in canals the budget components are calculated based on spatially distributed hydrologic variables such as the soil moisture and gw head and streamflow as simulated by an integrated sw gw model gsflow in this study both gsflow and wra are run at a daily time step object oriented programming oop was adopted to develop the wra module which considers two unique types of objects i e the irrigation district and irrigation hru ihru objects the irrigation district object is adopted for simulating water management operations such as prioritizing sw and gw sources and restricting the gw level drawdown the ihru object refers to the hru with irrigation an irrigation district can contain a number of ihrus the ihrus within the same irrigation district adopt the same water management operations and divert sw from the same stream reach many agricultural areas have a complicated canal system it is difficult or time consuming to simulate the detailed water delivery processes in canals using traditional channel flow dynamics methods at the large basin scale at least thousands to tens of thousands of square kilometers on the other hand detailed pumping rate data from individual wells are usually unavailable and even if they are available it is technically infeasible to represent all individual wells in basin scale modeling as the number of wells can be very large therefore adequately representing the water losses from canals and internally estimating the gw pumpage are two challenges instead of routing water in the real canal system the wra module assumes that each ihru has a virtual canal and the water losses water evaporation and canal seepage are estimated for each virtual canal similarly the wra module defines a virtual pumping well in each ihru to represent gw exploitation a common management policy to prevent overpumping is to set a tolerable drawdown of the gw level to represent this policy option the wra module introduces a policy parameter the maximum groundwater level drawdown gld as the constraint for pumping if the drawdown with regard to a reference level defined by the manager at a virtual well exceeds the gld pumping is stopped and the final pumpage is set to zero more technical details on the wra module are provided below note that a recent study developed an ag package niswonger 2020 for gsflow to simulate the conjunctive use of sw and gw for irrigation our wra is different from this package in several aspects first the ag package utilizes the modflow sfr package to divert and route sw to fields and detailed information about the canal network and its physical properties e g cross sectional geometry hydraulic conductivity of the canal bed etc is required when modeling a large area with a complex canal network canal information is often unavailable or time consuming to collect and process in contrast the wra module in this study represents water diversion and routing in a simpler way using the concept of virtual canals and is therefore much easier to parameterize in large scale applications second in the current version of the ag package sw is assumed to be exploited first while the wra module allows the prioritization of different water sources 2 2 1 crop water demand the wra module calculates the crop water demand by either the scheduled or automatic irrigation method these two methods have been adopted in classic models such as swat and modflow but have rarely been considered with integrated sw gw models the scheduled irrigation method requires predefined timing and quota levels e g the depth of water irrigated during a certain time period for irrigation the crop water demand d i h r u i j t m3 d 1 for the i th i h r u in the j th irrigation district at time step t can be calculated as 1 d i h r u i j t a i h r u i j i q k t i 1 n h r u j j 1 n d f o r k 1 n c r o p where a i h r u i j m2 is the cropland area of the i th i h r u i q k t m d 1 is the net daily irrigation quota for crop type k at time step t n h r u j is the number of ihrus in the j th irrigation district n d is the total number of irrigation districts and n c r o p is the total number of crop types the crop types and their respective irrigation quotas are predefined by the modeler and can be either fixed throughout the entire simulation period or changed annually as part of the crop water demand can be fulfilled by natural precipitation a further adjustment is made 2 p d i h r u i j t e p a i h r u i j p i h r u i j t 3 a d i h r u i j t 0 i f p d i h r u i j t d i h r u i j t d i h r u i j t p d i h r u i j t i f p d i h r u i j t d i h r u i j t where p i h r u i j t m d 1 is the precipitation rate in terms of the water depth p d i h r u i j t m3 d 1 is the volumetric precipitation rate actually reaching the ground e p is the effective precipitation coefficient which represents the overall effect of canopy interception and a d i h r u i j t m3 d 1 is the adjusted crop water demand in the automatic irrigation method irrigation is triggered when the deficit between the soil water content and field capacity exceeds a certain threshold value 4 s m d i h r u i j t f c i h r u i j s m i h r u i j t 5 d i h r u i j t s f s m d i h r u i j t a i h r u i j i f s m d i h r u i j t s m t i h r u i j 0 i f s m d i h r u i j t s m t i h r u i j where s m i h r u i j t mm is the soil moisture content of the i th i h r u in the j th irrigation district at time step t f c i h r u i j mm is the field capacity s m d i h r u i j t mm and s m t i h r u i j mm are the soil moisture deficit and deficit threshold respectively and s f is a unit conversion factor that converts millimeters to meters whose value is equal to 0 001 similar to the scheduled irrigation method the crop water demand is adjusted using eq 3 when considering the contribution of natural precipitation 2 2 2 allocation of water sources the crop water demand can be satisfied by diversion from a river reach and the gw pumped from an underlying aquifer to determine the appropriate source allocation the wra module adopts a rule based approach that requires the preferences for different water sources to be specified in advance fig 1 shows the allocation procedure in an ihru which starts with the crop water demand the wra module allows irrigation districts to have specific preferences for water sources if sw is preferred the calculation procedure proceeds as described below first calculate the volumetric rate of the sw demand at the diversion point where river water is delivered from the p th reach to the j th irrigation district s w d d i s t r i c t p j t m3 d 1 as 6 s w d d i s t r i c t p j t i 1 n h r u j a d i h r u i j t c e i h r u i j where c e i h r u i j dimensionless is the canal efficiency of the i th ihru in the j th irrigation district the canal efficiency is defined as the ratio of the water amount reaching the fields to the water amount drawn at the diversion point which represents the efficiency of water transport in the virtual canal second apply any ecological constraints to adjust the sw demand according to 7 s w a r e a c h p j t e f r e a c h p j t s w d r e a c h p j t where s w a r e a c h p j t m3 d 1 is the adjusted sw demand and e f r e a c h p j t is an adjusting factor between 0 and 1 to represent the impact of the ecological constraints e f r e a c h p j t needs to be determined externally and separately by the modeler in this study an environment flow constraint module is further developed to calculate e f r e a c h p j t as introduced in section 2 3 if no ecological constraints are applied e f r e a c h p j t is set to 1 third estimate the streamflow potentially available at each diversion point using simple kinematic wave routing to alleviate the computational burden the routing calculation is isolated from the gsflow iteration process and uses the hydrological conditions of the previous day determined by the gsflow model a preliminary estimate of the sw diversion from the p th reach to the j th irrigation district at time step t denoted as d i v r e a c h p j t m3 d 1 can be calculated as 8 q a r e a c h p t m i n q r e a c h p t m d r e a c h p 9 d i v r e a c h p j t m i n s w a d i s t r i c t p j t q a r e a c h p t where q a r e a c h p t m3 d 1 is the volumetric rate of the water available for diversion from the p th reach at time step t q r e a c h p t m3 d 1 is the volumetric water rate flowing into the p th reach which is calculated by isolated stream routing and m d r e a c h p m3 d 1 is the maximum daily diversion capability from the p th reach which depends on the hydraulic facilities fourth estimate the volumetric water rate reaching the fields which is denoted as s w c i h r u i j t m3 d 1 by calculating the water balance in the virtual canal as 10 s w c i h r u i j t d i v r e a c h p j t a r i h r u i j c p i h r u i j t c e i h r u i j t c l i h r u i j t where c p i h r u i j t m3 d 1 is the volumetric precipitation rate in the virtual canal c e i h r u i j t m3 d 1 is the volumetric evaporation rate from the canal c l i h r u i j t m3 d 1 is the volumetric water leakage rate and a r i h r u i j dimensionless is the ratio of the water allocated to the i th ihru to the total water d i v r e a c h p t which is proportional to the ihru area because the flow velocity in the canals is high the storage change in the virtual canals is ignored fifth if the crop water demand is satisfied by sw no gw pumping is needed otherwise estimate the daily pumping rate g w i h r u i j t m3 d 1 as 11 g w i h r u i j t m i n a d i h r u i j t s w c i h r u i j t p m i h r u i j where p m i h r u i j m3 d 1 is the maximum daily pumping capability in the ihru which mainly depends on the pump power and aquifer properties estimating daily pumping rates internally by eq 11 is a unique feature of the wra module sixth enter the preliminarily estimated d i v r e a c h p j t and g w i h r u i j t into the gsflow simulation to obtain the final diversion and pumpage estimates gsflow has an internal iteration loop to adopt the preliminary diversion and pumping estimates as sink terms and generate the respective final estimates the extended gsflow model allows imposing a gw level drawdown constraint if the constraint is activated once the gw head decline compared to the head on the reference day specified by the modeler in an ihru exceeds the maximum daily allowed value the pumpage is set to zero on that day finally update the canal water balance to obtain the final estimate of the diverted water reaching the fields the final diversion and pumpage are distributed to each ihru in the next time step in cases where sw is limited e g far away from the river or when sw diversion is costly and gw is preferred the calculation proceeds as follows see fig 1 first estimate the daily gw pumping rate in each ihru as 12 g w i h r u i j t m i n a d i h r u i j t p m i h r u i j second if the crop water demand can be satisfied by gw alone no sw diversion is needed otherwise calculate the sw demand at a diversion point as 13 s w d d i s t r i c t p j t i 1 n h r u a d i h r u i j t g w i h r u i j t c e i h r u i j third derive the sw diversion amount using eqs 6 9 as described above finally enter the preliminarily estimated sw diversionand gw pumping in the gsflow simulation to obtain the final diversion and pumpage estimates which are distributed to the ihru in the next time step as described above the wra module employs a rule based framework that can flexibly emulate different partitioning schemes of surface water and groundwater in the irrigation water supply varying from surface water only to groundwater only for example if the maximum daily pumping capability in eq 11 is set to zero the irrigation district will use only surface water for irrigation in contrast if the adjusting factor in eq 7 is set to zero the irrigation district will use only groundwater 2 2 3 canal water losses the water losses from the virtual canals are calculated based on eq 10 the surface runoff interflow gw discharge to the canal and storage change in the canal are ignored in the equation which is consistent with previous studies alam and bhutta 2004 kinzli et al 2010 meredith and blais 2019 zhang et al 2017 in arid regions shallow gw is often meters beneath the ground thus it is unusual that the gw table exceeds the canal bottom and gw discharges into the canals additionally according to our field investigations canals have water storage when water diversion and irrigation are operated but are nearly empty beyond the operational period the wra module assigns a virtual canal to each ihru at this small spatial scale it is reasonable to assume that water diversion and irrigation are operated for a part of an irrigation day and therefore the water storage in a virtual canal would not undergo a significant storage change at the daily time scale the precipitation in the canal is calculated as follows 14 c p i h r u i j t p i h r u i j t a h r u i j r c i h r u i j where r c i h r u i j dimensionless is the ratio of the canal surface area to the ihru area the daily canal evaporation c e i h r u i j t m3 d 1 is calculated as follows 15 c i i h r u i j t d i v r e a c h p j t a r i h r u i j c p i h r u i j t 16 c p e i h r u i j t p e t i h r u i j t a h r u i j r c i h r u i j 17 c e i h r u i j t c p e i h r u i j t w h e n c i i h r u i j t c p e i h r u i j t c i i h r u i j t w h e n c i i h r u i j t c p e i h r u i j t where c i i h r u i j t m3 d 1 is the volumetric water rate flowing into the canal p e t i h r u i j t m d 1 is the potential evapotranspiration et flux across the i th ihru and c p e i h r u i j t m3 d 1 is the volumetric rate of the potential et attributed to the virtual canal the maximum volumetric rate of the water available for downward leakage c l i h r u i j t m3 d 1 is calculated as follows 18 c l i h r u i j t c i i h r u i j t c e i h r u i j t 19 c l f i h r u i j t c l i h r u i j t a h r u i j where c l f i h r u i j t m d 1 is the flux of c l i h r u i j t when calculating the mass balance for a virtual canal only downward leakage is considered mohammadi et al 2019 the downward leakage flux is directly applied to the unsaturated zone in the ihru and the net gravity leakage is set to the potential gravity leakage flux c l f i h r u i j t calculated above if the net gravity leakage flux c l f i h r u i j t exceeds the vertical hydraulic conductivity of the unsaturated zone k s i j it is set as k s i j and the amount in excess of k s i j is returned to the canal if c l i h r u i j t equals zero the downward leakage is also set to zero 2 2 4 coupling with gsflow the wra module is integrated into gsflow at the iteration level the computational sequence of the integrated simulation is illustrated in fig 2 the major steps are introduced below i run the gsflow declaration initialization allocation and read procedures upon gsflow initialization the wra module is also initialized the wra objects including the irrigation districts and ihrus are loaded from corresponding input files ii begin the daily time step loop and if the loop is at time step 1 and the initial stress period is at the steady state execute the modflow module in gsflow and exclude the prms i e the surface hydrological module in gsflow and wra computations from the initial steady state stress period otherwise execute the modflow prms and wra procedures iii simulate the land surface and soil zone hydrologic processes in the prms and obtain the preliminary stream diversion and gw pumping estimates when simulating the soil zone processes the final diversion and pumpage estimates after the previous time step are combined with the net precipitation reaching the soil surface at the present step iv begin the iteration loop the preliminary stream diversion and gw pumping estimates by the wra module see fig 1 are added as sink terms to the modflow modules in gsflow the interdependent equations within gsflow are solved using iteration loops such as the gw head and head dependent flow during the solution process the gw level drawdown constraint is assessed once the iteration is stopped the final stream diversion and gw pumpage estimates are obtained v compute the water budget in each zone the whole basin and all irrigation districts write the values of the state and flux variables in the surface and subsurface zones and irrigation districts to output files vi repeat the time loop steps iii v until the end of the simulation period when the simulation ends close all files and clear the computer memory 2 3 the environmental flow constraint module environmental flow regulation is a common water resource management practice that has been implemented in many river basins such as the murray darling basin in australia kirby et al 2014 the ebro river basin in spain almazán gómez et al 2018 the colorado river in the united states kendy et al 2017 and the tarim river basin in china xue et al 2017 environmental flow regulation is needed in these river basins to address some common challenges such as land degradation poff et al 2017 deterioration in water dependent ecosystems yao et al 2018a and intensive human nature competition for limited water resources environmental flow regulation inevitably reshapes the regional irrigation behavior and the approach used to account for the impact in integrated hydrological models at the basin scale remains a challenge as mentioned in the introduction an ewdp has been implemented in the hrb in 2000 which represents a typical environmental flow regulation scheme the hrb is the second largest endorheic basin in china fig 3 the heihe river originates in the qilian mountains located along the margin of the qinghai tibetan plateau and then flows northward into the terminal lake east juyan lake in the gobi desert yingluoxia and zhengyixia are two dividing points defining the upper middle and lower reaches of the main heihe river fig 3 the ewdp in the hrb specifies five minimum required outflows at zhengyxia the dividing point between the middle and lower reaches under different inflow conditions at yingluoxia the dividing point between the upper and middle reaches based on these five control points a flow regulation curve frc with a linear shape can be drawn fig 4 a the frc imposes critical constraints on the water consumption in the mhrb and represents the rule used to allocate the runoff between the mhrb and lhrb however the frc only specifies the total annual environmental flow amount without any explicit advice on how to achieve this goal in real management practice the frc is implemented by the simultaneous shutdown approach which means that all the diversion points along the midstream of the heihe river are mandatorily closed at the same time during particular time windows of a year a shutdown window typically lasts for several days the decision regarding the number length and timing of the yearly shutdown periods is made by the basin water resources agency in an ad hoc manner the decision making approach is largely empirical without formal and rigorous calculations considering a number of factors including historical and recent meteorological hydrological and socioeconomic conditions in fact from 2002 to 2015 the required flow volumes per frc were never fully met and the average gap was 0 17 109 m3 a in this study an environmental flow constraint efc module is developed as a formal approach to represent regulation decisions this efc module provides the ecological constraint factor e f r e a c h p j t in eq 7 to the wra module the efc module includes an algorithm to manipulate the cumulative streamflow at zhengyixia on a daily basis considering its target annual value per the frc fig 4b illustrates the principle of the manipulation algorithm let tan α denote the ratio of the annual outflow at zhengyixia to the inflow at yingluoxia tan α 0 represent the condition strictly following the frc and tan α m a x and tan α min represent the minimum and maximum allowed water diversion conditions respectively at each time step the algorithm reduces the difference between the target outflow r t and actual outflow r t based on two principles first less more water is diverted at the next time step if r t is smaller larger than the target outflow r t at the present time step and second a higher adjustment is needed when there is a larger discrepancy between r t and r t let tan β t 1 denote the ratio of the outflow δ r t 1 p r e d to the inflow δ i t 1 p r e d at time step t 1 i e tan β t 1 δ r t 1 p r e d δ i t 1 p r e d the abovementioned two rules can be modeled as eqs 20 and 21 respectively 20 β t 1 λ t α m a x 1 λ t α 0 i f r t r t λ t α m i n 1 λ t α 0 i f r t r t 21 λ t k r t r t r t i f r t r t k r t r t r t i f r t r t where k 0 is a behavioral parameter measuring the extent to which the water manager seeks to reduce the discrepancy between r t 1 and r t 1 a larger k represents a more aggressive adjustment the allowable water diversion amount which is denoted as δ w t 1 at time step t 1 can be approximated as 22 δ w t 1 δ i t 1 p r e d 1 tan β t 1 then the ecological constraint factor e f r e a c h p j t in the wra module see eq 9 can be calculated as 23 e f r e a c h p j t 1 1 i f δ w t 1 d i v r e a c h p j t δ w t 1 d i v r e a c h p j t i f δ w t 1 d i v r e a c h p j t through the adaptive adjustment of e f r e a c h p t the derived cumulative streamflow at zhengyixia approaches its target value per the frc it is worth emphasizing that eqs 20 23 are not necessarily applied in the real decision making process but they capture the major behavioral features of decision makers and appropriately reflect the effect of the ecological flow constraint i e the frc 3 study area and management scenarios 3 1 study area and data the hrb can be divided into upper middle and lower reaches fig 3 from the upper stream to the lower stream the elevation of the hrb changes from 5500 900 m the landscape changes from alpine vegetation in the upper stream to agricultural ecosystems in the middle stream and to riparian ecosystems and gobi deserts in the lower stream the annual precipitation changes from 407 mm y in the upper stream to 189 mm y in the middle stream and to 52 mm y in the lower stream li et al 2018 the major land cover types in the mhrb and lhrb are croplands forests grasslands wetlands built up areas the gobi desert and vacant lands intensive irrigated agriculture has long been implemented in the mhrb the dominant crop is maize followed by wheat sunflowers melons beans and others the irrigation season starts from late march and ends in late november there are 20 major irrigation districts in the mhrb fig 5 and the total irrigated cropland area exceeded 233 000 ha as of 2007 the water sources used for irrigation in the mhrb consist of sw from the main heihe river and tributaries 70 gw 23 and precipitation 7 liu et al 2010 sw is delivered through a complex canal network the total length exceeds 12 430 km towards the croplands gw is pumped from more than 10 000 wells in our previous study tian et al 2018 an improved version of gsflow was calibrated and validated for the entire mhrb and lhrb from 2000 to 2012 this existing model is equipped with the wra module and the efc module developed in this study with the calibrated prms and modflow parameters unchanged and the modeling period extended to 2015 16 years in total the meteorological datasets produced by xiong and yan 2013 were collected for the period of 2000 2015 and used as the driving forces the daily streamflow observations at zhengyixia and gaoya see figs 3 and 5 from 2000 to 2015 and the monthly gw levels measured at 80 monitoring wells from 2000 to 2012 were used to evaluate the performance of the extended model geospatial data e g land use and canal networks and statistical reports were collected and used to parameterize the wra module the cropland area i e a i h r u i j in each ihru was determined based on the land use data the gis layers of the canal network in 2007 were adopted to estimate the ratio of the canal surface area to the ihru area i e r c i h r u i j in the case shown the scheduled irrigation method is adopted to calculate the crop water demand a typical irrigation schedule is presented in fig s1 in the supplementary materials which was determined based on the two major crops in the study area maize and wheat different irrigation districts share the same temporal pattern of irrigation schedules with different total water quotas the annual average canal efficiency and the daily scale irrigation schedule were derived for each irrigation district based on the annual water resource reports 2000 2015 provided by the local government the total irrigation quota for an individual district equals the sum of the scheduled daily irrigation water all ihrus in the same irrigation district share the same canal efficiency and irrigation schedule in the previous model tian et al 2018 land cover datasets in three different periods were used to configure the time variant land use in the extended model three wra input files were generated corresponding to the land uses in 2000 2006 2007 2010 and 2011 2015 in each of the three simulation segments the wra parameters remain fixed table 1 lists the values of key wra parameters in the 2000 2006 simulation segment and the parameter values in the other two simulation segments are summarized in tables s1 and s2 in the supplementary materials 3 2 water management scenarios to investigate the impacts of different water management strategies on the hydrological cycle in the hrb two types of management scenarios in addition to the baseline scenario i e the actual conditions are hypothesized as summarized in table 2 sw is preferred in all scenarios while in the baseline scenario environmental flow regulation is implemented as the simultaneous shutdown operation in the model this operation is represented by setting e f r e a c h p j t see eq 7 to zero on the shutdown days per the historical records maintained by the heihe river bureau in the baseline scenario both the impacts of the frc and the maximum groundwater level drawdown gld constraint are not explicitly simulated but are implicitly reflected by historical shutdown operations simulation of the baseline scenario requires the wra module with the gw constraint deactivated but not the efc module the type a scenarios are hypothesized to investigate the impact of gw management policies on the hydrological cycle in which the gw constraint is activated and different maximum gld values are specified the heads computed at the end of the first year are used as the reference heads for computing the gld gw pumping has been largely unregulated in the study area only in very recent years has the local government started to meter the gw pumpage in certain irrigation districts it is expected that gw pumping will be strictly regulated by the local government in the future ideally every pumping well can be monitored in real time and remotely controlled once the gld at a well exceeds the predefined maximum value pumping would be mandatorily stopped the type a scenarios reflect this expected regulation design the maximum gld values are set to 1 2 3 4 and 5 m and are tested in scenarios a1 to a5 see table 2 the stopped pumping ratio spr is further defined as 26 s p r d a y s t o p p e d d a y t o t a l where d a y s t o p p e d is the number of days on which gw pumping is stopped because the gld value exceeds the maximum allowed value and d a y t o t a l is the total number of days when gw is needed for irrigation spr is between 0 and 1 the type b scenarios are hypothesized to investigate the effects of the efc in scenario b1 the efc module is implemented but no gld constraint is applied in scenarios b2 and b3 both environmental flow and gld constraints are applied but the maximum gld values are 5 and 1 m respectively 4 results and discussion 4 1 high resolution simulated diversion and pumping the extended gsflow model is run from 2000 to 2015 at a daily time step under the baseline condition with simultaneous shutdown operations and no gld constraints table 2 the first year i e 2000 is the warm up period during which to establish the initial soil zone and unsaturated zone storage levels huntington and niswonger 2012 but is excluded from the following analysis fig 6 a and b compare the mean annual sw diversion and gw pumpage levels simulated by the model respectively against the statistics derived from the historical data of the 20 irrigation districts the simulation results match the data very well at the irrigation district scale as indicated by the high r2 values more importantly the wra module provides a systematic way to derive high resolution diversion and pumping data when only annual or monthly statistics are available which is a common situation in reality fig 6c illustrates the simulated daily time series of the diversion and pumping levels in the baseline scenario the capacity of the wra module to derive high resolution diversion and pumping data is also of great value for evaluating future scenarios fig 7 a further illustrates the simulated source composition of the irrigation water at the irrigation district level the spatial heterogeneity is notable some districts mainly depend on sw while others rely more on gw for example sw accounts for 88 and 89 of the total water supply in districts 1 and 4 respectively while gw accounts for 83 of the total water supply in district 15 irrigation districts 1 and 4 are located in the upper reaches of the heihe river and their diversion demands are prioritized in contrast irrigation district 15 is located in the lower reaches of the heihe river and far away from the river and the available streamflow amount for diversion is limited thus more gw is pumped to meet the irrigation demand overall sw gw and precipitation account for 67 25 and 8 of the total water supply in the 20 irrigation districts respectively table s3 in the supplementary materials provides more details on the water demand supply and losses in the 20 irrigation districts these estimated ratios are consistent with a previous study zhang et al 2018a b c fig 7b shows the spatial pattern of the water losses in the canals at the irrigation district scale in all irrigation districts except districts 6 8 and 15 canal drainages account for the majority of the canal water loss in general canal drainage accounts for 75 of the canal water loss the magnitude and spatial pattern of the simulated water supply components are consistent with previous studies li et al 2018 liu et al 2016 in fact the wra module can also provide simulation results similar to fig 7 at the hru level 9858 ihrus in total within the modeling domain fig s5 in the supplementary materials provides an example which is an advantage of this module because such detailed data are usually unavailable in reality 4 2 shift in the hydrological regime under groundwater management as mentioned previously the regulation of gw pumping has been largely overlooked in the hrb but has recently gained increasing management attention to examine the potential impact of pumping regulations on the hydrological cycle and irrigation behavior the type a scenarios were simulated with the extended model under the simultaneous shutdown operation table 2 scenario a1 represents the strictest regulation while scenario a5 represents the least strict regulation fig 8 shows the key water budget components and changes in gw storage in the mhrb based on the model simulations demonstrating the considerable impact of the constraint as expected gw pumpage is reduced in all four scenarios fig 10a gld based gw management is beneficial for gw storage recovery fig 8b in comparison with the baseline scenario gw storage in scenario a1 is greatly recovered however the reduction in gw pumpage results in a reduction in et fig 8c because less water is supplied for irrigation and the streamflow at the zhengyixia gauging station is increased fig 8d as indicated by scenario a1 limiting the groundwater level drawdown compared to the historical condition within 1 m would lead to a 3 9 decrease in cropland et in the middle basin and a 1 5 increase in the environmental flow towards the lower basin overall fig 8 shows that the hydrological system in the mhrb responds to gw regulation and tuning the regulation would effectively alter the complex nexus between the agricultural production in the mhrb as represented by the et the gw resources in the mhrb and the sw accessible to the ecosystems in the lhrb an important implication is that the gld constraint needs to be carefully determined on a scientific basis to address the water conflict between agriculture and ecosystems fig 8a also compares the averaged spr in the five type a scenarios it is evident that the spr increases nonlinearly with an accelerating tendency with the strengthening of the gw pumping restriction the loss of agriculture would increase sharply as the gw regulation is tightened however gw would be better recovered with tighter regulations which is beneficial for the wetlands and other ecosystems in the mhrb therefore it is necessary to balance the ecological services and agricultural production in the mhrb when designing gw management policies fig 9 depicts the substantial spatial heterogeneity in the spr in scenarios a1 and a5 as shown in fig 9a under the most rigid regulation i e scenario a1 most of the ihrus in irrigation districts 6 9 13 18 and 20 have a zero spr indicating no impact from the gld constraint according to previous studies hu et al 2016 intensive river gw interactions occur in the stream segments flowing through these districts which lead to quick replenishment of the shallow aquifer beneath these districts thus diminishing the impact of the gld constraint in contrast a large portion of the ihrus in districts 8 11 12 and 15 have sprs over 70 these districts are away from the main river and have much weaker sw gw interactions tian et al 2018 thus these districts are more sensitive to gw regulations fig 9b shows the spatial distribution of the spr in scenario a5 in which no districts are notably impacted by the regulation except district 15 fig 9 reveals two important management implications first gw management policies need to be spatially heterogeneous to cope with the heterogeneity in hydrogeological conditions second there exists a gw management policy threshold below which the policy will have a trivial effect and therefore a rigorous assessment such as the one performed here is critical for identifying this threshold fig 10 further illustrates the temporal pattern of the spr in scenario a1 the color of an ihru indicates a month during which the ihru attains a monthly spr peak value fig 10 shows that the monthly spr peaks during the flood season june july and august in most ihrus except those with spr 0 gray color in the figure from june to august the irrigation demand in the study area peaks and gw pumping is intensified to bridge the gap between the demand and available streamflow however gw recharge is not able to fully compensate for the exploitation during the flood season because the velocity of the infiltrated water traversing the vadose zone is much lower than the pumping rate as a result the gw level continues to decrease until the gld reaches its limit and pumping is stopped it is also noted that the monthly spr is the highest in october in several irrigation districts this timing is related to the custom irrigation practices in the hrb in late october or early november while no crops are planted croplands are still irrigated to maintain the soil moisture level which is beneficial for the production next year fig 10 implies that a gw management policy with temporal variability may be desirable during critical periods in the study area such as june to august and october gw regulation could be loosened to avoid agricultural production damage due to water deficits and in other periods a tighter constraint could be applied 4 3 water conflicts influenced by management policies as mentioned previously the present diversion management under the simultaneous shutdown operation is subject to an ad hoc decision process conditioned upon historical and recent meteorological hydrological and socioeconomic conditions in fact the frc has never been fully met in the past and the gap was even larger in wet years fig 11 the reasoning behind this result is threefold first a systematic approach to planning diversions under environmental flow regulation is lacking second the regulation is not equipped with effective incentive and or penalty mechanisms and therefore the midstream irrigation districts commonly tend to overuse the streamflow scenario b1 emulates the situation in which the midstream districts are wholly motivated to achieve the designed environmental flow goals and the approach i e the efc module proposed in section 2 3 is applied fig 11 shows that the efc module works well in reconfiguring the midstream diversion towards attaining the regulation goals table 4 compares scenario b1 against the baseline scenario reflecting the actual conditions with regard to key hydrological variables to meet the regulation goals the mhrb would have to reduce its diversion 21 85 and increase its pumpage for compensation 7 04 but the reduction in the total irrigation water supply would still be remarkable 15 94 the change in the water supply is beneficial to the ecosystems in the lhrb as the environmental flows both through zhengyixia and into the end lake are notably increased 22 47 and 51 92 respectively and gw storage in the lhrb is increased 15 28 however enhanced ecological security is attained at the cost of losing agricultural production in the mhrb as indicated by the cropland et 8 57 and the accelerated depletion of gw storage in the mhrb 43 37 scenarios b2 and b3 are simulated to further evaluate the consequences of adding gw management to the environmental flow regulation and their comparison against the baseline scenario and scenario b1 is also summarized in table 4 scenario b2 represents light gw control and scenario b3 represents tight gw control table 4 indicates that implementing the gw level drawdown constraint would inhibit gw overpumping but would further exacerbate the reduction in the total irrigation water supply see the shaded numbers in the table gw storage and cropland et in the mhrb are sensitive to such changes with regard to both the absolute and relative differences while the other variables are not in the extreme case with a tight pumping limit scenario b3 the cropland et in the mhrb would be reduced by 13 04 while compared to the baseline scenario the decrease in gw storage in the mhrb would be slowed by 56 63 this comparison indicates that the impact of gw regulation is largely confined within the mhrb and has a much smaller transboundary effect than that of environmental flow regulation overall the above analysis demonstrates the complex nexus among water agriculture and ecosystems in the mhrb and lhrb as well as the water conflict between the two subbasins and how it would evolve under conjunctive sw and gw management there are several important associated water resource management implications for arid areas similar to the mhrb and lhrb first management policies need to maintain an appropriate balance between water security e g gw storage socioeconomic development e g agriculture and ecological services e g vegetation and wetlands to this end an appropriate approach to weighting competing goals is critical but remains a challenge in the hrb whether the frc is appropriate and how much inflow into the terminal lake is needed have long been disputed second in water limited areas it may be very difficult if not impossible to satisfy all competing systems or subareas simply through water allocation and specific fundamental shifts are necessary such as introducing water saving technologies for irrigation sun et al 2018 and converting agriculture into a more water efficient industry with the belt and road initiative bri of china inland china as well as vast central asia is anticipated to experience more infrastructure construction and faster urbanization which may provide an opportunity for these areas to alleviate their current water conflicts 5 conclusions this study developed modules for emulating high resolution daily resolution in time and down to the model grid size in space irrigation activities and representing water management policies in an integrated hydrologic model the improved gsflow model with the new modules addresses the challenge of hydrological simulations and predictions in areas with intensive agricultural irrigation but limited data on irrigation practices and enables the systematic assessment of the complex nexus of water agriculture and ecosystems the improved gsflow model also offers powerful support for iwrm in arid and semiarid areas as a case study the model is applied in the hrb which is the second largest endorheic river basin in china with typical water conflicts between agricultural development and ecological conservation the major study findings include the following first the model attains a good performance in reconstructing high resolution streamflow diversion and gw pumping data from coarse information as well as in emulating the procedure of environmental flow regulation second the hydrological regime in the mhrb is sensitive to gw management in terms of the gld limit imposing a tight limit would be beneficial for the ecological services in the mhrb due to gw storage recovery but would cause a significant loss in agricultural production due to the irrigation water supply deficit which is a critical tradeoff to be balanced via management third sw management i e environmental flow regulation and gw management i e gld constraints have both synergistic e g in reducing the irrigation water supply and offset e g in maintaining gw storage effects reflecting the complex nexus of water agriculture and ecosystems in the mhrb and lhrb this study has important water resource management implications for arid and semiarid areas first gw management policies with spatial heterogeneity and temporal variability are highly desirable particularly in areas where the hydrogeological conditions vary notably in space second it is critical to identify the threshold effects of policies in the hrb for instance there exists a gw management policy threshold below which the policy will have a trivial effect third management policies need to maintain an appropriate balance between water security e g gw storage socioeconomic development e g agriculture and ecological services e g vegetation and wetlands finally in water limited areas it may be very difficult if not impossible to satisfy all competing systems or subareas simply through water allocation and certain fundamental shifts may be necessary such as introducing water saving technologies for irrigation and converting agriculture into a more water efficient industry the current model can be further improved to better represent complex human nature interactions and support the decision making process for example the water allocation method in the model is currently rule based and optimization methods can be employed in future studies modeling capability for the operation of reservoirs weirs and other hydraulic structures is also desirable for a better understanding of the hydrological cycle in endorheic river basins influenced by considerable human intervention and climate change declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the strategic priority research program of the chinese academy of sciences xda20100104 and the national natural science foundation of china no 42071244 no 51961125203 no 41807164 and no 41861124003 the improved gsflow model with the new modules can be freely downloaded from https github com deephydro wra appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104874 
25952,in this manuscript we describe a computational model to delineate watershed boundaries for simple geometries points lines or polygons where maximum water flow distance can be constrained by a user defined variable this method generalizes earlier research to delineate watershed boundaries using a marching algorithm our proposed method allows users to delineate watersheds for a number of use cases including evaluating best management practices bmp s measuring water impacts to municipal city and parcel boundaries and others we describe computational complexity and space costs and compare them experimentally to standard techniques keywords watershed boundary marching algorithm modified nested set algorithm constrained watershed boundary 1 introduction delineating watershed boundaries is a well studied field o callaghan and mark 1984 tarboton 1997a matthies et al 2007 haag et al 2018 providing information for the management of hydrologic systems watershed delineation and mapping is part of a larger field of study used to connect the state of upland uphill regions through hydrologic flow and transport e g pollutants we acknowledge that the data models and algorithms described in this paper do not solve all of the issues required for modelling hydrologic systems rather they provide an improvement in the efficiency for one important step in the larger modelling process we begin by a broad overview of the field of hydrologic modelling the sub field of watershed modelling and lastly existing method to delineate watershed boundaries this is used to place the contributions of this paper withing the larger field of study in which it sits 1 1 background hydrologic modelling is the field of study that is used to understand the impact that the water cycle has on the past current and future system state both natural and anthropogenic in this review we relied heavily on vijay singh s singh 2018 survey manuscript to review the history of hydrologic modelling generally hydrologic modelling started at first principles with discoveries on general rules of water diffusion mulvany 1850 ground water diffusion d 1856 and evaporation john 1798 these constructs where identified and codified through empirical models and large scale in situ experimental techniques and were codified in 1964 by chow 1964 the 1960 s saw the beginning stages in the transfer of physical watershed models to digital representations of hydrologic system within computational systems as the utilization of sensing modalities increased in step with computational power it became easier to model larger more complete systems for example by 1966 the stanford watershed model was able to model the entire hydrologic cycle within one modelling regime singh 2018 the work presented in our manuscript focuses on watershed modelling or the study of the runoff collection and impact of precipitation over terrestrial land features since 1960 s this field has been heavily integrated with advances in computer and sensor technology singh 2018 modern remote sensing technologies provide a wealth of information on the current and historical state of terrestrial systems these include high resolution maps of surface elevation and bathymetry through digital ranging technologies like lidar and synthetic aperture radar sar air photo interpretation and stereoscopy and others jensen 2014 information on what land is used for and what is covering it e g forest urban wetland etc soil and bedrock types snow and ice cover thickness and even changes in gravitational strength that can be correlated to local mass loss or gain yeh et al 2006 as the spatial and temporal resolution of remotely sensed data increased so did the need to convert these datasets into near real time forecasting systems for example the iowa flood information system ifis is designed to merge multiple real time stream gauges into a holistic view of flooding in the state of iowa modern computer mapping systems have allowed detailed models of sources and sinks of contaminates to inform the management of aquatic systems and other associated resources parajuli et al 2013 there are several types of watershed models including models to estimate nutrient and sediment flux or models that are used to predict the physical state of a stream such as temperature or water flow and volume examples include the spatially referenced regression on watershed attributes sparrow and mapsheds mapshed 2014 the main objective of these modelling efforts is to quantitatively connect what is happening between upstream and downstream regions either in stream or adjacent e g flood modelling an important component of watershed modelling and by extension hydrologic modelling is the identification and delineation of a watershed boundary as input geography tesfa et al 2011 a number of existing software systems have pre built routines that return watershed boundaries mostly for raster based representations of digital elevation models dems e g geographic resources analysis support system grass grass development team 2017 terrain analysis using digital elevation models taudem tarboton 2015 and whitebox lindsay 2020 among others to support these efforts a number of methods have been developed to model how water flows from higher elevation to lower over adjacent dem grid cells this paper focuses on the d8 flow model where water flows to the one neighboring grid cell that has the largest slope is both lower then the current cell and has the steepest gradation however there are other arguably more accurate ways to model this flow such as the d method tarboton 1997b or the multiple flow direction mfd wilson 2012 method which partitions flow into all adjacent cells which are lower then the current cell we hope in the future to extend our work from d8 to other flow models but it is unclear if they will provide the same advantages described here originally watersheds modelling systems were developed for main frame and desktop computational systems more recently a general trend has to been to move these geospatial models into cloud based computational environments with application programming interfaces api serving as connectors between multiple numerical models khan et al 2017 an example of this paradigm is the model my watershed web application developed by the stroud water research center model my watershed relies on a number of interconnected modelling api s disclaimer two authors of this manuscript are members of the team that helped developed model my watershed to calculate nutrient loading and attenuation watershed boundaries site stormwater predictions and rapid calculations of land use land cover estimates for watershed boundaries this paper describes an algorithm and associated data model to delineate watershed boundaries over larger geographies in computational times that support the development of web based modelling tools the transformation from desktop computing systems to cloud based decision support tools has allowed non technical users to run models that are designed to mimic natural processes salewicz and nakayama 2004 for example the model my watershed web application allows users to delineate and describe watershed boundaries based on digital elevation and grid based models of land use and land cover these systems need to provide information in retrieval times consistent with modern web browsing preferences on the order of seconds to minutes some work has been done to create faster methods to return watershed boundaries given an input pour point the first example developed by perez 2000 calculates upstream watersheds at confluence points and only partially calculates the local catchment boundary the local catchment boundary is then merged with the upstream watershed creating a singular watershed boundary for a given input point it is the authors understanding that a similar method is employed by both wiki watershed watershed delineation module and the esri watershed delineation api neither of these tools to our knowledge has published methods on these approaches neither of these approaches would be amenable to calculating the union of a watershed given a line or polygon without some modifications lastly sit and sermet yusuf 2020 has developed an algorithm that quickly merges multiple adjacent catchment boundaries together on the client side we could not compare this method to our approach directly because it only returns watersheds at some minimum size unlike the methods described by perez 2000 1 2 contribution this manuscript describes several innovations that differentiate it from prior methods our proposed method directly computes the merged watershed boundary for a set of contiguous pour points the majority of existing software systems grass grass development team 2017 taudem tarboton 2015 and whitebox lindsay 2020 return watershed boundaries for a singular pour point our proposed model extends our original watershed marching algorithm wma modified nested set mns algorithm and data structures published in haag et al 2018 to calculate a set of intervals based on a set of pour points our method only visits the exterior of the input geometric object corresponding to a query region therefore for any polygonal query the proposed algorithm only has to walk around the border to identify the watershed boundary our approach relies heavily on a novel dynamic data structure to merge the set of intervals called the modified interval search tree mist the proposed data structure relies on the structure of balanced binary search trees and the generalized notion of interval trees for continuous sets erwig 1998 this algorithm returns the watershed boundary for a linear or closed polygonal geometric object characterized by an ordered sequence of points x 1 y 1 x 2 y 2 x ℓ y ℓ r 2 the mist data structure allows the marching algorithm to efficiently examine multiple possible basin locations during the marching steps extending the original wma algorithm to support linear and polygonal geometric objects lastly we describe a modification to the mist structure the constrained mist by using a constrained interval insertion strategy we allow the creation of a constrained watershed boundary where water flow distance is less then some user supplied distance maximum flow distance see fig 1 for an illustrative examples this new algorithm and the associated data structures allow the creation of web based interactive map visualization and analytical application resources to delineate watershed boundaries for polygonal linear or point regions of interest this is an important advance as current techniques designed to support the delineation of watershed boundaries for a singular location rely on partially solving watershed boundaries as described by perez 2000 and therefore would not be amiable to the line or polygonal form without the additional computational costs associated with merging polygons our proposed method returns watershed boundaries using the marching method i e walking around the watershed boundary never entering into or leaving the watershed there are a number of practical uses for this algorithm for example we have partnered with the chesapeake bay program cbp and the chesapeake conservancy a regional partnership of federal state local and not for profit organizations focused on protecting and restoring the ecological habitat of chesapeake bay and it s tributaries the cbp has identified agricultural and urban nutrient loading as drivers of bay ecological health and has invested heavily in the development of best management practices bmps to reduce the impact of these land use activities on the bay the cbp has developed a bay wide modelling framework called the chesapeake assessment scenario tool cast chesapeake bay program 2017 that has been used to apportion nutrient loading to larger geographic regions stream and river catchments the cast modelling framework does not translate to site level modelling because it makes a number of assumptions that do not translate to local specific sites for example cast assumes that a one acre riparian restoration in a coastal plain upland will intercept a total of 5 acres 1 in the buffer and 4 additional upland acres bmp modelling chesapeake bay program 2018 pg 62 these numbers are based on the average watershed area for a riparian region across larger geographic regions the algorithm described in this manuscript will allow the cbp to accurately map these watershed boundaries for a specific bmp allowing the differentiating and prioritization of bmps at the site or parcel level and hopefully leading to a reduction of nutrient and sediment loading given the same resources an important consequence of the availability of this algorithm is that it can be integrated into other decision support systems as an example by integrating our algorithms the end users are able to model the efficacy of various best management practices using a web based tool built by chesapeake commons field docs fielddoc org 2016 this tool will allow users to input a potential bmp and to get instantaneous in seconds estimates of pollutant load reductions based on the drainage area elevation and local land cover in a typical use case a user can upload a polygon representing a forested buffer strip the watershed boundary is rapidly generated using the algorithm described in this manuscript next we use a fast zonal algorithm haag et al 2020 to characterize the land cover type associated with that buffer s watershed using this information estimated reduction in nutrient and sediment load to the stream can be calculated an illustration of such a scenario for this tool is shown in fig 2 the constrained version of our algorithm has implications for allowing the determination of affected stream length by waste water treatment plants and other point sources of pollution for example an observed fish kill could be tracked to all point sources of pollution within 10 miles upstream of the observed event this is done by submitting the observed event point entering in a distance relevant to a fish kill event e g 10 miles and receiving the polygonal watershed boundary for the upland area responsible for the event i e the watershed boundary for all adjacent and overland flow between the point and 10 miles upstream 1 3 existing techniques most existing computational methods for delineating watershed boundaries utilize a single pour point or outlet point to describe the location at the bottom most flow of the watershed whitebox lindsay 2020 taudem tarboton 2015 and grass grass development team 2017 these existing software packages then initiate a flow path tracing algorithm across the flow directional grid fdg identifying grid cells where water will flow through the pour point we are only aware of one software implementation that can return watershed boundaries for polygonal or linear objects instead of for singular input locations the batch watershed delineation for polygons function within esri s arc hydro tools we note that it would be possible to return similar results for other watershed delineation methods by intersecting a line or a polygon with the dem raster and then creating and merging watershed for every grid cell intersected by the line or polygon in order to better understand how esri s arc hydro tool works we ran a number of test geometric objects through the desktop application while we could not directly examine the source code to model how the algorithms works we where able to deduce the major steps in the process based on how the tool created temporary files during its processing steps based on these results we believe the execution process of existing esri tool set follows first the input geometric object is rasterized by snapping same extent and cell size to the input fdg next a second temporary raster dataset again snapped to the input fdg is created to store the identity of the grid cells that are in the watershed of the geometric query object at this point the esri s tool set has three raster datasets the first is the input fdg the second is the rasterized version of the geometric object and the third is the output raster file that stores the identity of grid cells that are inside of the watershed it then appears that the esri s arc hydro tools walks across all of the positive cells in the original raster representing the geometric object and searches across the fdg using a path tracing algorithm if a cell is found to flow into the geometric object it s value in the last raster is set to positive an additional step is required of this algorithm compared to the single flow point versions namely that it is possible to discover raster cells in the fdg multiple times and therefore it is necessary to check to determine if a cell is already positive in value when all of the cells in the rasterized version of the geometric object have been visited the second raster layer is converted into a vector file and the intermediate raster layers are deleted when we discuss complexity in this manuscript we are referring to the number of operations required to calculate a result within a computational system we use the unlimited ram model as discussed by cormen et al 2009 in their book introduction to algorithms this model does not take into account different speeds of ram cache ssd cpu vs gpu speed etc but rather it focuses on the complexity as the total set of questions that an algorithm requires to be answered before a result is returned we could find no formal discussion on the complexity of esri s batch watershed algorithm but based on our understanding of the algorithm the complexity can be described by the cost of these five steps 1 the cost to rasterize the geometric object 2 the cost to search across the grid representing the geometric object 3 the cost to search across the fdg 4 the cost to store and search to make sure a cell has not already been visited the output raster file and 5 the cost to vectorize the final output grid cell the complexity to search across the vectorized grid is at its worst case in relation to the size of the grid as the data is stored in esri s tool as a linked list e g a vector the complexity of looping through this list is simply the number of grid cells contained by the original geometric object as identified in step 1 the complexity to search across the fdg is related to the number of grid cells that are identified by the flow tracing algorithm because flow paths are unique for a d8 fdg it is possible though unlikely that a grid cell that is inside the watershed could be found at most 2 times this is caused directly by the geometry of a d8 fdg which limits the number of possible paths to a location either a path tracing algorithm is started at a grid cell or it passes through it therefore the computational complexity of searching the fdg is directly related to the number of grid cells that are identified by the flow tracing algorithm adding up the total computational complexity from all of these steps and we find that the existing algorithm runs in complexity of the area of the original geometric object projected over the input grid plus the area of the output watershed boundary functionally this means that with increasing resolution of dems this method will scale exponentially in time with the increased grid resolution as proved later in this paper our proposed method scales in constant time relative to the number of cells in the length of the watershed perimeter thus out performing the known esri method as both watershed area increase and or dem models get smaller as an example consider that the chesapeake bay region has gone within recent history from a 30m dem based on the national elevation dataset ned to a 10m dem from the national hydrography dataset high resolution furthermore the cbp is currently working on a 1m bay wide dem model 2 notation our proposed watershed marching algorithm wma p for a polygonal query region takes as its inputs a geometric object p and a regular d 8 dem grid g and produces the boundary ω w of the watershed w p g that flows through p with respect to g the polygonal object p is represented in terms of a list of ordered vertices v 1 v 2 v ℓ we will assume that for each i 1 ℓ 1 the ordered pair v i v i 1 corresponds to a boundary edge of p and if the interior of p is non empty then v 1 v ℓ we also assume each boundary point v p is uniquely identified by its x v y v coordinates in r 2 the regular grid g consists of a set of n m cells each of which is a subset of r 2 with a uniform cell size c s g r in both the x and y coordinates we will use the notation g i j to denote the grid cell located at column i 1 n and row j 1 m for each cell g i j we will use d 8 i j to represent its flow direction value and interval pair d i j f i j with d i j f i j representing its discovery and finish values we use an existing algorithm bresenham s algorithm bresenham 1965 to intersect or rasterize the lines segments forming the boundary of p with the grid cells in g creating a set of grid cells φ p g g 1 g 2 g q given the grid cells φ p g we obtain a unique set of intervals i p g d u f u u φ p g without loss of generality we use the notations i p g and φ p g interchangeably as one uniquely identifies the other a set of intervals i d 1 f 1 d 2 f 2 d r f r is referred to as disjoint interval set if for all i j 1 r we have d i f i d j f j 0 given an arbitrary set of intervals j d 1 f 1 d 2 f 2 d k f k we define τ j as its equivalent disjoint interval set j d 1 f 1 d 2 f 2 d l f l i e d f j d f d f j d f we implement a variation of the τ operator using a modified interval search tree mist t τ j whose vertices correspond to a disjoint set of a possibly overlapping set of intervals j the mist data structure follows standard convention for a rooted binary tree with a modification that each node in it contains an unique interval range d f as its key value which is non overlapping with keys of any other node in t the interval associated with the left child of a node will always have boundaries less than the interval of the parent while the right child will always have greater key intervals than the parent to implement the mist data structure we use an existing data structure the left leaning red and black tree sedgewick 2008 to guarantee a balanced binary search tree bst structure and associated fast deletion insertion and deletion to the data structure we extend the mist data structure into the constrained mist by modifying its internal structure to represent a constrained distance such that each node contains a third key as well as its unique interval range furthermore each node is represented by a triplet d f r where d f remains as its key value that is non overlapping with keys of any other node in t but has an extra constraint r the radius that must be maintained to restrict the domain of the watershed s water flow we will use d f r to define the attributes of a new node being inserted into the constrained mist that adhere to the same properties we will use w p to represent the watershed of cells associated with p with respect to grid g to create the boundary ω w p of w p we create a secondary abstraction of the regular grid g to denote the lattice corners of each grid cell v v specifically we use l λ γ to denote a regular square lattice with λ r 2 denoting the set of lattice points and γ r 2 r 2 as the set of edges connecting lattice points the algorithm returns the watershed boundary in the form of a closed lattice walk ω w p made up of a list of lattice points in l as we will show the flow edges in the d 8 grid flow to the set of grid cells in φ p g implying that w p w φ p g 3 mist data structure the original marching algorithm as described in haag et al 2018 used a simple probe mechanism to classify grid cells with respect to watershed inclusion or exclusion specifically this simple probe determines if a grid cell is inside the watershed if and only if its discovery time d value is contained between the range described by discovery d q and finish f q values of the staring query vertex q it follows then that the combined watershed for a number of input locations can be defined by the union of the set of ranges described by the input locations in extending this probe to query polygons we will provide a necessary and sufficient condition based on the minimum set of non overlapping ranges induced by cells associated with query region the mist data structure will efficiently support this extended probe condition we define a non degenerate interval i d f d f as a non empty subset of r the main structural invariance of the mist data structure is to maintain a disjoint interval family i i 1 i 2 i n of n non degenerate intervals i e with i i i j for all 1 i j n we will assume the set i is created by incremental insertions of a set of intervals in the form of d f pairs in practice these intervals represent the mns labels associated with cells resulting from the linear intersect of query object p across grid g it is our objective to design a data structure to represent the family i using a mist t a main requirement of the data structure is that it needs to support dynamic operations including insertions and deletions in our implementation each interval will be stored as the key values of the nodes in t we will assume that these operations can be invoked in any arbitrary order in the case of insertion the new interval may have non empty intersection with existing intervals in set i which may result in further processing to maintain the integrity of disjoint family i assuming a temporal order of insertion on the elements of i the initial interval i 1 will be added as the root of t inserting any other node to the mist can be resolved in two steps first we search within the existing mist to determine if the interval is unique or if it overlaps or is contained by an existing interval starting from the root we compare the interval i k d k f k to each node s key in the tree following the bst property if the new interval i k does not intersect with the interval i i d i f i of an existing node of the mist we compare d k with d i if d k is less than d i we proceed to the left subtree otherwise we will follow the right subtree following this rule several cases may occur along the path from root to a leaf case i the algorithm reaches no node along the root to leaf path which its interval overlaps with the new interval i k d k f k i e all node in the path fall in no intersecting category when compared to i k fig 3 shows an example of this case case ii the algorithm reaches a node whose key i i d i f i contains the new interval i k d k f k i e the relationship between i i and i k falls into contained category case iii the algorithm reaches a node that its key i i d i f i intersect with the new interval i k d k f k i e the relationship between i i and i k falls into overlapping category in case i the algorithm reaches a node with no right or left subtree in this case a new node with key i k will be inserted as a leaf our implementation of mist data structure uses a modified variation of so called left leaning red black tree llrb strategy we will assign a color red to the edge connecting i k to its new parent sedgewick 2008 after inserting a node the mist tree will be re balanced the basic operations that help to maintain balance are left rotate right rotate and flip colors a llrb requires that the red edge always lean to the left therefore in the case that the new inserted node is a right child of its parent left rotate operation is used if the new inserted node or the left rotate operation from the previous step of re balancing violates the invariant property of the llrb tree i e causing two consecutive red edges both leaning on left right rotate operation is used in addition the new node might be inserted as a right child of a parent node which its left child has red edge this situation can be encountered after right rotate operation as well in this case the color of the edges that connect parent to children nodes will be flipped to black and the color of the edge attached to the parent node will be flipped to red in other words flipping colors passes red edge up one level if flip colors operation causes two consecutive red edges rotate right operations is used to maintain invariant properties of the llrb flip colors may cause a right leaning red edge this situation will be resolved with rotate left therefore starting from the new inserted node with red edge we repeatedly use one of the operations to maintain left leaning red edges and its invariant properties these operations are used to pass a red link up in a llrb tree in order to re balance the tree more detail of the code and algorithm of these operations can be found in sedgewick 2008 in case ii according to our initial goal for designing a data structure with non overlapping intervals no insertion or deletion is needed fig 4 provides an illustrative example of this case in case iii the node with key i i must be deleted from the mist tree to delete an internal node the node s key will be replaced with the key to its successor i e the minimum node in its right subtree then the successor will be deleted to delete a leaf node if the node is attached with a red edge it will be removed easily otherwise a red edge should be pushed down the tree the left rotations right rotations and flip colors will be used on the way down the search path to push red edge right or left so that the edge attached to the leaf node getting deleted flips to red then the node will be deleted starting from the deleted node the rotations and color flip operations will be used on the way up to fix right leaning red edges and consecutive red edges which are not allowed more detail of the code and node deletion algorithm can be found in sedgewick 2008 the interval i i will be merged with i k using algorithm 2 and results in interval i z d z f z the new interval will be inserted in to the mist tree starting from the root as we explained earlier fig 5 shows an example of this case algorithm 1 build mist a procedure for building a modified interval search tree mist image 1 algorithm 2 update a helper algorithm to insert an interval i k into a mist image 2 algorithm 3 stabs mist a procedure to check if a value stabs any interval in a mist with a constrained distance image 3 4 constrained version of the mist data structure we extend the mist data structure to allow the delineation of constrained flow distance boundaries from a regular d8 fdg the constrained flow distance is restricted by the maximum distance constraint which in this case constrained denotes a maximum flow distance fig 1 which is controlled by a user defined parameter r which denotes the constrained distance the constrained flow distance may also be restricted by the minimum distance constraint which in this case constrained denotes a minimum flow distance across the d8 fdg which is controlled by a user defined parameter r which denotes the constrained distance without a loss of generality for both constraints of the mist data structure when r is set to the constrained version of the mist is equal to the base form this extended version of the mist maintains the distance of the watershed to any grid and the flow distance by storing a unique flow distance parameter for every set within the mist structure this new data structure requires an additional o 1 probe when two intervals intersect without loss of generality take the case when the mist maintains the shortest flow path for every interval this check compares the original interval that overlap and the new interval to be inserted in the maximum distance strategy two cases exist a if the new interval is further away and overlaps a portion of the original interval the overlap is spliced from the new interval and the non existent interval remains or b if the new interval does not overlap pre existing intervals the whole interval remains alternatively for the minimum distance strategy two cases exist a if the new interval is closer and overlaps a portion of the original interval the overlap is spliced from the new interval and the non existent interval remains or b if the new interval does not overlap pre existing intervals the whole interval remains the constrained insertion strategy is depicted in algorithm 2 update in the case of insertion for the constrained mist the same algorithm is followed unless the new interval overlaps or is contained by an existing interval starting from the root we compare the interval i k d k f k to each node s key in the tree following the bst property algorithm 4 latticemove ℓ μ t image 4 algorithm 5 watershed marching algorithm polygon input g t p image 5 if the new interval i k does not intersect with the interval i i d i f i of the current existing node of the mist we compare d k with d i if d k is less than d i we go to the left subtree otherwise we will follow the right subtree following this rule several cases of overlap may occur along a root leaf path as depicted in table 1 case i the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case ii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case iii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case iv the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case v the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case vi the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case vii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case viii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i in what follows we will show that if a dem cell belongs to the watershed i e flows to any point interior or on the boundary of p the query region polygon a probe of the constrained mist data structure will return a positive result and the minimum flow path distance travelled between the query point and the boundary of p the inverse is also true that any point that does not flow into p will return a negative result and therefore a null distance to the boundary of p proposition 1 if a dem cell flows to a query region then one and only one of the nodes in the constrained mist will have an interval that is a witness for the dem cell proof we provide a case analysis by considering various possible flow paths of the dem cell case 1 the flow path from the dem cell to the sink does not intersect the query region by the description of the modified nested set algorithm the d value for the dem cell will not be contained in any of the intervals in the query region because the constrained mist is constructed from the aggregation of the intervals on the boundary of the query region it follows that the set of disjoint intervals maintained by the constrained mist will not contain d for the dem cell case 2 the flow path from the dem cell to the sink intersects the query region we note that there are two distinct sub cases which we discuss in turn in the first case the query point flows through an interval that is represented in the constrained mist uniquely that is it is disjoint from all the other intervals that make up the boundary of the query region this sub case leads to the observation that the path from the dem cell to the sink hits exactly one location on the boundary of p else the interval defined by the boundary point intersected by this path would not be disjoint in this case the discovery time of dem cell will intersect the interval of a node in the constrained mist and return a positive result and the minimum distance between the dem cell and the query boundary will be the flow distance between the boundary point and the cell the second sub case occurs when the flow path from the dem cell to a sink intersects the boundary of p at multiple locations see fig 6 by definition of the modified nested set labelling algorithm the path from the dem cell to the sink can be represented by a negative monotonic distance function this leads to the observation that the first boundary point intersected along the path from the cell to the sink through p will be the closets to the query point this directly leads to the fact that for the interval defined by the dem cell there can be no other interval on the boundary of p with a larger flow distance else it would have been found first since the constrained mist only subsumes parts of intervals when the flow distance is larger the interval containing the d and f for the dem cell will always be associated with the distance from the first boundary p therefore when the constrained mist is queried it will return a positive result and the distance between the query point and p can be ascertained proposition 2 if there is a dem cell in the watershed that flows to an interior point inside the query region there exists at least one point on the boundary of query region that it must flow through proof using proposition 1 there exists a witness present in the constrained mist for the dem cell such that it would cross the boundary at one point and will satisfy the same distance constraint given the direction of flow furthermore by construction of the constrained mist using algorithm 1 build mist for each interval and constrained distance associated inserted there must be no overlap such that there exists at least one interval on the boundary of the query region captured by the constrained mist for which the point must flow through before reaching the interior point to reiterate if any point with a distance constraint flows to a point in p the query polygon then the constrained mist data structure already captures it and probing the constrained mist is sufficient to verify whether a point flows to a query region 5 complexity of building the mist in this section we discuss the cost of building and storing the mist data structure for a given set of arbitrary intervals we show that the mist structure has several attributes in common to comparison based sorting algorithms namely the worst case cost for building it is o n log n with an o n space complexity for a query polygon p with n vertices since the mist structure is actively merging segments the true storage cost at the end could be lower then n lastly the cost to search the mist structure for an intersection in r is log n this cost is an important consideration as the wma p searches across the mist at least twice for every forward march proposition 3 the cost of constructing a mist data structure for a family of intervals i i 1 i 2 i n using algorithm 1 build mist is o n log n proof we start this proof by noting that the cost to search t is log n a defined property of a llrb next by definition of algorithm 2 update every call results in either an insertion deletion or nothing in the contained case of a node in t again by definition of a llrb tree an insertion and deletion cost log n we use a proof by induction to demonstrate the that total number of times that the build mist algorithm can be called is n to start we introduce a new value j which denotes the number of intervals in i that are disjoint for the case where n j every call to the mist structure will result in an insert into t this is true because at no point do any intervals intersect so every call to build mist results in an insert because every search and insert costs log n and the insertion step will be done n times the total complexity for this case will be o n log n this is true because a deletion only occurs when the underlying ranges intersect therefore the delete algorithm will never be called next take the case where j n 1 this can only be true if and only if exactly two intervals intersect without loss of generality assume that the intersection happens between the last and second to last interval submitted to the constrained mist i n and i n 1 respectively in this case the insertion will be called n 1 times prior to the intersection we then analyze the intersection via a case analysis on the value of insertmax if insertmax is true then the constraint distances associated with the new interval and existing intervals are analyzed and worst case three conditionals are evaluated a constant number as well as a constant number of calls to algorithm 2 update similarly if the value of insertmax is false then the intersection will cause a deletion a merge and another insertion to occur note that for any interval submitted to the constrained mist it ends with an insertion or a contains case therefore the total number of insertions even with intersections is bound by 3 n since the worst case where the value of insertmax is true and all conditionals are true then three segments of the interval are inserted each time this is logically true because the size of the mist structure can never be higher than n and worst case there may be three insertions for each call the number of deletions for this case is exactly one as deletions can only be called during intersections therefore the total number of insertions and deletions can never be greater than 2 3 n 6 n because each insertion and deletion costs o log n we get an o n log n upper bound on the complexity of building the constrained mist 6 correctness of the marching algorithm at the heart of our algorithm is the simple observation that the outcome of the proposed marching algorithm of the intervals induced by the boundary of the query region in conjunction with the cells of the query region will form the watershed boundary of a complex query region to show this we will first convert the piece wise linear query region into the cells of the actual dem throughout this section we abuse the notation and interchangeably refer to edge segments of query polygon and cells corresponding to these segments while different in nature these objects induce the same subset of dem in digitized representation this is a consequence of the following correctness result for bresenham s algorithm bresenham 1965 proposition 4 the rasterization of line segment induced by any two points in r 2 across a regular grid using the bresenham algorithm creates a set of grid cells that completely contain the line created by connecting the two points the correctness of the marching algorithm is a consequence of the following structural result observe that in computing the actual watershed one has to include the interior cells of query region p as part of the final outcome proposition 5 given a dem g the boundary of the watershed w p g created by intersecting p with g is equal to the boundary of the watershed w p g created by the union of the vertices v 1 v 2 v ℓ representing the boundary p of p proof we provide a case analysis discussing by considering various possible forms for p case 1 assume p represents a piece wise linear object with an empty interior in this case the intersection of the piece wise edge segments of p over g is equivalent to that of intersecting p with g fig 7 in this case the wma p creates a surface using the mist to separate the interior and exterior cells and the statement of the proposition follows case 2 assume p represents a simple closed polygon whose interior is not empty we argue that replacing p with its boundary p produces the same watershed boundary when using the wma p fig 8 observe that the intersection of the piece wise segments of p with the regular grid g forms an exterior region that may enclose some of the internal grid cells figs 9 and 10 similar to case 1 the exterior grid cells corresponding to p are included in the final watershed but not necessarily on the boundary of watershed it remains to show that the watershed associated with the interior grid cells will always be included in the final watershed to this end we will consider two sub cases see fig 11 sub case 2a in this case we will assume that for a given grid cell a in the interior of p its watershed w a is completely contained within the interior of p see fig 9 by definition of a d 8 fdg and the mns labelling algorithm there exists no path or set of grid cells that connect a to any grid cells outside polygon p otherwise the watershed of a would cross p which is contrary to assertion of proposition 4 of line segments in p this is contrary to the premise of this case sub case 2b next assume p contains an interior cell a whose watershed boundary lies partially outside p fig 10 in this case the watershed of a w a consists of a section i n t w a entirely inside the cells spanned by p and exterior portion e x t w a i e w a e x t w a i n t w a similar to the sub case 2 a the interior watershed is contained entirely within the boundary of p since the watershed for cell a extends beyond the boundary of p it must intersect cells associated with p at a number of grid cells b p a with respect to g fig 10 since all flow from the exterior of p must pass through b p a it is clear that e x t w a w b p a by definition the march around the cells associated with p will visit all of the grid cells in b p a it is clear that the watershed boundary of e x t w a is always inside the watershed boundary of p i e e x t w a w p this in turn implies the statement of the proposition 7 time and space complexities of the wma p the space and time complexity required to return a watershed boundary ω w for a region p given grid g i e wma p p g can be broken down into two steps the first is the complexity of creating the mist data structure from the intersection of the boundary of p with grid g and the second is the march around ω w using the mist in a probe step during every forward march in proposition 3 we show that the complexity to build the mist data structure is o n log n where n is the number of grid cells returned when p is intersected with g in the original wma algorithm the complexity was shown to be constant in relation to the boundary length of the watershed l divided by the stride s length of each forward march the stride length is equal to the distance between each lattice corner or the underlying grid cell resolution the wma p variant of the marching algorithm requires a further query of the mist data structure we have shown that the cost to probe the mist structure is based on the number of nodes m stored in the binary tree so that each probe can be bound be o log m putting this all together the total complexity of the wmpa p is o l s log m 8 comparison to existing techniques computational methods to delineate watershed boundaries utilize a single pour point or outlet point to describe the location at the lowest pour point of the watershed taudem tarboton 2015 whitebox lindsay 2020 and grass grass development team 2017 existing software packages next initiate a flow path tracing algorithm across the fdg identifying every grid cell where water will flow through the pour point we are only aware of one software implementation that can return watershed boundaries for polygonal or linear objects instead of for singular input locations the batch watershed delineation for polygons function within esri s arc hydro tools we therefore compared the speed and operations of the wma p algorithm with this tool multiple test cases were carried out comparing performance and accuracy of the new watershed boundary delineation versus the method used in esri s arc hydro tools batch watershed delineation for polygons fig 12 the wma p algorithm performed between 114 and 1175 times faster then the esri implementation wxith an average of 540 times faster table 2 in even the most simple case the process in esri needed 372 s to complete whereas the wma p took 0 5 s for the largest watershed tested the wma p ran in 15 s compared to 17 890 s with arc hydro or 0 1 of the time as the watershed size increases the number of polygon vertices increases and so the time it takes to delineate watersheds increases since the wma p algorithm is based on the traversal of only the watershed boundary itself this implementation returns only the boundary and does not return sinks however the arc hydro algorithm does return the internal sinks fig 12 this resulting watershed has many holes sinks that would need to be filled if the boundary alone was desired sinks are areas of the watershed where water is trapped in depressions or flows underground conversely if sinks are required and useful the wma p algorithm would need to be modified to return internal sinks this could be accomplished by storing the locations of grid cells that are sinks e g do not flow into any of their 8 neighbors as points and their associated watershed boundary as polygons inside of a geographic information system or relational database management systems e g postgis these sinks root locations points could be spatially intersected with the watershed boundary returned from the wma p once selected the internal sinks polygons could be subtracted from the outer boundary creating internal sink by using a r tree spatial index this process could avoid the large io costs of loading sinks watershed boundaries for sinks that are not within the output watershed 9 conclusion existing web based tools and associated algorithms are designed to return watershed boundaries in an efficient way for a singular drainage location e g x y r 2 this manuscript describes a method that can return the watershed boundary for a linear or polygonal geometric object e g x 1 y 1 x 2 y 2 x ℓ y ℓ r 2 fig 1 it extends the original watershed marching algorithm wma and data structures described by haag et al 2018 with a new data structure called the modified interval search tree mist the mist data structure allows the marching algorithm to consider multiple locations during the marching steps extending the original wma algorithm to support linear and polygon geometric objects support this research has been supported in part by a grant from the u s environmental protection agency epa under grant cb 96363001 0 any opinions findings and conclusion or recommendations expressed in this manuscript are those of the authors and do not necessarily reflect the view of the epa or the us government declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests ali shokoufandeh and scott haag have applied for a patent for the watershed marching algorithm this uses this algorithm but it does not focus on it ali shokoufandeh also has a contract to provide an api to the algorithms described in this paper to model the nutrient estimates of the chesapeake bay acknowledgment the authors would like to extend their gratitude for insightful comments on manuscript from lin perez jeffery pennington and john dawes and chesapeake commons for providing us with access to fielddoc org application lastly we thank the anonymous reviewers for their insightful suggestions that have made the manuscript more readable and accurate 
25952,in this manuscript we describe a computational model to delineate watershed boundaries for simple geometries points lines or polygons where maximum water flow distance can be constrained by a user defined variable this method generalizes earlier research to delineate watershed boundaries using a marching algorithm our proposed method allows users to delineate watersheds for a number of use cases including evaluating best management practices bmp s measuring water impacts to municipal city and parcel boundaries and others we describe computational complexity and space costs and compare them experimentally to standard techniques keywords watershed boundary marching algorithm modified nested set algorithm constrained watershed boundary 1 introduction delineating watershed boundaries is a well studied field o callaghan and mark 1984 tarboton 1997a matthies et al 2007 haag et al 2018 providing information for the management of hydrologic systems watershed delineation and mapping is part of a larger field of study used to connect the state of upland uphill regions through hydrologic flow and transport e g pollutants we acknowledge that the data models and algorithms described in this paper do not solve all of the issues required for modelling hydrologic systems rather they provide an improvement in the efficiency for one important step in the larger modelling process we begin by a broad overview of the field of hydrologic modelling the sub field of watershed modelling and lastly existing method to delineate watershed boundaries this is used to place the contributions of this paper withing the larger field of study in which it sits 1 1 background hydrologic modelling is the field of study that is used to understand the impact that the water cycle has on the past current and future system state both natural and anthropogenic in this review we relied heavily on vijay singh s singh 2018 survey manuscript to review the history of hydrologic modelling generally hydrologic modelling started at first principles with discoveries on general rules of water diffusion mulvany 1850 ground water diffusion d 1856 and evaporation john 1798 these constructs where identified and codified through empirical models and large scale in situ experimental techniques and were codified in 1964 by chow 1964 the 1960 s saw the beginning stages in the transfer of physical watershed models to digital representations of hydrologic system within computational systems as the utilization of sensing modalities increased in step with computational power it became easier to model larger more complete systems for example by 1966 the stanford watershed model was able to model the entire hydrologic cycle within one modelling regime singh 2018 the work presented in our manuscript focuses on watershed modelling or the study of the runoff collection and impact of precipitation over terrestrial land features since 1960 s this field has been heavily integrated with advances in computer and sensor technology singh 2018 modern remote sensing technologies provide a wealth of information on the current and historical state of terrestrial systems these include high resolution maps of surface elevation and bathymetry through digital ranging technologies like lidar and synthetic aperture radar sar air photo interpretation and stereoscopy and others jensen 2014 information on what land is used for and what is covering it e g forest urban wetland etc soil and bedrock types snow and ice cover thickness and even changes in gravitational strength that can be correlated to local mass loss or gain yeh et al 2006 as the spatial and temporal resolution of remotely sensed data increased so did the need to convert these datasets into near real time forecasting systems for example the iowa flood information system ifis is designed to merge multiple real time stream gauges into a holistic view of flooding in the state of iowa modern computer mapping systems have allowed detailed models of sources and sinks of contaminates to inform the management of aquatic systems and other associated resources parajuli et al 2013 there are several types of watershed models including models to estimate nutrient and sediment flux or models that are used to predict the physical state of a stream such as temperature or water flow and volume examples include the spatially referenced regression on watershed attributes sparrow and mapsheds mapshed 2014 the main objective of these modelling efforts is to quantitatively connect what is happening between upstream and downstream regions either in stream or adjacent e g flood modelling an important component of watershed modelling and by extension hydrologic modelling is the identification and delineation of a watershed boundary as input geography tesfa et al 2011 a number of existing software systems have pre built routines that return watershed boundaries mostly for raster based representations of digital elevation models dems e g geographic resources analysis support system grass grass development team 2017 terrain analysis using digital elevation models taudem tarboton 2015 and whitebox lindsay 2020 among others to support these efforts a number of methods have been developed to model how water flows from higher elevation to lower over adjacent dem grid cells this paper focuses on the d8 flow model where water flows to the one neighboring grid cell that has the largest slope is both lower then the current cell and has the steepest gradation however there are other arguably more accurate ways to model this flow such as the d method tarboton 1997b or the multiple flow direction mfd wilson 2012 method which partitions flow into all adjacent cells which are lower then the current cell we hope in the future to extend our work from d8 to other flow models but it is unclear if they will provide the same advantages described here originally watersheds modelling systems were developed for main frame and desktop computational systems more recently a general trend has to been to move these geospatial models into cloud based computational environments with application programming interfaces api serving as connectors between multiple numerical models khan et al 2017 an example of this paradigm is the model my watershed web application developed by the stroud water research center model my watershed relies on a number of interconnected modelling api s disclaimer two authors of this manuscript are members of the team that helped developed model my watershed to calculate nutrient loading and attenuation watershed boundaries site stormwater predictions and rapid calculations of land use land cover estimates for watershed boundaries this paper describes an algorithm and associated data model to delineate watershed boundaries over larger geographies in computational times that support the development of web based modelling tools the transformation from desktop computing systems to cloud based decision support tools has allowed non technical users to run models that are designed to mimic natural processes salewicz and nakayama 2004 for example the model my watershed web application allows users to delineate and describe watershed boundaries based on digital elevation and grid based models of land use and land cover these systems need to provide information in retrieval times consistent with modern web browsing preferences on the order of seconds to minutes some work has been done to create faster methods to return watershed boundaries given an input pour point the first example developed by perez 2000 calculates upstream watersheds at confluence points and only partially calculates the local catchment boundary the local catchment boundary is then merged with the upstream watershed creating a singular watershed boundary for a given input point it is the authors understanding that a similar method is employed by both wiki watershed watershed delineation module and the esri watershed delineation api neither of these tools to our knowledge has published methods on these approaches neither of these approaches would be amenable to calculating the union of a watershed given a line or polygon without some modifications lastly sit and sermet yusuf 2020 has developed an algorithm that quickly merges multiple adjacent catchment boundaries together on the client side we could not compare this method to our approach directly because it only returns watersheds at some minimum size unlike the methods described by perez 2000 1 2 contribution this manuscript describes several innovations that differentiate it from prior methods our proposed method directly computes the merged watershed boundary for a set of contiguous pour points the majority of existing software systems grass grass development team 2017 taudem tarboton 2015 and whitebox lindsay 2020 return watershed boundaries for a singular pour point our proposed model extends our original watershed marching algorithm wma modified nested set mns algorithm and data structures published in haag et al 2018 to calculate a set of intervals based on a set of pour points our method only visits the exterior of the input geometric object corresponding to a query region therefore for any polygonal query the proposed algorithm only has to walk around the border to identify the watershed boundary our approach relies heavily on a novel dynamic data structure to merge the set of intervals called the modified interval search tree mist the proposed data structure relies on the structure of balanced binary search trees and the generalized notion of interval trees for continuous sets erwig 1998 this algorithm returns the watershed boundary for a linear or closed polygonal geometric object characterized by an ordered sequence of points x 1 y 1 x 2 y 2 x ℓ y ℓ r 2 the mist data structure allows the marching algorithm to efficiently examine multiple possible basin locations during the marching steps extending the original wma algorithm to support linear and polygonal geometric objects lastly we describe a modification to the mist structure the constrained mist by using a constrained interval insertion strategy we allow the creation of a constrained watershed boundary where water flow distance is less then some user supplied distance maximum flow distance see fig 1 for an illustrative examples this new algorithm and the associated data structures allow the creation of web based interactive map visualization and analytical application resources to delineate watershed boundaries for polygonal linear or point regions of interest this is an important advance as current techniques designed to support the delineation of watershed boundaries for a singular location rely on partially solving watershed boundaries as described by perez 2000 and therefore would not be amiable to the line or polygonal form without the additional computational costs associated with merging polygons our proposed method returns watershed boundaries using the marching method i e walking around the watershed boundary never entering into or leaving the watershed there are a number of practical uses for this algorithm for example we have partnered with the chesapeake bay program cbp and the chesapeake conservancy a regional partnership of federal state local and not for profit organizations focused on protecting and restoring the ecological habitat of chesapeake bay and it s tributaries the cbp has identified agricultural and urban nutrient loading as drivers of bay ecological health and has invested heavily in the development of best management practices bmps to reduce the impact of these land use activities on the bay the cbp has developed a bay wide modelling framework called the chesapeake assessment scenario tool cast chesapeake bay program 2017 that has been used to apportion nutrient loading to larger geographic regions stream and river catchments the cast modelling framework does not translate to site level modelling because it makes a number of assumptions that do not translate to local specific sites for example cast assumes that a one acre riparian restoration in a coastal plain upland will intercept a total of 5 acres 1 in the buffer and 4 additional upland acres bmp modelling chesapeake bay program 2018 pg 62 these numbers are based on the average watershed area for a riparian region across larger geographic regions the algorithm described in this manuscript will allow the cbp to accurately map these watershed boundaries for a specific bmp allowing the differentiating and prioritization of bmps at the site or parcel level and hopefully leading to a reduction of nutrient and sediment loading given the same resources an important consequence of the availability of this algorithm is that it can be integrated into other decision support systems as an example by integrating our algorithms the end users are able to model the efficacy of various best management practices using a web based tool built by chesapeake commons field docs fielddoc org 2016 this tool will allow users to input a potential bmp and to get instantaneous in seconds estimates of pollutant load reductions based on the drainage area elevation and local land cover in a typical use case a user can upload a polygon representing a forested buffer strip the watershed boundary is rapidly generated using the algorithm described in this manuscript next we use a fast zonal algorithm haag et al 2020 to characterize the land cover type associated with that buffer s watershed using this information estimated reduction in nutrient and sediment load to the stream can be calculated an illustration of such a scenario for this tool is shown in fig 2 the constrained version of our algorithm has implications for allowing the determination of affected stream length by waste water treatment plants and other point sources of pollution for example an observed fish kill could be tracked to all point sources of pollution within 10 miles upstream of the observed event this is done by submitting the observed event point entering in a distance relevant to a fish kill event e g 10 miles and receiving the polygonal watershed boundary for the upland area responsible for the event i e the watershed boundary for all adjacent and overland flow between the point and 10 miles upstream 1 3 existing techniques most existing computational methods for delineating watershed boundaries utilize a single pour point or outlet point to describe the location at the bottom most flow of the watershed whitebox lindsay 2020 taudem tarboton 2015 and grass grass development team 2017 these existing software packages then initiate a flow path tracing algorithm across the flow directional grid fdg identifying grid cells where water will flow through the pour point we are only aware of one software implementation that can return watershed boundaries for polygonal or linear objects instead of for singular input locations the batch watershed delineation for polygons function within esri s arc hydro tools we note that it would be possible to return similar results for other watershed delineation methods by intersecting a line or a polygon with the dem raster and then creating and merging watershed for every grid cell intersected by the line or polygon in order to better understand how esri s arc hydro tool works we ran a number of test geometric objects through the desktop application while we could not directly examine the source code to model how the algorithms works we where able to deduce the major steps in the process based on how the tool created temporary files during its processing steps based on these results we believe the execution process of existing esri tool set follows first the input geometric object is rasterized by snapping same extent and cell size to the input fdg next a second temporary raster dataset again snapped to the input fdg is created to store the identity of the grid cells that are in the watershed of the geometric query object at this point the esri s tool set has three raster datasets the first is the input fdg the second is the rasterized version of the geometric object and the third is the output raster file that stores the identity of grid cells that are inside of the watershed it then appears that the esri s arc hydro tools walks across all of the positive cells in the original raster representing the geometric object and searches across the fdg using a path tracing algorithm if a cell is found to flow into the geometric object it s value in the last raster is set to positive an additional step is required of this algorithm compared to the single flow point versions namely that it is possible to discover raster cells in the fdg multiple times and therefore it is necessary to check to determine if a cell is already positive in value when all of the cells in the rasterized version of the geometric object have been visited the second raster layer is converted into a vector file and the intermediate raster layers are deleted when we discuss complexity in this manuscript we are referring to the number of operations required to calculate a result within a computational system we use the unlimited ram model as discussed by cormen et al 2009 in their book introduction to algorithms this model does not take into account different speeds of ram cache ssd cpu vs gpu speed etc but rather it focuses on the complexity as the total set of questions that an algorithm requires to be answered before a result is returned we could find no formal discussion on the complexity of esri s batch watershed algorithm but based on our understanding of the algorithm the complexity can be described by the cost of these five steps 1 the cost to rasterize the geometric object 2 the cost to search across the grid representing the geometric object 3 the cost to search across the fdg 4 the cost to store and search to make sure a cell has not already been visited the output raster file and 5 the cost to vectorize the final output grid cell the complexity to search across the vectorized grid is at its worst case in relation to the size of the grid as the data is stored in esri s tool as a linked list e g a vector the complexity of looping through this list is simply the number of grid cells contained by the original geometric object as identified in step 1 the complexity to search across the fdg is related to the number of grid cells that are identified by the flow tracing algorithm because flow paths are unique for a d8 fdg it is possible though unlikely that a grid cell that is inside the watershed could be found at most 2 times this is caused directly by the geometry of a d8 fdg which limits the number of possible paths to a location either a path tracing algorithm is started at a grid cell or it passes through it therefore the computational complexity of searching the fdg is directly related to the number of grid cells that are identified by the flow tracing algorithm adding up the total computational complexity from all of these steps and we find that the existing algorithm runs in complexity of the area of the original geometric object projected over the input grid plus the area of the output watershed boundary functionally this means that with increasing resolution of dems this method will scale exponentially in time with the increased grid resolution as proved later in this paper our proposed method scales in constant time relative to the number of cells in the length of the watershed perimeter thus out performing the known esri method as both watershed area increase and or dem models get smaller as an example consider that the chesapeake bay region has gone within recent history from a 30m dem based on the national elevation dataset ned to a 10m dem from the national hydrography dataset high resolution furthermore the cbp is currently working on a 1m bay wide dem model 2 notation our proposed watershed marching algorithm wma p for a polygonal query region takes as its inputs a geometric object p and a regular d 8 dem grid g and produces the boundary ω w of the watershed w p g that flows through p with respect to g the polygonal object p is represented in terms of a list of ordered vertices v 1 v 2 v ℓ we will assume that for each i 1 ℓ 1 the ordered pair v i v i 1 corresponds to a boundary edge of p and if the interior of p is non empty then v 1 v ℓ we also assume each boundary point v p is uniquely identified by its x v y v coordinates in r 2 the regular grid g consists of a set of n m cells each of which is a subset of r 2 with a uniform cell size c s g r in both the x and y coordinates we will use the notation g i j to denote the grid cell located at column i 1 n and row j 1 m for each cell g i j we will use d 8 i j to represent its flow direction value and interval pair d i j f i j with d i j f i j representing its discovery and finish values we use an existing algorithm bresenham s algorithm bresenham 1965 to intersect or rasterize the lines segments forming the boundary of p with the grid cells in g creating a set of grid cells φ p g g 1 g 2 g q given the grid cells φ p g we obtain a unique set of intervals i p g d u f u u φ p g without loss of generality we use the notations i p g and φ p g interchangeably as one uniquely identifies the other a set of intervals i d 1 f 1 d 2 f 2 d r f r is referred to as disjoint interval set if for all i j 1 r we have d i f i d j f j 0 given an arbitrary set of intervals j d 1 f 1 d 2 f 2 d k f k we define τ j as its equivalent disjoint interval set j d 1 f 1 d 2 f 2 d l f l i e d f j d f d f j d f we implement a variation of the τ operator using a modified interval search tree mist t τ j whose vertices correspond to a disjoint set of a possibly overlapping set of intervals j the mist data structure follows standard convention for a rooted binary tree with a modification that each node in it contains an unique interval range d f as its key value which is non overlapping with keys of any other node in t the interval associated with the left child of a node will always have boundaries less than the interval of the parent while the right child will always have greater key intervals than the parent to implement the mist data structure we use an existing data structure the left leaning red and black tree sedgewick 2008 to guarantee a balanced binary search tree bst structure and associated fast deletion insertion and deletion to the data structure we extend the mist data structure into the constrained mist by modifying its internal structure to represent a constrained distance such that each node contains a third key as well as its unique interval range furthermore each node is represented by a triplet d f r where d f remains as its key value that is non overlapping with keys of any other node in t but has an extra constraint r the radius that must be maintained to restrict the domain of the watershed s water flow we will use d f r to define the attributes of a new node being inserted into the constrained mist that adhere to the same properties we will use w p to represent the watershed of cells associated with p with respect to grid g to create the boundary ω w p of w p we create a secondary abstraction of the regular grid g to denote the lattice corners of each grid cell v v specifically we use l λ γ to denote a regular square lattice with λ r 2 denoting the set of lattice points and γ r 2 r 2 as the set of edges connecting lattice points the algorithm returns the watershed boundary in the form of a closed lattice walk ω w p made up of a list of lattice points in l as we will show the flow edges in the d 8 grid flow to the set of grid cells in φ p g implying that w p w φ p g 3 mist data structure the original marching algorithm as described in haag et al 2018 used a simple probe mechanism to classify grid cells with respect to watershed inclusion or exclusion specifically this simple probe determines if a grid cell is inside the watershed if and only if its discovery time d value is contained between the range described by discovery d q and finish f q values of the staring query vertex q it follows then that the combined watershed for a number of input locations can be defined by the union of the set of ranges described by the input locations in extending this probe to query polygons we will provide a necessary and sufficient condition based on the minimum set of non overlapping ranges induced by cells associated with query region the mist data structure will efficiently support this extended probe condition we define a non degenerate interval i d f d f as a non empty subset of r the main structural invariance of the mist data structure is to maintain a disjoint interval family i i 1 i 2 i n of n non degenerate intervals i e with i i i j for all 1 i j n we will assume the set i is created by incremental insertions of a set of intervals in the form of d f pairs in practice these intervals represent the mns labels associated with cells resulting from the linear intersect of query object p across grid g it is our objective to design a data structure to represent the family i using a mist t a main requirement of the data structure is that it needs to support dynamic operations including insertions and deletions in our implementation each interval will be stored as the key values of the nodes in t we will assume that these operations can be invoked in any arbitrary order in the case of insertion the new interval may have non empty intersection with existing intervals in set i which may result in further processing to maintain the integrity of disjoint family i assuming a temporal order of insertion on the elements of i the initial interval i 1 will be added as the root of t inserting any other node to the mist can be resolved in two steps first we search within the existing mist to determine if the interval is unique or if it overlaps or is contained by an existing interval starting from the root we compare the interval i k d k f k to each node s key in the tree following the bst property if the new interval i k does not intersect with the interval i i d i f i of an existing node of the mist we compare d k with d i if d k is less than d i we proceed to the left subtree otherwise we will follow the right subtree following this rule several cases may occur along the path from root to a leaf case i the algorithm reaches no node along the root to leaf path which its interval overlaps with the new interval i k d k f k i e all node in the path fall in no intersecting category when compared to i k fig 3 shows an example of this case case ii the algorithm reaches a node whose key i i d i f i contains the new interval i k d k f k i e the relationship between i i and i k falls into contained category case iii the algorithm reaches a node that its key i i d i f i intersect with the new interval i k d k f k i e the relationship between i i and i k falls into overlapping category in case i the algorithm reaches a node with no right or left subtree in this case a new node with key i k will be inserted as a leaf our implementation of mist data structure uses a modified variation of so called left leaning red black tree llrb strategy we will assign a color red to the edge connecting i k to its new parent sedgewick 2008 after inserting a node the mist tree will be re balanced the basic operations that help to maintain balance are left rotate right rotate and flip colors a llrb requires that the red edge always lean to the left therefore in the case that the new inserted node is a right child of its parent left rotate operation is used if the new inserted node or the left rotate operation from the previous step of re balancing violates the invariant property of the llrb tree i e causing two consecutive red edges both leaning on left right rotate operation is used in addition the new node might be inserted as a right child of a parent node which its left child has red edge this situation can be encountered after right rotate operation as well in this case the color of the edges that connect parent to children nodes will be flipped to black and the color of the edge attached to the parent node will be flipped to red in other words flipping colors passes red edge up one level if flip colors operation causes two consecutive red edges rotate right operations is used to maintain invariant properties of the llrb flip colors may cause a right leaning red edge this situation will be resolved with rotate left therefore starting from the new inserted node with red edge we repeatedly use one of the operations to maintain left leaning red edges and its invariant properties these operations are used to pass a red link up in a llrb tree in order to re balance the tree more detail of the code and algorithm of these operations can be found in sedgewick 2008 in case ii according to our initial goal for designing a data structure with non overlapping intervals no insertion or deletion is needed fig 4 provides an illustrative example of this case in case iii the node with key i i must be deleted from the mist tree to delete an internal node the node s key will be replaced with the key to its successor i e the minimum node in its right subtree then the successor will be deleted to delete a leaf node if the node is attached with a red edge it will be removed easily otherwise a red edge should be pushed down the tree the left rotations right rotations and flip colors will be used on the way down the search path to push red edge right or left so that the edge attached to the leaf node getting deleted flips to red then the node will be deleted starting from the deleted node the rotations and color flip operations will be used on the way up to fix right leaning red edges and consecutive red edges which are not allowed more detail of the code and node deletion algorithm can be found in sedgewick 2008 the interval i i will be merged with i k using algorithm 2 and results in interval i z d z f z the new interval will be inserted in to the mist tree starting from the root as we explained earlier fig 5 shows an example of this case algorithm 1 build mist a procedure for building a modified interval search tree mist image 1 algorithm 2 update a helper algorithm to insert an interval i k into a mist image 2 algorithm 3 stabs mist a procedure to check if a value stabs any interval in a mist with a constrained distance image 3 4 constrained version of the mist data structure we extend the mist data structure to allow the delineation of constrained flow distance boundaries from a regular d8 fdg the constrained flow distance is restricted by the maximum distance constraint which in this case constrained denotes a maximum flow distance fig 1 which is controlled by a user defined parameter r which denotes the constrained distance the constrained flow distance may also be restricted by the minimum distance constraint which in this case constrained denotes a minimum flow distance across the d8 fdg which is controlled by a user defined parameter r which denotes the constrained distance without a loss of generality for both constraints of the mist data structure when r is set to the constrained version of the mist is equal to the base form this extended version of the mist maintains the distance of the watershed to any grid and the flow distance by storing a unique flow distance parameter for every set within the mist structure this new data structure requires an additional o 1 probe when two intervals intersect without loss of generality take the case when the mist maintains the shortest flow path for every interval this check compares the original interval that overlap and the new interval to be inserted in the maximum distance strategy two cases exist a if the new interval is further away and overlaps a portion of the original interval the overlap is spliced from the new interval and the non existent interval remains or b if the new interval does not overlap pre existing intervals the whole interval remains alternatively for the minimum distance strategy two cases exist a if the new interval is closer and overlaps a portion of the original interval the overlap is spliced from the new interval and the non existent interval remains or b if the new interval does not overlap pre existing intervals the whole interval remains the constrained insertion strategy is depicted in algorithm 2 update in the case of insertion for the constrained mist the same algorithm is followed unless the new interval overlaps or is contained by an existing interval starting from the root we compare the interval i k d k f k to each node s key in the tree following the bst property algorithm 4 latticemove ℓ μ t image 4 algorithm 5 watershed marching algorithm polygon input g t p image 5 if the new interval i k does not intersect with the interval i i d i f i of the current existing node of the mist we compare d k with d i if d k is less than d i we go to the left subtree otherwise we will follow the right subtree following this rule several cases of overlap may occur along a root leaf path as depicted in table 1 case i the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case ii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case iii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case iv the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case v the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case vi the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case vii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i case viii the interval to be inserted d k f k with constrained radius r k overlaps with an existing interval d i f i with constrained radius r i such that d k d i f k f i and r k r i in what follows we will show that if a dem cell belongs to the watershed i e flows to any point interior or on the boundary of p the query region polygon a probe of the constrained mist data structure will return a positive result and the minimum flow path distance travelled between the query point and the boundary of p the inverse is also true that any point that does not flow into p will return a negative result and therefore a null distance to the boundary of p proposition 1 if a dem cell flows to a query region then one and only one of the nodes in the constrained mist will have an interval that is a witness for the dem cell proof we provide a case analysis by considering various possible flow paths of the dem cell case 1 the flow path from the dem cell to the sink does not intersect the query region by the description of the modified nested set algorithm the d value for the dem cell will not be contained in any of the intervals in the query region because the constrained mist is constructed from the aggregation of the intervals on the boundary of the query region it follows that the set of disjoint intervals maintained by the constrained mist will not contain d for the dem cell case 2 the flow path from the dem cell to the sink intersects the query region we note that there are two distinct sub cases which we discuss in turn in the first case the query point flows through an interval that is represented in the constrained mist uniquely that is it is disjoint from all the other intervals that make up the boundary of the query region this sub case leads to the observation that the path from the dem cell to the sink hits exactly one location on the boundary of p else the interval defined by the boundary point intersected by this path would not be disjoint in this case the discovery time of dem cell will intersect the interval of a node in the constrained mist and return a positive result and the minimum distance between the dem cell and the query boundary will be the flow distance between the boundary point and the cell the second sub case occurs when the flow path from the dem cell to a sink intersects the boundary of p at multiple locations see fig 6 by definition of the modified nested set labelling algorithm the path from the dem cell to the sink can be represented by a negative monotonic distance function this leads to the observation that the first boundary point intersected along the path from the cell to the sink through p will be the closets to the query point this directly leads to the fact that for the interval defined by the dem cell there can be no other interval on the boundary of p with a larger flow distance else it would have been found first since the constrained mist only subsumes parts of intervals when the flow distance is larger the interval containing the d and f for the dem cell will always be associated with the distance from the first boundary p therefore when the constrained mist is queried it will return a positive result and the distance between the query point and p can be ascertained proposition 2 if there is a dem cell in the watershed that flows to an interior point inside the query region there exists at least one point on the boundary of query region that it must flow through proof using proposition 1 there exists a witness present in the constrained mist for the dem cell such that it would cross the boundary at one point and will satisfy the same distance constraint given the direction of flow furthermore by construction of the constrained mist using algorithm 1 build mist for each interval and constrained distance associated inserted there must be no overlap such that there exists at least one interval on the boundary of the query region captured by the constrained mist for which the point must flow through before reaching the interior point to reiterate if any point with a distance constraint flows to a point in p the query polygon then the constrained mist data structure already captures it and probing the constrained mist is sufficient to verify whether a point flows to a query region 5 complexity of building the mist in this section we discuss the cost of building and storing the mist data structure for a given set of arbitrary intervals we show that the mist structure has several attributes in common to comparison based sorting algorithms namely the worst case cost for building it is o n log n with an o n space complexity for a query polygon p with n vertices since the mist structure is actively merging segments the true storage cost at the end could be lower then n lastly the cost to search the mist structure for an intersection in r is log n this cost is an important consideration as the wma p searches across the mist at least twice for every forward march proposition 3 the cost of constructing a mist data structure for a family of intervals i i 1 i 2 i n using algorithm 1 build mist is o n log n proof we start this proof by noting that the cost to search t is log n a defined property of a llrb next by definition of algorithm 2 update every call results in either an insertion deletion or nothing in the contained case of a node in t again by definition of a llrb tree an insertion and deletion cost log n we use a proof by induction to demonstrate the that total number of times that the build mist algorithm can be called is n to start we introduce a new value j which denotes the number of intervals in i that are disjoint for the case where n j every call to the mist structure will result in an insert into t this is true because at no point do any intervals intersect so every call to build mist results in an insert because every search and insert costs log n and the insertion step will be done n times the total complexity for this case will be o n log n this is true because a deletion only occurs when the underlying ranges intersect therefore the delete algorithm will never be called next take the case where j n 1 this can only be true if and only if exactly two intervals intersect without loss of generality assume that the intersection happens between the last and second to last interval submitted to the constrained mist i n and i n 1 respectively in this case the insertion will be called n 1 times prior to the intersection we then analyze the intersection via a case analysis on the value of insertmax if insertmax is true then the constraint distances associated with the new interval and existing intervals are analyzed and worst case three conditionals are evaluated a constant number as well as a constant number of calls to algorithm 2 update similarly if the value of insertmax is false then the intersection will cause a deletion a merge and another insertion to occur note that for any interval submitted to the constrained mist it ends with an insertion or a contains case therefore the total number of insertions even with intersections is bound by 3 n since the worst case where the value of insertmax is true and all conditionals are true then three segments of the interval are inserted each time this is logically true because the size of the mist structure can never be higher than n and worst case there may be three insertions for each call the number of deletions for this case is exactly one as deletions can only be called during intersections therefore the total number of insertions and deletions can never be greater than 2 3 n 6 n because each insertion and deletion costs o log n we get an o n log n upper bound on the complexity of building the constrained mist 6 correctness of the marching algorithm at the heart of our algorithm is the simple observation that the outcome of the proposed marching algorithm of the intervals induced by the boundary of the query region in conjunction with the cells of the query region will form the watershed boundary of a complex query region to show this we will first convert the piece wise linear query region into the cells of the actual dem throughout this section we abuse the notation and interchangeably refer to edge segments of query polygon and cells corresponding to these segments while different in nature these objects induce the same subset of dem in digitized representation this is a consequence of the following correctness result for bresenham s algorithm bresenham 1965 proposition 4 the rasterization of line segment induced by any two points in r 2 across a regular grid using the bresenham algorithm creates a set of grid cells that completely contain the line created by connecting the two points the correctness of the marching algorithm is a consequence of the following structural result observe that in computing the actual watershed one has to include the interior cells of query region p as part of the final outcome proposition 5 given a dem g the boundary of the watershed w p g created by intersecting p with g is equal to the boundary of the watershed w p g created by the union of the vertices v 1 v 2 v ℓ representing the boundary p of p proof we provide a case analysis discussing by considering various possible forms for p case 1 assume p represents a piece wise linear object with an empty interior in this case the intersection of the piece wise edge segments of p over g is equivalent to that of intersecting p with g fig 7 in this case the wma p creates a surface using the mist to separate the interior and exterior cells and the statement of the proposition follows case 2 assume p represents a simple closed polygon whose interior is not empty we argue that replacing p with its boundary p produces the same watershed boundary when using the wma p fig 8 observe that the intersection of the piece wise segments of p with the regular grid g forms an exterior region that may enclose some of the internal grid cells figs 9 and 10 similar to case 1 the exterior grid cells corresponding to p are included in the final watershed but not necessarily on the boundary of watershed it remains to show that the watershed associated with the interior grid cells will always be included in the final watershed to this end we will consider two sub cases see fig 11 sub case 2a in this case we will assume that for a given grid cell a in the interior of p its watershed w a is completely contained within the interior of p see fig 9 by definition of a d 8 fdg and the mns labelling algorithm there exists no path or set of grid cells that connect a to any grid cells outside polygon p otherwise the watershed of a would cross p which is contrary to assertion of proposition 4 of line segments in p this is contrary to the premise of this case sub case 2b next assume p contains an interior cell a whose watershed boundary lies partially outside p fig 10 in this case the watershed of a w a consists of a section i n t w a entirely inside the cells spanned by p and exterior portion e x t w a i e w a e x t w a i n t w a similar to the sub case 2 a the interior watershed is contained entirely within the boundary of p since the watershed for cell a extends beyond the boundary of p it must intersect cells associated with p at a number of grid cells b p a with respect to g fig 10 since all flow from the exterior of p must pass through b p a it is clear that e x t w a w b p a by definition the march around the cells associated with p will visit all of the grid cells in b p a it is clear that the watershed boundary of e x t w a is always inside the watershed boundary of p i e e x t w a w p this in turn implies the statement of the proposition 7 time and space complexities of the wma p the space and time complexity required to return a watershed boundary ω w for a region p given grid g i e wma p p g can be broken down into two steps the first is the complexity of creating the mist data structure from the intersection of the boundary of p with grid g and the second is the march around ω w using the mist in a probe step during every forward march in proposition 3 we show that the complexity to build the mist data structure is o n log n where n is the number of grid cells returned when p is intersected with g in the original wma algorithm the complexity was shown to be constant in relation to the boundary length of the watershed l divided by the stride s length of each forward march the stride length is equal to the distance between each lattice corner or the underlying grid cell resolution the wma p variant of the marching algorithm requires a further query of the mist data structure we have shown that the cost to probe the mist structure is based on the number of nodes m stored in the binary tree so that each probe can be bound be o log m putting this all together the total complexity of the wmpa p is o l s log m 8 comparison to existing techniques computational methods to delineate watershed boundaries utilize a single pour point or outlet point to describe the location at the lowest pour point of the watershed taudem tarboton 2015 whitebox lindsay 2020 and grass grass development team 2017 existing software packages next initiate a flow path tracing algorithm across the fdg identifying every grid cell where water will flow through the pour point we are only aware of one software implementation that can return watershed boundaries for polygonal or linear objects instead of for singular input locations the batch watershed delineation for polygons function within esri s arc hydro tools we therefore compared the speed and operations of the wma p algorithm with this tool multiple test cases were carried out comparing performance and accuracy of the new watershed boundary delineation versus the method used in esri s arc hydro tools batch watershed delineation for polygons fig 12 the wma p algorithm performed between 114 and 1175 times faster then the esri implementation wxith an average of 540 times faster table 2 in even the most simple case the process in esri needed 372 s to complete whereas the wma p took 0 5 s for the largest watershed tested the wma p ran in 15 s compared to 17 890 s with arc hydro or 0 1 of the time as the watershed size increases the number of polygon vertices increases and so the time it takes to delineate watersheds increases since the wma p algorithm is based on the traversal of only the watershed boundary itself this implementation returns only the boundary and does not return sinks however the arc hydro algorithm does return the internal sinks fig 12 this resulting watershed has many holes sinks that would need to be filled if the boundary alone was desired sinks are areas of the watershed where water is trapped in depressions or flows underground conversely if sinks are required and useful the wma p algorithm would need to be modified to return internal sinks this could be accomplished by storing the locations of grid cells that are sinks e g do not flow into any of their 8 neighbors as points and their associated watershed boundary as polygons inside of a geographic information system or relational database management systems e g postgis these sinks root locations points could be spatially intersected with the watershed boundary returned from the wma p once selected the internal sinks polygons could be subtracted from the outer boundary creating internal sink by using a r tree spatial index this process could avoid the large io costs of loading sinks watershed boundaries for sinks that are not within the output watershed 9 conclusion existing web based tools and associated algorithms are designed to return watershed boundaries in an efficient way for a singular drainage location e g x y r 2 this manuscript describes a method that can return the watershed boundary for a linear or polygonal geometric object e g x 1 y 1 x 2 y 2 x ℓ y ℓ r 2 fig 1 it extends the original watershed marching algorithm wma and data structures described by haag et al 2018 with a new data structure called the modified interval search tree mist the mist data structure allows the marching algorithm to consider multiple locations during the marching steps extending the original wma algorithm to support linear and polygon geometric objects support this research has been supported in part by a grant from the u s environmental protection agency epa under grant cb 96363001 0 any opinions findings and conclusion or recommendations expressed in this manuscript are those of the authors and do not necessarily reflect the view of the epa or the us government declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests ali shokoufandeh and scott haag have applied for a patent for the watershed marching algorithm this uses this algorithm but it does not focus on it ali shokoufandeh also has a contract to provide an api to the algorithms described in this paper to model the nutrient estimates of the chesapeake bay acknowledgment the authors would like to extend their gratitude for insightful comments on manuscript from lin perez jeffery pennington and john dawes and chesapeake commons for providing us with access to fielddoc org application lastly we thank the anonymous reviewers for their insightful suggestions that have made the manuscript more readable and accurate 
25953,due to the growing amount of data from in situ sensors in environmental monitoring it becomes necessary to automatically detect anomalous data points nowadays this is mainly performed using supervised machine learning models which need a fully labelled data set for their training process however the process of labelling data is typically cumbersome and as a result a hindrance to the adoption of machine learning methods for automated anomaly detection in this work we propose to address this challenge by means of active learning this method consists of querying the domain expert for the labels of only a selected subset of the full data set we show that this reduces the time and costs associated to labelling while delivering the same or similar anomaly detection performances finally we also show that machine learning models providing a nonlinear classification boundary are to be recommended for anomaly detection in complex environmental data sets keywords active learning anomaly detection machine learning environmental monitoring 1 introduction due to the intensified deployment of in situ sensors for environmental monitoring experts in environmental science and monitoring are inundated with ever increasing quantities of high resolution data whose quality is not guaranteed horsburgh et al 2010 rieger and vanrolleghem 2008 extensive research has therefore been focused on the design of automated anomaly detection ad systems with the aim of automatically identifying unusual patterns in data aggarwal 2015 hill and minsker 2010 alferes and vanrolleghem 2016 aguado and rosen 2008 compared to manual ad it is surmised automatic ad systems can accelerate the otherwise laborious task of visually identifying any outlying data samples in complex data sets machine learning ml is a branch of artificial intelligence that focuses on learning from data murphy 2012 domingos 2012 ml models provide a valuable set of tools in science and engineering thanks to their ability of extracting meaningful information from data and can be used for automatic ad aggarwal 2015 active learning al aggarwal 2015 mussmann and liang 2018 is a special case of ml and has been proposed as a way to enable use of supervised learning models while also minimising the burdens associated with human expert labelling in al the domain expert is queried for the labels of a subset of the available data samples this is an iterative approach which selects a number of samples that when paired with the expert label are expected to improve the supervised learning model the most at each iteration the supervised ml model is trained with the data set consisting of labelled data samples from here on data records to distinguish them from unlabelled data samples this procedure is usually repeated until the model reaches satisfactory performance in the literature al has been successfully employed in several applications in the al challenge guyon et al 2011a b several data sets including handwriting and speech recognition document classification and protein engineering have been used and tested by different research groups to study the performance of al in the results achieved among the almost 300 participants the teams showed that al could achieve good performance in terms of classification accuracy with less labelled data however al was also showing lower performance at the beginning of the learning curve while it was faster and smoother in reaching high levels of accuracy in the second stage when a higher number of labelled examples was available in pimentel et al 2018 an al approach for unsupervised anomaly detection was presented and tested on synthetic and real data sets comprising images medical data for thyroid and arrhythmia issues identification in romero et al 2018 al was successfully used for handwritten text recognition pelleg and moore 2005 identify anomalies of special interest by using an al model in the presence of noisy data the method was tested over a number of distinct fields at different scales as engineer space shuttle data abalone biological data and astronomical data from uci university of california irvine data repository 1 1 https archive ics uci edu ml index php in an intrusion detection application the labelling time was reduced from two weeks to 1 h which was as high as 99 almgren and jonsson 2004 however quantifying the typical labelling times is generally hard as this depends on the nature of the input data e g images or multivariate timeseries the expertise of the domain expert and the design of the user interface while the studies cited above show promise only few of them have explored ad applications where al may be more challenging as the whole data set is highly unbalanced with infrequent anomalies present in only 0 1 10 of the total data moreover no work has been published so far to study the performance of al in environmental ad applications while there are no studies that provide empirical evidence for the differences between environmental and other types of data most of them argue in broad terms regarding its challenges not encountered elsewhere such as a seasonality at yearly monthly weekly and daily scales b non stationarity and non ergodicity c nonlinear system dynamics d a variety of systematic and incipient sensor faults such as sensor drift cherkassky et al 2006 hill and minsker 2010 eggimann et al 2017 leigh et al 2019 this makes the application of ad in an environmental setting more challenging as anomalies might not only be few but also different from each other therefore the main purpose of this paper is to evaluate whether al can successfully be used for ad in environmental monitoring to evaluate the utility of al we compare it with conventional supervised learning and random sampling strategy using 5 distinct ml model types the following models were evaluated based on their off the shelf availability in many data science platforms and ease of use random forest k nearest neighbours classifier logistic regression naive bayes and artificial neural networks al is implemented with the uncertainty sampling strategy settles 2010 we discuss how al can reduce the time spent by the domain expert to label a data set and how the ml model chosen as a base for the al strategy can influence the effectiveness of the algorithm 2 methods the first part of this section provides the reader with the necessary information on supervised ml for ad applications next we discuss the most important aspects of the tested ml models after that our implementation of al and the chosen sampling strategies are discussed we then explain the performance metrics used to evaluate our experimental results the second part of the section describes the case study and the data used for this work finally we provide a detailed explanation of our experimental procedure and we explain how the methods previously described are implemented in the generation and evaluation of the results 2 1 supervised learning supervised ad requires a training data set d x i y i i 1 n consisting of n data records where x i x d is a data sample represented as a d dimensional feature vector and y i y is the provided label for it we refer to the elements of x i as features in ad applications y i can only acquire two label values that is y 0 1 in ad applications the normal data is generally named as negative class with label value 0 and the anomalies are named positive class with label value 1 a class denotes a set of data having common characteristics during training of the model φ the task is to learn a function f x y to predict the most likely label given a new unseen data sample after training the model φ is able to compute probabilities p φ y 0 x and p φ y 1 x between 0 and 1 for a new test data sample x where p φ y 0 x p φ y 1 x is the predicted chance that a human expert would label the sample x as normal anomalous the classifier model then predicts the most likely label y ˆ f x argmax y p φ y y x different classification ml models can be used for supervised ad applications in this work we have deliberately focused on on the shelf softwares and chosen to test 5 ml models from a wide range as they are popular in the ml community and available in every commonly used data science platform e g scikit learn matlab ml toolbox spark mllib weka each model can be configured with different hyperparameters hyperparameters are defined as those parameters that determine the detailed structure and flexibility of the models these do not change during model training as they cannot be learned by the model and are fixed a priori based on prior knowledge experience and or exploratory analysis of the training data in what follows we discuss the most important aspects of the chosen models for a deeper dive into these models we refer to murphy 2012 bishop 2006 and james et al 2013 a naive bayes nb classifier rish et al 2001 models the distribution of individual classes using bayes theorem and predicts the class probabilities for each class in this work we use gaussian nb unlike the other models used in this study the nb model models the joint distribution of both data samples and labels specifically by assuming a that the features are completely independent of each other and b that the data samples in each class follow a multivariate normal distribution a particular advantage of the nb model is that there are no hyperparameters to be determined before training logistic regression lr ng and jordan 2002 is the classification equivalent to linear regression to this end the predicted probability consists of a linear combination of the input values subsequently transformed non linearly with the log sigmoid function to solve lr s optimization function different types of solvers are available here we report the ones that are implemented as libraries in many data science platforms the newton cg solver böhning 1992 the solver liblinear fan et al 2008 which uses a coordinate descent algorithm and the sag solver schmidt et al 2017 the regularisation type the amount of penalty and solver are all hyperparameters of the lr model bishop 2006 k nearest neighbours knn liao and vemuri 2002 is a non parametric model which classifies a new data sample by a plurality vote amongst its k closest neighbours in knn the predicted class probabilities are computed as the frequency of data records in the set of selected neighbours that belong to the considered class to determine what the k closest neighbours to a new data sample are a distance measure is used popular distance measures are euclidean hamming or minkowski in the simplest case all neighbours are given equal weight however one can also discount points that are further away by giving closer neighbours more weight the main hyperparameters are a the number of neighbours k b the chosen distance measure and c the weighting method a random forest rf quinlan 1987 is an ensemble approach based on decision trees decision trees are used repeatedly to split the input data in a top down approach with the intent to separate samples with different labels from each other the most important hyperparameters for this classifier are a the number of trees in the forest b the optimization function used to measure the quality of a split e g gini impurity or entropy information gain and c the maximum depth of the trees the predicted class probability is the fraction of decision trees in the ensemble that predicts the considered class artificial neural network ann classifiers dreiseitl and ohno machado 2002 transform the input data into the predicted class probabilities by means of a complex network of simple yet non linear unit operations neurons each neuron has its own set of parameters while anns are essentially nonlinear regression models they are extremely flexible due to the ability to string an almost arbitrary number of neurons together specifically by using large numbers of layers between data samples and predicted labels hidden layers each including many neurons the main hyperparameters for ann classifiers are a the number of hidden layers b the number of neurons in each hidden layer c the type of nonlinearity in each neuron d the solver algorithm used for calibrate the parameters of each neuron bishop 1995 and e the learning rate for weight updates note that the main impact of the learning rate is on convergence speed while a large learning rate allows the model to learn faster this is at the cost of arriving on a sub optimal final set of weights local minimum small values for the learning rate instead can cause small weight changes and slow learning attoh okine 1999 zeiler 2012 in this work we have selected a small learning rate as for the solver recent research nur et al 2014 kingma and ba 2015 suggests that the chosen optimization algorithm leads to inductive bias which in turn can be interpreted as an implicit type of regularisation or prior 2 1 1 additional notes on the chosen models as mentioned above the nb model describes the joint density of both data samples and labels as a result this allows generation of artificial data records according to the calibrated model for this reason it is known as a generative model in contrast the lr knn rf and ann models do not describe the joint density only the distribution of the labels conditional to the samples these models are known as discriminative models generative models like nb often outperform discriminative models such as rfs and anns on smaller data sets because their generative assumptions prevent overfitting ng and jordan 2002 discriminative models on the other hand are generally expected to perform better when a the assumptions in the generative models are untrue and b large and representative data sets are available ann models in particular are well known for their flexibility and outperform other models especially in the large and representative data regime sarle 1994 nevertheless they need extensive hyperparameter tuning to correctly choose the right architecture bardenet et al 2013 additionally ann models are prone to overfitting to avoid this issue in this work we only consider small ann architectures low number of hidden layers and neurons in each hidden layer in contrast rfs are faster and easier to train as they require fewer hyperparameters to tune in addition compared to ann classifiers rfs are less prone to overfitting and can learn from smaller data sizes liu et al 2013 lr is a simple discriminative classifier but as nb fits the data with low flexibility compared to other methods this is also because they both present a linear decision boundary between the classes finally knn is also easy to implement in terms of hyperparameter tuning and training time and as it is an instance based learning classifier it can immediately adapt as new training data comes in however it is also known to be sensitive to noisy data and might not perform well on unbalanced data unless the classes are well separated cho et al 1991 note that we did not consider other well known models such as fuzzy models or support vector machines svms as for fuzzy classification models unlike statistical classification models they require the assumptions that a single data point can simultaneously belong to multiple classes through use of fuzzy memberships in this work we assumed that a data point can belong to one class only thus leading to the use of models that predict a probability not a fuzzy membership as for svms we avoided this model structure as the large size of the kernel matrix prevented an efficient execution of our experiments 2 1 2 data pre processing data preprocessing steps such as centring and scaling are a common practice of data preparation for ml for two main reasons kotsiantis et al 2006 first these operations can improve the chances of convergence to optimal parameters during model training and the rate of convergence to the final parameters in turn improving the efficiency of the applied training algorithms second for models based on distance measures like knn centring and scaling can affect the modelled relationship in this work we have standardised all samples by centring to zero mean and scaling to unit variance as is common for classification purposes to this end the mean m and standard deviation s were always computed for each feature separately and only on the basis of the data records available for training 2 2 labelling strategies in this study we have applied three methods for labelling a complete labelling b random sampling and c al with uncertainty sampling complete labelling conceptually speaking complete labelling is the most simple method it assumes that a human expert has manually labelled all available data samples and corresponds to conventional use of supervised learning models labelling based on random sampling rnd this incremental learning method is initiated by querying the human expert for labels for a small set of data samples this leads to the production of an initial set of data records d 0 which is used to train the initial supervised model φ 0 the pool of unlabelled samples is given as t t i i 1 m where m is the data pool size a small number of samples are randomly selected from this pool the domain expert is then asked once more to provide a label y for the selected samples following this the new augmented set of data records d 1 x i y i i 1 n n is used to re train the model obtaining φ 1 the process is repeated for nit number of iterations until all samples have been labelled or the model reaches satisfactory performance this workflow is shown in fig 1 note that other stop criteria can be used e g the data samples for the query can only be obtained at a cost generally the labelling costs and the procedure is repeated until the labelling budget is reached settles 2011 to select the samples in this work we have employed a pseudo random number generator as a python module in our code and used the generated numbers to select the indices of the data points to sample in such a way that each sample in the pool is equally likely to be drawn labelling based on active learning using an uncertainty criterion unc al is also an incremental learning method that differs from rnd sampling in how the samples are selected from the unlabelled data pool after obtaining an initial model φ 0 as with random sampling the model is tested on the pool of unlabelled samples t the predicted class probabilities p φ y y x are then used to select those samples that are expected to improve the model prediction performance the most following labelling inclusion in the augmented labelled data set and model re training as with random sampling the domain expert is then asked to provide a label y for the selected samples the operating assumption underlying al is that predicted class probabilities carry information about the utility of a label for model training before this label is actually available i e expected utility al comes in many variants reflecting the circumstances under which data are produced and how soon after data collection one can expect a domain expert to provide labels for instance there are al methods that query samples by selecting them from on line signals while being collected simultaneously angluin 1988 lewis and gale 1994 atlas et al 1990 in this work we adopt pool based al lewis and gale 1994 where the complete set of unlabelled data samples x i is already available before querying starts this is the most common case in many ad research works meng et al 2013 almgren and jonsson 2004 al methods can also differ in how the utility of a yet unseen label is estimated settles 2010 in this work we employ uncertainty sampling unc a selection strategy where the al model selects the input x i for which the model s predicted label is most uncertain 1 i argmin i max y p φ y y x i where max y p φ y y x i is the maximal class probability given x i our implementation of al is described in algorithm 1 note that another sampling method is based on selecting the samples with maximum entropy holub et al 2008 instead of label uncertainty this however produces the same selection in binary classification problems and is not tested in this work another possibility includes using membership query sampling in which the most informative samples are selected based on classification disagreement between different ml models trained on the same data set as this involves training multiple models at the same time the computational costs are too high and unfeasible for the goal of this study algorithm 1 supervised anomaly detection with active learning 2 3 performance evaluation in this work we are dealing with an unbalanced data set therefore intuitive detection accuracy metrics e g ratio of correctly identified data samples vs total number of data samples are not recommended as performance metrics see e g géron 2019 we will refer as true positives tp the number of correctly identified anomalies y 1 y ˆ 1 true negatives t n is the number of correctly classified normal data y 0 y ˆ 0 finally false negatives fn and false positives fp are respectively the number of incorrectly classified normal data y 0 y ˆ 1 and anomalies y 1 y ˆ 0 accordingly to the above definitions the following measures can be computed 2 p r e c i s i o n t p t p f p r e c a l l t p t p f n while recall expresses the ability to find all relevant instances in a dataset what proportion of actual anomalies was identified correctly precision expresses the proportion of the data points the model classifies as anomalies were real e g a model that produces no false positives has a precision 1 generally there is an inverse relationship between these measures as precision increases recall decreases this is called the precision recall tradeoff for this reason a popular score used to measure model performance for unbalanced classification problems is the f 1 score this is computed as the geometric mean of precision and recall 3 f 1 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l note that we have intentionally avoided testing for multiple criteria to keep the evaluation simple however other performance measures exist and can be also used one example is the area under the receiver operating characteristic auc roc which provides an aggregate measure of performance across all possible model classification thresholds finally to critically evaluate the results and understand the behaviour of the models we will also investigate the type of data that is queried during the iterations from unc and rnd sampling strategies 2 4 case study data set the data set used for this study is a multivariate time series data of high spatial and temporal resolution that was collected as part of a long term ecological experiment described in narwani et al 2019 the primary goal of the experiment was to quantify the resilience of aquatic ecosystems in the face of eutrophication using replicated experimental pond ecosystems hereafter ponds 16 of such 15 000 l fiberglass ponds fig 2 a were set up at eawag dübendorf switzerland with a layer of gravel and a mix of tap and lake water successively macrophytes myriophyllum spicatum and dreissena mussels dreissena polymorpha were added to the ponds and inorganic nutrients were increased progressively as part of a fully factorial design four randomly chosen ponds received either no species one of the species or both species together in this work we focused on data from four ponds added with macrophytes as these data were the ones with the most anomalies each of these ponds was equipped with a multi variable instruments exo2 sonde from xylem inc 2 2 https www ysi com exo2 each of the 16 instruments contained sensors for eight variables conductivity chlorophyll and phycocyanin fluorescence dissolved organic matter fluorescence dissolved oxygen saturation and concentration ph and temperature fig 2b measurements of these eight water parameters were recorded simultaneously in each multi sensor with a fixed time interval of 15 min as for the calibration protocol for the instruments before placing them in the mesocosms a 48 h cross comparison trial was performed where the water parameters were measured using all 16 instruments inside a single tank with this data initial off factory differences between the instruments were corrected then the same cross comparison and a calibration were repeated for two maintenance periods during the experiment data labelling the domain expert manually labelled anomalies for specific conductivity the labelling was conducted through visual inspection of interactive high resolution charts that were created in r using ggplot2 wickham 2016 plotly sievert 2018 and htmlwidgets vaidyanathan et al 2018 these interactive graphs were saved in html and opened in a standard internet browser and screened by the domain expert for anomalies all measured water parameters were plotted in full temporal resolution within a single chart window 2 weeks at a time this way the domain expert could compare all data streams for a given period and decide whether a segment would qualify as anomalous after screening the data from the measured variables the domain expert supplied us with the labels for specific conductivity the above process resulted in a labelled data set containing n 22464 data records with d 8 dimensions the measured water quality variables the data set covers a period of 234 consecutive days and includes 2 anomalies this was then used to train the chosen ml models which involves learning determining the values for all the model s parameters from the provided labelled data records details on the training procedure are described next 2 5 experimental procedure the complete experiment performed in this work consisted of the following steps 1 data set splitting the full data set was split into 80 training data d and 20 test data in a stratified manner domingos 2012 2 hyperparameter tuning hyperparameter tuning was based on supervised learning with complete labelling as described above during this step optimal hyperparameters were selected by means of grid search to this end model performance were measured by stratified k fold cross validation of the f1 score where k is the number of folds in our case k 10 refaeilzadeh et al 2009 this was done by using the training data set only the implemented hyperparameters for the chosen ml models are reported in table 1 and the grid search results for each model are shown and discussed in detail in the supporting information note that we have included the regularisation hyperparameter c for lr it in the grid search even though the under parametrised regime we are working in does not require this bishop 2006 the results confirm that lr without regularisation is to be preferred in our case this is further discussed in the supporting information 3 best performance after selection of the optimal hyperparameters each model was trained with the complete training set and using complete labelling the f1 score was computed with the test data set to obtain the best performance for each model 4 incremental learning each model was then tested in combination with the incremental learning methods using the unc and rnd sampling strategies an initial subset of the training data d 0 containing 182 data records 0 25 of the training data was used for both learning methods and all 5 models each model was then updated in an iterative manner through one of the incremental learning methods as described earlier to this end n 10 samples were selected for querying at every iteration this choice is in line with current practice smailović et al 2014 ramirez loaiza et al 2017 zhu and hovy 2007 for example in zhu et al 2008 the selection of n 10 samples with uncertainty sampling showed efficient results as well as in brinker 2003 where the authors proved that smaller batch sizes e g 8 16 have higher learning efficiency at each iteration the f 1 score over the test data was used to measure model performance the incremental learning strategies were continued until all data samples had been labelled considering that the initial set of data records d 0 can influence the benefit of incremental learning relative to complete labelling as well as the benefit of al to learning with rnd sampling we repeated the execution of each incremental learning method r 10 times the initial sets of data records were sampled in a stratified manner and without repetition so that the fraction of anomalies in the initial sets of data records would match the fraction in the complete training data set 2 and that no data record was used more than once for initialisation in total incremental learning was applied 100 times 5 models 2 incremental learning methods 10 repetitions 5 to evaluate the benefits of incremental learning and al in particular the f1 scores on the test data set are reported as a function of the model type the learning method and repetitions in addition the following summary statistics were computed best f 1 the f 1 score on the test data set obtained with the complete labelling strategy for all 5 models f 1 i n i t the mean f 1 score on the test data set after initialisation of incremental learning across the r 10 repetitions this is reported for all 5 models and both incremental learning methods s t d i n i t is the standard deviation of the f 1 score after initialisation of incremental learning across the r 10 repetitions this is reported for all 5 models and both incremental learning methods f 1 95 95 of the best f 1 score and is used to compute the amount of labelled data needed to reach this value this is reported as labels f 1 95 note that we report the lowest f 1 score between the r 10 repetitions s t d 95 the standard deviation of f1 score between the different data folds when 95 of f 1 score is reached qanom the number of selected anomalies at f 1 95 tot q the total number of queries at f 1 95 3 results we discuss the results obtained with complete labelling first this is followed by a detailed discussion of the results obtained with incremental learning the code developed for this study and the labelled ecological data needed to reproduce the results reported in this work have been made publicly available at https doi org 10 25678 00023y 3 1 best performance with complete labelling the selected ml models were first trained with the full training data set and then tested on the test data the resulting best f 1 scores were computed following equation 3 on the test set and are reported in table 2 it is evident that rf knn and ann models produce a superior ad performance compared to lr and nb on the full data set these results might be explained by the reduced flexibility of lr and nb models we discuss this further in section 4 1 0 1 3 2 random and uncertainty sampling evolution of model performance during incremental learning fig 3 shows the average f 1 score over 10 repetitions as a function of the queried number of samples with the rf model and with both unc and rnd sampling strategies qualitatively speaking it can be seen that the rf model using unc sampling has reached the best f1 score already after few al iterations while the same model needed a greater number of iterations to reach the same value using the rnd sampling strategy expanding these results to the other models we plot in fig 4 the amount of training data needed to reach f 1 95 for all models and both incremental learning strategies as rf knn and ann models produced a high performance with complete labelling their f 1 95 was higher than lr and nb models additionally it is worth noticing that these models have reached their corresponding f 1 95 with considerable fewer iterations using unc sampling than using rnd sampling strategies on the other hand lr and nb models while presenting lower f 1 95 also did not show significantly difference between the two sampling strategies these results are reported in table 2 and further discussed in section 4 1 0 2 going back to fig 3 we have plotted the standard deviation of the f1 score across the 10 repetitions which is shown as a grey area obviously at 100 of training data the learning curve for both strategies converged to the same value because we have used the same training set in all cases additionally the initial value of standard deviation was high meaning that the variability in model performance induced by random initialisation disappeared as more training data were selected this is true for both incremental learning strategies but appears to occur faster with active learning unc the above results are quantitatively reported in the form of summary statistic in table 2 for all models and both incremental learning strategies queried samples as discussed in section 2 3 unc sampling selects data samples for which the model is the least confident about its prediction for rnd sampling these data are randomly selected for this reason it is interesting to understand what kind of samples are selected for querying by the human expert fig 5 shows the type of samples that have been queried with unc and rnd sampling for the rf model in the first 50 iterations corresponding to 0 94 of the training data for a specific repetition repetition 1 out of 10 here red squares represent anomalies and green squares normal data the horizontal axis represents the iteration number and the vertical axis are the n 10 queries for each iteration it is easy to see that the unc sampling strategy has selected a considerable amount of anomalies based on this result we speculate that due to the severe class unbalance the model was more uncertain about examples of anomalies which were the least seen during training the initial model φ 0 was trained on only 182 data records of which 2 are anomalies for the rnd strategy however where the data samples were selected randomly the data samples that were queried the most were mainly from the normal class due to their dominant presence in the data set fig 6 shows the same kind of information in a different format the accumulated number of queried samples that are anomalous are shown as a function of the total number of queried samples both as a fraction of the number of samples in the training data it is visible that the rnd sampling starting from the first iteration selected fewer anomalies than the unc sampling strategy the curves shapes follow a similar trend to the curves in fig 3 which might indicate that the selection of anomalies is decisive for improving the models performance the results from all the models using unc and rnd sampling strategies are shown in table 2 where we report the number of selected anomalies qanom f 1 95 and the total number of queries tot q f 1 95 4 discussion our results demonstrate that for ad applications in environmental monitoring the labelling efforts could be greatly reduced by using an al strategy specifically unc sampling we discuss below the main findings of this work and its consequences for sensor management in the environmental sector we then conclude with our practical recommendations for the implementation of al strategies 4 1 summary of results best f1 score the experimental results for our case study showed that in terms of best f1 score rf knn and ann models result in a considerably higher performance compared to lr and nb models this result might be explained by the reduced flexibility of lr and nb models which present a linear decision boundary between the 2 classes this jeopardises their learning process preventing them to clearly separate the anomalous from the normal class although further tests are needed to prove this hypothesis we believe that this is the most important cause for the reduced performance of nb and lr since we do not apply regularisation in lr this can be excluded as a factor for its reduced performance in addition specifically for nb the above results may be explained by the nature of the input data in environmental applications normal data might not be generated from the same distribution as it presents baseline changes due to different seasons and anomalies may be generated by different events which do not show the same pattern we suspect that nb model failed at correctly classifying such data because due to its generative properties it makes assumptions on the distribution of the data lastly the lr model presented better performance than nb for best f 1 and for starting f 1 f 1 i n i t a reason behind this behaviour is that lr is still a discriminative model this outcome follows the results presented in ng and jordan 2002 where it is shown that as the number of data records available for training is increased lr overtakes the performance of nb because of its discriminative behaviour this was also articulated by vapnik 1999 one should solve the classification problem directly using discriminative models thus modelling p y x and never solve a more general problem as an intermediate step thus modelling p x y and p y whether this holds also for more flexible and nonlinear model structures of which only discriminative variants ann knn rf were used in this study remains to be studied random and uncertainty sampling nb and lr models however even if their best f1 score was 0 1 0 2 points lower than the other models were able to converge to 95 of their best f 1 only with 0 33 1 08 of rnd sampling iterations this means that there is an important trade off to be made between two competing and important objectives model performance and labelling cost additionally as for the rnd sampling type the models were initially mainly trained with normal data because it has the highest probability of being selected this suggests that nb and lr models might not need many examples of anomalies during training to learn the best available decision function between the two classes as expected the unc sampling strategy is much more effective than the rnd sampling strategy it offers better or equal classification performance regardless of the number of samples that have been queried for these cases in fact our results show that by applying al with unc strategy it is possible to reach a high model performance in just a few iterations in the best case the ann model only needed 0 48 of labelled data to reach 95 of the best f1 performance score note also that incremental learning with either rnd or unc sampling was always more effective than complete labelling and could potentially save time and costs associated with labelling large data sets in environmental applications anomalies in addition to being low in number can be caused by a large variety of disturbances so ml models cannot easily generalise from them we have shown in table 2 that all models with unc strategy favoured the selection of anomalous data samples for querying which suggests that ml models tend to be particularly uncertain about the predicted label for anomalous samples relative to normal samples in turn leading to the selection of anomalous samples with higher frequency this result corroborates that providing enough and representative data records for model training is one of the main bottlenecks of supervised ad 4 2 consequences for data and sensor management in the environmental sector and future research our results indicate that the amount of labelled data could be greatly reduced by using an incremental learning strategy this lifts one of the main barriers to the applications of ml techniques in the environmental sector which is the burden of labelling in our experiments during consecutive al iterations anomalous data samples are identified as the samples with maximal uncertainty this suggests that al is not only useful to reduce the burden associated with labelling for model training but may also help in identifying anomalous samples as they are added to a data set indeed one could conceive of alerting human operators of the sensor network not only when an input is classified as anomalous but also when the predicted class is uncertain a similar idea was developed by giudici et al 2020 where al has been used to identify the most informative scenarios in the optimization process for robust planning in decision making all while decreasing computational requirements the same result could also inspire use of models that only learn from normal data potentially one class models trained with normal data only could offer a more certain classification for anomalies in the test class one implementation of al for ad with one class models can be found in barnabé lortie et al 2015 note however that one class models still require an expert based separation between anomalous and normal data records in the data used for training thus not eliminating the need for expert based labelling this is contrary to frequent claims in the literature on one class models amer et al 2013 sabokrou et al 2018 another option could be to use a background or garbage class which can be used to account for the presence of anomalies that are so diverse and rare that their class cannot be learned effectively for more details on this and related concepts please see dhamija et al 2018 in doing so the model would only need to represent the normal class which is likely easier to describe mathematically compared to the anomaly class as for the variables used for model training our domain expert has supplied us with the labels for specific conductivity and we have trained our models based on this sole information however the other variables chlorophyll and phycocyanin fluorescence dissolved organic matter fluorescence may also present anomalies which could not be easily identified by our domain expert note that ph sensor signals are known to be subject to incipient and always present drift phenomena ohmura et al 2019 which we consider faults but not anomalies for this reason we considered removing the ph from the data set variables but this has not resulted in an improvement of the model performance while we acknowledge that our choice of using all variables for model was subjective compared to using the conductivity signal exclusively the tested models have resulted in an improved performance when provided with all the available variables nonetheless the effects of including these variables which were possibly contaminated with unlabelled anomalies on anomaly detection performance should be quantified in future studies e g by labelling all anomalies in all sensor signals or by implementing feature selection if dealing with high dimensional data sets mukherjee and sharma 2012 zargari and voorhis 2012 in this work we have treated the domain expert as an oracle i e providing perfect and time invariant labels however uncertainty in the labels provided by the domain expert exists russo et al 2019 villez and habermacher 2016 and may be due to fatigue learning curve of the human expert user interface etc methods to account for imperfect oracles exist donmez and carbonell 2008 du and ling 2010 but they are not commonly studied or tested broadly for example magder and hughes 1997 discuss that when the degree of uncertainty of a diagnostic test in our case the labels is known this information can be incorporated into the training of lr models improving their performance we find these aspects very important for future research additionally we believe it would be beneficial to incorporate some sort of mechanism in al to obtain additional or revised labels from human experts as a way to gauge temporal inter and intra personal variability in expert opinion in this work and as is typical for al research we have quantified the uncertainty associated with individual samples as the uncertainty in the predicted label conditional the most up to date model while this has led to very convincing results it is very likely that al could be improved further by accounting for model uncertainty as well for example van daele et al 2015 take confidence intervals of the predicted probabilities james et al 2013 into account another strategy could consist of replacing the mean probability with a distribution which can be performed for example by implementing the delta and laplace s methods xu and long 2005 tierney and kadane 1986 note that these methods quantify the uncertainty given the currently available information in the data records available for training and the set of unlabelled data samples a further improvement in the number and or utility of the queried samples could be expected from quantification of the expected model output change cai et al 2016 in this case one simulates the effects of obtaining yet unknown labels for an input thus trying to evaluate the uncertainty in potential future models not the current one as a way to select the most informative samples for labelling this is similar in philosophy to the anticipatory experimental design methods developed by donckels et al 2012 and schwaab et al 2008 for the purpose of mechanistic model identification in our opinion al strategies being a special kind of experimental design could be improved further by borrowing from these and other experimental design methods in the context of mechanistic modelling finally as mentioned earlier the choice of number of queried samples per iteration needs further study in greater detail as most of the current practice is based on empirical evidence smailović et al 2014 ramirez loaiza et al 2017 zhu and hovy 2007 another important consideration for future works is the integration on al with models that incorporate temporal dynamics explicitly this would include recurrent neural networks or long short term memory networks which exist in the family of the deep learning models malhotra et al 2015 but also more conventional linear models like multivariate arima tsitsika et al 2007 peter and silvia 2012 al approaches for these models have yet to be well studied 4 3 criticism and practical concerns for implementation of active learning strategies in this work we have used k fold cross validation on the training data set for hyperparameter tuning on the employed ml classifiers note that for experimental purposes our hyperparameter selection was performed based on the models performance on the entire training data set we acknowledge that in real time applications this could not be implemented beforehand as hyperparameter selection is too time intensive to be executed at each iteration of the al algorithm the grid search times for each model are shown in the supporting information therefore even if our results suggest that ann models could be more suitable for applying al for ad in environmental applications we believe there some aspects of ann models need to be taken into considerations for example we envision that ann models would be harder to deploy in real time applications of al because they are more sensitive to hyperparameter tuning which cannot be performed at the start of an al procedure due to the lack of data furthermore performing grid search on the wide anns hyperparameter space at each al iteration is likely too expensive computationally speaking for practical applications however if running a hyperparameter optimization becomes necessary in real time one could also think about choosing a different method such as bayesian optimization bergstra et al 2013 simplex search method nelder and mead 1965 random search bergstra and bengio 2012 or direct search hooke and jeeves 1961 additionally the training speed of ann can be decreased by increasing the learning rate with the risk of degrading the learning as the model could only arrive to a sub optimal final set of weights training speed remains a significant drawback that should be taken into account when implementing anns for ad using al and should be weighted against the considerable advantages that ann models bring because of their flexibility especially when dealing with complex data something else to take into account is that it is advisable to always have the model s predictions compared to a test labelled data set the reason behind this is twofold first it allows to monitor the performance s improvement during the al iterations exactly as it was done in this study second specifically for environmental applications where data can change over time it might be beneficial to monitor the model s performance over a long run however that also implies that the test data set should be frequently updated with more recent data alternatively it could be advisable to schedule models to retrain at specific times of the year finally although we have studied the performance of al for conductivity data collected in an ecological experiment which might present some of the broad characteristics of environmental data further works are needed to generalise the current results to a wider range of environmental monitoring applications 5 concluding remarks the results of this work have indicated that for anomaly detection applications in environmental monitoring i active learning could make anomaly detection feasible as it reduces the burden of data labelling by human experts regardless of model choice ii flexible model structures like knn ann and rf are recommended for anomaly detection in complex environmental sets as opposed to rigid model structures such as nb and lr because they lead to higher accuracy as measured by the f1 score iii the class prediction of flexible model structures tends to be more uncertain for anomalous data samples as a result active learning strategies tend to select anomalous data samples more than normal samples which in our experiments has resulted in considerable benefits for model identification copyright notice this manuscript has been authored in part by ut battelle llc under contract de ac05 00or22725 with the us department of energy doe the us government retains and the publisher by accepting the article for publication acknowledges that the us government retains a nonexclusive paid up irrevocable worldwide license to publish or reproduce the published form of this manuscript or allow others to do so for us government purposes doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan http energy gov downloads doepublic access plan declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank anita narwani and piet spaak for their contributions to the work presented in this paper the study has been made possible by the eawag discretionary funds grant number 5221 00492 012 02 project df2018 adasen appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104869 
25953,due to the growing amount of data from in situ sensors in environmental monitoring it becomes necessary to automatically detect anomalous data points nowadays this is mainly performed using supervised machine learning models which need a fully labelled data set for their training process however the process of labelling data is typically cumbersome and as a result a hindrance to the adoption of machine learning methods for automated anomaly detection in this work we propose to address this challenge by means of active learning this method consists of querying the domain expert for the labels of only a selected subset of the full data set we show that this reduces the time and costs associated to labelling while delivering the same or similar anomaly detection performances finally we also show that machine learning models providing a nonlinear classification boundary are to be recommended for anomaly detection in complex environmental data sets keywords active learning anomaly detection machine learning environmental monitoring 1 introduction due to the intensified deployment of in situ sensors for environmental monitoring experts in environmental science and monitoring are inundated with ever increasing quantities of high resolution data whose quality is not guaranteed horsburgh et al 2010 rieger and vanrolleghem 2008 extensive research has therefore been focused on the design of automated anomaly detection ad systems with the aim of automatically identifying unusual patterns in data aggarwal 2015 hill and minsker 2010 alferes and vanrolleghem 2016 aguado and rosen 2008 compared to manual ad it is surmised automatic ad systems can accelerate the otherwise laborious task of visually identifying any outlying data samples in complex data sets machine learning ml is a branch of artificial intelligence that focuses on learning from data murphy 2012 domingos 2012 ml models provide a valuable set of tools in science and engineering thanks to their ability of extracting meaningful information from data and can be used for automatic ad aggarwal 2015 active learning al aggarwal 2015 mussmann and liang 2018 is a special case of ml and has been proposed as a way to enable use of supervised learning models while also minimising the burdens associated with human expert labelling in al the domain expert is queried for the labels of a subset of the available data samples this is an iterative approach which selects a number of samples that when paired with the expert label are expected to improve the supervised learning model the most at each iteration the supervised ml model is trained with the data set consisting of labelled data samples from here on data records to distinguish them from unlabelled data samples this procedure is usually repeated until the model reaches satisfactory performance in the literature al has been successfully employed in several applications in the al challenge guyon et al 2011a b several data sets including handwriting and speech recognition document classification and protein engineering have been used and tested by different research groups to study the performance of al in the results achieved among the almost 300 participants the teams showed that al could achieve good performance in terms of classification accuracy with less labelled data however al was also showing lower performance at the beginning of the learning curve while it was faster and smoother in reaching high levels of accuracy in the second stage when a higher number of labelled examples was available in pimentel et al 2018 an al approach for unsupervised anomaly detection was presented and tested on synthetic and real data sets comprising images medical data for thyroid and arrhythmia issues identification in romero et al 2018 al was successfully used for handwritten text recognition pelleg and moore 2005 identify anomalies of special interest by using an al model in the presence of noisy data the method was tested over a number of distinct fields at different scales as engineer space shuttle data abalone biological data and astronomical data from uci university of california irvine data repository 1 1 https archive ics uci edu ml index php in an intrusion detection application the labelling time was reduced from two weeks to 1 h which was as high as 99 almgren and jonsson 2004 however quantifying the typical labelling times is generally hard as this depends on the nature of the input data e g images or multivariate timeseries the expertise of the domain expert and the design of the user interface while the studies cited above show promise only few of them have explored ad applications where al may be more challenging as the whole data set is highly unbalanced with infrequent anomalies present in only 0 1 10 of the total data moreover no work has been published so far to study the performance of al in environmental ad applications while there are no studies that provide empirical evidence for the differences between environmental and other types of data most of them argue in broad terms regarding its challenges not encountered elsewhere such as a seasonality at yearly monthly weekly and daily scales b non stationarity and non ergodicity c nonlinear system dynamics d a variety of systematic and incipient sensor faults such as sensor drift cherkassky et al 2006 hill and minsker 2010 eggimann et al 2017 leigh et al 2019 this makes the application of ad in an environmental setting more challenging as anomalies might not only be few but also different from each other therefore the main purpose of this paper is to evaluate whether al can successfully be used for ad in environmental monitoring to evaluate the utility of al we compare it with conventional supervised learning and random sampling strategy using 5 distinct ml model types the following models were evaluated based on their off the shelf availability in many data science platforms and ease of use random forest k nearest neighbours classifier logistic regression naive bayes and artificial neural networks al is implemented with the uncertainty sampling strategy settles 2010 we discuss how al can reduce the time spent by the domain expert to label a data set and how the ml model chosen as a base for the al strategy can influence the effectiveness of the algorithm 2 methods the first part of this section provides the reader with the necessary information on supervised ml for ad applications next we discuss the most important aspects of the tested ml models after that our implementation of al and the chosen sampling strategies are discussed we then explain the performance metrics used to evaluate our experimental results the second part of the section describes the case study and the data used for this work finally we provide a detailed explanation of our experimental procedure and we explain how the methods previously described are implemented in the generation and evaluation of the results 2 1 supervised learning supervised ad requires a training data set d x i y i i 1 n consisting of n data records where x i x d is a data sample represented as a d dimensional feature vector and y i y is the provided label for it we refer to the elements of x i as features in ad applications y i can only acquire two label values that is y 0 1 in ad applications the normal data is generally named as negative class with label value 0 and the anomalies are named positive class with label value 1 a class denotes a set of data having common characteristics during training of the model φ the task is to learn a function f x y to predict the most likely label given a new unseen data sample after training the model φ is able to compute probabilities p φ y 0 x and p φ y 1 x between 0 and 1 for a new test data sample x where p φ y 0 x p φ y 1 x is the predicted chance that a human expert would label the sample x as normal anomalous the classifier model then predicts the most likely label y ˆ f x argmax y p φ y y x different classification ml models can be used for supervised ad applications in this work we have deliberately focused on on the shelf softwares and chosen to test 5 ml models from a wide range as they are popular in the ml community and available in every commonly used data science platform e g scikit learn matlab ml toolbox spark mllib weka each model can be configured with different hyperparameters hyperparameters are defined as those parameters that determine the detailed structure and flexibility of the models these do not change during model training as they cannot be learned by the model and are fixed a priori based on prior knowledge experience and or exploratory analysis of the training data in what follows we discuss the most important aspects of the chosen models for a deeper dive into these models we refer to murphy 2012 bishop 2006 and james et al 2013 a naive bayes nb classifier rish et al 2001 models the distribution of individual classes using bayes theorem and predicts the class probabilities for each class in this work we use gaussian nb unlike the other models used in this study the nb model models the joint distribution of both data samples and labels specifically by assuming a that the features are completely independent of each other and b that the data samples in each class follow a multivariate normal distribution a particular advantage of the nb model is that there are no hyperparameters to be determined before training logistic regression lr ng and jordan 2002 is the classification equivalent to linear regression to this end the predicted probability consists of a linear combination of the input values subsequently transformed non linearly with the log sigmoid function to solve lr s optimization function different types of solvers are available here we report the ones that are implemented as libraries in many data science platforms the newton cg solver böhning 1992 the solver liblinear fan et al 2008 which uses a coordinate descent algorithm and the sag solver schmidt et al 2017 the regularisation type the amount of penalty and solver are all hyperparameters of the lr model bishop 2006 k nearest neighbours knn liao and vemuri 2002 is a non parametric model which classifies a new data sample by a plurality vote amongst its k closest neighbours in knn the predicted class probabilities are computed as the frequency of data records in the set of selected neighbours that belong to the considered class to determine what the k closest neighbours to a new data sample are a distance measure is used popular distance measures are euclidean hamming or minkowski in the simplest case all neighbours are given equal weight however one can also discount points that are further away by giving closer neighbours more weight the main hyperparameters are a the number of neighbours k b the chosen distance measure and c the weighting method a random forest rf quinlan 1987 is an ensemble approach based on decision trees decision trees are used repeatedly to split the input data in a top down approach with the intent to separate samples with different labels from each other the most important hyperparameters for this classifier are a the number of trees in the forest b the optimization function used to measure the quality of a split e g gini impurity or entropy information gain and c the maximum depth of the trees the predicted class probability is the fraction of decision trees in the ensemble that predicts the considered class artificial neural network ann classifiers dreiseitl and ohno machado 2002 transform the input data into the predicted class probabilities by means of a complex network of simple yet non linear unit operations neurons each neuron has its own set of parameters while anns are essentially nonlinear regression models they are extremely flexible due to the ability to string an almost arbitrary number of neurons together specifically by using large numbers of layers between data samples and predicted labels hidden layers each including many neurons the main hyperparameters for ann classifiers are a the number of hidden layers b the number of neurons in each hidden layer c the type of nonlinearity in each neuron d the solver algorithm used for calibrate the parameters of each neuron bishop 1995 and e the learning rate for weight updates note that the main impact of the learning rate is on convergence speed while a large learning rate allows the model to learn faster this is at the cost of arriving on a sub optimal final set of weights local minimum small values for the learning rate instead can cause small weight changes and slow learning attoh okine 1999 zeiler 2012 in this work we have selected a small learning rate as for the solver recent research nur et al 2014 kingma and ba 2015 suggests that the chosen optimization algorithm leads to inductive bias which in turn can be interpreted as an implicit type of regularisation or prior 2 1 1 additional notes on the chosen models as mentioned above the nb model describes the joint density of both data samples and labels as a result this allows generation of artificial data records according to the calibrated model for this reason it is known as a generative model in contrast the lr knn rf and ann models do not describe the joint density only the distribution of the labels conditional to the samples these models are known as discriminative models generative models like nb often outperform discriminative models such as rfs and anns on smaller data sets because their generative assumptions prevent overfitting ng and jordan 2002 discriminative models on the other hand are generally expected to perform better when a the assumptions in the generative models are untrue and b large and representative data sets are available ann models in particular are well known for their flexibility and outperform other models especially in the large and representative data regime sarle 1994 nevertheless they need extensive hyperparameter tuning to correctly choose the right architecture bardenet et al 2013 additionally ann models are prone to overfitting to avoid this issue in this work we only consider small ann architectures low number of hidden layers and neurons in each hidden layer in contrast rfs are faster and easier to train as they require fewer hyperparameters to tune in addition compared to ann classifiers rfs are less prone to overfitting and can learn from smaller data sizes liu et al 2013 lr is a simple discriminative classifier but as nb fits the data with low flexibility compared to other methods this is also because they both present a linear decision boundary between the classes finally knn is also easy to implement in terms of hyperparameter tuning and training time and as it is an instance based learning classifier it can immediately adapt as new training data comes in however it is also known to be sensitive to noisy data and might not perform well on unbalanced data unless the classes are well separated cho et al 1991 note that we did not consider other well known models such as fuzzy models or support vector machines svms as for fuzzy classification models unlike statistical classification models they require the assumptions that a single data point can simultaneously belong to multiple classes through use of fuzzy memberships in this work we assumed that a data point can belong to one class only thus leading to the use of models that predict a probability not a fuzzy membership as for svms we avoided this model structure as the large size of the kernel matrix prevented an efficient execution of our experiments 2 1 2 data pre processing data preprocessing steps such as centring and scaling are a common practice of data preparation for ml for two main reasons kotsiantis et al 2006 first these operations can improve the chances of convergence to optimal parameters during model training and the rate of convergence to the final parameters in turn improving the efficiency of the applied training algorithms second for models based on distance measures like knn centring and scaling can affect the modelled relationship in this work we have standardised all samples by centring to zero mean and scaling to unit variance as is common for classification purposes to this end the mean m and standard deviation s were always computed for each feature separately and only on the basis of the data records available for training 2 2 labelling strategies in this study we have applied three methods for labelling a complete labelling b random sampling and c al with uncertainty sampling complete labelling conceptually speaking complete labelling is the most simple method it assumes that a human expert has manually labelled all available data samples and corresponds to conventional use of supervised learning models labelling based on random sampling rnd this incremental learning method is initiated by querying the human expert for labels for a small set of data samples this leads to the production of an initial set of data records d 0 which is used to train the initial supervised model φ 0 the pool of unlabelled samples is given as t t i i 1 m where m is the data pool size a small number of samples are randomly selected from this pool the domain expert is then asked once more to provide a label y for the selected samples following this the new augmented set of data records d 1 x i y i i 1 n n is used to re train the model obtaining φ 1 the process is repeated for nit number of iterations until all samples have been labelled or the model reaches satisfactory performance this workflow is shown in fig 1 note that other stop criteria can be used e g the data samples for the query can only be obtained at a cost generally the labelling costs and the procedure is repeated until the labelling budget is reached settles 2011 to select the samples in this work we have employed a pseudo random number generator as a python module in our code and used the generated numbers to select the indices of the data points to sample in such a way that each sample in the pool is equally likely to be drawn labelling based on active learning using an uncertainty criterion unc al is also an incremental learning method that differs from rnd sampling in how the samples are selected from the unlabelled data pool after obtaining an initial model φ 0 as with random sampling the model is tested on the pool of unlabelled samples t the predicted class probabilities p φ y y x are then used to select those samples that are expected to improve the model prediction performance the most following labelling inclusion in the augmented labelled data set and model re training as with random sampling the domain expert is then asked to provide a label y for the selected samples the operating assumption underlying al is that predicted class probabilities carry information about the utility of a label for model training before this label is actually available i e expected utility al comes in many variants reflecting the circumstances under which data are produced and how soon after data collection one can expect a domain expert to provide labels for instance there are al methods that query samples by selecting them from on line signals while being collected simultaneously angluin 1988 lewis and gale 1994 atlas et al 1990 in this work we adopt pool based al lewis and gale 1994 where the complete set of unlabelled data samples x i is already available before querying starts this is the most common case in many ad research works meng et al 2013 almgren and jonsson 2004 al methods can also differ in how the utility of a yet unseen label is estimated settles 2010 in this work we employ uncertainty sampling unc a selection strategy where the al model selects the input x i for which the model s predicted label is most uncertain 1 i argmin i max y p φ y y x i where max y p φ y y x i is the maximal class probability given x i our implementation of al is described in algorithm 1 note that another sampling method is based on selecting the samples with maximum entropy holub et al 2008 instead of label uncertainty this however produces the same selection in binary classification problems and is not tested in this work another possibility includes using membership query sampling in which the most informative samples are selected based on classification disagreement between different ml models trained on the same data set as this involves training multiple models at the same time the computational costs are too high and unfeasible for the goal of this study algorithm 1 supervised anomaly detection with active learning 2 3 performance evaluation in this work we are dealing with an unbalanced data set therefore intuitive detection accuracy metrics e g ratio of correctly identified data samples vs total number of data samples are not recommended as performance metrics see e g géron 2019 we will refer as true positives tp the number of correctly identified anomalies y 1 y ˆ 1 true negatives t n is the number of correctly classified normal data y 0 y ˆ 0 finally false negatives fn and false positives fp are respectively the number of incorrectly classified normal data y 0 y ˆ 1 and anomalies y 1 y ˆ 0 accordingly to the above definitions the following measures can be computed 2 p r e c i s i o n t p t p f p r e c a l l t p t p f n while recall expresses the ability to find all relevant instances in a dataset what proportion of actual anomalies was identified correctly precision expresses the proportion of the data points the model classifies as anomalies were real e g a model that produces no false positives has a precision 1 generally there is an inverse relationship between these measures as precision increases recall decreases this is called the precision recall tradeoff for this reason a popular score used to measure model performance for unbalanced classification problems is the f 1 score this is computed as the geometric mean of precision and recall 3 f 1 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l note that we have intentionally avoided testing for multiple criteria to keep the evaluation simple however other performance measures exist and can be also used one example is the area under the receiver operating characteristic auc roc which provides an aggregate measure of performance across all possible model classification thresholds finally to critically evaluate the results and understand the behaviour of the models we will also investigate the type of data that is queried during the iterations from unc and rnd sampling strategies 2 4 case study data set the data set used for this study is a multivariate time series data of high spatial and temporal resolution that was collected as part of a long term ecological experiment described in narwani et al 2019 the primary goal of the experiment was to quantify the resilience of aquatic ecosystems in the face of eutrophication using replicated experimental pond ecosystems hereafter ponds 16 of such 15 000 l fiberglass ponds fig 2 a were set up at eawag dübendorf switzerland with a layer of gravel and a mix of tap and lake water successively macrophytes myriophyllum spicatum and dreissena mussels dreissena polymorpha were added to the ponds and inorganic nutrients were increased progressively as part of a fully factorial design four randomly chosen ponds received either no species one of the species or both species together in this work we focused on data from four ponds added with macrophytes as these data were the ones with the most anomalies each of these ponds was equipped with a multi variable instruments exo2 sonde from xylem inc 2 2 https www ysi com exo2 each of the 16 instruments contained sensors for eight variables conductivity chlorophyll and phycocyanin fluorescence dissolved organic matter fluorescence dissolved oxygen saturation and concentration ph and temperature fig 2b measurements of these eight water parameters were recorded simultaneously in each multi sensor with a fixed time interval of 15 min as for the calibration protocol for the instruments before placing them in the mesocosms a 48 h cross comparison trial was performed where the water parameters were measured using all 16 instruments inside a single tank with this data initial off factory differences between the instruments were corrected then the same cross comparison and a calibration were repeated for two maintenance periods during the experiment data labelling the domain expert manually labelled anomalies for specific conductivity the labelling was conducted through visual inspection of interactive high resolution charts that were created in r using ggplot2 wickham 2016 plotly sievert 2018 and htmlwidgets vaidyanathan et al 2018 these interactive graphs were saved in html and opened in a standard internet browser and screened by the domain expert for anomalies all measured water parameters were plotted in full temporal resolution within a single chart window 2 weeks at a time this way the domain expert could compare all data streams for a given period and decide whether a segment would qualify as anomalous after screening the data from the measured variables the domain expert supplied us with the labels for specific conductivity the above process resulted in a labelled data set containing n 22464 data records with d 8 dimensions the measured water quality variables the data set covers a period of 234 consecutive days and includes 2 anomalies this was then used to train the chosen ml models which involves learning determining the values for all the model s parameters from the provided labelled data records details on the training procedure are described next 2 5 experimental procedure the complete experiment performed in this work consisted of the following steps 1 data set splitting the full data set was split into 80 training data d and 20 test data in a stratified manner domingos 2012 2 hyperparameter tuning hyperparameter tuning was based on supervised learning with complete labelling as described above during this step optimal hyperparameters were selected by means of grid search to this end model performance were measured by stratified k fold cross validation of the f1 score where k is the number of folds in our case k 10 refaeilzadeh et al 2009 this was done by using the training data set only the implemented hyperparameters for the chosen ml models are reported in table 1 and the grid search results for each model are shown and discussed in detail in the supporting information note that we have included the regularisation hyperparameter c for lr it in the grid search even though the under parametrised regime we are working in does not require this bishop 2006 the results confirm that lr without regularisation is to be preferred in our case this is further discussed in the supporting information 3 best performance after selection of the optimal hyperparameters each model was trained with the complete training set and using complete labelling the f1 score was computed with the test data set to obtain the best performance for each model 4 incremental learning each model was then tested in combination with the incremental learning methods using the unc and rnd sampling strategies an initial subset of the training data d 0 containing 182 data records 0 25 of the training data was used for both learning methods and all 5 models each model was then updated in an iterative manner through one of the incremental learning methods as described earlier to this end n 10 samples were selected for querying at every iteration this choice is in line with current practice smailović et al 2014 ramirez loaiza et al 2017 zhu and hovy 2007 for example in zhu et al 2008 the selection of n 10 samples with uncertainty sampling showed efficient results as well as in brinker 2003 where the authors proved that smaller batch sizes e g 8 16 have higher learning efficiency at each iteration the f 1 score over the test data was used to measure model performance the incremental learning strategies were continued until all data samples had been labelled considering that the initial set of data records d 0 can influence the benefit of incremental learning relative to complete labelling as well as the benefit of al to learning with rnd sampling we repeated the execution of each incremental learning method r 10 times the initial sets of data records were sampled in a stratified manner and without repetition so that the fraction of anomalies in the initial sets of data records would match the fraction in the complete training data set 2 and that no data record was used more than once for initialisation in total incremental learning was applied 100 times 5 models 2 incremental learning methods 10 repetitions 5 to evaluate the benefits of incremental learning and al in particular the f1 scores on the test data set are reported as a function of the model type the learning method and repetitions in addition the following summary statistics were computed best f 1 the f 1 score on the test data set obtained with the complete labelling strategy for all 5 models f 1 i n i t the mean f 1 score on the test data set after initialisation of incremental learning across the r 10 repetitions this is reported for all 5 models and both incremental learning methods s t d i n i t is the standard deviation of the f 1 score after initialisation of incremental learning across the r 10 repetitions this is reported for all 5 models and both incremental learning methods f 1 95 95 of the best f 1 score and is used to compute the amount of labelled data needed to reach this value this is reported as labels f 1 95 note that we report the lowest f 1 score between the r 10 repetitions s t d 95 the standard deviation of f1 score between the different data folds when 95 of f 1 score is reached qanom the number of selected anomalies at f 1 95 tot q the total number of queries at f 1 95 3 results we discuss the results obtained with complete labelling first this is followed by a detailed discussion of the results obtained with incremental learning the code developed for this study and the labelled ecological data needed to reproduce the results reported in this work have been made publicly available at https doi org 10 25678 00023y 3 1 best performance with complete labelling the selected ml models were first trained with the full training data set and then tested on the test data the resulting best f 1 scores were computed following equation 3 on the test set and are reported in table 2 it is evident that rf knn and ann models produce a superior ad performance compared to lr and nb on the full data set these results might be explained by the reduced flexibility of lr and nb models we discuss this further in section 4 1 0 1 3 2 random and uncertainty sampling evolution of model performance during incremental learning fig 3 shows the average f 1 score over 10 repetitions as a function of the queried number of samples with the rf model and with both unc and rnd sampling strategies qualitatively speaking it can be seen that the rf model using unc sampling has reached the best f1 score already after few al iterations while the same model needed a greater number of iterations to reach the same value using the rnd sampling strategy expanding these results to the other models we plot in fig 4 the amount of training data needed to reach f 1 95 for all models and both incremental learning strategies as rf knn and ann models produced a high performance with complete labelling their f 1 95 was higher than lr and nb models additionally it is worth noticing that these models have reached their corresponding f 1 95 with considerable fewer iterations using unc sampling than using rnd sampling strategies on the other hand lr and nb models while presenting lower f 1 95 also did not show significantly difference between the two sampling strategies these results are reported in table 2 and further discussed in section 4 1 0 2 going back to fig 3 we have plotted the standard deviation of the f1 score across the 10 repetitions which is shown as a grey area obviously at 100 of training data the learning curve for both strategies converged to the same value because we have used the same training set in all cases additionally the initial value of standard deviation was high meaning that the variability in model performance induced by random initialisation disappeared as more training data were selected this is true for both incremental learning strategies but appears to occur faster with active learning unc the above results are quantitatively reported in the form of summary statistic in table 2 for all models and both incremental learning strategies queried samples as discussed in section 2 3 unc sampling selects data samples for which the model is the least confident about its prediction for rnd sampling these data are randomly selected for this reason it is interesting to understand what kind of samples are selected for querying by the human expert fig 5 shows the type of samples that have been queried with unc and rnd sampling for the rf model in the first 50 iterations corresponding to 0 94 of the training data for a specific repetition repetition 1 out of 10 here red squares represent anomalies and green squares normal data the horizontal axis represents the iteration number and the vertical axis are the n 10 queries for each iteration it is easy to see that the unc sampling strategy has selected a considerable amount of anomalies based on this result we speculate that due to the severe class unbalance the model was more uncertain about examples of anomalies which were the least seen during training the initial model φ 0 was trained on only 182 data records of which 2 are anomalies for the rnd strategy however where the data samples were selected randomly the data samples that were queried the most were mainly from the normal class due to their dominant presence in the data set fig 6 shows the same kind of information in a different format the accumulated number of queried samples that are anomalous are shown as a function of the total number of queried samples both as a fraction of the number of samples in the training data it is visible that the rnd sampling starting from the first iteration selected fewer anomalies than the unc sampling strategy the curves shapes follow a similar trend to the curves in fig 3 which might indicate that the selection of anomalies is decisive for improving the models performance the results from all the models using unc and rnd sampling strategies are shown in table 2 where we report the number of selected anomalies qanom f 1 95 and the total number of queries tot q f 1 95 4 discussion our results demonstrate that for ad applications in environmental monitoring the labelling efforts could be greatly reduced by using an al strategy specifically unc sampling we discuss below the main findings of this work and its consequences for sensor management in the environmental sector we then conclude with our practical recommendations for the implementation of al strategies 4 1 summary of results best f1 score the experimental results for our case study showed that in terms of best f1 score rf knn and ann models result in a considerably higher performance compared to lr and nb models this result might be explained by the reduced flexibility of lr and nb models which present a linear decision boundary between the 2 classes this jeopardises their learning process preventing them to clearly separate the anomalous from the normal class although further tests are needed to prove this hypothesis we believe that this is the most important cause for the reduced performance of nb and lr since we do not apply regularisation in lr this can be excluded as a factor for its reduced performance in addition specifically for nb the above results may be explained by the nature of the input data in environmental applications normal data might not be generated from the same distribution as it presents baseline changes due to different seasons and anomalies may be generated by different events which do not show the same pattern we suspect that nb model failed at correctly classifying such data because due to its generative properties it makes assumptions on the distribution of the data lastly the lr model presented better performance than nb for best f 1 and for starting f 1 f 1 i n i t a reason behind this behaviour is that lr is still a discriminative model this outcome follows the results presented in ng and jordan 2002 where it is shown that as the number of data records available for training is increased lr overtakes the performance of nb because of its discriminative behaviour this was also articulated by vapnik 1999 one should solve the classification problem directly using discriminative models thus modelling p y x and never solve a more general problem as an intermediate step thus modelling p x y and p y whether this holds also for more flexible and nonlinear model structures of which only discriminative variants ann knn rf were used in this study remains to be studied random and uncertainty sampling nb and lr models however even if their best f1 score was 0 1 0 2 points lower than the other models were able to converge to 95 of their best f 1 only with 0 33 1 08 of rnd sampling iterations this means that there is an important trade off to be made between two competing and important objectives model performance and labelling cost additionally as for the rnd sampling type the models were initially mainly trained with normal data because it has the highest probability of being selected this suggests that nb and lr models might not need many examples of anomalies during training to learn the best available decision function between the two classes as expected the unc sampling strategy is much more effective than the rnd sampling strategy it offers better or equal classification performance regardless of the number of samples that have been queried for these cases in fact our results show that by applying al with unc strategy it is possible to reach a high model performance in just a few iterations in the best case the ann model only needed 0 48 of labelled data to reach 95 of the best f1 performance score note also that incremental learning with either rnd or unc sampling was always more effective than complete labelling and could potentially save time and costs associated with labelling large data sets in environmental applications anomalies in addition to being low in number can be caused by a large variety of disturbances so ml models cannot easily generalise from them we have shown in table 2 that all models with unc strategy favoured the selection of anomalous data samples for querying which suggests that ml models tend to be particularly uncertain about the predicted label for anomalous samples relative to normal samples in turn leading to the selection of anomalous samples with higher frequency this result corroborates that providing enough and representative data records for model training is one of the main bottlenecks of supervised ad 4 2 consequences for data and sensor management in the environmental sector and future research our results indicate that the amount of labelled data could be greatly reduced by using an incremental learning strategy this lifts one of the main barriers to the applications of ml techniques in the environmental sector which is the burden of labelling in our experiments during consecutive al iterations anomalous data samples are identified as the samples with maximal uncertainty this suggests that al is not only useful to reduce the burden associated with labelling for model training but may also help in identifying anomalous samples as they are added to a data set indeed one could conceive of alerting human operators of the sensor network not only when an input is classified as anomalous but also when the predicted class is uncertain a similar idea was developed by giudici et al 2020 where al has been used to identify the most informative scenarios in the optimization process for robust planning in decision making all while decreasing computational requirements the same result could also inspire use of models that only learn from normal data potentially one class models trained with normal data only could offer a more certain classification for anomalies in the test class one implementation of al for ad with one class models can be found in barnabé lortie et al 2015 note however that one class models still require an expert based separation between anomalous and normal data records in the data used for training thus not eliminating the need for expert based labelling this is contrary to frequent claims in the literature on one class models amer et al 2013 sabokrou et al 2018 another option could be to use a background or garbage class which can be used to account for the presence of anomalies that are so diverse and rare that their class cannot be learned effectively for more details on this and related concepts please see dhamija et al 2018 in doing so the model would only need to represent the normal class which is likely easier to describe mathematically compared to the anomaly class as for the variables used for model training our domain expert has supplied us with the labels for specific conductivity and we have trained our models based on this sole information however the other variables chlorophyll and phycocyanin fluorescence dissolved organic matter fluorescence may also present anomalies which could not be easily identified by our domain expert note that ph sensor signals are known to be subject to incipient and always present drift phenomena ohmura et al 2019 which we consider faults but not anomalies for this reason we considered removing the ph from the data set variables but this has not resulted in an improvement of the model performance while we acknowledge that our choice of using all variables for model was subjective compared to using the conductivity signal exclusively the tested models have resulted in an improved performance when provided with all the available variables nonetheless the effects of including these variables which were possibly contaminated with unlabelled anomalies on anomaly detection performance should be quantified in future studies e g by labelling all anomalies in all sensor signals or by implementing feature selection if dealing with high dimensional data sets mukherjee and sharma 2012 zargari and voorhis 2012 in this work we have treated the domain expert as an oracle i e providing perfect and time invariant labels however uncertainty in the labels provided by the domain expert exists russo et al 2019 villez and habermacher 2016 and may be due to fatigue learning curve of the human expert user interface etc methods to account for imperfect oracles exist donmez and carbonell 2008 du and ling 2010 but they are not commonly studied or tested broadly for example magder and hughes 1997 discuss that when the degree of uncertainty of a diagnostic test in our case the labels is known this information can be incorporated into the training of lr models improving their performance we find these aspects very important for future research additionally we believe it would be beneficial to incorporate some sort of mechanism in al to obtain additional or revised labels from human experts as a way to gauge temporal inter and intra personal variability in expert opinion in this work and as is typical for al research we have quantified the uncertainty associated with individual samples as the uncertainty in the predicted label conditional the most up to date model while this has led to very convincing results it is very likely that al could be improved further by accounting for model uncertainty as well for example van daele et al 2015 take confidence intervals of the predicted probabilities james et al 2013 into account another strategy could consist of replacing the mean probability with a distribution which can be performed for example by implementing the delta and laplace s methods xu and long 2005 tierney and kadane 1986 note that these methods quantify the uncertainty given the currently available information in the data records available for training and the set of unlabelled data samples a further improvement in the number and or utility of the queried samples could be expected from quantification of the expected model output change cai et al 2016 in this case one simulates the effects of obtaining yet unknown labels for an input thus trying to evaluate the uncertainty in potential future models not the current one as a way to select the most informative samples for labelling this is similar in philosophy to the anticipatory experimental design methods developed by donckels et al 2012 and schwaab et al 2008 for the purpose of mechanistic model identification in our opinion al strategies being a special kind of experimental design could be improved further by borrowing from these and other experimental design methods in the context of mechanistic modelling finally as mentioned earlier the choice of number of queried samples per iteration needs further study in greater detail as most of the current practice is based on empirical evidence smailović et al 2014 ramirez loaiza et al 2017 zhu and hovy 2007 another important consideration for future works is the integration on al with models that incorporate temporal dynamics explicitly this would include recurrent neural networks or long short term memory networks which exist in the family of the deep learning models malhotra et al 2015 but also more conventional linear models like multivariate arima tsitsika et al 2007 peter and silvia 2012 al approaches for these models have yet to be well studied 4 3 criticism and practical concerns for implementation of active learning strategies in this work we have used k fold cross validation on the training data set for hyperparameter tuning on the employed ml classifiers note that for experimental purposes our hyperparameter selection was performed based on the models performance on the entire training data set we acknowledge that in real time applications this could not be implemented beforehand as hyperparameter selection is too time intensive to be executed at each iteration of the al algorithm the grid search times for each model are shown in the supporting information therefore even if our results suggest that ann models could be more suitable for applying al for ad in environmental applications we believe there some aspects of ann models need to be taken into considerations for example we envision that ann models would be harder to deploy in real time applications of al because they are more sensitive to hyperparameter tuning which cannot be performed at the start of an al procedure due to the lack of data furthermore performing grid search on the wide anns hyperparameter space at each al iteration is likely too expensive computationally speaking for practical applications however if running a hyperparameter optimization becomes necessary in real time one could also think about choosing a different method such as bayesian optimization bergstra et al 2013 simplex search method nelder and mead 1965 random search bergstra and bengio 2012 or direct search hooke and jeeves 1961 additionally the training speed of ann can be decreased by increasing the learning rate with the risk of degrading the learning as the model could only arrive to a sub optimal final set of weights training speed remains a significant drawback that should be taken into account when implementing anns for ad using al and should be weighted against the considerable advantages that ann models bring because of their flexibility especially when dealing with complex data something else to take into account is that it is advisable to always have the model s predictions compared to a test labelled data set the reason behind this is twofold first it allows to monitor the performance s improvement during the al iterations exactly as it was done in this study second specifically for environmental applications where data can change over time it might be beneficial to monitor the model s performance over a long run however that also implies that the test data set should be frequently updated with more recent data alternatively it could be advisable to schedule models to retrain at specific times of the year finally although we have studied the performance of al for conductivity data collected in an ecological experiment which might present some of the broad characteristics of environmental data further works are needed to generalise the current results to a wider range of environmental monitoring applications 5 concluding remarks the results of this work have indicated that for anomaly detection applications in environmental monitoring i active learning could make anomaly detection feasible as it reduces the burden of data labelling by human experts regardless of model choice ii flexible model structures like knn ann and rf are recommended for anomaly detection in complex environmental sets as opposed to rigid model structures such as nb and lr because they lead to higher accuracy as measured by the f1 score iii the class prediction of flexible model structures tends to be more uncertain for anomalous data samples as a result active learning strategies tend to select anomalous data samples more than normal samples which in our experiments has resulted in considerable benefits for model identification copyright notice this manuscript has been authored in part by ut battelle llc under contract de ac05 00or22725 with the us department of energy doe the us government retains and the publisher by accepting the article for publication acknowledges that the us government retains a nonexclusive paid up irrevocable worldwide license to publish or reproduce the published form of this manuscript or allow others to do so for us government purposes doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan http energy gov downloads doepublic access plan declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank anita narwani and piet spaak for their contributions to the work presented in this paper the study has been made possible by the eawag discretionary funds grant number 5221 00492 012 02 project df2018 adasen appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104869 
25954,megafires are large wildfires that occur under extreme weather conditions and produce mixed burn severities across diverse environmental gradients assessing megafire effects requires data covering large spatiotemporal extents which are difficult to collect from field inventories remote sensing provides an alternative but is limited in revealing post fire recovery trajectories and the underlying processes that drive the recovery we developed a novel framework to spatially reconstruct the post fire time series of forest conditions after the 1987 black dragon fire of china by integrating a forest landscape model landis with remote sensing and inventory data we derived pre fire 1985 forest composition and the megafire perimeter and severity using remote sensing and inventory data we simulated the megafire and the post megafire forest recovery from 1985 to 2015 using the landis model we demonstrated that the framework was effective in reconstructing the post fire stand dynamics and that it is applicable to other types of disturbances keywords megafire reconstruction forest landscape model remote sensing field inventory 1 introduction forest fire is a primary disturbance in many forest ecosystems influencing succession dynamics and carbon storage lecomte et al 2006 bowman et al 2009 fires that burned large areas with high intensity areal extent 100 km2 often called megafires can cause abrupt changes to ecosystems and have distinctly different ecological effects from other fires bradstock 2008 keane et al 2008 stephens et al 2014 post fire recovery is an important variable for understanding fire effects on forest ecosystems which is mainly determined by burn severity and species regeneration strategies johnstone et al 2010 halofsky et al 2011 megafires often result in a heterogeneous mosaic of burn severities across a wide range of environmental conditions consequently the vegetational response can be complex seedlings regenerated after the fire vary strongly among areas with contrasting burn severities due to species specific differences in dispersal seed size shade tolerance and parent tree locations large seeded species e g pinus spp have higher regeneration rates under partial shade and thus have higher regeneration rates in areas with low or moderate severity burns while fecund light seeded broadleaf species e g betula spp are wind dispersed and are more likely to colonize in areas with high severity burns greene et al 2007 johnstone et al 2010 megafires can also create large high severity burn patches that could delay tree regeneration and prolong early seral conditions by limiting the reach of seed dispersal johnstone et al 2016 which may even trigger a shift from forest to shrub or grass dominated cover types due to seed limitation and climate induced regeneration failure collins and roller 2013 savage et al 2013 harvey et al 2016 even with similar burn severity and sufficient seed availability germination and establishment can be affected by tolerances to temperature and moisture that vary by species petrie et al 2016 davis et al 2018 and microsite conditions which can influence the success of tree establishment and regeneration with fewer tree seedlings found on harsh sites broncano and retana 2004 bonnet et al 2005 kemp et al 2019 the complex vegetation responses to megafires make assessments of post fire recovery challenging assessment of post fire forest recovery is traditionally completed with plot based field inventories this method can provide relatively accurate and detailed measurements of post fire plant communities which can be used to quantify burn severity and recovery based on the time the plots were surveyed after the fire e g johnstone et al 2004 turner et al 2016 however field based inventories generally cover small spatial extents and provide plot based information on burn severity and recovery but not about the size and shape of burned patches e g crotteau et al 2013 since megafires burn large areas across a range of environmental gradients and a mix of burn severities it is challenging to capture the heterogeneous burn severities and post fire recovery patterns using field based methods alone in addition forest inventories before and immediately after megafires and the subsequent monitoring of vegetation recovery are often lacking these limitations hinder field based approaches for assessing megafire effects and post fire recovery remote sensing is effective in capturing burn severity patterns and monitoring post fire vegetation recovery for megafires french et al 2008 gitas et al 2012 chu and guo 2014 remote sensing based vegetation indices such as the normalized burn ratio nbr garcía and caselles 1991 epting et al 2005 and its derivatives differenced nbr dnbr and relative differenced nbr rdnbr key and benson 2005 miller and thode 2007 have been widely used for detecting burn severity patterns eidenshink et al 2007 the normalized difference vegetation index ndvi enhanced vegetation index evi and soil adjusted vegetation index savi have been used for monitoring post fire recovery van leeuwen et al 2010 gitas et al 2012 veraverbeke et al 2012 however a great deal of uncertainty exists when using these vegetation indices to assess post fire recovery in terms of species composition and forest structure forest recovery assessments using vegetation indices can become complicated when different vegetation recovery states have similar vegetation index values glenn et al 2008 chu and guo 2014 for instance young e g two years post fire broadleaf forest pixels may exhibit the same ndvi value as the unburned coniferous forest pixels that are on a very different successional stage idris et al 2005 cuevas gonzalez et al 2009 cai et al 2018 the limited availability of cloud free satellite images during the growing season can also impede continuous assessment of post fire forest recovery ju and roy 2008 in addition remote sensing based vegetation indices are limited in their ability to monitor demographic processes such as seed dispersal tree establishment and mortality species competition and competition caused mortality self thinning which drive post fire forest recovery fire succession models have been used to understand the interactions between vegetation response to forest fire including post fire forest structure composition and diversity boychuk et al 1997 millington et al 2009 miller and ager 2013 however most of these models lack demographic processes to capture the species regeneration traits alternatively forest landscape models flms spatially simulate forest dynamics seed establishment growth competition and succession accounting for the processes not captured by remote sensing and fire succession models and have been effectively applied to spatially reconstructing historical post disturbance forest conditions he 2008 seidl et al 2014 thrippleton et al 2014 flms can also incorporate information from fire perimeters and the spatial patterns of burn severity derived from remote sensing as inputs wang et al 2009 they can track the location and abundance of parent trees and seedlings when simulating the demographic processes that drive post fire forest recovery wang et al 2013 2014a bib wang et al 2013 bib wang et al 2014a finally flms can be calibrated and validated with forest inventory data seidl et al 2012 wang et al 2014b luo et al 2015 the 1987 black dragon fire which occurred in the boreal forest of china stood out due to its size and severity the fire burned 1 3 104 km2 resulted in a high degree of tree mortality and reset forest succession for most burned stands it created opportunities to study post fire forest dynamics at an unprecedented scale in this study our objectives were to 1 present a novel framework that integrates an flm with field inventory and remote sensing data to spatially reconstruct the burn severity of the black dragon fire and the post fire time series of forest conditions i e forest composition structure and aboveground biomass and 2 evaluate whether the reconstructed forest conditions could realistically capture the post fire recovery e g density and basal area at the level of individual tree species under different burn severities spatiotemporal reconstruction of the post megafire forest condition provides a platform to investigate the recovery rate and trajectories through model simulations and thus improve realism and reduce uncertainties 2 data and methods 2 1 study area our study area is located in the great xing an mountains and encompasses approximately 8 46 104 km2 50 10 n 121 12 e to 53 33 n 127 00 e in northeast china fig 1 the area is hilly and mountainous altitudes ranging from 134 to 1511 m and falls within the continental cold temperate climate zone with long and severe winters but short summers the average annual temperature is 3 9 c with an average temperature of 33 c in the coldest month january and an average temperature of 17 5 c in the hottest month july the annual cumulative precipitation ranges from 400 to 500 mm more than 60 of the annual precipitation occurs in the summer season from june to august zhou 1991 xu 1998 vegetation in this region is representative of cool boreal coniferous forests that cover 83 of the study area the canopy species composition is relatively simple dahurian larch larix gmelini rupr kuzen hereafter larch a deciduous conifer and white birch betula platyphylla suk a deciduous broadleaved species are dominant covering more than 80 of the study area other tree species include the evergreen conifers korean spruce picea koriensis nakai hereafter spruce and scots pine pinus sylvestris var mongolica litvinov hereafter pine and the deciduous broadleaved species aspen populus davidiana dole and p suaveolens fischer willow chosenia arbutifolia pall a skv asian black birch betula davurica pall hereafter black birch and mongolian oak quercus mongolica fisch ex ledeb black birch and mongolian oak are mainly distributed in the southeastern low elevation part of the study area whereas pine is distributed in the northern part wildfire frequency and area burned in our study area are linked to human disturbances and annual variations in monsoonal strength liu et al 2012 low intensity surface fires mean return interval ca 30 yr were historically frequent occasionally mixed with infrequent stand replacing fires mean return interval ca 120 yr in the high elevation regions xu et al 1997 however long term fire exclusion and timber harvest have altered the fire regime where fires are infrequent but more intense chang et al 2007 in our study area fires burned 6 64 104 km2 from 1967 to 2005 chang et al 2008 liu et al 2012 one of the most noteworthy fires known as the black dragon fire ignited on may 6 1987 and burned four forest bureaus xilinji tuqiang amuer and tahe the black dragon fire resulted in over 200 deaths and 4 billion yuan of losses at that time causing the most forest fire damage in the history of china 2 2 general approach we first reconstructed forest stand conditions in our study area before the black dragon fire i e in 1985 using remote sensing and forest inventory data fig 2 since the remote sensing data could not provide detailed stand information at the pixel level across our entire landscape and pre fire field inventory data were not available we constructed the pre fire forest conditions following the k nearest neighbor knn stand imputation approach of zhang et al 2018a 2018b we used landsat data to delineate fire perimeters and derive burn severity classes associated with total tree mortalities xu et al 2020 with the pre fire landscape as a starting point we burned in the black dragon fire perimeter and severity in landis pro at the fire year 1987 and simulated the post megafire time series forest conditions using a forest landscape model landis pro fig 2 landis pro has been parameterized for numerous forest regions under different environmental settings huang et al 2018 wang et al 2019 however to ensure tree species parameters of the model e g growth rate available seeds maximum stand density index and maximum diameter accurately captured the tree species in our study area we used a data assimilation approach luo et al 2011 wang et al 2014b that iteratively calibrated the parameters by comparing the simulated results at years with the respective forest inventory data fig 2a the calibration process was followed by validation against field inventory data at a later stage 2015 to ensure that forest dynamics under no disturbance were correctly simulated fig 2a to ensure the response of tree species corresponded to various burn severities we also applied the data assimilation approach to calibrate fire parameters e g height of bark charring in landis pro to precisely constrain post fire tree species recovery processes fig 2b we used the year 2000 post fire inventory data for the calibration and used the year 2015 post fire inventory data for model validation since these were the only post fire inventory data available in the burned area fig 2b through the iterative calibration and validation processes we were able to derive the continuous time series forest conditions from which post fire forest landscape recovery could be analyzed spatially and temporally fig 2c 2 3 forest inventory data the forest inventory data used for model initialization calibration and results validation in this study were collected from the china forestry science data center cfsdc http www cfsdc org including forest plot inventory data from 2000 2010 and 2015 and a forest stand map in polygons with relatively complete attributes from the early 2000s fig s1 the plot inventory data includes 5752 unburned plots and 1305 burned plots following the black dragon fire each plot contained the number of trees and diameter at breast height dbh 5 cm class by species the forest stand map comprised 276 273 stands with homogeneous forest attributes e g dominant tree species stand age and site index in each stand the data contained stand area mean dbh stand height stand age stand volume tree species composition species percent volume forest origin natural regeneration vs afforestation and management and disturbances harvest plantation and forest fires information in each stand polygon 2 4 remote sensing data in this study 20 pre and post fire landsat tm thematic mapper images table s1 from the u s geological survey usgs http earthexplorer usgs gov were used to estimate pre fire forest composition and burn severity of the 1987 black dragon fire the images were processed by the usgs to convert from dn digital numbers to surface reflectance using the ledaps algorithm landsat ecosystem disturbance adaptive processing system masek et al 2006 clouds cloud shadows and snow pixels were masked using the function of mask algorithm fmask zhu and woodcock 2012 the 1980s images were processed by radiometric normalization based on the images from the 2000s using a histogram matching method to reduce radiometric differences among images caused by inconsistencies of acquisition conditions 2 5 pre fire forest conditions forest inventory data before the black dragon fire were lacking we combined 2000s forest inventory data with 1980s and 2000s landsat tm data to map 1980s aboveground forest biomass and tree lists i e lists of species and diameter for every tree following the approaches of zhang et al 2018a 2018b we used 2000s forest inventory data and 2000s landsat tm data as the training samples to fit a nonparametric random forest based knn model for biomass and knn and weibull parameter prediction models wppms for tree lists then we mapped species level biomass and tree lists before the black dragon fire at 30 m resolution using 1980s landsat tm data based on the developed biomass estimation model and tree lists estimation model thus the pre fire forest composition figs s2 and s3 represents the distribution and abundance of tree species before the 1987 black dragon fire our imputation results figs s2 and s3 conformed to previous studies and field observations the total tree density ranging from 400 to 2500 trees ha fig s2 and aboveground biomass 62 4 22 76 mg ha fig s3 were close to the values reported by zhai et al 1990 hu et al 2015 and fang et al 2001 for northeastern china moreover our estimates of the species distribution figs s2 and s3 were consistent with the environmental niches of tree species larch the representative siberian boreal tree species is distributed most widely since it can endure extremely cold winters and a short growing season and it can grow in both well drained and boggy sites due to its shallow roots xu 1998 kajimoto et al 2003 yang et al 2014 white birch is also distributed widely in the area but is less adaptable to shade and humid environments and typically has less biomass than larch xu 1998 aspen requires warmer temperatures and higher soil fertility and is negatively correlated with elevation xu 1998 scots pine has high tolerance of drought and low temperatures and consequently was mainly distributed on the sunny slopes and ridges of the northern part of our study area zhu et al 2006 mongolian oak and black birch were mainly distributed in the southeast of the area since they thrive in areas with higher temperatures xu 1998 spruce grows under cold environments and therefore was mainly mapped in areas of relatively high elevation of the northern area willow requires sufficient humidity to survive and is therefore widespread along rivers which are fed by water from the glaciers and snows of the high surrounding mountains these evaluations ensured the subsequent reconstructions of forest recovery rate and trajectories bear high realism temperli et al 2013 2 6 landscape model parameterization we used the landis pro forest landscape model to simulate forest landscape changes and reconstruct tree species recovery trajectories after the black dragon fire the model tracks the number density of each tree species by age cohort size class at the pixel level i e 100 m resolution in this study and simulates species stand and landscape scale processes over large spatial and temporal extents wang et al 2013 2014a bib wang et al 2013 bib wang et al 2014a we used the succession module in landis pro to simulate individual tree establishment growth resprouting and mortality at the species level resources competition self thinning and seedling establishment at stand level and seed dispersal at the landscape level establishment success is determined by species specific biological traits such as shade tolerance and suitability to establish under the other environmental conditions besides shade mortality is determined by longevity i e maximum lifespans competition and disturbances other landscape scale processes i e natural and anthropogenic disturbances were simulated by independent modules e g fire harvesting and fuel treatments are simulated using the fire harvest and fuel modules respectively forest change is determined by the interactions of species stand and landscape scale processes we modeled the eight most common tree species which accounted for approximately 95 of stand volume in this study region tree species life history attributes included longevity age of reproductive maturity shade tolerance fire tolerance seed dispersal distance maximum tree diameter maximum stand density index and number of potential germination seeds table 1 landis pro does not require climate and soil parameters however it requires species establishment probability sep and maximum growing space occupied mgso by land type which delineates heterogeneous landscapes into smaller but relatively homogeneous land type units within each land type unit resource availability represented by the mgso and sep is assumed to be homogeneous for this study sep and mgso were derived from an ecosystem process model linkages 3 0 dijak et al 2017 for each land type see supplement 2 7 black dragon fire and its implementation in landis pro the spatial pattern and variability of burn severity strongly influences vegetation response forest structure and post fire successional trajectories halofsky et al 2011 the burn severity map of the black dragon fire used in this study was extracted based on the remote sensing classification from a relationship between normalized burn ration nbr and composite burn index cbi xu et al 2020 the burn severity explicitly accounted for different levels of tree mortality that are important for post fire forest succession unburned no sign of fire effects nbr 585 low severity 252 nbr 585 moderate severity 53 nbr 252 and high severity nbr 53 the simulation of fire is treated as a stochastic process in the landis pro fire module however there was no guarantee that the black dragon fire would occur in 1987 in our simulation thus we mimicked this specific fire event and its effects using the landis pro fuel module he et al 2004 which can deterministically specify how live fuel loads are reduced corresponding to tree mortality detected for each burn severity class in fuel reduction treatments post fire live tree mortality was modeled using a logistic regression equation equation 1 where p is probability of mortality following fire β i are model coefficients determining fire tolerance x1 is tree diameter cm and x2 is height of bark charring m analogously for burn severity based on previous studies woolley et al 2012 fraser et al 2019 we divided our study area into four fuel management areas based on the four fire severity classes xu et al 2020 initial model coefficients for each species were defined based on the species fire tolerance fraser et al 2019 equation 1 p 1 e β 0 β 1 x 1 β 2 x 2 1 2 8 model calibration and results validation forest inventory data in unburned area were available for years 2000 2010 and 2015 the 2000 2010 data were used to calibrate tree species parameters while the 2015 data were used to validate the simulated results forest inventory data in burned areas were only available for years 2000 and 2015 the 2000 data were used to calibrate fire parameters and the 2015 data were used to validate the simulated fire effects fig 2a and b only simulated trees with dbh 5 cm and forest inventory data that were in the study areas with no evidence of disturbance e g logging insects disease and fire after 1987 were used in the calibration and validation processes model fit can be assessed using the difference between the simulated results and true values while overfitting can result in deterioration in prediction accuracy lever et al 2016 in this study to calibrate tree species parameters we iteratively adjusted the age dbh relationship and available seeds for each species by land types until the differences between simulated density and basal area by species and the forest inventory were not significant no differences based on a one way analysis of variance anova test p 0 05 at 2000 and 2010 to ensure that the these parameters realistically represented the actual forests in our study area we validated the simulated results at the landscape scale by stratifying the simulation results and forest inventory data into subecoregions based on the ecoregion classification of xu 1998 and soil types fig s4 because resource availability and species assemblages were relatively homogeneous within a subecoregion and heterogeneous among subecoregions in landis pro for undisturbed forests specifically we compared the simulated mean basal area and tree density of all cells to the observed mean values of all plots for each subecoregion in 2015 by using paired t test to evaluate the overall accuracy and the square of the pearson correlation r2 and root mean square deviation rmsd for each species rmsd was based on squared simulation errors and thus was sensitive to outliers to calibrate fire parameters we iteratively adjusted fuel model coefficients for each species fire tolerance class and height of bark charring for each burn severity until the comparison between simulated density and basal area by species and the forest inventory data passed the significance test no differences based on an anova test p 0 05 at 2000 we validated the simulation results for each burn severity class at site scale by extracting simulated results from raster cells corresponding to the forest inventory plot locations because heterogeneous post fire tree species recovery patterns overrode environmental heterogeneity delineated by subecoregion specifically we compared basal area and tree density within extracted raster cells with observed values in inventory plots for each burn severity class at 2015 by using two sample t test to evaluate the accuracy of simulated post fire recovery all statistical analyses were performed using r statistical software r core team 2015 2 9 post fire tree planting simulation large scale tree plantings in the years immediately after the fire were implemented in each forest bureau thus we simulated planting using the landis pro harvest module fraser et al 2013 the planting management units in this study were constructed based on the burn severity map of the 1987 black dragon fire forest bureau boundaries and harvest management units to capture the variation in planting practices across the region we parameterized the percent area and number of trees planted every two years for each management unit based on forest management records from the china national forest inventory third tier data http www cfsdc org and previous studies yang et al 1998 chen et al 2014 only coniferous species of larch and scots pine were planted in the high burn severity area 70 larch 30 pine with a regular plant spacing 1 5 m 1 5 m or 1 5 m 2 m according to field conditions chen et al 2014 by the end of the 1990s less than 10 of the burned forests were managed with planting yang et al 1998 3 results 3 1 results validation model simulations showed high agreement in the magnitude and time of observed basal area and density from unburned forests at the landscape scale paired t tests p 0 05 r2 0 8 is high and rmsd is low for both basal area and density for all species fig 3 the comparisons of different species demonstrated that dominant tree species larch and white birch had relatively higher accuracy than other species the simulation accuracy for tree density was higher than for basal area because the density in the model was largely determined by a single parameter available seeds while the basal area was affected by the species age dbh relationship that introduced additional uncertainties into estimation overall our results indicated that the simulated forest development was consistent with the actual forest dynamics and simulated density had higher accuracy than simulated basal area comparison between simulated data and observed data showed that post fire forest composition and structure closely represented the real forest composition and structure at different burn severity classes in 2015 for all species and severities two sample t test p 0 05 fig 4 our results indicated that the simulated post fire forest development captured current forest composition and structure after the black dragon fire and thus the simulated fire caused tree species mortality could be close to the real tree species mortality of the black dragon fire the observed and simulated density of conifer and broadleaf species in burned areas showed similar patterns in relation to distance to live tree edges trees mortality rate 90 by the black dragon fire fig 5 post fire conifer density showed a decline with increasing distance to the live tree edge and was almost absent in the interior of high burn severity patches in 2015 fig 5a post fire broadleaf density was high across the whole high severity area and had an opposite trend of density versus distance relationship compared with conifer species fig 5b the self thinning among competing trees led to a decrease in tree density near seed sources the evaluation results increased our confidence in the ability of the calibrated landis pro model to explore the long term fire effects and post fire forest recovery 3 2 the black dragon fire effects and post fire forest recovery trajectories tree mortality and post fire recovery varied greatly among burn severities and tree species fig 6 and s5 mortalities of conifer and broadleaf species were both positively related to burn severity and were very high in areas with moderate and high burn severity fig 6 a and b approximately 50 90 of conifer and almost 100 of broadleaf stems died in areas of moderate and high burn severity nevertheless recruitment of broadleaf trees was abundant in moderate and high severity burned areas the post fire density of broadleaf species gradually increased over the first 12 years then sharply peaked between 2005 and 2010 12 000 and 10 000 trees ha and finally decreased to 10 000 and 7000 trees ha by 2015 in high and moderate burn severity areas respectively the changes in density basal area and biomass over time in unburned and low severity areas were not significant fig 6b the post fire density of conifer species showed a low rate of increase compared to broadleaf species fig 6a the post fire basal areas of conifer and broadleaf species both showed increasing trends throughout the simulation years under all burn severities fig 6c and d for broadleaf species the basal area has recovered to and even exceeded pre fire levels although aboveground biomass always remained lower than pre fire levels fig 6c f for conifer species both basal area and biomass remained below pre fire levels but the recovery rate in low burn severity was fastest fig 6c and e with time the percentage of coniferous species decreased in high and moderate severity burned areas but no significant changes occurred in low severity and unburned areas fig 7 a the percent of coniferous species in high and moderate severity burned areas was far less than the value in unburned forests at 2015 but the edges of these burned patches showed a composition recovery sign with larger conifer percentages than the interior fig 7 4 discussion we presented a spatially explicit framework to reconstruct post megafire forest recovery through integrating a forest landscape model flm and remote sensing and field inventory data reconstruction results showed that burn severity affected the relative dominance of broadleaf vs conifer species in burned stands with probable effects on subsequent canopy dominance in high severity burned areas broadleaf species e g white birch rapidly emerged despite the large burn size while regeneration of coniferous species e g larch was minimal in the interior of the burned patches this pattern matches expectations because white birch can rapidly regenerate either by resprouting from stumps or roots that survived fires or by long distance seed dispersal e g 1000 m however the regeneration of larch depends on the seeds from surviving trees and a relatively short seed dispersal distance 400 m xu 1998 thirty years after the megafire the broadleaf species fully recovered and white birch stands went into the self thinning stage in the interior of the high severity burned area as the newly established trees matured however coniferous species were still in the initial stand development stage in contrast more conifers than broadleaf species regrew in the low severity burned areas where canopies provided suitable conditions for the relatively more shade tolerant conifers and where sufficient seeds from surviving trees had a higher chance to reach fire released areas in the low severity burned patches the comparison of pre and post fire tree composition in the burned patches indicated that self replacement succession was likely to occur in areas that burned with low severity whereas high severity burned areas are more likely to shift forest successional trajectories away from conifer self replacement to pathways with greater broadleaf dominance a finding that is reported in other post fire studies johnstone and chapin 2006 johnstone et al 2010 cai et al 2013 our reconstructed trajectories of post megafire forest recovery were well supported by known data and empirical knowledge of forest stand development after large scale disturbances oliver et al 1996 turner et al 1998 kurkowski et al 2008 suggesting that our model framework is effective in spatially reconstructing post megafire historical forest conditions validating simulation results from flms is critical in quantifying the reliability and credibility of landscape reconstructions our framework of reconstructing the spatiotemporal history of post megafire forest conditions provided not only the spatial pattern and dynamics at the landscape scale but also detailed stand attributes such as basal area tree density and age classes by species we validated the simulated stand attributes with the contemporary inventory data our simulated tree mortality rates were close to the estimates of tree mortality rates from field inventories within different burn severity classes of the black dragon fire luo 2002 the simulated post fire density and biomass were comparable to the field observations reported by wang et al 2001 wang et al 2003 and hu et al 2016 for the same fire and other post fire studies in boreal forests after similar recovery periods johnstone et al 2004 alexander et al 2012 cai et al 2013 our validation results from current forest inventory data demonstrated the calibrated model performed well and ensured that the model was a reasonable and reliable platform for subsequent applications to quantify the effects of megafires on forest composition and landscape succession a notable benefit of our framework is that once calibrated the model is highly scalable and can be applied to filling the gaps where inventory data are not available to assess recovery trajectories across the whole landscape this method offers an approach to augment traditional uses of forest inventory data in post fire studies our approach realistically captures the heterogeneity of post fire recovery process in both time and space figs 5 and 6 s5 in contrast forest inventories when applied alone have generally focused on fixed plots or time periods and on describing entire landscapes which limit their ability to constrain the impacts of heterogeneity on timber volumes and carbon stocks e g kashian et al 2005 furthermore inventory approaches can suffer from biased sampling design for example wang et al 2001 and hu et al 2016 conducted studies near the live tree edges due to the logistical limitations observed a higher biomass recovery than we did in this study and thus overestimated the post fire recovery status they assumed that the mature forests were representative of the forests in this region prior to the 1987 fire however through historical forest conditions reconstruction we found that pre fire forests in this area were younger than wang et al 2001 and hu et al 2016 assumed because forests in this region were affected by historical harvesting and fires which resulted in relatively young stands with most trees between 40 and 60 years old these studies may have overestimated pre fire forest biomass and consequently biomass loss due to the black dragon fire while our simulated biomass loss was closer to the estimated values from the traditional bottom up method xu et al 2020 this highlights the importance of our approach that reconstruction of the entire historical landscape reduced the biases from field sampling this framework can be applied to assess the legacy effects of a megafire over long time periods i e decades to centuries while previous empirical and field based studies documented legacy effects e g downing et al 2019 such effects can persist for decades to centuries seidl et al 2012 and thus require long term assessment our framework can be further applied to projecting how forest landscapes respond to future megafires which are expected to increase under warming climate and increased fuel accumulation from fire exclusion policies chang et al 2007 flannigan et al 2009 liu et al 2012 our framework can also be used to examine alternative management and disturbance scenarios managers could use the framework to evaluate forest resistance rate of recovery and the time to return to pre disturbance states after megafires under various management and climate scenarios as well as to study the effect of alternative managements on mitigating future megafire risk for example reforestation is increasingly used to assist forest restoration and improve resilience especially under warming climate as conifer forests will be increasingly regeneration limited with intensifying fire regimes hof et al 2017 north et al 2019 different reforestation strategies e g planting intensity and spatial assignment can be evaluated with flms wang et al 2006a 2006b bib wang et al 2006a bib wang et al 2006b researchers can also use this framework to evaluate the response of forest landscapes to other forest disturbances such as drought insect and harvest under different environmental settings declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the national biologic carbon sequestration assessment program under the u s geological survey climate and land use mission area any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government xu w he h s fraser j s hawbaker t j henne p d duan s and zhu z 2020 data release for spatially explicit reconstruction of post megafire forest recovery through landscape modeling u s geological survey data release https doi org 10 5066 p9hrhbxz appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104884 
25954,megafires are large wildfires that occur under extreme weather conditions and produce mixed burn severities across diverse environmental gradients assessing megafire effects requires data covering large spatiotemporal extents which are difficult to collect from field inventories remote sensing provides an alternative but is limited in revealing post fire recovery trajectories and the underlying processes that drive the recovery we developed a novel framework to spatially reconstruct the post fire time series of forest conditions after the 1987 black dragon fire of china by integrating a forest landscape model landis with remote sensing and inventory data we derived pre fire 1985 forest composition and the megafire perimeter and severity using remote sensing and inventory data we simulated the megafire and the post megafire forest recovery from 1985 to 2015 using the landis model we demonstrated that the framework was effective in reconstructing the post fire stand dynamics and that it is applicable to other types of disturbances keywords megafire reconstruction forest landscape model remote sensing field inventory 1 introduction forest fire is a primary disturbance in many forest ecosystems influencing succession dynamics and carbon storage lecomte et al 2006 bowman et al 2009 fires that burned large areas with high intensity areal extent 100 km2 often called megafires can cause abrupt changes to ecosystems and have distinctly different ecological effects from other fires bradstock 2008 keane et al 2008 stephens et al 2014 post fire recovery is an important variable for understanding fire effects on forest ecosystems which is mainly determined by burn severity and species regeneration strategies johnstone et al 2010 halofsky et al 2011 megafires often result in a heterogeneous mosaic of burn severities across a wide range of environmental conditions consequently the vegetational response can be complex seedlings regenerated after the fire vary strongly among areas with contrasting burn severities due to species specific differences in dispersal seed size shade tolerance and parent tree locations large seeded species e g pinus spp have higher regeneration rates under partial shade and thus have higher regeneration rates in areas with low or moderate severity burns while fecund light seeded broadleaf species e g betula spp are wind dispersed and are more likely to colonize in areas with high severity burns greene et al 2007 johnstone et al 2010 megafires can also create large high severity burn patches that could delay tree regeneration and prolong early seral conditions by limiting the reach of seed dispersal johnstone et al 2016 which may even trigger a shift from forest to shrub or grass dominated cover types due to seed limitation and climate induced regeneration failure collins and roller 2013 savage et al 2013 harvey et al 2016 even with similar burn severity and sufficient seed availability germination and establishment can be affected by tolerances to temperature and moisture that vary by species petrie et al 2016 davis et al 2018 and microsite conditions which can influence the success of tree establishment and regeneration with fewer tree seedlings found on harsh sites broncano and retana 2004 bonnet et al 2005 kemp et al 2019 the complex vegetation responses to megafires make assessments of post fire recovery challenging assessment of post fire forest recovery is traditionally completed with plot based field inventories this method can provide relatively accurate and detailed measurements of post fire plant communities which can be used to quantify burn severity and recovery based on the time the plots were surveyed after the fire e g johnstone et al 2004 turner et al 2016 however field based inventories generally cover small spatial extents and provide plot based information on burn severity and recovery but not about the size and shape of burned patches e g crotteau et al 2013 since megafires burn large areas across a range of environmental gradients and a mix of burn severities it is challenging to capture the heterogeneous burn severities and post fire recovery patterns using field based methods alone in addition forest inventories before and immediately after megafires and the subsequent monitoring of vegetation recovery are often lacking these limitations hinder field based approaches for assessing megafire effects and post fire recovery remote sensing is effective in capturing burn severity patterns and monitoring post fire vegetation recovery for megafires french et al 2008 gitas et al 2012 chu and guo 2014 remote sensing based vegetation indices such as the normalized burn ratio nbr garcía and caselles 1991 epting et al 2005 and its derivatives differenced nbr dnbr and relative differenced nbr rdnbr key and benson 2005 miller and thode 2007 have been widely used for detecting burn severity patterns eidenshink et al 2007 the normalized difference vegetation index ndvi enhanced vegetation index evi and soil adjusted vegetation index savi have been used for monitoring post fire recovery van leeuwen et al 2010 gitas et al 2012 veraverbeke et al 2012 however a great deal of uncertainty exists when using these vegetation indices to assess post fire recovery in terms of species composition and forest structure forest recovery assessments using vegetation indices can become complicated when different vegetation recovery states have similar vegetation index values glenn et al 2008 chu and guo 2014 for instance young e g two years post fire broadleaf forest pixels may exhibit the same ndvi value as the unburned coniferous forest pixels that are on a very different successional stage idris et al 2005 cuevas gonzalez et al 2009 cai et al 2018 the limited availability of cloud free satellite images during the growing season can also impede continuous assessment of post fire forest recovery ju and roy 2008 in addition remote sensing based vegetation indices are limited in their ability to monitor demographic processes such as seed dispersal tree establishment and mortality species competition and competition caused mortality self thinning which drive post fire forest recovery fire succession models have been used to understand the interactions between vegetation response to forest fire including post fire forest structure composition and diversity boychuk et al 1997 millington et al 2009 miller and ager 2013 however most of these models lack demographic processes to capture the species regeneration traits alternatively forest landscape models flms spatially simulate forest dynamics seed establishment growth competition and succession accounting for the processes not captured by remote sensing and fire succession models and have been effectively applied to spatially reconstructing historical post disturbance forest conditions he 2008 seidl et al 2014 thrippleton et al 2014 flms can also incorporate information from fire perimeters and the spatial patterns of burn severity derived from remote sensing as inputs wang et al 2009 they can track the location and abundance of parent trees and seedlings when simulating the demographic processes that drive post fire forest recovery wang et al 2013 2014a bib wang et al 2013 bib wang et al 2014a finally flms can be calibrated and validated with forest inventory data seidl et al 2012 wang et al 2014b luo et al 2015 the 1987 black dragon fire which occurred in the boreal forest of china stood out due to its size and severity the fire burned 1 3 104 km2 resulted in a high degree of tree mortality and reset forest succession for most burned stands it created opportunities to study post fire forest dynamics at an unprecedented scale in this study our objectives were to 1 present a novel framework that integrates an flm with field inventory and remote sensing data to spatially reconstruct the burn severity of the black dragon fire and the post fire time series of forest conditions i e forest composition structure and aboveground biomass and 2 evaluate whether the reconstructed forest conditions could realistically capture the post fire recovery e g density and basal area at the level of individual tree species under different burn severities spatiotemporal reconstruction of the post megafire forest condition provides a platform to investigate the recovery rate and trajectories through model simulations and thus improve realism and reduce uncertainties 2 data and methods 2 1 study area our study area is located in the great xing an mountains and encompasses approximately 8 46 104 km2 50 10 n 121 12 e to 53 33 n 127 00 e in northeast china fig 1 the area is hilly and mountainous altitudes ranging from 134 to 1511 m and falls within the continental cold temperate climate zone with long and severe winters but short summers the average annual temperature is 3 9 c with an average temperature of 33 c in the coldest month january and an average temperature of 17 5 c in the hottest month july the annual cumulative precipitation ranges from 400 to 500 mm more than 60 of the annual precipitation occurs in the summer season from june to august zhou 1991 xu 1998 vegetation in this region is representative of cool boreal coniferous forests that cover 83 of the study area the canopy species composition is relatively simple dahurian larch larix gmelini rupr kuzen hereafter larch a deciduous conifer and white birch betula platyphylla suk a deciduous broadleaved species are dominant covering more than 80 of the study area other tree species include the evergreen conifers korean spruce picea koriensis nakai hereafter spruce and scots pine pinus sylvestris var mongolica litvinov hereafter pine and the deciduous broadleaved species aspen populus davidiana dole and p suaveolens fischer willow chosenia arbutifolia pall a skv asian black birch betula davurica pall hereafter black birch and mongolian oak quercus mongolica fisch ex ledeb black birch and mongolian oak are mainly distributed in the southeastern low elevation part of the study area whereas pine is distributed in the northern part wildfire frequency and area burned in our study area are linked to human disturbances and annual variations in monsoonal strength liu et al 2012 low intensity surface fires mean return interval ca 30 yr were historically frequent occasionally mixed with infrequent stand replacing fires mean return interval ca 120 yr in the high elevation regions xu et al 1997 however long term fire exclusion and timber harvest have altered the fire regime where fires are infrequent but more intense chang et al 2007 in our study area fires burned 6 64 104 km2 from 1967 to 2005 chang et al 2008 liu et al 2012 one of the most noteworthy fires known as the black dragon fire ignited on may 6 1987 and burned four forest bureaus xilinji tuqiang amuer and tahe the black dragon fire resulted in over 200 deaths and 4 billion yuan of losses at that time causing the most forest fire damage in the history of china 2 2 general approach we first reconstructed forest stand conditions in our study area before the black dragon fire i e in 1985 using remote sensing and forest inventory data fig 2 since the remote sensing data could not provide detailed stand information at the pixel level across our entire landscape and pre fire field inventory data were not available we constructed the pre fire forest conditions following the k nearest neighbor knn stand imputation approach of zhang et al 2018a 2018b we used landsat data to delineate fire perimeters and derive burn severity classes associated with total tree mortalities xu et al 2020 with the pre fire landscape as a starting point we burned in the black dragon fire perimeter and severity in landis pro at the fire year 1987 and simulated the post megafire time series forest conditions using a forest landscape model landis pro fig 2 landis pro has been parameterized for numerous forest regions under different environmental settings huang et al 2018 wang et al 2019 however to ensure tree species parameters of the model e g growth rate available seeds maximum stand density index and maximum diameter accurately captured the tree species in our study area we used a data assimilation approach luo et al 2011 wang et al 2014b that iteratively calibrated the parameters by comparing the simulated results at years with the respective forest inventory data fig 2a the calibration process was followed by validation against field inventory data at a later stage 2015 to ensure that forest dynamics under no disturbance were correctly simulated fig 2a to ensure the response of tree species corresponded to various burn severities we also applied the data assimilation approach to calibrate fire parameters e g height of bark charring in landis pro to precisely constrain post fire tree species recovery processes fig 2b we used the year 2000 post fire inventory data for the calibration and used the year 2015 post fire inventory data for model validation since these were the only post fire inventory data available in the burned area fig 2b through the iterative calibration and validation processes we were able to derive the continuous time series forest conditions from which post fire forest landscape recovery could be analyzed spatially and temporally fig 2c 2 3 forest inventory data the forest inventory data used for model initialization calibration and results validation in this study were collected from the china forestry science data center cfsdc http www cfsdc org including forest plot inventory data from 2000 2010 and 2015 and a forest stand map in polygons with relatively complete attributes from the early 2000s fig s1 the plot inventory data includes 5752 unburned plots and 1305 burned plots following the black dragon fire each plot contained the number of trees and diameter at breast height dbh 5 cm class by species the forest stand map comprised 276 273 stands with homogeneous forest attributes e g dominant tree species stand age and site index in each stand the data contained stand area mean dbh stand height stand age stand volume tree species composition species percent volume forest origin natural regeneration vs afforestation and management and disturbances harvest plantation and forest fires information in each stand polygon 2 4 remote sensing data in this study 20 pre and post fire landsat tm thematic mapper images table s1 from the u s geological survey usgs http earthexplorer usgs gov were used to estimate pre fire forest composition and burn severity of the 1987 black dragon fire the images were processed by the usgs to convert from dn digital numbers to surface reflectance using the ledaps algorithm landsat ecosystem disturbance adaptive processing system masek et al 2006 clouds cloud shadows and snow pixels were masked using the function of mask algorithm fmask zhu and woodcock 2012 the 1980s images were processed by radiometric normalization based on the images from the 2000s using a histogram matching method to reduce radiometric differences among images caused by inconsistencies of acquisition conditions 2 5 pre fire forest conditions forest inventory data before the black dragon fire were lacking we combined 2000s forest inventory data with 1980s and 2000s landsat tm data to map 1980s aboveground forest biomass and tree lists i e lists of species and diameter for every tree following the approaches of zhang et al 2018a 2018b we used 2000s forest inventory data and 2000s landsat tm data as the training samples to fit a nonparametric random forest based knn model for biomass and knn and weibull parameter prediction models wppms for tree lists then we mapped species level biomass and tree lists before the black dragon fire at 30 m resolution using 1980s landsat tm data based on the developed biomass estimation model and tree lists estimation model thus the pre fire forest composition figs s2 and s3 represents the distribution and abundance of tree species before the 1987 black dragon fire our imputation results figs s2 and s3 conformed to previous studies and field observations the total tree density ranging from 400 to 2500 trees ha fig s2 and aboveground biomass 62 4 22 76 mg ha fig s3 were close to the values reported by zhai et al 1990 hu et al 2015 and fang et al 2001 for northeastern china moreover our estimates of the species distribution figs s2 and s3 were consistent with the environmental niches of tree species larch the representative siberian boreal tree species is distributed most widely since it can endure extremely cold winters and a short growing season and it can grow in both well drained and boggy sites due to its shallow roots xu 1998 kajimoto et al 2003 yang et al 2014 white birch is also distributed widely in the area but is less adaptable to shade and humid environments and typically has less biomass than larch xu 1998 aspen requires warmer temperatures and higher soil fertility and is negatively correlated with elevation xu 1998 scots pine has high tolerance of drought and low temperatures and consequently was mainly distributed on the sunny slopes and ridges of the northern part of our study area zhu et al 2006 mongolian oak and black birch were mainly distributed in the southeast of the area since they thrive in areas with higher temperatures xu 1998 spruce grows under cold environments and therefore was mainly mapped in areas of relatively high elevation of the northern area willow requires sufficient humidity to survive and is therefore widespread along rivers which are fed by water from the glaciers and snows of the high surrounding mountains these evaluations ensured the subsequent reconstructions of forest recovery rate and trajectories bear high realism temperli et al 2013 2 6 landscape model parameterization we used the landis pro forest landscape model to simulate forest landscape changes and reconstruct tree species recovery trajectories after the black dragon fire the model tracks the number density of each tree species by age cohort size class at the pixel level i e 100 m resolution in this study and simulates species stand and landscape scale processes over large spatial and temporal extents wang et al 2013 2014a bib wang et al 2013 bib wang et al 2014a we used the succession module in landis pro to simulate individual tree establishment growth resprouting and mortality at the species level resources competition self thinning and seedling establishment at stand level and seed dispersal at the landscape level establishment success is determined by species specific biological traits such as shade tolerance and suitability to establish under the other environmental conditions besides shade mortality is determined by longevity i e maximum lifespans competition and disturbances other landscape scale processes i e natural and anthropogenic disturbances were simulated by independent modules e g fire harvesting and fuel treatments are simulated using the fire harvest and fuel modules respectively forest change is determined by the interactions of species stand and landscape scale processes we modeled the eight most common tree species which accounted for approximately 95 of stand volume in this study region tree species life history attributes included longevity age of reproductive maturity shade tolerance fire tolerance seed dispersal distance maximum tree diameter maximum stand density index and number of potential germination seeds table 1 landis pro does not require climate and soil parameters however it requires species establishment probability sep and maximum growing space occupied mgso by land type which delineates heterogeneous landscapes into smaller but relatively homogeneous land type units within each land type unit resource availability represented by the mgso and sep is assumed to be homogeneous for this study sep and mgso were derived from an ecosystem process model linkages 3 0 dijak et al 2017 for each land type see supplement 2 7 black dragon fire and its implementation in landis pro the spatial pattern and variability of burn severity strongly influences vegetation response forest structure and post fire successional trajectories halofsky et al 2011 the burn severity map of the black dragon fire used in this study was extracted based on the remote sensing classification from a relationship between normalized burn ration nbr and composite burn index cbi xu et al 2020 the burn severity explicitly accounted for different levels of tree mortality that are important for post fire forest succession unburned no sign of fire effects nbr 585 low severity 252 nbr 585 moderate severity 53 nbr 252 and high severity nbr 53 the simulation of fire is treated as a stochastic process in the landis pro fire module however there was no guarantee that the black dragon fire would occur in 1987 in our simulation thus we mimicked this specific fire event and its effects using the landis pro fuel module he et al 2004 which can deterministically specify how live fuel loads are reduced corresponding to tree mortality detected for each burn severity class in fuel reduction treatments post fire live tree mortality was modeled using a logistic regression equation equation 1 where p is probability of mortality following fire β i are model coefficients determining fire tolerance x1 is tree diameter cm and x2 is height of bark charring m analogously for burn severity based on previous studies woolley et al 2012 fraser et al 2019 we divided our study area into four fuel management areas based on the four fire severity classes xu et al 2020 initial model coefficients for each species were defined based on the species fire tolerance fraser et al 2019 equation 1 p 1 e β 0 β 1 x 1 β 2 x 2 1 2 8 model calibration and results validation forest inventory data in unburned area were available for years 2000 2010 and 2015 the 2000 2010 data were used to calibrate tree species parameters while the 2015 data were used to validate the simulated results forest inventory data in burned areas were only available for years 2000 and 2015 the 2000 data were used to calibrate fire parameters and the 2015 data were used to validate the simulated fire effects fig 2a and b only simulated trees with dbh 5 cm and forest inventory data that were in the study areas with no evidence of disturbance e g logging insects disease and fire after 1987 were used in the calibration and validation processes model fit can be assessed using the difference between the simulated results and true values while overfitting can result in deterioration in prediction accuracy lever et al 2016 in this study to calibrate tree species parameters we iteratively adjusted the age dbh relationship and available seeds for each species by land types until the differences between simulated density and basal area by species and the forest inventory were not significant no differences based on a one way analysis of variance anova test p 0 05 at 2000 and 2010 to ensure that the these parameters realistically represented the actual forests in our study area we validated the simulated results at the landscape scale by stratifying the simulation results and forest inventory data into subecoregions based on the ecoregion classification of xu 1998 and soil types fig s4 because resource availability and species assemblages were relatively homogeneous within a subecoregion and heterogeneous among subecoregions in landis pro for undisturbed forests specifically we compared the simulated mean basal area and tree density of all cells to the observed mean values of all plots for each subecoregion in 2015 by using paired t test to evaluate the overall accuracy and the square of the pearson correlation r2 and root mean square deviation rmsd for each species rmsd was based on squared simulation errors and thus was sensitive to outliers to calibrate fire parameters we iteratively adjusted fuel model coefficients for each species fire tolerance class and height of bark charring for each burn severity until the comparison between simulated density and basal area by species and the forest inventory data passed the significance test no differences based on an anova test p 0 05 at 2000 we validated the simulation results for each burn severity class at site scale by extracting simulated results from raster cells corresponding to the forest inventory plot locations because heterogeneous post fire tree species recovery patterns overrode environmental heterogeneity delineated by subecoregion specifically we compared basal area and tree density within extracted raster cells with observed values in inventory plots for each burn severity class at 2015 by using two sample t test to evaluate the accuracy of simulated post fire recovery all statistical analyses were performed using r statistical software r core team 2015 2 9 post fire tree planting simulation large scale tree plantings in the years immediately after the fire were implemented in each forest bureau thus we simulated planting using the landis pro harvest module fraser et al 2013 the planting management units in this study were constructed based on the burn severity map of the 1987 black dragon fire forest bureau boundaries and harvest management units to capture the variation in planting practices across the region we parameterized the percent area and number of trees planted every two years for each management unit based on forest management records from the china national forest inventory third tier data http www cfsdc org and previous studies yang et al 1998 chen et al 2014 only coniferous species of larch and scots pine were planted in the high burn severity area 70 larch 30 pine with a regular plant spacing 1 5 m 1 5 m or 1 5 m 2 m according to field conditions chen et al 2014 by the end of the 1990s less than 10 of the burned forests were managed with planting yang et al 1998 3 results 3 1 results validation model simulations showed high agreement in the magnitude and time of observed basal area and density from unburned forests at the landscape scale paired t tests p 0 05 r2 0 8 is high and rmsd is low for both basal area and density for all species fig 3 the comparisons of different species demonstrated that dominant tree species larch and white birch had relatively higher accuracy than other species the simulation accuracy for tree density was higher than for basal area because the density in the model was largely determined by a single parameter available seeds while the basal area was affected by the species age dbh relationship that introduced additional uncertainties into estimation overall our results indicated that the simulated forest development was consistent with the actual forest dynamics and simulated density had higher accuracy than simulated basal area comparison between simulated data and observed data showed that post fire forest composition and structure closely represented the real forest composition and structure at different burn severity classes in 2015 for all species and severities two sample t test p 0 05 fig 4 our results indicated that the simulated post fire forest development captured current forest composition and structure after the black dragon fire and thus the simulated fire caused tree species mortality could be close to the real tree species mortality of the black dragon fire the observed and simulated density of conifer and broadleaf species in burned areas showed similar patterns in relation to distance to live tree edges trees mortality rate 90 by the black dragon fire fig 5 post fire conifer density showed a decline with increasing distance to the live tree edge and was almost absent in the interior of high burn severity patches in 2015 fig 5a post fire broadleaf density was high across the whole high severity area and had an opposite trend of density versus distance relationship compared with conifer species fig 5b the self thinning among competing trees led to a decrease in tree density near seed sources the evaluation results increased our confidence in the ability of the calibrated landis pro model to explore the long term fire effects and post fire forest recovery 3 2 the black dragon fire effects and post fire forest recovery trajectories tree mortality and post fire recovery varied greatly among burn severities and tree species fig 6 and s5 mortalities of conifer and broadleaf species were both positively related to burn severity and were very high in areas with moderate and high burn severity fig 6 a and b approximately 50 90 of conifer and almost 100 of broadleaf stems died in areas of moderate and high burn severity nevertheless recruitment of broadleaf trees was abundant in moderate and high severity burned areas the post fire density of broadleaf species gradually increased over the first 12 years then sharply peaked between 2005 and 2010 12 000 and 10 000 trees ha and finally decreased to 10 000 and 7000 trees ha by 2015 in high and moderate burn severity areas respectively the changes in density basal area and biomass over time in unburned and low severity areas were not significant fig 6b the post fire density of conifer species showed a low rate of increase compared to broadleaf species fig 6a the post fire basal areas of conifer and broadleaf species both showed increasing trends throughout the simulation years under all burn severities fig 6c and d for broadleaf species the basal area has recovered to and even exceeded pre fire levels although aboveground biomass always remained lower than pre fire levels fig 6c f for conifer species both basal area and biomass remained below pre fire levels but the recovery rate in low burn severity was fastest fig 6c and e with time the percentage of coniferous species decreased in high and moderate severity burned areas but no significant changes occurred in low severity and unburned areas fig 7 a the percent of coniferous species in high and moderate severity burned areas was far less than the value in unburned forests at 2015 but the edges of these burned patches showed a composition recovery sign with larger conifer percentages than the interior fig 7 4 discussion we presented a spatially explicit framework to reconstruct post megafire forest recovery through integrating a forest landscape model flm and remote sensing and field inventory data reconstruction results showed that burn severity affected the relative dominance of broadleaf vs conifer species in burned stands with probable effects on subsequent canopy dominance in high severity burned areas broadleaf species e g white birch rapidly emerged despite the large burn size while regeneration of coniferous species e g larch was minimal in the interior of the burned patches this pattern matches expectations because white birch can rapidly regenerate either by resprouting from stumps or roots that survived fires or by long distance seed dispersal e g 1000 m however the regeneration of larch depends on the seeds from surviving trees and a relatively short seed dispersal distance 400 m xu 1998 thirty years after the megafire the broadleaf species fully recovered and white birch stands went into the self thinning stage in the interior of the high severity burned area as the newly established trees matured however coniferous species were still in the initial stand development stage in contrast more conifers than broadleaf species regrew in the low severity burned areas where canopies provided suitable conditions for the relatively more shade tolerant conifers and where sufficient seeds from surviving trees had a higher chance to reach fire released areas in the low severity burned patches the comparison of pre and post fire tree composition in the burned patches indicated that self replacement succession was likely to occur in areas that burned with low severity whereas high severity burned areas are more likely to shift forest successional trajectories away from conifer self replacement to pathways with greater broadleaf dominance a finding that is reported in other post fire studies johnstone and chapin 2006 johnstone et al 2010 cai et al 2013 our reconstructed trajectories of post megafire forest recovery were well supported by known data and empirical knowledge of forest stand development after large scale disturbances oliver et al 1996 turner et al 1998 kurkowski et al 2008 suggesting that our model framework is effective in spatially reconstructing post megafire historical forest conditions validating simulation results from flms is critical in quantifying the reliability and credibility of landscape reconstructions our framework of reconstructing the spatiotemporal history of post megafire forest conditions provided not only the spatial pattern and dynamics at the landscape scale but also detailed stand attributes such as basal area tree density and age classes by species we validated the simulated stand attributes with the contemporary inventory data our simulated tree mortality rates were close to the estimates of tree mortality rates from field inventories within different burn severity classes of the black dragon fire luo 2002 the simulated post fire density and biomass were comparable to the field observations reported by wang et al 2001 wang et al 2003 and hu et al 2016 for the same fire and other post fire studies in boreal forests after similar recovery periods johnstone et al 2004 alexander et al 2012 cai et al 2013 our validation results from current forest inventory data demonstrated the calibrated model performed well and ensured that the model was a reasonable and reliable platform for subsequent applications to quantify the effects of megafires on forest composition and landscape succession a notable benefit of our framework is that once calibrated the model is highly scalable and can be applied to filling the gaps where inventory data are not available to assess recovery trajectories across the whole landscape this method offers an approach to augment traditional uses of forest inventory data in post fire studies our approach realistically captures the heterogeneity of post fire recovery process in both time and space figs 5 and 6 s5 in contrast forest inventories when applied alone have generally focused on fixed plots or time periods and on describing entire landscapes which limit their ability to constrain the impacts of heterogeneity on timber volumes and carbon stocks e g kashian et al 2005 furthermore inventory approaches can suffer from biased sampling design for example wang et al 2001 and hu et al 2016 conducted studies near the live tree edges due to the logistical limitations observed a higher biomass recovery than we did in this study and thus overestimated the post fire recovery status they assumed that the mature forests were representative of the forests in this region prior to the 1987 fire however through historical forest conditions reconstruction we found that pre fire forests in this area were younger than wang et al 2001 and hu et al 2016 assumed because forests in this region were affected by historical harvesting and fires which resulted in relatively young stands with most trees between 40 and 60 years old these studies may have overestimated pre fire forest biomass and consequently biomass loss due to the black dragon fire while our simulated biomass loss was closer to the estimated values from the traditional bottom up method xu et al 2020 this highlights the importance of our approach that reconstruction of the entire historical landscape reduced the biases from field sampling this framework can be applied to assess the legacy effects of a megafire over long time periods i e decades to centuries while previous empirical and field based studies documented legacy effects e g downing et al 2019 such effects can persist for decades to centuries seidl et al 2012 and thus require long term assessment our framework can be further applied to projecting how forest landscapes respond to future megafires which are expected to increase under warming climate and increased fuel accumulation from fire exclusion policies chang et al 2007 flannigan et al 2009 liu et al 2012 our framework can also be used to examine alternative management and disturbance scenarios managers could use the framework to evaluate forest resistance rate of recovery and the time to return to pre disturbance states after megafires under various management and climate scenarios as well as to study the effect of alternative managements on mitigating future megafire risk for example reforestation is increasingly used to assist forest restoration and improve resilience especially under warming climate as conifer forests will be increasingly regeneration limited with intensifying fire regimes hof et al 2017 north et al 2019 different reforestation strategies e g planting intensity and spatial assignment can be evaluated with flms wang et al 2006a 2006b bib wang et al 2006a bib wang et al 2006b researchers can also use this framework to evaluate the response of forest landscapes to other forest disturbances such as drought insect and harvest under different environmental settings declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the national biologic carbon sequestration assessment program under the u s geological survey climate and land use mission area any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government xu w he h s fraser j s hawbaker t j henne p d duan s and zhu z 2020 data release for spatially explicit reconstruction of post megafire forest recovery through landscape modeling u s geological survey data release https doi org 10 5066 p9hrhbxz appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104884 
