index,text
160,identification of groundwater contaminant sources in a highly heterogenous geosystems results in a high dimensional inverse problem and is often solved based on a surrogate model to alleviate the computational burden surrogate modeling through deep learning has a great potential for learning complex nonlinear relationships between model inputs and outputs most of the developed surrogates however can only estimate the contaminant concentration fields at a limit number of time steps with a relatively large lag in this paper a transformer based surrogate model is developed to provide a detailed release history of contaminant which allows more accurate analyzing the distributions and planning as such a koopman operator based convolutional autoencoder is trained and fixed prior to the training of transformer here the encoder converts the concentration fields into a one dimensional embedding space and the transformer is trained on this space to learn the system dynamics and predict the embedding feature at next time step which is reconstructed back to the original space with the decoder the proposed surrogate model is tested on a complex problem and the results demonstrate that the proposed transformer based surrogate can efficiently provide an accurate estimation of the evolution of contaminant concentration field at a greater number of time steps compared to the previous works keywords transformer machine learning groundwater contamination forecasting 1 introduction characterization of the release history of contamination in groundwater systems is a significant prerequisite for the design of remediation scheme and regulatory policy due to the high invisibility and complexity of subsurface systems however only some indirect measurements of the contaminant concentration are often obtained at the observation wells through which one can infer the location and strength of the contaminant source therefore this topic is generally casted as an inverse problem where the predictions from a contaminant transport model needs to be matched up with the measured data in a sense that they cannot be produced initially and requires some iterations on the other hand the hydrological models are mathematically abstracted from the physical reality and uncertainties are usually introduced during different stage of the model building process a major source of uncertainty comes from the heterogeneity of geological formations due to incomplete knowledge which poses a big challenge for accurate identification of groundwater contaminant sources mahmud et al 2014 tahmasebi and sahimi 2016a 2016b given such limited data resources and a very large search space for finding the optimal solution various methods have been proposed the traditional optimization based methods such as least square regression white 2015 nonlinear programming mahar and datta 1997 and hybrid optimization with a genetic algorithm ayvaz 2016 leichombam and bhattacharjya 2018 can provide acceptable predictions for the release history however this inverse problem is commonly recognized as ill posed with non existed or non unique solution hill 1996 tarantola 2005 zhao et al 2020 moreover the uncertainty of the predictions cannot be effectively quantified with this approach on the other hand statistical methods such as data assimilation with extended and ensemble kalman filter xu and gomez hernandez 2018 2016 bayesian inference based on markov chain monte carlo mcmc yan et al 2019 have the capability to estimate such uncertainty the geostatistical approaches also have been applied to provide a stochastic function estimate of the historical distribution of contaminant with high computational efficiency by combining with an adjoint methodology michalak and kitanidis 2004a 2004b in depth reviews of the applications of inverse modeling methods for identifying groundwater contaminant sources can be found in work by sun 2013 and sun and sun 2015 in the bayesian approach for example the model input parameters and outputs are treated as random variables and their posterior distributions can be estimated through mcmc specifically the contaminant transport model must be run repetitively to obtain the target distribution as a result the computational cost is very high particularly for a high dimensional inverse problem which is prohibitive to address this issue surrogate based approaches have been proposed to substitute the original computational intensive simulations a surrogate model can approximate the relationship between model inputs and outputs and is cheaper to evaluate than the underlying physics based model thus instead of conducting a full forward modeling surrogate methods aim to estimate the response by establishing a map between the input and output parameters typical surrogates used in the field of contaminant source characterization include polynomial chaos expansions ciriello et al 2019 adaptive sparse grid interpolation zeng et al 2012 zhang et al 2015 and gaussian process zhang et al 2016 one of the disadvantages of such methods is that their performance degrades rapidly when the number of random input variables becomes large asher et al 2015 razavi et al 2012 traditional machine learning methods such as artificial neural networks singh and srivastava 2013 radial basis functions li et al 2003 support vector machine vojinovic and kecman 2004 wang and harrison 2012 kernel extreme learning machine jiang et al 2015 have also been implemented to capture the complex mappings between inputs and outputs however they could not provide an accurate prediction for highly nonlinear functional mapping with limited training data and have usually been applied in relatively simple flow and models recently deep learning have demonstrated great performance in many fields in particular those related to geo systems materials due to its robustness and powerful generalization capability ardabili et al 2019 bai and tahmasebi 2022 bergen et al 2019 kamrava et al 2021a 2021b shen 2018 tahmasebi et al 2020 compared to traditional machine learning models they have greater capabilities to approximate highly nonlinear mapping functions as they take advantage of large depth architecture miikkulainen et al 2019 hierarchical feature extraction principle lecun and yoshua 1998 and selective information storage mechanism greff et al 2017 the optimization algorithm namely the minibatch stochastic gradient descent can reach an acceptable balance between robustness and efficiency for training large scale deep learning model goyal et al 2017 previous works have successfully utilized deep learning models to characterize groundwater contamination mo et al 2019 developed a dense convolutional autoencoder network to learn the mapping function between highly heterogeneous hydraulic conductivity and contaminant concentration and proposed an autoregressive strategy to consider the effect of a time varying source strength similarly zhou and tartakovsky 2020 developed a deep convolutional neural network as a surrogate to replace the process based contaminant transport model and thus accelerate the mcmc simulation similarly and in a different field autoencoder is integrated with a autoregressive strategy to better link the time steps and provide more coherent predictions jiang et al 2021 autoencoder has also been used with a long short term memory lstm through which a more robust connection between time steps is established kamrava et al 2021b on the other hand physics informed neural networks can also be used as a robust and efficient surrogate model with limited training realizations by incorporating the governing equations of contaminant transport into the loss function he et al 2021 although the above studies obtained satisfying results their predictions are only available at a limited number of time steps usually less than 20 and the details between each of them are not provided one can possibly increase the number of time steps which can result in a much higher computational time and stronger error propagation on the other hand li et al 2021 developed an lstm surrogate to identify the release history of a contaminant source this lstm model due to the effectiveness of this method is dealing with time series data could generate consecutive predictions over the entire simulation period this method however is limited to only the evolution of contaminant concentration in the observation well and does not offer a full visualization of the simulation domain moreover all these works made the predictions completely depending on the most recent time steps to provide information regarding to the current and past state of the system alternatively transformer vaswani et al 2017 based on the self attention mechanism can learn a longer term dependencies without computationally inefficient recurrent connections used in lstm it is mainly designed for natural language processing nlp such as machine translation ahmed et al 2017 wang et al 2019 text classification gong et al 2020 shaheen et al 2020 and question answering shao et al 2019 zhao et al 2020 where a longer connection between the terms and elements should be established on the other hand the koopman operator has also gained a lot of attentions recently due to its powerful capability to infer the evolution of a nonlinear dynamic system that is either partially or completely unknown mamakoukas et al 2020 a data driven version of koopman operator has already been applied in neuroscience brunton et al 2016 fluid mechanics mezic 2013 and climate forecast hogg et al 2020 a combination of these two techniques has also been successfully used to predict various dynamic system geneva and zabaras 2020a in this paper inspired by the previous works and the successful application of transformer model and koopman operator for solving long term dependency problem and learning nonlinear dynamic system a deep learning based surrogate model is proposed and applied to identify the release history of groundwater contaminants this surrogate model consists of a koopman operator based convolutional autoencoder kcae and a transformer the kcae model projects the 2d contaminant concentration field at each time step into a 1d vector representation namely an embedding which is then processed by the transformer to learn the dynamics of this physical system with the trained surrogate model the groundwater contaminant concentration fields can be predicted and visualized at a greater number of time steps compared to the previous works whereas the accuracy is not decayed significantly 2 problem formulation 2 1 governing equations this study aims at developing a surrogate model to identify the release history of a contaminant source in a strongly heterogeneous groundwater system the governing equation for groundwater flow in a steady state regime is solved first to provide the flow field using 1 x i k i h x i 0 where ki lt 1 denotes the hydraulic conductivity tensor and h l is the hydraulic head in our case only the advection and dispersion terms are considered as the dominant mechanism for contaminant transportation and molecular diffusion is neglected as well thus the governing equation for contaminant transportation in groundwater flow can be reduced to 2 ϕ c t x i ϕ d i j c x j x i ϕ v i c q s c s where t t is time xi l is the spatial direction i 1 2 φ is the effective porosity c ml 3 is the contaminant concentration v i lt 1 is the groundwater flow velocity in the direction of xi qs t 1 is the volumetric flow rate per unit volume of aquifer representing fluid sources and sinks cs ml 3 is the concentration of the sources or sinks dij l2t 1 is the hydrodynamic dispersion coefficient tensor determined by v i α l l is longitudinal dispersivity and α t l is transverse dispersivity which all these three parameters are respectively defined by 3 d x x α l v x 2 α t v y 2 v d y y α l v y 2 α t v x 2 v d x y d y x α l α t v x v y v where vx and vy are the components of groundwater flow velocity and v is its magnitude the contaminant transport equation is related to the groundwater flow equation through the darcy s law 4 v i k i ϕ h x i the contaminant concentration can be obtained by solving above equations with appropriate initial and boundary conditions here they are numerically solved by the groundwater flow simulator modflow harbaugh 2005 and contaminant transport simulator mt3dms zheng et al 1999 respectively when the property and its conditions are uncertain the above governing equations become stochastic in this study we assume that the uncertainty only comes from the highly heterogeneous hydraulic conductivity which is considered to be a gaussian random field and the source strength initial and boundary conditions and other parameters like porosity and dispersivities are assumed to be known and constant as well to add more complexities the strength of contaminant source is assumed to vary with time eqs 1 19 2 2 iterative local updating ensemble smoother ilues in this work the iterative local updating ensemble smoother ilues developed by zhang et al 2018 is used as the inversion framework to complete the contaminant source identification process this algorithm takes the local updating strategy for ensemble samples along with a simple iterative process to solve nonlinear problems mathematically the model m n m 1 representing the uncertain parameters is related to the measured values d obs nd 1 and the response of the forward modeling f through following equation 5 d o b s f m ε where ε is the observed error model parameters m can be updated by ensemble smoother algorithm defined by 6 m j a m j f c m d f c d d f c d 1 d j f m j f j 1 n e where m f m 1 f m n e f is an ensemble of ne parameter samples obtained from the prior distribution m a m 1 a m n e a is the updated conditional ensemble c m d f is the cross variance matrix between m f and model responses d f f m 1 f f m n e f c d is the covariance matrix of the observed errors and c d d f is the autocovariance matrix of d f a local updating strategy for ensemble is also adopted when the prior or posterior distribution of m is multimodal the local ensemble for each sample can be constructed based on the ensemble index 7 j m j 1 m j 1 m a x j 2 m j 2 m a x where j 1 m is the distance between model response f m and the measured values d obs j 2 m is the distance between the model parameter m and the sample m j f j 1 m a x and j 2 m a x are the maximum values of j 1 m and j 2 m then the local ensemble of m j f is defined by nl αne α 0 1 samples with the nl smallest j values a simple iterative process is used to repeatedly apply this local updating strategy until the allowed maximum number of iterations is achieved more details about the ilues algorithm can be found in zhang et al 2018 3 methods 3 1 attention and transformer transformer models are designed for sequence to sequence modeling vaswani et al 2017 they completely depend on an attention mechanism to derive the global dependency between each word in a sequence the model receives an input sequence and projects it into two new sequences one word vector embedding for the convenience of numeric processing and another positional encoding to retain position information of each word see fig 1 then these two sequences are added up and passed through a series of encoders and decoders the encoder converts the input sequence to a new sequence where each element of it has an embedding that relies on all other elements the decoder is responsible for converting it back to a sequence of probabilities of different output words by a softmax function in the original transformer model vaswani et al 2017 the encoder consists of six identical blocks and each block has two layers a multi head attention layer and a feed forward layer in particular the multi head attention layer teaches the model how important other elements in the input sequence are for the embedding of a given element the architecture of decoder is almost similar to that of encoder except that another multi head attention layer is introduced into each block to compute the attention over the output of encoder note that the decoder works in an autoregressive manner where only previously processed elements are considered for predicting next element which is achieved by masking the subsequent target element and the following elements as zero the residual connection and layer normalization ba et al 2016 has also been applied in each layer to retain the information passed from previous layer unlike recurrent neural network rnn which sequentially processes each element in the sequence and usually retains little useful information about early elements the transformer can simultaneously calculate the attention over the entire input sequence although the gated design of lstm can resolve the long term dependency issue faced by rnn it still deals with each element sequentially moreover the parallel processing principle and multi head attention mechanism used in the transformer also allows it to take advantage of high performance computing devices such as gpu the following provides more details about the attention mechanism and positional encoding the attention mechanism has demonstrated a lot of improvements to neural machine translation which was firstly designed by bahdanau et al 2014 to address the dependencies between long source and target sentence particularly the scaled dot product attention mechanism shown in fig 1 b enables the sequence to sequence modeling to discard recurrent connections used in lstm where the long dependency between each element of the sequence is captured as a whole instead of sequentially deriving the dependency with previous elements based on past hidden states the first step is to get the target token query q and the source token key value k v pairs through linear transformations of the input features x x r d e n where n is the number of components in the input sequence and de is the embedding size 8 q w q x k w k x v w v x where w q r d q d e w k r d k d e w v r d v d e are learnable parameters note that dq is equal to dk in the original transformer model then a self attention mechanism is implemented with these projections of inputs it works like a fuzzy dictionary lookup where the similarity between current query and each of keys is evaluated through the dot product and a probability distribution is then assigned to all the keys via the softmax function finally a weighted sum of all the values is provided as the result for current query based on the calculated probability distribution 9 a t t e n t i o n q k v s o f t m a x q t k d k v t the denominator can ensure that the dot products do not grow too large otherwise a small gradient will be backpropagated from the softmax function this mechanism could guide the model to pay more attention to some specific sections of the embedding which may have more importance for dealing with current element of the sequence each self attention layer is referred as an attention head multi head attention which is shown in fig 1 b has been implemented in the transformer and each head has its corresponding learnable parameters wq wk wv this unit works like the multiple kernels used in convolutional neural networks to capture different features and hence it can calculate the attention for multiple positions in the sequence the output of each head with a size of n dv is concatenated horizontally and then pass through a linear transformation parameterized by wo w o r d o h d v where do hdv and h is the number of attention heads or self attention layers in addition to the attention mechanism a positional encoding was also implemented to consider the relative position of each component in the sequence this is especially important for attention mechanism since the same element may have different importance when its position varies it was implemented by adding the trigonometric function to the embedding 10 p e p o s 2 i sin p o s 10000 2 i d e p e p o s 2 i 1 cos p o s 10000 2 i d e where 2i and 2i 1 denotes the 2i th and 2i 1 th element of the embedding pos is the embedding s global position in the entire input sequence 3 2 the koopman operator since the input of transformer model usually is represented as a series of 1d vectors we also project the contaminant concentration field at each time step into a 1d embedding through a convolutional autoencoder cae model and the koopman operator is introduced as its middle latent variable to link the embedding with system dynamics such that the embedding at next time step predicted by transformer could be projected back to the original space with the decoder of cae model the koopman operator proposed by koopman 1931 koopman and neumann 1932 can project the state space of a nonlinear dynamical system to a observable space through which the system can be linearly propagated into the future let x t m r n denote the state of the system at time t a nonlinear discrete time dynamical system is considered and its evolution is described using 11 x t 1 f x t x t δ t x t t t δ t f x τ d τ where f m m is the evolution operator acting on a single state vector and m r n is the state space then an observation function g m r belonging to f is defined where f includes all the measurements also referred to as observables of the states and forms an infinite dimensional hilbert space the koopman operator k f f is a linear transformation defined as 12 k g x t δ g f x t g f x t g x t 1 where denotes the function composition of g and f thus the koopman operator can lift the original nonlinear dynamical system m t f to a new linear dynamical system f t k owing to the linearity nature of the defined function composition g f although this lifting operation makes the system state become more complex the system dynamics become simpler fig 2 shows how the koopman operator can convert the nonlinear dynamic system to a linear one the bottom path updates the system state x using evolution operator f while the top path evolves the outputs of observation function g f through koopman operator k these two paths are linked through the identity operator or full state observable g xt xt clearly one can solve a linear but infinite dimensional dynamic problem top path only if a finite dimensional approximation to the infinite dimensional koopman operator could be found a possible solution is to find a koopman invariant subspace of the observation function space takeishi et al 2017 however this requires identification of the essential observation functions that governs the underlying dynamics recently the data driven methods have proven to be useful for automatically determining the koopman invariant subspace by representing the observation functions with deep neural networks lusch et al 2018 otto and rowley 2019 this makes it possible to predict the evolution of system observables by repeatedly applying the koopman operator 13 g x t m k g x t m 1 k 2 g x t m 2 k m g x t where km represents a m fold function composition 3 3 proposed method the proposed deep learning based surrogate model has two major components a koopman operator based convolutional autoencoder kcae and a transformer the kcae model is trained prior to the transformer and then fixed to convert all the data into a 1d embedding space then the transformer is trained on this dimension reduced space to predict the embedding at next time step which is reconstructed back to the original space by the decoder of the kcae model specifically the kcae model projects the contaminant concentration field at each time step into a vector representation this is similar to the word embedding model as in word2vec each word is represented by a real valued vector and then the entire sentence is converted into a series of 1d vectors and processed by the transformer church 2017 the architecture of the kcae model follows a standard convolutional autoencoder which is shown in fig 3 here the encoder encodes the hydraulic conductivity field and the contaminant concentration field at a certain time step into an embedding vector encoder r t 2 l w r t d where t is the size of the context window or the number of time step considered in a single batch d is the dimension of embedding vector l and w are the length and width of the simulation domain respectively and the decoder reconstructs the concentration field from the embedding decoder r t d r t 1 l w the size of 1d embedding vector is set as 128 i e d 128 the detailed architecture of the kcae model is shown in table 1 where the values of kernel size stride and padding for each convolutional layer are also provided note that the hydraulic conductivity also consists of part of the inputs since it is the primary uncertainty source that could affect the contaminant concentration distribution in our case to enable the decoder to learn the reconstruction process from the predicted embedding a koopman operator k is introduced as the middle latent variable for data in a single batch with t time steps the process to train the kcae model is described as follows the encoder first converts the contaminant concentration field at the first time step x 1 into a 1d embedding vector observable g x 1 encoder x 1 which is then stored and mapped back to the original space through the decoder x 1 d e c o d e r g x 1 starting from the second time step expect for reconstructing the concentration field from the true embedding g xt the decoder could also recreate it based on the estimated embedding g x t for example the contaminant concentration field x 2 is first reconstructed based on the embedding g x 2 then g x 2 could be estimated from the stored embedding at last time step g x 1 with the koopman operator k g x 2 k g x 1 the decoder recreates the concentration field x 2 from this estimated embedding g x 2 x 2 d e c o d e r g x 2 although both outputs are the reconstruction of the original contaminant concentration field they are implemented based on different perspectives one is for learning the compression process and the other is for learning the reconstruction from the embedding predicted by the transformer such a procedure is repeated for remaining t 2 time steps the process of recreating the concentration field from the estimated embedding g x t t 2 is starting from the second time step since no embedding at previous time step i e g x 0 exists to estimate g x 1 note that we manually restrict the form of the koopman operator which is defined as a sparse banded matrix this additional constraint could improve the model s performance in terms of eliminating the overfitting issue caused by high frequency noise and outliers bruder et al 2019 kaiser et al 2017 and demanding less memory to store the following loss function is defined to train the kcae model 14 l k c a e i 1 n j 1 t α m s e x i j x i j β m s e x i j x i j γ k 2 2 x i j d e c o d e r e n c o d e r x i j x i j d e c o d e r k j 1 e n c o d e r x i 1 where n is number of training batches t is number of consecutive time steps in a single batch α β and γ are hyperparameters the loss function respectively consists of reconstruction loss dynamics loss and a l2 norm based regularization on the defined koopman operator to prevent overfitting the adam algorithm kingma and ba 2015 is used to optimize the model parameters with an initial learning rate of 0 001 besides an exponential scheduler is also applied to decay the learning rate with the hyperparameter gamma setting as 0 995 note that we set the values of hyperparameters α β and γ as 10000 1 and 0 1 respectively geneva and zabaras 2020a the main objective of the koopman operator is to link the reconstruction process with the learned transformer model rather than finding its finite dimensional approximation since it will not be used anymore once training of the kcae model is completed therefore we set a much smaller value for the weight of dynamic loss compared to the one for reconstruction loss every training example includes contaminant concentration fields at t time steps and each of them represents the physical state of system at the corresponding time as fig 4 shows each physical state is converted into the embedding g xt with the encoder of the trained kcae model like the original transformer model a trigonometric function based positional encoding shown in eq 7 is added to this physical embedding to represent the relative position of each obtained embedding in the input sequence since there is no recurrent connection implemented in the transformer model to maintain that information then this processed embedding is passed to the transformer model to sequentially predict the embedding at next time step finally the decoder of the trained kcae model is used to reconstruct the contaminant concentration field with the predicted embedding many variants of original transformer model have been developed such as gpt radford et al 2019 bert devlin et al 2018 transformer xl dai et al 2019 and xlnet yang et al 2019 the gpt model stands for generative pretrained transformer is used here to complete this task which only consists of a stack of transformer decoder with masked attention i e the encoder part of the original transformer model is unused its architecture is based on the implementation of gpt 2 by the hugging face transformer repository wolf et al 2019 but has a much smaller size compared to the transformer used for modern nlp the major consideration is that the autoregression nature of the transformer decoder is more suitable for learning and predicting the evolution of a dynamic system this inherent nature indeed has proven to be similar to the explicit linear time integration methods which has been widely used in deep learning to predict physical time series geneva and zabaras 2020b jiang et al 2021 kamrava et al 2021b sanchez gonzalez et al 2020 in detail each newly predicted embedding g x t is added to the input sequence which then becomes the new input of the model to get the prediction of g x t 1 initially the only active element of the input sequence is a start token s to remind the model to start working which is processed successively by all the transformer decoders to get the prediction of the first embedding i e g x 1 then this predicted embedding is added to the input sequence such that s g x 1 is used to get the prediction of second embedding g x 2 through the model this process is repeated until the prediction of last embedding note that the masked attention can prevent the decoder from cheating in terms of obtaining any information about next and following embeddings of the input sequence technically this is achieved by adding a mask to the dot products of query q and key k of future embeddings and enforcing them to have a value of negative infinity the resulted softmax scores also referred to as the weights of value v corresponding to future embeddings will be zero which means only information about previous embeddings will be used to calculate the attention a mean square error between the predicted embeddings and the corresponding targets is defined to train the transformer model using the adam optimization algorithm 15 l t r a n s f o r m e r i 1 n j 1 t m s e g x i j g x i j 4 results and discussion 4 1 case introduction a synthetic two dimensional numerical case study is used to demonstrate the accuracy and efficiency of proposed method as shown in fig 5 the considered domain has a size of 10 l 20 l and is uniformly discretized into 64 128 cells the left and right boundaries represent constant head with predefined heads of 9 l and 1 l respectively the upper and lower boundaries are assumed to be no flow boundaries there is one contaminant source and six observation wells in the area the contaminant concentration in each observation well is recorded as the simulation proceeds from 0 t to 16 t to add more temporal complexity the contaminant is released from 0 t to 8 t with a time varying strength furthermore the source strength is represented by the mass loading rate mt 1 which is the product of concentration ml 3 and flow rate l3t 1 table 2 lists the reference time varying source strength and their prior distribution from which the source strength used in the numerical simulation is sampled we assumed that the values of effective porosity longitudinal and transverse dispersivities are known and constant θ 0 25 α l 1 0 l and α t 0 1 l the hydraulic conductivity field is assumed to be a log gaussian random field described by 16 k x y exp g x y g x y n m x y c and the covariance matrix is parameterized by the matern covariance function 17 c x 1 y 1 x 2 y 2 σ 2 2 1 ν γ ν 2 ν r ν k ν 2 ν r r x 1 x 2 2 λ x 2 y 1 y 2 2 λ y 2 where m x y 2 is the mean σ2 0 5 is the variance and λx 4 l and λy 2 l are the correlation lengths in the x and y directions respectively 100 hydraulic conductivity fields were produced and then passed to the described forward modeling to generate the necessary training and test dataset the source strength applied for each conductivity field is randomly sampled from the prior distribution given in table 2 the contaminant concentration fields of 200 time steps were evenly sampled from each model note that the number of concentration fields we gathered from each numerical case is far more than that of previous works mo et al 2019 zhou and tartakovsky 2020 here 85 of the simulated numerical cases were used in the training dataset the training samples were sampled consecutively from each model with a stride of 1 and a context window size of 25 i e t 25 therefore there is 14 875 training samples in total the remaining numerical cases form the testing dataset to evaluate the accuracy of trained model the input of each sample consists of the conductivity field and the contaminant concentration fields for t consecutive time steps from xt to x t t 1 while the output is the contaminant concentration fields from x t 1 to x t t both of the kcae and the transformer models were trained for 100 epochs the performance of the surrogate is evaluated using the coefficient of determination r 2 and the root mean square error rmse which are given by 18 r j 2 1 i 1 n t e s t x i j x i j 2 2 i 1 n t e s t x i j x j 2 2 19 rms e j 1 n t e s t i 1 n t e s t x i j x i j 2 2 where n test is the number of samples in the testing set and x j 1 n t e s t i 1 n t e s t x i j higher r 2 score and lower rmse is always preferred 4 2 accuracy assessment fig 6 shows r 2 score and rmse of the predictions at each time step for all samples in the testing set which are shown by surrogate curves it can be seen that the proposed surrogate provides an accurate prediction of the concentration field over the entire time steps for example even at the last time step it achieves r 2 score of 0 9482 and rmse of 0 0363 respectively on the other hand the r 2 score decreases gradually with the proceeding of time while the rmse presents an opposite trend this is due to the autoregressive nature of the transformer where the errors originated from the prediction at previous time steps can be propagated and accumulated to the one at next time step however as mentioned the overall and final values of the above statistics are within an acceptable range the performance of the proposed surrogate model is further demonstrated in fig 7 which shows the predicted results of a random test sample with hydraulic conductivity field shown in fig 5 the applied source strength at each time period is 1 08 4 37 5 91 1 74 mt 1 respectively these results are also compared with those obtained using the numerical simulation and the differences between them are presented in the third column the movement of the contaminant plume is as expected controlled by the time varying source strength we just want to reemphasis that the contaminate does vary with time and is not injected during the entire simulation it is clear that the proposed surrogate method is capable of capturing the evolution of concentration field reasonably accurately although some errors are noticeable at the periphery of the contaminant plume this is possibly caused by compressing two dimensional concentration field into one dimensional embedding vector where part of the spatial information can be lost in this process overall however the general trend of the concentration field and the system dynamics are essentially reproduced note that for conciseness the concentration fields only five different time steps are selected to make this comparison otherwise the simulation is conducted over all proposed time steps the accuracy of the surrogate in predicting the evolution of contaminant distribution is further illustrated in fig 8 which performs a point wise comparison of the prediction and target models it is observed that most of the data points are evenly distributed around the 45 degree line similarly due to the errors automatically accumulated from the autoregressive predictions their degree of dispersion increases with time and the coefficient of determination is decreased from 0 9983 to 0 9528 however the general level of agreement at the last time step is still quite close which again indicates a high degree of accuracy in the predicted concentration fields on the other hand the contaminant concentration at each observation well is also of great importance since data of this type is sequentially integrated into the inverse problem the prediction accuracy of the contaminant concentration for the model shown in fig 7 at six observation wells is demonstrated in fig 9 the predictions at 200 time steps are connected linearly to form a continuous curve a great agreement between the results from numerical simulations and the proposed surrogate model can be seen such that the general trends and breakthrough time are predicted with reasonable accuracy which is essential if this surrogate is to be used for data assimilation there are however apparent but small discrepancies in the plots of some observation wells such as a relative larger prediction in the fourth observation well 4 3 inversion results to accelerate the convergence speed of the inversion method ilues the conductivity field is parameterized by the karhunen loeve expansion kle which is mainly controlled by the number of kle terms to preserve the field variance wang et al 2020 a value of 679 suggested by mo et al 2019 is used here to preserve 95 of the total field variance the uncertain source strength at each time periods is assumed to be uniformly distributed whose reference values and prior distributions are given in table 2 the contaminant concentration data of six observation wells see fig 5 at 40 time steps t 5 10 15 195 200 are collected and used for parameter estimation the ensemble size and the iteration number are set to be 5 000 and 20 respectively fig 10 compares the updating process of the source strength at different time periods when numerical simulator and the proposed surrogate is used as the forward model separately it can be seen that both results converge to the reference values indicating the effectiveness of developed surrogate for estimating the time varying contaminant source strength due to the parallel processing principle implemented in the transformer and the application of gpus for all training and testing process the proposed surrogate model can obtain a significant computational speedup it is worth noting that the surrogate based ilues approach uses the trained transformer based surrogate to perform the forward simulation i e we only need to train the surrogate once in the whole process which could substantially improve the computational efficiency of the inversion the training and prediction times of the surrogate is 64 min and 0 018 s respectively whereas the average computational time to solve a single example with the numerical methods is 2 94 s thus if 10 000 forward models are required to solve the considered inverse problem the inverse process can gain a roughly seven times speedup furthermore more computational time is expected to be saved if the forward model is used in 3d 4 4 comparison with the surrogate of convlstm to further demonstrate the effectiveness of the proposed surrogate model in learning long term dependency in a dynamic system a convolutional lstm convlstm model is trained on the same dataset for 400 epochs such model has shown to be able to capture both spatial and temporal features and has been successfully applied in modeling of subsurface systems and porous media kamrava et al 2021b tang et al 2021 2020 note that these works still make predictions at a limit number of time steps around 10 and are often combined with the convolutional autoencoder the lstm receives the most compressed feature map from the encoder and generates a sequence of feature maps representing the physical state at different time steps which are then decoded separately with the same decoder the architecture of our convlstm model is similar to that of the kcae model except that the koopman operator in the middle is replaced by the convlstm layer to produce a sequence of compressed state maps at 200 different time steps fig 6 also shows r 2 score and rmse of the predictions at each time step generated by the convlstm model it is clear that the performance of this model degrades much faster compared to the proposed surrogate as a lower r 2 score and higher rmse are achieved at the last time step fig 11 also compares the predictions of convlstm and the numerical method on the same example shown in fig 7 it can be seen that some unreasonable predictions happened at later time steps the trend of the contaminant diverges from the main plume and its tail even tends to move toward the upper right corner this is because the recurrent connection in the convlstm layer still sequentially process the physical state at each time step and can only selectively retain limited information from early periods while the transformer model can simultaneously obtain the relative importance of each element in the input sequence this again demonstrates the accuracy and robustness of the surrogate in capturing the temporal evolution of the system with long term dependencies 5 conclusion in this study a transformer based surrogate model was developed to identify the groundwater contaminant sources in a highly heterogeneous environment which often is recognized as a high dimensional inverse problem the proposed model has two major components a koopman operator based convolutional autoencoder kcae model is trained first to project the contaminant concentration fields into a one dimensional embedding space for the convenience of training the transformer model since its input usually is represented as a sequence of vectors the koopman operator is introduced to map the original nonlinear dynamic system to a new linear dynamic system and ease the learning of reconstruction process from the predicted embedding due to the inherent autoregressive nature of transformer decoder the evolution of contaminant concentration field can be predicted step by step even when dealing with a time varying source term the physical state of system at previous time step is converted into a vector embedding by the encoder of trained kcae model which is treated as input of the transformer model to predict the embedding at current time step then it is projected back to the original space with the decoder of the kcae model the performance of the proposed surrogate is demonstrated using a synthetic problem with highly heterogenous conductivity field the results show that our transformer based surrogate model can accurately approximate the complex relationship between high dimensional model inputs and outputs and estimate the evolution of nonlinear dynamic system the self attention mechanism used in the transformer model greatly improves the network s performance in learning the long term dependencies of a dynamic system finally the multi gpu based parallel computation also enables the surrogate model to gain a significant computational speedup note that the proposed surrogate can only identify the contaminant release history but source locations can be unknown as well one potential solution informed by the work of mo et al 2019 is to explicitly represent the source location as a binary image and constitute a part of the inputs of the model our results on this important extension will be reported in our future work credit authorship contribution statement tao bai methodology software writing original draft pejman tahmasebi conceptualization supervision writing original draft funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the financial support from the university of wyoming the school of energy resources for this research is greatly acknowledged the authors would like to thank the three anonymous reviewers for their constructive comments to improve the paper the codes and data used to produce the results are available at the website https github com tbaiwyo groundwater contaminant identification with transformer 
160,identification of groundwater contaminant sources in a highly heterogenous geosystems results in a high dimensional inverse problem and is often solved based on a surrogate model to alleviate the computational burden surrogate modeling through deep learning has a great potential for learning complex nonlinear relationships between model inputs and outputs most of the developed surrogates however can only estimate the contaminant concentration fields at a limit number of time steps with a relatively large lag in this paper a transformer based surrogate model is developed to provide a detailed release history of contaminant which allows more accurate analyzing the distributions and planning as such a koopman operator based convolutional autoencoder is trained and fixed prior to the training of transformer here the encoder converts the concentration fields into a one dimensional embedding space and the transformer is trained on this space to learn the system dynamics and predict the embedding feature at next time step which is reconstructed back to the original space with the decoder the proposed surrogate model is tested on a complex problem and the results demonstrate that the proposed transformer based surrogate can efficiently provide an accurate estimation of the evolution of contaminant concentration field at a greater number of time steps compared to the previous works keywords transformer machine learning groundwater contamination forecasting 1 introduction characterization of the release history of contamination in groundwater systems is a significant prerequisite for the design of remediation scheme and regulatory policy due to the high invisibility and complexity of subsurface systems however only some indirect measurements of the contaminant concentration are often obtained at the observation wells through which one can infer the location and strength of the contaminant source therefore this topic is generally casted as an inverse problem where the predictions from a contaminant transport model needs to be matched up with the measured data in a sense that they cannot be produced initially and requires some iterations on the other hand the hydrological models are mathematically abstracted from the physical reality and uncertainties are usually introduced during different stage of the model building process a major source of uncertainty comes from the heterogeneity of geological formations due to incomplete knowledge which poses a big challenge for accurate identification of groundwater contaminant sources mahmud et al 2014 tahmasebi and sahimi 2016a 2016b given such limited data resources and a very large search space for finding the optimal solution various methods have been proposed the traditional optimization based methods such as least square regression white 2015 nonlinear programming mahar and datta 1997 and hybrid optimization with a genetic algorithm ayvaz 2016 leichombam and bhattacharjya 2018 can provide acceptable predictions for the release history however this inverse problem is commonly recognized as ill posed with non existed or non unique solution hill 1996 tarantola 2005 zhao et al 2020 moreover the uncertainty of the predictions cannot be effectively quantified with this approach on the other hand statistical methods such as data assimilation with extended and ensemble kalman filter xu and gomez hernandez 2018 2016 bayesian inference based on markov chain monte carlo mcmc yan et al 2019 have the capability to estimate such uncertainty the geostatistical approaches also have been applied to provide a stochastic function estimate of the historical distribution of contaminant with high computational efficiency by combining with an adjoint methodology michalak and kitanidis 2004a 2004b in depth reviews of the applications of inverse modeling methods for identifying groundwater contaminant sources can be found in work by sun 2013 and sun and sun 2015 in the bayesian approach for example the model input parameters and outputs are treated as random variables and their posterior distributions can be estimated through mcmc specifically the contaminant transport model must be run repetitively to obtain the target distribution as a result the computational cost is very high particularly for a high dimensional inverse problem which is prohibitive to address this issue surrogate based approaches have been proposed to substitute the original computational intensive simulations a surrogate model can approximate the relationship between model inputs and outputs and is cheaper to evaluate than the underlying physics based model thus instead of conducting a full forward modeling surrogate methods aim to estimate the response by establishing a map between the input and output parameters typical surrogates used in the field of contaminant source characterization include polynomial chaos expansions ciriello et al 2019 adaptive sparse grid interpolation zeng et al 2012 zhang et al 2015 and gaussian process zhang et al 2016 one of the disadvantages of such methods is that their performance degrades rapidly when the number of random input variables becomes large asher et al 2015 razavi et al 2012 traditional machine learning methods such as artificial neural networks singh and srivastava 2013 radial basis functions li et al 2003 support vector machine vojinovic and kecman 2004 wang and harrison 2012 kernel extreme learning machine jiang et al 2015 have also been implemented to capture the complex mappings between inputs and outputs however they could not provide an accurate prediction for highly nonlinear functional mapping with limited training data and have usually been applied in relatively simple flow and models recently deep learning have demonstrated great performance in many fields in particular those related to geo systems materials due to its robustness and powerful generalization capability ardabili et al 2019 bai and tahmasebi 2022 bergen et al 2019 kamrava et al 2021a 2021b shen 2018 tahmasebi et al 2020 compared to traditional machine learning models they have greater capabilities to approximate highly nonlinear mapping functions as they take advantage of large depth architecture miikkulainen et al 2019 hierarchical feature extraction principle lecun and yoshua 1998 and selective information storage mechanism greff et al 2017 the optimization algorithm namely the minibatch stochastic gradient descent can reach an acceptable balance between robustness and efficiency for training large scale deep learning model goyal et al 2017 previous works have successfully utilized deep learning models to characterize groundwater contamination mo et al 2019 developed a dense convolutional autoencoder network to learn the mapping function between highly heterogeneous hydraulic conductivity and contaminant concentration and proposed an autoregressive strategy to consider the effect of a time varying source strength similarly zhou and tartakovsky 2020 developed a deep convolutional neural network as a surrogate to replace the process based contaminant transport model and thus accelerate the mcmc simulation similarly and in a different field autoencoder is integrated with a autoregressive strategy to better link the time steps and provide more coherent predictions jiang et al 2021 autoencoder has also been used with a long short term memory lstm through which a more robust connection between time steps is established kamrava et al 2021b on the other hand physics informed neural networks can also be used as a robust and efficient surrogate model with limited training realizations by incorporating the governing equations of contaminant transport into the loss function he et al 2021 although the above studies obtained satisfying results their predictions are only available at a limited number of time steps usually less than 20 and the details between each of them are not provided one can possibly increase the number of time steps which can result in a much higher computational time and stronger error propagation on the other hand li et al 2021 developed an lstm surrogate to identify the release history of a contaminant source this lstm model due to the effectiveness of this method is dealing with time series data could generate consecutive predictions over the entire simulation period this method however is limited to only the evolution of contaminant concentration in the observation well and does not offer a full visualization of the simulation domain moreover all these works made the predictions completely depending on the most recent time steps to provide information regarding to the current and past state of the system alternatively transformer vaswani et al 2017 based on the self attention mechanism can learn a longer term dependencies without computationally inefficient recurrent connections used in lstm it is mainly designed for natural language processing nlp such as machine translation ahmed et al 2017 wang et al 2019 text classification gong et al 2020 shaheen et al 2020 and question answering shao et al 2019 zhao et al 2020 where a longer connection between the terms and elements should be established on the other hand the koopman operator has also gained a lot of attentions recently due to its powerful capability to infer the evolution of a nonlinear dynamic system that is either partially or completely unknown mamakoukas et al 2020 a data driven version of koopman operator has already been applied in neuroscience brunton et al 2016 fluid mechanics mezic 2013 and climate forecast hogg et al 2020 a combination of these two techniques has also been successfully used to predict various dynamic system geneva and zabaras 2020a in this paper inspired by the previous works and the successful application of transformer model and koopman operator for solving long term dependency problem and learning nonlinear dynamic system a deep learning based surrogate model is proposed and applied to identify the release history of groundwater contaminants this surrogate model consists of a koopman operator based convolutional autoencoder kcae and a transformer the kcae model projects the 2d contaminant concentration field at each time step into a 1d vector representation namely an embedding which is then processed by the transformer to learn the dynamics of this physical system with the trained surrogate model the groundwater contaminant concentration fields can be predicted and visualized at a greater number of time steps compared to the previous works whereas the accuracy is not decayed significantly 2 problem formulation 2 1 governing equations this study aims at developing a surrogate model to identify the release history of a contaminant source in a strongly heterogeneous groundwater system the governing equation for groundwater flow in a steady state regime is solved first to provide the flow field using 1 x i k i h x i 0 where ki lt 1 denotes the hydraulic conductivity tensor and h l is the hydraulic head in our case only the advection and dispersion terms are considered as the dominant mechanism for contaminant transportation and molecular diffusion is neglected as well thus the governing equation for contaminant transportation in groundwater flow can be reduced to 2 ϕ c t x i ϕ d i j c x j x i ϕ v i c q s c s where t t is time xi l is the spatial direction i 1 2 φ is the effective porosity c ml 3 is the contaminant concentration v i lt 1 is the groundwater flow velocity in the direction of xi qs t 1 is the volumetric flow rate per unit volume of aquifer representing fluid sources and sinks cs ml 3 is the concentration of the sources or sinks dij l2t 1 is the hydrodynamic dispersion coefficient tensor determined by v i α l l is longitudinal dispersivity and α t l is transverse dispersivity which all these three parameters are respectively defined by 3 d x x α l v x 2 α t v y 2 v d y y α l v y 2 α t v x 2 v d x y d y x α l α t v x v y v where vx and vy are the components of groundwater flow velocity and v is its magnitude the contaminant transport equation is related to the groundwater flow equation through the darcy s law 4 v i k i ϕ h x i the contaminant concentration can be obtained by solving above equations with appropriate initial and boundary conditions here they are numerically solved by the groundwater flow simulator modflow harbaugh 2005 and contaminant transport simulator mt3dms zheng et al 1999 respectively when the property and its conditions are uncertain the above governing equations become stochastic in this study we assume that the uncertainty only comes from the highly heterogeneous hydraulic conductivity which is considered to be a gaussian random field and the source strength initial and boundary conditions and other parameters like porosity and dispersivities are assumed to be known and constant as well to add more complexities the strength of contaminant source is assumed to vary with time eqs 1 19 2 2 iterative local updating ensemble smoother ilues in this work the iterative local updating ensemble smoother ilues developed by zhang et al 2018 is used as the inversion framework to complete the contaminant source identification process this algorithm takes the local updating strategy for ensemble samples along with a simple iterative process to solve nonlinear problems mathematically the model m n m 1 representing the uncertain parameters is related to the measured values d obs nd 1 and the response of the forward modeling f through following equation 5 d o b s f m ε where ε is the observed error model parameters m can be updated by ensemble smoother algorithm defined by 6 m j a m j f c m d f c d d f c d 1 d j f m j f j 1 n e where m f m 1 f m n e f is an ensemble of ne parameter samples obtained from the prior distribution m a m 1 a m n e a is the updated conditional ensemble c m d f is the cross variance matrix between m f and model responses d f f m 1 f f m n e f c d is the covariance matrix of the observed errors and c d d f is the autocovariance matrix of d f a local updating strategy for ensemble is also adopted when the prior or posterior distribution of m is multimodal the local ensemble for each sample can be constructed based on the ensemble index 7 j m j 1 m j 1 m a x j 2 m j 2 m a x where j 1 m is the distance between model response f m and the measured values d obs j 2 m is the distance between the model parameter m and the sample m j f j 1 m a x and j 2 m a x are the maximum values of j 1 m and j 2 m then the local ensemble of m j f is defined by nl αne α 0 1 samples with the nl smallest j values a simple iterative process is used to repeatedly apply this local updating strategy until the allowed maximum number of iterations is achieved more details about the ilues algorithm can be found in zhang et al 2018 3 methods 3 1 attention and transformer transformer models are designed for sequence to sequence modeling vaswani et al 2017 they completely depend on an attention mechanism to derive the global dependency between each word in a sequence the model receives an input sequence and projects it into two new sequences one word vector embedding for the convenience of numeric processing and another positional encoding to retain position information of each word see fig 1 then these two sequences are added up and passed through a series of encoders and decoders the encoder converts the input sequence to a new sequence where each element of it has an embedding that relies on all other elements the decoder is responsible for converting it back to a sequence of probabilities of different output words by a softmax function in the original transformer model vaswani et al 2017 the encoder consists of six identical blocks and each block has two layers a multi head attention layer and a feed forward layer in particular the multi head attention layer teaches the model how important other elements in the input sequence are for the embedding of a given element the architecture of decoder is almost similar to that of encoder except that another multi head attention layer is introduced into each block to compute the attention over the output of encoder note that the decoder works in an autoregressive manner where only previously processed elements are considered for predicting next element which is achieved by masking the subsequent target element and the following elements as zero the residual connection and layer normalization ba et al 2016 has also been applied in each layer to retain the information passed from previous layer unlike recurrent neural network rnn which sequentially processes each element in the sequence and usually retains little useful information about early elements the transformer can simultaneously calculate the attention over the entire input sequence although the gated design of lstm can resolve the long term dependency issue faced by rnn it still deals with each element sequentially moreover the parallel processing principle and multi head attention mechanism used in the transformer also allows it to take advantage of high performance computing devices such as gpu the following provides more details about the attention mechanism and positional encoding the attention mechanism has demonstrated a lot of improvements to neural machine translation which was firstly designed by bahdanau et al 2014 to address the dependencies between long source and target sentence particularly the scaled dot product attention mechanism shown in fig 1 b enables the sequence to sequence modeling to discard recurrent connections used in lstm where the long dependency between each element of the sequence is captured as a whole instead of sequentially deriving the dependency with previous elements based on past hidden states the first step is to get the target token query q and the source token key value k v pairs through linear transformations of the input features x x r d e n where n is the number of components in the input sequence and de is the embedding size 8 q w q x k w k x v w v x where w q r d q d e w k r d k d e w v r d v d e are learnable parameters note that dq is equal to dk in the original transformer model then a self attention mechanism is implemented with these projections of inputs it works like a fuzzy dictionary lookup where the similarity between current query and each of keys is evaluated through the dot product and a probability distribution is then assigned to all the keys via the softmax function finally a weighted sum of all the values is provided as the result for current query based on the calculated probability distribution 9 a t t e n t i o n q k v s o f t m a x q t k d k v t the denominator can ensure that the dot products do not grow too large otherwise a small gradient will be backpropagated from the softmax function this mechanism could guide the model to pay more attention to some specific sections of the embedding which may have more importance for dealing with current element of the sequence each self attention layer is referred as an attention head multi head attention which is shown in fig 1 b has been implemented in the transformer and each head has its corresponding learnable parameters wq wk wv this unit works like the multiple kernels used in convolutional neural networks to capture different features and hence it can calculate the attention for multiple positions in the sequence the output of each head with a size of n dv is concatenated horizontally and then pass through a linear transformation parameterized by wo w o r d o h d v where do hdv and h is the number of attention heads or self attention layers in addition to the attention mechanism a positional encoding was also implemented to consider the relative position of each component in the sequence this is especially important for attention mechanism since the same element may have different importance when its position varies it was implemented by adding the trigonometric function to the embedding 10 p e p o s 2 i sin p o s 10000 2 i d e p e p o s 2 i 1 cos p o s 10000 2 i d e where 2i and 2i 1 denotes the 2i th and 2i 1 th element of the embedding pos is the embedding s global position in the entire input sequence 3 2 the koopman operator since the input of transformer model usually is represented as a series of 1d vectors we also project the contaminant concentration field at each time step into a 1d embedding through a convolutional autoencoder cae model and the koopman operator is introduced as its middle latent variable to link the embedding with system dynamics such that the embedding at next time step predicted by transformer could be projected back to the original space with the decoder of cae model the koopman operator proposed by koopman 1931 koopman and neumann 1932 can project the state space of a nonlinear dynamical system to a observable space through which the system can be linearly propagated into the future let x t m r n denote the state of the system at time t a nonlinear discrete time dynamical system is considered and its evolution is described using 11 x t 1 f x t x t δ t x t t t δ t f x τ d τ where f m m is the evolution operator acting on a single state vector and m r n is the state space then an observation function g m r belonging to f is defined where f includes all the measurements also referred to as observables of the states and forms an infinite dimensional hilbert space the koopman operator k f f is a linear transformation defined as 12 k g x t δ g f x t g f x t g x t 1 where denotes the function composition of g and f thus the koopman operator can lift the original nonlinear dynamical system m t f to a new linear dynamical system f t k owing to the linearity nature of the defined function composition g f although this lifting operation makes the system state become more complex the system dynamics become simpler fig 2 shows how the koopman operator can convert the nonlinear dynamic system to a linear one the bottom path updates the system state x using evolution operator f while the top path evolves the outputs of observation function g f through koopman operator k these two paths are linked through the identity operator or full state observable g xt xt clearly one can solve a linear but infinite dimensional dynamic problem top path only if a finite dimensional approximation to the infinite dimensional koopman operator could be found a possible solution is to find a koopman invariant subspace of the observation function space takeishi et al 2017 however this requires identification of the essential observation functions that governs the underlying dynamics recently the data driven methods have proven to be useful for automatically determining the koopman invariant subspace by representing the observation functions with deep neural networks lusch et al 2018 otto and rowley 2019 this makes it possible to predict the evolution of system observables by repeatedly applying the koopman operator 13 g x t m k g x t m 1 k 2 g x t m 2 k m g x t where km represents a m fold function composition 3 3 proposed method the proposed deep learning based surrogate model has two major components a koopman operator based convolutional autoencoder kcae and a transformer the kcae model is trained prior to the transformer and then fixed to convert all the data into a 1d embedding space then the transformer is trained on this dimension reduced space to predict the embedding at next time step which is reconstructed back to the original space by the decoder of the kcae model specifically the kcae model projects the contaminant concentration field at each time step into a vector representation this is similar to the word embedding model as in word2vec each word is represented by a real valued vector and then the entire sentence is converted into a series of 1d vectors and processed by the transformer church 2017 the architecture of the kcae model follows a standard convolutional autoencoder which is shown in fig 3 here the encoder encodes the hydraulic conductivity field and the contaminant concentration field at a certain time step into an embedding vector encoder r t 2 l w r t d where t is the size of the context window or the number of time step considered in a single batch d is the dimension of embedding vector l and w are the length and width of the simulation domain respectively and the decoder reconstructs the concentration field from the embedding decoder r t d r t 1 l w the size of 1d embedding vector is set as 128 i e d 128 the detailed architecture of the kcae model is shown in table 1 where the values of kernel size stride and padding for each convolutional layer are also provided note that the hydraulic conductivity also consists of part of the inputs since it is the primary uncertainty source that could affect the contaminant concentration distribution in our case to enable the decoder to learn the reconstruction process from the predicted embedding a koopman operator k is introduced as the middle latent variable for data in a single batch with t time steps the process to train the kcae model is described as follows the encoder first converts the contaminant concentration field at the first time step x 1 into a 1d embedding vector observable g x 1 encoder x 1 which is then stored and mapped back to the original space through the decoder x 1 d e c o d e r g x 1 starting from the second time step expect for reconstructing the concentration field from the true embedding g xt the decoder could also recreate it based on the estimated embedding g x t for example the contaminant concentration field x 2 is first reconstructed based on the embedding g x 2 then g x 2 could be estimated from the stored embedding at last time step g x 1 with the koopman operator k g x 2 k g x 1 the decoder recreates the concentration field x 2 from this estimated embedding g x 2 x 2 d e c o d e r g x 2 although both outputs are the reconstruction of the original contaminant concentration field they are implemented based on different perspectives one is for learning the compression process and the other is for learning the reconstruction from the embedding predicted by the transformer such a procedure is repeated for remaining t 2 time steps the process of recreating the concentration field from the estimated embedding g x t t 2 is starting from the second time step since no embedding at previous time step i e g x 0 exists to estimate g x 1 note that we manually restrict the form of the koopman operator which is defined as a sparse banded matrix this additional constraint could improve the model s performance in terms of eliminating the overfitting issue caused by high frequency noise and outliers bruder et al 2019 kaiser et al 2017 and demanding less memory to store the following loss function is defined to train the kcae model 14 l k c a e i 1 n j 1 t α m s e x i j x i j β m s e x i j x i j γ k 2 2 x i j d e c o d e r e n c o d e r x i j x i j d e c o d e r k j 1 e n c o d e r x i 1 where n is number of training batches t is number of consecutive time steps in a single batch α β and γ are hyperparameters the loss function respectively consists of reconstruction loss dynamics loss and a l2 norm based regularization on the defined koopman operator to prevent overfitting the adam algorithm kingma and ba 2015 is used to optimize the model parameters with an initial learning rate of 0 001 besides an exponential scheduler is also applied to decay the learning rate with the hyperparameter gamma setting as 0 995 note that we set the values of hyperparameters α β and γ as 10000 1 and 0 1 respectively geneva and zabaras 2020a the main objective of the koopman operator is to link the reconstruction process with the learned transformer model rather than finding its finite dimensional approximation since it will not be used anymore once training of the kcae model is completed therefore we set a much smaller value for the weight of dynamic loss compared to the one for reconstruction loss every training example includes contaminant concentration fields at t time steps and each of them represents the physical state of system at the corresponding time as fig 4 shows each physical state is converted into the embedding g xt with the encoder of the trained kcae model like the original transformer model a trigonometric function based positional encoding shown in eq 7 is added to this physical embedding to represent the relative position of each obtained embedding in the input sequence since there is no recurrent connection implemented in the transformer model to maintain that information then this processed embedding is passed to the transformer model to sequentially predict the embedding at next time step finally the decoder of the trained kcae model is used to reconstruct the contaminant concentration field with the predicted embedding many variants of original transformer model have been developed such as gpt radford et al 2019 bert devlin et al 2018 transformer xl dai et al 2019 and xlnet yang et al 2019 the gpt model stands for generative pretrained transformer is used here to complete this task which only consists of a stack of transformer decoder with masked attention i e the encoder part of the original transformer model is unused its architecture is based on the implementation of gpt 2 by the hugging face transformer repository wolf et al 2019 but has a much smaller size compared to the transformer used for modern nlp the major consideration is that the autoregression nature of the transformer decoder is more suitable for learning and predicting the evolution of a dynamic system this inherent nature indeed has proven to be similar to the explicit linear time integration methods which has been widely used in deep learning to predict physical time series geneva and zabaras 2020b jiang et al 2021 kamrava et al 2021b sanchez gonzalez et al 2020 in detail each newly predicted embedding g x t is added to the input sequence which then becomes the new input of the model to get the prediction of g x t 1 initially the only active element of the input sequence is a start token s to remind the model to start working which is processed successively by all the transformer decoders to get the prediction of the first embedding i e g x 1 then this predicted embedding is added to the input sequence such that s g x 1 is used to get the prediction of second embedding g x 2 through the model this process is repeated until the prediction of last embedding note that the masked attention can prevent the decoder from cheating in terms of obtaining any information about next and following embeddings of the input sequence technically this is achieved by adding a mask to the dot products of query q and key k of future embeddings and enforcing them to have a value of negative infinity the resulted softmax scores also referred to as the weights of value v corresponding to future embeddings will be zero which means only information about previous embeddings will be used to calculate the attention a mean square error between the predicted embeddings and the corresponding targets is defined to train the transformer model using the adam optimization algorithm 15 l t r a n s f o r m e r i 1 n j 1 t m s e g x i j g x i j 4 results and discussion 4 1 case introduction a synthetic two dimensional numerical case study is used to demonstrate the accuracy and efficiency of proposed method as shown in fig 5 the considered domain has a size of 10 l 20 l and is uniformly discretized into 64 128 cells the left and right boundaries represent constant head with predefined heads of 9 l and 1 l respectively the upper and lower boundaries are assumed to be no flow boundaries there is one contaminant source and six observation wells in the area the contaminant concentration in each observation well is recorded as the simulation proceeds from 0 t to 16 t to add more temporal complexity the contaminant is released from 0 t to 8 t with a time varying strength furthermore the source strength is represented by the mass loading rate mt 1 which is the product of concentration ml 3 and flow rate l3t 1 table 2 lists the reference time varying source strength and their prior distribution from which the source strength used in the numerical simulation is sampled we assumed that the values of effective porosity longitudinal and transverse dispersivities are known and constant θ 0 25 α l 1 0 l and α t 0 1 l the hydraulic conductivity field is assumed to be a log gaussian random field described by 16 k x y exp g x y g x y n m x y c and the covariance matrix is parameterized by the matern covariance function 17 c x 1 y 1 x 2 y 2 σ 2 2 1 ν γ ν 2 ν r ν k ν 2 ν r r x 1 x 2 2 λ x 2 y 1 y 2 2 λ y 2 where m x y 2 is the mean σ2 0 5 is the variance and λx 4 l and λy 2 l are the correlation lengths in the x and y directions respectively 100 hydraulic conductivity fields were produced and then passed to the described forward modeling to generate the necessary training and test dataset the source strength applied for each conductivity field is randomly sampled from the prior distribution given in table 2 the contaminant concentration fields of 200 time steps were evenly sampled from each model note that the number of concentration fields we gathered from each numerical case is far more than that of previous works mo et al 2019 zhou and tartakovsky 2020 here 85 of the simulated numerical cases were used in the training dataset the training samples were sampled consecutively from each model with a stride of 1 and a context window size of 25 i e t 25 therefore there is 14 875 training samples in total the remaining numerical cases form the testing dataset to evaluate the accuracy of trained model the input of each sample consists of the conductivity field and the contaminant concentration fields for t consecutive time steps from xt to x t t 1 while the output is the contaminant concentration fields from x t 1 to x t t both of the kcae and the transformer models were trained for 100 epochs the performance of the surrogate is evaluated using the coefficient of determination r 2 and the root mean square error rmse which are given by 18 r j 2 1 i 1 n t e s t x i j x i j 2 2 i 1 n t e s t x i j x j 2 2 19 rms e j 1 n t e s t i 1 n t e s t x i j x i j 2 2 where n test is the number of samples in the testing set and x j 1 n t e s t i 1 n t e s t x i j higher r 2 score and lower rmse is always preferred 4 2 accuracy assessment fig 6 shows r 2 score and rmse of the predictions at each time step for all samples in the testing set which are shown by surrogate curves it can be seen that the proposed surrogate provides an accurate prediction of the concentration field over the entire time steps for example even at the last time step it achieves r 2 score of 0 9482 and rmse of 0 0363 respectively on the other hand the r 2 score decreases gradually with the proceeding of time while the rmse presents an opposite trend this is due to the autoregressive nature of the transformer where the errors originated from the prediction at previous time steps can be propagated and accumulated to the one at next time step however as mentioned the overall and final values of the above statistics are within an acceptable range the performance of the proposed surrogate model is further demonstrated in fig 7 which shows the predicted results of a random test sample with hydraulic conductivity field shown in fig 5 the applied source strength at each time period is 1 08 4 37 5 91 1 74 mt 1 respectively these results are also compared with those obtained using the numerical simulation and the differences between them are presented in the third column the movement of the contaminant plume is as expected controlled by the time varying source strength we just want to reemphasis that the contaminate does vary with time and is not injected during the entire simulation it is clear that the proposed surrogate method is capable of capturing the evolution of concentration field reasonably accurately although some errors are noticeable at the periphery of the contaminant plume this is possibly caused by compressing two dimensional concentration field into one dimensional embedding vector where part of the spatial information can be lost in this process overall however the general trend of the concentration field and the system dynamics are essentially reproduced note that for conciseness the concentration fields only five different time steps are selected to make this comparison otherwise the simulation is conducted over all proposed time steps the accuracy of the surrogate in predicting the evolution of contaminant distribution is further illustrated in fig 8 which performs a point wise comparison of the prediction and target models it is observed that most of the data points are evenly distributed around the 45 degree line similarly due to the errors automatically accumulated from the autoregressive predictions their degree of dispersion increases with time and the coefficient of determination is decreased from 0 9983 to 0 9528 however the general level of agreement at the last time step is still quite close which again indicates a high degree of accuracy in the predicted concentration fields on the other hand the contaminant concentration at each observation well is also of great importance since data of this type is sequentially integrated into the inverse problem the prediction accuracy of the contaminant concentration for the model shown in fig 7 at six observation wells is demonstrated in fig 9 the predictions at 200 time steps are connected linearly to form a continuous curve a great agreement between the results from numerical simulations and the proposed surrogate model can be seen such that the general trends and breakthrough time are predicted with reasonable accuracy which is essential if this surrogate is to be used for data assimilation there are however apparent but small discrepancies in the plots of some observation wells such as a relative larger prediction in the fourth observation well 4 3 inversion results to accelerate the convergence speed of the inversion method ilues the conductivity field is parameterized by the karhunen loeve expansion kle which is mainly controlled by the number of kle terms to preserve the field variance wang et al 2020 a value of 679 suggested by mo et al 2019 is used here to preserve 95 of the total field variance the uncertain source strength at each time periods is assumed to be uniformly distributed whose reference values and prior distributions are given in table 2 the contaminant concentration data of six observation wells see fig 5 at 40 time steps t 5 10 15 195 200 are collected and used for parameter estimation the ensemble size and the iteration number are set to be 5 000 and 20 respectively fig 10 compares the updating process of the source strength at different time periods when numerical simulator and the proposed surrogate is used as the forward model separately it can be seen that both results converge to the reference values indicating the effectiveness of developed surrogate for estimating the time varying contaminant source strength due to the parallel processing principle implemented in the transformer and the application of gpus for all training and testing process the proposed surrogate model can obtain a significant computational speedup it is worth noting that the surrogate based ilues approach uses the trained transformer based surrogate to perform the forward simulation i e we only need to train the surrogate once in the whole process which could substantially improve the computational efficiency of the inversion the training and prediction times of the surrogate is 64 min and 0 018 s respectively whereas the average computational time to solve a single example with the numerical methods is 2 94 s thus if 10 000 forward models are required to solve the considered inverse problem the inverse process can gain a roughly seven times speedup furthermore more computational time is expected to be saved if the forward model is used in 3d 4 4 comparison with the surrogate of convlstm to further demonstrate the effectiveness of the proposed surrogate model in learning long term dependency in a dynamic system a convolutional lstm convlstm model is trained on the same dataset for 400 epochs such model has shown to be able to capture both spatial and temporal features and has been successfully applied in modeling of subsurface systems and porous media kamrava et al 2021b tang et al 2021 2020 note that these works still make predictions at a limit number of time steps around 10 and are often combined with the convolutional autoencoder the lstm receives the most compressed feature map from the encoder and generates a sequence of feature maps representing the physical state at different time steps which are then decoded separately with the same decoder the architecture of our convlstm model is similar to that of the kcae model except that the koopman operator in the middle is replaced by the convlstm layer to produce a sequence of compressed state maps at 200 different time steps fig 6 also shows r 2 score and rmse of the predictions at each time step generated by the convlstm model it is clear that the performance of this model degrades much faster compared to the proposed surrogate as a lower r 2 score and higher rmse are achieved at the last time step fig 11 also compares the predictions of convlstm and the numerical method on the same example shown in fig 7 it can be seen that some unreasonable predictions happened at later time steps the trend of the contaminant diverges from the main plume and its tail even tends to move toward the upper right corner this is because the recurrent connection in the convlstm layer still sequentially process the physical state at each time step and can only selectively retain limited information from early periods while the transformer model can simultaneously obtain the relative importance of each element in the input sequence this again demonstrates the accuracy and robustness of the surrogate in capturing the temporal evolution of the system with long term dependencies 5 conclusion in this study a transformer based surrogate model was developed to identify the groundwater contaminant sources in a highly heterogeneous environment which often is recognized as a high dimensional inverse problem the proposed model has two major components a koopman operator based convolutional autoencoder kcae model is trained first to project the contaminant concentration fields into a one dimensional embedding space for the convenience of training the transformer model since its input usually is represented as a sequence of vectors the koopman operator is introduced to map the original nonlinear dynamic system to a new linear dynamic system and ease the learning of reconstruction process from the predicted embedding due to the inherent autoregressive nature of transformer decoder the evolution of contaminant concentration field can be predicted step by step even when dealing with a time varying source term the physical state of system at previous time step is converted into a vector embedding by the encoder of trained kcae model which is treated as input of the transformer model to predict the embedding at current time step then it is projected back to the original space with the decoder of the kcae model the performance of the proposed surrogate is demonstrated using a synthetic problem with highly heterogenous conductivity field the results show that our transformer based surrogate model can accurately approximate the complex relationship between high dimensional model inputs and outputs and estimate the evolution of nonlinear dynamic system the self attention mechanism used in the transformer model greatly improves the network s performance in learning the long term dependencies of a dynamic system finally the multi gpu based parallel computation also enables the surrogate model to gain a significant computational speedup note that the proposed surrogate can only identify the contaminant release history but source locations can be unknown as well one potential solution informed by the work of mo et al 2019 is to explicitly represent the source location as a binary image and constitute a part of the inputs of the model our results on this important extension will be reported in our future work credit authorship contribution statement tao bai methodology software writing original draft pejman tahmasebi conceptualization supervision writing original draft funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the financial support from the university of wyoming the school of energy resources for this research is greatly acknowledged the authors would like to thank the three anonymous reviewers for their constructive comments to improve the paper the codes and data used to produce the results are available at the website https github com tbaiwyo groundwater contaminant identification with transformer 
161,underground hydrogen storage uhs is an attractive technology for large scale twh renewable energy storage to ensure the safety and efficiency of the uhs it is crucial to quantify the h 2 interactions with the reservoir fluids and rocks across scales including the micro scale this paper reports the experimental measurements of advancing and receding contact angles for different channel widths for a h2 water system at p 10 bar and t 20 c using a microfluidic chip to analyse the characteristics of the h2 flow in straight pore throats the network is designed such that it holds several straight channels more specifically the width of the microchannels range between 50 μ m and 130 μ m for the drainage experiments h2 is injected into a fully water saturated system while for the imbibition tests water is injected into a fully h2 saturated system for both scenarios high resolution images are captured starting the introduction of the new phase into the system allowing for fully dynamic transport analyses for better insights n2 water and co2 water flows were also analysed and compared with h2 water results indicate strong water wet conditions with h 2 water advancing and receding contact angles of respectively 13 39 and 6 23 it was found that the contact angles decrease with increasing channel widths the receding contact angle measured in the 50 μ m channel agrees well with the results presented in the literature by conducting a core flood test for a sandstone rock furthermore the n2 water and co2 water systems showed similar characteristics as the h 2 water system in addition to the important characterization of the dynamic wettability the results are also crucially important for accurate construction of pore scale simulators graphical abstract keywords underground hydrogen storage dynamic contact angle wettability microfluidics hysteresis 1 introduction the contribution of renewable energy especially wind and solar in the future global energy mix is expected to increase significantly mackay 2008 the intermittent nature of these energy resources makes the development of large scale twh energy storage facilities an essential component of future green energy systems heinemann et al 2021 hydrogen h 2 is considered an attractive energy carrier because of its high energy content per mass and its clean combustion products however because of its low density being the tiniest molecule surface based storage facilities do not offer the volumes required for large scale twh energy storage geological formations such as depleted oil and gas reservoirs aquifers and salt caverns have proven to provide safe storage options for gasses such as methane and carbon dioxide and could also offer potential solutions for hydrogen storage hashemi et al 2021a ali et al 2021 zivar et al 2020 heinemann et al 2021 the feasibility of underground hydrogen storage in porous reservoirs highly depends on the flow and transport behaviour of hydrogen during subsequent injection and withdrawal cycles in the reservoir which is governed by complex pore scale processes hashemi et al 2021b rücker et al 2019 kunz et al 2018 pan et al 2021 carden and paterson 1979 heinemann et al 2021 a common approach to investigate the impact of pore scale processes on continuum scale behaviour as well as to derive meaningful continuum scale transport parameters such as relative permeability and capillary pressure is pore network modelling blunt et al 2002 joekar niasar and hassanizadeh 2012 celia et al 1995 constantinides and payatakes 1996 oren et al 1998 patzek 2001 in pore network modelling the relevant physics at the pore scale e g contact angles and interfacial tension is explicitly taken into account however for pore network models to be successful representative models of the pore structure of the porous medium are required as well as accurate descriptions of the wettability of the system dong et al 2008 ryazanov et al 2009 øren and bakke 2003 the wettability can be then characterized by the contact angle between the h 2 brine rock interfaces section 2 1 fig 1 hashemi et al 2021b blunt 2017 bear 2013 the contact angle is a function of among other factors pore size and pore geometry behnoudfar et al 2022 rabbani et al 2016 2017 furthermore the contact angle can potentially be different during drainage and imbibition cycles even in tube like channels used in pore network systems this phenomena is known as the contact angle hysteresis contact angle hysteresis is the result of the pore structure heterogeneity chemical interactions between the fluid and solid rock surface as well as surface roughness behnoudfar et al 2022 sauer and carney 1990 tadmor 2004 contact angle hysteresis has a direct impact on the amount of residual and capillary trapped non wetting phase juanes et al 2006 in this case h 2 and the economic feasibility of uhs as the trapped h 2 cannot be produced from the reservoir contact angles can be directly measured experimentally using the captive bubble method hashemi et al 2021b saraji et al 2014 sessile drop method dickson et al 2006 espinoza and santamarina 2010 dalton et al 2020 capillary tubes li et al 2013 heshmati and piri 2014 al zaidi and fan 2018 castro et al 2018 tilted plate method arif et al 2017 wilhelmy plate method kim et al 2015 microfluidic chips jafari and jung 2017 castro et al 2018 and in situ μ ct measurements dalton et al 2020 sun et al 2020 alhammadi et al 2017 higgs et al 2022 of which μ ct measurements of the contact angles are most representative of local pore geometries morrow 1975 studied the dependence of the advancing and receding contact angles on intrinsic contact angles the intrinsic contact angles were measured on a smooth ptfe surface for different fluid pairs however importantly to note the corresponding advancing and receding contact angles were measured in roughened ptfe tubes under these conditions morrow 1975 showed a systematic dependency of the advancing and receding contact angles on the intrinsic contact angle note that on smooth surfaces no hysteresis was expected to be observed as such the advancing and receding contact angles for smoothed surfaces were assumed to be equal to the intrinsic contact angle it is therefore expected that in the relatively new pore scale simulation studies morrow s curves have been modified to obtain meaningful simulations hashemi et al 2021a in general there are only limited experimental data sets available for h 2 water contact angles in porous reservoirs moreover no direct measurements to quantify the effect of pore size channel width on this parameter have been reported furthermore no direct measurement of h 2 dynamic contact angles in tube like micro channels have been done the analyses of hydrogen transport properties in tube like channels sheds new lights on meaningful design of pore scale network simulation frameworks relevant to uhs in the literature some experimental studies to characterize hydrogen transport properties have been conducted yekta et al 2018 performed core flooding tests in which hydrogen was injected into a water saturated vosges sandstone rock to derive drainage relative permeability and capillary pressure curves the experiments were carried out to represent shallow 50 bar 20 c and deep 100 bar 45 c aquifers by combining the capillary pressure results with mercury injection capillary pressure micp measurements and using the young laplace scaling they found the receding contact angles of 21 6 and 34 9 for the first and second conditions respectively iglauer et al 2021 used the tilted plate experimental technique to determine advancing and receding contact angles for the h 2 brine quartz system the experiments were performed for a pressure range of 50 250 bar and a temperature range of 23 70 c a brine with a salinity of 100 000 ppm nacl was used they found that increasing pressure temperature and organic surface concentration increased the hydrogen wettability with contact angles ranging between 0 to maximum 50 hashemi et al 2021b 2022 performed direct static contact angle measurements for h 2 brine sandstone rock using a captive bubble cell device no correlations between the static contact angle and the pressure 20 100 bar temperature 20 50 c and salinity 0 50 000 ppm nacl of the brine were found intrinsic contact angles between 25 min to 45 max were found for a variety of tests most recently higgs et al 2022 determined wettability of h 2 brine quartz systems using captive bubble pendant drop and in situ 3d micro ct methods they found contact angles of 29 39 at pressures of 69 210 bar and salinities of 0 5000 ppm nacl consistent with the earlier findings hashemi et al 2021b in their results no conclusive impact of salinity or pressure was observed on the contact angle although water wet conditions were commonly found in all the past experiments a wide variation in the currently reported h 2 brine contact angle data exists this could possibly be explained by differences in the measurement techniques where pore size channel width could play a defining factor in order to provide valuable and missing wettability information for pore network models as well as to improve the scientific community s understanding of currently reported data on h 2 brine contact angles in this work we present a systematic study to measure h 2 brine contact angles for different channel widths in microfluidic systems for both drainage receding and imbibition advancing processes experimental microfluidic investigations can provide insight in the dynamics of pore scale processes porter et al 2015 karadimitriou and hassanizadeh 2012 and provide a bridge between the pore and the core scale karadimitriou et al 2019 microfluidic experiments at early stages were based on simple micromodels but later involved more complex network geometries porter et al 2015 the main limitations of most micromodels to represent actual subsurface systems however include their restriction to 2d networks with a uniform etch depth in the third dimension uniform surface chemistry and minimum channel width of 10 μ m kim et al 2012 there are a few examples of micromodels which resemble actual 3d rock systems more closely gunda et al 2011 song et al 2014 micromodels have widely been used for experiments on the fundamentals of multiphase flow godinez brizuela et al 2017 cheng et al 2004 transport karadimitriou et al 2016 2017 and wettability sharbatian et al 2018 kim et al 2012 chalbaud et al 2009 hu et al 2017 jafari and jung 2017 karadimitriou et al 2019 micromodels can for example be used to simulate drainage and imbibition processes and to study underlying processes like viscous or capillary fingering and snap off gutiérrez et al 2008 ferer et al 2004 zhang et al 2011 joekar niasar et al 2009 the complex pore geometries in real rocks make it difficult to systematically analyse the impact of factors such as pore size on the in situ contact angle measurements higgs et al 2022 alhammadi et al 2017 the advantage of using microfluidic chips is that the in situ dynamic contact angles can be measured in simplified systems but with channel widths that are representative of porous media in the subsurface formations however to the best of the authors knowledge no study has been reported in which micromodels are used to directly measure dynamic contact angles of h 2 brine systems the objective of this work is to characterize dynamic contact angles for different channel widths in a h 2 water glass microfluidic system in addition the experiments will be carried out for co 2 and n 2 to allow for the comparison with more commonly stored gasses in subsurface reservoirs the results reported in this study can directly be used to take into account the impact of pore size channel width on contact angle in pore scale modelling approaches such as the one performed by hashemi et al 2021a in order to find meaningful relative permeability and capillary pressure curves furthermore this work can be used to improve our understanding of the wide variation in the limited h 2 brine contact angle data that is currently reported the remainder of the paper is structured as follows first the experimental setup and procedure will be described followed by the method of image analysis then the results and their relevance for uhs will be provided and discussed finally concluding remarks will be presented 2 materials and methods in this study microfluidic chips are utilized to measure dynamic contact angles of gas solid liquid interfaces for the h 2 water n 2 water and co 2 water systems the experimental test groups and conditions are summarized in table 1 2 1 theory the wettability is a measure of the ability of a fluid to interact with a solid surface in combination with another fluid and can be represented by the contact angle the contact angle is defined as the angle that a two fluid interface makes with the solid surface a contact angle bigger than 90 defines the non wetting phase while a contact angle smaller than 90 defines the wetting phase blunt 2017 bear 2013 in cases where the interface is not moving the contact angle is static sca dynamic contact angles can be measured during drainage where the non wetting phase is displacing the wetting phase corresponding to the injection of hydrogen in the reservoir the contact angles measured during drainage are receding contact angles rca when the process is reversed and hydrogen is withdrawn from the reservoir the wetting phase is displacing the non wetting phase which is called imbibition the contact angles measured during imbibition are advancing contact angles aca an illustration is provided in fig 1 hysteresis is defined as the difference in contact angle during the drainage and imbibition phase of the process rapp 2016 when the surface is smooth and the fluids are at rest and free of polar impurities the static contact angle is a fundamental property of the system and is called the intrinsic contact angle ica morrow 1975 2 2 materials the microfluidic device used in the experiments consisted of a microchip 10 20 mm supplied by micronit company the material of the chip is borosilicate glass with a pattern of a random square network the channels have widths of 50 70 90 110 and 130 μ m measured in the widest part of the channel see fig 2 the width of the smallest channel is in the range of the most common pore sizes measured in berea and bentheimer sandstones hashemi et al 2021b anon 2022 the depth of the channels is 20 μ m the shape of the channels is near rectangular with edged sides at the bottom the permeability of the chip is 1 6 d which is comparable with the permeability found for bentheimer sandstone peksa et al 2015 an overview of the chip can be found in fig 2 h 2 n 2 and co 2 gas with respective purity of 99 99 99 7 and 99 7 were used degassed deionized water was used during the experiments for the co 2 experiments the water was pre saturated with co 2 because of the high solubility of co 2 in water h 2 and n 2 solubility in water is very low with no expected impact on the contact angle hashemi et al 2022 therefore the water used in these experiments was not pre equilibrated 2 3 experimental apparatus fig 3 provides a schematic overview of the microfluidic apparatus the microscope is a leica dmi8 dfc7000 and was used in combination with a lens with a magnifying factor of 10 videos of the experiments were taken with a leica dfc7000t camera with las software the frame rate of the videos was approximately 5 frames per second the resolution of the videos was 1920 1440 pixels with a pixel size of 0 75 0 75 μ m to prevent the chip from being contaminated multiple filters were incorporated in the apparatus the injected water and ethanol was filtered with a 0 2 μ m vici filter in addition there were two extra 0 5 μ m filters vici jour peek encased frits installed in the liquid and the gas lines two pumps were included in the setup a quizix qx6000 pump and a phd ultra 4400 programmable syringe pump harvard apparatus with a 250 μ l syringe the quizix pump was used to refill the highly accurate syringe pump which can deliver flow rates as small as a few nanolitres per minute the syringe pump was used for the injection into and the withdrawal from the microchip the valves installed swagelok were all 1 16 in size also the tubes with 0 25 mm inner diameter id with fep transparent material were used to connect the pumps to the chip the gas cylinder served to maintain the pressure the pressure was monitored by the quizix pump the experimental setup was limited to a pressure of 10 bar safety aspects are crucially important when working with h 2 therefore in this study the small volume of h 2 needed for the experiments was obtained from the gas regulator so that the gas cylinders could be closed during experiments the volume of the regulator is approximately 99 ml which is large enough to carry out several experiments but small enough to pose limited safety risks in a well ventilated lab the setup was calibrated against the existing literature data of jafari and jung 2017 in which the same chip was used to measure dynamic contact angles for the co 2 water system an important difference between the experimental apparatus of jafari and jung 2017 and the one used in this study is the presence of several filters the filters keep the system clean and avoid any impact of impurities on the contact angle measurements more details about the validation tests and the impact of impurities on contact angle measurements can be found in appendices a and c 2 4 experimental procedure in this study drainage and imbibition experiments are carried out as separate experiments the objective is to measure dynamic contact angles as a function of channel width for these two different displacement processes during the experiments videos were taken to capture the moving gas water interfaces within the channels the drainage and imbibition tests were repeated until a sufficient amount of measurements for each channel width could be derived from the videos a flow rate of 0 1 μ l min has been used during all experiments this corresponds to an interstitial velocity v of 1 0 4 m s a capillary number c a μ v σ of 1 0 8 was calculated using an interfacial tension σ of 72 9 mn m yan et al 2001 and a dynamic viscosity μ of 9 μ pa s yusibani et al 2011 which indicates a capillary dominated flow regime 2 4 1 cleaning procedure prior to each experiment the microchips were thoroughly cleaned the cleaning procedure involved rinsing the chips with 5 ml of filtered ethanol followed by flushing with filtered n2 until no liquid was visible in the chip the cleaning procedure was carried out at ambient pressure to remove the n 2 from the clean chip the system was flushed with the gas used in the experiment followed by thoroughly rinsing with deionized water 2 4 2 drainage experiments before the start of the drainage experiments the chips were saturated with deionized water at ambient pressure after which the system was pressurized to the experimental pressure of 10 bar however for the experiments where the deionized water was pre equilibrated with co 2 see table 1 the system was saturated at the experimental pressure of 10 bar to avoid exolution of dissolved co 2 from the water for these experiments back pressure regulators were installed at the outlets of valve 2 and 5 the drainage experiments were conducted by withdrawing water from the fully water saturated chips with a flow rate of 0 1 μ l min 2 4 3 imbibition experiments before the start of an imbibition test the chip was flushed with the gas of the experiment until the water inlet was completely filled with gas this forced the injected water during the imbibition experiment to flow through channels filled with gas instead of pre existing wet flow paths which was necessary to be able to make measurements in all channel widths the flushing was carried out at ambient pressure after which the pressure was increased to the experimental pressure of 10 bar except for the experiments in which the deionized water was equilibrated with co 2 for these experiments the flushing pressure was kept at 10 bar the imbibition experiments were conducted by injecting water with a flow rate of 0 1 μ l min into the microchip 2 5 image analysis to calculate the dynamic contact angles snapshots from videos of the moving gas water interfaces in different channel widths were captured throughout the entire chip snapshots were taken at locations where the interfaces met the following requirements 1 the interface is moving 2 the interface is not too close to the corners of the channels and 3 the meniscus snapshot is sharp furthermore only one snapshot per straight channel was taken to obtain a good representation of the whole microchip interfaces which met the three requirements were analysed using an in house code to process an image first it was converted to a grey scale format and the desired interface within a channel was cropped then by using the scale factor of the microscope and selecting two points on the channel walls the channel width was identified to detect the boundary of the meniscus the cropped section of the image was binarized and contact points in addition to the apex were identified the best polynomials on each half of the curvature were fitted by both minimizing the error at the contact points rmse and choosing a polynomial that optimally fits the meniscus darzi and park 2017 heiskanen et al 2008 all the steps of the image analysis are shown in fig 4 contact angles are found first by fitting a polynomial of order n on the image i e 1 z c 1 r n c 2 r n 1 c i r n i 1 c n 1 here c i coefficients are constants allowing the z curve to be found as a function of r naturally the derivative of z with respect to r reads 2 d z d r n c 1 r n 1 n 1 c 2 r n 2 c n as such the contact angle θ is found at the gas liquid solid contact point according to 3 θ π 2 t a n 1 d z d r as shown in fig 4 e the origin of the coordinate system was placed at the apex point the z axis is parallel to the flow direction and the r axis perpendicular to the flow direction therefore by solving eqs 1 2 and 3 at the contact points the contact angle θ can be found a validation of the image analysis code is provided in appendix b 3 results fig 5 a shows an overview image of a drainage experiment for the h 2 water system at the point of breakthrough the flow path is different during each experiment the small images on the right hand side of fig 5 show drainage and imbibition examples used for measurement of advancing and receding contact angles for channel width of 70 μ m and 110 μ m from left to right the columns show the results for h 2 n 2 and co 2 respectively no significant difference between the wettability behaviour of these three gasses can be seen however in some cases a difference in interface properties visibility of water films could be observed between advancing and receding contact angles the drainage and imbibition tests were repeated until a sufficient amount of contact angle measurements for each channel width were obtained for each of the gasses and for both drainage and imbibition the advancing and receding contact angles aca and rca for the h2 water n2 water and co2 water systems at a pressure of 10 1 bar and a temperature of 20 2 c test a b and c measured in channel widths varying from 50 to 130 μ m are presented in fig 6 the error bars represent the standard deviation of the measurements table 2 shows the number of measurements per measurement category based on this data fig 7 was created in order to compare the results of all channel widths gasses and for both drainage and imbibition the bars represent the mean values of the measurements shown in fig 6 and the error bars represent the standard deviation similar to the error bars in fig 6 the intrinsic contact angle of h2 water sandstone measured by hashemi et al 2021b p 20 bar and t 20 c is indicated by the horizontal dashed line h2 water receding contact angles of 6 23 and advancing contact angles of 13 39 were determined based on the microfluidic experiments the highest angles were measured in the smallest channels and the lowest angles were measured in the widest channels no significant difference in the receding contact angles was observed for the three different gases the wider spread of measurements of the advancing contact angles compared to the receding contact angles imply that the advancing contact angles are less reproducible the advancing contact angles measured for the h2 water system are larger 1 14 compared to n2 water and co2 water systems however the overlap in error bars shows that this difference is not evident and is within the experimental accuracy this is because the error bars represent the standard deviation of the measurements and the range in measurements is much larger than the measured difference between the gases the overlap in the error bars between advancing and receding contact angles for all cases studied indicates that there was no significant difference in the measured advancing contact angles and receding contact angles 4 discussion the microfluidic system used in these experiments is not an accurate representation of a subsurface system because of the uniform etch depth uniform surface chemistry simplified pore network of the chip and relatively low temperature and pressure however the objective of the experiments is to derive contact angles for different channel widths and the random network of the microfluidic chip used in the experiments allowed for this in order to find representative average values for each channel width measurements were taken throughout the entire chip taking into account a range of velocities and other effects like flow patterns this type of systematic analysis would have been difficult to carry out in real rocks with complex pore geometries the experiments provide valuable information about the impact of channel width on the contact angle which is needed for a basic understanding of tube flow and can be used to improve the wettability characterization in pore network modelling approaches pore network models can be applied to derive meaningful continuum scale transport parameters such as relative permeability and capillary pressure by correctly taking into account the pore scale physics these hysteretic transport parameters are important input parameters for reservoir scale simulations methods which are needed for the optimization of underground hydrogen storage as well as to ensure its safety underground hydrogen storage is relatively new compared to the much more mature technology of geological storage of co 2 understanding the similarities and differences between the h 2 water n 2 water and co 2 water system will help in the advancement of underground hydrogen storage which is another objective of this study because h 2 is very prone to leakage the setup was designed such that the least amount of connections and pumps were involved this resulted in a setup where during drainage instead of injecting hydrogen which is the most common approach during these type of experiments water was withdrawn in both approaches h 2 will first enter the pores with the lowest capillary entry pressure when the non wetting phase is injected the capillary pressure p c p n w p w is increased by increasing the pressure of the non wetting phase when the wetting phase is withdrawn as in our case the capillary pressure is increased by lowering the pressure in the wetting phase 4 1 characteristics of the interface during the drainage and imbibition experiments for h 2 water n 2 water and co 2 water different kind of interfaces were observed the main difference was in the existence or non existence of visible water films it is important to mention that the term water film is used for all cases where water was observed on the channel walls of gas filled channels water on the channel walls can be caused by both corner flow and actual water films but we will refer to it as water films as we are not able to distinguish between the two a water film is expected to be present in all cases hirasaki 1991 however in some of our cases the water film was so thin that it was not visible with the camera during drainage when the receding contact angles rca were measured water films were visible on both sides of the channel fig 8 d for the measurements of advancing contact angles aca during imbibition interfaces were observed both with and without visible water films figs 8 c 8 a and 8 b this was caused by the mainly dry channel walls at the start of the imbibition experiments during the experiment in some cases the walls of the channels were first covered with a water film before the interface arrived while in other cases this effect was not visible the differences in thickness of water films caused the spread of measurements to be bigger for advancing contact angles compared to receding contact angles due to the curved interface of the channel walls as well as the limitations of the microscope and camera used the thickness of the water film could not be measured fig 9 shows the type of interface with or without visible water film that was encountered at each advancing contact angle measurement for all three gasses it can be seen that in bigger channel widths fewer measurements are without water film since a water film can more easily enter the bigger channels furthermore higher values of advancing contact angles are mainly measured without a visible water film present the interfaces are very sensitive to contamination within the gas water glass system the differences in contact angle measurements between clean and impure systems as well as example images of the polluting particles can be found in appendix c 4 2 influence of channel width on contact angles according to behnoudfar et al 2022 the contact angle θ is besides the pore size r related to the radius of curvature of the interface r and the convergence angle of the pores ϕ by 4 θ c o s 1 r r ϕ because the microchip consisted of straight channels the convergence angle in our study is equal to zero using this equation it can be calculated that the average radius of curvature ranges from 27 to 65 μ m for the h 2 water receding contact angles and 32 to 67 μ m for the h 2 water advancing contact angles this shows that the channel width not only affects the contact angle but also the radius of curvature of the interface in dynamic systems the radius of curvature is impacted by the velocity in our system a wide range of velocities were measured for each channel width however no systematic relationship between velocity and contact angle was observed furthermore similar velocities were measured for each channel width which suggests that the dependency of contact angle on channel width was not due to differences in velocity further explanation can be found in appendix d the results show a relationship where dynamic contact angles increase with decreasing channel width similar trends have been observed in the literature for co 2 water system jafari and jung 2017 to theoretically explain the observed behaviour i e increasing contact angle with decreasing channel width two main situations were considered on the basis of 1 equal capillary pressure in all channel widths and 2 equal average velocity in all channel widths both approaches provide a trend between contact angles and channel width the trend resulting from the first approach i e equal capillary pressure in rectangular pores derived by joekar niasar et al 2009 and explained in eq d 1 appendix d does not fully match with our experimental observations however the trends found based on the second approach i e constant velocities as explained in eq d 8 appendix d matched our experimental observations pretty well this shows that for our specific setup and experimental condition unsteady state it is highly unlikely that the capillary pressure is constant across channel widths the scientific basis for the observed trend of our study therefore can be found by the fact that in average the velocities across the channel widths in our experimental condition are comparable 4 3 comparison between advancing and receding contact angles besides the spread of measurements mentioned in the previous section there was no significant difference found between the advancing and receding contact angles contact angle hysteresis the difference between advancing and receding contact angles is the result of pore structure heterogeneity chemical interactions between the fluid and solid rock surface as well as surface roughness behnoudfar et al 2022 sauer and carney 1990 tadmor 2004 morrow 1975 the microchips used were made of smooth pure borosilicate glass and although the chip consisted of different channel widths there were no tapered structures that could lead to snap off furthermore the fluids used in the experiments were very pure since they were filtered before injection into the chip no significant hysteresis is therefore expected surprisingly the results of the work of jafari and jung 2017 did show contact angle hysteresis for the co2 water system using the same chip we believe however that this is likely the result of impurities in the system this is discussed in detail in appendix c which shows that impurities can cause an increase in hysteresis of contact angles due to chemical structural imperfections the systematic dependency of the advancing and receding contact angles on the intrinsic contact angle that morrow 1975 found on roughened tubes was not observed in our measurements because of the smooth channel walls of the microchip used in this experiment 4 4 comparison of gases h 2 water n 2 water and co 2 water systems behaved similarly during drainage when the receding contact angles were measured however a larger spread of measurements was observed in h 2 advancing contact angles in comparison to n 2 and co 2 advancing contact angles the minimum values of contact angles measured are similar for the three gases while the maximum of the range of measurements of h 2 advancing contact angles is higher than n 2 and co 2 this can be explained by the fact that more contact angles without water films were observed in the case of h 2 resulting in higher contact angles the reason for this is currently unknown and will be part of our future work the similar wettability behaviour of co 2 and n 2 has previously been observed using core flooding experiments garing and benson 2019 al menhali et al 2015 the data on hydrogen wettability compared to other gases however is still limited note that the interfacial tension of h 2 water n 2 water and co 2 water at 10 bar and 25 c are respectively 72 9 71 4 and 65 7 mn m yan et al 2001 chow et al 2018 georgiadis et al 2010 these values are within 10 of each other the close interfacial tension values could provide insight on the comparable contact angles found in our experiments note also to the point that in a gas liquid interface the neighbouring molecules of the liquid are all in close proximity while the neighbouring molecules of the gas are relatively far away therefore the energy of a molecule at the interface is expected to be mainly determined by the fluid phase while the gas phase is expected to have little impact 4 5 h2 water contact angles compared with literature the bar plot of fig 10 compares the findings on h2 water dynamic contact angles both advancing and receding with the currently available literature data on dynamic contact angles measured for this system the dynamic contact angles for the h 2 water system measured with the microfluidic device of this study test a for channel widths ranging from 50 to 130 μ m are presented together with the h 2 brine quartz results of the tilted plate experiment of iglauer et al 2021 p 50 bar t 23 c and salinity 100 000 ppm referred to as tpm and the h 2 water sandstone core flooding experiment of yekta et al 2018 p 50 bar and t 20 c referred to as cf in addition the horizontal blue dashed line represents the intrinsic contact angle for the h 2 water sandstone found by the captive bubble experiments of hashemi et al 2021b p 20 bar and t 20 c referred to as cbc when comparing the results of iglauer et al 2021 with the results of our microfluidic experiment it can be seen that the results of iglauer et al 2021 are similar to the results of the 130 μ m channel the receding contact angles are the same the advancing contact angles however differ by 5 in both experimental methods the advancing contact angles are found to be less reproducible which could explain the difference the receding contact angle found by yekta et al 2018 is very similar to the receding contact angle measured in channels of 50 μ m in width with only 1 difference yekta et al 2018 used a vosges sandstone for the core flooding experiment it is possible that the common pore sizes in this sandstone are in the range of 50 μ m 5 conclusions prediction of rock wettability in contact with brine and hydrogen is crucial for modelling the displacement processes in underground hydrogen storage uhs this paper reports experimental measurements of the advancing and receding contact angles of h2 water n2 water and co2 water systems at p 10 bar and t 20 c using a microfluidic device the channel widths of the microfluidic chip ranged between 50 and 130 μ m and the size of the smallest channel corresponds to the range of most common pore sizes found in typical sandstones hashemi et al 2021a anon 2022 results of this study allow for more accurate design of pore scale pore network systems as they require dynamic contact angles as input data furthermore it can be used to understand the wide variation in the limited h 2 brine contact angle data which is currently reported the results indicate water wet conditions with h 2 water advancing contact angles ranging between 13 39 and receding contact angles between 6 23 the contact angles decreased with increasing channel widths the receding contact angle measured for the smallest channel width 50 μ m is in agreement with the receding contact angle determined by yekta et al 2018 on the vosges sandstone suggesting that this channel width could be representative of actual subsurface systems the n 2 water and co2 water systems showed similar behaviour to the h 2 water system and no significant differences were observed for the three different gases credit authorship contribution statement willemijn van rooijen methodology experiments writing original draft leila hashemi methodology experiments writing review editing maartje boon methodology experiments writing review editing rouhi farajzadeh methodology writing review editing hadi hajibeygi conceptualization methodology writing review editing grant acquiring declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements hadi hajibeygi willemijn van rooijen and maartje boon were sponsored by the dutch national science foundation nwo talent programme vidi project admire grant number 17509 we thank all admire members and its user committee for allowing us publish this paper groups members of darsim delft advanced reservoir simulation and admire adaptive dynamic multiscale integrated reservoir earth are acknowledged for useful discussions during the development of this work this study was conducted in the laboratory of geoscience and engineering at delft university of technology we gratefully thank the technical staff of the laboratory specially michiel slob we also thank dr sian jones for her help in the laboratory during designing the setup lastly authors would like to thank professor volkert van steijn of tu delft for fruitful discussions on gas liquid solid contact angle and interfacial tension appendix a validation of setup the setup was successfully calibrated against the existing literature data of jafari and jung 2017 they performed microfluidic measurements of advancing and receding contact angles of co 2 water at p 10 bar and t 21 c using a chip identical to the chip used in this research the cleaning method of jafari and jung was similar to our cleaning method in order to match the setup of jafari and jung 2017 for the first validation test v1 a setup without filters and valves was used however because knowledge was gained about the behaviour and sensitivities of the system appendix c two additional co 2 water tests v2 and v3 were performed with the regular setup presented in 2 3 fig 3 one test was done after saturating the water with co 2 by filling the pump cylinders with half co 2 and half pure water and leaving it overnight at a pressure of 20 bar the other test was done with pure water an overview of all validation tests and the test of jafari and jung can be found in table a 3 the results of the validation tests are shown in fig a 11 the orange bars represent the measurements of jafari and jung 2017 using a similar chip at p 10 bar and t 21 c the green yellow and purple bars represent respectively validation experiments v1 v2 and v3 all carried out at p 10 1 bar and t 20 2 c fig a 11 shows that the results of experiment v1 using a similar setup match the results of jafari and jung the deviation is within the experimental error fig a 11 also indicates a clear effect of the filters in the setup on the advancing contact angles when the system is not properly filtered presence of contaminants can alter the glass wettability resulting in an increase of advancing contact angles of up to 45 this effect occurs especially when no valves are used because the system has to be opened and closed for flushing before every experiment which allows dust to attach on the open wet ends of the tubes the effect on the receding contact angles is less evident receding contact angles in unfiltered systems can be up to 5 higher but this is still within the experimental error lastly when comparing the results of experiments v2 and v3 it can be seen that saturating the water with co 2 had no significant influence on the results this is in line with the finding that even though the water in experiment v1 was not saturated with co 2 the results are still in agreement with the experiment of jafari and jung the finding that saturating the water with a soluble gas like co 2 does not impact the contact angle confirms that pre equilibrating the water during experiments with much less soluble gases like h 2 and n 2 is not necessary which is in agreement with hashemi et al 2022 appendix b validation of image analysis the image analysis code was validated against the generated specific curvature using matlab with the known angles at the contact points the accuracy of the developed in house code is 4 which is shown in fig b 12 appendix c sensitivity analysis contamination it was found that surface contamination can alter the wettability of the system this was initially found when unfiltered ethanol was injected and the system became clearly less water wet in order to investigate the effect that presence of contaminants can have on the dynamic contact angles three experiments were performed for the first test chip 1 2 and 3 were used which were all injected with the unfiltered ethanol instead of the filtered ethanol during the cleaning procedure this resulted in severe contamination of the system furthermore the chips were heated up to 400 c in an unsuccessful attempt to clean the chip the second test was done using chip 4 this chip was cleaned with the filtered ethanol but some tests were done without the inline filters in the gas and water lines filters 2 and 3 in fig 3 which caused minor contamination the third test was done with the unused chip which was also cleaned with the filtered ethanol images of these chips can be seen in fig c 13 all chips were used for tests to investigate the influence of this contamination contact angles were only measured if there were no visible contamination particles in the vicinity of the interface fig c 14 shows the results of using different chips the red bars represent the results of chip 1 3 the green bars represent the results of chip 4 and the blue bars represent the results of chip 5 all experiments were carried out at p 10 1 bar and t 20 2 c fig c 14 indicates a clear effect of contamination on contact angles it is clear that in chip 1 2 and 3 significantly higher advancing and receding contact angles were measured compared to chip 4 and 5 the difference between the results of chip 4 and chip 5 is less evident the receding contact angles are very similar and the deviation are within the range of experimental error the difference in advancing contact angles is more significant but in most cases this is within experimental error as well it is remarkable that the cleanest chip chip 5 shows a wider range of advancing contact angles than chip 4 this is mainly because during imbibition experiments measurement of advancing contact angle in chip 4 almost no water films were observed while in chip 5 the system was more water wet because the chip was clean in this case some interfaces used for advancing contact angle measurement had water films while others did not have a visible water film like is shown in the images in fig 8 because a clear effect of contamination was found even though contact angles were not measured within the vicinity of visible pollution all main experiments were carried out with filters and only filtered fluids were injected furthermore the system was opened inside the filters minimally in order to prevent dust from coming into the system installing valves 2 and 5 fig 3 enabled this since the lines did not have to be disconnected for flushing when using valves the findings of this appendix are in line with the findings of the co2 experiments in appendix a appendix d sensitivity analysis velocity and channel width for one of the h 2 water drainage experiments the velocity of each specific interface was measured using imagej fig d 15 shows the velocity distribution corresponding to the measured receding contact angles for different channel widths no systematic relationship can be observed between receding contact angles and the velocity this could possibly be explained by the fact that these measurements are influenced by many factors like local flow velocity flow patterns image analysis image quality etc which results in a relatively wide range of values fig d 16 shows the average velocity of the receding contact angles of this test and the error bars represent the standard deviation of the measurements it can be observed that similar velocities were measured for each channel width which suggests that the dependency of contact angles on channel width was not due to differences in velocity instead one can fairly conclude that the average velocities across channel widths are in fact comparable the contact angles are expected to be influenced by the pore size behnoudfar et al 2022 rabbani et al 2016 2017 our results show a relationship where dynamic contact angles increase with decreasing channel width in agreement with the trend observed in the literature for co 2 jafari and jung 2017 multiple factors like the shape of the channels the capillary pressure and the velocity can influence the relationship between pore size and contact angles joekar niasar et al 2009 found that the capillary pressure in rectangular channels can be expressed as d 1 p c σ n w h w cos θ h w 2 cos 2 θ 4 h w π 4 θ 2 cos π 4 θ cos θ 4 π 4 θ 2 cos π 4 θ cos θ 1 where p c is the capillary pressure h is the channel depth and w is the channel width the capillary pressure was calculated for a reference channel width in this case 110 μ m using the measured contact angle θ if the capillary pressure is assumed to be constant in the whole chip because the regime is capillary dominated the relationship between contact angles and channel width can be obtained by using the capillary pressure of the reference channel width and calculating the contact angles for other channel widths based on that value however because we deal with a dynamic system where the capillary entry pressure needs be overcome in order for fluids to enter a channel the assumption that capillary pressure is equal in each channel width may not be valid from fig d 16 one can conclude that the average velocities are comparable across channel widths berthier et al 2015 proposed d 2 p c μ p w z λ s c v as the capillary pressure in rectangular channels where d 3 s c h w d 4 p w 2 w h d 5 λ ɛ q ɛ w h 2 d 6 ɛ h w d 7 q ɛ 1 3 64 π 5 ɛ tanh π 2 ɛ here μ is the viscosity and z is the distance from the inlet of the channel to the interface using the fact that in our experiments velocities across the channel widths are comparable v i v r e f and assuming that z is on average similar for each channel width the equation can be rewritten as d 8 p c r e f λ r e f s c r e f p w r e f p c i λ r i s c i p w i the left hand side term of this equations represents the velocity factor for the reference channel width the capillary pressure p c r e f is calculated using eq d 1 eq d 8 was used to derive contact angle values for the other channel widths using eq d 1 to calculate p c i the calculated contact angles represent the relationship between contact angle and channel width based on the assumption that velocity is equal in each channel width fig d 17 shows the average receding contact angles rca of h 2 and pure water measurements the triangles represent the expected relationship between contact angle and channel width assuming the velocity is equal for each channel width and the circles represent the values assuming the capillary pressure is equal for all channel widths for both relationships the contact angle of the channel width of 110 μ m was used as a reference point it can be observed that the relationship assuming equal velocity resembles the measurements more closely 
161,underground hydrogen storage uhs is an attractive technology for large scale twh renewable energy storage to ensure the safety and efficiency of the uhs it is crucial to quantify the h 2 interactions with the reservoir fluids and rocks across scales including the micro scale this paper reports the experimental measurements of advancing and receding contact angles for different channel widths for a h2 water system at p 10 bar and t 20 c using a microfluidic chip to analyse the characteristics of the h2 flow in straight pore throats the network is designed such that it holds several straight channels more specifically the width of the microchannels range between 50 μ m and 130 μ m for the drainage experiments h2 is injected into a fully water saturated system while for the imbibition tests water is injected into a fully h2 saturated system for both scenarios high resolution images are captured starting the introduction of the new phase into the system allowing for fully dynamic transport analyses for better insights n2 water and co2 water flows were also analysed and compared with h2 water results indicate strong water wet conditions with h 2 water advancing and receding contact angles of respectively 13 39 and 6 23 it was found that the contact angles decrease with increasing channel widths the receding contact angle measured in the 50 μ m channel agrees well with the results presented in the literature by conducting a core flood test for a sandstone rock furthermore the n2 water and co2 water systems showed similar characteristics as the h 2 water system in addition to the important characterization of the dynamic wettability the results are also crucially important for accurate construction of pore scale simulators graphical abstract keywords underground hydrogen storage dynamic contact angle wettability microfluidics hysteresis 1 introduction the contribution of renewable energy especially wind and solar in the future global energy mix is expected to increase significantly mackay 2008 the intermittent nature of these energy resources makes the development of large scale twh energy storage facilities an essential component of future green energy systems heinemann et al 2021 hydrogen h 2 is considered an attractive energy carrier because of its high energy content per mass and its clean combustion products however because of its low density being the tiniest molecule surface based storage facilities do not offer the volumes required for large scale twh energy storage geological formations such as depleted oil and gas reservoirs aquifers and salt caverns have proven to provide safe storage options for gasses such as methane and carbon dioxide and could also offer potential solutions for hydrogen storage hashemi et al 2021a ali et al 2021 zivar et al 2020 heinemann et al 2021 the feasibility of underground hydrogen storage in porous reservoirs highly depends on the flow and transport behaviour of hydrogen during subsequent injection and withdrawal cycles in the reservoir which is governed by complex pore scale processes hashemi et al 2021b rücker et al 2019 kunz et al 2018 pan et al 2021 carden and paterson 1979 heinemann et al 2021 a common approach to investigate the impact of pore scale processes on continuum scale behaviour as well as to derive meaningful continuum scale transport parameters such as relative permeability and capillary pressure is pore network modelling blunt et al 2002 joekar niasar and hassanizadeh 2012 celia et al 1995 constantinides and payatakes 1996 oren et al 1998 patzek 2001 in pore network modelling the relevant physics at the pore scale e g contact angles and interfacial tension is explicitly taken into account however for pore network models to be successful representative models of the pore structure of the porous medium are required as well as accurate descriptions of the wettability of the system dong et al 2008 ryazanov et al 2009 øren and bakke 2003 the wettability can be then characterized by the contact angle between the h 2 brine rock interfaces section 2 1 fig 1 hashemi et al 2021b blunt 2017 bear 2013 the contact angle is a function of among other factors pore size and pore geometry behnoudfar et al 2022 rabbani et al 2016 2017 furthermore the contact angle can potentially be different during drainage and imbibition cycles even in tube like channels used in pore network systems this phenomena is known as the contact angle hysteresis contact angle hysteresis is the result of the pore structure heterogeneity chemical interactions between the fluid and solid rock surface as well as surface roughness behnoudfar et al 2022 sauer and carney 1990 tadmor 2004 contact angle hysteresis has a direct impact on the amount of residual and capillary trapped non wetting phase juanes et al 2006 in this case h 2 and the economic feasibility of uhs as the trapped h 2 cannot be produced from the reservoir contact angles can be directly measured experimentally using the captive bubble method hashemi et al 2021b saraji et al 2014 sessile drop method dickson et al 2006 espinoza and santamarina 2010 dalton et al 2020 capillary tubes li et al 2013 heshmati and piri 2014 al zaidi and fan 2018 castro et al 2018 tilted plate method arif et al 2017 wilhelmy plate method kim et al 2015 microfluidic chips jafari and jung 2017 castro et al 2018 and in situ μ ct measurements dalton et al 2020 sun et al 2020 alhammadi et al 2017 higgs et al 2022 of which μ ct measurements of the contact angles are most representative of local pore geometries morrow 1975 studied the dependence of the advancing and receding contact angles on intrinsic contact angles the intrinsic contact angles were measured on a smooth ptfe surface for different fluid pairs however importantly to note the corresponding advancing and receding contact angles were measured in roughened ptfe tubes under these conditions morrow 1975 showed a systematic dependency of the advancing and receding contact angles on the intrinsic contact angle note that on smooth surfaces no hysteresis was expected to be observed as such the advancing and receding contact angles for smoothed surfaces were assumed to be equal to the intrinsic contact angle it is therefore expected that in the relatively new pore scale simulation studies morrow s curves have been modified to obtain meaningful simulations hashemi et al 2021a in general there are only limited experimental data sets available for h 2 water contact angles in porous reservoirs moreover no direct measurements to quantify the effect of pore size channel width on this parameter have been reported furthermore no direct measurement of h 2 dynamic contact angles in tube like micro channels have been done the analyses of hydrogen transport properties in tube like channels sheds new lights on meaningful design of pore scale network simulation frameworks relevant to uhs in the literature some experimental studies to characterize hydrogen transport properties have been conducted yekta et al 2018 performed core flooding tests in which hydrogen was injected into a water saturated vosges sandstone rock to derive drainage relative permeability and capillary pressure curves the experiments were carried out to represent shallow 50 bar 20 c and deep 100 bar 45 c aquifers by combining the capillary pressure results with mercury injection capillary pressure micp measurements and using the young laplace scaling they found the receding contact angles of 21 6 and 34 9 for the first and second conditions respectively iglauer et al 2021 used the tilted plate experimental technique to determine advancing and receding contact angles for the h 2 brine quartz system the experiments were performed for a pressure range of 50 250 bar and a temperature range of 23 70 c a brine with a salinity of 100 000 ppm nacl was used they found that increasing pressure temperature and organic surface concentration increased the hydrogen wettability with contact angles ranging between 0 to maximum 50 hashemi et al 2021b 2022 performed direct static contact angle measurements for h 2 brine sandstone rock using a captive bubble cell device no correlations between the static contact angle and the pressure 20 100 bar temperature 20 50 c and salinity 0 50 000 ppm nacl of the brine were found intrinsic contact angles between 25 min to 45 max were found for a variety of tests most recently higgs et al 2022 determined wettability of h 2 brine quartz systems using captive bubble pendant drop and in situ 3d micro ct methods they found contact angles of 29 39 at pressures of 69 210 bar and salinities of 0 5000 ppm nacl consistent with the earlier findings hashemi et al 2021b in their results no conclusive impact of salinity or pressure was observed on the contact angle although water wet conditions were commonly found in all the past experiments a wide variation in the currently reported h 2 brine contact angle data exists this could possibly be explained by differences in the measurement techniques where pore size channel width could play a defining factor in order to provide valuable and missing wettability information for pore network models as well as to improve the scientific community s understanding of currently reported data on h 2 brine contact angles in this work we present a systematic study to measure h 2 brine contact angles for different channel widths in microfluidic systems for both drainage receding and imbibition advancing processes experimental microfluidic investigations can provide insight in the dynamics of pore scale processes porter et al 2015 karadimitriou and hassanizadeh 2012 and provide a bridge between the pore and the core scale karadimitriou et al 2019 microfluidic experiments at early stages were based on simple micromodels but later involved more complex network geometries porter et al 2015 the main limitations of most micromodels to represent actual subsurface systems however include their restriction to 2d networks with a uniform etch depth in the third dimension uniform surface chemistry and minimum channel width of 10 μ m kim et al 2012 there are a few examples of micromodels which resemble actual 3d rock systems more closely gunda et al 2011 song et al 2014 micromodels have widely been used for experiments on the fundamentals of multiphase flow godinez brizuela et al 2017 cheng et al 2004 transport karadimitriou et al 2016 2017 and wettability sharbatian et al 2018 kim et al 2012 chalbaud et al 2009 hu et al 2017 jafari and jung 2017 karadimitriou et al 2019 micromodels can for example be used to simulate drainage and imbibition processes and to study underlying processes like viscous or capillary fingering and snap off gutiérrez et al 2008 ferer et al 2004 zhang et al 2011 joekar niasar et al 2009 the complex pore geometries in real rocks make it difficult to systematically analyse the impact of factors such as pore size on the in situ contact angle measurements higgs et al 2022 alhammadi et al 2017 the advantage of using microfluidic chips is that the in situ dynamic contact angles can be measured in simplified systems but with channel widths that are representative of porous media in the subsurface formations however to the best of the authors knowledge no study has been reported in which micromodels are used to directly measure dynamic contact angles of h 2 brine systems the objective of this work is to characterize dynamic contact angles for different channel widths in a h 2 water glass microfluidic system in addition the experiments will be carried out for co 2 and n 2 to allow for the comparison with more commonly stored gasses in subsurface reservoirs the results reported in this study can directly be used to take into account the impact of pore size channel width on contact angle in pore scale modelling approaches such as the one performed by hashemi et al 2021a in order to find meaningful relative permeability and capillary pressure curves furthermore this work can be used to improve our understanding of the wide variation in the limited h 2 brine contact angle data that is currently reported the remainder of the paper is structured as follows first the experimental setup and procedure will be described followed by the method of image analysis then the results and their relevance for uhs will be provided and discussed finally concluding remarks will be presented 2 materials and methods in this study microfluidic chips are utilized to measure dynamic contact angles of gas solid liquid interfaces for the h 2 water n 2 water and co 2 water systems the experimental test groups and conditions are summarized in table 1 2 1 theory the wettability is a measure of the ability of a fluid to interact with a solid surface in combination with another fluid and can be represented by the contact angle the contact angle is defined as the angle that a two fluid interface makes with the solid surface a contact angle bigger than 90 defines the non wetting phase while a contact angle smaller than 90 defines the wetting phase blunt 2017 bear 2013 in cases where the interface is not moving the contact angle is static sca dynamic contact angles can be measured during drainage where the non wetting phase is displacing the wetting phase corresponding to the injection of hydrogen in the reservoir the contact angles measured during drainage are receding contact angles rca when the process is reversed and hydrogen is withdrawn from the reservoir the wetting phase is displacing the non wetting phase which is called imbibition the contact angles measured during imbibition are advancing contact angles aca an illustration is provided in fig 1 hysteresis is defined as the difference in contact angle during the drainage and imbibition phase of the process rapp 2016 when the surface is smooth and the fluids are at rest and free of polar impurities the static contact angle is a fundamental property of the system and is called the intrinsic contact angle ica morrow 1975 2 2 materials the microfluidic device used in the experiments consisted of a microchip 10 20 mm supplied by micronit company the material of the chip is borosilicate glass with a pattern of a random square network the channels have widths of 50 70 90 110 and 130 μ m measured in the widest part of the channel see fig 2 the width of the smallest channel is in the range of the most common pore sizes measured in berea and bentheimer sandstones hashemi et al 2021b anon 2022 the depth of the channels is 20 μ m the shape of the channels is near rectangular with edged sides at the bottom the permeability of the chip is 1 6 d which is comparable with the permeability found for bentheimer sandstone peksa et al 2015 an overview of the chip can be found in fig 2 h 2 n 2 and co 2 gas with respective purity of 99 99 99 7 and 99 7 were used degassed deionized water was used during the experiments for the co 2 experiments the water was pre saturated with co 2 because of the high solubility of co 2 in water h 2 and n 2 solubility in water is very low with no expected impact on the contact angle hashemi et al 2022 therefore the water used in these experiments was not pre equilibrated 2 3 experimental apparatus fig 3 provides a schematic overview of the microfluidic apparatus the microscope is a leica dmi8 dfc7000 and was used in combination with a lens with a magnifying factor of 10 videos of the experiments were taken with a leica dfc7000t camera with las software the frame rate of the videos was approximately 5 frames per second the resolution of the videos was 1920 1440 pixels with a pixel size of 0 75 0 75 μ m to prevent the chip from being contaminated multiple filters were incorporated in the apparatus the injected water and ethanol was filtered with a 0 2 μ m vici filter in addition there were two extra 0 5 μ m filters vici jour peek encased frits installed in the liquid and the gas lines two pumps were included in the setup a quizix qx6000 pump and a phd ultra 4400 programmable syringe pump harvard apparatus with a 250 μ l syringe the quizix pump was used to refill the highly accurate syringe pump which can deliver flow rates as small as a few nanolitres per minute the syringe pump was used for the injection into and the withdrawal from the microchip the valves installed swagelok were all 1 16 in size also the tubes with 0 25 mm inner diameter id with fep transparent material were used to connect the pumps to the chip the gas cylinder served to maintain the pressure the pressure was monitored by the quizix pump the experimental setup was limited to a pressure of 10 bar safety aspects are crucially important when working with h 2 therefore in this study the small volume of h 2 needed for the experiments was obtained from the gas regulator so that the gas cylinders could be closed during experiments the volume of the regulator is approximately 99 ml which is large enough to carry out several experiments but small enough to pose limited safety risks in a well ventilated lab the setup was calibrated against the existing literature data of jafari and jung 2017 in which the same chip was used to measure dynamic contact angles for the co 2 water system an important difference between the experimental apparatus of jafari and jung 2017 and the one used in this study is the presence of several filters the filters keep the system clean and avoid any impact of impurities on the contact angle measurements more details about the validation tests and the impact of impurities on contact angle measurements can be found in appendices a and c 2 4 experimental procedure in this study drainage and imbibition experiments are carried out as separate experiments the objective is to measure dynamic contact angles as a function of channel width for these two different displacement processes during the experiments videos were taken to capture the moving gas water interfaces within the channels the drainage and imbibition tests were repeated until a sufficient amount of measurements for each channel width could be derived from the videos a flow rate of 0 1 μ l min has been used during all experiments this corresponds to an interstitial velocity v of 1 0 4 m s a capillary number c a μ v σ of 1 0 8 was calculated using an interfacial tension σ of 72 9 mn m yan et al 2001 and a dynamic viscosity μ of 9 μ pa s yusibani et al 2011 which indicates a capillary dominated flow regime 2 4 1 cleaning procedure prior to each experiment the microchips were thoroughly cleaned the cleaning procedure involved rinsing the chips with 5 ml of filtered ethanol followed by flushing with filtered n2 until no liquid was visible in the chip the cleaning procedure was carried out at ambient pressure to remove the n 2 from the clean chip the system was flushed with the gas used in the experiment followed by thoroughly rinsing with deionized water 2 4 2 drainage experiments before the start of the drainage experiments the chips were saturated with deionized water at ambient pressure after which the system was pressurized to the experimental pressure of 10 bar however for the experiments where the deionized water was pre equilibrated with co 2 see table 1 the system was saturated at the experimental pressure of 10 bar to avoid exolution of dissolved co 2 from the water for these experiments back pressure regulators were installed at the outlets of valve 2 and 5 the drainage experiments were conducted by withdrawing water from the fully water saturated chips with a flow rate of 0 1 μ l min 2 4 3 imbibition experiments before the start of an imbibition test the chip was flushed with the gas of the experiment until the water inlet was completely filled with gas this forced the injected water during the imbibition experiment to flow through channels filled with gas instead of pre existing wet flow paths which was necessary to be able to make measurements in all channel widths the flushing was carried out at ambient pressure after which the pressure was increased to the experimental pressure of 10 bar except for the experiments in which the deionized water was equilibrated with co 2 for these experiments the flushing pressure was kept at 10 bar the imbibition experiments were conducted by injecting water with a flow rate of 0 1 μ l min into the microchip 2 5 image analysis to calculate the dynamic contact angles snapshots from videos of the moving gas water interfaces in different channel widths were captured throughout the entire chip snapshots were taken at locations where the interfaces met the following requirements 1 the interface is moving 2 the interface is not too close to the corners of the channels and 3 the meniscus snapshot is sharp furthermore only one snapshot per straight channel was taken to obtain a good representation of the whole microchip interfaces which met the three requirements were analysed using an in house code to process an image first it was converted to a grey scale format and the desired interface within a channel was cropped then by using the scale factor of the microscope and selecting two points on the channel walls the channel width was identified to detect the boundary of the meniscus the cropped section of the image was binarized and contact points in addition to the apex were identified the best polynomials on each half of the curvature were fitted by both minimizing the error at the contact points rmse and choosing a polynomial that optimally fits the meniscus darzi and park 2017 heiskanen et al 2008 all the steps of the image analysis are shown in fig 4 contact angles are found first by fitting a polynomial of order n on the image i e 1 z c 1 r n c 2 r n 1 c i r n i 1 c n 1 here c i coefficients are constants allowing the z curve to be found as a function of r naturally the derivative of z with respect to r reads 2 d z d r n c 1 r n 1 n 1 c 2 r n 2 c n as such the contact angle θ is found at the gas liquid solid contact point according to 3 θ π 2 t a n 1 d z d r as shown in fig 4 e the origin of the coordinate system was placed at the apex point the z axis is parallel to the flow direction and the r axis perpendicular to the flow direction therefore by solving eqs 1 2 and 3 at the contact points the contact angle θ can be found a validation of the image analysis code is provided in appendix b 3 results fig 5 a shows an overview image of a drainage experiment for the h 2 water system at the point of breakthrough the flow path is different during each experiment the small images on the right hand side of fig 5 show drainage and imbibition examples used for measurement of advancing and receding contact angles for channel width of 70 μ m and 110 μ m from left to right the columns show the results for h 2 n 2 and co 2 respectively no significant difference between the wettability behaviour of these three gasses can be seen however in some cases a difference in interface properties visibility of water films could be observed between advancing and receding contact angles the drainage and imbibition tests were repeated until a sufficient amount of contact angle measurements for each channel width were obtained for each of the gasses and for both drainage and imbibition the advancing and receding contact angles aca and rca for the h2 water n2 water and co2 water systems at a pressure of 10 1 bar and a temperature of 20 2 c test a b and c measured in channel widths varying from 50 to 130 μ m are presented in fig 6 the error bars represent the standard deviation of the measurements table 2 shows the number of measurements per measurement category based on this data fig 7 was created in order to compare the results of all channel widths gasses and for both drainage and imbibition the bars represent the mean values of the measurements shown in fig 6 and the error bars represent the standard deviation similar to the error bars in fig 6 the intrinsic contact angle of h2 water sandstone measured by hashemi et al 2021b p 20 bar and t 20 c is indicated by the horizontal dashed line h2 water receding contact angles of 6 23 and advancing contact angles of 13 39 were determined based on the microfluidic experiments the highest angles were measured in the smallest channels and the lowest angles were measured in the widest channels no significant difference in the receding contact angles was observed for the three different gases the wider spread of measurements of the advancing contact angles compared to the receding contact angles imply that the advancing contact angles are less reproducible the advancing contact angles measured for the h2 water system are larger 1 14 compared to n2 water and co2 water systems however the overlap in error bars shows that this difference is not evident and is within the experimental accuracy this is because the error bars represent the standard deviation of the measurements and the range in measurements is much larger than the measured difference between the gases the overlap in the error bars between advancing and receding contact angles for all cases studied indicates that there was no significant difference in the measured advancing contact angles and receding contact angles 4 discussion the microfluidic system used in these experiments is not an accurate representation of a subsurface system because of the uniform etch depth uniform surface chemistry simplified pore network of the chip and relatively low temperature and pressure however the objective of the experiments is to derive contact angles for different channel widths and the random network of the microfluidic chip used in the experiments allowed for this in order to find representative average values for each channel width measurements were taken throughout the entire chip taking into account a range of velocities and other effects like flow patterns this type of systematic analysis would have been difficult to carry out in real rocks with complex pore geometries the experiments provide valuable information about the impact of channel width on the contact angle which is needed for a basic understanding of tube flow and can be used to improve the wettability characterization in pore network modelling approaches pore network models can be applied to derive meaningful continuum scale transport parameters such as relative permeability and capillary pressure by correctly taking into account the pore scale physics these hysteretic transport parameters are important input parameters for reservoir scale simulations methods which are needed for the optimization of underground hydrogen storage as well as to ensure its safety underground hydrogen storage is relatively new compared to the much more mature technology of geological storage of co 2 understanding the similarities and differences between the h 2 water n 2 water and co 2 water system will help in the advancement of underground hydrogen storage which is another objective of this study because h 2 is very prone to leakage the setup was designed such that the least amount of connections and pumps were involved this resulted in a setup where during drainage instead of injecting hydrogen which is the most common approach during these type of experiments water was withdrawn in both approaches h 2 will first enter the pores with the lowest capillary entry pressure when the non wetting phase is injected the capillary pressure p c p n w p w is increased by increasing the pressure of the non wetting phase when the wetting phase is withdrawn as in our case the capillary pressure is increased by lowering the pressure in the wetting phase 4 1 characteristics of the interface during the drainage and imbibition experiments for h 2 water n 2 water and co 2 water different kind of interfaces were observed the main difference was in the existence or non existence of visible water films it is important to mention that the term water film is used for all cases where water was observed on the channel walls of gas filled channels water on the channel walls can be caused by both corner flow and actual water films but we will refer to it as water films as we are not able to distinguish between the two a water film is expected to be present in all cases hirasaki 1991 however in some of our cases the water film was so thin that it was not visible with the camera during drainage when the receding contact angles rca were measured water films were visible on both sides of the channel fig 8 d for the measurements of advancing contact angles aca during imbibition interfaces were observed both with and without visible water films figs 8 c 8 a and 8 b this was caused by the mainly dry channel walls at the start of the imbibition experiments during the experiment in some cases the walls of the channels were first covered with a water film before the interface arrived while in other cases this effect was not visible the differences in thickness of water films caused the spread of measurements to be bigger for advancing contact angles compared to receding contact angles due to the curved interface of the channel walls as well as the limitations of the microscope and camera used the thickness of the water film could not be measured fig 9 shows the type of interface with or without visible water film that was encountered at each advancing contact angle measurement for all three gasses it can be seen that in bigger channel widths fewer measurements are without water film since a water film can more easily enter the bigger channels furthermore higher values of advancing contact angles are mainly measured without a visible water film present the interfaces are very sensitive to contamination within the gas water glass system the differences in contact angle measurements between clean and impure systems as well as example images of the polluting particles can be found in appendix c 4 2 influence of channel width on contact angles according to behnoudfar et al 2022 the contact angle θ is besides the pore size r related to the radius of curvature of the interface r and the convergence angle of the pores ϕ by 4 θ c o s 1 r r ϕ because the microchip consisted of straight channels the convergence angle in our study is equal to zero using this equation it can be calculated that the average radius of curvature ranges from 27 to 65 μ m for the h 2 water receding contact angles and 32 to 67 μ m for the h 2 water advancing contact angles this shows that the channel width not only affects the contact angle but also the radius of curvature of the interface in dynamic systems the radius of curvature is impacted by the velocity in our system a wide range of velocities were measured for each channel width however no systematic relationship between velocity and contact angle was observed furthermore similar velocities were measured for each channel width which suggests that the dependency of contact angle on channel width was not due to differences in velocity further explanation can be found in appendix d the results show a relationship where dynamic contact angles increase with decreasing channel width similar trends have been observed in the literature for co 2 water system jafari and jung 2017 to theoretically explain the observed behaviour i e increasing contact angle with decreasing channel width two main situations were considered on the basis of 1 equal capillary pressure in all channel widths and 2 equal average velocity in all channel widths both approaches provide a trend between contact angles and channel width the trend resulting from the first approach i e equal capillary pressure in rectangular pores derived by joekar niasar et al 2009 and explained in eq d 1 appendix d does not fully match with our experimental observations however the trends found based on the second approach i e constant velocities as explained in eq d 8 appendix d matched our experimental observations pretty well this shows that for our specific setup and experimental condition unsteady state it is highly unlikely that the capillary pressure is constant across channel widths the scientific basis for the observed trend of our study therefore can be found by the fact that in average the velocities across the channel widths in our experimental condition are comparable 4 3 comparison between advancing and receding contact angles besides the spread of measurements mentioned in the previous section there was no significant difference found between the advancing and receding contact angles contact angle hysteresis the difference between advancing and receding contact angles is the result of pore structure heterogeneity chemical interactions between the fluid and solid rock surface as well as surface roughness behnoudfar et al 2022 sauer and carney 1990 tadmor 2004 morrow 1975 the microchips used were made of smooth pure borosilicate glass and although the chip consisted of different channel widths there were no tapered structures that could lead to snap off furthermore the fluids used in the experiments were very pure since they were filtered before injection into the chip no significant hysteresis is therefore expected surprisingly the results of the work of jafari and jung 2017 did show contact angle hysteresis for the co2 water system using the same chip we believe however that this is likely the result of impurities in the system this is discussed in detail in appendix c which shows that impurities can cause an increase in hysteresis of contact angles due to chemical structural imperfections the systematic dependency of the advancing and receding contact angles on the intrinsic contact angle that morrow 1975 found on roughened tubes was not observed in our measurements because of the smooth channel walls of the microchip used in this experiment 4 4 comparison of gases h 2 water n 2 water and co 2 water systems behaved similarly during drainage when the receding contact angles were measured however a larger spread of measurements was observed in h 2 advancing contact angles in comparison to n 2 and co 2 advancing contact angles the minimum values of contact angles measured are similar for the three gases while the maximum of the range of measurements of h 2 advancing contact angles is higher than n 2 and co 2 this can be explained by the fact that more contact angles without water films were observed in the case of h 2 resulting in higher contact angles the reason for this is currently unknown and will be part of our future work the similar wettability behaviour of co 2 and n 2 has previously been observed using core flooding experiments garing and benson 2019 al menhali et al 2015 the data on hydrogen wettability compared to other gases however is still limited note that the interfacial tension of h 2 water n 2 water and co 2 water at 10 bar and 25 c are respectively 72 9 71 4 and 65 7 mn m yan et al 2001 chow et al 2018 georgiadis et al 2010 these values are within 10 of each other the close interfacial tension values could provide insight on the comparable contact angles found in our experiments note also to the point that in a gas liquid interface the neighbouring molecules of the liquid are all in close proximity while the neighbouring molecules of the gas are relatively far away therefore the energy of a molecule at the interface is expected to be mainly determined by the fluid phase while the gas phase is expected to have little impact 4 5 h2 water contact angles compared with literature the bar plot of fig 10 compares the findings on h2 water dynamic contact angles both advancing and receding with the currently available literature data on dynamic contact angles measured for this system the dynamic contact angles for the h 2 water system measured with the microfluidic device of this study test a for channel widths ranging from 50 to 130 μ m are presented together with the h 2 brine quartz results of the tilted plate experiment of iglauer et al 2021 p 50 bar t 23 c and salinity 100 000 ppm referred to as tpm and the h 2 water sandstone core flooding experiment of yekta et al 2018 p 50 bar and t 20 c referred to as cf in addition the horizontal blue dashed line represents the intrinsic contact angle for the h 2 water sandstone found by the captive bubble experiments of hashemi et al 2021b p 20 bar and t 20 c referred to as cbc when comparing the results of iglauer et al 2021 with the results of our microfluidic experiment it can be seen that the results of iglauer et al 2021 are similar to the results of the 130 μ m channel the receding contact angles are the same the advancing contact angles however differ by 5 in both experimental methods the advancing contact angles are found to be less reproducible which could explain the difference the receding contact angle found by yekta et al 2018 is very similar to the receding contact angle measured in channels of 50 μ m in width with only 1 difference yekta et al 2018 used a vosges sandstone for the core flooding experiment it is possible that the common pore sizes in this sandstone are in the range of 50 μ m 5 conclusions prediction of rock wettability in contact with brine and hydrogen is crucial for modelling the displacement processes in underground hydrogen storage uhs this paper reports experimental measurements of the advancing and receding contact angles of h2 water n2 water and co2 water systems at p 10 bar and t 20 c using a microfluidic device the channel widths of the microfluidic chip ranged between 50 and 130 μ m and the size of the smallest channel corresponds to the range of most common pore sizes found in typical sandstones hashemi et al 2021a anon 2022 results of this study allow for more accurate design of pore scale pore network systems as they require dynamic contact angles as input data furthermore it can be used to understand the wide variation in the limited h 2 brine contact angle data which is currently reported the results indicate water wet conditions with h 2 water advancing contact angles ranging between 13 39 and receding contact angles between 6 23 the contact angles decreased with increasing channel widths the receding contact angle measured for the smallest channel width 50 μ m is in agreement with the receding contact angle determined by yekta et al 2018 on the vosges sandstone suggesting that this channel width could be representative of actual subsurface systems the n 2 water and co2 water systems showed similar behaviour to the h 2 water system and no significant differences were observed for the three different gases credit authorship contribution statement willemijn van rooijen methodology experiments writing original draft leila hashemi methodology experiments writing review editing maartje boon methodology experiments writing review editing rouhi farajzadeh methodology writing review editing hadi hajibeygi conceptualization methodology writing review editing grant acquiring declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements hadi hajibeygi willemijn van rooijen and maartje boon were sponsored by the dutch national science foundation nwo talent programme vidi project admire grant number 17509 we thank all admire members and its user committee for allowing us publish this paper groups members of darsim delft advanced reservoir simulation and admire adaptive dynamic multiscale integrated reservoir earth are acknowledged for useful discussions during the development of this work this study was conducted in the laboratory of geoscience and engineering at delft university of technology we gratefully thank the technical staff of the laboratory specially michiel slob we also thank dr sian jones for her help in the laboratory during designing the setup lastly authors would like to thank professor volkert van steijn of tu delft for fruitful discussions on gas liquid solid contact angle and interfacial tension appendix a validation of setup the setup was successfully calibrated against the existing literature data of jafari and jung 2017 they performed microfluidic measurements of advancing and receding contact angles of co 2 water at p 10 bar and t 21 c using a chip identical to the chip used in this research the cleaning method of jafari and jung was similar to our cleaning method in order to match the setup of jafari and jung 2017 for the first validation test v1 a setup without filters and valves was used however because knowledge was gained about the behaviour and sensitivities of the system appendix c two additional co 2 water tests v2 and v3 were performed with the regular setup presented in 2 3 fig 3 one test was done after saturating the water with co 2 by filling the pump cylinders with half co 2 and half pure water and leaving it overnight at a pressure of 20 bar the other test was done with pure water an overview of all validation tests and the test of jafari and jung can be found in table a 3 the results of the validation tests are shown in fig a 11 the orange bars represent the measurements of jafari and jung 2017 using a similar chip at p 10 bar and t 21 c the green yellow and purple bars represent respectively validation experiments v1 v2 and v3 all carried out at p 10 1 bar and t 20 2 c fig a 11 shows that the results of experiment v1 using a similar setup match the results of jafari and jung the deviation is within the experimental error fig a 11 also indicates a clear effect of the filters in the setup on the advancing contact angles when the system is not properly filtered presence of contaminants can alter the glass wettability resulting in an increase of advancing contact angles of up to 45 this effect occurs especially when no valves are used because the system has to be opened and closed for flushing before every experiment which allows dust to attach on the open wet ends of the tubes the effect on the receding contact angles is less evident receding contact angles in unfiltered systems can be up to 5 higher but this is still within the experimental error lastly when comparing the results of experiments v2 and v3 it can be seen that saturating the water with co 2 had no significant influence on the results this is in line with the finding that even though the water in experiment v1 was not saturated with co 2 the results are still in agreement with the experiment of jafari and jung the finding that saturating the water with a soluble gas like co 2 does not impact the contact angle confirms that pre equilibrating the water during experiments with much less soluble gases like h 2 and n 2 is not necessary which is in agreement with hashemi et al 2022 appendix b validation of image analysis the image analysis code was validated against the generated specific curvature using matlab with the known angles at the contact points the accuracy of the developed in house code is 4 which is shown in fig b 12 appendix c sensitivity analysis contamination it was found that surface contamination can alter the wettability of the system this was initially found when unfiltered ethanol was injected and the system became clearly less water wet in order to investigate the effect that presence of contaminants can have on the dynamic contact angles three experiments were performed for the first test chip 1 2 and 3 were used which were all injected with the unfiltered ethanol instead of the filtered ethanol during the cleaning procedure this resulted in severe contamination of the system furthermore the chips were heated up to 400 c in an unsuccessful attempt to clean the chip the second test was done using chip 4 this chip was cleaned with the filtered ethanol but some tests were done without the inline filters in the gas and water lines filters 2 and 3 in fig 3 which caused minor contamination the third test was done with the unused chip which was also cleaned with the filtered ethanol images of these chips can be seen in fig c 13 all chips were used for tests to investigate the influence of this contamination contact angles were only measured if there were no visible contamination particles in the vicinity of the interface fig c 14 shows the results of using different chips the red bars represent the results of chip 1 3 the green bars represent the results of chip 4 and the blue bars represent the results of chip 5 all experiments were carried out at p 10 1 bar and t 20 2 c fig c 14 indicates a clear effect of contamination on contact angles it is clear that in chip 1 2 and 3 significantly higher advancing and receding contact angles were measured compared to chip 4 and 5 the difference between the results of chip 4 and chip 5 is less evident the receding contact angles are very similar and the deviation are within the range of experimental error the difference in advancing contact angles is more significant but in most cases this is within experimental error as well it is remarkable that the cleanest chip chip 5 shows a wider range of advancing contact angles than chip 4 this is mainly because during imbibition experiments measurement of advancing contact angle in chip 4 almost no water films were observed while in chip 5 the system was more water wet because the chip was clean in this case some interfaces used for advancing contact angle measurement had water films while others did not have a visible water film like is shown in the images in fig 8 because a clear effect of contamination was found even though contact angles were not measured within the vicinity of visible pollution all main experiments were carried out with filters and only filtered fluids were injected furthermore the system was opened inside the filters minimally in order to prevent dust from coming into the system installing valves 2 and 5 fig 3 enabled this since the lines did not have to be disconnected for flushing when using valves the findings of this appendix are in line with the findings of the co2 experiments in appendix a appendix d sensitivity analysis velocity and channel width for one of the h 2 water drainage experiments the velocity of each specific interface was measured using imagej fig d 15 shows the velocity distribution corresponding to the measured receding contact angles for different channel widths no systematic relationship can be observed between receding contact angles and the velocity this could possibly be explained by the fact that these measurements are influenced by many factors like local flow velocity flow patterns image analysis image quality etc which results in a relatively wide range of values fig d 16 shows the average velocity of the receding contact angles of this test and the error bars represent the standard deviation of the measurements it can be observed that similar velocities were measured for each channel width which suggests that the dependency of contact angles on channel width was not due to differences in velocity instead one can fairly conclude that the average velocities across channel widths are in fact comparable the contact angles are expected to be influenced by the pore size behnoudfar et al 2022 rabbani et al 2016 2017 our results show a relationship where dynamic contact angles increase with decreasing channel width in agreement with the trend observed in the literature for co 2 jafari and jung 2017 multiple factors like the shape of the channels the capillary pressure and the velocity can influence the relationship between pore size and contact angles joekar niasar et al 2009 found that the capillary pressure in rectangular channels can be expressed as d 1 p c σ n w h w cos θ h w 2 cos 2 θ 4 h w π 4 θ 2 cos π 4 θ cos θ 4 π 4 θ 2 cos π 4 θ cos θ 1 where p c is the capillary pressure h is the channel depth and w is the channel width the capillary pressure was calculated for a reference channel width in this case 110 μ m using the measured contact angle θ if the capillary pressure is assumed to be constant in the whole chip because the regime is capillary dominated the relationship between contact angles and channel width can be obtained by using the capillary pressure of the reference channel width and calculating the contact angles for other channel widths based on that value however because we deal with a dynamic system where the capillary entry pressure needs be overcome in order for fluids to enter a channel the assumption that capillary pressure is equal in each channel width may not be valid from fig d 16 one can conclude that the average velocities are comparable across channel widths berthier et al 2015 proposed d 2 p c μ p w z λ s c v as the capillary pressure in rectangular channels where d 3 s c h w d 4 p w 2 w h d 5 λ ɛ q ɛ w h 2 d 6 ɛ h w d 7 q ɛ 1 3 64 π 5 ɛ tanh π 2 ɛ here μ is the viscosity and z is the distance from the inlet of the channel to the interface using the fact that in our experiments velocities across the channel widths are comparable v i v r e f and assuming that z is on average similar for each channel width the equation can be rewritten as d 8 p c r e f λ r e f s c r e f p w r e f p c i λ r i s c i p w i the left hand side term of this equations represents the velocity factor for the reference channel width the capillary pressure p c r e f is calculated using eq d 1 eq d 8 was used to derive contact angle values for the other channel widths using eq d 1 to calculate p c i the calculated contact angles represent the relationship between contact angle and channel width based on the assumption that velocity is equal in each channel width fig d 17 shows the average receding contact angles rca of h 2 and pure water measurements the triangles represent the expected relationship between contact angle and channel width assuming the velocity is equal for each channel width and the circles represent the values assuming the capillary pressure is equal for all channel widths for both relationships the contact angle of the channel width of 110 μ m was used as a reference point it can be observed that the relationship assuming equal velocity resembles the measurements more closely 
162,we present a novel approach to adaptive optimal design of groundwater surveys a methodology for choosing the location of the next monitoring well our dual weighted approach borrows ideas from bayesian optimisation and goal oriented error estimation to propose the next monitoring well given that some data is already available from existing wells our method is distinct from other optimal design strategies in that it does not rely on fisher information and it instead directly exploits the posterior uncertainty and the expected solution to a dual or adjoint problem to construct an acquisition function that optimally reduces the uncertainty in the model as a whole and some engineering quantity of interest in particular we demonstrate our approach in the context of 2d groundwater flow example and show that the dual weighted approach outperforms the baseline approach with respect to reducing the error in the posterior estimate of the quantity of interest keywords adaptive optimal design groundwater surveying uncertainty quantification bayesian inverse problems adjoint state equations 1 introduction in this paper we present a novel approach to optimally choosing the location of the next monitoring well when conducting a groundwater survey establishing a monitoring well is generally costly and depends on the specific geological context and the required penetration depth and choosing the most informative location for each well is a critical task when designing a groundwater survey groundwater surveying and modelling are intrinsically imbued with uncertainty and solutions and predictions are without exception non unique anderson et al 2015 hence in this paper we assume the perspective that a useful sampling location is one that most significantly reduces the uncertainty in the solution while simultaneously having a substantial influence on some quantity of interest qoi while multiple non invasive and relatively inexpensive methods for groundwater surveying exist loke et al 2013 saey et al 2015 auken et al 2017 2019 these methods all involve solving an inverse problem to reconstruct the hydraulic head which introduces an additional layer of uncertainty hence in this work we focus on the problem of determining aquifer characteristics from direct point measurements of hydraulic head and flux from monitoring wells and how to optimally choose the locations of such wells given existing data while the method is here contextualised within this particular problem it can easily be generalised to any setting where a continuous function and a derived qoi are approximated with point measurements in the classic theory of optimal design we often distinguish between optimality criteria that minimise the estimated parameter variances e g a d and e optimality and those that minimise the prediction variance e g g v and i optimality pukelsheim 2006 myers et al 2016 since in this study we are primarily concerned with the prediction variance the method presented here belongs in the latter category in this context our method can broadly be considered g optimal since our vanilla acquisition function targets the location of the highest posterior dispersion see e g myers et al 2016 however rather than iteratively searching for a design that maximises an optimality criterion we directly utilise a posterior dispersion estimate to construct an acquisition function we remark that while there are some abstract parallels between the method presented here and classic optimal design our method is probably better understood in the context of bayesian optimisation as discussed later additionally the classic optimal design approach is typically centred around the problem of choosing an experimental design that is optimal with respect to an optimality criterion before taking any measurements in this paper we take an adaptive approach and assume that some measurements are already available and we want to propose optimal new sampling locations given the data we already have how the initial measurement locations are optimally chosen is beyond the scope of this paper but we refer to e g cox and reid 2000 pukelsheim 2006 and myers et al 2016 for an extensive overview of optimal design of experiments we remark that our dual weighted method could in theory be employed to choose initial measurement locations but in that case the dispersion of the solution would be constrained only by the prior distribution of parameters and the constraints imposed by the constitutive equations in this case the method presented herein may be used in conjunction with some space filling design strategy or using local penalisation functions as described in section 2 3 2 however either of these workarounds would require an informed prior to work well we recycle the notion from classic optimal design that the information gain is driven by minimising the dispersion of a target distribution lindley 1956 however rather than integrating out all possible measurements and model parameters to find the utility of a given design we take a simpler approach namely we use a monte carlo estimate of the current posterior dispersion of the solution to a partial differential equation pde or some appropriate function thereof as an acquisition function the underlying rationale being that if we wish to know more about the distribution of our solution the most useful place to take a new sample is at the point of the highest posterior uncertainty in this context our vanilla approach see section 2 3 1 is similar to the maximum entropy approach to the optimal sensor placement problem shewry and wynn 1987 where sensors are added at the point of the highest uncertainty of some probabilistic function that is fitted to current sensor measurements for example a gaussian process gp emulator while this strategy will typically place many sensors at the boundaries of the sampling space in the context of adaptive gp fitting mohammadi et al 2021 this is not necessarily the case when targeting the uncertainty of the solution to a pde since that will be constrained by boundary conditions the sensor placement problem has been studied extensively in the context of gp emulators and multiple improvements to the maximum entropy approach have been made see e g krause et al 2008 beck and guillas 2016 and mohammadi et al 2021 however since our objective is to minimise the uncertainty of a pde derived qoi and not a gp emulator many of the recent developments are not immediately applicable since they are tailored for use with a gp emulator hence the vanilla approach presented herein can be considered a reformulation of the original maximum entropy approach particularly tailored for the probabilistic solution of a pde our method see section 2 3 borrows ideas from other fields not obviously related to classic optimal design first our adaptive optimal design approach is formulated in terms of an acquisition function a term typically associated with bayesian optimisation bo močkus 1989 frazier 2018 moreover our approach uses ideas from both prior guided bo souza et al 2021 and batch bo gonzalez et al 2016 the similarities with which are discussed in section 2 3 3 while in the context of bo the aim is to find the maximum or minimum of some function that is expensive to evaluate our objective is to simply reduce the uncertainty of our model predictions hence our vanilla acquisition function addresses solely the uncertainty of some target function and not the function value itself second our approach is inspired by the goal oriented error estimation used in mesh adaptation for pdes prudhomme and oden 1999 oden and prudhomme 2001 where the intention is to refine a mesh locally and parsimoniously to reduce the simulation error with respect to some qoi using an influence function that is the solution to an adjoint pde this approach however is most useful for forward problems where the domain and coefficients are well known and the groundwater flow problem is typically not of this kind instead we use the same approach of computing an influence function with respect to the qoi to determine not where the mesh should be refined but from where we need more data the idea of exploiting the adjoint or dual problem to minimise the posterior uncertainty with respect to a qoi was first explored by attia et al 2018 in a similar context as our model problem however there are several crucial differences between their approach and the one presented in this paper first their method is set in the classic optimal design context where a number of sampling locations are determined before taking any measurements based on the maximising the expected information gain according to some criterion derived from the fisher information matrix second since only a finite number of designs can be explored this way the prospective sampling locations are fixed to a relatively coarse grid finally the approach described in attia et al 2018 requires the adjoint operator to be linear an assumption which is suitable for only a subset of qois we employ markov chain monte carlo mcmc techniques see section 2 1 to generate samples from the posterior distribution of the model parameters given the data π θ d where the model parameters θ in this case describe hydraulic conductivity and the data d are point measurements of hydraulic head and flux see section 2 2 even if the model parameters themselves are of secondary interest to a given problem we can use the mcmc samples to construct monte carlo estimates of any parameter derived quantity or function such as the hydraulic flux across a boundary or the peak concentration of a contaminant at a well additionally unlike traditional inversion techniques mcmc allows for rigorously quantifying the uncertainty of the inverse problem which is useful in the context of engineering decision support systems in particular risk assessment studies we believe that there are many unexploited application opportunities tangential to the study of bayesian posteriors and demonstrate in this paper one such application fig 1 illustrates the proposed workflow at a high level where new wells are sequentially established at locations of high uncertainty and influence on a qoi as dictated by the acquisition function this paper is mainly concerned with the construction of optimal acquisition functions based on the posterior information which would be immediately available from quantifying the uncertainty of the bayesian inverse problem in the following sections we briefly summarise the theory of bayesian inverse problems mcmc and groundwater flow modelling we then outline the proposed methodology and demonstrate the effectiveness of methodology on a synthetic example we show that efficient acquisition functions can easily be constructed from information that would already be available from solving the bayesian inverse problem using mcmc the method avoids many of the complex calculations that are associated with classic optimal design and exploits information about the bayesian posterior in a direct and straightforward way 2 theory in this section we first briefly outline the framework of bayesian inverse problems and markov chain monte carlo mcmc a popular technique employed to draw samples from the bayesian posterior we then summarise the fundamentals of groundwater flow modelling for steady state groundwater flow in a confined aquifer using the finite element method fem finally we describe our novel approach to adaptive optimal design of groundwater surveys 2 1 bayesian inversion a bayesian inverse problem can be stated compactly as given some data d find the distribution π θ d with model parameters θ θ where θ is the parameter space so that 1 d f θ ε where f θ is the model output and ε is the measurement error which is typically assumed to be gaussian bayes theorem then states that 2 π θ d π p θ l d θ π d where π θ d is referred to as the posterior distribution π p θ is prior distribution encapsulating what we already know about our model parameters and l d θ is called the likelihood essentially a measure of misfit between the model output f θ and the data d while the so called evidence π d θ π p θ l d θ d θ is generally infeasible or impossible to determine in most real world scenarios various sampling techniques allows us to make statistical inferences from π θ d anyway examples include importance sampling is and markov chain monte carlo mcmc methods while these methods are not the object of this study a short summary of the main ideas of mcmc which is the specific method employed for inversion in this study is provided for completeness in mcmc we exploit that π d is constant and does not depend on the parameters θ we can therefore write 3 π θ d π p θ l d θ or equivalently for x y θ 4 π y d π x d π p y l d y π p x l d x we then introduce a transition kernel or proposal distribution q y x allowing us to transition from one state x to another y repeatedly applying the transition kernel q y x followed by an accept reject step prescribed by 5 we construct a markov chain where the samples after an initial burn in are precisely from the required distribution π θ d here burn in refers to the initial mcmc samples which are discarded since they may not be representative of the equilibrium distribution of the markov chain this procedure is described in the box below metropolis et al 1953 hastings 1970 gelman 2004 the acceptance probability eq 5 ensures that the algorithm is in detailed balance with the target posterior distribution π θ d see e g liu 2004 sec 5 3 for more details note that when the measurement error ε is gaussian ε n 0 σ ε which we assume in the experiment in section 3 then the unnormalised likelihood functional takes the following form 6 l d θ exp 1 2 f θ d t σ ε 1 f θ d in this study we employ a number of extensions to the metropolis hastings algorithm to speed up inference namely the delayed acceptance da christen and fox 2005 algorithm with finite subchains lykkegaard et al 2020 2022 also referred to as the surrogate transition method by liu 2004 the da algorithm exploits an approximate forward model or reduced order model rom f ˆ to filter mcmc proposals before evaluating them with the fully resolved forward model f resulting in a reduction in computational cost moreover we employ a state independent approximation error model aem to probabilistically correct for model reduction errors introduced by the approximate model as described by cui et al 2018 finally we use the adaptive metropolis am algorithm as the transition kernel haario et al 2001 in this work we used the open source da mcmc framework tinyda 1 1 https github com mikkelbue tinyda to perform the mcmc sampling 2 2 groundwater flow the groundwater flow equation for steady flow in a confined inhomogeneous aquifer occupying the domain ω with boundary γ can be written as the scalar elliptic partial differential equation 7 k x u x g x for all x ω subject to boundary conditions on γ γ d γ n with the constraints 8 u x u d x on γ d and k x u x n q n x on γ n here k x is the hydraulic conductivity u x is the hydraulic head g x are sources and sinks and γ d and γ n are boundaries with dirichlet and neumann conditions respectively see e g diersch 2014 if θ somehow parameterises the conductivity then we have k x k x θ this equation can be converted into the weak form by multiplying with a test function v h 0 1 ω and integrating by parts 9 ω v k x θ u d x γ n v q n x d s ω v g x d x v h 0 1 ω subject to the boundary condition u x u d x on γ d where h 0 1 ω is the hilbert space of weakly differentiable functions on ω which vanish on the dirichlet portion of the boundary γ d we approximate the solution u x in a finite element space v τ h 1 ω on a finite element mesh q τ ω defined by piecewise linear lagrange polynomials ϕ i x i 1 m associated with the m finite element nodes this can be rewritten as a sparse system of equations 10 a θ u b where a i j ω ϕ i x k x θ ϕ j x d x and 11 b i γ n ϕ i x q n x d s ω ϕ i x g x d x where a θ r m m is the global stiffness matrix and b r m is the load vector the solution to this system u u 1 u 2 u m r m represents the hydraulic head at each node which can be interpolated to the entire domain using the finite element shape functions u x i 1 m u i ϕ i x in our numerical experiments we used the open source high performance finite elements package fenics langtangen and logg 2016 to solve these equations 2 3 adaptive optimal design the overarching research question of this paper is this if we want to collect more data to reduce the variance in our posterior monte carlo estimates where in the modelling domain ω should we add a new borehole to maximise the expected value of the collected data more formally if we let t denote the current design of the survey so that d t and π t θ d t denote respectively the data and posterior distribution corresponding to that design we want to find the next sampling point x that constrains π t 1 θ d t 1 in an optimal way after setting d t 1 d t d t where d is the newly collected data at x 2 3 1 vanilla approach as outlined in section 2 1 bayesian inversion allows us to construct the posterior distribution of parameters given the data π t θ d t if the inversion was completed using mcmc and obtaining the model output f θ involved solving some partial differential equation with solution u x we can cache these solutions during sampling and would after sampling possess a set of pairs θ i u i x i 0 n since θ i i 0 n are distributed exactly according to π t θ d t so are any functions of θ such as u x here n is the number of mcmc samples after discarding the burn in hence we can easily obtain monte carlo estimates for e π t θ d t u x θ and d π t θ d t u x θ here d signifies some measure of statistical dispersion for example variance standard deviation or entropy we could in accordance with the maximum entropy approach shewry and wynn 1987 postulate that the accuracy of our inversion is driven by the dispersion in u x and hence we could solve the following optimisation problem 12 x arg max x ω d π t θ d t u x θ 2 3 2 dual weighted approach the simple approach outlined above will improve the general quality of u x but it is limited by the fact that it is not tailored for a particular quantity of interest q and this is where the dual weighted approach comes into play in this context rather than simply sampling from places with high uncertainty we aim to pick sampling points that also have a high expected influence on our quantity of interest q this is exactly the problem that adjoint or dual state methods aim to solve plessix 2006 suppose in a particular application we are interested in estimating a particular quantity of interest q u which we can write as a functional of the solution for example if our quantity of interest is the hydraulic head around a point x ω we could choose 13 q x u ω u x exp x x 2 λ d x for some sufficiently small length scale λ this however is a trivial problem since if the quantity of interest is the hydraulic head at some point we can just place our monitoring well at that point and measure it it would be much more useful to target a quantity of interest that we cannot measure directly hence in this study we consider flux over a boundary γ with the following functional 14 q γ u γ k x θ u x n d s the adjoint state equation associated with eq 14 is 15 k ω 0 subject to the boundary conditions ω d x 0 on γ d γ ω γ x 1 on γ q n ω x k x ω x n 0 on γ n the solution ω x is called the adjoint state or influence function please refer to sykes et al 1985 and theappendix for details on the derivation of the adjoint state equation and its associated boundary conditions integrating by parts and multiplying with a test function v h 0 1 ω we arrive at the weak form of the adjoint state equation 16 ω v k x θ ω d x γ n v q n ω x d s 0 v h 0 1 ω subject to boundary conditions ω d x 0 on γ d γ and ω γ x 1 on γ given some conductivity parameters θ 16 can be discretised using the same finite element grid as 10 leading to the following sparse system of equations 17 a θ ω b ω where a i j ω ϕ i x k x θ ϕ j x d x and 18 b ω i γ n ϕ i x q n ω x d s it is important to note here that the stiffness matrix a θ since the steady state groundwater flow equation is self adjoint is exactly the same as in eq 10 and the assembled system can hence be partially recycled when solving both equations however since the boundary conditions for the adjoint state equation are different than for the primal problem care must be taken when assembling the adjoint system of equations after solving this system of equations the influence function can be interpolated to the entire domain using our finite element shape functions ω x i 1 m ω i ϕ i x where ω ω 1 ω 2 ω m t the influence function is commonly interpreted as the sensitivity of the quantity of interest to a unit point source anywhere on the domain sykes et al 1985 wilson and metcalfe 1985 or in this particular case as the sensitivity of flow anywhere on the domain to the boundary condition broadly speaking the influence function directs us towards areas of the modelling domain with a potentially high influence on our quantity of interest which is what we required for our dual weighted approach we note that ω x is now a random function which depends on model parameters θ and we can obtain estimates for e π t θ d t ω x θ hence we propose the following acquisition function 19 x arg max x ω d π t θ d t u x θ e π t θ d t ω x θ where denotes the absolute value we use the absolute value of the expectation of the influence function to make sure that the weighting is always positive since ω x θ is not guaranteed to be positive in all scenarios we call this approach dual weighted since we are essential re weighting the dispersion d π t θ d t u x θ by the expected solution of the dual problem fig 2 illustrates the different steps in the proposed adaptive optimal design procedure 2 3 3 remarks 1 the dual weighted approach can be considered a hybrid between the goal oriented error estimation employed for mesh adaptation in the context of various expensive and mesh sensitive pde problems see e g prudhomme and oden 1999 and oden and prudhomme 2001 and bayesian optimisation bo typically used to optimise some unknown function approximated with sparse and or noisy data see e g močkus 1989 and frazier 2018 in this context our dual weighted approach could be framed as a form of prior guided bo souza et al 2021 where ω x broadly represents our prior belief that any point x constitutes a good sampling location however we remark that in our formulation ω x is not a probability distribution but a random weighting function 2 in the above formulations we have chosen the dispersion of the hydraulic head d π t θ d t u x θ as the function representing uncertainty in the model other sensible choices of uncertainty metrics would be the dispersion of the hydraulic conductivity d π t θ d t k x θ or of some norm of the flux d π t θ d t q x θ p 3 since sampling from π t θ d t can be computationally expensive it may be desirable to pick multiple new sampling locations at each step of the algorithm denote the number of new sampling locations in each such batch acquisition as n then this can be achieved by penalising the acquisition function by some local penalisation functions ψ x i x i 1 n 1 centred on the previous sampling points x i i 1 n 1 of the current batch as described in gonzalez et al 2016 this approach would yield the following dual weighted batch acquisition function for x i i 2 n 20 x i arg max x ω d π t θ d t u x θ e π t θ d t ω x θ j 1 i 1 ψ x j x similarly the batch acquisition function for the vanilla approach takes the form 21 x i arg max x ω d π t θ d t u x θ j 1 i 1 ψ x j x a reasonable choice of penalisation functions would be the gaussian 22 ψ x x 1 exp 1 2 x x 2 2 l ψ where l ψ controls the dispersion of the function and 2 is the l 2 norm using such a penalisation function the acquisition function would be exactly zero at previous sampling points from the current batch and smoothly rebound to eq 19 or eq 12 as the distance to previous sampling points increases 4 as mentioned earlier we formulate our method in the context of steady state groundwater flow in a confined aquifer while this is the most common approach to groundwater flow modelling it is naturally not exhaustive for a detailed analysis of the adjoint state equations for transient groundwater flow we refer the to e g sun 1999 and lu and vesselinov 2015 the unconfined case is considerably more complex since the constitutive equations are nonlinear while unconfined groundwater flow can under some assumptions be reasonably approximated by the constitutive equations for confined flow wang and anderson 1982 this is not always the case for a derivation and analysis of the adjoint equations pertaining to unconfined and coupled aquifers we refer to e g sun 1999 and neupauer and griebling 2012 5 note that the constitutive and adjoint equations are discretised using fem in the above section we restrict ourselves to this method for brevity but remark that the proposed acquisition functions eqs 12 19 20 and 21 are valid for any discretisation scheme also note that if piecewise linear shape functions are employed to approximate u x the maxima of the acquisition functions will occur at finite element nodes 3 example in this section we demonstrate the vanilla and dual weighted approach in the context of a synthetic groundwater flow example we first outline the model setup including the geological model and finite element representation we then explain the particular methodology for this example in detail finally we present the results 3 1 model setup we model the hydraulic conductivity as a log gaussian random field with a matern 3 2 covariance kernel 23 c x y 1 3 x y 2 l exp 3 x y 2 l where l is the length scale rasmussen and williams 2006 and 2 is the l 2 norm the resulting random field is expanded in an orthogonal eigenbasis with n kl karhunen loève kl eigenmodes to this end we construct a matrix of covariances between each pair of finite element nodes c r m m according to eq 23 so that c i j c x i x j this covariance matrix c is decomposed into the n kl largest eigenvalues λ i i 1 n kl and eigenvectors ψ i i 1 n kl the nodal conductivities k k 1 k 2 k m are then given by 24 log k μ σ ψ λ 1 2 θ with λ diag λ 1 λ 2 λ n kl and ψ ψ 1 ψ 2 ψ n kl the vector μ μ 1 is the mean of the log conductivity σ is the standard deviation of the log conductivity and θ n 0 i n kl dodwell et al 2015 when defined in this way the associated bayesian inverse problem involves exploring π θ d i e the posterior distribution of hydraulic conductivity parameters θ given measurements d where the aforementioned normal distribution constitutes the prior distribution of parameters π p θ n 0 i n kl we used three different models for the experiments fig 3 one data generating model representing the ground truth a fine forward model representing the fully resolved forward model f in the bayesian inverse problem see eq 1 and a coarse forward model corresponding to the reduced order forward model in the delayed acceptance mcmc sampler f ˆ as described in e g christen and fox 2005 liu 2004 cui et al 2018 lykkegaard et al 2020 and lykkegaard et al 2022 note that using the dual weighted approach described herein does not require a delayed acceptance mcmc sampler any method capable of producing monte carlo samples from the posterior will do the experiments were performed on a rectangular domain ω 0 2 0 1 meshed using a structured triangular grid with m f i n e 1326 degrees of freedom for the data generating model and the fine forward model and m c o a r s e 703 degrees of freedom for the coarse forward model for the data generating model the log gaussian random conductivity was truncated at n kl 256 kl eigenmodes while for the fine and coarse models it was truncated at n kl 128 hence the dimensionality of the inverse problem in these experiments was 128 which is very high and a challenging problem for any mcmc algorithm moreover we set l 0 1 μ 2 and σ 1 0 for every model this resulted in strongly anisotropic conductivity fields with log conductivities broadly between 5 and 1 fig 3 a we imposed fixed head dirichlet boundary conditions of 1 and 0 on the left and right boundaries respectively and no flow neumann conditions on the remaining top and bottom boundaries we set the right hand side of eq 7 to g x 0 we chose flux across the right boundary γ r as our quantity of interest q corresponding to the following functional as in eq 14 25 q u γ r k x θ u x n d s and the associated adjoint state equation shown in 15 with γ γ r fig 3 f shows an example of the influence function generated by this adjoint state equation the left column of fig 3 shows the conductivity associated with a random draw from the prior π p θ for the data generating model the fine model and the coarse model respectively the right column of fig 3 shows the corresponding hydraulic head flux and influence function for the data generating model 3 1 1 methodology using the above setup we completed a total of n 30 independent numerical experiments to demonstrate the feasibility of the dual weighted approach we chose the standard deviation of the l 2 norm of the flux s q x 2 as the general measure of uncertainty in the model for each independent experiment the following experimental procedure was observed 1 the hydraulic conductivity for the data generating model was initialised with a random draw from the prior and the primary problem was solved 2 eight observation wells were placed randomly on the domain by latin hypercube sampling mckay et al 1979 see fig 4 3 for each observation well x i the hydraulic head u x i and the norm of the flux q x i 2 were computed these head and flux observations were contaminated with white noise from ε u n 0 0 0 1 2 and ε q 2 n 0 0 00 1 2 respectively 4 delayed acceptance mcmc sampling was completed with 2 independent samplers each drawing n 25000 fine samples with a subsampling length of 5 see e g lykkegaard et al 2020 and lykkegaard et al 2022 and a burn in of n b u r n 5000 was discarded this resulted in a total number of mcmc samples of n 40000 for each experiment 5 the standard deviation of the l 2 norm of the flux s q x 2 and the mean of the influence function ω x were computed at the finite element nodes and interpolated to the entire domain using the finite element shape functions and eight new observation wells were placed according to the batch vanilla and dual weighted acquisition functions see eqs 20 and 21 fig 4 shows the vanilla and dual weighted acquisition functions for one sample of the n 30 models as expected the weighting function ω x prioritised observation wells closer to the boundary of the quantity of interest 6 data were extracted from the eight new observation wells as in step 3 and appended to the data vector 7 delayed acceptance mcmc sampling was repeated using the new data vectors for both the vanilla and dual weighted approaches for each experiment and each posterior distribution initial vanilla and dual weighted with each n 40000 posterior samples we computed the mean squared error mse and variance of the predicted quantity of interest q i i 1 n compared to the true value q t r u e the mse of the predicted value of the quantity of interest q i with respect to the true value q t r u e was computed as 26 mse 1 n i 1 n q t r u e q i 2 similarly the sample variance of q for each experiment was computed as 27 s 2 1 n 1 i 1 n q i q 2 finally we constructed gaussian kernel posterior density estimates f ˆ π θ d q from the posterior samples from each experiment q i i 1 n and computed the kernel density of the true value q t r u e with respect to this density estimate kernel density estimates were computed using scipy virtanen et al 2020 with automatic bandwidth determination scott 1992 3 1 2 results we compared the mse variance and kernel density of both the vanilla and dual weighted posterior samples with the corresponding values for the initial posterior samples for all n 30 experiments with respect to the mse the vanilla approach yielded a median reduction of 22 while the dual weighted approach yielded a median reduction of 30 fig 5 a this demonstrates that both acquisition strategies approach the true value when we add more datapoints but that the dual weighted approach is more efficient with respect to the variance of the quantity of interest the vanilla approach yielded a median reduction of 31 while the dual weighted approach yielded a median reduction of 34 fig 5 b this shows that for both acquisition strategies the posterior distribution contracts as more data is added and that the two approaches differ less with respect to this feature however this metric shows only that the posterior contracts and not if it moves closer to the true value finally we computed the posterior densities of the true quantity of interest with respect to kernel posterior density estimates f ˆ π θ d q for each experiment here the vanilla approach yielded a median improvement of 12 while the dual weighted approach yielded a median improvement of 17 since the prediction variance of the quantity of interest reduced in every experiment fig 5 b this again shows that the posterior distribution moves closer to the true value as more data is added but that the dual weighted approach is better we note that in neither method was capable of improving the posterior estimate of the quantity of interest for every experiment hence in 8 30 vanilla experiments and 5 30 dual weighted experiments adding additional wells resulted in a worse posterior mse than the initial one this is not surprising since we are dealing with a very ill posed inverse problem and any new datapoint may reinforce the initial bias rather than reduce it while both approaches occasionally failed to improve the posterior estimate the dual weighted approach performed better than the vanilla approach we computed the gaussian kernel density estimates of the error ɛ i q t r u e q i for two samples of the n 30 experiments the left panel shows a typical example where the vanilla approach resulted in a moderate improvement while the dual weighted approach yielded a more dramatic improvement the right panel shows an example where both the dual weighted and vanilla approaches failed to produce any improvement 4 discussion in this paper we have proposed a novel approach to the problem of optimally choosing the next location for a monitoring well given existing data and some quantity of interest qoi the proposed methodology exploits the solution of an adjoint problem to weigh such an acquisition function according to the expected influence on the qoi numerical experiments have demonstrated that the approach works for our model problem we emphasise that the problem is intrinsically probabilistic and hence subject to uncertainty we have demonstrated that the approach works on average for our model problem but there were certain experiments where the dual weighted acquisition strategy did not approach the true qoi see e g fig 6 b as the number of wells approach infinity the posterior distribution will certainly approach the true value but for any one new observation well there are no such guarantees in a sense the dual weighted approach merely increases the chance of improving the posterior distribution of the qoi while we formulated and demonstrated the approach in the context of a groundwater surveying problem the method could be applicable to other areas of science and engineering where measurements are expensive the most obvious parallel application is petroleum engineering where there are similarities both in terms of the constituent equations and the mode of sampling but the method could be adapted with little effort to any inverse problem where establishing sensors is expensive we note however that the dual problem in our case was unusually simple since the groundwater flow equation is self adjoint clearly the dual weighted approach can only be used as written for qois where an adjoint problem can be formulated and solved directly for more complicated qois an alternative approach would be to perturb the posterior mean or mode to approximate the influence function using such an approach would yield ω x e θ rather than e ω x θ as a weighting function a bottleneck of our approach is that the mcmc sampler is rerun after each batch data acquisition running mcmc for expensive forward models is notoriously computationally demanding and while we employ various tricks to reduce the cost such as delayed acceptance and proposal adaptivity this is not the most elegant approach one way to significantly alleviate the cost of subsequent posterior distributions would be to employ a particle filter to sequentially reweigh mcmc samples according to the new data chopin 2002 this sequential approach was investigated in this study but it did not work well mainly because of very high sample degeneracy when the variance of the solution as in our case is relatively high at unobserved locations only few posterior samples fit the new observations well with the mentioned sample degeneracy as a result moreover we found that the dispersion measures in eqs 12 19 20 and 21 where highly sensitive to this sample degeneracy this challenge could be alleviated by drawing more posterior samples for the initial mcmc but that would only offset the cost we remark that this approach might work better for lower dimensional problems than the one investigated in this study we highlight this problem as a potential target for future research the methodology was demonstrated empirically in the context of a synthetic groundwater flow example this gives rise to at least three additional interesting directions of future research first showing theoretically that the distribution of the quantity of interest does indeed converge faster to the true value when using the dual weighted approach and examining the mechanisms that govern this process in detail second testing the method in practice in the context of an actual groundwater survey while testing the method in practice would certainly expose limitations and complications that were not identified in this study it would be difficult to validate the method further in this fashion since the true value of the qoi is rarely known in reality this may be overcome by testing the method under controlled laboratory conditions third generalising the dual weighted approach to a wider range of pde problems with different constituent equations and qois credit authorship contribution statement mikkel b lykkegaard conceptualisation methodology software visualisation investigation analysis draft preparation writing editing tim j dodwell supervision conceptualisation methodology review declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the mcmc code used for delayed acceptance sampling can be found at https github com mikkelbue tinyda and additional code is available in the open research exeter data repository at http hdl handle net 10871 129475 the authors would like to thank the reviewers for helpful comments and suggestion and robert scheichl and karina koval for advice with regards to the formulation of the adjoint state equation ml was funded as part of the water informatics science and engineering centre for doctoral training wise cdt under a grant from the engineering and physical sciences research council epsrc uk grant number ep l016214 1 td was funded by a turing ai fellowship uk 2taffp 100007 appendix adjoint state equations a 1 domain integral as objective function given an objective function defined as an integral over the entire domain a 1 q ω f d x sykes et al 1985 eq 15 write the derivative of q with respect to some parameter α as a 2 d q d α ω f α ψ f u k ω ω g α ω k α u d x γ ψ k ω n ω q n α d s to eliminate the unknown state sensitivities ψ u α they solve a 3 k ω f u 0 with boundary conditions ω d 0 on γ d and q n ω k ω n 0 on γ n a 2 boundary integral as objective function the problem addressed in this paper involves an objective function defined on a fixed head boundary γ a 4 q γ f d s with f q k u n where n is the outward normal hence the derivative of the objective function instead takes the form a 5 d q d α ω ψ k ω ω g α ω k α u d x γ ψ k ω n ω q α n q u ψ n d s γ f α f u ψ d s where n is the inward normal sykes et al 1985 and a 6 q α n q u ψ n q n α on γ n to eliminate the unknown state sensitivities ψ we now solve a 7 k ω 0 with boundary conditions ω d 0 on γ d γ and q n ω k ω n 0 on γ n for the remaining boundary γ we impose a 8 f u ω q u n 0 since on γ we have a 9 q u n f u we can substitute a 9 into a 8 to get a 10 f u ω f u 0 on γ and so the operative boundary condition on γ is ω γ 1 
162,we present a novel approach to adaptive optimal design of groundwater surveys a methodology for choosing the location of the next monitoring well our dual weighted approach borrows ideas from bayesian optimisation and goal oriented error estimation to propose the next monitoring well given that some data is already available from existing wells our method is distinct from other optimal design strategies in that it does not rely on fisher information and it instead directly exploits the posterior uncertainty and the expected solution to a dual or adjoint problem to construct an acquisition function that optimally reduces the uncertainty in the model as a whole and some engineering quantity of interest in particular we demonstrate our approach in the context of 2d groundwater flow example and show that the dual weighted approach outperforms the baseline approach with respect to reducing the error in the posterior estimate of the quantity of interest keywords adaptive optimal design groundwater surveying uncertainty quantification bayesian inverse problems adjoint state equations 1 introduction in this paper we present a novel approach to optimally choosing the location of the next monitoring well when conducting a groundwater survey establishing a monitoring well is generally costly and depends on the specific geological context and the required penetration depth and choosing the most informative location for each well is a critical task when designing a groundwater survey groundwater surveying and modelling are intrinsically imbued with uncertainty and solutions and predictions are without exception non unique anderson et al 2015 hence in this paper we assume the perspective that a useful sampling location is one that most significantly reduces the uncertainty in the solution while simultaneously having a substantial influence on some quantity of interest qoi while multiple non invasive and relatively inexpensive methods for groundwater surveying exist loke et al 2013 saey et al 2015 auken et al 2017 2019 these methods all involve solving an inverse problem to reconstruct the hydraulic head which introduces an additional layer of uncertainty hence in this work we focus on the problem of determining aquifer characteristics from direct point measurements of hydraulic head and flux from monitoring wells and how to optimally choose the locations of such wells given existing data while the method is here contextualised within this particular problem it can easily be generalised to any setting where a continuous function and a derived qoi are approximated with point measurements in the classic theory of optimal design we often distinguish between optimality criteria that minimise the estimated parameter variances e g a d and e optimality and those that minimise the prediction variance e g g v and i optimality pukelsheim 2006 myers et al 2016 since in this study we are primarily concerned with the prediction variance the method presented here belongs in the latter category in this context our method can broadly be considered g optimal since our vanilla acquisition function targets the location of the highest posterior dispersion see e g myers et al 2016 however rather than iteratively searching for a design that maximises an optimality criterion we directly utilise a posterior dispersion estimate to construct an acquisition function we remark that while there are some abstract parallels between the method presented here and classic optimal design our method is probably better understood in the context of bayesian optimisation as discussed later additionally the classic optimal design approach is typically centred around the problem of choosing an experimental design that is optimal with respect to an optimality criterion before taking any measurements in this paper we take an adaptive approach and assume that some measurements are already available and we want to propose optimal new sampling locations given the data we already have how the initial measurement locations are optimally chosen is beyond the scope of this paper but we refer to e g cox and reid 2000 pukelsheim 2006 and myers et al 2016 for an extensive overview of optimal design of experiments we remark that our dual weighted method could in theory be employed to choose initial measurement locations but in that case the dispersion of the solution would be constrained only by the prior distribution of parameters and the constraints imposed by the constitutive equations in this case the method presented herein may be used in conjunction with some space filling design strategy or using local penalisation functions as described in section 2 3 2 however either of these workarounds would require an informed prior to work well we recycle the notion from classic optimal design that the information gain is driven by minimising the dispersion of a target distribution lindley 1956 however rather than integrating out all possible measurements and model parameters to find the utility of a given design we take a simpler approach namely we use a monte carlo estimate of the current posterior dispersion of the solution to a partial differential equation pde or some appropriate function thereof as an acquisition function the underlying rationale being that if we wish to know more about the distribution of our solution the most useful place to take a new sample is at the point of the highest posterior uncertainty in this context our vanilla approach see section 2 3 1 is similar to the maximum entropy approach to the optimal sensor placement problem shewry and wynn 1987 where sensors are added at the point of the highest uncertainty of some probabilistic function that is fitted to current sensor measurements for example a gaussian process gp emulator while this strategy will typically place many sensors at the boundaries of the sampling space in the context of adaptive gp fitting mohammadi et al 2021 this is not necessarily the case when targeting the uncertainty of the solution to a pde since that will be constrained by boundary conditions the sensor placement problem has been studied extensively in the context of gp emulators and multiple improvements to the maximum entropy approach have been made see e g krause et al 2008 beck and guillas 2016 and mohammadi et al 2021 however since our objective is to minimise the uncertainty of a pde derived qoi and not a gp emulator many of the recent developments are not immediately applicable since they are tailored for use with a gp emulator hence the vanilla approach presented herein can be considered a reformulation of the original maximum entropy approach particularly tailored for the probabilistic solution of a pde our method see section 2 3 borrows ideas from other fields not obviously related to classic optimal design first our adaptive optimal design approach is formulated in terms of an acquisition function a term typically associated with bayesian optimisation bo močkus 1989 frazier 2018 moreover our approach uses ideas from both prior guided bo souza et al 2021 and batch bo gonzalez et al 2016 the similarities with which are discussed in section 2 3 3 while in the context of bo the aim is to find the maximum or minimum of some function that is expensive to evaluate our objective is to simply reduce the uncertainty of our model predictions hence our vanilla acquisition function addresses solely the uncertainty of some target function and not the function value itself second our approach is inspired by the goal oriented error estimation used in mesh adaptation for pdes prudhomme and oden 1999 oden and prudhomme 2001 where the intention is to refine a mesh locally and parsimoniously to reduce the simulation error with respect to some qoi using an influence function that is the solution to an adjoint pde this approach however is most useful for forward problems where the domain and coefficients are well known and the groundwater flow problem is typically not of this kind instead we use the same approach of computing an influence function with respect to the qoi to determine not where the mesh should be refined but from where we need more data the idea of exploiting the adjoint or dual problem to minimise the posterior uncertainty with respect to a qoi was first explored by attia et al 2018 in a similar context as our model problem however there are several crucial differences between their approach and the one presented in this paper first their method is set in the classic optimal design context where a number of sampling locations are determined before taking any measurements based on the maximising the expected information gain according to some criterion derived from the fisher information matrix second since only a finite number of designs can be explored this way the prospective sampling locations are fixed to a relatively coarse grid finally the approach described in attia et al 2018 requires the adjoint operator to be linear an assumption which is suitable for only a subset of qois we employ markov chain monte carlo mcmc techniques see section 2 1 to generate samples from the posterior distribution of the model parameters given the data π θ d where the model parameters θ in this case describe hydraulic conductivity and the data d are point measurements of hydraulic head and flux see section 2 2 even if the model parameters themselves are of secondary interest to a given problem we can use the mcmc samples to construct monte carlo estimates of any parameter derived quantity or function such as the hydraulic flux across a boundary or the peak concentration of a contaminant at a well additionally unlike traditional inversion techniques mcmc allows for rigorously quantifying the uncertainty of the inverse problem which is useful in the context of engineering decision support systems in particular risk assessment studies we believe that there are many unexploited application opportunities tangential to the study of bayesian posteriors and demonstrate in this paper one such application fig 1 illustrates the proposed workflow at a high level where new wells are sequentially established at locations of high uncertainty and influence on a qoi as dictated by the acquisition function this paper is mainly concerned with the construction of optimal acquisition functions based on the posterior information which would be immediately available from quantifying the uncertainty of the bayesian inverse problem in the following sections we briefly summarise the theory of bayesian inverse problems mcmc and groundwater flow modelling we then outline the proposed methodology and demonstrate the effectiveness of methodology on a synthetic example we show that efficient acquisition functions can easily be constructed from information that would already be available from solving the bayesian inverse problem using mcmc the method avoids many of the complex calculations that are associated with classic optimal design and exploits information about the bayesian posterior in a direct and straightforward way 2 theory in this section we first briefly outline the framework of bayesian inverse problems and markov chain monte carlo mcmc a popular technique employed to draw samples from the bayesian posterior we then summarise the fundamentals of groundwater flow modelling for steady state groundwater flow in a confined aquifer using the finite element method fem finally we describe our novel approach to adaptive optimal design of groundwater surveys 2 1 bayesian inversion a bayesian inverse problem can be stated compactly as given some data d find the distribution π θ d with model parameters θ θ where θ is the parameter space so that 1 d f θ ε where f θ is the model output and ε is the measurement error which is typically assumed to be gaussian bayes theorem then states that 2 π θ d π p θ l d θ π d where π θ d is referred to as the posterior distribution π p θ is prior distribution encapsulating what we already know about our model parameters and l d θ is called the likelihood essentially a measure of misfit between the model output f θ and the data d while the so called evidence π d θ π p θ l d θ d θ is generally infeasible or impossible to determine in most real world scenarios various sampling techniques allows us to make statistical inferences from π θ d anyway examples include importance sampling is and markov chain monte carlo mcmc methods while these methods are not the object of this study a short summary of the main ideas of mcmc which is the specific method employed for inversion in this study is provided for completeness in mcmc we exploit that π d is constant and does not depend on the parameters θ we can therefore write 3 π θ d π p θ l d θ or equivalently for x y θ 4 π y d π x d π p y l d y π p x l d x we then introduce a transition kernel or proposal distribution q y x allowing us to transition from one state x to another y repeatedly applying the transition kernel q y x followed by an accept reject step prescribed by 5 we construct a markov chain where the samples after an initial burn in are precisely from the required distribution π θ d here burn in refers to the initial mcmc samples which are discarded since they may not be representative of the equilibrium distribution of the markov chain this procedure is described in the box below metropolis et al 1953 hastings 1970 gelman 2004 the acceptance probability eq 5 ensures that the algorithm is in detailed balance with the target posterior distribution π θ d see e g liu 2004 sec 5 3 for more details note that when the measurement error ε is gaussian ε n 0 σ ε which we assume in the experiment in section 3 then the unnormalised likelihood functional takes the following form 6 l d θ exp 1 2 f θ d t σ ε 1 f θ d in this study we employ a number of extensions to the metropolis hastings algorithm to speed up inference namely the delayed acceptance da christen and fox 2005 algorithm with finite subchains lykkegaard et al 2020 2022 also referred to as the surrogate transition method by liu 2004 the da algorithm exploits an approximate forward model or reduced order model rom f ˆ to filter mcmc proposals before evaluating them with the fully resolved forward model f resulting in a reduction in computational cost moreover we employ a state independent approximation error model aem to probabilistically correct for model reduction errors introduced by the approximate model as described by cui et al 2018 finally we use the adaptive metropolis am algorithm as the transition kernel haario et al 2001 in this work we used the open source da mcmc framework tinyda 1 1 https github com mikkelbue tinyda to perform the mcmc sampling 2 2 groundwater flow the groundwater flow equation for steady flow in a confined inhomogeneous aquifer occupying the domain ω with boundary γ can be written as the scalar elliptic partial differential equation 7 k x u x g x for all x ω subject to boundary conditions on γ γ d γ n with the constraints 8 u x u d x on γ d and k x u x n q n x on γ n here k x is the hydraulic conductivity u x is the hydraulic head g x are sources and sinks and γ d and γ n are boundaries with dirichlet and neumann conditions respectively see e g diersch 2014 if θ somehow parameterises the conductivity then we have k x k x θ this equation can be converted into the weak form by multiplying with a test function v h 0 1 ω and integrating by parts 9 ω v k x θ u d x γ n v q n x d s ω v g x d x v h 0 1 ω subject to the boundary condition u x u d x on γ d where h 0 1 ω is the hilbert space of weakly differentiable functions on ω which vanish on the dirichlet portion of the boundary γ d we approximate the solution u x in a finite element space v τ h 1 ω on a finite element mesh q τ ω defined by piecewise linear lagrange polynomials ϕ i x i 1 m associated with the m finite element nodes this can be rewritten as a sparse system of equations 10 a θ u b where a i j ω ϕ i x k x θ ϕ j x d x and 11 b i γ n ϕ i x q n x d s ω ϕ i x g x d x where a θ r m m is the global stiffness matrix and b r m is the load vector the solution to this system u u 1 u 2 u m r m represents the hydraulic head at each node which can be interpolated to the entire domain using the finite element shape functions u x i 1 m u i ϕ i x in our numerical experiments we used the open source high performance finite elements package fenics langtangen and logg 2016 to solve these equations 2 3 adaptive optimal design the overarching research question of this paper is this if we want to collect more data to reduce the variance in our posterior monte carlo estimates where in the modelling domain ω should we add a new borehole to maximise the expected value of the collected data more formally if we let t denote the current design of the survey so that d t and π t θ d t denote respectively the data and posterior distribution corresponding to that design we want to find the next sampling point x that constrains π t 1 θ d t 1 in an optimal way after setting d t 1 d t d t where d is the newly collected data at x 2 3 1 vanilla approach as outlined in section 2 1 bayesian inversion allows us to construct the posterior distribution of parameters given the data π t θ d t if the inversion was completed using mcmc and obtaining the model output f θ involved solving some partial differential equation with solution u x we can cache these solutions during sampling and would after sampling possess a set of pairs θ i u i x i 0 n since θ i i 0 n are distributed exactly according to π t θ d t so are any functions of θ such as u x here n is the number of mcmc samples after discarding the burn in hence we can easily obtain monte carlo estimates for e π t θ d t u x θ and d π t θ d t u x θ here d signifies some measure of statistical dispersion for example variance standard deviation or entropy we could in accordance with the maximum entropy approach shewry and wynn 1987 postulate that the accuracy of our inversion is driven by the dispersion in u x and hence we could solve the following optimisation problem 12 x arg max x ω d π t θ d t u x θ 2 3 2 dual weighted approach the simple approach outlined above will improve the general quality of u x but it is limited by the fact that it is not tailored for a particular quantity of interest q and this is where the dual weighted approach comes into play in this context rather than simply sampling from places with high uncertainty we aim to pick sampling points that also have a high expected influence on our quantity of interest q this is exactly the problem that adjoint or dual state methods aim to solve plessix 2006 suppose in a particular application we are interested in estimating a particular quantity of interest q u which we can write as a functional of the solution for example if our quantity of interest is the hydraulic head around a point x ω we could choose 13 q x u ω u x exp x x 2 λ d x for some sufficiently small length scale λ this however is a trivial problem since if the quantity of interest is the hydraulic head at some point we can just place our monitoring well at that point and measure it it would be much more useful to target a quantity of interest that we cannot measure directly hence in this study we consider flux over a boundary γ with the following functional 14 q γ u γ k x θ u x n d s the adjoint state equation associated with eq 14 is 15 k ω 0 subject to the boundary conditions ω d x 0 on γ d γ ω γ x 1 on γ q n ω x k x ω x n 0 on γ n the solution ω x is called the adjoint state or influence function please refer to sykes et al 1985 and theappendix for details on the derivation of the adjoint state equation and its associated boundary conditions integrating by parts and multiplying with a test function v h 0 1 ω we arrive at the weak form of the adjoint state equation 16 ω v k x θ ω d x γ n v q n ω x d s 0 v h 0 1 ω subject to boundary conditions ω d x 0 on γ d γ and ω γ x 1 on γ given some conductivity parameters θ 16 can be discretised using the same finite element grid as 10 leading to the following sparse system of equations 17 a θ ω b ω where a i j ω ϕ i x k x θ ϕ j x d x and 18 b ω i γ n ϕ i x q n ω x d s it is important to note here that the stiffness matrix a θ since the steady state groundwater flow equation is self adjoint is exactly the same as in eq 10 and the assembled system can hence be partially recycled when solving both equations however since the boundary conditions for the adjoint state equation are different than for the primal problem care must be taken when assembling the adjoint system of equations after solving this system of equations the influence function can be interpolated to the entire domain using our finite element shape functions ω x i 1 m ω i ϕ i x where ω ω 1 ω 2 ω m t the influence function is commonly interpreted as the sensitivity of the quantity of interest to a unit point source anywhere on the domain sykes et al 1985 wilson and metcalfe 1985 or in this particular case as the sensitivity of flow anywhere on the domain to the boundary condition broadly speaking the influence function directs us towards areas of the modelling domain with a potentially high influence on our quantity of interest which is what we required for our dual weighted approach we note that ω x is now a random function which depends on model parameters θ and we can obtain estimates for e π t θ d t ω x θ hence we propose the following acquisition function 19 x arg max x ω d π t θ d t u x θ e π t θ d t ω x θ where denotes the absolute value we use the absolute value of the expectation of the influence function to make sure that the weighting is always positive since ω x θ is not guaranteed to be positive in all scenarios we call this approach dual weighted since we are essential re weighting the dispersion d π t θ d t u x θ by the expected solution of the dual problem fig 2 illustrates the different steps in the proposed adaptive optimal design procedure 2 3 3 remarks 1 the dual weighted approach can be considered a hybrid between the goal oriented error estimation employed for mesh adaptation in the context of various expensive and mesh sensitive pde problems see e g prudhomme and oden 1999 and oden and prudhomme 2001 and bayesian optimisation bo typically used to optimise some unknown function approximated with sparse and or noisy data see e g močkus 1989 and frazier 2018 in this context our dual weighted approach could be framed as a form of prior guided bo souza et al 2021 where ω x broadly represents our prior belief that any point x constitutes a good sampling location however we remark that in our formulation ω x is not a probability distribution but a random weighting function 2 in the above formulations we have chosen the dispersion of the hydraulic head d π t θ d t u x θ as the function representing uncertainty in the model other sensible choices of uncertainty metrics would be the dispersion of the hydraulic conductivity d π t θ d t k x θ or of some norm of the flux d π t θ d t q x θ p 3 since sampling from π t θ d t can be computationally expensive it may be desirable to pick multiple new sampling locations at each step of the algorithm denote the number of new sampling locations in each such batch acquisition as n then this can be achieved by penalising the acquisition function by some local penalisation functions ψ x i x i 1 n 1 centred on the previous sampling points x i i 1 n 1 of the current batch as described in gonzalez et al 2016 this approach would yield the following dual weighted batch acquisition function for x i i 2 n 20 x i arg max x ω d π t θ d t u x θ e π t θ d t ω x θ j 1 i 1 ψ x j x similarly the batch acquisition function for the vanilla approach takes the form 21 x i arg max x ω d π t θ d t u x θ j 1 i 1 ψ x j x a reasonable choice of penalisation functions would be the gaussian 22 ψ x x 1 exp 1 2 x x 2 2 l ψ where l ψ controls the dispersion of the function and 2 is the l 2 norm using such a penalisation function the acquisition function would be exactly zero at previous sampling points from the current batch and smoothly rebound to eq 19 or eq 12 as the distance to previous sampling points increases 4 as mentioned earlier we formulate our method in the context of steady state groundwater flow in a confined aquifer while this is the most common approach to groundwater flow modelling it is naturally not exhaustive for a detailed analysis of the adjoint state equations for transient groundwater flow we refer the to e g sun 1999 and lu and vesselinov 2015 the unconfined case is considerably more complex since the constitutive equations are nonlinear while unconfined groundwater flow can under some assumptions be reasonably approximated by the constitutive equations for confined flow wang and anderson 1982 this is not always the case for a derivation and analysis of the adjoint equations pertaining to unconfined and coupled aquifers we refer to e g sun 1999 and neupauer and griebling 2012 5 note that the constitutive and adjoint equations are discretised using fem in the above section we restrict ourselves to this method for brevity but remark that the proposed acquisition functions eqs 12 19 20 and 21 are valid for any discretisation scheme also note that if piecewise linear shape functions are employed to approximate u x the maxima of the acquisition functions will occur at finite element nodes 3 example in this section we demonstrate the vanilla and dual weighted approach in the context of a synthetic groundwater flow example we first outline the model setup including the geological model and finite element representation we then explain the particular methodology for this example in detail finally we present the results 3 1 model setup we model the hydraulic conductivity as a log gaussian random field with a matern 3 2 covariance kernel 23 c x y 1 3 x y 2 l exp 3 x y 2 l where l is the length scale rasmussen and williams 2006 and 2 is the l 2 norm the resulting random field is expanded in an orthogonal eigenbasis with n kl karhunen loève kl eigenmodes to this end we construct a matrix of covariances between each pair of finite element nodes c r m m according to eq 23 so that c i j c x i x j this covariance matrix c is decomposed into the n kl largest eigenvalues λ i i 1 n kl and eigenvectors ψ i i 1 n kl the nodal conductivities k k 1 k 2 k m are then given by 24 log k μ σ ψ λ 1 2 θ with λ diag λ 1 λ 2 λ n kl and ψ ψ 1 ψ 2 ψ n kl the vector μ μ 1 is the mean of the log conductivity σ is the standard deviation of the log conductivity and θ n 0 i n kl dodwell et al 2015 when defined in this way the associated bayesian inverse problem involves exploring π θ d i e the posterior distribution of hydraulic conductivity parameters θ given measurements d where the aforementioned normal distribution constitutes the prior distribution of parameters π p θ n 0 i n kl we used three different models for the experiments fig 3 one data generating model representing the ground truth a fine forward model representing the fully resolved forward model f in the bayesian inverse problem see eq 1 and a coarse forward model corresponding to the reduced order forward model in the delayed acceptance mcmc sampler f ˆ as described in e g christen and fox 2005 liu 2004 cui et al 2018 lykkegaard et al 2020 and lykkegaard et al 2022 note that using the dual weighted approach described herein does not require a delayed acceptance mcmc sampler any method capable of producing monte carlo samples from the posterior will do the experiments were performed on a rectangular domain ω 0 2 0 1 meshed using a structured triangular grid with m f i n e 1326 degrees of freedom for the data generating model and the fine forward model and m c o a r s e 703 degrees of freedom for the coarse forward model for the data generating model the log gaussian random conductivity was truncated at n kl 256 kl eigenmodes while for the fine and coarse models it was truncated at n kl 128 hence the dimensionality of the inverse problem in these experiments was 128 which is very high and a challenging problem for any mcmc algorithm moreover we set l 0 1 μ 2 and σ 1 0 for every model this resulted in strongly anisotropic conductivity fields with log conductivities broadly between 5 and 1 fig 3 a we imposed fixed head dirichlet boundary conditions of 1 and 0 on the left and right boundaries respectively and no flow neumann conditions on the remaining top and bottom boundaries we set the right hand side of eq 7 to g x 0 we chose flux across the right boundary γ r as our quantity of interest q corresponding to the following functional as in eq 14 25 q u γ r k x θ u x n d s and the associated adjoint state equation shown in 15 with γ γ r fig 3 f shows an example of the influence function generated by this adjoint state equation the left column of fig 3 shows the conductivity associated with a random draw from the prior π p θ for the data generating model the fine model and the coarse model respectively the right column of fig 3 shows the corresponding hydraulic head flux and influence function for the data generating model 3 1 1 methodology using the above setup we completed a total of n 30 independent numerical experiments to demonstrate the feasibility of the dual weighted approach we chose the standard deviation of the l 2 norm of the flux s q x 2 as the general measure of uncertainty in the model for each independent experiment the following experimental procedure was observed 1 the hydraulic conductivity for the data generating model was initialised with a random draw from the prior and the primary problem was solved 2 eight observation wells were placed randomly on the domain by latin hypercube sampling mckay et al 1979 see fig 4 3 for each observation well x i the hydraulic head u x i and the norm of the flux q x i 2 were computed these head and flux observations were contaminated with white noise from ε u n 0 0 0 1 2 and ε q 2 n 0 0 00 1 2 respectively 4 delayed acceptance mcmc sampling was completed with 2 independent samplers each drawing n 25000 fine samples with a subsampling length of 5 see e g lykkegaard et al 2020 and lykkegaard et al 2022 and a burn in of n b u r n 5000 was discarded this resulted in a total number of mcmc samples of n 40000 for each experiment 5 the standard deviation of the l 2 norm of the flux s q x 2 and the mean of the influence function ω x were computed at the finite element nodes and interpolated to the entire domain using the finite element shape functions and eight new observation wells were placed according to the batch vanilla and dual weighted acquisition functions see eqs 20 and 21 fig 4 shows the vanilla and dual weighted acquisition functions for one sample of the n 30 models as expected the weighting function ω x prioritised observation wells closer to the boundary of the quantity of interest 6 data were extracted from the eight new observation wells as in step 3 and appended to the data vector 7 delayed acceptance mcmc sampling was repeated using the new data vectors for both the vanilla and dual weighted approaches for each experiment and each posterior distribution initial vanilla and dual weighted with each n 40000 posterior samples we computed the mean squared error mse and variance of the predicted quantity of interest q i i 1 n compared to the true value q t r u e the mse of the predicted value of the quantity of interest q i with respect to the true value q t r u e was computed as 26 mse 1 n i 1 n q t r u e q i 2 similarly the sample variance of q for each experiment was computed as 27 s 2 1 n 1 i 1 n q i q 2 finally we constructed gaussian kernel posterior density estimates f ˆ π θ d q from the posterior samples from each experiment q i i 1 n and computed the kernel density of the true value q t r u e with respect to this density estimate kernel density estimates were computed using scipy virtanen et al 2020 with automatic bandwidth determination scott 1992 3 1 2 results we compared the mse variance and kernel density of both the vanilla and dual weighted posterior samples with the corresponding values for the initial posterior samples for all n 30 experiments with respect to the mse the vanilla approach yielded a median reduction of 22 while the dual weighted approach yielded a median reduction of 30 fig 5 a this demonstrates that both acquisition strategies approach the true value when we add more datapoints but that the dual weighted approach is more efficient with respect to the variance of the quantity of interest the vanilla approach yielded a median reduction of 31 while the dual weighted approach yielded a median reduction of 34 fig 5 b this shows that for both acquisition strategies the posterior distribution contracts as more data is added and that the two approaches differ less with respect to this feature however this metric shows only that the posterior contracts and not if it moves closer to the true value finally we computed the posterior densities of the true quantity of interest with respect to kernel posterior density estimates f ˆ π θ d q for each experiment here the vanilla approach yielded a median improvement of 12 while the dual weighted approach yielded a median improvement of 17 since the prediction variance of the quantity of interest reduced in every experiment fig 5 b this again shows that the posterior distribution moves closer to the true value as more data is added but that the dual weighted approach is better we note that in neither method was capable of improving the posterior estimate of the quantity of interest for every experiment hence in 8 30 vanilla experiments and 5 30 dual weighted experiments adding additional wells resulted in a worse posterior mse than the initial one this is not surprising since we are dealing with a very ill posed inverse problem and any new datapoint may reinforce the initial bias rather than reduce it while both approaches occasionally failed to improve the posterior estimate the dual weighted approach performed better than the vanilla approach we computed the gaussian kernel density estimates of the error ɛ i q t r u e q i for two samples of the n 30 experiments the left panel shows a typical example where the vanilla approach resulted in a moderate improvement while the dual weighted approach yielded a more dramatic improvement the right panel shows an example where both the dual weighted and vanilla approaches failed to produce any improvement 4 discussion in this paper we have proposed a novel approach to the problem of optimally choosing the next location for a monitoring well given existing data and some quantity of interest qoi the proposed methodology exploits the solution of an adjoint problem to weigh such an acquisition function according to the expected influence on the qoi numerical experiments have demonstrated that the approach works for our model problem we emphasise that the problem is intrinsically probabilistic and hence subject to uncertainty we have demonstrated that the approach works on average for our model problem but there were certain experiments where the dual weighted acquisition strategy did not approach the true qoi see e g fig 6 b as the number of wells approach infinity the posterior distribution will certainly approach the true value but for any one new observation well there are no such guarantees in a sense the dual weighted approach merely increases the chance of improving the posterior distribution of the qoi while we formulated and demonstrated the approach in the context of a groundwater surveying problem the method could be applicable to other areas of science and engineering where measurements are expensive the most obvious parallel application is petroleum engineering where there are similarities both in terms of the constituent equations and the mode of sampling but the method could be adapted with little effort to any inverse problem where establishing sensors is expensive we note however that the dual problem in our case was unusually simple since the groundwater flow equation is self adjoint clearly the dual weighted approach can only be used as written for qois where an adjoint problem can be formulated and solved directly for more complicated qois an alternative approach would be to perturb the posterior mean or mode to approximate the influence function using such an approach would yield ω x e θ rather than e ω x θ as a weighting function a bottleneck of our approach is that the mcmc sampler is rerun after each batch data acquisition running mcmc for expensive forward models is notoriously computationally demanding and while we employ various tricks to reduce the cost such as delayed acceptance and proposal adaptivity this is not the most elegant approach one way to significantly alleviate the cost of subsequent posterior distributions would be to employ a particle filter to sequentially reweigh mcmc samples according to the new data chopin 2002 this sequential approach was investigated in this study but it did not work well mainly because of very high sample degeneracy when the variance of the solution as in our case is relatively high at unobserved locations only few posterior samples fit the new observations well with the mentioned sample degeneracy as a result moreover we found that the dispersion measures in eqs 12 19 20 and 21 where highly sensitive to this sample degeneracy this challenge could be alleviated by drawing more posterior samples for the initial mcmc but that would only offset the cost we remark that this approach might work better for lower dimensional problems than the one investigated in this study we highlight this problem as a potential target for future research the methodology was demonstrated empirically in the context of a synthetic groundwater flow example this gives rise to at least three additional interesting directions of future research first showing theoretically that the distribution of the quantity of interest does indeed converge faster to the true value when using the dual weighted approach and examining the mechanisms that govern this process in detail second testing the method in practice in the context of an actual groundwater survey while testing the method in practice would certainly expose limitations and complications that were not identified in this study it would be difficult to validate the method further in this fashion since the true value of the qoi is rarely known in reality this may be overcome by testing the method under controlled laboratory conditions third generalising the dual weighted approach to a wider range of pde problems with different constituent equations and qois credit authorship contribution statement mikkel b lykkegaard conceptualisation methodology software visualisation investigation analysis draft preparation writing editing tim j dodwell supervision conceptualisation methodology review declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the mcmc code used for delayed acceptance sampling can be found at https github com mikkelbue tinyda and additional code is available in the open research exeter data repository at http hdl handle net 10871 129475 the authors would like to thank the reviewers for helpful comments and suggestion and robert scheichl and karina koval for advice with regards to the formulation of the adjoint state equation ml was funded as part of the water informatics science and engineering centre for doctoral training wise cdt under a grant from the engineering and physical sciences research council epsrc uk grant number ep l016214 1 td was funded by a turing ai fellowship uk 2taffp 100007 appendix adjoint state equations a 1 domain integral as objective function given an objective function defined as an integral over the entire domain a 1 q ω f d x sykes et al 1985 eq 15 write the derivative of q with respect to some parameter α as a 2 d q d α ω f α ψ f u k ω ω g α ω k α u d x γ ψ k ω n ω q n α d s to eliminate the unknown state sensitivities ψ u α they solve a 3 k ω f u 0 with boundary conditions ω d 0 on γ d and q n ω k ω n 0 on γ n a 2 boundary integral as objective function the problem addressed in this paper involves an objective function defined on a fixed head boundary γ a 4 q γ f d s with f q k u n where n is the outward normal hence the derivative of the objective function instead takes the form a 5 d q d α ω ψ k ω ω g α ω k α u d x γ ψ k ω n ω q α n q u ψ n d s γ f α f u ψ d s where n is the inward normal sykes et al 1985 and a 6 q α n q u ψ n q n α on γ n to eliminate the unknown state sensitivities ψ we now solve a 7 k ω 0 with boundary conditions ω d 0 on γ d γ and q n ω k ω n 0 on γ n for the remaining boundary γ we impose a 8 f u ω q u n 0 since on γ we have a 9 q u n f u we can substitute a 9 into a 8 to get a 10 f u ω f u 0 on γ and so the operative boundary condition on γ is ω γ 1 
163,characterizing the wettability of hydrogen h 2 methane ch 4 mixtures in subsurface reservoirs is the first step towards understanding containment and transport properties for underground hydrogen storage uhs in this study we investigate the static contact angles of h 2 ch 4 mixtures in contact with brine and bentheimer sandstone rock using a captive bubble cell device at different pressures temperatures and brine salinity values it is found that under the studied conditions h 2 and ch 4 show comparable wettability behaviour with contact angles ranging between 25 45 and consequently their mixtures behave similar to the pure gas systems independent of composition pressure temperature and salinity for the system at rest the acting buoyancy and surface forces allow for theoretical sensitivity analysis for the captive bubble cell approach to characterize the wettability moreover it is theoretically validated that under similar bond numbers and similar bubble sizes the contact angles of h 2 and ch 4 bubbles and their mixtures are indeed comparable consequently in large scale subsurface storage systems where buoyancy and capillary are the main acting forces h 2 ch 4 and their mixtures will have similar wettability characteristics graphical abstract keywords underground hydrogen storage h2 ch4 mixtures wettability contact angle captive bubble cell 1 introduction development of large scale twh energy storage technologies is essential in the successful transition towards renewable energy systems therefore energy has to be converted into forms that can be stored at such large scales one of the attractive energy carriers is hydrogen h2 due to its high energy content per mass 141 86 mj kg and its carbon free combustion products hassanpouryouzband et al 2021 however there exists a major challenge in development of storage technologies for hydrogen being the lightest molecule its volumetric energy content is relatively low stone et al 2009 zivar et al 2021 more specifically it stores only about 132 kwh in 1 m 3 at relatively high pressure of 50 bars and temperature of 298 k kabuth et al 2017 as such to achieve feasible large scale storage for compressed hydrogen gas gigantic volumes are needed these volumes are beyond the technical economical land usage and safety scope of surface based storage tanks taylor et al 1986 schaber et al 2004 underground reservoirs on the other hand provide giant volumes to store hydrogen in the expected twh scales these formations can be in the form of solution mined salt caverns ramesh kumar et al 2021 or geological porous reservoirs walters 1976 tarkowski 2019 gabrielli et al 2020 including depleted hydrocarbon fields and saline aquifers hashemi et al 2021a there exist a few experiences with storing hydrogen or its mixture with methane in porous reservoirs panfilov et al 2006 kruck et al 2013 and several pilot projects are currently underway rag 2021 pérez et al 2016 however to date a rigorous understanding of many aspects related to subsurface storage of pure hydrogen and its mixture with methane is still lacking heinemann et al 2021 in some aspects underground hydrogen storage uhs is similar to that of underground gas storage ugs as both are compressed gases being stored cyclically in subsurface formations however in many aspects uhs is expected to behave quite differently than ugs firstly h 2 is very different than ch 4 gas in its molecular weight diffusivity dissolution density and surface interfacial tension secondly the cyclic loading and frequency of the green hydrogen injection and production supplied by the intermittent green energy production is expected to be much different than that of ugs lastly hydrogen purity is expected to be maintained during the storage period as sensitivities towards hydrogen impurities in fuel cells are very high laban 2020 these differences have recently motivated the scientific community to study hydrogen properties in detail specially its wettability characteristics in contact with reservoir brine and rock iglauer et al 2021 hashemi et al 2021b ali et al 2021 this is due to the fact that hydrogen will come in contact with brine whether in aquifers or porous rocks containing connate water hashemi et al 2021a heinemann et al 2021 h 2 brine rock wettability is a key factor in identification of the hydrogen interaction with reservoir brine and rock more precisely it allows for understanding the distribution of hydrogen through the porous rock micro channels according to young s equation it is characterized by the contact angle between the interface of gas brine in contact with the rock surface de gennes 1985 i e 1 cos θ σ r b σ r g σ b g here σ r b σ r g σ b g correspond to the interfacial forces between each pair of the phases respectively rock brine rock gas and brine gas young 1805 typically in geological reservoirs the adhesive forces between brine and rock are much bigger than between gas and rock because molecules in the liquid phase are much closer to each other than in the gas phase therefore the contact angle in gas brine rock systems is likely to be less than 90 degrees consequently water wet conditions can be expected during underground hydrogen storage the distribution of hydrogen and brine in the porous rock influences multiphase flow properties such as relative permeability and capillary pressure in water wet systems the non wetting phase in our case hydrogen will preferentially flow through the larger pores resulting in a higher relative permeability this facilitates the injectivity of the reservoir while the amount of capillary trapped hydrogen will be smaller both aspects are favourable for uhs alhammadi et al 2017 arif et al 2019 wettability of the h 2 brine rock system has been the focus of some recent studies all of which were conducted using water wet rocks yekta et al 2018 van rooijen et al 2021 iglauer et al 2021 hashemi et al 2021b with pure hydrogen gas these studies collectively revealed static hashemi et al 2021b and dynamic yekta et al 2018 van rooijen et al 2021 iglauer et al 2021 ali et al 2021 contact angles of hydrogen by different experimental methods captive bubble cell hashemi et al 2021b higgs et al 2021 tilted plate iglauer et al 2021 ali et al 2021 microfluidics van rooijen et al 2021 and indirectly yekta et al 2018 and directly higgs et al 2021 through core flooding techniques a summary of the measured contact angles as well as the conditions and experimental techniques can be found in table 1 the characterization of h 2 ch 4 wettability is important for comparison of uhs and ugs despite its importance there exists no rigorous study which investigates and reports how hydrogen wettability compares with that of methane and hydrogen methane mixtures of different concentrations in addition there can be cases in which h 2 mixes with the reservoir ch 4 for example if it is used as cushion gas or traces of it exists in the subsurface environment such as depleted gas fields moreover it is important for industrial applications where first h 2 ch 4 mixture is introduced in the gas grid and storage facilities as the production of h 2 scales up in the future the fraction of h 2 concentration in the mixture is expected to be further increased for example the first uhs project in europe underground sun storage by rag austria ag stored 20 h 2 and 80 ch 4 rag 2017 in a porous reservoir mixing of h 2 with ch 4 impacts the physio chemical properties of the injected hydrogen and consequently its displacement process hassanpouryouzband et al 2020 tek 2012 simon et al 2015 sáinz garcía et al 2016 this can potentially impact the upscaled multiphase flow functions of capillary pressure and relative permeability hashemi et al 2021a rücker et al 2019 kunz et al 2018 pan et al 2021 carden and paterson 1979 a correct description of these upscaled flow functions is needed to ensure the safety of underground hydrogen storage as well as to optimize the cyclic injection and production of hydrogen as such characterization of the h 2 ch 4 mixture wettability is crucially important which is the focus of the present study in this work we directly measure the static contact angles of h 2 ch 4 mixtures in contact with brine and sandstone rock using a captive bubble cell experimental methodology kaveh et al 2014 hashemi et al 2021b we systematically analyse contact angles of different size gas bubbles and different mixture concentrations by providing a modelling analysis we validate our methodology and the findings of this study the structure of the paper is as follows in the materials and methods section a description of the methodology and test conditions are presented in detail this is followed by the results and interpretation of the data a sensitivity analysis is also performed on the basis of the young laplace equation to better analyse and justify the experimental observations finally the main learning points are presented in the conclusion 2 materials and methods in this study static contact angles for h 2 ch 4 gas mixtures as well as pure ch 4 gas in contact with brine and bentheimer sandstone rock are measured using the captive bubble cell device kaveh et al 2014 hashemi et al 2021b 2 1 materials the gas mixtures consisted of 99 99 mol purity h 2 and 99 5 mol purity ch 4 both produced by linde gas company the gas mixture was prepared by filling up the pump with both gases in the desired concentration at the desired pressure having the pump with the gasses stand for one day so to allow for a fully mixed gas liquid system the brines were made by dissolving nacl in deionized water a bentheimer sandstone rock slab with dimensions of 30 6 12 mm was used in the experiments the sample was untreated and cut from the same clean bentheimer sandstone block as in the bentheimer rock sample used in hashemi et al 2021b the permeability of the sample was 2 3 darcy and the porosity was around 20 the mineral composition consisted for 95 of quartz which was evenly distributed throughout the rock matrix peksa et al 2015 the surface roughness was 0 03 mm and was determined by microscopic analysis hashemi et al 2021b the experimental conditions used for each h 2 ch 4 gas mixtures and pure ch 4 gas can be found in table 2 2 2 experimental apparatus and procedure a schematic of the experimental apparatus can be seen in fig a 1 of appendix a it is similar to the setup used by hashemi et al 2021b adapted in this study for gas mixtures the apparatus consisted of a high pressure high temperature single steel cell with a volume of 150 ml filled with brine the rock sample was attached to the centre of the cell the cell was placed in an oven to control the temperature brine was continuously injected at a flow rate of 0 02ml min from the bottom of the cell the pressure was regulated with a back pressure device connected to the top of the cell and attached to a n 2 cylinder gas bubbles of approximately 2 mm in diameter were released from a nozzle at the bottom of the cell into the brine the bubble buoyantly rose until it reached the rock surface the bubble slowly dissolved and diffused into the brine resulting in bubbles of different sizes images with a resolution of 6 9 mp 3216 2136 were taken at evenly spaced time intervals using a canon 90 camera with a maximum resolution of 12 3 mp attached to an endoscope the pressure and temperature in the cell were continuously monitored the lines of the system were thoroughly cleaned with water and ethanol at the start of each experiment to avoid any impact of contamination on the contact angle measurements 2 3 image analysis contact angles were derived for each of the images taken during the experiment using an in house matlab code which is based on the adsa p technique li et al 1992 the adsa p technique fits the best theoretical laplacian curve on the physical observed bubble interface and is based on the young laplace equation next section for this purpose the images are cropped and binarized such that the interface including the apex and contact points can be detected to find the size of the bubble the outer diameter of the nozzle is used the brine and gas density values used in the analysis are reported in appendix b tables b 12 b 22 for more details about the image analysis procedure the reader is referred to hashemi et al 2021b 2 4 theoretical analyses based on young laplace equation fig 1 shows a schematic of an axisymmetric gas bubble the blue contour indicates the gas brine interface the pressure difference across this interface i e δ p can be described by the young laplace equation li et al 1992 as 2 δ p σ 1 r 1 1 r 2 where σ n m is the interfacial tension and r 1 m and r 2 m are the principle radii of the curvature the pressure difference across the interface is due to the interfacial tension as well as the force of gravity i e 3 δ p δ p 0 δ p g at gravity capillary equilibrium the pressure difference across the interface can be described as a function of depth z m i e 4 δ p δ p 0 δ ρ g z here δ ρ kg m 3 is the density difference between the gas and the brine phase and g m s 2 is the gravitational acceleration since the apex point is taken as the reference i e z 0 no gravity term is considered there and thus one can write r 1 r 2 b at this point therefore at the apex point eq 2 can be written as 5 δ p a p e x 2 σ b by substituting eq 4 into eq 2 it is found that 6 σ 1 r 1 1 r 2 2 σ b δ ρ g z holds for any depth z in cylindrical coordinate one can write 7 1 r 1 d θ d s and 8 1 r 2 sin θ x here θ is the contact angle and s m is the distance along the surface contour as illustrated in fig 1 by replacing r 1 and r 2 in eq 6 with eqs 7 and 8 respectively one obtains 9 d θ d s 2 b δ ρ g z σ sin θ x eq 9 can be stated in dimensionless form as 10 d θ d s 2 b δ ρ g r 2 σ z sin θ x where the bubble radius r is used as characteristic length scale i e x x r the second term on the right hand side of eq 9 is the bond number n b o defined as 11 n b o δ ρ g r 2 σ n b o is the ratio of gravitational forces to interfacial forces alvarez et al 2009 hessel et al 2004 berg 2010 3 results and discussion contact angles for h 2 ch 4 brine rock systems were measured using the captive bubble cell method although this method does not take into account the impact of pore structures and flow dynamics on the wettability it sheds lights on the wettability behaviour in systems where buoyancy and capillary are the main driving forces furthermore it provides insights on the overall wettability state of sandstone rock in contact with gas mixtures of h 2 ch 4 and brine the experiments were carried out for a range of pressures two temperatures and two different brine salinities the results of which can be seen in fig 3 and tables b 1 b 11 of appendix b the results for pure h 2 are based on the experimental observations of hashemi et al 2021b who used the same captive bubble cell device to measure contact angles for the h 2 brine bentheimer system in addition in fig 6 the contact angles for pure h 2 measured on a third bentheimer sandstone sample are presented to highlight the systematic change in contact angle that is observed when different samples are used all bentheimer sandstone samples used in this study were cut from the same bentheimer sandstone block to verify whether changes in the chemical composition or rock structure occurred over time and consequently changed the contact angle experiments were repeated fig 2 a shows the contact angle for different bubble sizes for three hydrogen experiments carried out on the same rock slab between each of the experiments the rock slab was taken out of the apparatus and put in the vacuum oven to dry it can be seen that similar results were obtained for each of these experiments this indicates that no mineral alteration or other changes in the structure of the rock surface has taken place that significantly impacted the wettability the bubble size decreased with time which is likely due to dissolution and diffusion into the brine to verify whether dissolution into the brine would have an impact on the contact angle measurement experiments were carried out using brine with different levels of hydrogen saturation fig 2 a shows the contact angle versus volume while fig 2 b shows the contact angle versus time for a system where the brine was highly saturated with hydrogen and a system where the brine was unsaturated it can be seen that the dissolution rate of the unsaturated brine is almost 10 times higher than the highly saturated brine however the contact angles obtained are the same in each of the experiments this shows that the contact angle is a function of the bubble size and does not depend on the saturation level using unsaturated brine allowed us to make contact angle measurements for a range of bubble sizes it is observed that the contact angle increases with decreasing bubble size the minimum and maximum contact angle values in fig 3 correspond to the largest and smallest bubble sizes 3 1 effect of bubble size and gas composition the gas bubbles were released in under saturated brine solutions and slowly dissolved and diffused into the brine images were taken every minute except for pure h 2 where the time step between images was four minutes for each of these images the contour of the interface was detected and contact angles were calculated fig 4 shows the contours of the interfaces at each time step as well as the corresponding bubble volume and contact angle for three h 2 ch 4 mixtures pure h 2 and pure ch 4 for a system with pure water at 30 c and 100 bar due to the roughness of the sample pinning of the gas bubble occurred and as a result the dissolution was not symmetric in some cases this pinning led to higher contact angles as can be observed in fig 4 e for 50 h 2 ch 4 mixture for bubbles bigger than 4 mm 3 overall the gas bubbles of the different mixtures show comparable behaviour although the contact angles of the h 2 bubbles are slightly higher this is likely due to the fact that a different bentheimer sandstone sample although obtained from the same block was used for the h 2 experiments since the contact angles of the different mixture compositions are indistinguishable the contact angle is a function of the bubble volume and no distinction can be made between the different gases this behaviour of increasing contact angle with decreasing bubble volume has also previously been reported for co2 brine rock systems kaveh et al 2014 drelich 1997 haeri et al 2020 jung and wan 2012 3 2 effect of pressure temperature and salinities for all experimental conditions water wet behaviour was observed with contact angles ranging between 25 45 for all h 2 ch 4 mixtures as well as for pure h 2 and pure ch 4 as can be seen in fig 3 no obvious correlation between the measured contact angle and the pressures temperatures or salinity could be observed note that the range of bubble sizes was different for the different experiments high contact angle values correspond to smaller bubble sizes however for similar bubble sizes all the data points fall within the accuracy range of the conducted experiments 3 to validate our findings a sensitivity study of the captive bubble cell approach to measure wettability based on the young laplace equation has been performed this is presented in the following section 3 3 sensitivity analysis of the captive bubble cell approach the contact angles presented in this study were obtained by fitting eq 9 onto the captured images of the gas bubbles a closer look at eq 9 shows that three parameters impact the fitting curve formula apex radius b density difference δ ρ and interfacial tension σ of which the combined impact can be characterized by the bond number as defined in eq 11 to investigate the impact of each of these three parameters on the contact angle a systematic sensitivity analysis has been performed the results of which can be seen in fig 5 based on the experimental observations the ratio of the surface position to the height h see fig 1 is changing from 0 8 to 0 95 corresponding to the biggest and smallest bubble sizes respectively therefore as the base case σ was set to 60 mn m δ ρ to 1000 kg m 3 b to 1 mm and the surface location z was set to 0 9 of the bubble height it is important to note that the ratio of the surface position to the height h depends on the wetting state of the system fig 5a shows the results of the analysis for the apex radius effect it can be seen that the contact angle increases with decreasing apex radius in a similar fashion as was observed from the experiments the effect of the density contrast on the contact angle is presented in fig 5b here the contact angle increases with decreasing δ ρ however for the δ ρ of this study which is in the range of 900 1000 kg m 3 the contact angle ranges between 41 7 43 4 which is within the accuracy range of the measuring technique fig 5c shows the effect of interfacial tension on the contact angle it can be seen that the contact angle increases with increasing interfacial tension for the pressures and temperatures of our study the interfacial tensions of the different h 2 ch 4 mixtures likely ranged between 50 70 mn m hassanpouryouzband et al 2021 chow et al 2018 pan et al 2020 in this range the contact angle varies between 38 3 44 for a particular set of conditions this is again within the accuracy range of the measuring technique the above analysis shows that no significant pressure temperature and salinity dependency is expected in systems where buoyancy and capillarity are the main driving forces such as our captive bubble cell which validates our results however in different systems where other driving forces come into play including different experimental measurement techniques these factors could have a bigger impact on the contact angle as has been observed in literature the bond number for the h 2 water and ch 4 water experiments are plotted in fig 6a as a function of radius at the apex point it can be seen that the bond numbers of both the h 2 water and ch 4 water systems are comparable and depend on the size of the bubble fig 6b shows that under similar bond numbers and similar bubble sizes the contact angles of h 2 and ch 4 bubbles and their mixtures are indeed comparable the slight difference between the contact angle plots is due to the fact that different bentheimer sandstone samples were used the experiments of this study were carried out for pressures ranging between 20 100 bar and temperatures between 20 50 c bond numbers for much higher pressures 0 450 bar and temperatures 25 175 c were also calculated and plotted in fig 7 using the literature values for σ and δ ρ it can be seen that the bond number stays relatively constant through the entire studied range and only a small increase with temperature is observed this analysis shows that the bond numbers of h 2 and ch 4 are indeed comparable even for this wide range of conditions this indicates that in real field processes in which buoyancy and capillary are the main acting forces h 2 ch 4 and their mixture in contact with brine will have similar wettability characteristics independent of pressure and temperature 4 conclusions in this study static contact angles for h 2 ch 4 mixtures pure h 2 and pure ch 4 in contact with brine and bentheimer sandstone rock were measured using the captive bubble cell device for a range of pressures 20 100 bar two temperatures 30 50 and two salinities pure water 5000 ppm strongly water wet conditions were observed with contact angles ranging between 25 45 for all ch 4 h 2 mixtures all of the gas mixtures showed comparable behaviour and no pressure temperature or salinity dependency was observed the size of the injected gas bubbles decreased with time due to dissolution and diffusion into the brine which allowed for static contact angle measurements for various bubble sizes our analysis showed that contact angles increased with decreasing bubble volume for the static system the acting buoyancy and surface forces allow for analytical sensitivity analysis for the captive bubble cell approach which is based on the young laplace equation to characterize the wettability three parameters in the young laplace equation affect the contact angle radius at the apex density difference and interfacial tension the sensitivity analysis showed an increase in contact angle for a decrease in radius at the apex similar to what was observed in the experiments furthermore for the range of interfacial tensions and density differences that correspond to the experimental conditions of this study the analysis showed that the changes in contact angle are such that they fall within the accuracy range of the experiment validating our results it is mathematically shown that for comparable bubble sizes and under similar bond numbers the contact angles of hydrogen and methane bubbles and their mixtures in contact with brine will indeed be comparable this indicates that for real field processes in which buoyancy and capillary are the main acting forces hydrogen methane and their mixture will have similar wettability characteristics credit authorship contribution statement leila hashemi methodology image processing writing original draft maartje boon methodology experiments writing original draft wuis glerum experiments rouhi farajzadeh methodology writing review editing supervision hadi hajibeygi conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements hadi hajibeygi and maartje boon were sponsored by dutch national science foundation nwo under vidi project admire grant number 17509 we thank admire user committee for allowing us publish this paper groups members of darsim delft advanced reservoir simulation and admire adaptive dynamic multiscale integrated reservoir earth are acknowledged for fruitful discussions during the development of this work this study was conducted in the hydrogen laboratory of the geoscience and engineering department at delft university of technology we gratefully thank the technical staff of the laboratory specially michiel slob appendix a experimental apparatus and procedure a schematic of the experimental apparatus can be seen in fig a 1 appendix b contact angle measurements in the following the contact angles for the gas brine bentheimer systems measured directly from the images at different pressure temperature and salinity values are listed tables b 1 b 11 for completeness the contact angles for h 2 which were presented in hashemi et al 2021b are included as well in addition the density values used in the calculation of the contact angles can be found in tables b 12 b 22 the densities of the mixtures were calculated based on the pure gas densities according to b 1 ρ m ρ 1 v 1 ρ 2 v 2 v 1 v 2 here ρ is density kg m 3 v is volume m 3 and subscripts m 1 2 stand for mixture gas 1 and gas 2 respectively this can conveniently be calculated using the website www fluidat com 
163,characterizing the wettability of hydrogen h 2 methane ch 4 mixtures in subsurface reservoirs is the first step towards understanding containment and transport properties for underground hydrogen storage uhs in this study we investigate the static contact angles of h 2 ch 4 mixtures in contact with brine and bentheimer sandstone rock using a captive bubble cell device at different pressures temperatures and brine salinity values it is found that under the studied conditions h 2 and ch 4 show comparable wettability behaviour with contact angles ranging between 25 45 and consequently their mixtures behave similar to the pure gas systems independent of composition pressure temperature and salinity for the system at rest the acting buoyancy and surface forces allow for theoretical sensitivity analysis for the captive bubble cell approach to characterize the wettability moreover it is theoretically validated that under similar bond numbers and similar bubble sizes the contact angles of h 2 and ch 4 bubbles and their mixtures are indeed comparable consequently in large scale subsurface storage systems where buoyancy and capillary are the main acting forces h 2 ch 4 and their mixtures will have similar wettability characteristics graphical abstract keywords underground hydrogen storage h2 ch4 mixtures wettability contact angle captive bubble cell 1 introduction development of large scale twh energy storage technologies is essential in the successful transition towards renewable energy systems therefore energy has to be converted into forms that can be stored at such large scales one of the attractive energy carriers is hydrogen h2 due to its high energy content per mass 141 86 mj kg and its carbon free combustion products hassanpouryouzband et al 2021 however there exists a major challenge in development of storage technologies for hydrogen being the lightest molecule its volumetric energy content is relatively low stone et al 2009 zivar et al 2021 more specifically it stores only about 132 kwh in 1 m 3 at relatively high pressure of 50 bars and temperature of 298 k kabuth et al 2017 as such to achieve feasible large scale storage for compressed hydrogen gas gigantic volumes are needed these volumes are beyond the technical economical land usage and safety scope of surface based storage tanks taylor et al 1986 schaber et al 2004 underground reservoirs on the other hand provide giant volumes to store hydrogen in the expected twh scales these formations can be in the form of solution mined salt caverns ramesh kumar et al 2021 or geological porous reservoirs walters 1976 tarkowski 2019 gabrielli et al 2020 including depleted hydrocarbon fields and saline aquifers hashemi et al 2021a there exist a few experiences with storing hydrogen or its mixture with methane in porous reservoirs panfilov et al 2006 kruck et al 2013 and several pilot projects are currently underway rag 2021 pérez et al 2016 however to date a rigorous understanding of many aspects related to subsurface storage of pure hydrogen and its mixture with methane is still lacking heinemann et al 2021 in some aspects underground hydrogen storage uhs is similar to that of underground gas storage ugs as both are compressed gases being stored cyclically in subsurface formations however in many aspects uhs is expected to behave quite differently than ugs firstly h 2 is very different than ch 4 gas in its molecular weight diffusivity dissolution density and surface interfacial tension secondly the cyclic loading and frequency of the green hydrogen injection and production supplied by the intermittent green energy production is expected to be much different than that of ugs lastly hydrogen purity is expected to be maintained during the storage period as sensitivities towards hydrogen impurities in fuel cells are very high laban 2020 these differences have recently motivated the scientific community to study hydrogen properties in detail specially its wettability characteristics in contact with reservoir brine and rock iglauer et al 2021 hashemi et al 2021b ali et al 2021 this is due to the fact that hydrogen will come in contact with brine whether in aquifers or porous rocks containing connate water hashemi et al 2021a heinemann et al 2021 h 2 brine rock wettability is a key factor in identification of the hydrogen interaction with reservoir brine and rock more precisely it allows for understanding the distribution of hydrogen through the porous rock micro channels according to young s equation it is characterized by the contact angle between the interface of gas brine in contact with the rock surface de gennes 1985 i e 1 cos θ σ r b σ r g σ b g here σ r b σ r g σ b g correspond to the interfacial forces between each pair of the phases respectively rock brine rock gas and brine gas young 1805 typically in geological reservoirs the adhesive forces between brine and rock are much bigger than between gas and rock because molecules in the liquid phase are much closer to each other than in the gas phase therefore the contact angle in gas brine rock systems is likely to be less than 90 degrees consequently water wet conditions can be expected during underground hydrogen storage the distribution of hydrogen and brine in the porous rock influences multiphase flow properties such as relative permeability and capillary pressure in water wet systems the non wetting phase in our case hydrogen will preferentially flow through the larger pores resulting in a higher relative permeability this facilitates the injectivity of the reservoir while the amount of capillary trapped hydrogen will be smaller both aspects are favourable for uhs alhammadi et al 2017 arif et al 2019 wettability of the h 2 brine rock system has been the focus of some recent studies all of which were conducted using water wet rocks yekta et al 2018 van rooijen et al 2021 iglauer et al 2021 hashemi et al 2021b with pure hydrogen gas these studies collectively revealed static hashemi et al 2021b and dynamic yekta et al 2018 van rooijen et al 2021 iglauer et al 2021 ali et al 2021 contact angles of hydrogen by different experimental methods captive bubble cell hashemi et al 2021b higgs et al 2021 tilted plate iglauer et al 2021 ali et al 2021 microfluidics van rooijen et al 2021 and indirectly yekta et al 2018 and directly higgs et al 2021 through core flooding techniques a summary of the measured contact angles as well as the conditions and experimental techniques can be found in table 1 the characterization of h 2 ch 4 wettability is important for comparison of uhs and ugs despite its importance there exists no rigorous study which investigates and reports how hydrogen wettability compares with that of methane and hydrogen methane mixtures of different concentrations in addition there can be cases in which h 2 mixes with the reservoir ch 4 for example if it is used as cushion gas or traces of it exists in the subsurface environment such as depleted gas fields moreover it is important for industrial applications where first h 2 ch 4 mixture is introduced in the gas grid and storage facilities as the production of h 2 scales up in the future the fraction of h 2 concentration in the mixture is expected to be further increased for example the first uhs project in europe underground sun storage by rag austria ag stored 20 h 2 and 80 ch 4 rag 2017 in a porous reservoir mixing of h 2 with ch 4 impacts the physio chemical properties of the injected hydrogen and consequently its displacement process hassanpouryouzband et al 2020 tek 2012 simon et al 2015 sáinz garcía et al 2016 this can potentially impact the upscaled multiphase flow functions of capillary pressure and relative permeability hashemi et al 2021a rücker et al 2019 kunz et al 2018 pan et al 2021 carden and paterson 1979 a correct description of these upscaled flow functions is needed to ensure the safety of underground hydrogen storage as well as to optimize the cyclic injection and production of hydrogen as such characterization of the h 2 ch 4 mixture wettability is crucially important which is the focus of the present study in this work we directly measure the static contact angles of h 2 ch 4 mixtures in contact with brine and sandstone rock using a captive bubble cell experimental methodology kaveh et al 2014 hashemi et al 2021b we systematically analyse contact angles of different size gas bubbles and different mixture concentrations by providing a modelling analysis we validate our methodology and the findings of this study the structure of the paper is as follows in the materials and methods section a description of the methodology and test conditions are presented in detail this is followed by the results and interpretation of the data a sensitivity analysis is also performed on the basis of the young laplace equation to better analyse and justify the experimental observations finally the main learning points are presented in the conclusion 2 materials and methods in this study static contact angles for h 2 ch 4 gas mixtures as well as pure ch 4 gas in contact with brine and bentheimer sandstone rock are measured using the captive bubble cell device kaveh et al 2014 hashemi et al 2021b 2 1 materials the gas mixtures consisted of 99 99 mol purity h 2 and 99 5 mol purity ch 4 both produced by linde gas company the gas mixture was prepared by filling up the pump with both gases in the desired concentration at the desired pressure having the pump with the gasses stand for one day so to allow for a fully mixed gas liquid system the brines were made by dissolving nacl in deionized water a bentheimer sandstone rock slab with dimensions of 30 6 12 mm was used in the experiments the sample was untreated and cut from the same clean bentheimer sandstone block as in the bentheimer rock sample used in hashemi et al 2021b the permeability of the sample was 2 3 darcy and the porosity was around 20 the mineral composition consisted for 95 of quartz which was evenly distributed throughout the rock matrix peksa et al 2015 the surface roughness was 0 03 mm and was determined by microscopic analysis hashemi et al 2021b the experimental conditions used for each h 2 ch 4 gas mixtures and pure ch 4 gas can be found in table 2 2 2 experimental apparatus and procedure a schematic of the experimental apparatus can be seen in fig a 1 of appendix a it is similar to the setup used by hashemi et al 2021b adapted in this study for gas mixtures the apparatus consisted of a high pressure high temperature single steel cell with a volume of 150 ml filled with brine the rock sample was attached to the centre of the cell the cell was placed in an oven to control the temperature brine was continuously injected at a flow rate of 0 02ml min from the bottom of the cell the pressure was regulated with a back pressure device connected to the top of the cell and attached to a n 2 cylinder gas bubbles of approximately 2 mm in diameter were released from a nozzle at the bottom of the cell into the brine the bubble buoyantly rose until it reached the rock surface the bubble slowly dissolved and diffused into the brine resulting in bubbles of different sizes images with a resolution of 6 9 mp 3216 2136 were taken at evenly spaced time intervals using a canon 90 camera with a maximum resolution of 12 3 mp attached to an endoscope the pressure and temperature in the cell were continuously monitored the lines of the system were thoroughly cleaned with water and ethanol at the start of each experiment to avoid any impact of contamination on the contact angle measurements 2 3 image analysis contact angles were derived for each of the images taken during the experiment using an in house matlab code which is based on the adsa p technique li et al 1992 the adsa p technique fits the best theoretical laplacian curve on the physical observed bubble interface and is based on the young laplace equation next section for this purpose the images are cropped and binarized such that the interface including the apex and contact points can be detected to find the size of the bubble the outer diameter of the nozzle is used the brine and gas density values used in the analysis are reported in appendix b tables b 12 b 22 for more details about the image analysis procedure the reader is referred to hashemi et al 2021b 2 4 theoretical analyses based on young laplace equation fig 1 shows a schematic of an axisymmetric gas bubble the blue contour indicates the gas brine interface the pressure difference across this interface i e δ p can be described by the young laplace equation li et al 1992 as 2 δ p σ 1 r 1 1 r 2 where σ n m is the interfacial tension and r 1 m and r 2 m are the principle radii of the curvature the pressure difference across the interface is due to the interfacial tension as well as the force of gravity i e 3 δ p δ p 0 δ p g at gravity capillary equilibrium the pressure difference across the interface can be described as a function of depth z m i e 4 δ p δ p 0 δ ρ g z here δ ρ kg m 3 is the density difference between the gas and the brine phase and g m s 2 is the gravitational acceleration since the apex point is taken as the reference i e z 0 no gravity term is considered there and thus one can write r 1 r 2 b at this point therefore at the apex point eq 2 can be written as 5 δ p a p e x 2 σ b by substituting eq 4 into eq 2 it is found that 6 σ 1 r 1 1 r 2 2 σ b δ ρ g z holds for any depth z in cylindrical coordinate one can write 7 1 r 1 d θ d s and 8 1 r 2 sin θ x here θ is the contact angle and s m is the distance along the surface contour as illustrated in fig 1 by replacing r 1 and r 2 in eq 6 with eqs 7 and 8 respectively one obtains 9 d θ d s 2 b δ ρ g z σ sin θ x eq 9 can be stated in dimensionless form as 10 d θ d s 2 b δ ρ g r 2 σ z sin θ x where the bubble radius r is used as characteristic length scale i e x x r the second term on the right hand side of eq 9 is the bond number n b o defined as 11 n b o δ ρ g r 2 σ n b o is the ratio of gravitational forces to interfacial forces alvarez et al 2009 hessel et al 2004 berg 2010 3 results and discussion contact angles for h 2 ch 4 brine rock systems were measured using the captive bubble cell method although this method does not take into account the impact of pore structures and flow dynamics on the wettability it sheds lights on the wettability behaviour in systems where buoyancy and capillary are the main driving forces furthermore it provides insights on the overall wettability state of sandstone rock in contact with gas mixtures of h 2 ch 4 and brine the experiments were carried out for a range of pressures two temperatures and two different brine salinities the results of which can be seen in fig 3 and tables b 1 b 11 of appendix b the results for pure h 2 are based on the experimental observations of hashemi et al 2021b who used the same captive bubble cell device to measure contact angles for the h 2 brine bentheimer system in addition in fig 6 the contact angles for pure h 2 measured on a third bentheimer sandstone sample are presented to highlight the systematic change in contact angle that is observed when different samples are used all bentheimer sandstone samples used in this study were cut from the same bentheimer sandstone block to verify whether changes in the chemical composition or rock structure occurred over time and consequently changed the contact angle experiments were repeated fig 2 a shows the contact angle for different bubble sizes for three hydrogen experiments carried out on the same rock slab between each of the experiments the rock slab was taken out of the apparatus and put in the vacuum oven to dry it can be seen that similar results were obtained for each of these experiments this indicates that no mineral alteration or other changes in the structure of the rock surface has taken place that significantly impacted the wettability the bubble size decreased with time which is likely due to dissolution and diffusion into the brine to verify whether dissolution into the brine would have an impact on the contact angle measurement experiments were carried out using brine with different levels of hydrogen saturation fig 2 a shows the contact angle versus volume while fig 2 b shows the contact angle versus time for a system where the brine was highly saturated with hydrogen and a system where the brine was unsaturated it can be seen that the dissolution rate of the unsaturated brine is almost 10 times higher than the highly saturated brine however the contact angles obtained are the same in each of the experiments this shows that the contact angle is a function of the bubble size and does not depend on the saturation level using unsaturated brine allowed us to make contact angle measurements for a range of bubble sizes it is observed that the contact angle increases with decreasing bubble size the minimum and maximum contact angle values in fig 3 correspond to the largest and smallest bubble sizes 3 1 effect of bubble size and gas composition the gas bubbles were released in under saturated brine solutions and slowly dissolved and diffused into the brine images were taken every minute except for pure h 2 where the time step between images was four minutes for each of these images the contour of the interface was detected and contact angles were calculated fig 4 shows the contours of the interfaces at each time step as well as the corresponding bubble volume and contact angle for three h 2 ch 4 mixtures pure h 2 and pure ch 4 for a system with pure water at 30 c and 100 bar due to the roughness of the sample pinning of the gas bubble occurred and as a result the dissolution was not symmetric in some cases this pinning led to higher contact angles as can be observed in fig 4 e for 50 h 2 ch 4 mixture for bubbles bigger than 4 mm 3 overall the gas bubbles of the different mixtures show comparable behaviour although the contact angles of the h 2 bubbles are slightly higher this is likely due to the fact that a different bentheimer sandstone sample although obtained from the same block was used for the h 2 experiments since the contact angles of the different mixture compositions are indistinguishable the contact angle is a function of the bubble volume and no distinction can be made between the different gases this behaviour of increasing contact angle with decreasing bubble volume has also previously been reported for co2 brine rock systems kaveh et al 2014 drelich 1997 haeri et al 2020 jung and wan 2012 3 2 effect of pressure temperature and salinities for all experimental conditions water wet behaviour was observed with contact angles ranging between 25 45 for all h 2 ch 4 mixtures as well as for pure h 2 and pure ch 4 as can be seen in fig 3 no obvious correlation between the measured contact angle and the pressures temperatures or salinity could be observed note that the range of bubble sizes was different for the different experiments high contact angle values correspond to smaller bubble sizes however for similar bubble sizes all the data points fall within the accuracy range of the conducted experiments 3 to validate our findings a sensitivity study of the captive bubble cell approach to measure wettability based on the young laplace equation has been performed this is presented in the following section 3 3 sensitivity analysis of the captive bubble cell approach the contact angles presented in this study were obtained by fitting eq 9 onto the captured images of the gas bubbles a closer look at eq 9 shows that three parameters impact the fitting curve formula apex radius b density difference δ ρ and interfacial tension σ of which the combined impact can be characterized by the bond number as defined in eq 11 to investigate the impact of each of these three parameters on the contact angle a systematic sensitivity analysis has been performed the results of which can be seen in fig 5 based on the experimental observations the ratio of the surface position to the height h see fig 1 is changing from 0 8 to 0 95 corresponding to the biggest and smallest bubble sizes respectively therefore as the base case σ was set to 60 mn m δ ρ to 1000 kg m 3 b to 1 mm and the surface location z was set to 0 9 of the bubble height it is important to note that the ratio of the surface position to the height h depends on the wetting state of the system fig 5a shows the results of the analysis for the apex radius effect it can be seen that the contact angle increases with decreasing apex radius in a similar fashion as was observed from the experiments the effect of the density contrast on the contact angle is presented in fig 5b here the contact angle increases with decreasing δ ρ however for the δ ρ of this study which is in the range of 900 1000 kg m 3 the contact angle ranges between 41 7 43 4 which is within the accuracy range of the measuring technique fig 5c shows the effect of interfacial tension on the contact angle it can be seen that the contact angle increases with increasing interfacial tension for the pressures and temperatures of our study the interfacial tensions of the different h 2 ch 4 mixtures likely ranged between 50 70 mn m hassanpouryouzband et al 2021 chow et al 2018 pan et al 2020 in this range the contact angle varies between 38 3 44 for a particular set of conditions this is again within the accuracy range of the measuring technique the above analysis shows that no significant pressure temperature and salinity dependency is expected in systems where buoyancy and capillarity are the main driving forces such as our captive bubble cell which validates our results however in different systems where other driving forces come into play including different experimental measurement techniques these factors could have a bigger impact on the contact angle as has been observed in literature the bond number for the h 2 water and ch 4 water experiments are plotted in fig 6a as a function of radius at the apex point it can be seen that the bond numbers of both the h 2 water and ch 4 water systems are comparable and depend on the size of the bubble fig 6b shows that under similar bond numbers and similar bubble sizes the contact angles of h 2 and ch 4 bubbles and their mixtures are indeed comparable the slight difference between the contact angle plots is due to the fact that different bentheimer sandstone samples were used the experiments of this study were carried out for pressures ranging between 20 100 bar and temperatures between 20 50 c bond numbers for much higher pressures 0 450 bar and temperatures 25 175 c were also calculated and plotted in fig 7 using the literature values for σ and δ ρ it can be seen that the bond number stays relatively constant through the entire studied range and only a small increase with temperature is observed this analysis shows that the bond numbers of h 2 and ch 4 are indeed comparable even for this wide range of conditions this indicates that in real field processes in which buoyancy and capillary are the main acting forces h 2 ch 4 and their mixture in contact with brine will have similar wettability characteristics independent of pressure and temperature 4 conclusions in this study static contact angles for h 2 ch 4 mixtures pure h 2 and pure ch 4 in contact with brine and bentheimer sandstone rock were measured using the captive bubble cell device for a range of pressures 20 100 bar two temperatures 30 50 and two salinities pure water 5000 ppm strongly water wet conditions were observed with contact angles ranging between 25 45 for all ch 4 h 2 mixtures all of the gas mixtures showed comparable behaviour and no pressure temperature or salinity dependency was observed the size of the injected gas bubbles decreased with time due to dissolution and diffusion into the brine which allowed for static contact angle measurements for various bubble sizes our analysis showed that contact angles increased with decreasing bubble volume for the static system the acting buoyancy and surface forces allow for analytical sensitivity analysis for the captive bubble cell approach which is based on the young laplace equation to characterize the wettability three parameters in the young laplace equation affect the contact angle radius at the apex density difference and interfacial tension the sensitivity analysis showed an increase in contact angle for a decrease in radius at the apex similar to what was observed in the experiments furthermore for the range of interfacial tensions and density differences that correspond to the experimental conditions of this study the analysis showed that the changes in contact angle are such that they fall within the accuracy range of the experiment validating our results it is mathematically shown that for comparable bubble sizes and under similar bond numbers the contact angles of hydrogen and methane bubbles and their mixtures in contact with brine will indeed be comparable this indicates that for real field processes in which buoyancy and capillary are the main acting forces hydrogen methane and their mixture will have similar wettability characteristics credit authorship contribution statement leila hashemi methodology image processing writing original draft maartje boon methodology experiments writing original draft wuis glerum experiments rouhi farajzadeh methodology writing review editing supervision hadi hajibeygi conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements hadi hajibeygi and maartje boon were sponsored by dutch national science foundation nwo under vidi project admire grant number 17509 we thank admire user committee for allowing us publish this paper groups members of darsim delft advanced reservoir simulation and admire adaptive dynamic multiscale integrated reservoir earth are acknowledged for fruitful discussions during the development of this work this study was conducted in the hydrogen laboratory of the geoscience and engineering department at delft university of technology we gratefully thank the technical staff of the laboratory specially michiel slob appendix a experimental apparatus and procedure a schematic of the experimental apparatus can be seen in fig a 1 appendix b contact angle measurements in the following the contact angles for the gas brine bentheimer systems measured directly from the images at different pressure temperature and salinity values are listed tables b 1 b 11 for completeness the contact angles for h 2 which were presented in hashemi et al 2021b are included as well in addition the density values used in the calculation of the contact angles can be found in tables b 12 b 22 the densities of the mixtures were calculated based on the pure gas densities according to b 1 ρ m ρ 1 v 1 ρ 2 v 2 v 1 v 2 here ρ is density kg m 3 v is volume m 3 and subscripts m 1 2 stand for mixture gas 1 and gas 2 respectively this can conveniently be calculated using the website www fluidat com 
164,recent years have witnessed global massive property losses and casualties caused by extreme precipitation and its subsequent natural disasters including floods and landslides china is one of the countries deeply affected by these casualties if the statistical characteristics and laws of extreme precipitation could be clearly grasped then the negative impacts triggered by it may be minimized china is a vast country and diverse in climate and terrain hence different regions may be suitable for different analyses and research methods therefore it is necessary to clarify the research progress methods and current status of extreme precipitation across the country this paper attempts to provide a comprehensive review of techniques and methods used in extreme precipitation research and engineering practice and their applications the literature is reviewed focusing on seven aspects 1 annual maxima method am 2 peaks over threshold method pot 3 probable maximum precipitation pmp 4 non stationary analysis of precipitation extremes 5 intensity duration frequency curves idf 6 uncertainty in extreme precipitation frequency analysis and 7 spatial variability of extreme precipitation research on extreme precipitation in china is generally based or centered on the above seven aspects the current study aims to provide ideas for further research on extreme precipitation frequency analysis and its response to climate change and human activities graphical abstract image graphical abstract keywords extreme precipitation china annual maxima peaks over threshold pmp non stationary idf uncertainty spatial variability introduction extreme precipitation drives adverse phenomena with implications on both human and natural systems soil erosion with related loss of organic matter and nutrients dondini et al 2018 damage of agricultural production fatal landslides and damage of infrastructure are only some of the repercussions of extreme precipitation the most important of which is heavy flooding doocy et al 2013 reported over half a million fatalities and about 350 thousand injuries due to rain induced floods during 1980 2009 with the number of people affected by floods approaching 3 billion in china alone the documented flash flood events from 2000 to 2015 caused about 900 deaths per year on average he et al 2018 such catastrophic impacts can be alleviated through more efficient heavy precipitation modeling and forecasting smith et al 2016 precipitation is usually considered a random variable that follows a probability distribution the estimation of return periods corresponding to any rainfall amount is then possible the extreme events occupy the distribution tail which is the upper part of the distribution distributions can be categorized into heavy and light tailed in regards to their tail behavior heavy tailed distributions are characterized by more intense and frequent extreme events e g papalexiou et al 2013 the definition and study of extreme rainfall constitute challenging tasks and the investigation of the rainfall tail is a dynamic constantly developing process for an extensive review on probabilistic methods used for hydroclimatic extremes see nerantzaki and papalexiou 2022 studies performed on a global scale provide evidence that daily precipitation extremes are more adequately described by heavy rather than light tailed distributions which were very popular until recently nerantzaki and papalexiou 2019 papalexiou et al 2013 papalexiou and koutsoyiannis 2016 2013 2012 using global datasets and a variety of approaches researchers almost unanimously agree with the heavy tail conclusion cavanaugh et al 2015 nerantzaki and papalexiou 2019 papalexiou et al 2013 papalexiou and koutsoyiannis 2016 2013 serinaldi and kilsby 2014 wilson and toumi 2005 the vast country of china is included in most of the large scale assessments of extreme precipitation cavanaugh et al 2015 mention that the east asia coasts including those of china display very heavy tails due to the influence of cyclone landfalls nerantzaki and papalexiou 2019 also noticed heavier tails along the east coasts and lighter tails in the central part of the country papalexiou et al 2013 and papalexiou and koutsoyiannis 2013 had drawn similar conclusions the assessment of the shape parameter of the stretched exponential distribution performed by wilson and toumi 2005 also provided the same spatial distribution of tail heaviness in china due to the large geographic and climatic variability of this immense country the extreme precipitation distribution differs among regions guan et al 2017 li and hu 2019 τhis diversity requires the use of different approaches to estimate extreme precipitation across the country using reliable and reasonable methods to understand the statistical characteristics of extreme precipitation and calculate the design rainstorm is of great significance to the national economy and people s livelihood in recent years a significant amount of research has been carried out on sample selection probability distribution model selection parameter estimation and non stationary frequency analysis of precipitation extremes based on the relevant research results in the country this paper reviews the literature mainly focusing on seven aspects 1 annual maxima method am 2 peaks over threshold method pot 3 probable maximum precipitation pmp 4 non stationary analysis of precipitation extremes 5 intensity duration frequency curves idf 6 uncertainty in extreme precipitation frequency analysis and 7 spatial variability of extreme precipitation the study clarifies the research progress and context of precipitation extremes in china aiming to provide ideas for further research on the frequency analysis of precipitation extremes in the current changing environment annual maxima the annual maxima am sampling constructs series composed of the annual maxima precipitation using daily or hourly values the frequency analysis based on am in china mainly includes distribution fitting with the optimal probability distribution function pdf parameter estimation of the pdf extreme rainstorm design and rainstorm risk analysis including spatial distribution characteristics of shape parameters at present chinese scholars have carried out many studies on the above aspects and obtained some unique regional characteristics and laws for the country of china fitting an appropriate probability distribution model is the premise of extreme precipitation frequency analysis the design rainstorm is one of the most important parameters for the design of river basin and urban flood control facilities therefore the core content of the frequency analysis is to determine the optimal probability distribution and select a robust parameter estimation method for rainstorm design annual maxima distributions the pearson type iii distribution p e 3 is recommended by china s standard guidelines under the am framework for rainstorm and flood design ministry of water resources of the people s republic of china 2006 since the 1960s p e 3 has been generally adopted in extreme flood and rainstorm design of water conservancy and hydropower engineering in china however the world meteorological organization recommends the use of the generalized extreme value g e v distribution as the theoretical distribution for am series world meteorological organization 2009 for short duration am series of extreme precipitation such as annual maximum precipitation of 1 h or 2 h previous studies have shown that p e 3 can be used as a theoretical frequency distribution niu et al 2019 niu et al 2019 used am on 90 sets of short duration extreme precipitation series ranging from 10 to 120 minutes at 18 stations in the shaanxi province and evaluated the applicability of six distributions including p e 3 and g e v they found that p e 3 was suitable for 90 of the data series and therefore can be used as the theoretical distribution function of short duration rainstorms in the study area for relatively longer durations such as 1 day and 5 day studies have pointed out that the performance of p e 3 is not ideal in some areas du et al 2014 zhang et al 2015a du et al 2020 zhang et al 2015a studied the annual summer and winter maximum daily precipitation series of 10 representative stations in 10 major river basins in china using the kolmogorov smirnov k s and anderson darling a d tests as selection criteria for distributions they found that the degree of dispersion and skewness of the am series leads to different curves for the same station in these three periods in general g e v usually passed the two hypothesis tests at the same time and was the optimal distribution for most am series followed by the three parameter lognormal distribution l n 3 the p e 3 distribution passed the hypothesis test fewer times song et al 2007 provided the first application of the g e v distribution on annual extreme precipitation events in china some studies are not limited to distribution fitting but also focus on the tail characteristics i e tail thickness of the g e v gu et al 2017a gu et al 2017a evaluated 728 meteorological stations in china using am and analyzed the statistical characteristics of the g e v applied on annual and seasonal extreme precipitation they pointed out that the g e v distribution tail of annual and seasonal extreme precipitation was heavy for most regions of china having no upper boundary in general the tails are heavier in north china than in south china as well as in the autumn and winter compared to summer and annual series in addition to ground based data there are also studies using g e v with climate model generated data to explore the response of extreme precipitation to climate change or human activities li et al 2017 li et al 2018a li et al 2019a wen et al 2016 based on ground data and the coupled model intercomparison project phase 5 cmip5 li et al 2017 focused on the probability based index derived from 1 day and 5 day am precipitation by fitting g e v they assessed the effects of anthropogenic forcings and other external factors on observed increases in extreme precipitation in addition to g e v and p e 3 distributions such as l n 3 and the five parameter wakeby w a k have also been used to study the statistical characteristics of am series in china jiang et al 2008 su et al 2009 fischer et al 2012 zhang et al 2017a lu et al 2016 su et al 2009 used the g e v generalized pareto g p generalized logistic g l o and w a k distributions with am series of observed data in the yangtze river basin and grid data derived from the echam5 mpi om climate model they showed that w a k has a better fitting and yields the minimum value of the k s statistic they also found that the design precipitation under the current 50 year return period level may become more frequent in the future turning into a 25 year return period some studies have also pointed out that when selecting a probability distribution for hydrological analysis the distribution with fewer parameters should be given priority blum et al 2017 therefore although the w a k distribution has a better fit due to the data length limitation there may be significant sampling errors papalexiou et al 2013 thus the w a k distribution is not always the best choice and its applicability needs further study annual maxima parameter estimation methods reasonable and accurate estimation of distribution parameters is fundamental for extreme precipitation frequency analysis and by extension rainstorm design and risk analysis estimation the methods used for probability distribution parameter estimation in china can be divided into two categories one is the analytical method including the commonly used method of moments mom christopeit 1994 pearson 1902a 1902b weighting function method probability weighted moments pwm greenwood et al 1979 hosking et al 1985 linear moments l moments greenwood et al 1979 hosking et al 1985 sillitto 1951 etc another method is the optimized curve fitting method which establishes an objective function and iteratively adjusts the parameter set so that the known theoretical frequency curve fits the empirical frequency point data optimally in engineering practice in china the combination of the two categories of methods is often adopted to determine the pdf parameters lei et al 2017 the mom is simple and easy to operate it is one of the earliest methods used for parameter estimation wang and chen 1994 used the gumbel g distribution a special form of the g e v with am in chengdu city combined with successive approximations least squares and the mom to estimate the distribution parameters the error between the estimated rainstorm and the observed values was assessed to determine the optimal parameter estimation method the mom was the most robust method with a relative error less than 1 however when the sample size is small the method yields large errors and the design value has a large negative bias jin 2005 the pwm method is applicable when the analytic expression of the inverse function of the probability distribution can be obtained since the inverse function of the p e 3 distribution is not easy to obtain the application of this method in china is limited song and ding 1988 deduced the relationship between the parameters of p e 3 and pwm and showed that when estimating the parameters of the p e 3 distribution the pwm method is better than mom due to its impartiality and effectiveness then li 1989 derived a simplified formula of the pwm of p e 3 and made a look up table making this method more convenient the l moments are derived from the pwm compared to the pwm the l moments are more convenient because they directly provide scale and shape measures of the pdf su et al 2009 yang et al 2010 yuan et al 2017a even for the parameter estimation of small samples these parameters are unbiased chen et al 2014 at present l moments are widely used in research in china cai et al 2007 jiang et al 2009 su et al 2009 yang et al 2010 fischer et al 2012 chen et al 2014 yuan et al 2017a cai et al 2007 applied the g distribution to fit the am series of 210 stations in eastern china they showed that compared to the method of moments the l moments improved the goodness of fit of the g greatly and were successful in estimating the extreme quantiles under the given return period liang et al 2013 used monte carlo analysis to simulate the am series of 96 stations in taihu lake basin they compared the parameters estimated by l moments and mom and found that the parameters estimated by the former were more accurate unbiased and robust the maximum likelihood ml fisher and russell 1922 method is also frequently used for parameter estimation in china song et al 2007 fischer et al 2012 zhang et al 2012 however huang et al 1994 revealed that ml is very sensitive to the sample s minimum value when estimating parameters of the p e 3 also when the coefficient of variation of the series is greater than 2 the p e 3 parameters have no solution in addition studies have pointed out that due to the cumbersome calculation of the ml an analytical solution cannot be obtained for some distribution curves including the p e 3 with limitations in practical applications liang et al 2013 from the above literature review it can be concluded that methods other than l moments present great limitations therefore in recent years the l moments have become the mainstream method for parameter estimation of extreme precipitation using am since the p e 3 is recommended in china s standard guidelines the research on parameter estimation is basically based on this distribution the gauss newton iterative algorithm method is recommended by guidelines hydrology bureau of changjiang water resources commission of the ministry of water resources 1995 for the numerical solution of the p e 3 distribution parameters but this method is affected by the initial value and the convergence speed is slow li and song 2009 in recent years artificial intelligence genetic and ant colony algorithms among others have been used for the parameter estimation of the p e 3 li and song 2009 lei et al 2017 in these cases an optimization algorithm is used to find the optimal value depending on the objective function in addition to the algorithm itself research also focuses on determining the objective function qiu et al 1998 xie and zheng 2000 copulas for the dependence among precipitation extremes in china the hydrological frequency analysis in practical engineering only involves one aspect of extreme events however correlation generally exists among the internal attributes of extreme events such as the duration and intensity of rainstorm the peak and volume of flood the duration and intensity of drought etc liu and guo 2021 investigating the dependence of these attributes in extreme events and conducting multivariate hydrological frequency analysis is bound to be of great benefit to improve the accuracy and reliability of hydrological frequency analysis this can be achieved with the copula function which describes the nonlinear and asymmetric correlation between variables without being restricted by the type of the marginal distribution guo et al 2008 xiong and guo 2004 applied the copula function to construct the joint distribution of flood peak and flood volume with the gumbel hougaard copula in the yangtze river basin they are one of the first groups of scholars in china to apply the copula function in hydrology in china the application of the copula function on extreme precipitation mainly focuses on the construction of bivariate joint distributions to describe the interdependence and intrinsic relationship between different indices obtained by am of rainstorm and then calculate the design rainstorm and evaluate the flood risk under a multivariable framework wang et al 2017a zhang et al 2013 based on the gumbel hougaard copula zhang et al 2008 constructed the joint distribution of annual maximum 1 day and 7 day precipitation with p e 3 as marginal distributions and calculated the value and process of the design rainstorm their results showed that the design rainstorm based on the joint distribution is safer than that based on univariate analysis and it is more conducive to flood control safety zhang et al 2012 applied the archimedean copula and elliptical copula families to analyze the joint probability distribution of 8 precipitation extremes including continuous and discrete variables and calculated the joint return periods of two different precipitation indices decreasing joint return periods of p75 total precipitation amount larger than the 75th percentile within a year mm and i75 precipitation intensity of larger than the 75th percentile mm day were found in most of the xinjiang area implying that the probability of concurrent occurrence of high intensity extreme precipitation increased after 1980 jhong and tung 2018 obtained four extreme precipitation indices from a weather generator model conducted with general circulation models and investigated the future changes in the joint probability behaviors of precipitation extremes in shih men reservoir in northern taiwan province china they found that the occurrence probability of the joint extreme precipitation events considering amount and intensity may increase in the upstream of the study area causing high risk of floods in the future in addition to the frequency analysis of multi characteristic attributes of extreme precipitation events the copula function is widely used for the encounter probability of rainstorm rainstorm and flood and the joint distribution of rainstorm and various environmental extreme events yan et al 2007 liu and chen 2009 yan and chen 2013 zhao et al 2012 analyzed the probability of the synchronous occurrence of precipitation extremes including annual maxima of 1 3 5 and 7 day precipitation between different regions in the pearl river basin the lognormal distribution was selected as the marginal distribution of these extreme variables and most of the joint distributions were fitted to the gumbel hougaard copula family based on akaike information criterion aic their results revealed the underlying linkages between precipitation and floods from geographical perspective yan and chen 2013 used the clayton copula function to quantify the synchrony and asynchrony of precipitation including combined frequencies of wet dry and normal conditions in the source area and the receiving area of the middle route of the south to north water transfer project they provided corresponding scientific basis for the implementation and operation of the water conveyance project besides application some studies also focus on the improvement of multivariable frequency analysis methods qian et al 2018 proposed the maximum entropy method to estimate the parameters of the gumbel and gumbel hougaard copula function which requires only the lower and upper bounds of two hydrological variables and tested their method in heihe and jinghe river basin their results proved maximum entropy estimation to be a reliable and robust parameter estimation method in data scarce regions peaks over threshold although the am method is one of the most commonly used extreme precipitation sampling methods it selects one precipitation value every year without considering the inter annual differences in climatic conditions this may lead to the inclusion of false data into the extreme precipitation series or the loss of valuable information ding et al 2011 xia et al 2012 the peaks over threshold pot sampling method sets a threshold and considers the values exceeding it as the extreme precipitation series the process improves the am method to a certain extent and makes more effective use of the precipitation data the threshold selection affects the analysis of extreme precipitation since different thresholds lead to series with different statistical characteristics liu et al 2017a the current research on pot in china mainly focuses on two aspects one is to investigate the selection and the regional characteristics of the threshold the other is the frequency analysis threshold selection the threshold should be in line with the climatic characteristics of the study area and the size of the extreme precipitation series should be considered for its selection wang et al 2020 at present the main methods for threshold selection in china are the percentile method and the average annual occurrence frequency method the most commonly used percentile thresholds in china are the 95th 99th and 99 5th xia et al 2012 wang et al 2015 he et al 2017 kong et al 2019 xia et al 2012 used the 99 5th percentile threshold to select extreme precipitation series in the huaihe river basin he et al 2017 adopted the 99th percentile as the extreme precipitation threshold in the minjiang river basin the average annual occurrence frequency method assumes that extreme precipitation over the threshold follows the poisson distribution the study of jiang et al 2009 showed that a threshold that is too large or too small affects the correlation coefficient between the simulated theoretical and measured values of extreme precipitation in their study the occurrence number was set to 1 and they pointed out that the daily extreme precipitation during summer in eastern china conforms to the g p distribution guo et al 2010 studied the relationship between the 50 year return period of precipitation the scale parameters of the g p and the length of the over threshold sample series under different occurrence numbers in the bohai bay region the optimal occurrence number for each representative station was determined based on the value of the scale parameter and the stability of the return period the optimal average annual occurrence frequency was estimated between 1 and 2 5 in addition some studies have explored the impact of threshold selection on extreme precipitation analysis liu et al 2013 applied various threshold selection methods namely the absolute critical value the percentile the parametric and the detrended fluctuation analysis to construct extreme precipitation series in the pearl river basin they noted that the detrended fluctuation analysis could provide a unique extreme precipitation threshold set for a large basin with uneven spatiotemporal precipitation distribution this method was the most suitable for threshold selection in pearl river basin however it requires a large rainfall series and complicated calculations a small number of studies used graphical methods to determine the threshold qu et al 2014 applied the mean residual life plot method to obtain the pot series threshold and li et al 2014 used the hill plot at present studies on the selection of optimal thresholds focus on local areas or watersheds and few have aimed to explore the threshold selection criteria at the national scale zhang et al 2017b used the average annual occurrence frequency method and the percentile method to propose selection criteria of extreme precipitation thresholds for different regions across china they found a correlation between the optimal threshold scheme and the division of dry and wet regions in china in humid regions such as the southeast the 90th 94th percentile should be used as threshold the 94th 97th percentile is suggested for semi humid and semi arid regions such as western and northern regions while in arid areas such as northwest china the 97th 99th percentile is more suitable peaks over threshold distributions in terms of frequency analysis the contents of the pot and am series are basically the same including the probability distribution selection parameter estimation etc the g p distribution is often used to describe the probability distribution of the series obtained by the pot sampling method cheng et al 2008 analyzed the applicability of the g e v and g p distributions for the pot series in chongqing using the k s correlation coefficient and mean square error as evaluation criteria they verified that the g p distribution fits the observed data of the pot series significantly better than g e v ding et al 2008 estimated the relationship between the distribution parameters and quantiles of g p and g e v and used it to calculate the design rainstorm in beijing considering its thick tail characteristics g p is more suitable for the frequency analysis of extreme precipitation pei et al 2018 studied the spatial and temporal characteristics of extreme precipitation in the yangtze river delta by fitting g p to the pot series and found that the extreme precipitation is likely to be greatly affected by large scale monsoons and urbanization kong et al 2019 used the 90th 95th and 99th percentile thresholds to extract the extreme precipitation series they used the weibull distribution to fit these series and analyze the spatial variation characteristics of precipitation intensity for different return periods in china their results showed that it is reasonable to use the 95th and 99th percentiles as thresholds to study extreme rain and precipitation respectively in eastern china the spatial pattern of precipitation intensity was also found to vary greatly under different thresholds for the same return period peaks over threshold parameter estimation methods the commonly used methods for parameter estimation based on the pot series mainly refers to g p distribution are similar to those of am the l moments and ml are the main parameter estimation approaches jiang et al 2009 used l moments to estimate the parameters of the g p distribution and proved the suitability of the method li et al 2014 used both l moments and ml to obtain the parameters of the g p through a fitting test it was found that the design values obtained by the two methods had little difference however the threshold must be determined first for the g p introducing uncertainty through subjectivity zhao and zhai 2015 applied the pickands bootstrap moment estimation method for the parameter estimation of extreme precipitation in beijing city they found that this method can obtain a reasonable pot threshold while estimating the parameters at the same time compared to the results of the l moments with the same threshold the pickands bootstrap moment provided better fitting comparison of am and pot in china there are numerous studies analyzing and comparing the applicability of the am and pot for frequency analysis and rainstorm design song et al 2018 gao and xie 2016 xia et al 2012 he et al 2017 he et al 2020 su et al 2008 yuan et al 2017b xia et al 2012 constructed am and pot series using the 99 5th percentile as a threshold in the huaihe river basin they selected the g e v g p and gamma as candidate distributions and found that the best distribution for the am series is g e v and the optimal for the pot series is g p for the same return period the design rainstorm based on am is lower than that based on pot from the perspective of disaster prevention and mitigation and data reliability the pot series better describes the extreme precipitation events in the huaihe river basin compared to am he et al 2020 used the g p and g e v distributions to fit the pot 99 5th percentile and am series respectively in beijing jinan and shenzhen they applied the markov chain monte carlo mcmc method to estimate the distribution parameters they showed that the confidence interval of the design rainstorm obtained by g p exhibits lower uncertainty under the same return period the design rainstorm obtained by g e v is larger he et al 2017 explored the applicability of different distributions to extreme precipitation at 15 stations across the minjiang river basin the optimal distribution of all am series was g e v and the optimal distribution of 13 stations of the pot series was g p the design rainstorm for different return periods calculated by the optimal distribution of the am and pot series is quite different therefore special attention should be paid to the construction of extreme precipitation series and the selection of the distribution for the design rainstorm furthermore lu et al 2016 found that for the same station and time series length the range and standard deviation of the pot series are smaller while the kurtosis skewness and mean value are larger compared to the am series indicating that the pot series is more stable due to the fact that numerous indices obtained by am or pot series can describe extreme precipitation it is difficult to determine which index is the most representative so some studies used the results of prototype analysis of extreme precipitation observations as the research object su et al 2017 in addition to the statistical indicators there is another indicator named event based extreme precipitation which is defined as a continuous process of precipitation over several days including at least one daily precipitation exceeding the 90th or larger quantiles this kind of precipitation often occurs in southern china which has also attracted the attention of some scholars shang et al 2020 continuous precipitation including event based extreme precipitation may cause more serious floods and special attention should be paid in future research studies on extreme precipitation in china based on the sampling method including the region of application the data used the period of analysis the parameter estimation methods and distributions and the main conclusions are summarized in table 1 probable maximum precipitation estimation method probable maximum precipitation pmp is defined as the greatest depth of precipitation for a given duration that is physically possible for a design watershed or a given size storm area at a particular location at a given time of the year with no allowance made for long term climatic trends world meteorological organization 2009 pmp is the highest standard for flood control design of water conservancy and hydropower projects in china and it is also the flood protection standard for nuclear power projects national energy administration of the people s republic of china 2010 national standards of the people s republic of china 2014 it can be divided into two categories recommended by wmo statistical estimation method and hydrometeorological method hydrometeorological method includes the rainstorm maximization method rainstorm transposition method and generalized estimation method the pmp methods originate in the united states but in the case of china these methods have made some minor adjustments the main estimation methods for pmp in china are described in the next sub sections and the spatial distribution and study progress of pmp methods in china is shown in fig 1 statistical estimation method the statistical estimation method refers to hershfield s km value method hershfield 1961 this method is based on chow s general frequency equation and derived from the modified frequency analysis of am series or precipitation quantile which belongs to quasi statistical methods koutsoyiannis 1999 liao et al 2020 the formula of the pmp statistical estimation method is x m x n k m s n and k m x m x n 1 s n 1 where the frequency factor km can be estimated by samples x n 1 and s n 1 are the mean value and the standard deviation respectively of the annual maximum series with the largest term removed according to hershfield s km value method the measured maximum rainfall is not included in the am series mean value and standard deviation that is the maximum event is observed after the basic statistics are determined when the method was introuced in china it was modified and the km was replaced with the standardized variable i e the deviation coefficient φ m the maximum deviation from the mean of a sample is scaled by the standard deviation of the sample and pmp is estimated from the actual heavy rain the following equation is generally used to calculate pmp x m x n φ m s n and φ m x m x n s n when calculating the mean and standard deviation the maximum term of the annual maximum series is retained in the 1970s the statistical estimation method was used for the 24 hour pmp mapping of shaanxi and hubei provinces in china hua et al 2007 believed that this method is just calculated by the actual heavy rain and there is no concept of amplification so the calculated pmp value is often small they introduced the concept of amplification into the statistical estimation method by using the moisture maximization to amplify the locally measured rainstorm then the amplified value was substituted into the above frequency equation to calculate the pmp and applied in ningde city they showed that the pmp obtained with the statistical estimation method alone is 825 mm while the value estimated by the statistical estimation combined with the amplification concept is 1096 mm some scholars did not apply the φ m commonly used in china when calculating pmp but continued to use the km value as early as 1981 lin 1981 discussed the statistical relationship between the traditional frequency factor km and the standardized variable φ m he derived the relationship between them and obtained the expression of the range of φ m related to the data length n he proved that km is a consistent estimator of φ m and has the property to approach the upper limit of φ m as n increases he also presented the length of data needed to obtain reliable pmp estimations for specific φ m values τhe shortest series length satisfying the estimation conditions is n m φ m 2 2 summarized as the modified km method lin 1981 showed that φ m is the screening condition of data series availability and it is not used to replace km however the modified km method does not reveal the essence of rainstorm formation from the perspective of physical causes and it still does not properly solve the fundamental problem of instability in short term data frequency calculation lin 1981 believes that this method can only be a transition method from the frequency calculation to the hydrometeorological method lan et al 2017 used the modified km method to calculate the 24 h pmp in hong kong china they obtained the value of 1753 mm which was substantially greater than 1250 mm given by chang and hui 2001 using the method of moisture maximization they pointed out that this difference may stem from the use of totally different methods based on different data the modified km method can be used as a preliminary method to obtain the pmp of a single station or small watershed however although the modified km method utilizes data from the whole study area the results of pmp mainly represent potential rainstorm centers and there is no concept of temporal and spatial distribution the pmp estimation is generally based on gauge records and in this case it can only be calculated at specific points or a small area where the gauges are located yang et al 2018 provided a new perspective of data sources for pmp estimation in sparsely gauged or ungauged areas they explored the above mentioned statistical method based on satellite precipitation datasets and found that cmorph ashouri et al 2015 and 3b42v7 huffman et al 2007 can be well applied for the estimation of pmp and underlined the potential of the satellite data in pmp estimation rainstorm maximization method the main idea of the rainstorm maximization method is to consider the measured rainstorm as a typical example select the relevant influence factors and amplify reasonably this method mainly includes the moisture maximization moisture and wind maximization and moisture transport efficiency maximization lin et al 2018 svensson and rakhecha 1998 zhan and zhou 1984 as early as the 1970s zou 1977 used the water vapor convergence index maximization to estimate the pmp in the eastern part of the qinghai tibet plateau they pointed out that this method is suitable for a small scale 24h pmp estimation hua et al 2007 combined the moisture maximization method with the statistical estimation method to assess the pmp in ningde city they considered this approach to be more reasonable than the traditional statistical estimation method although the moisture and wind maximization mwm is widely used to estimate the 24 hour pmp zhou et al 2020 the variables for calculating the mwm factor do not consider the main direction of moisture inflow tending to overestimate the pmp therefore liang et al 2017a proposed an improved moisture and wind maximization method βased on the wind rose diagram the data were grouped according to the main water vapor inflow direction of the study basin and the typical rainstorms in each wind direction were amplified then the maximum value in all directions was taken as the final result of pmp this method was applied to estimate the pmp of a nuclear power plant in hubei province results suggested that the pmp is 1 14 times higher than that of the rainstorm with a 10000 year return period this ratio is consistent with the cognition that the ratio of pmp to the 10000 year design rainstorm is between 1 1 and 1 2 in most areas of china therefore the improved mwm is considered to be reliable and reasonable and it provides a new idea for pmp calculation zhou et al 2020 used this improved mwm method to estimate the 24 hour pmp in yanshan basin hebei province and further derived short duration pmps with statistical methods τhen they obtained the corresponding probable maximum flood by using the unit hydrograph and the empirical formula instantaneously the ratio of the 24 hour pmp 1026 2 mm calculated by this method to the design value of rainstorm with a 10000 year return period 875 8 mm showed that the improved mwm is reasonable and can provide the scientific basis for flood control engineering construction given that the flow concentration time in some areas may be longer there are also studies focusing on the pmp over a longer duration based on the grid daily precipitation datasets liu et al 2018b applied the rainstorm maximization method to estimate the value and spatiotemporal distribution of pmp in the upper reach of nujiang river basin qinghai tibet plateau they then used the swat model to simulate probable maximum flood pmf and their study presented a procedure for estimating long duration pmp and pmf of sparsely gauged large river basins rainstorm transposition method the rainstorm transposition is by far the most commonly used method for pmp estimation in mountainous areas lin et al 2018 the core of this method is that the measured rainstorm needs to be divided into convergence rain component caused only by atmospheric factors and terrain rain component affected by the terrain as the convergence rain and its spatial distribution is not affected by terrain it can be transported different terrains have different effects on the water vapor circulation and rainstorm distributions therefore the quantitative analysis and calculation of the terrain impact on rainstorm are particularly important for the accuracy of rainstorm separation and the rainstorm transposition method constituting a hot spot for chinese scholars as early as the 1980s some scholars began to study the correction of terrain rain gao and xiong 1983 established the correlation between the vertical change of the terrain ascent speed and atmospheric stability and the length of terrain wave according to the weather characteristics different expressions of terrain ascent speed were selected and applied for the correction of the terrain rain in the huaihe river basin obtaining a credible result lin 1988 proposed the step duration orographic intensification factor sdoif method to estimate terrain rain which uses the two dimensional difference to approximate the impact of the terrain on the rainstorm and the process of spatiotemporal changes the concept of orographic intensification factors is introduced which includes the comprehensive effects of terrain on triggering uplifting and blocking the water vapor flow the pmp application in a river basin of hainan island proves that this method can not only estimate the pmp value in mountainous area but also obtain the spatial and temporal variation of pmp isolines following the cause analysis lan et al 2018 used the rainstorm transposition method based on the orographic intensification factors and the improved statistical estimation method of km value to calculate the 4 h pmp in hong kong they suggested using both of the two methods to calculate the pmp generally the statistical estimation method is used to calculate a prompt result for reference but the rainstorm transposition after moisture maximization can consider various factors and has more reasonable and reliable results liao et al 2020 improved the sdoif method by merging the data of multiple sites with similar characteristics to supplement the limited sample size and used the regional linear moment method to estimate the rainstorm quantiles then the am precipitation was replaced by the quantile value to calculate the orographic intensification factors oif finally the convergence rain separated in taiwan was moved to hong kong to estimate the pmp of 4 and 24 hour the oif obtained through rainfall quantiles could better reflect the enhancement effects in orographic intensification areas also the pmps calculated by this method i e 1341 58 mm calculated with 500 year rainfall quantiles are smaller compared to the values estimated by a revised km value method lan et al 2017 generalized estimation method the generalized estimation is a set of methods used to estimate pmp in small watersheds including rainstorm maximization transposition statistical method etc in china it is generally referred to as the time area depth generalized method it is the regional generalization of the spatiotemporal distribution of pmp estimates in many watersheds of different sizes in a large region the advantage of the generalized estimation method is that it can make full use of various data in a region this method was used to draw the national pmp contour map in the 1970s ye and hu 1979 zhan and zou 1980 in the united states the results of the generalized estimation method have guided the flood control design of large scale water conservancy project constructions however the pmp isoline in china has not played a corresponding role and the design of major water conservancy projects is still based on the estimated value of single station pmp lin et al 2018 in addition to the above four methods there is also a small number of studies using numerical weather prediction models i e numerical simulation methods to estimate pmp wang 1988 used a limited area fine grid numerical precipitation prediction model to evaluate the pmp value of the upper hanjiang river basin the pmp was estimated by the net water vapor transportation method providing a new perspective the numerical simulation method can be used to estimate pmp of areas with sparse gauged sites and it also opens up a new way to study the impact of climate change on pmp however numerical weather prediction models are generally large scaled and there are great uncertainties for pmp estimation at watershed and point scale which still face many challenges in practical applications non stationary analysis of extreme precipitation the frequency analysis of extreme precipitation is generally based on the stationarity assumption which considers that the distribution parameters are constant and so is the value of the design rainstorm in recent years due to the influence of climate change and human activities the hypothesis of stationarity has been questioned milly et al 2008 the hydrometeorologic extreme events in some areas in china show characteristics of non stationarity such as trends abrupt change or the combination of them studies have pointed out that the magnitude of extreme precipitation events in the middle and lower reaches of the yangtze river western china and in parts of the southwest and south china coastal areas shows an upward trend the north shows a downward trend zhai et al 2005 lu et al 2020 the frequency of 1 day extreme precipitation event is increasing in most parts of china while it is decreasing in north and southwest china the frequency of extreme precipitation lasting for more than two days has a significant increasing trend in the middle and lower reaches of the yangtze river jiangnan area and eastern plateau while it has a decreasing trend in north and southwest china wang and qian 2009 due to the extreme precipitation changing trends non stationary hydrometeorologic analysis has attracted much attention in recent years the current non stationary research on precipitation extremes in china can be summarized into two aspects which will be described in this section the study of driving factors i e climate factors and the frequency analysis under non stationary conditions the summary of the studies of non stationary analysis of extreme precipitation is shown in table 2 and the climate covariates which affect extreme precipitation in china are shown in fig 2 driving factors exploring the correlation between extreme precipitation and climate covariate factors is expected to provide a root cause explanation for the trend and non stationarity of extreme precipitation the first aspect in the study of non stationary extreme precipitation is the correlation analysis between precipitation extremes and the climate covariates that induce the non stationarity the climate in eastern china is dominated by the east asian monsoon and in recent years studies have revealed that the east asian monsoon is significantly affected by el niño and southern oscillation enso some studies have pointed out that the east asian monsoon is also affected by the north atlantic oscillation nao the indian ocean dipole iod and the pacific decadal oscillation pdo xiao et al 2017 therefore enso monsoon index iod pdo arctic oscillation ao and north pacific oscillation npo are mainly selected as the climate covariate factors in extreme precipitation studies in china gao et al 2017 xiao et al 2017 xu et al 2016 research on the climate covariate factors that lead to the trend non stationarity of extreme precipitation is summarized in table 2 among these numerous climate covariate factors enso is the most widely studied xiao et al 2017 duan et al 2017 lv et al 2018 wang et al 2021 zhang et al 2014a li and zhai 2009 xiao et al 2017 showed that in central china the extreme precipitation intensity tends to decrease when enso is in the positive phase while in the year after its positive phase the intensity in eastern china increases duan et al 2017 pointed out that enso has a significant impact on the extreme precipitation process over the entire pearl river basin in the warm enso episodes the magnitude and frequency of extreme precipitation events in the west of the basin will increase while in the eastern part the magnitude will decrease enso may exert different effects on extreme precipitation in different regions of china lv et al 2018 explored the potential relationship between annual and seasonal maximum daily precipitation and local temperature ltem global surface temperature gtem and enso in the yangtze river basin they found that ltem and enso had no significant effect on the annual maximum daily precipitation however there is a negative correlation between the winter maximum daily precipitation and enso in addition to enso other climatic factors also show significant effects on extreme precipitation in china su et al 2017 deng et al 2018 liu et al 2017b zhao et al 2014 gu et al 2017b zhang et al 2015b zhao et al 2014 explored the relationship between extreme precipitation and climate factors by continuous wavelet analysis at 42 gagued stations in the pearl river basin and their results suggested that pdo and soi were important factors affecting extreme precipitation gu et al 2017b conducted a study on non stationarity and probable causes of the heavy precipitation occurrence rate by a series of methods including pot sampling technique kernel density estimation method diggle 1985 cox smith and karr 1986 poisson regression model villarini et al 2011 and generalized additive models for location scale and shape gamlss rigby and stasinopoulos 2005 their results manifested that extreme precipitation exhibited non stationary at seasonal scale and the seasonal variability was significantly related to the climate factors as the sampling threshold increases the impact of climate factors on the occurrence rate of extreme precipitation gradually weakens in addition higher soi and pdo may increase the frequency of extreme precipitation in northeast and eastern china zhang et al 2015b analyzed the correlation between winter extreme precipitation and climate factors in southeast china and showed that the probability of extreme precipitation events will be higher lower in the positive negative phase of iod frequency analysis under non stationarity the second aspect of the non stationary extreme precipitation analysis is to introduce time or climate factors as covariates into the probability distribution to analyze the frequency and magnitude of precipitation extremes the rise and development of this part are relatively short the core content is to 1 determine the best covariate factors that can explain the statistical characteristics of precipitation extremes 2 construct the optimal non stationary probability distribution model 3 carry out the rainstorm design and risk analysis under non stationary conditions and 4 compare the calculation results with those of the stationary model at present there are mainly three models for constructing the non stationary analysis one is the gamlss and the second is the g e v distribution having location and scale parameters variant with time or climate factors the above two models focus on am series the third model is a non stationary one based on pot series which has not been widely used in the study of non stationary extreme precipitation analysis in china for instance wu et al 2015 constructed a time varying pot model using the g p distribution to explore the spatiotemporal variation characteristics and the causes of the non stationarity of extreme precipitation in the pearl river basin they used random forests genuer et al 2010 to measure the importance of the climate factors affecting precipitation extremes and demonstrated that the el nino index smei has the most significant impact on extreme precipitation and the time varying pot model with the g p can deal well with the non stationary characteristics in some regions of the pearl river basin in addition some studies introduced time and other covariates into the lognormal l n and p e 3 distributions to construct non stationary models chen et al 2017 zeng et al 2017 some studies considered the whole study area under a non stationary framework constructing non stationary models for all stations su and chen 2019 set up link function combinations each function contains only one covariate factor with different climate factor covariates for g e v location and scale parameters they then constructed non stationary models of the dry and rainy season in the pearl river basin based on am series to explore the best climate factor covariates the results showed that the spatial distribution of the best covariates varies greatly and enso is the best covariate for most stations in the study area but not all the best covariates are significant in addition the design rainstorm calculated by the non stationary model presents great variability in space and the difference between the maximum and minimum design values of the same station is significant indicating that covariates introduce great uncertainty zhang et al 2015c constructed two non stationary models based on gamlss with time and climate factors as covariates for the am series in the beijing tianjin hebei region they demonstrated that the goodness of fit of non stationary model is often better than the stationary one the non stationary model with climate factors as covariates has better goodness of fit than that with time as covariate which can capture the variation characteristics of extreme precipitation moreover they pointed out that the east asian monsoon index is the most significant covariate in the study area instead of directly using climate factors as covariates to construct non stationary models zeng et al 2017 introduced principal component factors of climate variables and time as covariates into l n and g e v distributions and discussed the performance of the stationary and two non stationary models considering the maximum 5 day precipitation and the number of very heavy precipitation days in the hanjiang river basin hao et al 2019 constructed one stationary and two non stationary models based on gamlss then they identified the homogeneous subregions for precipitation extremes based on cluster analysis according to climate covariate predictors and found that the behaviour of extreme precipitation is significantly affected by climate factors specifically the non stationary model with climate covariates describes the change of extreme precipitation better than the model with time as a covariate there are regional patterns for the dominating climate factors and hanjiang river basin can be divided into three homogeneous regions according to climatic indices chen et al 2017 introduced several trend models of the mean and variance of am series into the extreme value type i distribution l n and p e 3 distributions to construct a non stationary model with time as covariate for 9 rainfall stations in taiwan china they analyzed the trend of two return periods defined under non stationary conditions and showed that the linear model can capture the time trends of the mean and variance well but not all stations are under a state of non stationarity with the consideration of non stationarity the return periods of rainstorms calculated by the expected recurrence time compared to those values estimated by the reciprocal of the exceedance probability of occurrence is small before constructing the non stationary model some studies test the non stationarity of the extreme precipitation series first gao et al 2016 tested the am series of 631 meteorological stations in china by the kwiatkowski phillips schmidt shin kpss and mann kendall m k tests and found that only the extreme series of 48 stations showed non stationarity further the g e v non stationary model using time as covariate was constructed for the non stationary stations with an obvious trend of extreme series while the g e v non stationary model using climate factors as covariates was constructed for the non stationary stations without an obvious trend of extreme series the results indicated that the non stationary model can better capture the interannual variation of precipitation extremes compared to the stationary one song et al 2020 used five methods to test data trends abrupt change points and non stationarity of am series in the beijing tianjing hebei region they constructed a non stationary g e v model for non stationary stations they set the three link functions and 84 function combinations between covariate factors and distribution parameters explored the optimal non stationary model and calculated return periods of rainstorms under non stationary conditions they demonstrated that the non stationarity of extreme precipitation is not significant and enso is determined as the optimal covariate in the beijing tianjing hebei region compared to the stationary model the non stationary model can better capture the interannual variations of precipitation extremes however there is no significant difference in the design rainstorm between the stationary model and the median value of the non stationary model based on the am series of 14 stations in poyang lake basin yin et al 2016 constructed a non stationary g e v model with time as the covariate of location parameter they calculated the design rainstorms of non stationary stations for different return periods and found that only one station in 14 showed a significant upward trend and the non stationary model was better than the stationary model in terms of the likelihood ratio test the design rainstorm increases under non stationary conditions and the design rainstorm with 100 year return period in 1951 is comparable to the design rainstorm with 50 year return period in 2010 a relatively consistent conclusion of these studies can be reached it is found that only a small number of extreme precipitation series show non stationarity through the trend and non stationary test of extreme series the non stationary assumption is valid only in some regions and the extreme precipitation in other regions still satisfies the stationary assumption in addition a small number of studies have focused on the non stationary of the dependence between different extreme precipitation variables li et al 2019b used the time dependent archimedes copula function and the time varying g e v to fit the joint and marginal distribution of extreme precipitation amount 90th percentile threshold of daily precipitation and intensity annual maximum daily precipitation in the eastern coastal area of china respectively they studied the evolution law of the correlation of multivariate variables and found that the spatial distribution of extreme precipitation is very uneven in eastern coastal china and the extreme precipitation occurrence risk of shandong province was found to be the largest corresponding to long joint return period intensity duration frequency curves under the combined impact of global climate change and urbanization urban floods are becoming more and more prominent causing severe challenges to urban drainage and waterlogging prevention zhang et al 2014b more than 60 of the cities in china were affected by urban flood to varying degrees of which nearly 140 experienced floods more than three times from 2008 to 2010 xiong et al 2017 in order to alleviate the problem of urban flooding the construction of urban drainage networks and flood control infrastructures needs to be further strengthened the current drainage system in china is designed to be safe under a certain design rainstorm obtaining the reasonable design rainstorms under various durations is the premise for reasonably determining the construction scale of drainage and waterlogging infrastructure therefore it is of great important to scientifically compile the urban rainstorm intensity duration frequency idf curve or further summarize it into the form of rainstorm intensity formula xiong et al 2017 water conservancy departments and municipal departments in china focus on different aspects of urban flooding which is pipe network drainage and regional waterlogging removal respectively jia and li 2015 municipal departments usually apply the short duration rainstorm intensity formula compiled by the am series within 3 hour while water conservancy departments generally conduct frequency analysis based on the 24 hour maximum precipitation to determine the design rainstorm jia et al 2021 in china the research on idf generally aims at the formulation of the rainstorm intensity formula which describes precipitation intensities for different durations and return periods in 2011 the ministry of housing and urban rural development of the people s republic of china revised the urban rainstorm intensity formula of 606 cities in china based on the am method and g distribution comprehensively and issued a new edition of code for design of outdoor wastewater engineering shao and shao 2013 subsequently the 2014 year version of the aforesaid code was revised by focusing more on actual urban waterlogging problems finally a complete set of theories and methods has been formed for the calculation and application of urban design rainstorm in china the procedure of compiling the rainstorm intensity formula in china can be simply summarized as two step optimal method including frequency distribution curve fitting and parameter estimation of rainstorm intensity formula liu et al 2018c the frequency distribution curve fitting is a complete set of extreme precipitation frequency analysis process including sample selection optimal probability distribution selection and parameter estimation in the early stage of compiling rainstorm formula annual multisampling method is the most commonly used due to the short data length of precipitation series deng 2006 nowdays standard guidelines in china stipulate that the am series should be used to fit a frequency distribution curve in areas when there is more than 20 years of automatic recorded rainfall records shanghai municipal engineering design instituter group co lid 2006 in china p e 3 distribution has been adopted for urban rainstorm frequency analysis for a long time in recent years there are also studies using g distribution and the exponential distribution huang et al 2013 shao and liu 2018 analyzed 24 933 rainfall samples under various specified precipitation durations 5 120min of 607 cities and verified that a combined p e 3 and g distribution using the am sampling methods is a superior theoretical distribution type for short duration rainstorm intensity formulas after obtaining the frequency distribution curve the second step is the estimation of the parameters of the rainstorm intensity formula research on the rainstorm intensity formula in china dates back to the 1970s deng 1979 the mathematical expression of the comprehensive formula recommended in the code shanghai municipal engineering design instituter group co lid 2006 is i a l 1 c lg t t b n where i denotes the rainstorm intensity and al c b and rainstorm attenuation index n are four important parameters that are derived and modified by the gauss newton iterative algorithm liu et al 2018c pointed out that the expression a l 1 c lg t is an empirically determined function depending on the return period of the rainstorm as a shortcoming furthermore only a single station is employed for urban rainstorm frequency analysis in china without considering the heterogeneity of extreme precipitation distribution in space liu et al 2018c at present there are numerous applications of the short duration rainstorm intensity formula in china liu et al 2019 yuan et al 2020 zhuang et al 2015 chen 2013 such applications were generally based on the above two step optimal method to determine the rain intensity formula of a city local area or update the parameters of the existing formula to analyze the variation characteristics of the design rainstorm regarding to the data usage for deriving idf curves conventional raingauges exist long before automatic raingauges were deployed jiang and tung 2013 however daily precipitation data with long records at conventional raingauges are of limited use to derive rainfall idf relationships to fully utilize available long period daily precipitation records jiang and tung 2013 presented a methodological framework based on the scale invariant properties of the rainfall scaling model to derive idf relationships with short duration they confirmed that the g e v based scale invariant model is applicable in hong kong china the above applications and research mostly focus on short duration rainstorms but there are relatively a few studies on long duration intensity formulas the difference of rainstorm attenuation characteristics in different durations is also not considered in the formulation of rainstorm formula in china jia et al 2021 focused on analyzing the rainstorm attenuation law pointing out that the rainstorm intensity of 1 24h duration attenuates with the same attenuation index of 0 74 based on this a long term comprehensive rainstorm intensity formula can be obtained which can calculate the design rainstorm with arbitrary duration of 1 24h and any return period of 2 100 years uncertainty in data sources model structures and parameters uncertainty prevails in the extreme precipitation frequency analysis xu et al 2010 and we can distinguish roughly three uncertainty sources data acquisition model selection and parameter estimation the sources of uncertainty in data acquisition mainly include the process of gauge measurement and observed data sampling analyzing and evaluating these uncertainties is of great significance for reducing the negative impact on extreme precipitation frequency analysis the deviation of gauge measurements caused by external factors is inevitable and ubiquitous gauge measurements of precipitation are always subjected to negative biases due to the wind induced undercatch trace precipitation and wetting losses folland 1988 the biases can result in a loss of 90 of the precipitation in some extreme cases which undoubtedly exert negative impacts on the frequency analysis of extreme precipitation zhang et al 2020 ignoring the precipitation measurement error caused by external factors may lead to changes in the precipitation amount or trend law li et al 2018b ren et al 2003 ye et al 2004 and consequent affect the accuracy of frequency analysis in order to ascertain and quantify various error sources including the random error the wind induced error the evaporation error and the wetted error of precipitation measurement ren et al 2003 set up 30 rainfall gauged stations across the mainland china at different climatic zones and elevations and conducted comparative experiments lasting for 7 years for 29000 precipitation events their results showed that the range of rainfall measurement errors in china is in 4 34 15 28 with an average of 6 52 including the wind induced error of 3 17 and the wetting error of 3 35 ye et al 2004 also reported that wind induced gauge undercatch is the greatest error in most regions in china and wetting loss and trace amount are important in the low precipitation regions in northwest china their findings clearly suggested that annual precipitation in china is much higher than previously reported zhang et al 2020 confirmed that the trends of measured annual precipitation have been overestimated to varying degrees before removal of measurement biases in most regions with the exceptions of northwest china and northern tibet as precipitation measurement errors could be very big the impact of the bias correction of precipitation on the accuracy of extreme precipitation frequency analysis needs to be further studied in addition to the measurement error another source of uncertainty in data is the selection of sampling methods to obtain the extreme precipitation series although the am sampling method is widely used in china the series obtained by am is generally short in length and the sampling error is exacerbated due to the lack of sufficient representativeness resulting in great uncertainty pot sampling can expand the usage of rainstorm information however there is still subjectivity in the selection of threshold liu et al 2013 chow 1964 pointed that the relative difference between the design rainstorm calculated by different sampling methods can reach 10 when the return period is less than 5 years the return period of the design storm for urban drainage works is usually less than 2 years mei et al 2017 therefore the uncertainty caused by different sampling methods can exert a great impact on the design of municipal drainage engineering regarding reducing uncertainties regional frequency analysis is an effective way to reduce the sampling error in extreme precipitation frequency analysis and improve the estimation accuracy of the design rainstorm the most significant advantage of regional frequency analysis is that the information of adjacent stations can be effectively used to overcome the shortage of data series for a single station and could also solve the estimation problem of design rainstorm in areas without data the theory and method of regional frequency analysis are mature and widely used in practical engineering abroad robson and reed 1999 however in china single station frequency analysis is still commonly used to obtain the design storm ministry of water resources of the people s republic of china 2006 and the regional frequency analysis is mostly adopted in research li et al 2018c liang et al 2017b shao et al 2020 zhou et al 2017 du et al 2014 used monte carlo simulations to evaluate the uncertainty caused by at site and regional frequency analysis they found that the regional method has less uncertainty yang et al 2010 used cluster analysis to divide the pearl river basin into 6 homogenous regions and explored the temporal and spatial distribution of their design rainstorms they noticed that the design rainstorms under different return periods showed a gradual increase from the upper to the lower reaches at present the regional frequency analysis mostly focuses on the division of the homogenous regions and there are few studies on the improvement of the division method hu et al 2019 introduced the fuzzy c means method combined with the extended xie benn index for the division of homogenous regions they showed that the modified method can identify the optimal number of homogenous regions and divide them reasonably the probability distribution function is actually a model for the expression of the statistical law of data distribution which not only interpolates the designed values in the range of existing data series but also performs the epitaxial estimation for the rare rainstorms jin 1999 at present a suitable distribution is chosen mainly by comparing the goodness of fit between potential distributions even if there are only tiny differences between the candidates the probability with a better goodness of fit index is commonly considered to be the right choice li et al 2019a guo et al 2019 estimating parameters of distributions with more than 3 parameters especially when using the method of moments is uncertain since it involves estimation of higher order moments gu et al 2021 this is one of the reasons that the guidelines in china recommend the three parameter p e 3 as a theoretical distribution in engineering practice guo et al 2016 since the 1950s the weibull formula has been used to calculate empirical frequency of hydrological variable series in china hua 1987 which proved to be safe for engineering design and has been used until now however different theoretical distributions are suitable for different empirical frequency formulas hua 1984 and the uncertainty of choosing the empirical frequency formula should not be ignored in addition for non stationary models due to the complexity of the model structure and the natural variability of the driving factors it usually leads to great uncertainty which limits the application of non stationary models as for parameter estimation the advantages and disadvantages of the commonly used estimation methods in china as well as the potential uncertainties have been described in the previous section and will not be repeated here it should be noted that the empirical curve fitting method has been widely applied to determine the parameters of theoretical distributions for precipitation extremes in engineering applications in china till now jin 1990 zhu et al 2011 however the empirical curve fitting method assumes that the ratio between the skewness coefficient and the variation coefficient is almost constant guo et al 2016 although the empirical curve fitting method can flexibly integrate a lot of information and take the historical heavy rain data into account the design rainstorm obtained by this method is generally larger and the arbitrariness and uncertainty are inevitable guo et al 2016 spatial variability of extreme precipitation in china due to the large geographic and climatic variability in china the characteristics of extreme precipitation differ across regions guan et al 2017 li and hu 2019 studies on the spatial variability of extreme precipitation generally adopt am or pot methods to obtain extreme precipitation series from gridded data gauged or climate models the studies focus on the distribution or the change in intensity and frequency of extreme precipitation deng et al 2018 used pot 95th percentile to define extremes and found that the spatial variation of extreme precipitation intensity and frequency has obvious regularity and spatial differences showing a decreasing trend along the strip from northeast to southwest and an increasing trend on both sides of the strip li et al 2019c studied 539 stations in china to find that there has been an increasing trend in annual and daily extreme precipitation during 1960 2010 especially in the south east and north west part of the country they also noted that the trends were higher in stations located in urban areas other studies also support these findings li et al 2019d zhai et al 2005 and additionally observe exceptions in some southeast and southwest regions which have presented decreasing trends in extremes fu et al 2013 examined the spatial and temporal variability of the frequency of extreme precipitation events for 599 stations in china and revealed that extreme precipitation in northeast china north china and the yellow river basin tended to decrease while the extreme precipitation in the yangtze river basin southeast coast south china inner mongolia northwest china and qinghai tibet plateau tended to increase wu et al 2018 revealed that the frequency of extreme precipitation events has increased significantly and the occurrence time has become more concentrated in western china by using the g p distribution they found that the 20 year and 50 year return periods of extreme precipitation events in the southwest and southeast of the lancang river have significantly increased from the perspective of dry and wet regions han et al 2019 used the gridded daily precipitation data set constructed based on 2 472 stations across china and defined extreme precipitation with 95th percentile based threshold the increasing trends in extreme precipitation frequency and relative intensity are detected in most parts of the dry regions especially in northwestern china and inner mongolia in wet regions increasing trend prevails in northeastern china and southern china they also speculated that the risk of flooding may raise in both dry and wet regions particularly in wet regions although the data and extreme precipitation indices adopted by these studies are different the conclusions on the spatial variability of the extreme precipitation are relatively consistent the increasing trends of extreme precipitation frequency and intensity were detected in south china northwest china inner mongolia etc human induced greenhouse gas emissions in china also had a detectable effect on the shift from light to heavy precipitation ma et al 2017 the spatial variation characteristics of extreme precipitation in future have also attracted the attention of numerous scholars and global climate models are effective tools to study the future change of extreme precipitation li et al 2012 bao et al 2015 wen et al 2016 xu et al 2019 guo et al 2013 xu et al 2019 predicted the changes in extreme precipitation in nine major river basins in china in the future by applying the outputs of 18 global climate models gcms with 3 representative concentration pathways rcps from the cmip5 the results manifested that the predicted changes have clear spatial differences the magnitude and intensity of extreme precipitation in high latitudes and high altitudes regions have a clear increasing trend and there is a great increasing trend in the frequency of extreme precipitation in southeast and southern china wen et al 2016 investigated the possible changes throughout china in the future through bias correction and spatial disaggregation of downscaled cmip5 projections they found that the growth of extreme precipitation intensity is unequivocal with an increasing rate from 7 9 to 15 5 in cmip5 projections in general the most evident increase was detected with a rate over 35 as mentioned in section 8 each link of extreme precipitation frequency analysis is inevitably fraught with uncertainties however obvious uncertainties exist in future projections of extreme climate indices due to model structure emission scenario and natural variability li et al 2012 woldemeskel et al 2016 the gcms uncertainty is larger in regions receiving heavy rainfall as well as mountainous and coastal areas woldemeskel et al 2016 if gcms are applied to study extreme precipitation inappropriately the uncertainty may be magnified to a great extent and added to the uncertainties included in the process of frequency analysis applicability assessment and uncertainty quantification will be the major research directions for the introduction of climate models into the extreme precipitation frequency analysis synopsis and synthesis the review focused on the probabilistic methods used to assess extreme precipitation in china the main findings of this review as well as the future prospects and directions are the following research involving frequency analysis of extreme precipitation is almost exclusively based on the am or pot sampling methods although there are a lot of studies on the threshold selection of the pot method in china there is no unified threshold selection method and subjectivity exists the commonly used distributions for the representation of extreme precipitation in china are mainly the three parameter ones such as the g e v g p and p e 3 the g e v distribution is more suitable for fitting am series while the optimal probability distribution of the pot series is g p which is consistent with the distribution line recommended by wmo p e 3 is recommended by the standard guidelines in china for frequency analysis and is still widely used in practical engineering applications especially for short duration rainstorms 1 2 h for longer durations the p e 3 may not be ideal and the g e v may be a better fit e g for annual summer and winter daily maximum precipitation at present there is no unified conclusion about which sampling method or distribution should be used for rainstorm design although g p was shown to be a better fit than g e v in most comparison studies e g beijing chongqing huai river basin l moments are widely used for the parameter estimation of extreme precipitation in the research the conventional moments and maximum likelihood estimation have shortcomings which limit their application in research and engineering practice the pickands bootstrap moment estimation method seems promising zhao and zhai 2015 the gauss newton method is recommended by guidelines for the numerical solution of the p e 3 distribution parameters despite its shortcomings sensitivity to the initial value slow convergence speed recently artificial intelligence algorithms have also been proposed for the parameter estimation of the p e 3 the probable maximum precipitation pmp adapted for china is the verification standard for flood control design of water conservancy and hydropower projects in the country the methods used for the estimation of pmp in china mainly include the statistical estimation method rainstorm maximization method rainstorm transposition method and generalized estimation method the generalized estimation method which is a combination of various methods is mostly used for the calculation of the pmp in large areas and the compilation of national pmp contours the hydrometeorological method represented by the rainstorm maximization method and rainstorm transposition method is recommended for pmp calculation in recent years this method has a clear physical mechanism and the calculated pmp is more in line with reality the rainstorm transposition method combined with water vapor maximization and convergence rain azimuth adjustment is still a feasible way to estimate pmp for the present state and the foreseeable future when using the statistical estimation method hershfield s km value method in china the deviation coefficient is generally used instead of the frequency factor km in the original formula statistical estimation can be used as an auxiliary method to quickly and preliminarily estimate the pmp of a single station or small watershed this method has been used in the compilation of pmp maps in hubei shaanxi and other provinces in the guideline documents of china pmp is linked to the rainstorm of 10000 year return period in frequency calculations but the exact return period of pmp corresponding to the design rainstorm is still unclear some scholars believe that the ratio of pmp to 10000 year design rainstorm is between 1 1 and 1 2 in most areas of china other methods for estimating pmp such as storm model the numerical simulation method multifractal method etc are rarely used in the research and application in china these methods may provide feasible ideas for pmp estimation and pmp response to climate change in the future the calculation results of pmp in china are generally carried out for a specific project and most of them were completed in the 1970s so there may be great uncertainties and subjective experience in the calculation process it is necessary to learn from the practices of some foreign countries carry out national watershed pmp estimation including pmf and compile the distribution map of national pmp estimation results non stationary methods are becoming more prevalent due to the effects of climate change or human activities compared to foreign countries the non stationary frequency analysis in china started relatively late and became the focus of extreme precipitation studies only during recent years the early research related to non stationary in china mainly focused on the analysis of temporal and spatial characteristics of extreme precipitation trend changes many studies showed that across china extreme precipitation shows a changing trend and presents regional patterns some studies pointed out that only a few regions in china exhibit a non stationary state of extreme precipitation the climate in most regions of china is significantly affected by monsoons so enso is the most widely studied climate factor being the most significant covariate affecting extreme precipitation in most regions of china for local scale studies significant covariate factors can be easily identified but on a national scale climate factors will influence and regulate each other therefore some studies are not limited to individual climate factors but carry out principal component analysis of climate factors most studies choose indicators based on statistical concepts such as the am and pot series as extreme precipitation indicators related to climate factors the am series is used to construct non stationary probability distribution models in most studies while a few take the occurrence of extreme precipitation as the research object in china the construction of the non stationary models is mainly based on gamlss and g e v distributions and the location and scale parameters change with covariates most of the covariate factors are time and climate factors and urbanization human activity is also used as a covariate in a very small number of studies generally the aic bayesian information criterion bic etc are selected to evaluate the non stationary and stationary model the aic is most commonly used for the optimization of the covariate factors and non stationary models in china almost all evaluation indices agree that the non stationary model with time as covariate better describes extreme precipitation in china compared to the stationary one and the non stationary model with climate factors performs even better moreover the non stationary model with climate factors as covariates can better describe the fluctuation of precipitation extremes although the non stationary model can capture the variation characteristics of precipitation extremes well it also introduces a lot of uncertainty the design rainstorm of the non stationary model is an interval value not a fixed one with the increase of the return period the range of the design value increases and so does the uncertainty some studies have pointed out that under non stationary conditions the return period of the design rainstorm of the same magnitude presents a decreasing trend indicating that the risk of rainstorm occurrence is increasing at present in the guidelines on rainstorm design in china ministry of water resources of the people s republic of china 2006 the stationary model is still recommended in future project construction for the sake of safety it is necessary to consider the influence of non stationarity when applying copula function to modeling the marginal distribution of a single extreme precipitation variable and the dependent structure between precipitation extremes may change due to climate change and human activities therefore it is necessary to carry out research on the modeling method and application of copula function under non stationary condition time varying copula model in china future research on non stationary extreme precipitation should be further strengthened in the following aspects 1 a comparative study on the non stationary variation law of precipitation extremes in similar climatic regions to enhance the credibility of the results a longer series of hydrological data and appropriate regional analysis methods should be used to study the impact of climate change on hydrological extremes 2 research on covariate selection of the non stationary model whether the best climate covariates have regional characteristics needs to be further analyzed in addition to climate factors the way of quantifying the impact of human activities on extreme precipitation needs to be further explored 3 research on the uncertainty of non stationary model results the means for uncertainty evaluation of the return period and the risk of extreme variables under non stationary conditions needs to be studied in practical applications which model stationary or non stationary should be used as the final basis for rainstorm design is worth further discussion 4 research on parameter estimation methods of the non stationary model the existing parameter estimation methods such as weight function method and l moments should be improved to make them suitable for parameter estimation under non stationary conditions and minimize the uncertainty in china the research and application of idf mainly focuses on the formulation of the rainstorm intensity formula to serve urban flood control and drainage and establish the rainstorm intensity formula for more than 600 cities the formula of rainstorm intensity is compiled by using the two step optimal method and the comprehensive formula the two step optimal method contains a complete set of extreme precipitation frequency analysis but it focuses on short durations generally less than 3h compared with the frequency analysis at basin regional and national scales the current design standard of drainage system in china is based on the condition of stationarity however under the dual impact of climate change and urbanization the mechanism and main characteristics of urban rainstorm have changed significantly and the risk of urban flood is incline to increase it is necessary to strengthen the research on the characteristics of short duration rainstorm in urban areas under a changing environment including occurrence and evolution mechanism temporal and spatial structure and change characteristics of rainstorm and compile time varying rainstorm intensity formulas or time varying idf curves uncertainty pervades every process of extreme precipitation frequency analysis due to industry habits there is a relatively fixed choice of the empirical frequency formula weibull formula in china in terms of parameter estimation of the theoretical frequency curve in order to take into account the historical heavy rain the empirical curve fitting method assumes there is a multiple relationship between the skewness coefficient and the variation coefficient which will also cause great uncertainty in terms of reducing uncertainty in extreme precipitation frequency analysis there are few achievements available for practical application of regional analysis methods and at present they are only in the research stage in china the uncertainty research on multivariable frequency analysis is still in its infancy so it is necessary to supplement and improve the uncertainty analysis theoretical system of extreme precipitation extreme hydrological events there are greater uncertainties for multivariable than univariate models based on the copula function multivariable models not only include the uncertainty of the marginal distribution but also the uncertainty of the dependence structure these two types of uncertainties involve data model structure parameter estimation etc so it is necessary to establish a comprehensive evaluation method that considers the three types of uncertainties as well as a multivariate model uncertainty analysis framework studies on spatial variability in china mainly focus on exploring the changes and spatial distribution of frequency and intensity of extreme precipitation based on gridded data gauged data or climate models at the national scale even though different data sources and definition methods of extreme precipitation are adopted different studies show relatively consistent laws increasing trends in extreme precipitation frequency and relative intensity were detected in south china northwest china inner mongolia etc in addition to using historical data a number of studies introduced climate models to explore the future changes of extreme precipitation however the introduction of climate models will undoubtedly increase the uncertainty applicability assessment and uncertainty quantification will be the major research content on the introduction of climate models into the extreme precipitation frequency analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national natural science foundation of china nos 51925902 52179006 the fundamental research funds for the central universities no dut20rc 3 019 the fund of innovation research team from the department of science and technology in liaoning province no xlyc1908023 and the fund of innovation research team in dalian university of technology no dut19td31 
164,recent years have witnessed global massive property losses and casualties caused by extreme precipitation and its subsequent natural disasters including floods and landslides china is one of the countries deeply affected by these casualties if the statistical characteristics and laws of extreme precipitation could be clearly grasped then the negative impacts triggered by it may be minimized china is a vast country and diverse in climate and terrain hence different regions may be suitable for different analyses and research methods therefore it is necessary to clarify the research progress methods and current status of extreme precipitation across the country this paper attempts to provide a comprehensive review of techniques and methods used in extreme precipitation research and engineering practice and their applications the literature is reviewed focusing on seven aspects 1 annual maxima method am 2 peaks over threshold method pot 3 probable maximum precipitation pmp 4 non stationary analysis of precipitation extremes 5 intensity duration frequency curves idf 6 uncertainty in extreme precipitation frequency analysis and 7 spatial variability of extreme precipitation research on extreme precipitation in china is generally based or centered on the above seven aspects the current study aims to provide ideas for further research on extreme precipitation frequency analysis and its response to climate change and human activities graphical abstract image graphical abstract keywords extreme precipitation china annual maxima peaks over threshold pmp non stationary idf uncertainty spatial variability introduction extreme precipitation drives adverse phenomena with implications on both human and natural systems soil erosion with related loss of organic matter and nutrients dondini et al 2018 damage of agricultural production fatal landslides and damage of infrastructure are only some of the repercussions of extreme precipitation the most important of which is heavy flooding doocy et al 2013 reported over half a million fatalities and about 350 thousand injuries due to rain induced floods during 1980 2009 with the number of people affected by floods approaching 3 billion in china alone the documented flash flood events from 2000 to 2015 caused about 900 deaths per year on average he et al 2018 such catastrophic impacts can be alleviated through more efficient heavy precipitation modeling and forecasting smith et al 2016 precipitation is usually considered a random variable that follows a probability distribution the estimation of return periods corresponding to any rainfall amount is then possible the extreme events occupy the distribution tail which is the upper part of the distribution distributions can be categorized into heavy and light tailed in regards to their tail behavior heavy tailed distributions are characterized by more intense and frequent extreme events e g papalexiou et al 2013 the definition and study of extreme rainfall constitute challenging tasks and the investigation of the rainfall tail is a dynamic constantly developing process for an extensive review on probabilistic methods used for hydroclimatic extremes see nerantzaki and papalexiou 2022 studies performed on a global scale provide evidence that daily precipitation extremes are more adequately described by heavy rather than light tailed distributions which were very popular until recently nerantzaki and papalexiou 2019 papalexiou et al 2013 papalexiou and koutsoyiannis 2016 2013 2012 using global datasets and a variety of approaches researchers almost unanimously agree with the heavy tail conclusion cavanaugh et al 2015 nerantzaki and papalexiou 2019 papalexiou et al 2013 papalexiou and koutsoyiannis 2016 2013 serinaldi and kilsby 2014 wilson and toumi 2005 the vast country of china is included in most of the large scale assessments of extreme precipitation cavanaugh et al 2015 mention that the east asia coasts including those of china display very heavy tails due to the influence of cyclone landfalls nerantzaki and papalexiou 2019 also noticed heavier tails along the east coasts and lighter tails in the central part of the country papalexiou et al 2013 and papalexiou and koutsoyiannis 2013 had drawn similar conclusions the assessment of the shape parameter of the stretched exponential distribution performed by wilson and toumi 2005 also provided the same spatial distribution of tail heaviness in china due to the large geographic and climatic variability of this immense country the extreme precipitation distribution differs among regions guan et al 2017 li and hu 2019 τhis diversity requires the use of different approaches to estimate extreme precipitation across the country using reliable and reasonable methods to understand the statistical characteristics of extreme precipitation and calculate the design rainstorm is of great significance to the national economy and people s livelihood in recent years a significant amount of research has been carried out on sample selection probability distribution model selection parameter estimation and non stationary frequency analysis of precipitation extremes based on the relevant research results in the country this paper reviews the literature mainly focusing on seven aspects 1 annual maxima method am 2 peaks over threshold method pot 3 probable maximum precipitation pmp 4 non stationary analysis of precipitation extremes 5 intensity duration frequency curves idf 6 uncertainty in extreme precipitation frequency analysis and 7 spatial variability of extreme precipitation the study clarifies the research progress and context of precipitation extremes in china aiming to provide ideas for further research on the frequency analysis of precipitation extremes in the current changing environment annual maxima the annual maxima am sampling constructs series composed of the annual maxima precipitation using daily or hourly values the frequency analysis based on am in china mainly includes distribution fitting with the optimal probability distribution function pdf parameter estimation of the pdf extreme rainstorm design and rainstorm risk analysis including spatial distribution characteristics of shape parameters at present chinese scholars have carried out many studies on the above aspects and obtained some unique regional characteristics and laws for the country of china fitting an appropriate probability distribution model is the premise of extreme precipitation frequency analysis the design rainstorm is one of the most important parameters for the design of river basin and urban flood control facilities therefore the core content of the frequency analysis is to determine the optimal probability distribution and select a robust parameter estimation method for rainstorm design annual maxima distributions the pearson type iii distribution p e 3 is recommended by china s standard guidelines under the am framework for rainstorm and flood design ministry of water resources of the people s republic of china 2006 since the 1960s p e 3 has been generally adopted in extreme flood and rainstorm design of water conservancy and hydropower engineering in china however the world meteorological organization recommends the use of the generalized extreme value g e v distribution as the theoretical distribution for am series world meteorological organization 2009 for short duration am series of extreme precipitation such as annual maximum precipitation of 1 h or 2 h previous studies have shown that p e 3 can be used as a theoretical frequency distribution niu et al 2019 niu et al 2019 used am on 90 sets of short duration extreme precipitation series ranging from 10 to 120 minutes at 18 stations in the shaanxi province and evaluated the applicability of six distributions including p e 3 and g e v they found that p e 3 was suitable for 90 of the data series and therefore can be used as the theoretical distribution function of short duration rainstorms in the study area for relatively longer durations such as 1 day and 5 day studies have pointed out that the performance of p e 3 is not ideal in some areas du et al 2014 zhang et al 2015a du et al 2020 zhang et al 2015a studied the annual summer and winter maximum daily precipitation series of 10 representative stations in 10 major river basins in china using the kolmogorov smirnov k s and anderson darling a d tests as selection criteria for distributions they found that the degree of dispersion and skewness of the am series leads to different curves for the same station in these three periods in general g e v usually passed the two hypothesis tests at the same time and was the optimal distribution for most am series followed by the three parameter lognormal distribution l n 3 the p e 3 distribution passed the hypothesis test fewer times song et al 2007 provided the first application of the g e v distribution on annual extreme precipitation events in china some studies are not limited to distribution fitting but also focus on the tail characteristics i e tail thickness of the g e v gu et al 2017a gu et al 2017a evaluated 728 meteorological stations in china using am and analyzed the statistical characteristics of the g e v applied on annual and seasonal extreme precipitation they pointed out that the g e v distribution tail of annual and seasonal extreme precipitation was heavy for most regions of china having no upper boundary in general the tails are heavier in north china than in south china as well as in the autumn and winter compared to summer and annual series in addition to ground based data there are also studies using g e v with climate model generated data to explore the response of extreme precipitation to climate change or human activities li et al 2017 li et al 2018a li et al 2019a wen et al 2016 based on ground data and the coupled model intercomparison project phase 5 cmip5 li et al 2017 focused on the probability based index derived from 1 day and 5 day am precipitation by fitting g e v they assessed the effects of anthropogenic forcings and other external factors on observed increases in extreme precipitation in addition to g e v and p e 3 distributions such as l n 3 and the five parameter wakeby w a k have also been used to study the statistical characteristics of am series in china jiang et al 2008 su et al 2009 fischer et al 2012 zhang et al 2017a lu et al 2016 su et al 2009 used the g e v generalized pareto g p generalized logistic g l o and w a k distributions with am series of observed data in the yangtze river basin and grid data derived from the echam5 mpi om climate model they showed that w a k has a better fitting and yields the minimum value of the k s statistic they also found that the design precipitation under the current 50 year return period level may become more frequent in the future turning into a 25 year return period some studies have also pointed out that when selecting a probability distribution for hydrological analysis the distribution with fewer parameters should be given priority blum et al 2017 therefore although the w a k distribution has a better fit due to the data length limitation there may be significant sampling errors papalexiou et al 2013 thus the w a k distribution is not always the best choice and its applicability needs further study annual maxima parameter estimation methods reasonable and accurate estimation of distribution parameters is fundamental for extreme precipitation frequency analysis and by extension rainstorm design and risk analysis estimation the methods used for probability distribution parameter estimation in china can be divided into two categories one is the analytical method including the commonly used method of moments mom christopeit 1994 pearson 1902a 1902b weighting function method probability weighted moments pwm greenwood et al 1979 hosking et al 1985 linear moments l moments greenwood et al 1979 hosking et al 1985 sillitto 1951 etc another method is the optimized curve fitting method which establishes an objective function and iteratively adjusts the parameter set so that the known theoretical frequency curve fits the empirical frequency point data optimally in engineering practice in china the combination of the two categories of methods is often adopted to determine the pdf parameters lei et al 2017 the mom is simple and easy to operate it is one of the earliest methods used for parameter estimation wang and chen 1994 used the gumbel g distribution a special form of the g e v with am in chengdu city combined with successive approximations least squares and the mom to estimate the distribution parameters the error between the estimated rainstorm and the observed values was assessed to determine the optimal parameter estimation method the mom was the most robust method with a relative error less than 1 however when the sample size is small the method yields large errors and the design value has a large negative bias jin 2005 the pwm method is applicable when the analytic expression of the inverse function of the probability distribution can be obtained since the inverse function of the p e 3 distribution is not easy to obtain the application of this method in china is limited song and ding 1988 deduced the relationship between the parameters of p e 3 and pwm and showed that when estimating the parameters of the p e 3 distribution the pwm method is better than mom due to its impartiality and effectiveness then li 1989 derived a simplified formula of the pwm of p e 3 and made a look up table making this method more convenient the l moments are derived from the pwm compared to the pwm the l moments are more convenient because they directly provide scale and shape measures of the pdf su et al 2009 yang et al 2010 yuan et al 2017a even for the parameter estimation of small samples these parameters are unbiased chen et al 2014 at present l moments are widely used in research in china cai et al 2007 jiang et al 2009 su et al 2009 yang et al 2010 fischer et al 2012 chen et al 2014 yuan et al 2017a cai et al 2007 applied the g distribution to fit the am series of 210 stations in eastern china they showed that compared to the method of moments the l moments improved the goodness of fit of the g greatly and were successful in estimating the extreme quantiles under the given return period liang et al 2013 used monte carlo analysis to simulate the am series of 96 stations in taihu lake basin they compared the parameters estimated by l moments and mom and found that the parameters estimated by the former were more accurate unbiased and robust the maximum likelihood ml fisher and russell 1922 method is also frequently used for parameter estimation in china song et al 2007 fischer et al 2012 zhang et al 2012 however huang et al 1994 revealed that ml is very sensitive to the sample s minimum value when estimating parameters of the p e 3 also when the coefficient of variation of the series is greater than 2 the p e 3 parameters have no solution in addition studies have pointed out that due to the cumbersome calculation of the ml an analytical solution cannot be obtained for some distribution curves including the p e 3 with limitations in practical applications liang et al 2013 from the above literature review it can be concluded that methods other than l moments present great limitations therefore in recent years the l moments have become the mainstream method for parameter estimation of extreme precipitation using am since the p e 3 is recommended in china s standard guidelines the research on parameter estimation is basically based on this distribution the gauss newton iterative algorithm method is recommended by guidelines hydrology bureau of changjiang water resources commission of the ministry of water resources 1995 for the numerical solution of the p e 3 distribution parameters but this method is affected by the initial value and the convergence speed is slow li and song 2009 in recent years artificial intelligence genetic and ant colony algorithms among others have been used for the parameter estimation of the p e 3 li and song 2009 lei et al 2017 in these cases an optimization algorithm is used to find the optimal value depending on the objective function in addition to the algorithm itself research also focuses on determining the objective function qiu et al 1998 xie and zheng 2000 copulas for the dependence among precipitation extremes in china the hydrological frequency analysis in practical engineering only involves one aspect of extreme events however correlation generally exists among the internal attributes of extreme events such as the duration and intensity of rainstorm the peak and volume of flood the duration and intensity of drought etc liu and guo 2021 investigating the dependence of these attributes in extreme events and conducting multivariate hydrological frequency analysis is bound to be of great benefit to improve the accuracy and reliability of hydrological frequency analysis this can be achieved with the copula function which describes the nonlinear and asymmetric correlation between variables without being restricted by the type of the marginal distribution guo et al 2008 xiong and guo 2004 applied the copula function to construct the joint distribution of flood peak and flood volume with the gumbel hougaard copula in the yangtze river basin they are one of the first groups of scholars in china to apply the copula function in hydrology in china the application of the copula function on extreme precipitation mainly focuses on the construction of bivariate joint distributions to describe the interdependence and intrinsic relationship between different indices obtained by am of rainstorm and then calculate the design rainstorm and evaluate the flood risk under a multivariable framework wang et al 2017a zhang et al 2013 based on the gumbel hougaard copula zhang et al 2008 constructed the joint distribution of annual maximum 1 day and 7 day precipitation with p e 3 as marginal distributions and calculated the value and process of the design rainstorm their results showed that the design rainstorm based on the joint distribution is safer than that based on univariate analysis and it is more conducive to flood control safety zhang et al 2012 applied the archimedean copula and elliptical copula families to analyze the joint probability distribution of 8 precipitation extremes including continuous and discrete variables and calculated the joint return periods of two different precipitation indices decreasing joint return periods of p75 total precipitation amount larger than the 75th percentile within a year mm and i75 precipitation intensity of larger than the 75th percentile mm day were found in most of the xinjiang area implying that the probability of concurrent occurrence of high intensity extreme precipitation increased after 1980 jhong and tung 2018 obtained four extreme precipitation indices from a weather generator model conducted with general circulation models and investigated the future changes in the joint probability behaviors of precipitation extremes in shih men reservoir in northern taiwan province china they found that the occurrence probability of the joint extreme precipitation events considering amount and intensity may increase in the upstream of the study area causing high risk of floods in the future in addition to the frequency analysis of multi characteristic attributes of extreme precipitation events the copula function is widely used for the encounter probability of rainstorm rainstorm and flood and the joint distribution of rainstorm and various environmental extreme events yan et al 2007 liu and chen 2009 yan and chen 2013 zhao et al 2012 analyzed the probability of the synchronous occurrence of precipitation extremes including annual maxima of 1 3 5 and 7 day precipitation between different regions in the pearl river basin the lognormal distribution was selected as the marginal distribution of these extreme variables and most of the joint distributions were fitted to the gumbel hougaard copula family based on akaike information criterion aic their results revealed the underlying linkages between precipitation and floods from geographical perspective yan and chen 2013 used the clayton copula function to quantify the synchrony and asynchrony of precipitation including combined frequencies of wet dry and normal conditions in the source area and the receiving area of the middle route of the south to north water transfer project they provided corresponding scientific basis for the implementation and operation of the water conveyance project besides application some studies also focus on the improvement of multivariable frequency analysis methods qian et al 2018 proposed the maximum entropy method to estimate the parameters of the gumbel and gumbel hougaard copula function which requires only the lower and upper bounds of two hydrological variables and tested their method in heihe and jinghe river basin their results proved maximum entropy estimation to be a reliable and robust parameter estimation method in data scarce regions peaks over threshold although the am method is one of the most commonly used extreme precipitation sampling methods it selects one precipitation value every year without considering the inter annual differences in climatic conditions this may lead to the inclusion of false data into the extreme precipitation series or the loss of valuable information ding et al 2011 xia et al 2012 the peaks over threshold pot sampling method sets a threshold and considers the values exceeding it as the extreme precipitation series the process improves the am method to a certain extent and makes more effective use of the precipitation data the threshold selection affects the analysis of extreme precipitation since different thresholds lead to series with different statistical characteristics liu et al 2017a the current research on pot in china mainly focuses on two aspects one is to investigate the selection and the regional characteristics of the threshold the other is the frequency analysis threshold selection the threshold should be in line with the climatic characteristics of the study area and the size of the extreme precipitation series should be considered for its selection wang et al 2020 at present the main methods for threshold selection in china are the percentile method and the average annual occurrence frequency method the most commonly used percentile thresholds in china are the 95th 99th and 99 5th xia et al 2012 wang et al 2015 he et al 2017 kong et al 2019 xia et al 2012 used the 99 5th percentile threshold to select extreme precipitation series in the huaihe river basin he et al 2017 adopted the 99th percentile as the extreme precipitation threshold in the minjiang river basin the average annual occurrence frequency method assumes that extreme precipitation over the threshold follows the poisson distribution the study of jiang et al 2009 showed that a threshold that is too large or too small affects the correlation coefficient between the simulated theoretical and measured values of extreme precipitation in their study the occurrence number was set to 1 and they pointed out that the daily extreme precipitation during summer in eastern china conforms to the g p distribution guo et al 2010 studied the relationship between the 50 year return period of precipitation the scale parameters of the g p and the length of the over threshold sample series under different occurrence numbers in the bohai bay region the optimal occurrence number for each representative station was determined based on the value of the scale parameter and the stability of the return period the optimal average annual occurrence frequency was estimated between 1 and 2 5 in addition some studies have explored the impact of threshold selection on extreme precipitation analysis liu et al 2013 applied various threshold selection methods namely the absolute critical value the percentile the parametric and the detrended fluctuation analysis to construct extreme precipitation series in the pearl river basin they noted that the detrended fluctuation analysis could provide a unique extreme precipitation threshold set for a large basin with uneven spatiotemporal precipitation distribution this method was the most suitable for threshold selection in pearl river basin however it requires a large rainfall series and complicated calculations a small number of studies used graphical methods to determine the threshold qu et al 2014 applied the mean residual life plot method to obtain the pot series threshold and li et al 2014 used the hill plot at present studies on the selection of optimal thresholds focus on local areas or watersheds and few have aimed to explore the threshold selection criteria at the national scale zhang et al 2017b used the average annual occurrence frequency method and the percentile method to propose selection criteria of extreme precipitation thresholds for different regions across china they found a correlation between the optimal threshold scheme and the division of dry and wet regions in china in humid regions such as the southeast the 90th 94th percentile should be used as threshold the 94th 97th percentile is suggested for semi humid and semi arid regions such as western and northern regions while in arid areas such as northwest china the 97th 99th percentile is more suitable peaks over threshold distributions in terms of frequency analysis the contents of the pot and am series are basically the same including the probability distribution selection parameter estimation etc the g p distribution is often used to describe the probability distribution of the series obtained by the pot sampling method cheng et al 2008 analyzed the applicability of the g e v and g p distributions for the pot series in chongqing using the k s correlation coefficient and mean square error as evaluation criteria they verified that the g p distribution fits the observed data of the pot series significantly better than g e v ding et al 2008 estimated the relationship between the distribution parameters and quantiles of g p and g e v and used it to calculate the design rainstorm in beijing considering its thick tail characteristics g p is more suitable for the frequency analysis of extreme precipitation pei et al 2018 studied the spatial and temporal characteristics of extreme precipitation in the yangtze river delta by fitting g p to the pot series and found that the extreme precipitation is likely to be greatly affected by large scale monsoons and urbanization kong et al 2019 used the 90th 95th and 99th percentile thresholds to extract the extreme precipitation series they used the weibull distribution to fit these series and analyze the spatial variation characteristics of precipitation intensity for different return periods in china their results showed that it is reasonable to use the 95th and 99th percentiles as thresholds to study extreme rain and precipitation respectively in eastern china the spatial pattern of precipitation intensity was also found to vary greatly under different thresholds for the same return period peaks over threshold parameter estimation methods the commonly used methods for parameter estimation based on the pot series mainly refers to g p distribution are similar to those of am the l moments and ml are the main parameter estimation approaches jiang et al 2009 used l moments to estimate the parameters of the g p distribution and proved the suitability of the method li et al 2014 used both l moments and ml to obtain the parameters of the g p through a fitting test it was found that the design values obtained by the two methods had little difference however the threshold must be determined first for the g p introducing uncertainty through subjectivity zhao and zhai 2015 applied the pickands bootstrap moment estimation method for the parameter estimation of extreme precipitation in beijing city they found that this method can obtain a reasonable pot threshold while estimating the parameters at the same time compared to the results of the l moments with the same threshold the pickands bootstrap moment provided better fitting comparison of am and pot in china there are numerous studies analyzing and comparing the applicability of the am and pot for frequency analysis and rainstorm design song et al 2018 gao and xie 2016 xia et al 2012 he et al 2017 he et al 2020 su et al 2008 yuan et al 2017b xia et al 2012 constructed am and pot series using the 99 5th percentile as a threshold in the huaihe river basin they selected the g e v g p and gamma as candidate distributions and found that the best distribution for the am series is g e v and the optimal for the pot series is g p for the same return period the design rainstorm based on am is lower than that based on pot from the perspective of disaster prevention and mitigation and data reliability the pot series better describes the extreme precipitation events in the huaihe river basin compared to am he et al 2020 used the g p and g e v distributions to fit the pot 99 5th percentile and am series respectively in beijing jinan and shenzhen they applied the markov chain monte carlo mcmc method to estimate the distribution parameters they showed that the confidence interval of the design rainstorm obtained by g p exhibits lower uncertainty under the same return period the design rainstorm obtained by g e v is larger he et al 2017 explored the applicability of different distributions to extreme precipitation at 15 stations across the minjiang river basin the optimal distribution of all am series was g e v and the optimal distribution of 13 stations of the pot series was g p the design rainstorm for different return periods calculated by the optimal distribution of the am and pot series is quite different therefore special attention should be paid to the construction of extreme precipitation series and the selection of the distribution for the design rainstorm furthermore lu et al 2016 found that for the same station and time series length the range and standard deviation of the pot series are smaller while the kurtosis skewness and mean value are larger compared to the am series indicating that the pot series is more stable due to the fact that numerous indices obtained by am or pot series can describe extreme precipitation it is difficult to determine which index is the most representative so some studies used the results of prototype analysis of extreme precipitation observations as the research object su et al 2017 in addition to the statistical indicators there is another indicator named event based extreme precipitation which is defined as a continuous process of precipitation over several days including at least one daily precipitation exceeding the 90th or larger quantiles this kind of precipitation often occurs in southern china which has also attracted the attention of some scholars shang et al 2020 continuous precipitation including event based extreme precipitation may cause more serious floods and special attention should be paid in future research studies on extreme precipitation in china based on the sampling method including the region of application the data used the period of analysis the parameter estimation methods and distributions and the main conclusions are summarized in table 1 probable maximum precipitation estimation method probable maximum precipitation pmp is defined as the greatest depth of precipitation for a given duration that is physically possible for a design watershed or a given size storm area at a particular location at a given time of the year with no allowance made for long term climatic trends world meteorological organization 2009 pmp is the highest standard for flood control design of water conservancy and hydropower projects in china and it is also the flood protection standard for nuclear power projects national energy administration of the people s republic of china 2010 national standards of the people s republic of china 2014 it can be divided into two categories recommended by wmo statistical estimation method and hydrometeorological method hydrometeorological method includes the rainstorm maximization method rainstorm transposition method and generalized estimation method the pmp methods originate in the united states but in the case of china these methods have made some minor adjustments the main estimation methods for pmp in china are described in the next sub sections and the spatial distribution and study progress of pmp methods in china is shown in fig 1 statistical estimation method the statistical estimation method refers to hershfield s km value method hershfield 1961 this method is based on chow s general frequency equation and derived from the modified frequency analysis of am series or precipitation quantile which belongs to quasi statistical methods koutsoyiannis 1999 liao et al 2020 the formula of the pmp statistical estimation method is x m x n k m s n and k m x m x n 1 s n 1 where the frequency factor km can be estimated by samples x n 1 and s n 1 are the mean value and the standard deviation respectively of the annual maximum series with the largest term removed according to hershfield s km value method the measured maximum rainfall is not included in the am series mean value and standard deviation that is the maximum event is observed after the basic statistics are determined when the method was introuced in china it was modified and the km was replaced with the standardized variable i e the deviation coefficient φ m the maximum deviation from the mean of a sample is scaled by the standard deviation of the sample and pmp is estimated from the actual heavy rain the following equation is generally used to calculate pmp x m x n φ m s n and φ m x m x n s n when calculating the mean and standard deviation the maximum term of the annual maximum series is retained in the 1970s the statistical estimation method was used for the 24 hour pmp mapping of shaanxi and hubei provinces in china hua et al 2007 believed that this method is just calculated by the actual heavy rain and there is no concept of amplification so the calculated pmp value is often small they introduced the concept of amplification into the statistical estimation method by using the moisture maximization to amplify the locally measured rainstorm then the amplified value was substituted into the above frequency equation to calculate the pmp and applied in ningde city they showed that the pmp obtained with the statistical estimation method alone is 825 mm while the value estimated by the statistical estimation combined with the amplification concept is 1096 mm some scholars did not apply the φ m commonly used in china when calculating pmp but continued to use the km value as early as 1981 lin 1981 discussed the statistical relationship between the traditional frequency factor km and the standardized variable φ m he derived the relationship between them and obtained the expression of the range of φ m related to the data length n he proved that km is a consistent estimator of φ m and has the property to approach the upper limit of φ m as n increases he also presented the length of data needed to obtain reliable pmp estimations for specific φ m values τhe shortest series length satisfying the estimation conditions is n m φ m 2 2 summarized as the modified km method lin 1981 showed that φ m is the screening condition of data series availability and it is not used to replace km however the modified km method does not reveal the essence of rainstorm formation from the perspective of physical causes and it still does not properly solve the fundamental problem of instability in short term data frequency calculation lin 1981 believes that this method can only be a transition method from the frequency calculation to the hydrometeorological method lan et al 2017 used the modified km method to calculate the 24 h pmp in hong kong china they obtained the value of 1753 mm which was substantially greater than 1250 mm given by chang and hui 2001 using the method of moisture maximization they pointed out that this difference may stem from the use of totally different methods based on different data the modified km method can be used as a preliminary method to obtain the pmp of a single station or small watershed however although the modified km method utilizes data from the whole study area the results of pmp mainly represent potential rainstorm centers and there is no concept of temporal and spatial distribution the pmp estimation is generally based on gauge records and in this case it can only be calculated at specific points or a small area where the gauges are located yang et al 2018 provided a new perspective of data sources for pmp estimation in sparsely gauged or ungauged areas they explored the above mentioned statistical method based on satellite precipitation datasets and found that cmorph ashouri et al 2015 and 3b42v7 huffman et al 2007 can be well applied for the estimation of pmp and underlined the potential of the satellite data in pmp estimation rainstorm maximization method the main idea of the rainstorm maximization method is to consider the measured rainstorm as a typical example select the relevant influence factors and amplify reasonably this method mainly includes the moisture maximization moisture and wind maximization and moisture transport efficiency maximization lin et al 2018 svensson and rakhecha 1998 zhan and zhou 1984 as early as the 1970s zou 1977 used the water vapor convergence index maximization to estimate the pmp in the eastern part of the qinghai tibet plateau they pointed out that this method is suitable for a small scale 24h pmp estimation hua et al 2007 combined the moisture maximization method with the statistical estimation method to assess the pmp in ningde city they considered this approach to be more reasonable than the traditional statistical estimation method although the moisture and wind maximization mwm is widely used to estimate the 24 hour pmp zhou et al 2020 the variables for calculating the mwm factor do not consider the main direction of moisture inflow tending to overestimate the pmp therefore liang et al 2017a proposed an improved moisture and wind maximization method βased on the wind rose diagram the data were grouped according to the main water vapor inflow direction of the study basin and the typical rainstorms in each wind direction were amplified then the maximum value in all directions was taken as the final result of pmp this method was applied to estimate the pmp of a nuclear power plant in hubei province results suggested that the pmp is 1 14 times higher than that of the rainstorm with a 10000 year return period this ratio is consistent with the cognition that the ratio of pmp to the 10000 year design rainstorm is between 1 1 and 1 2 in most areas of china therefore the improved mwm is considered to be reliable and reasonable and it provides a new idea for pmp calculation zhou et al 2020 used this improved mwm method to estimate the 24 hour pmp in yanshan basin hebei province and further derived short duration pmps with statistical methods τhen they obtained the corresponding probable maximum flood by using the unit hydrograph and the empirical formula instantaneously the ratio of the 24 hour pmp 1026 2 mm calculated by this method to the design value of rainstorm with a 10000 year return period 875 8 mm showed that the improved mwm is reasonable and can provide the scientific basis for flood control engineering construction given that the flow concentration time in some areas may be longer there are also studies focusing on the pmp over a longer duration based on the grid daily precipitation datasets liu et al 2018b applied the rainstorm maximization method to estimate the value and spatiotemporal distribution of pmp in the upper reach of nujiang river basin qinghai tibet plateau they then used the swat model to simulate probable maximum flood pmf and their study presented a procedure for estimating long duration pmp and pmf of sparsely gauged large river basins rainstorm transposition method the rainstorm transposition is by far the most commonly used method for pmp estimation in mountainous areas lin et al 2018 the core of this method is that the measured rainstorm needs to be divided into convergence rain component caused only by atmospheric factors and terrain rain component affected by the terrain as the convergence rain and its spatial distribution is not affected by terrain it can be transported different terrains have different effects on the water vapor circulation and rainstorm distributions therefore the quantitative analysis and calculation of the terrain impact on rainstorm are particularly important for the accuracy of rainstorm separation and the rainstorm transposition method constituting a hot spot for chinese scholars as early as the 1980s some scholars began to study the correction of terrain rain gao and xiong 1983 established the correlation between the vertical change of the terrain ascent speed and atmospheric stability and the length of terrain wave according to the weather characteristics different expressions of terrain ascent speed were selected and applied for the correction of the terrain rain in the huaihe river basin obtaining a credible result lin 1988 proposed the step duration orographic intensification factor sdoif method to estimate terrain rain which uses the two dimensional difference to approximate the impact of the terrain on the rainstorm and the process of spatiotemporal changes the concept of orographic intensification factors is introduced which includes the comprehensive effects of terrain on triggering uplifting and blocking the water vapor flow the pmp application in a river basin of hainan island proves that this method can not only estimate the pmp value in mountainous area but also obtain the spatial and temporal variation of pmp isolines following the cause analysis lan et al 2018 used the rainstorm transposition method based on the orographic intensification factors and the improved statistical estimation method of km value to calculate the 4 h pmp in hong kong they suggested using both of the two methods to calculate the pmp generally the statistical estimation method is used to calculate a prompt result for reference but the rainstorm transposition after moisture maximization can consider various factors and has more reasonable and reliable results liao et al 2020 improved the sdoif method by merging the data of multiple sites with similar characteristics to supplement the limited sample size and used the regional linear moment method to estimate the rainstorm quantiles then the am precipitation was replaced by the quantile value to calculate the orographic intensification factors oif finally the convergence rain separated in taiwan was moved to hong kong to estimate the pmp of 4 and 24 hour the oif obtained through rainfall quantiles could better reflect the enhancement effects in orographic intensification areas also the pmps calculated by this method i e 1341 58 mm calculated with 500 year rainfall quantiles are smaller compared to the values estimated by a revised km value method lan et al 2017 generalized estimation method the generalized estimation is a set of methods used to estimate pmp in small watersheds including rainstorm maximization transposition statistical method etc in china it is generally referred to as the time area depth generalized method it is the regional generalization of the spatiotemporal distribution of pmp estimates in many watersheds of different sizes in a large region the advantage of the generalized estimation method is that it can make full use of various data in a region this method was used to draw the national pmp contour map in the 1970s ye and hu 1979 zhan and zou 1980 in the united states the results of the generalized estimation method have guided the flood control design of large scale water conservancy project constructions however the pmp isoline in china has not played a corresponding role and the design of major water conservancy projects is still based on the estimated value of single station pmp lin et al 2018 in addition to the above four methods there is also a small number of studies using numerical weather prediction models i e numerical simulation methods to estimate pmp wang 1988 used a limited area fine grid numerical precipitation prediction model to evaluate the pmp value of the upper hanjiang river basin the pmp was estimated by the net water vapor transportation method providing a new perspective the numerical simulation method can be used to estimate pmp of areas with sparse gauged sites and it also opens up a new way to study the impact of climate change on pmp however numerical weather prediction models are generally large scaled and there are great uncertainties for pmp estimation at watershed and point scale which still face many challenges in practical applications non stationary analysis of extreme precipitation the frequency analysis of extreme precipitation is generally based on the stationarity assumption which considers that the distribution parameters are constant and so is the value of the design rainstorm in recent years due to the influence of climate change and human activities the hypothesis of stationarity has been questioned milly et al 2008 the hydrometeorologic extreme events in some areas in china show characteristics of non stationarity such as trends abrupt change or the combination of them studies have pointed out that the magnitude of extreme precipitation events in the middle and lower reaches of the yangtze river western china and in parts of the southwest and south china coastal areas shows an upward trend the north shows a downward trend zhai et al 2005 lu et al 2020 the frequency of 1 day extreme precipitation event is increasing in most parts of china while it is decreasing in north and southwest china the frequency of extreme precipitation lasting for more than two days has a significant increasing trend in the middle and lower reaches of the yangtze river jiangnan area and eastern plateau while it has a decreasing trend in north and southwest china wang and qian 2009 due to the extreme precipitation changing trends non stationary hydrometeorologic analysis has attracted much attention in recent years the current non stationary research on precipitation extremes in china can be summarized into two aspects which will be described in this section the study of driving factors i e climate factors and the frequency analysis under non stationary conditions the summary of the studies of non stationary analysis of extreme precipitation is shown in table 2 and the climate covariates which affect extreme precipitation in china are shown in fig 2 driving factors exploring the correlation between extreme precipitation and climate covariate factors is expected to provide a root cause explanation for the trend and non stationarity of extreme precipitation the first aspect in the study of non stationary extreme precipitation is the correlation analysis between precipitation extremes and the climate covariates that induce the non stationarity the climate in eastern china is dominated by the east asian monsoon and in recent years studies have revealed that the east asian monsoon is significantly affected by el niño and southern oscillation enso some studies have pointed out that the east asian monsoon is also affected by the north atlantic oscillation nao the indian ocean dipole iod and the pacific decadal oscillation pdo xiao et al 2017 therefore enso monsoon index iod pdo arctic oscillation ao and north pacific oscillation npo are mainly selected as the climate covariate factors in extreme precipitation studies in china gao et al 2017 xiao et al 2017 xu et al 2016 research on the climate covariate factors that lead to the trend non stationarity of extreme precipitation is summarized in table 2 among these numerous climate covariate factors enso is the most widely studied xiao et al 2017 duan et al 2017 lv et al 2018 wang et al 2021 zhang et al 2014a li and zhai 2009 xiao et al 2017 showed that in central china the extreme precipitation intensity tends to decrease when enso is in the positive phase while in the year after its positive phase the intensity in eastern china increases duan et al 2017 pointed out that enso has a significant impact on the extreme precipitation process over the entire pearl river basin in the warm enso episodes the magnitude and frequency of extreme precipitation events in the west of the basin will increase while in the eastern part the magnitude will decrease enso may exert different effects on extreme precipitation in different regions of china lv et al 2018 explored the potential relationship between annual and seasonal maximum daily precipitation and local temperature ltem global surface temperature gtem and enso in the yangtze river basin they found that ltem and enso had no significant effect on the annual maximum daily precipitation however there is a negative correlation between the winter maximum daily precipitation and enso in addition to enso other climatic factors also show significant effects on extreme precipitation in china su et al 2017 deng et al 2018 liu et al 2017b zhao et al 2014 gu et al 2017b zhang et al 2015b zhao et al 2014 explored the relationship between extreme precipitation and climate factors by continuous wavelet analysis at 42 gagued stations in the pearl river basin and their results suggested that pdo and soi were important factors affecting extreme precipitation gu et al 2017b conducted a study on non stationarity and probable causes of the heavy precipitation occurrence rate by a series of methods including pot sampling technique kernel density estimation method diggle 1985 cox smith and karr 1986 poisson regression model villarini et al 2011 and generalized additive models for location scale and shape gamlss rigby and stasinopoulos 2005 their results manifested that extreme precipitation exhibited non stationary at seasonal scale and the seasonal variability was significantly related to the climate factors as the sampling threshold increases the impact of climate factors on the occurrence rate of extreme precipitation gradually weakens in addition higher soi and pdo may increase the frequency of extreme precipitation in northeast and eastern china zhang et al 2015b analyzed the correlation between winter extreme precipitation and climate factors in southeast china and showed that the probability of extreme precipitation events will be higher lower in the positive negative phase of iod frequency analysis under non stationarity the second aspect of the non stationary extreme precipitation analysis is to introduce time or climate factors as covariates into the probability distribution to analyze the frequency and magnitude of precipitation extremes the rise and development of this part are relatively short the core content is to 1 determine the best covariate factors that can explain the statistical characteristics of precipitation extremes 2 construct the optimal non stationary probability distribution model 3 carry out the rainstorm design and risk analysis under non stationary conditions and 4 compare the calculation results with those of the stationary model at present there are mainly three models for constructing the non stationary analysis one is the gamlss and the second is the g e v distribution having location and scale parameters variant with time or climate factors the above two models focus on am series the third model is a non stationary one based on pot series which has not been widely used in the study of non stationary extreme precipitation analysis in china for instance wu et al 2015 constructed a time varying pot model using the g p distribution to explore the spatiotemporal variation characteristics and the causes of the non stationarity of extreme precipitation in the pearl river basin they used random forests genuer et al 2010 to measure the importance of the climate factors affecting precipitation extremes and demonstrated that the el nino index smei has the most significant impact on extreme precipitation and the time varying pot model with the g p can deal well with the non stationary characteristics in some regions of the pearl river basin in addition some studies introduced time and other covariates into the lognormal l n and p e 3 distributions to construct non stationary models chen et al 2017 zeng et al 2017 some studies considered the whole study area under a non stationary framework constructing non stationary models for all stations su and chen 2019 set up link function combinations each function contains only one covariate factor with different climate factor covariates for g e v location and scale parameters they then constructed non stationary models of the dry and rainy season in the pearl river basin based on am series to explore the best climate factor covariates the results showed that the spatial distribution of the best covariates varies greatly and enso is the best covariate for most stations in the study area but not all the best covariates are significant in addition the design rainstorm calculated by the non stationary model presents great variability in space and the difference between the maximum and minimum design values of the same station is significant indicating that covariates introduce great uncertainty zhang et al 2015c constructed two non stationary models based on gamlss with time and climate factors as covariates for the am series in the beijing tianjin hebei region they demonstrated that the goodness of fit of non stationary model is often better than the stationary one the non stationary model with climate factors as covariates has better goodness of fit than that with time as covariate which can capture the variation characteristics of extreme precipitation moreover they pointed out that the east asian monsoon index is the most significant covariate in the study area instead of directly using climate factors as covariates to construct non stationary models zeng et al 2017 introduced principal component factors of climate variables and time as covariates into l n and g e v distributions and discussed the performance of the stationary and two non stationary models considering the maximum 5 day precipitation and the number of very heavy precipitation days in the hanjiang river basin hao et al 2019 constructed one stationary and two non stationary models based on gamlss then they identified the homogeneous subregions for precipitation extremes based on cluster analysis according to climate covariate predictors and found that the behaviour of extreme precipitation is significantly affected by climate factors specifically the non stationary model with climate covariates describes the change of extreme precipitation better than the model with time as a covariate there are regional patterns for the dominating climate factors and hanjiang river basin can be divided into three homogeneous regions according to climatic indices chen et al 2017 introduced several trend models of the mean and variance of am series into the extreme value type i distribution l n and p e 3 distributions to construct a non stationary model with time as covariate for 9 rainfall stations in taiwan china they analyzed the trend of two return periods defined under non stationary conditions and showed that the linear model can capture the time trends of the mean and variance well but not all stations are under a state of non stationarity with the consideration of non stationarity the return periods of rainstorms calculated by the expected recurrence time compared to those values estimated by the reciprocal of the exceedance probability of occurrence is small before constructing the non stationary model some studies test the non stationarity of the extreme precipitation series first gao et al 2016 tested the am series of 631 meteorological stations in china by the kwiatkowski phillips schmidt shin kpss and mann kendall m k tests and found that only the extreme series of 48 stations showed non stationarity further the g e v non stationary model using time as covariate was constructed for the non stationary stations with an obvious trend of extreme series while the g e v non stationary model using climate factors as covariates was constructed for the non stationary stations without an obvious trend of extreme series the results indicated that the non stationary model can better capture the interannual variation of precipitation extremes compared to the stationary one song et al 2020 used five methods to test data trends abrupt change points and non stationarity of am series in the beijing tianjing hebei region they constructed a non stationary g e v model for non stationary stations they set the three link functions and 84 function combinations between covariate factors and distribution parameters explored the optimal non stationary model and calculated return periods of rainstorms under non stationary conditions they demonstrated that the non stationarity of extreme precipitation is not significant and enso is determined as the optimal covariate in the beijing tianjing hebei region compared to the stationary model the non stationary model can better capture the interannual variations of precipitation extremes however there is no significant difference in the design rainstorm between the stationary model and the median value of the non stationary model based on the am series of 14 stations in poyang lake basin yin et al 2016 constructed a non stationary g e v model with time as the covariate of location parameter they calculated the design rainstorms of non stationary stations for different return periods and found that only one station in 14 showed a significant upward trend and the non stationary model was better than the stationary model in terms of the likelihood ratio test the design rainstorm increases under non stationary conditions and the design rainstorm with 100 year return period in 1951 is comparable to the design rainstorm with 50 year return period in 2010 a relatively consistent conclusion of these studies can be reached it is found that only a small number of extreme precipitation series show non stationarity through the trend and non stationary test of extreme series the non stationary assumption is valid only in some regions and the extreme precipitation in other regions still satisfies the stationary assumption in addition a small number of studies have focused on the non stationary of the dependence between different extreme precipitation variables li et al 2019b used the time dependent archimedes copula function and the time varying g e v to fit the joint and marginal distribution of extreme precipitation amount 90th percentile threshold of daily precipitation and intensity annual maximum daily precipitation in the eastern coastal area of china respectively they studied the evolution law of the correlation of multivariate variables and found that the spatial distribution of extreme precipitation is very uneven in eastern coastal china and the extreme precipitation occurrence risk of shandong province was found to be the largest corresponding to long joint return period intensity duration frequency curves under the combined impact of global climate change and urbanization urban floods are becoming more and more prominent causing severe challenges to urban drainage and waterlogging prevention zhang et al 2014b more than 60 of the cities in china were affected by urban flood to varying degrees of which nearly 140 experienced floods more than three times from 2008 to 2010 xiong et al 2017 in order to alleviate the problem of urban flooding the construction of urban drainage networks and flood control infrastructures needs to be further strengthened the current drainage system in china is designed to be safe under a certain design rainstorm obtaining the reasonable design rainstorms under various durations is the premise for reasonably determining the construction scale of drainage and waterlogging infrastructure therefore it is of great important to scientifically compile the urban rainstorm intensity duration frequency idf curve or further summarize it into the form of rainstorm intensity formula xiong et al 2017 water conservancy departments and municipal departments in china focus on different aspects of urban flooding which is pipe network drainage and regional waterlogging removal respectively jia and li 2015 municipal departments usually apply the short duration rainstorm intensity formula compiled by the am series within 3 hour while water conservancy departments generally conduct frequency analysis based on the 24 hour maximum precipitation to determine the design rainstorm jia et al 2021 in china the research on idf generally aims at the formulation of the rainstorm intensity formula which describes precipitation intensities for different durations and return periods in 2011 the ministry of housing and urban rural development of the people s republic of china revised the urban rainstorm intensity formula of 606 cities in china based on the am method and g distribution comprehensively and issued a new edition of code for design of outdoor wastewater engineering shao and shao 2013 subsequently the 2014 year version of the aforesaid code was revised by focusing more on actual urban waterlogging problems finally a complete set of theories and methods has been formed for the calculation and application of urban design rainstorm in china the procedure of compiling the rainstorm intensity formula in china can be simply summarized as two step optimal method including frequency distribution curve fitting and parameter estimation of rainstorm intensity formula liu et al 2018c the frequency distribution curve fitting is a complete set of extreme precipitation frequency analysis process including sample selection optimal probability distribution selection and parameter estimation in the early stage of compiling rainstorm formula annual multisampling method is the most commonly used due to the short data length of precipitation series deng 2006 nowdays standard guidelines in china stipulate that the am series should be used to fit a frequency distribution curve in areas when there is more than 20 years of automatic recorded rainfall records shanghai municipal engineering design instituter group co lid 2006 in china p e 3 distribution has been adopted for urban rainstorm frequency analysis for a long time in recent years there are also studies using g distribution and the exponential distribution huang et al 2013 shao and liu 2018 analyzed 24 933 rainfall samples under various specified precipitation durations 5 120min of 607 cities and verified that a combined p e 3 and g distribution using the am sampling methods is a superior theoretical distribution type for short duration rainstorm intensity formulas after obtaining the frequency distribution curve the second step is the estimation of the parameters of the rainstorm intensity formula research on the rainstorm intensity formula in china dates back to the 1970s deng 1979 the mathematical expression of the comprehensive formula recommended in the code shanghai municipal engineering design instituter group co lid 2006 is i a l 1 c lg t t b n where i denotes the rainstorm intensity and al c b and rainstorm attenuation index n are four important parameters that are derived and modified by the gauss newton iterative algorithm liu et al 2018c pointed out that the expression a l 1 c lg t is an empirically determined function depending on the return period of the rainstorm as a shortcoming furthermore only a single station is employed for urban rainstorm frequency analysis in china without considering the heterogeneity of extreme precipitation distribution in space liu et al 2018c at present there are numerous applications of the short duration rainstorm intensity formula in china liu et al 2019 yuan et al 2020 zhuang et al 2015 chen 2013 such applications were generally based on the above two step optimal method to determine the rain intensity formula of a city local area or update the parameters of the existing formula to analyze the variation characteristics of the design rainstorm regarding to the data usage for deriving idf curves conventional raingauges exist long before automatic raingauges were deployed jiang and tung 2013 however daily precipitation data with long records at conventional raingauges are of limited use to derive rainfall idf relationships to fully utilize available long period daily precipitation records jiang and tung 2013 presented a methodological framework based on the scale invariant properties of the rainfall scaling model to derive idf relationships with short duration they confirmed that the g e v based scale invariant model is applicable in hong kong china the above applications and research mostly focus on short duration rainstorms but there are relatively a few studies on long duration intensity formulas the difference of rainstorm attenuation characteristics in different durations is also not considered in the formulation of rainstorm formula in china jia et al 2021 focused on analyzing the rainstorm attenuation law pointing out that the rainstorm intensity of 1 24h duration attenuates with the same attenuation index of 0 74 based on this a long term comprehensive rainstorm intensity formula can be obtained which can calculate the design rainstorm with arbitrary duration of 1 24h and any return period of 2 100 years uncertainty in data sources model structures and parameters uncertainty prevails in the extreme precipitation frequency analysis xu et al 2010 and we can distinguish roughly three uncertainty sources data acquisition model selection and parameter estimation the sources of uncertainty in data acquisition mainly include the process of gauge measurement and observed data sampling analyzing and evaluating these uncertainties is of great significance for reducing the negative impact on extreme precipitation frequency analysis the deviation of gauge measurements caused by external factors is inevitable and ubiquitous gauge measurements of precipitation are always subjected to negative biases due to the wind induced undercatch trace precipitation and wetting losses folland 1988 the biases can result in a loss of 90 of the precipitation in some extreme cases which undoubtedly exert negative impacts on the frequency analysis of extreme precipitation zhang et al 2020 ignoring the precipitation measurement error caused by external factors may lead to changes in the precipitation amount or trend law li et al 2018b ren et al 2003 ye et al 2004 and consequent affect the accuracy of frequency analysis in order to ascertain and quantify various error sources including the random error the wind induced error the evaporation error and the wetted error of precipitation measurement ren et al 2003 set up 30 rainfall gauged stations across the mainland china at different climatic zones and elevations and conducted comparative experiments lasting for 7 years for 29000 precipitation events their results showed that the range of rainfall measurement errors in china is in 4 34 15 28 with an average of 6 52 including the wind induced error of 3 17 and the wetting error of 3 35 ye et al 2004 also reported that wind induced gauge undercatch is the greatest error in most regions in china and wetting loss and trace amount are important in the low precipitation regions in northwest china their findings clearly suggested that annual precipitation in china is much higher than previously reported zhang et al 2020 confirmed that the trends of measured annual precipitation have been overestimated to varying degrees before removal of measurement biases in most regions with the exceptions of northwest china and northern tibet as precipitation measurement errors could be very big the impact of the bias correction of precipitation on the accuracy of extreme precipitation frequency analysis needs to be further studied in addition to the measurement error another source of uncertainty in data is the selection of sampling methods to obtain the extreme precipitation series although the am sampling method is widely used in china the series obtained by am is generally short in length and the sampling error is exacerbated due to the lack of sufficient representativeness resulting in great uncertainty pot sampling can expand the usage of rainstorm information however there is still subjectivity in the selection of threshold liu et al 2013 chow 1964 pointed that the relative difference between the design rainstorm calculated by different sampling methods can reach 10 when the return period is less than 5 years the return period of the design storm for urban drainage works is usually less than 2 years mei et al 2017 therefore the uncertainty caused by different sampling methods can exert a great impact on the design of municipal drainage engineering regarding reducing uncertainties regional frequency analysis is an effective way to reduce the sampling error in extreme precipitation frequency analysis and improve the estimation accuracy of the design rainstorm the most significant advantage of regional frequency analysis is that the information of adjacent stations can be effectively used to overcome the shortage of data series for a single station and could also solve the estimation problem of design rainstorm in areas without data the theory and method of regional frequency analysis are mature and widely used in practical engineering abroad robson and reed 1999 however in china single station frequency analysis is still commonly used to obtain the design storm ministry of water resources of the people s republic of china 2006 and the regional frequency analysis is mostly adopted in research li et al 2018c liang et al 2017b shao et al 2020 zhou et al 2017 du et al 2014 used monte carlo simulations to evaluate the uncertainty caused by at site and regional frequency analysis they found that the regional method has less uncertainty yang et al 2010 used cluster analysis to divide the pearl river basin into 6 homogenous regions and explored the temporal and spatial distribution of their design rainstorms they noticed that the design rainstorms under different return periods showed a gradual increase from the upper to the lower reaches at present the regional frequency analysis mostly focuses on the division of the homogenous regions and there are few studies on the improvement of the division method hu et al 2019 introduced the fuzzy c means method combined with the extended xie benn index for the division of homogenous regions they showed that the modified method can identify the optimal number of homogenous regions and divide them reasonably the probability distribution function is actually a model for the expression of the statistical law of data distribution which not only interpolates the designed values in the range of existing data series but also performs the epitaxial estimation for the rare rainstorms jin 1999 at present a suitable distribution is chosen mainly by comparing the goodness of fit between potential distributions even if there are only tiny differences between the candidates the probability with a better goodness of fit index is commonly considered to be the right choice li et al 2019a guo et al 2019 estimating parameters of distributions with more than 3 parameters especially when using the method of moments is uncertain since it involves estimation of higher order moments gu et al 2021 this is one of the reasons that the guidelines in china recommend the three parameter p e 3 as a theoretical distribution in engineering practice guo et al 2016 since the 1950s the weibull formula has been used to calculate empirical frequency of hydrological variable series in china hua 1987 which proved to be safe for engineering design and has been used until now however different theoretical distributions are suitable for different empirical frequency formulas hua 1984 and the uncertainty of choosing the empirical frequency formula should not be ignored in addition for non stationary models due to the complexity of the model structure and the natural variability of the driving factors it usually leads to great uncertainty which limits the application of non stationary models as for parameter estimation the advantages and disadvantages of the commonly used estimation methods in china as well as the potential uncertainties have been described in the previous section and will not be repeated here it should be noted that the empirical curve fitting method has been widely applied to determine the parameters of theoretical distributions for precipitation extremes in engineering applications in china till now jin 1990 zhu et al 2011 however the empirical curve fitting method assumes that the ratio between the skewness coefficient and the variation coefficient is almost constant guo et al 2016 although the empirical curve fitting method can flexibly integrate a lot of information and take the historical heavy rain data into account the design rainstorm obtained by this method is generally larger and the arbitrariness and uncertainty are inevitable guo et al 2016 spatial variability of extreme precipitation in china due to the large geographic and climatic variability in china the characteristics of extreme precipitation differ across regions guan et al 2017 li and hu 2019 studies on the spatial variability of extreme precipitation generally adopt am or pot methods to obtain extreme precipitation series from gridded data gauged or climate models the studies focus on the distribution or the change in intensity and frequency of extreme precipitation deng et al 2018 used pot 95th percentile to define extremes and found that the spatial variation of extreme precipitation intensity and frequency has obvious regularity and spatial differences showing a decreasing trend along the strip from northeast to southwest and an increasing trend on both sides of the strip li et al 2019c studied 539 stations in china to find that there has been an increasing trend in annual and daily extreme precipitation during 1960 2010 especially in the south east and north west part of the country they also noted that the trends were higher in stations located in urban areas other studies also support these findings li et al 2019d zhai et al 2005 and additionally observe exceptions in some southeast and southwest regions which have presented decreasing trends in extremes fu et al 2013 examined the spatial and temporal variability of the frequency of extreme precipitation events for 599 stations in china and revealed that extreme precipitation in northeast china north china and the yellow river basin tended to decrease while the extreme precipitation in the yangtze river basin southeast coast south china inner mongolia northwest china and qinghai tibet plateau tended to increase wu et al 2018 revealed that the frequency of extreme precipitation events has increased significantly and the occurrence time has become more concentrated in western china by using the g p distribution they found that the 20 year and 50 year return periods of extreme precipitation events in the southwest and southeast of the lancang river have significantly increased from the perspective of dry and wet regions han et al 2019 used the gridded daily precipitation data set constructed based on 2 472 stations across china and defined extreme precipitation with 95th percentile based threshold the increasing trends in extreme precipitation frequency and relative intensity are detected in most parts of the dry regions especially in northwestern china and inner mongolia in wet regions increasing trend prevails in northeastern china and southern china they also speculated that the risk of flooding may raise in both dry and wet regions particularly in wet regions although the data and extreme precipitation indices adopted by these studies are different the conclusions on the spatial variability of the extreme precipitation are relatively consistent the increasing trends of extreme precipitation frequency and intensity were detected in south china northwest china inner mongolia etc human induced greenhouse gas emissions in china also had a detectable effect on the shift from light to heavy precipitation ma et al 2017 the spatial variation characteristics of extreme precipitation in future have also attracted the attention of numerous scholars and global climate models are effective tools to study the future change of extreme precipitation li et al 2012 bao et al 2015 wen et al 2016 xu et al 2019 guo et al 2013 xu et al 2019 predicted the changes in extreme precipitation in nine major river basins in china in the future by applying the outputs of 18 global climate models gcms with 3 representative concentration pathways rcps from the cmip5 the results manifested that the predicted changes have clear spatial differences the magnitude and intensity of extreme precipitation in high latitudes and high altitudes regions have a clear increasing trend and there is a great increasing trend in the frequency of extreme precipitation in southeast and southern china wen et al 2016 investigated the possible changes throughout china in the future through bias correction and spatial disaggregation of downscaled cmip5 projections they found that the growth of extreme precipitation intensity is unequivocal with an increasing rate from 7 9 to 15 5 in cmip5 projections in general the most evident increase was detected with a rate over 35 as mentioned in section 8 each link of extreme precipitation frequency analysis is inevitably fraught with uncertainties however obvious uncertainties exist in future projections of extreme climate indices due to model structure emission scenario and natural variability li et al 2012 woldemeskel et al 2016 the gcms uncertainty is larger in regions receiving heavy rainfall as well as mountainous and coastal areas woldemeskel et al 2016 if gcms are applied to study extreme precipitation inappropriately the uncertainty may be magnified to a great extent and added to the uncertainties included in the process of frequency analysis applicability assessment and uncertainty quantification will be the major research directions for the introduction of climate models into the extreme precipitation frequency analysis synopsis and synthesis the review focused on the probabilistic methods used to assess extreme precipitation in china the main findings of this review as well as the future prospects and directions are the following research involving frequency analysis of extreme precipitation is almost exclusively based on the am or pot sampling methods although there are a lot of studies on the threshold selection of the pot method in china there is no unified threshold selection method and subjectivity exists the commonly used distributions for the representation of extreme precipitation in china are mainly the three parameter ones such as the g e v g p and p e 3 the g e v distribution is more suitable for fitting am series while the optimal probability distribution of the pot series is g p which is consistent with the distribution line recommended by wmo p e 3 is recommended by the standard guidelines in china for frequency analysis and is still widely used in practical engineering applications especially for short duration rainstorms 1 2 h for longer durations the p e 3 may not be ideal and the g e v may be a better fit e g for annual summer and winter daily maximum precipitation at present there is no unified conclusion about which sampling method or distribution should be used for rainstorm design although g p was shown to be a better fit than g e v in most comparison studies e g beijing chongqing huai river basin l moments are widely used for the parameter estimation of extreme precipitation in the research the conventional moments and maximum likelihood estimation have shortcomings which limit their application in research and engineering practice the pickands bootstrap moment estimation method seems promising zhao and zhai 2015 the gauss newton method is recommended by guidelines for the numerical solution of the p e 3 distribution parameters despite its shortcomings sensitivity to the initial value slow convergence speed recently artificial intelligence algorithms have also been proposed for the parameter estimation of the p e 3 the probable maximum precipitation pmp adapted for china is the verification standard for flood control design of water conservancy and hydropower projects in the country the methods used for the estimation of pmp in china mainly include the statistical estimation method rainstorm maximization method rainstorm transposition method and generalized estimation method the generalized estimation method which is a combination of various methods is mostly used for the calculation of the pmp in large areas and the compilation of national pmp contours the hydrometeorological method represented by the rainstorm maximization method and rainstorm transposition method is recommended for pmp calculation in recent years this method has a clear physical mechanism and the calculated pmp is more in line with reality the rainstorm transposition method combined with water vapor maximization and convergence rain azimuth adjustment is still a feasible way to estimate pmp for the present state and the foreseeable future when using the statistical estimation method hershfield s km value method in china the deviation coefficient is generally used instead of the frequency factor km in the original formula statistical estimation can be used as an auxiliary method to quickly and preliminarily estimate the pmp of a single station or small watershed this method has been used in the compilation of pmp maps in hubei shaanxi and other provinces in the guideline documents of china pmp is linked to the rainstorm of 10000 year return period in frequency calculations but the exact return period of pmp corresponding to the design rainstorm is still unclear some scholars believe that the ratio of pmp to 10000 year design rainstorm is between 1 1 and 1 2 in most areas of china other methods for estimating pmp such as storm model the numerical simulation method multifractal method etc are rarely used in the research and application in china these methods may provide feasible ideas for pmp estimation and pmp response to climate change in the future the calculation results of pmp in china are generally carried out for a specific project and most of them were completed in the 1970s so there may be great uncertainties and subjective experience in the calculation process it is necessary to learn from the practices of some foreign countries carry out national watershed pmp estimation including pmf and compile the distribution map of national pmp estimation results non stationary methods are becoming more prevalent due to the effects of climate change or human activities compared to foreign countries the non stationary frequency analysis in china started relatively late and became the focus of extreme precipitation studies only during recent years the early research related to non stationary in china mainly focused on the analysis of temporal and spatial characteristics of extreme precipitation trend changes many studies showed that across china extreme precipitation shows a changing trend and presents regional patterns some studies pointed out that only a few regions in china exhibit a non stationary state of extreme precipitation the climate in most regions of china is significantly affected by monsoons so enso is the most widely studied climate factor being the most significant covariate affecting extreme precipitation in most regions of china for local scale studies significant covariate factors can be easily identified but on a national scale climate factors will influence and regulate each other therefore some studies are not limited to individual climate factors but carry out principal component analysis of climate factors most studies choose indicators based on statistical concepts such as the am and pot series as extreme precipitation indicators related to climate factors the am series is used to construct non stationary probability distribution models in most studies while a few take the occurrence of extreme precipitation as the research object in china the construction of the non stationary models is mainly based on gamlss and g e v distributions and the location and scale parameters change with covariates most of the covariate factors are time and climate factors and urbanization human activity is also used as a covariate in a very small number of studies generally the aic bayesian information criterion bic etc are selected to evaluate the non stationary and stationary model the aic is most commonly used for the optimization of the covariate factors and non stationary models in china almost all evaluation indices agree that the non stationary model with time as covariate better describes extreme precipitation in china compared to the stationary one and the non stationary model with climate factors performs even better moreover the non stationary model with climate factors as covariates can better describe the fluctuation of precipitation extremes although the non stationary model can capture the variation characteristics of precipitation extremes well it also introduces a lot of uncertainty the design rainstorm of the non stationary model is an interval value not a fixed one with the increase of the return period the range of the design value increases and so does the uncertainty some studies have pointed out that under non stationary conditions the return period of the design rainstorm of the same magnitude presents a decreasing trend indicating that the risk of rainstorm occurrence is increasing at present in the guidelines on rainstorm design in china ministry of water resources of the people s republic of china 2006 the stationary model is still recommended in future project construction for the sake of safety it is necessary to consider the influence of non stationarity when applying copula function to modeling the marginal distribution of a single extreme precipitation variable and the dependent structure between precipitation extremes may change due to climate change and human activities therefore it is necessary to carry out research on the modeling method and application of copula function under non stationary condition time varying copula model in china future research on non stationary extreme precipitation should be further strengthened in the following aspects 1 a comparative study on the non stationary variation law of precipitation extremes in similar climatic regions to enhance the credibility of the results a longer series of hydrological data and appropriate regional analysis methods should be used to study the impact of climate change on hydrological extremes 2 research on covariate selection of the non stationary model whether the best climate covariates have regional characteristics needs to be further analyzed in addition to climate factors the way of quantifying the impact of human activities on extreme precipitation needs to be further explored 3 research on the uncertainty of non stationary model results the means for uncertainty evaluation of the return period and the risk of extreme variables under non stationary conditions needs to be studied in practical applications which model stationary or non stationary should be used as the final basis for rainstorm design is worth further discussion 4 research on parameter estimation methods of the non stationary model the existing parameter estimation methods such as weight function method and l moments should be improved to make them suitable for parameter estimation under non stationary conditions and minimize the uncertainty in china the research and application of idf mainly focuses on the formulation of the rainstorm intensity formula to serve urban flood control and drainage and establish the rainstorm intensity formula for more than 600 cities the formula of rainstorm intensity is compiled by using the two step optimal method and the comprehensive formula the two step optimal method contains a complete set of extreme precipitation frequency analysis but it focuses on short durations generally less than 3h compared with the frequency analysis at basin regional and national scales the current design standard of drainage system in china is based on the condition of stationarity however under the dual impact of climate change and urbanization the mechanism and main characteristics of urban rainstorm have changed significantly and the risk of urban flood is incline to increase it is necessary to strengthen the research on the characteristics of short duration rainstorm in urban areas under a changing environment including occurrence and evolution mechanism temporal and spatial structure and change characteristics of rainstorm and compile time varying rainstorm intensity formulas or time varying idf curves uncertainty pervades every process of extreme precipitation frequency analysis due to industry habits there is a relatively fixed choice of the empirical frequency formula weibull formula in china in terms of parameter estimation of the theoretical frequency curve in order to take into account the historical heavy rain the empirical curve fitting method assumes there is a multiple relationship between the skewness coefficient and the variation coefficient which will also cause great uncertainty in terms of reducing uncertainty in extreme precipitation frequency analysis there are few achievements available for practical application of regional analysis methods and at present they are only in the research stage in china the uncertainty research on multivariable frequency analysis is still in its infancy so it is necessary to supplement and improve the uncertainty analysis theoretical system of extreme precipitation extreme hydrological events there are greater uncertainties for multivariable than univariate models based on the copula function multivariable models not only include the uncertainty of the marginal distribution but also the uncertainty of the dependence structure these two types of uncertainties involve data model structure parameter estimation etc so it is necessary to establish a comprehensive evaluation method that considers the three types of uncertainties as well as a multivariate model uncertainty analysis framework studies on spatial variability in china mainly focus on exploring the changes and spatial distribution of frequency and intensity of extreme precipitation based on gridded data gauged data or climate models at the national scale even though different data sources and definition methods of extreme precipitation are adopted different studies show relatively consistent laws increasing trends in extreme precipitation frequency and relative intensity were detected in south china northwest china inner mongolia etc in addition to using historical data a number of studies introduced climate models to explore the future changes of extreme precipitation however the introduction of climate models will undoubtedly increase the uncertainty applicability assessment and uncertainty quantification will be the major research content on the introduction of climate models into the extreme precipitation frequency analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national natural science foundation of china nos 51925902 52179006 the fundamental research funds for the central universities no dut20rc 3 019 the fund of innovation research team from the department of science and technology in liaoning province no xlyc1908023 and the fund of innovation research team in dalian university of technology no dut19td31 
