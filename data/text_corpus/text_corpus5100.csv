index,text
25500,indicators of ecological condition status of water bodies associated with field measurements are often subject to data gaps this obstacle can often lead to abandonment of assessment furthermore it can lead to the use of methods based merely on their availability in response to these challenges a systematic approach for expert analyst interaction for missing data treatment is proposed a combination of algorithms with hierarchical clustering of results is used a particular emphasis is put on the preparation and interpretation of input data and the role of an expert in the workflow the proposed approach enhances the decision making process by improving communication and transparency throughout interactions between experts analysts and decision makers future research should focus on assessing the scale of the ecological data drift phenomenon which based on the observed climate change anthropological pressure and biodiversity loss may impact the broad concept of indicator construction for lake water ecological assessment graphical abstract image 1 keywords ecological assessment decision support system missing data lakes water quality data availability the data are available as supplementary materials 1 introduction 1 1 data quality issues in ecological assessment since the publication of the water framework directive in 2000 in the european union eu management of water resources has become a priority aiming to meet environmental objectives of water bodies di quarto and zinzani 2021 kallis and butler 2001 in this context pro ecosystem approaches require the use of methods that are based on a holistic understanding of dependencies in evaluation procedures potentially leading to 1 the emergence of innovative and genuine ecological approaches to water management practices gain et al 2021 giupponi 2007 poikane et al 2015 reis et al 2017 and also to 2 a rapid growth of methodologies data and indicators produced in eu member states birk et al 2013 booty et al 2001 carey et al 2021 kelly et al 2016 zambelli et al 2012 the variety of approaches to assessing the ecological potential of water bodies is inextricably linked with issues of production modeling and processing of observation and measurement data birk et al 2012 paruch et al 2017 posthuma et al 2020 at each stage of the creation of environmental indicators problems can arise related to the quality and availability of input values brito et al 2020 gobeyn et al 2016 lindholm et al 2007 matthies et al 2007 paruch et al 2017 a key task and at the same time challenge are the intercalibration procedures that allow to obtain common reference levels for the classification of the ecological state of lakes importantly any unification of indicators requires a clear recognition of input data and the development of coherent methods for managing incomplete information gobeyn et al 2016 lahtinen et al 2017 this can help to avoid undesirable consequences associated with ignoring unknowns 1 2 implications of missing information the effects of a lack of data in the process of assessing ecological conditions of aquatic ecosystems can be seen at every level of data processing including the ex post evaluation of indicators yang et al 2021 zhang et al 2019 identification of the type of missing information is a critical element in the initial phase of dealing with measurement data little 2021 the quantity nature and severity of data flaws have a direct impact on the methods that can be used to work with specific datasets in the case of measurement sets used to assess the ecological condition of lakes deficiencies in observations often result from a type of defect referred to as missing at random mar seaman et al 2013 in this context there is a need to rely on substantive acceptability as mar is an assumption which is impossible to prove statistically little 2021 due to contingent emptiness in datasets parameter bias can result in analyses schielzeth et al 2020 how to best solve this problem depends on the assumptions made as well as on the knowledge of the context koehler et al 2017 the most common consequences of mishandling gaps in data sets include information loss bias in statistical inference or modeling and results misinterpretation hossie et al 2021 noble and nakagawa 2021 another problem connected with an incomplete input dataset includes an inability to use certain data analysis methods algorithms e g pca svm neural networks ghannam and techtmann 2021 a consequence of these issues is that certain popular methods tend to be mostly used such as partial deletion interpolation or imputation curley et al 2019 johnson et al 2021 missing knowledge management requires informed decisions to be taken along the data analysis path likmeta et al 2021 newman 2014 wang and xue 2020 1 3 data imputation ecological assessment perspective the assessment of the condition of aquatic ecosystems is connected with the identification of activities aimed at maintaining or improving the status of them as required under article 11 of the water framework directive in practice this is associated with a planning process that takes place in a 6 year cycle responsible for these planning processes are water management boards together with the departments of boards of individual water sub regions usually within river basins water administrations are working together on identifying anthropogenic pressures updating environmental objectives and protected water areas restoring water bodies and setting boundary values for heavily modified and artificial water bodies an important stage is the preparation of strategic environmental assessment sea mustow 2021 at this key moment assessors have the opportunity to influence the shape of the analyzes and the interpretation of results furthermore they can apply for supplementing or correcting methodologies comments are directed to the authors of plans at the stage of public consultations among other measures indicators of the ecological status of lake ecosystems are used to obtain results that support the definition of management practices the evaluation of the structure and efficiency of surface water ecosystems is known as establishment ecological status this demonstrates how stresses such as pollution and habitat deterioration have an impact on specified quality components each surface water body has an ecological status that is assessed based on biological quality components and supported by physico chemical and hydromorphological quality elements according to the one out all out approach the element with the worst status out of all biological and supporting quality factors determines the overall ecological status rating for a water body data used to evaluate the ecological status of lakes are sets largely based on the results of field measurements observations are prone to errors that can occur at the stage of collecting samples yanai et al 2021 there is always uncertainty over results even if using different tools ejigu 2021 loss of data or a complete lack of them may result in abandoning the assessment which in some cases significantly reduces the pool of evaluated ecosystems this often leads to gaps in data sets that weaken results of individual measurement campaigns moreover the same input data serve as components necessary to construct different environmental indicators placing additional emphasis on the validity of an imputation attempt in research on the ecological quality of ecosystems various methods of supplementing missing values are used muharemi et al 2019 said et al 2019 zhang and thorburn 2022 the so called hot deck imputation is used for handling missing data on large scale water quality indices ahmed et al 2021 srebotnjak et al 2012 most extensively used are methods based on multiple imputation these are available for most data types ben aissia et al 2017 betrie et al 2016 neri et al 2018 ngouna et al 2020 when faced with a high level of missingness data machine learning techniques are adopted these are able to troubleshoot complex data issues irvin et al 2021 kim et al 2020 ngouna et al 2020 ratolojanahary et al 2019 rodríguez et al 2021 furthermore the spatial nature of the issue results in the introduction of time and space variables koki et al 2020 labuzzetta et al 2021 liu et al 2016 lou and obradovic 2011 sojka et al 2020 yüksel 2012 zhang and thorburn 2021 research with ecological water quality indicators uses methods based on a case study approach this confirms their effectiveness at the local scale bilgin and bayraktar 2021 liu et al 2011 ren et al 2008 sojka et al 2019 weerasinghe and handapangoda 2019 there is a noticeable trend in the research indicating the need to develop methods that work well at the regional level providing the option of later intercalibration of results akbar et al 2011 botha et al 2020 hu et al 2018 jiang et al 2017 lepš and šmilauer 2006 li et al 2021 luo et al 2019 holistic approaches facilitate macro quality management of water resources which is important in the context of policy design and pan regional impact assessment moreover monitoring of ecological indicators and the impact of climate change on phenomena that threaten the stability of ecosystems has lately been explored cheruvelil et al 2017 fazli et al 2018 hutjes 2019 krzeminski et al 2019 lizotte et al 2014 mankin et al 1999 mustajoki et al 2004 peters lidard et al 2021 1 4 research goals the main goal of the research underpinning this paper was to present a workflow that can be used when an expert group or an ecological assessor are faced with the problem of missing values in an input dataset in this context a novel combined expert and analyst approach to ecological assessment is introduced with the exemplary analysis performed on real data this approach gives experts the opportunity to influence and adjust processes by making decisions in key nodes a further goal is the identification of possible techniques of data visualization both with regards to raw data and analysis 2 materials and methods 2 1 data software previous research the input data come from the resources of the chief inspectorate of environmental protection in poland appendix a gioś 2015 they are derived from the data used to develop indicators of ecological conditions of lake ecosystems the same data are also reported to the european commission data repositories europe environment agency 2018 see fig 1 the assessment concern a set of 499 lakes for which measurements were made during the 2013 2015 measurement campaign chlorophyll a nitrogen phosphorus phytoplankton ecological state macrophyte index esmi diatom index for lakes ioj phytoplankton method for polish lakes pmpl visibility and conductivity are some of the measures used to determine ecological status originally missing data were not included in the calculations instead they influenced the appropriate value of the uncertainty of the result in the tables attached to the report some basic information on the data is provided in appendix a data produced by measurement are the starting point for the methodology used to improve the procedure for determining ecological status machine learning algorithms are at the heart of this methodology chrobak et al 2021b which is then used to support the prioritization of lakes with the aim to organize remedial measures if ecosystems do not achieve specific environmental goals chrobak et al 2021a in this context a problem for working with data is missing information and in this paper consequences of missing observations are discussed and missing data imputation is performed and tested 2 2 imputation and clustering techniques in order to select an optimal technique for imputation of missing observations the missingness type of a dataset needs to be identified zhou 2020 systematic tendencies were discovered that allow for missing observations to be predicted with the help of other information see section 3 2 predictor examination this is due to correlations between variables and knowledge of with regards to errors in measurements or deficiencies not being the result of a deliberate procedure as a consequence the missingness type was labeled as missing at random mar bhaskaran and smeeth 2014 the following procedure of iterative imputation of missing values was preceded by stages that involved 1 applying the pearson product moment correlation method to analyze the degree and direction of data association russo 2021 and 2 a principal component analysis pca performed on the dataset with missing values to investigate uncertainty related absence of information husson et al 2014 missing value imputation was done using methods of multiple imputation by chained equations mice for multivariate dataset cases zhang 2016 with the goal to replace missing values with plausible data this led to an estimation of a more realistic layout dataset affected only minimally by incomplete observations within the procedure the following steps were performed on the input dataset raghunathan et al 2001 step 1 for every missing value in the dataset random extraction was performed from non missing data to provide initial basic imputation d step 2 the field with the least missing values ratio f was selected and transformed back to feature missing values step 3 the f was regressed as a dependent variable onto the initially imputed dataset as f d step 4 predicted values were obtained from a regression model used to fill missing data in f both the non missing and imputed values were used once f acts as an independent variable in regression modeling for the following dependent variables step 5 steps 2 4 were repeated for each variable with missing data being identified one iteration is understood as an operation of cycling through each of the variables the cycle was finished once all missing values were replaced with regression predictions that match the data relationships observed in the initial dataset the mice model parameters include a dataset matrix 8 499 with missing values b data imputation method random forest imputation shah et al 2014 c visit sequence roman left to right about 10 iteration cycles were performed in most research tasks gelman et al 2011 at the conclusion of the iterative cycles the distribution of the imputation parameters for instance the regression model coefficients was found to be converged and become stable in order to eliminate the undesired dependency on the sequence in which variables are imputed the authors performed 50 iterations until reaching convergence fig 2 the algorithm performance resulted in 30 imputed datasets these were subject to a distribution based clustering process when conducting the methodology for each of the variables with missing values and as a result of the data imputation method 30 versions of the possible information supplementation were obtained the hierarchical clustering technique was used to select the imputation sets that correspond to the formation of the original variable wu et al 2009 initially each dataset was treated as a separate cluster in the agglomerative version of the algorithm subsequently similar clusters were merged to form larger units based on predefined rules when only one cluster emerged the algorithm concluded that no further agglomeration was possible murtagh and legendre 2014 the clustering procedure included the following steps hartigan and wong 1979 step 1 the distance matrix was computed between columns of versions of imputed columns the original field is a feature in proximity calculation as well with missing values allowed but excluded from analysis resulting in a cross distance matrix step 2 the cross distance matrix was used as a dissimilarity structure for an agglomeration method to perform proximity based merging every column was considered as an individual cluster step 3 the clusters with similar characteristics proximity were merged step 4 the cross distance matrix was recalculated for each cluster step 5 steps 3 4 were repeated until a single cluster remained in the construction of the cross distance matrix for each of the dataset fields the form of squared euclidean distance matrix was used sarstedt and mooi 2014 the ward s method based on the optimal value of an in this case objective function the minimum variance was used as a criterion for choosing a pair of clusters to merge at each step ward 1963 the overall within cluster variance was reduced using ward s minimal variance criterion kruskal and black 2012 d 1 2 2 k l k l k l where d 1 2 dissimilarity between cluster 1 and cluster 2 k l observations from cluster 1 and cluster 2 k l centroids for clusters 1 and 2 euclidean norm for using this approach the pair of clusters was selected that after merging resulted in the least amount of total within cluster variance a weighted squared distance between cluster centers was used to calculate this increase all clusters were singletons in the first stage clusters containing a single point the initial distance between individual objects was proportional to the squared euclidean distance in order to execute a recursive algorithm under the objective function everitt 1980 as d i j v d x v i x v j 2 where d i j distance between cells i and j x v i value of x variable at cell i d number of dataset dimensions every feasible cluster pair was examined at each phase and the two clusters whose merger resulted in the least amount of information loss were combined ward defines information loss in terms of an error sum of squares criterion ess ward 1963 e s s i 1 n x i 2 1 n i 1 n x i 2 where n number of observations x i the value of i tj observation and 0 being mean value of all the observations 2 3 proposed workflow within the block diagram of the suggested method the proposed data analysis processes for the efficient imputation of missing values are systematized fig 3 the workflow is created to supplement the methodology described in t previous works on optimizing the assessment of the ecological state of lake ecosystems chrobak et al 2021a 2021b this enables the evaluation of solutions to be tailored to the water framework directive which establishes a need to conduct assessments involving expert knowledge from a technical point of view the approach addresses cases where the analysis cannot be performed effectively due to a significant number of missing observations thus the decision whether to continue the analysis with use of data imputation is made by the expert who is guided by experience and aided with dataset recognition by skilled analysts the aim is to obtain reliable premises for the implementation legitimacy of subsequent steps of the ecological status determination process in the diagram of the analytical process shown below fig 3 the dataset objects lakes with measurements appear as rectangles with blue borders purple outlined hexagonal blocks denote lakes that could become new objects based on an analytical or computational process in some places these blocks are linked to orange colored square blocks for these an expert decision is advised given the number or severity of missing observations the expert may decide to end the process if the process is not stopped during the data triage stage section 3 1 the dataset is subjected to a multivariate imputation the results of which are clustered the sets of imputations proposed by the algorithm are reviewed again by experts finally the selected dataset with no missing values is submitted for further analyses serving as an input for the supervised classifier of the lake ecological state class the operation of such a classifier was described in earlier work chrobak et al 2021b the main purpose of arranging the steps taken into a procedural form is to systematize the methodology so that it is reproducible each of the process blocks enclosed by a purple frame symbolizes the action on the data the squares with an orange frame indicate the moment of the decision made by the analyst expert each of the steps of the analysis is discussed along with an example of implementation in the following subsections of this article 3 results graphic representation of complicated processes is crucial for effective cooperation in an expert team for this purpose a featured data treatment scheme takes the work of experts into account dealing with various assessment objects with different degrees of data completeness certain cross roads are highlighted where a decision is necessary made by a specialist or requiring consultation before proceeding with the analysis see point 2 3 proposed workflow above the data treatment framework guides the user through the steps of pre selecting data explained below in 3 1 missing data identification and triage identifying and selecting imputation predictors see 3 2 predictor examination the actual multiple data imputation process using the random forest algorithm see 3 3 missing data imputation and then introduces the step of clustering similar complementary sets based on their characteristics in the context of the ward criterion via hierarchical clustering see 3 4 clustering imputation and 3 5 data imputation results 3 1 missing data identification and triage the input data are characterized by different missing measurements the identification of shortcomings starts with the preparation of a chart showing the scale of the problem the visual representation of missing values across the dataset indicated deficiencies in five out of eight variables involved in the construction of the lake evaluation index in addition the number of objects 39 that have information gaps for more than one field is also indicated fig 4 the analysis did not reveal any cases where the object has gaps for each of the variables the fields to note are ioj and esmi together accounting for 80 of existing na statements which is a prerequisite for taking corrective action on the data according to the adopted classification missing grade deficiencies are identified in 5 out of 8 variables used in the process of assessing the ecological condition of lakes khorshidi et al 2020 in the analysis underlying this paper the spread of na s percentage ranged from 0 2 for the conductivity variable to 15 for the ioj parameter it is worth noting that the fields containing the measurement results for esmi and ioj together accounted for the existence of approx 80 of the deficiencies these deficiencies are characterized in the adopted methodology of data triage as notbad missing 20 values where the deficiencies in the field of pmpl chlorophyll a and conductivity are labeled as good less than 5 missing despite the lack of fields within the bad category it is important to remember that 1 the categories are arbitrary intervals that are largely dependent on the decision of an expert who knows the data and 2 it is possible that there are gaps in the intersection data that when accumulated at the intersections will give a picture of real losses in the set of measurements quality ioj and esmi parameters are components that strongly affect the results of ecological status classification as indicated by the pca analysis by chrobak et al 2021a leaving these fields out of the analysis may cause the final result to be skewed 3 2 predictor examination one of the data preprocessing steps which is crucial for later decisions made during data imputation is the exploratory analysis of predictors braun and oswald 2011 here variables are subjected to the analysis of mutual linear dependencies which allows for an assumption of the situation earlier referred to as mar in the context of missing observations in our analysis strong correlations 0 5 were identified e g for visibility pmpl or nitrogen chlorophyll pairs fig 5 variables that are strongly associated with each other are not preferred candidates for following multiple data imputation ellington et al 2015 in most situations the selected imputation method should omit these variables during the algorithm implementation alice 2015 for some instances it is also possible for algorithms to fail or produce unreliable overfitted results christie et al 1984 thus highly correlated variables are excluded from the imputation process for each of the fields with missing values a separate selection of predictors is performed on the basis of which the calculations are continued as a result in the case of the ioj variable in our analysis each of the possible predictors was qualified the weakest correlation concerned the relationship with nitrogen the strongest with phosphorus the following predictors were related to the esmi index phosphorus ioj and conductivity for the imputation of the field containing the pmpl measurement results the variables ioj conductivity and phosphorus were specified ioj and conductivity variables were used to supplement deficiencies in the chlorophyll field the ioj variable which is one of the imputation objects has no correlation identified in the data set this rules out using any of the variables due to concerns about multicollinearity induced bias we performed multiple imputation using all of the available predictors a pca plot shows the effect of ioj on data variability in the dataset fig 5 correlation analysis using the pearson product moment correlation coefficient method indicated the existence of a linear relationship between some sets of observations this information was used to select potential predictors of imputation of missing values the results visible on the vector pca indicated the importance of the ioj parameter affecting the diversity of the data set which affects the final ability of the variable to explain the differentiation in the shaping of the first coordinate variance in the reduced observation space furthermore the plot includes variable wise uncertainty due to the presence of empty observations husson et al 2008 the analysis demonstrated that variability across different possible imputation scenarios is limited implying that pca results may be perceived as plausible by a user benahmed and houichi 2018 it shows the need to monitor the impact of data imputation on the shaping of leading dimensions explanatory skills chrobak et al 2021b the basic sensitivity analysis was performed to show the impact of individual variables on shaping the ecological state indicator the variables used to construct the indicator are also presented within an initial descriptive form appendix b figs 1 7 with regards to possible approaches to dependency modeling three different regression models were used 1 linear regression model as a basic form of predictor behavior examination 2 principal component analysis regression as a link to research previously conducted by authors on ecological status prediction and 3 gaussian stochastic process with polynomial kernel realizing the assumption of predictors having multivariate random distribution in possible combinations of linear models for each of the models results are presented as normalized dependency graphs featuring used predictors with their relation to change ecological state indicators the results for the basic linear model indicate the pmpl and esmi variables as having the comparatively speaking greatest impact on the variability of the lake ecological status index appendix b fig 8 the assumption of significance of these variables is also relevant in the context of their inverse correlation at the level of 0 64 the results of the principal component analysis based model refer to the set of two main components that influenced the variability of the results created in the previous studies appendix b fig 9 the following were indicated as significant variables esmi conductivity and visibility within first leading dimension coupled with pmpl ioj and chlorophyll a in the second the gaussian process stochastically resolved dependencies point to pmpl and esmi the impact may however be altered by the indications of visibility and ioj fig 6 the participation of an expert in the early stages of the analysis is required during the control of filling gaps in variables of particular importance for the shaping of the result proposed imputation should not disrupt the identified relationship between the variables and the ecological assessment in the case of complex variables based on organism biomass analyzes such as esmi ioj and pmpl special care is recommended in imputation or possibly the expert may recommend that further data operations be discontinued the procedure may be terminated due to the discovery of relatively large deficiencies in the variables important for assessing ecologically specific lakes e g lobelia another reason could be the presence of particularly outliers in the measurements even when the variables are statistically less important for the outcome this must be investigated by an expert 3 3 missing data imputation missing data imputation concerned four variables ioj esmi pmpl chlorophyll a for which individual sets of predictors were selected in the previous stage of work described above the applied method of multiple imputation was the mice approach using the random forest algorithm xiao and bulut 2020 the method is effective when linear relationships exist between variables it does not require the use of hyperparameter calibration practices the distributions were assumed for each variable and imputation was performed according to the distribution characteristics obtained from the original non imputed dataset fig 7 the density plots for each of imputation dataset are red lines the density of original field is displayed as blue line the dataset desired to be the best imputation option is expected to be similar in context of data density distribution however different results for individual iterations of the rf algorithm do not give an unambiguous fit of the optimal solution the results also indicate the necessity of continuous monitoring of the model results in order to avoid the use of distributions the parameters of which e g kurtosis differ significantly from the expected fit it is not possible to know the true value of intercept term due to missing data in the source field thus introduction of a distribution assumptions was necessary the selection of the set of possible imputations was carried out for the ioj variable as a presentation of the functioning of the approach in practice according to the results of the pca analysis and the identification of predictors it is a variable that significantly influences the result of the final classification of the ecological state of lakes in the adopted methodology gaps in observations of 15 make it an indicator that has the potential to be the most difficult imputation compared with e g chlorophyll 5 the plot in fig 4 indicates the presence of imputation sets that may result in an optimal but not overfitted match radosavljevic and anderson 2014 3 4 clustering imputations according to the scheme of proceedings presented in the materials and methods section fig 3 the grouping of similar imputations was performed using the hierarchical clustering method cohen addad et al 2019 the aim of this part of the analysis was to use a tool that allows for fairly intuitive and quick interpretation of a given set of imputation sets bearing in mind the possibility of carrying out more imputation iterations in specific cases or if necessary indicating many supplementary series scenarios in order to minimize the cluster associated variance loss the ward s method was applied this allowed for the consideration of the combination of every possible cluster pair at each algorithm performance step it this case the information loss was defined in terms of an error sum of squares criterion ess each of the leaves of the resulting dendrogram refer to the series obtained in the multiple imputation process sets of similar observations according to ward s criterion were collected under the dendrogram branch fig 8 the height parameter of the combination displayed on the x axis indicated the similarity measure between two sets seven clusters within the data set were defined using the so called gap statistic method the total intra cluster variation for different cluster quantities were compared with their expected values under the null reference distribution of the dataset generated with use of monte carlo simulations during the sampling procedure tibshirani et al 2001 the original series of ioj which contains the missing observations marked as 31 was introduced to the analysis for which the distribution estimation was performed fig 8 the source set of observations was included within one cluster marked as 4 with the sets 1 3 20 and 24 during consecutive model runs seven separate clusters were distinguished the 4th cluster enclosed with blue frame contains the original ioj variable marked with the number 31 entered for the analysis high similarity in the context of distribution was recognized for imputation set no 1 the next options of field completion with similar distribution are found in sets 3 20 and 24 the sets from the fourth cluster in the given prioritization order constituted a pool of plausible solutions to the problem of missing values in the next steps this is considered a plausible and safe imputation option with regards to variance and distribution criteria in our analysis the distance obtained by the pair of objects 1 and 31 significantly differed from the other objects within cluster 4 despite the fact that this indicates the best match according to the adopted criteria it is advisable to perform a similarity test e g z statistic in order to recognize the differences between the objects cluster ben zvi 2004 3 5 data imputation results the results of data imputation for the ioj variable were presented in the form of sequences of corresponding series arranged according to lake id in the original dataset fig 9 dashed red lines indicate where data imputation has been performed for each of the options within the cluster no 4 the statistics of the shaping of the variable allow for safe imputation of data and the use of the set in subsequent analyzes on the way to obtain a reliable indicator of the ecological condition of lakes this allowed for the tracing of the imputation process within cluster no 4 as well as the final verification of the results using polynomial regression on each of the retrieved series treating the process aspect approach to data imputation is one of the most informative ways of presenting the process aspect approach to data imputation it proved to be highly informative to decision makers and water quality experts during the presentation of results and project group meetings the second way for visualizing the imputation process is to arrange lakes in order of catchment area allowing for simultaneous assessment of the degree of missing observations in spatial terms fig 10 the characteristics of each cluster can be distinguished during the reverse reasoning making it possible to determine entry requirements for next iterations of imputation algorithm when assessing dataset obtained in currently ongoing data collection campaign the method also makes it simple to partition the sets so that specialists working on specific catchment assessments can accurately evaluate the scope of the problem in their work area and compare it to the situation in other task groups furthermore the visualization enables for cross referencing of individual implementation outcomes across the cluster red dashed lines and tracing of the data imputation process to identify undesired outliers generated by the method used 4 discussion the analysis underlying this paper focused on how to deal with missing at random data curation and imputation in the process of assessing the ecological status of lake ecosystems it was based on a collection of data for 499 lakes in poland with missing values detected to various degrees a methodology was designed based on the authors knowledge and support in the field of expert evaluations allowing for the imputation of data gaps to be implemented an authentic dataset was used in the ecological status assessment with the goal of submitting the results to the respective european union bodies in relation to wfd obligations reyjol et al 2014 the methodology is complementary to previous works where the stage of incomplete information management is part of an extensive algorithm of the ecological assessment of lakes four ranked propositions of value imputation for the ioj index were selected which was characterized by a 15 share of incomplete values data imputation especially in the case of the identification of comparatively speaking large gaps in data sets e g 5 is always associated with the risk of introducing bias into the process this may negatively mis informatively affect the final results and their interpretation krueger 2017 as a result it s critical to intentionally employ various strategies for addressing flaws testing the susceptibility of values to outliers is a useful practice which is part of the input data recognition stage jackson and chen 2004 due to the emerging need to analyze lakes from a regional or sub basin perspective the future role of ecological status indicators should be taken into account mammides 2020 rivera rondón and catalan 2020 wu et al 2021 this is connected with going beyond locally understood and evaluated indicators baldera et al 2018 kraemer et al 2020 the management of gaps in large scale data requires the development of methods for analyzing the relationships between indicators and their components in the context of spatial and temporal relationships between the objects of assessment kolada et al 2014 rossaro et al 2012 werner et al 2016 this may ultimately lead to the observation of a phenomenon referred to as data drift which is defined as the difference in variation of the data used to construct an initial assessment framework and the observations feeding the assessment model in the next round of reporting brock and carpenter 2012 koehnken et al 2020 taking the changes in ecosystems and their internal relationships into account especially in the era of the identified impact of climate change effects new factors may affect the variability of the ecological state of lakes over time it is therefore critical to create a consistent procedure for detecting data drift to define drift percentage criteria and to configure pro active alerts so that the necessary action may be performed dong et al 2018 gupta et al 2020 shifts may manifest themselves in the data at the level of their covariate shift steering with data imputation should minimize the effect of completions on the distribution of the variable hilt et al 2017 martin et al 2020 the clustering approach introduced here supports the selection of plausible options and is an alternative solution to the pooling stage within a multiple imputation process the classification algorithm used is comparatively speaking easy to interpret cohen addad et al 2019 furthermore the user does not need to define the number of clusters a priori however during the process arbitrary decisions are made distance metric linkage criterion which means the expert will have to monitor the results in order to react quickly to noticeable errors related to e g the use of mixed data types karthikeyan et al 2020 zhang et al 2019 in addition the algorithm is sensitive to the increase in the number of dimensions in the data so an iterative analysis of successive variables requiring imputation is recommended contreras and murtagh 2015 the ward criterion used allows for the creation of clusters based on a minimal increase in cluster variance this makes the approach less susceptible to noise related to multiple imputation results mcinnes et al 2017 the main limitations of the proposed approach consist of two types first in terms of the algorithms used the method inherits some of their inherent limitations in the case of the applied data imputation using the mice method with the use of random forest function limitations result from the need to control the results of supplements the expert should control the process so as not to allow indiscriminate acceptance of results significantly deviating from the observed data this may affect the second element of the process which is hierarchical clustering this is sensitive to the presence of noise and outliers applying to both the original input data and the imputation results the second type of limitation is also related to noise however it concerns noise generated on the side of expert judgment the method does not allow for the complete elimination of cognitive errors resulting from the participation of expert decisions characterized by their own systematic noise or bias one of the indirect limitations of the whole assessment system which this methodology also inherits results from the dependence on measurement timing and hydrological background for subsequent analyzes as the analysts work within a given time window the measurement reports contain data that represent the ecological situation of a lake characterizing it in terms of a typical state in practice this means that the samples of the studied variables from extreme hydrological events e g droughts and floods are included in the reports for separate analyzes in the research dealing with extraordinary situations thus the relationship between extraordinary measures and normal periods is neglected undoubtedly periods of ecological stress can affect quality and values of measurements this can include a delayed ecosystem response to critical phenomena although striving for normality of results through their early averaging and sampling in arbitrarily selected typical periods has a mitigating effect on the variance of results the noise generated at the early stage of the assessment is not measured at present an important positive effect of the proposed imputation process is leading the data set to a smooth transition into subsequent evaluation steps where specialists often use tools that function only with non missing input due to the key nature of the input data management process the transparency aspect of the analytical procedures used is not without significance romañach et al 2014 zasada et al 2017 methods that include data visualizations as inseparable elements of data processing are beneficial and support the ability to explain actions taken especially at the level of expert decision makers interactions which are critical for the often overlooked data sense making stage of ecological assessment arciniegas et al 2013 5 conclusions the missing data treatment methodology presented in this paper is aimed at systematizing the value imputation stage in order to perform an efficient reproducible solution ready for implementation within existing lake ecological state assessment methods the analysis included eight variables there were gaps in the measurement data for five of them the number of missing items indicated the need to imputate data for four variables an approach was used based on random forest multiple imputation with predictors examination a hierarchical algorithm with a ward s variance minimization criterion was used to cluster plausible imputation solutions obtained in a previous step seven clusters of similar additions were identified cluster 4 contained the original data set as well as four completed sets that met the membership criteria the results were presented as a dendrogram in the case of the selection of clusters as well as with the help of ordered trajectories of the shaping of the variable for the set containing missing values in relation to the four possible supplementary series according to the adopted criteria the stage of missing data treatment was considered as an integral part of the process of assessing the ecological condition of lakes influencing the selection of modeling and classification methods in subsequent stages of analyzes related to the proper ecological assessment and prioritization of ecosystems in terms of the selection of remedial solutions the authors note the positive impact of methodological and visual communication on the experts analyst decision maker pipeline which should be carried out with the transparency of the process moallemi et al 2020 this can be facilitated for example with the use of available data visualization techniques the approach introduced in this paper concludes the three step approach to lake ecological assessment which now consists of 1 data preprocessing and missing values treatment 2 model based assessment and 3 lake prioritization for remedial purposes taking into account the holistic view of the research results the proposed solution is aimed at systematization of the process of supplementing gaps in data on measurements in contrast to the previous omission of this issue in the reports on the assessment of the ecological state of lakes some lakes were only assessed by experts while others used analytical approaches some of the assessments were carried over from previous measurement campaigns this resulted in a conflict of results in the event that the lake apparently did not achieve environmental objectives despite the implemented remedial measures this means that a certain kind of data result asymmetry occurred the proposed fragment of the methodology was therefore aimed at organizing the assessment process by 1 defining the role of an expert in the course of analyzes 2 introducing a consistent methodology of data pre processing 3 enabling the use of effective algorithms in the assessment which are sensitive to data deficiencies e g ksvm or pca and 4 enabling the preview of the entire assessment process so that it can be corrected or further improved in the future with reference to the results of the next campaign to assess the ecological status of waters future research should focus on assessing the scale of the phenomenon of ecological data drift which based on the observed climate change anthropological pressure and loss of biodiversity may have a significant impact on the broad concept indicator construction for lake water ecological assessment 6 software and data availability the research was conducted with use of software providing data visualization tableau 2021 1 1 https www tableau com data modelling r 4 0 5 via rstudio 1 4 1106 tiger daylily https www r project org https rstudio com and algorithm development draw io 15 9 1 https www diagrams net appendix c contains an r language script that converts all of the analysis procedures in this paper into an executable reproducible workflow the materials for this work are available from the hydrosource platform https www hydroshare org resource ebec024018be4c2ba04cbfa85bb14d8e in the repository titled lakeecomissingdata accessed as resource a r code for data preprocessing imputation and clustering as lakesmissingrcode r b xml file of featured workflow schema as lakemissingworkflow c csv file containing raw measurement results treated as input to this analysis d a set of results of the statistical analysis of the variables involved in the study declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105558 
25500,indicators of ecological condition status of water bodies associated with field measurements are often subject to data gaps this obstacle can often lead to abandonment of assessment furthermore it can lead to the use of methods based merely on their availability in response to these challenges a systematic approach for expert analyst interaction for missing data treatment is proposed a combination of algorithms with hierarchical clustering of results is used a particular emphasis is put on the preparation and interpretation of input data and the role of an expert in the workflow the proposed approach enhances the decision making process by improving communication and transparency throughout interactions between experts analysts and decision makers future research should focus on assessing the scale of the ecological data drift phenomenon which based on the observed climate change anthropological pressure and biodiversity loss may impact the broad concept of indicator construction for lake water ecological assessment graphical abstract image 1 keywords ecological assessment decision support system missing data lakes water quality data availability the data are available as supplementary materials 1 introduction 1 1 data quality issues in ecological assessment since the publication of the water framework directive in 2000 in the european union eu management of water resources has become a priority aiming to meet environmental objectives of water bodies di quarto and zinzani 2021 kallis and butler 2001 in this context pro ecosystem approaches require the use of methods that are based on a holistic understanding of dependencies in evaluation procedures potentially leading to 1 the emergence of innovative and genuine ecological approaches to water management practices gain et al 2021 giupponi 2007 poikane et al 2015 reis et al 2017 and also to 2 a rapid growth of methodologies data and indicators produced in eu member states birk et al 2013 booty et al 2001 carey et al 2021 kelly et al 2016 zambelli et al 2012 the variety of approaches to assessing the ecological potential of water bodies is inextricably linked with issues of production modeling and processing of observation and measurement data birk et al 2012 paruch et al 2017 posthuma et al 2020 at each stage of the creation of environmental indicators problems can arise related to the quality and availability of input values brito et al 2020 gobeyn et al 2016 lindholm et al 2007 matthies et al 2007 paruch et al 2017 a key task and at the same time challenge are the intercalibration procedures that allow to obtain common reference levels for the classification of the ecological state of lakes importantly any unification of indicators requires a clear recognition of input data and the development of coherent methods for managing incomplete information gobeyn et al 2016 lahtinen et al 2017 this can help to avoid undesirable consequences associated with ignoring unknowns 1 2 implications of missing information the effects of a lack of data in the process of assessing ecological conditions of aquatic ecosystems can be seen at every level of data processing including the ex post evaluation of indicators yang et al 2021 zhang et al 2019 identification of the type of missing information is a critical element in the initial phase of dealing with measurement data little 2021 the quantity nature and severity of data flaws have a direct impact on the methods that can be used to work with specific datasets in the case of measurement sets used to assess the ecological condition of lakes deficiencies in observations often result from a type of defect referred to as missing at random mar seaman et al 2013 in this context there is a need to rely on substantive acceptability as mar is an assumption which is impossible to prove statistically little 2021 due to contingent emptiness in datasets parameter bias can result in analyses schielzeth et al 2020 how to best solve this problem depends on the assumptions made as well as on the knowledge of the context koehler et al 2017 the most common consequences of mishandling gaps in data sets include information loss bias in statistical inference or modeling and results misinterpretation hossie et al 2021 noble and nakagawa 2021 another problem connected with an incomplete input dataset includes an inability to use certain data analysis methods algorithms e g pca svm neural networks ghannam and techtmann 2021 a consequence of these issues is that certain popular methods tend to be mostly used such as partial deletion interpolation or imputation curley et al 2019 johnson et al 2021 missing knowledge management requires informed decisions to be taken along the data analysis path likmeta et al 2021 newman 2014 wang and xue 2020 1 3 data imputation ecological assessment perspective the assessment of the condition of aquatic ecosystems is connected with the identification of activities aimed at maintaining or improving the status of them as required under article 11 of the water framework directive in practice this is associated with a planning process that takes place in a 6 year cycle responsible for these planning processes are water management boards together with the departments of boards of individual water sub regions usually within river basins water administrations are working together on identifying anthropogenic pressures updating environmental objectives and protected water areas restoring water bodies and setting boundary values for heavily modified and artificial water bodies an important stage is the preparation of strategic environmental assessment sea mustow 2021 at this key moment assessors have the opportunity to influence the shape of the analyzes and the interpretation of results furthermore they can apply for supplementing or correcting methodologies comments are directed to the authors of plans at the stage of public consultations among other measures indicators of the ecological status of lake ecosystems are used to obtain results that support the definition of management practices the evaluation of the structure and efficiency of surface water ecosystems is known as establishment ecological status this demonstrates how stresses such as pollution and habitat deterioration have an impact on specified quality components each surface water body has an ecological status that is assessed based on biological quality components and supported by physico chemical and hydromorphological quality elements according to the one out all out approach the element with the worst status out of all biological and supporting quality factors determines the overall ecological status rating for a water body data used to evaluate the ecological status of lakes are sets largely based on the results of field measurements observations are prone to errors that can occur at the stage of collecting samples yanai et al 2021 there is always uncertainty over results even if using different tools ejigu 2021 loss of data or a complete lack of them may result in abandoning the assessment which in some cases significantly reduces the pool of evaluated ecosystems this often leads to gaps in data sets that weaken results of individual measurement campaigns moreover the same input data serve as components necessary to construct different environmental indicators placing additional emphasis on the validity of an imputation attempt in research on the ecological quality of ecosystems various methods of supplementing missing values are used muharemi et al 2019 said et al 2019 zhang and thorburn 2022 the so called hot deck imputation is used for handling missing data on large scale water quality indices ahmed et al 2021 srebotnjak et al 2012 most extensively used are methods based on multiple imputation these are available for most data types ben aissia et al 2017 betrie et al 2016 neri et al 2018 ngouna et al 2020 when faced with a high level of missingness data machine learning techniques are adopted these are able to troubleshoot complex data issues irvin et al 2021 kim et al 2020 ngouna et al 2020 ratolojanahary et al 2019 rodríguez et al 2021 furthermore the spatial nature of the issue results in the introduction of time and space variables koki et al 2020 labuzzetta et al 2021 liu et al 2016 lou and obradovic 2011 sojka et al 2020 yüksel 2012 zhang and thorburn 2021 research with ecological water quality indicators uses methods based on a case study approach this confirms their effectiveness at the local scale bilgin and bayraktar 2021 liu et al 2011 ren et al 2008 sojka et al 2019 weerasinghe and handapangoda 2019 there is a noticeable trend in the research indicating the need to develop methods that work well at the regional level providing the option of later intercalibration of results akbar et al 2011 botha et al 2020 hu et al 2018 jiang et al 2017 lepš and šmilauer 2006 li et al 2021 luo et al 2019 holistic approaches facilitate macro quality management of water resources which is important in the context of policy design and pan regional impact assessment moreover monitoring of ecological indicators and the impact of climate change on phenomena that threaten the stability of ecosystems has lately been explored cheruvelil et al 2017 fazli et al 2018 hutjes 2019 krzeminski et al 2019 lizotte et al 2014 mankin et al 1999 mustajoki et al 2004 peters lidard et al 2021 1 4 research goals the main goal of the research underpinning this paper was to present a workflow that can be used when an expert group or an ecological assessor are faced with the problem of missing values in an input dataset in this context a novel combined expert and analyst approach to ecological assessment is introduced with the exemplary analysis performed on real data this approach gives experts the opportunity to influence and adjust processes by making decisions in key nodes a further goal is the identification of possible techniques of data visualization both with regards to raw data and analysis 2 materials and methods 2 1 data software previous research the input data come from the resources of the chief inspectorate of environmental protection in poland appendix a gioś 2015 they are derived from the data used to develop indicators of ecological conditions of lake ecosystems the same data are also reported to the european commission data repositories europe environment agency 2018 see fig 1 the assessment concern a set of 499 lakes for which measurements were made during the 2013 2015 measurement campaign chlorophyll a nitrogen phosphorus phytoplankton ecological state macrophyte index esmi diatom index for lakes ioj phytoplankton method for polish lakes pmpl visibility and conductivity are some of the measures used to determine ecological status originally missing data were not included in the calculations instead they influenced the appropriate value of the uncertainty of the result in the tables attached to the report some basic information on the data is provided in appendix a data produced by measurement are the starting point for the methodology used to improve the procedure for determining ecological status machine learning algorithms are at the heart of this methodology chrobak et al 2021b which is then used to support the prioritization of lakes with the aim to organize remedial measures if ecosystems do not achieve specific environmental goals chrobak et al 2021a in this context a problem for working with data is missing information and in this paper consequences of missing observations are discussed and missing data imputation is performed and tested 2 2 imputation and clustering techniques in order to select an optimal technique for imputation of missing observations the missingness type of a dataset needs to be identified zhou 2020 systematic tendencies were discovered that allow for missing observations to be predicted with the help of other information see section 3 2 predictor examination this is due to correlations between variables and knowledge of with regards to errors in measurements or deficiencies not being the result of a deliberate procedure as a consequence the missingness type was labeled as missing at random mar bhaskaran and smeeth 2014 the following procedure of iterative imputation of missing values was preceded by stages that involved 1 applying the pearson product moment correlation method to analyze the degree and direction of data association russo 2021 and 2 a principal component analysis pca performed on the dataset with missing values to investigate uncertainty related absence of information husson et al 2014 missing value imputation was done using methods of multiple imputation by chained equations mice for multivariate dataset cases zhang 2016 with the goal to replace missing values with plausible data this led to an estimation of a more realistic layout dataset affected only minimally by incomplete observations within the procedure the following steps were performed on the input dataset raghunathan et al 2001 step 1 for every missing value in the dataset random extraction was performed from non missing data to provide initial basic imputation d step 2 the field with the least missing values ratio f was selected and transformed back to feature missing values step 3 the f was regressed as a dependent variable onto the initially imputed dataset as f d step 4 predicted values were obtained from a regression model used to fill missing data in f both the non missing and imputed values were used once f acts as an independent variable in regression modeling for the following dependent variables step 5 steps 2 4 were repeated for each variable with missing data being identified one iteration is understood as an operation of cycling through each of the variables the cycle was finished once all missing values were replaced with regression predictions that match the data relationships observed in the initial dataset the mice model parameters include a dataset matrix 8 499 with missing values b data imputation method random forest imputation shah et al 2014 c visit sequence roman left to right about 10 iteration cycles were performed in most research tasks gelman et al 2011 at the conclusion of the iterative cycles the distribution of the imputation parameters for instance the regression model coefficients was found to be converged and become stable in order to eliminate the undesired dependency on the sequence in which variables are imputed the authors performed 50 iterations until reaching convergence fig 2 the algorithm performance resulted in 30 imputed datasets these were subject to a distribution based clustering process when conducting the methodology for each of the variables with missing values and as a result of the data imputation method 30 versions of the possible information supplementation were obtained the hierarchical clustering technique was used to select the imputation sets that correspond to the formation of the original variable wu et al 2009 initially each dataset was treated as a separate cluster in the agglomerative version of the algorithm subsequently similar clusters were merged to form larger units based on predefined rules when only one cluster emerged the algorithm concluded that no further agglomeration was possible murtagh and legendre 2014 the clustering procedure included the following steps hartigan and wong 1979 step 1 the distance matrix was computed between columns of versions of imputed columns the original field is a feature in proximity calculation as well with missing values allowed but excluded from analysis resulting in a cross distance matrix step 2 the cross distance matrix was used as a dissimilarity structure for an agglomeration method to perform proximity based merging every column was considered as an individual cluster step 3 the clusters with similar characteristics proximity were merged step 4 the cross distance matrix was recalculated for each cluster step 5 steps 3 4 were repeated until a single cluster remained in the construction of the cross distance matrix for each of the dataset fields the form of squared euclidean distance matrix was used sarstedt and mooi 2014 the ward s method based on the optimal value of an in this case objective function the minimum variance was used as a criterion for choosing a pair of clusters to merge at each step ward 1963 the overall within cluster variance was reduced using ward s minimal variance criterion kruskal and black 2012 d 1 2 2 k l k l k l where d 1 2 dissimilarity between cluster 1 and cluster 2 k l observations from cluster 1 and cluster 2 k l centroids for clusters 1 and 2 euclidean norm for using this approach the pair of clusters was selected that after merging resulted in the least amount of total within cluster variance a weighted squared distance between cluster centers was used to calculate this increase all clusters were singletons in the first stage clusters containing a single point the initial distance between individual objects was proportional to the squared euclidean distance in order to execute a recursive algorithm under the objective function everitt 1980 as d i j v d x v i x v j 2 where d i j distance between cells i and j x v i value of x variable at cell i d number of dataset dimensions every feasible cluster pair was examined at each phase and the two clusters whose merger resulted in the least amount of information loss were combined ward defines information loss in terms of an error sum of squares criterion ess ward 1963 e s s i 1 n x i 2 1 n i 1 n x i 2 where n number of observations x i the value of i tj observation and 0 being mean value of all the observations 2 3 proposed workflow within the block diagram of the suggested method the proposed data analysis processes for the efficient imputation of missing values are systematized fig 3 the workflow is created to supplement the methodology described in t previous works on optimizing the assessment of the ecological state of lake ecosystems chrobak et al 2021a 2021b this enables the evaluation of solutions to be tailored to the water framework directive which establishes a need to conduct assessments involving expert knowledge from a technical point of view the approach addresses cases where the analysis cannot be performed effectively due to a significant number of missing observations thus the decision whether to continue the analysis with use of data imputation is made by the expert who is guided by experience and aided with dataset recognition by skilled analysts the aim is to obtain reliable premises for the implementation legitimacy of subsequent steps of the ecological status determination process in the diagram of the analytical process shown below fig 3 the dataset objects lakes with measurements appear as rectangles with blue borders purple outlined hexagonal blocks denote lakes that could become new objects based on an analytical or computational process in some places these blocks are linked to orange colored square blocks for these an expert decision is advised given the number or severity of missing observations the expert may decide to end the process if the process is not stopped during the data triage stage section 3 1 the dataset is subjected to a multivariate imputation the results of which are clustered the sets of imputations proposed by the algorithm are reviewed again by experts finally the selected dataset with no missing values is submitted for further analyses serving as an input for the supervised classifier of the lake ecological state class the operation of such a classifier was described in earlier work chrobak et al 2021b the main purpose of arranging the steps taken into a procedural form is to systematize the methodology so that it is reproducible each of the process blocks enclosed by a purple frame symbolizes the action on the data the squares with an orange frame indicate the moment of the decision made by the analyst expert each of the steps of the analysis is discussed along with an example of implementation in the following subsections of this article 3 results graphic representation of complicated processes is crucial for effective cooperation in an expert team for this purpose a featured data treatment scheme takes the work of experts into account dealing with various assessment objects with different degrees of data completeness certain cross roads are highlighted where a decision is necessary made by a specialist or requiring consultation before proceeding with the analysis see point 2 3 proposed workflow above the data treatment framework guides the user through the steps of pre selecting data explained below in 3 1 missing data identification and triage identifying and selecting imputation predictors see 3 2 predictor examination the actual multiple data imputation process using the random forest algorithm see 3 3 missing data imputation and then introduces the step of clustering similar complementary sets based on their characteristics in the context of the ward criterion via hierarchical clustering see 3 4 clustering imputation and 3 5 data imputation results 3 1 missing data identification and triage the input data are characterized by different missing measurements the identification of shortcomings starts with the preparation of a chart showing the scale of the problem the visual representation of missing values across the dataset indicated deficiencies in five out of eight variables involved in the construction of the lake evaluation index in addition the number of objects 39 that have information gaps for more than one field is also indicated fig 4 the analysis did not reveal any cases where the object has gaps for each of the variables the fields to note are ioj and esmi together accounting for 80 of existing na statements which is a prerequisite for taking corrective action on the data according to the adopted classification missing grade deficiencies are identified in 5 out of 8 variables used in the process of assessing the ecological condition of lakes khorshidi et al 2020 in the analysis underlying this paper the spread of na s percentage ranged from 0 2 for the conductivity variable to 15 for the ioj parameter it is worth noting that the fields containing the measurement results for esmi and ioj together accounted for the existence of approx 80 of the deficiencies these deficiencies are characterized in the adopted methodology of data triage as notbad missing 20 values where the deficiencies in the field of pmpl chlorophyll a and conductivity are labeled as good less than 5 missing despite the lack of fields within the bad category it is important to remember that 1 the categories are arbitrary intervals that are largely dependent on the decision of an expert who knows the data and 2 it is possible that there are gaps in the intersection data that when accumulated at the intersections will give a picture of real losses in the set of measurements quality ioj and esmi parameters are components that strongly affect the results of ecological status classification as indicated by the pca analysis by chrobak et al 2021a leaving these fields out of the analysis may cause the final result to be skewed 3 2 predictor examination one of the data preprocessing steps which is crucial for later decisions made during data imputation is the exploratory analysis of predictors braun and oswald 2011 here variables are subjected to the analysis of mutual linear dependencies which allows for an assumption of the situation earlier referred to as mar in the context of missing observations in our analysis strong correlations 0 5 were identified e g for visibility pmpl or nitrogen chlorophyll pairs fig 5 variables that are strongly associated with each other are not preferred candidates for following multiple data imputation ellington et al 2015 in most situations the selected imputation method should omit these variables during the algorithm implementation alice 2015 for some instances it is also possible for algorithms to fail or produce unreliable overfitted results christie et al 1984 thus highly correlated variables are excluded from the imputation process for each of the fields with missing values a separate selection of predictors is performed on the basis of which the calculations are continued as a result in the case of the ioj variable in our analysis each of the possible predictors was qualified the weakest correlation concerned the relationship with nitrogen the strongest with phosphorus the following predictors were related to the esmi index phosphorus ioj and conductivity for the imputation of the field containing the pmpl measurement results the variables ioj conductivity and phosphorus were specified ioj and conductivity variables were used to supplement deficiencies in the chlorophyll field the ioj variable which is one of the imputation objects has no correlation identified in the data set this rules out using any of the variables due to concerns about multicollinearity induced bias we performed multiple imputation using all of the available predictors a pca plot shows the effect of ioj on data variability in the dataset fig 5 correlation analysis using the pearson product moment correlation coefficient method indicated the existence of a linear relationship between some sets of observations this information was used to select potential predictors of imputation of missing values the results visible on the vector pca indicated the importance of the ioj parameter affecting the diversity of the data set which affects the final ability of the variable to explain the differentiation in the shaping of the first coordinate variance in the reduced observation space furthermore the plot includes variable wise uncertainty due to the presence of empty observations husson et al 2008 the analysis demonstrated that variability across different possible imputation scenarios is limited implying that pca results may be perceived as plausible by a user benahmed and houichi 2018 it shows the need to monitor the impact of data imputation on the shaping of leading dimensions explanatory skills chrobak et al 2021b the basic sensitivity analysis was performed to show the impact of individual variables on shaping the ecological state indicator the variables used to construct the indicator are also presented within an initial descriptive form appendix b figs 1 7 with regards to possible approaches to dependency modeling three different regression models were used 1 linear regression model as a basic form of predictor behavior examination 2 principal component analysis regression as a link to research previously conducted by authors on ecological status prediction and 3 gaussian stochastic process with polynomial kernel realizing the assumption of predictors having multivariate random distribution in possible combinations of linear models for each of the models results are presented as normalized dependency graphs featuring used predictors with their relation to change ecological state indicators the results for the basic linear model indicate the pmpl and esmi variables as having the comparatively speaking greatest impact on the variability of the lake ecological status index appendix b fig 8 the assumption of significance of these variables is also relevant in the context of their inverse correlation at the level of 0 64 the results of the principal component analysis based model refer to the set of two main components that influenced the variability of the results created in the previous studies appendix b fig 9 the following were indicated as significant variables esmi conductivity and visibility within first leading dimension coupled with pmpl ioj and chlorophyll a in the second the gaussian process stochastically resolved dependencies point to pmpl and esmi the impact may however be altered by the indications of visibility and ioj fig 6 the participation of an expert in the early stages of the analysis is required during the control of filling gaps in variables of particular importance for the shaping of the result proposed imputation should not disrupt the identified relationship between the variables and the ecological assessment in the case of complex variables based on organism biomass analyzes such as esmi ioj and pmpl special care is recommended in imputation or possibly the expert may recommend that further data operations be discontinued the procedure may be terminated due to the discovery of relatively large deficiencies in the variables important for assessing ecologically specific lakes e g lobelia another reason could be the presence of particularly outliers in the measurements even when the variables are statistically less important for the outcome this must be investigated by an expert 3 3 missing data imputation missing data imputation concerned four variables ioj esmi pmpl chlorophyll a for which individual sets of predictors were selected in the previous stage of work described above the applied method of multiple imputation was the mice approach using the random forest algorithm xiao and bulut 2020 the method is effective when linear relationships exist between variables it does not require the use of hyperparameter calibration practices the distributions were assumed for each variable and imputation was performed according to the distribution characteristics obtained from the original non imputed dataset fig 7 the density plots for each of imputation dataset are red lines the density of original field is displayed as blue line the dataset desired to be the best imputation option is expected to be similar in context of data density distribution however different results for individual iterations of the rf algorithm do not give an unambiguous fit of the optimal solution the results also indicate the necessity of continuous monitoring of the model results in order to avoid the use of distributions the parameters of which e g kurtosis differ significantly from the expected fit it is not possible to know the true value of intercept term due to missing data in the source field thus introduction of a distribution assumptions was necessary the selection of the set of possible imputations was carried out for the ioj variable as a presentation of the functioning of the approach in practice according to the results of the pca analysis and the identification of predictors it is a variable that significantly influences the result of the final classification of the ecological state of lakes in the adopted methodology gaps in observations of 15 make it an indicator that has the potential to be the most difficult imputation compared with e g chlorophyll 5 the plot in fig 4 indicates the presence of imputation sets that may result in an optimal but not overfitted match radosavljevic and anderson 2014 3 4 clustering imputations according to the scheme of proceedings presented in the materials and methods section fig 3 the grouping of similar imputations was performed using the hierarchical clustering method cohen addad et al 2019 the aim of this part of the analysis was to use a tool that allows for fairly intuitive and quick interpretation of a given set of imputation sets bearing in mind the possibility of carrying out more imputation iterations in specific cases or if necessary indicating many supplementary series scenarios in order to minimize the cluster associated variance loss the ward s method was applied this allowed for the consideration of the combination of every possible cluster pair at each algorithm performance step it this case the information loss was defined in terms of an error sum of squares criterion ess each of the leaves of the resulting dendrogram refer to the series obtained in the multiple imputation process sets of similar observations according to ward s criterion were collected under the dendrogram branch fig 8 the height parameter of the combination displayed on the x axis indicated the similarity measure between two sets seven clusters within the data set were defined using the so called gap statistic method the total intra cluster variation for different cluster quantities were compared with their expected values under the null reference distribution of the dataset generated with use of monte carlo simulations during the sampling procedure tibshirani et al 2001 the original series of ioj which contains the missing observations marked as 31 was introduced to the analysis for which the distribution estimation was performed fig 8 the source set of observations was included within one cluster marked as 4 with the sets 1 3 20 and 24 during consecutive model runs seven separate clusters were distinguished the 4th cluster enclosed with blue frame contains the original ioj variable marked with the number 31 entered for the analysis high similarity in the context of distribution was recognized for imputation set no 1 the next options of field completion with similar distribution are found in sets 3 20 and 24 the sets from the fourth cluster in the given prioritization order constituted a pool of plausible solutions to the problem of missing values in the next steps this is considered a plausible and safe imputation option with regards to variance and distribution criteria in our analysis the distance obtained by the pair of objects 1 and 31 significantly differed from the other objects within cluster 4 despite the fact that this indicates the best match according to the adopted criteria it is advisable to perform a similarity test e g z statistic in order to recognize the differences between the objects cluster ben zvi 2004 3 5 data imputation results the results of data imputation for the ioj variable were presented in the form of sequences of corresponding series arranged according to lake id in the original dataset fig 9 dashed red lines indicate where data imputation has been performed for each of the options within the cluster no 4 the statistics of the shaping of the variable allow for safe imputation of data and the use of the set in subsequent analyzes on the way to obtain a reliable indicator of the ecological condition of lakes this allowed for the tracing of the imputation process within cluster no 4 as well as the final verification of the results using polynomial regression on each of the retrieved series treating the process aspect approach to data imputation is one of the most informative ways of presenting the process aspect approach to data imputation it proved to be highly informative to decision makers and water quality experts during the presentation of results and project group meetings the second way for visualizing the imputation process is to arrange lakes in order of catchment area allowing for simultaneous assessment of the degree of missing observations in spatial terms fig 10 the characteristics of each cluster can be distinguished during the reverse reasoning making it possible to determine entry requirements for next iterations of imputation algorithm when assessing dataset obtained in currently ongoing data collection campaign the method also makes it simple to partition the sets so that specialists working on specific catchment assessments can accurately evaluate the scope of the problem in their work area and compare it to the situation in other task groups furthermore the visualization enables for cross referencing of individual implementation outcomes across the cluster red dashed lines and tracing of the data imputation process to identify undesired outliers generated by the method used 4 discussion the analysis underlying this paper focused on how to deal with missing at random data curation and imputation in the process of assessing the ecological status of lake ecosystems it was based on a collection of data for 499 lakes in poland with missing values detected to various degrees a methodology was designed based on the authors knowledge and support in the field of expert evaluations allowing for the imputation of data gaps to be implemented an authentic dataset was used in the ecological status assessment with the goal of submitting the results to the respective european union bodies in relation to wfd obligations reyjol et al 2014 the methodology is complementary to previous works where the stage of incomplete information management is part of an extensive algorithm of the ecological assessment of lakes four ranked propositions of value imputation for the ioj index were selected which was characterized by a 15 share of incomplete values data imputation especially in the case of the identification of comparatively speaking large gaps in data sets e g 5 is always associated with the risk of introducing bias into the process this may negatively mis informatively affect the final results and their interpretation krueger 2017 as a result it s critical to intentionally employ various strategies for addressing flaws testing the susceptibility of values to outliers is a useful practice which is part of the input data recognition stage jackson and chen 2004 due to the emerging need to analyze lakes from a regional or sub basin perspective the future role of ecological status indicators should be taken into account mammides 2020 rivera rondón and catalan 2020 wu et al 2021 this is connected with going beyond locally understood and evaluated indicators baldera et al 2018 kraemer et al 2020 the management of gaps in large scale data requires the development of methods for analyzing the relationships between indicators and their components in the context of spatial and temporal relationships between the objects of assessment kolada et al 2014 rossaro et al 2012 werner et al 2016 this may ultimately lead to the observation of a phenomenon referred to as data drift which is defined as the difference in variation of the data used to construct an initial assessment framework and the observations feeding the assessment model in the next round of reporting brock and carpenter 2012 koehnken et al 2020 taking the changes in ecosystems and their internal relationships into account especially in the era of the identified impact of climate change effects new factors may affect the variability of the ecological state of lakes over time it is therefore critical to create a consistent procedure for detecting data drift to define drift percentage criteria and to configure pro active alerts so that the necessary action may be performed dong et al 2018 gupta et al 2020 shifts may manifest themselves in the data at the level of their covariate shift steering with data imputation should minimize the effect of completions on the distribution of the variable hilt et al 2017 martin et al 2020 the clustering approach introduced here supports the selection of plausible options and is an alternative solution to the pooling stage within a multiple imputation process the classification algorithm used is comparatively speaking easy to interpret cohen addad et al 2019 furthermore the user does not need to define the number of clusters a priori however during the process arbitrary decisions are made distance metric linkage criterion which means the expert will have to monitor the results in order to react quickly to noticeable errors related to e g the use of mixed data types karthikeyan et al 2020 zhang et al 2019 in addition the algorithm is sensitive to the increase in the number of dimensions in the data so an iterative analysis of successive variables requiring imputation is recommended contreras and murtagh 2015 the ward criterion used allows for the creation of clusters based on a minimal increase in cluster variance this makes the approach less susceptible to noise related to multiple imputation results mcinnes et al 2017 the main limitations of the proposed approach consist of two types first in terms of the algorithms used the method inherits some of their inherent limitations in the case of the applied data imputation using the mice method with the use of random forest function limitations result from the need to control the results of supplements the expert should control the process so as not to allow indiscriminate acceptance of results significantly deviating from the observed data this may affect the second element of the process which is hierarchical clustering this is sensitive to the presence of noise and outliers applying to both the original input data and the imputation results the second type of limitation is also related to noise however it concerns noise generated on the side of expert judgment the method does not allow for the complete elimination of cognitive errors resulting from the participation of expert decisions characterized by their own systematic noise or bias one of the indirect limitations of the whole assessment system which this methodology also inherits results from the dependence on measurement timing and hydrological background for subsequent analyzes as the analysts work within a given time window the measurement reports contain data that represent the ecological situation of a lake characterizing it in terms of a typical state in practice this means that the samples of the studied variables from extreme hydrological events e g droughts and floods are included in the reports for separate analyzes in the research dealing with extraordinary situations thus the relationship between extraordinary measures and normal periods is neglected undoubtedly periods of ecological stress can affect quality and values of measurements this can include a delayed ecosystem response to critical phenomena although striving for normality of results through their early averaging and sampling in arbitrarily selected typical periods has a mitigating effect on the variance of results the noise generated at the early stage of the assessment is not measured at present an important positive effect of the proposed imputation process is leading the data set to a smooth transition into subsequent evaluation steps where specialists often use tools that function only with non missing input due to the key nature of the input data management process the transparency aspect of the analytical procedures used is not without significance romañach et al 2014 zasada et al 2017 methods that include data visualizations as inseparable elements of data processing are beneficial and support the ability to explain actions taken especially at the level of expert decision makers interactions which are critical for the often overlooked data sense making stage of ecological assessment arciniegas et al 2013 5 conclusions the missing data treatment methodology presented in this paper is aimed at systematizing the value imputation stage in order to perform an efficient reproducible solution ready for implementation within existing lake ecological state assessment methods the analysis included eight variables there were gaps in the measurement data for five of them the number of missing items indicated the need to imputate data for four variables an approach was used based on random forest multiple imputation with predictors examination a hierarchical algorithm with a ward s variance minimization criterion was used to cluster plausible imputation solutions obtained in a previous step seven clusters of similar additions were identified cluster 4 contained the original data set as well as four completed sets that met the membership criteria the results were presented as a dendrogram in the case of the selection of clusters as well as with the help of ordered trajectories of the shaping of the variable for the set containing missing values in relation to the four possible supplementary series according to the adopted criteria the stage of missing data treatment was considered as an integral part of the process of assessing the ecological condition of lakes influencing the selection of modeling and classification methods in subsequent stages of analyzes related to the proper ecological assessment and prioritization of ecosystems in terms of the selection of remedial solutions the authors note the positive impact of methodological and visual communication on the experts analyst decision maker pipeline which should be carried out with the transparency of the process moallemi et al 2020 this can be facilitated for example with the use of available data visualization techniques the approach introduced in this paper concludes the three step approach to lake ecological assessment which now consists of 1 data preprocessing and missing values treatment 2 model based assessment and 3 lake prioritization for remedial purposes taking into account the holistic view of the research results the proposed solution is aimed at systematization of the process of supplementing gaps in data on measurements in contrast to the previous omission of this issue in the reports on the assessment of the ecological state of lakes some lakes were only assessed by experts while others used analytical approaches some of the assessments were carried over from previous measurement campaigns this resulted in a conflict of results in the event that the lake apparently did not achieve environmental objectives despite the implemented remedial measures this means that a certain kind of data result asymmetry occurred the proposed fragment of the methodology was therefore aimed at organizing the assessment process by 1 defining the role of an expert in the course of analyzes 2 introducing a consistent methodology of data pre processing 3 enabling the use of effective algorithms in the assessment which are sensitive to data deficiencies e g ksvm or pca and 4 enabling the preview of the entire assessment process so that it can be corrected or further improved in the future with reference to the results of the next campaign to assess the ecological status of waters future research should focus on assessing the scale of the phenomenon of ecological data drift which based on the observed climate change anthropological pressure and loss of biodiversity may have a significant impact on the broad concept indicator construction for lake water ecological assessment 6 software and data availability the research was conducted with use of software providing data visualization tableau 2021 1 1 https www tableau com data modelling r 4 0 5 via rstudio 1 4 1106 tiger daylily https www r project org https rstudio com and algorithm development draw io 15 9 1 https www diagrams net appendix c contains an r language script that converts all of the analysis procedures in this paper into an executable reproducible workflow the materials for this work are available from the hydrosource platform https www hydroshare org resource ebec024018be4c2ba04cbfa85bb14d8e in the repository titled lakeecomissingdata accessed as resource a r code for data preprocessing imputation and clustering as lakesmissingrcode r b xml file of featured workflow schema as lakemissingworkflow c csv file containing raw measurement results treated as input to this analysis d a set of results of the statistical analysis of the variables involved in the study declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105558 
25501,application of process based models at different spatial scales requires their proper parameterization this task is typically executed using trial and error parameter adjustment or a probabilistic method practical application of the probabilistic methods is hampered by methodological complexity and lack of interpretability here we present a novel approach for the parameterization of process based models that we call as conditional interval refinement method cirm the method can be best described as the combination of a probabilistic approach and the advantages of the expert based parameter adjustment cirm was demonstrated by optimizing the biome bgcmuso biogeochemical model using maize yield observations the proposed approach uses the general likelihood uncertainty estimation glue method with additional expert knowledge supplemented by the construction and interpretation of decision trees it was demonstrated that the iterative fully automatic method successfully constrained the parameter intervals meanwhile our confidence on the parameters increased the algorithm can easily be implemented with other process based models keywords model optimization bayesian calibration parameter constraints decision tree data availability observation data used in the study is available at github within the cirm branch of the rbbgcmuso software package https github com hollorol rbbgcmuso blob cirm docs cirm martonvasar maize obs nuts3 level census data on maize yield is available at the website of the hungarian central statistical office https www ksh hu docs hun xstadat xstadat eves i omn013a html climatic data used in the study have been obtained from the foresee database and is available at github https github com hollorol rbbgcmuso blob cirm docs cirm martonvasar wth 1 introduction process based models are typically associated with many parameters that have to be set by the user in any modeling exercise therond et al 2011 wöhling et al 2013 hararuk et al 2014 bilionis et al 2015 hidy et al 2022 model parameterization is a complex procedure as generally some of the adjustable constants of the model cannot be measured directly e g they are empirical coefficients associated with some process representation another problem is the well recognized uncertainty of the model parameters especially if the model is used at large spatial scales van oijen et al 2005 xiong et al 2008 angulo et al 2013 inverse modeling also referred as parameter estimation or model optimization calibration techniques are widely used to estimate unknown or uncertain model parameters based on observations braswell et al 2005 van oijen et al 2005 sadegh and vrugt 2013 bilionis et al 2015 inverse modelling is considered successful if and only if the model results based on the estimated parameters are reasonably close to the observations that represent the reality with some noise quantified by the measurement error if the uncertainty of the estimation is also low the model is considered appropriate to describe the studied system accurately and precisely the key factor here is the metric for the closeness of the simulations to the observations and the uncertainty of the parameter estimation where the latter can be represented by a probability distribution trudinger et al 2007 the metric is represented by the objective function e g likelihood for probabilistic methods while the latter is quantified by the uncertainty range i e posterior density function for probabilistic case up to recently most of the inverse modelling efforts directed toward only the objective function without estimating the parameter uncertainties e g wallach et al 2021 derivate based optimization techniques such as the levenberg marquardt method are frequently used moré 1978 trudinger et al 2007 brunetti et al 2022 these techniques can provide fast convergence with low stability ideal for deep learning applications where these are most commonly used the success of the procedure depends on the initial values of the parameters and the choice of meta parameters e g the learning rate for gradient descent goodfellow et al 2017 p 101 tarantola 2005 p 76 for complex process based models information about the derivate is usually not available therefore the so called derivate free methods are preferred two subcategories can be distinguished within the derivate free methods the direct search based methods and the probabilistic methods the first one is only suitable for finding the optimum parameter values and more subject to the curse of dimensionality bellman 1957 meaning that these methods are applicable only in low dimensional cases tarantola 2005 p 42 when we only have a few parameters to optimize grid search can be fast simple and stable meaning that after repeating the optimization with the same hyperparameters i e number of dividing points the result would be close to the original optimization output metaheuristic algorithms such as genetic algorithms can be used for addressing the curse of dimensionality but usually by sacrificing the stability or the unicity of the result gogna and tayal 2013 white et al 2022 finding the global optimum is also not guaranteed in such case the goal of the probabilistic methods which is the subject of this study is usually not only to find the optimum of the parameters but also to provide some uncertainty for these van oijen et al 2005 hartig et al 2011 these methods are largely flexible and naturally provide possibilities to calibrate input parameters while considering modelling parametrization and measurement errors at the same time tarantola 2005 however flexibility has its cost the more flexible the system is the more hyperparameters and assumptions have to be used in order to simplify the workflow building up a probabilistic model is generally much harder than finding a good objective function the least complicated probabilistic method that can be used for estimating the models input parameters is the maximum likelihood ml estimation this method relies on an efficient sampling algorithm which effectively samples from the input parameter space in this context effectivity means that the sampling distribution resembles the prior distribution with the minimum number of sampled parameters depending on the effectiveness of the sampling procedure the curse of dimensionality can be a major problem here as well however the biggest problem is that if the number of sampled parameters is low the confidence over the ml value is unreasonably strong and usually the ml values fail on independent data there are two possible directions to address this issue the frequentist and the bayesian approach the frequentist solution is regularization for example in the general likelihood uncertainty estimation glue method not all likelihood values are used for constructing the distribution of the estimated input parameter values but only the so called behavioral values which are usually defined as the top x typically x 5 of the sampled parameters prihodko et al 2008 beven and binley 2014 sexton et al 2016 the behavioral parameter values are all considered as good parameter values among them the parameter value with the ml value is just one of the many the median of the sampled parameter values is frequently considered as the optimum parameter value with the uncertainty interval defined by the empirical cumulative distribution function ecdf if the empirical probability density function is unimodal this approach is more stable and usually gives better results during validation with independent data than the ml estimation the other large family of the probabilistic methods is the bayesian one these methods are generally more flexible than the frequentist approaches and naturally provide interfaces to re use previous knowledge about the parameter space the use of previous knowledge prevents the optimization algorithms to be overly confident about the estimation which is based on small amount of data points the bayes rule is the unifying device with which the prior knowledge can be updated by the likelihood function to gather posterior knowledge gelman et al 1995 the posterior density function provides us with probabilistic information about our parameters for example one can answer questions like what is the probability of a given parameter value present within a certain range with this information we can estimate a parameter with its characteristic points such as a maximum posterior value map while providing information about the uncertainty intervals like highest posterior density interval hpdi however if our only expectation from the method is to produce reliable prediction instead of using the map value bayesian model averaging bma would be the best solution hinne et al 2020 as it uses all information from the posterior distribution to predict the output compared to the simpler ensemble method furthermore the posterior distribution is directly useable for hypothesis testing for example through bayesian factors bf gelman et al 1995 despite all of their strengths if executed properly bayesian methods are undoubtedly complicated thus during the inverse modelling process excessive number of assumptions and numerous choices have to be made first proper likelihood and prior functions have to be chosen the choice is always connected to the problem but not always obvious for example universally applicable uninformative prior distribution does not exist pericchi and walley 1991 although many use a uniform prior distribution for this purpose due to its simplicity pericchi and walley 1991 gelman 1996 gelman and yao 2020 wallach et al 2021 furthermore in spite of the fact that choosing the proper likelihood function is one of the most important decision to be made trudinger et al 2007 dumont et al 2014 modelers still usually use normal likelihood wallach et al 2021 however choosing the likelihood function family is still not sufficient to define the right priors or the global likelihood function there are several additional assumptions such as the independency of measurements and also the independency of the measurement and model errors tarantola 2005 the most frequent assumption by far is that gaining reliable inference about the input parameter space is possible by examining only the marginal posterior distributions there are few attempts to deal with the interdependence of the posterior distribution but these are usually examined pairwise or in a few cases by 3d scatter plots sadegh and vrugt 2013 beven and binley 2014 her and chaubey 2015 which clearly cannot capture higher dimensional relationships the assumptions about the sampling procedure are similar most of the proposed values for the parameters are sampled independently depending on the dimensionality of the parameter space probabilistic calibration is computationally expensive thus the methods which sample less while getting the most informative results are preferable if input parameter dependence is taken into account less simulations are needed bloom and williams 2015 for example if the model has two input parameters and the sum of the parameters must equal 1 the probability of getting the affordable parameter with random uniform sampling is virtually 0 because convex relationships are the most widespread using a sampling method designed specifically for convex cases can have a huge positive impact on the effectivity of the procedure otherwise every concave region can be split into disjoint convex regions where the input parameters can be sampled independently if the convex region is not a polytope it can be approximated with polytopes therefore techniques used to sample convex polytope regions are general enough to be good candidates to sample any parameter region one of these algorithms is the hit and run sampling lovász and vempala 2003 which according to our best knowledge has not been used for process based models yet at this point it is important to note that it is still an open question whether the bayesian method has advantages over the frequentist method even if bayesian methods are not used properly gelman and yao 2020 many studies focus on the practical application of different methods but we are very far from offering best practices or universally applicable solutions to the modeler community there are attempts to improve the success of optimization because of the complexity of the models and the related high dimensionality of the parameterization more and more observations are needed to confidently perform inverse modeling and most of the time the problem is still underdetermined sadegh and vrugt 2013 the modeler can theoretically tackle this problem by introducing additional information about the modelled system providing information about the relationship of the input parameters is a recently proposed new direction richardson et al 2010 bloom and williams 2015 this approach is based on the recognition that not every parameter combination is feasible based on the expert knowledge conditioning the input parameter space is not the only option to improve the results of model optimization process based models are inherently complex and produce more than one output streams calibrating models considering only one of these can result in good results for wrong reasons beven and binley 2014 as a consequence with the resulting parameterization the model can predict the given variable reasonably well on the training data but might fail on the validation data that is the typical case with overfitting the chance of overfitting is the highest if the parameters are associated with equifinality which means that in the predefined interval the likelihood of different parameter values is similar in the case of equifinality we cannot choose objectively from the posterior parameters which means that the parameter uncertainty cannot be constrained beven and freer 2001 equifinality does not necessarily mean failure of the simulation but rather it is a limitation of the inversion method one possible solution to avoid equifinality is the application of multi objective calibration by using multiple objective functions based on multiple observation data streams the result can be more stable with fewer parameters in equifinality her and seong 2018 however multi objective calibration needs good quality observation data streams to ensure a successful inversion because of the output stream interdependence adding one more data stream to the process can lead to a complicated workflow the noise of multiple data streams is not additive c f error covariance matrix in real life cases generally we do not have enough data for multi objective calibration or the data is too noisy in conjunction with multi objective calibration the modeler can use some rejection filtering techniques one of the main advantage of these approaches is the resilience against correlations between the different statistics of the model outputs hartig et al 2011 however applying rejection filtering alone does not shrink the parameter space and these methods cannot be easily interpreted hence the modelers who traditionally use trial and error approach with implicit rejection filtering do not have enough control over the inverse modeling procedure furthermore if the ratio of the filtered acceptable simulations is too low the convergence of the posterior sampling methods can be exceptionally slow or even questionable the complexity of the implementation of probabilistic methods and the possibility to get unusable results i e equifinality might be the main reasons why the majority of the researchers still prefer the trial and error method in model optimization wallach et al 2021 clearly there is a great need to develop interpretable easy to use and effective methods that support modelers to improve the optimization of the models especially in high dimensional cases in this study we introduce a novel method which can resolve some ongoing issues that are described above in probabilistic process based model optimization the method uses observations supplemented by expert knowledge on the simulated system unlike in case of the previously proposed methods richardson et al 2010 bloom and williams 2015 conditioning rejection filtering on the output data streams is introduced in our novel method a decision tree based algorithm is presented that reduces a priori input parameter intervals during the calibration of deterministic models and increases the reliability of simulation results of the calibrated model in this study we apply the glue method for its simplicity supported by an implementation of the hit and run algorithm our method can be easily combined with any bayesian or frequentist inverse modeling method and can contribute to improved and more efficient model optimization results in an array of scientific disciplines 2 material and methods 2 1 experimental data in the present study maize yield observations were used from martonvásár 47 19 56 97 n 18 47 50 61 e hungary central europe the climate of martonvásár is continental with mediterranean and oceanic influences mean annual temperature is 11 2 c while long term mean precipitation is around 550 mm according to the fao wrb classification system iuss working group 2015 the soil type of the area is a haplic chernozem with an average of 51 4 sand 34 silt and 14 6 clay content bulk density is 1 47 g cm 3 ph is 7 3 caco3 content is 0 1 and the mean soil organic matter content in the topsoil is 3 2 the plant available macronutrient supply in the soil is poor for phosphorus and medium to good for potassium based on the proplanta plant nutrition advisory system fodor et al 2011 in the ltfes every treatment is arranged in a random block design with 20 40 m2 plots in four replicates experimental data were collected in long term field experiments ltfe that were set up at the centre for agricultural research in the 1960s maize is grown in several ltfe sites within and around the city of martonvásár some of the experiments focus on the effect of organic and mineral fertilizer application on crop yield others focus on the effect of soil cultivation hybrid selection planting date and plant density on crop physiology and yield in the present study a composite maize yield dataset was used by retrieving the yield data of fao350 400 hybrids in high nitrogen level at least 160 kgn ha 1 year 1 treatments of four ltfes characteristic to the hungarian maize production system in the 1994 2018 period plant density 70 000 plant per hectare planting date second decade of april harvest date mid october the average yield was calculated from 16 yield data of the four selected ltfes with 4 repetitions in each treatment with the parcel size of 20 40 m2 for each year average maize yield calculated from all plots and all years in the experiments was 7 7 t ha 1 with a relatively large uncertainty sd 1 58 t ha 1 min 0 96 t ha 1 max 13 57 t ha 1 note that the observations refer to dry matter of the grain yield nuts3 level eurostat 2021 maize yield data from fejér county where the experimental site is located was also used in the study census data were retrieved from the database of the hungarian central statistical office for the period of 1991 2018 the average maize yield for the study period was 6 36 t ha 1 in fejér county sd 1 92 t ha 1 min 2 89 t ha 1 max 10 06 t ha 1 2 2 biome bgcmuso biogeochemical model the novel method introduced in the present study was linked with the biome bgcmuso process based model biome bgcmuso is a general purpose process based biogeochemical model that simulates the full carbon nitrogen and water budget of terrestrial ecosystems hidy et al 2012 2016 2021 2022 biome bgcmuso is a branch of the well known biome bgc model running and hunt 1993 thornton 1998 thornton et al 2002 churkina et al 2009 di vittorio et al 2010 biome bgc was significantly improved and extended in many terms relative to the original model developments addressed soil processes introduction of management options quantification of disturbance effect on plant physiology and many other processes one major milestone of the model development was the construction of a 10 layer soil submodule with sophisticated soil water balance routine the layer by layer representation of c and n dynamics within the soil and the implementation of detailed nitrification denitrification routine improvement of different stress factors drought stress nitrogen stress heat stress was also a major improvement growing degree day based phenophase specific allocation option was also included that enabled the detailed simulations of crops in cropland simulations detailed management information is essential for proper results including the timing and amount of applied fertilizer planting date soil cultivation type and date harvest date residue management the present biome bgcmuso model can be considered as a combined biogeochemical crop model with state of the art process representation at both scientific fields in this study biome bgcmuso v6 3 was used detailed description about the developments can be found in hidy et al 2012 2016 2022 fodor et al 2021 while additional details are available in the user s guide hidy et al 2021 the parameterization of biome bgcmuso is a complex task not only because of the high number of adjustable ecophysiological parameters but also because of the existence of some rules i e constraints that have to be fulfilled in order to ensure a successful simulation hidy et al 2021 for some parameters these rules are relatively simple e g c n ratio of leaf litter must be greater than the c n ratio of leaves but in some cases they are relatively complex e g the sum of the parameters that control the allocation of carbon to the different plant compartments must sum up to 1 these rules complicate the optimization of the model since in the monte carlo framework random numbers are generated within predefined intervals of selected parameters that drive the model clearly for example in the case of the allocation parameters random numbers will not sum up to 1 in the majority of the cases which will result in a large number of unsuccessful simulations 2 3 parameterization and model setup 2 3 1 a priori parameterization for the construction of the a priori maize ecophysiological parameterization literature search was conducted first for maize related data such as specific leaf area maximum stomatal conductance canopy light interception coefficient etc in the case of the empirical parameters hidy et al 2021 optimization was performed using a multi step procedure parameter adjustment was performed based on eddy covariance data gross primary production gpp and evapotranspiration et from the klingenberg cropland site de kli fluxnet code 50 53 35 n 13 31 20 6 e in germany prescher et al 2010 based on the maize years 2007 2012 and 2018 additionally leaf area index lai data was also used from the site for years when it was available phenological phase dependent allocation parameters were set based on a manual trial and error adjustment parameters for the penman monteith equation based evapotranspiration routine were also set using eddy covariance et data from klingenberg as part of previous modeling exercises maize parameterization was further evaluated previously and adjusted at other data rich experimental sites in the usa bushland lysimeter site in texas and mead eddy covariance site in nebraska us ne2 and us ne3 fluxnet codes it means that maize parameterization was tested for sites with different climatic conditions management types and with different maize cultivars characterized by a diversity of fao numbers the model optimization validation efforts revealed that it is not possible to construct a single universally applicable maize parameterization that is useable in different climatic and agro management conditions some of the parameters were highly site and in some cases year dependent senescence related parameter allocation parameters and plant tissue lifetime related parameters these previous experiences paved the way for the a priori parameterization of the model unfortunately at present there is no maize related eddy covariance data available for hungary in spite of the fact that 3 sites are running at present above croplands it means that optimization of the model is highly needed based on other available data 2 3 2 model setup biome bgcmuso was run at a plot level simulating a single generic maize parcel at martonvásár using 220 kgn ha 1 y 1 mineral fertilizer amount applied at the beginning of april planting and harvest dates were set according the reported dates driving meteorological data was retrieved from the foresee database open database for climate change related impact studies in central europe which is a free meteorological database for central europe with 0 1 0 1 spatial resolution dobor et al 2015 the soil input file of the model was constructed based on observations about the soil texture and soil water retention curve measurements using the pipette method iso 11277 on disturbed and sand kaolin box method iso 11274 on 100 cm3 undisturbed core samples collected from the ltfes for most of the nitrogen cycle parameters values proposed by hidy et al 2021 were used for the nuts3 level simulations the foresee database provided the meteorological data the dosoremi database pásztor et al 2020 was the source for the gridded soil data the simulations for the fejér county were performed on a predefined 0 1 0 1 resolution grid that was used in fodor et al 2021 fertilization was set according to data from the hungarian central statistical office planting data was 15 april and harvest date was 10 october in all simulations we used 51 grid cells covering an area of 4358 km2 the simulated yield data was aggregated at the county level by simple averaging of cell specific yields per year 2 4 description of the conditional interval reduction method 2 4 1 problem overview let m s o be an arbitrary process based model that estimates some output data o based on some input parameters θ s where s r n is the n dimensional input parameter space and o r l k is the matrix of model output where l is the number of time steps and k is the number of output data streams representing the simulated variables note that in this study the word parameter means adjustable input data in some cases convex constraints have to be defined over the s i e to handle dependencies between the input parameters see section 2 2 this can be done by introducing matrices g e and vectors e h for which 1 g θ h e θ e in the present study g controls the parameter ranges and the dependencies between parameters with relations i e one parameter must be smaller than the other one see section 2 2 e in this context is used for the allocation related parameters where the sum of all considered parameters must equal 1 hidy et al 2021 but of course the applicability of the constraint can be more general bloom and williams 2015 supplementary material contains the mathematical representation of the above described matrix based parameter dependencies with two examples inverse modeling has two major objectives tarantola 2005 the first one corresponds to the practical application of the model which needs proper parameterization to achieve reliable simulation results that are in optimal agreement with the observations this aim means that we need point estimation for the parameters single parameter set for all θ s the second one is the estimation of the uncertainty intervals of the parameters for future applications the intervals indicate our imperfect knowledge of the parameter values van oijen et el 2005 uncertainty intervals can be used to define uncertainty ranges of the simulated output variables given the observations d about the modelled system with predetermined uncertainties and the scientific knowledge of the environment the modeler can sample from a probability density function pdf for the model input parameters p θ d d d during the sampling procedure the model is evaluated against d this pdf is the so called posterior function and intuitively if the parameter space can be described by a continuous random variable it is the probability that θ lies in the interval of θ θ ε given the measured data d where ε is an infinitesimally small vector using the posterior distribution we can achieve both objectives defined above usually the posterior distribution cannot be sampled directly because its distribution is not known previous experience might be invalid in new optimization exercises van oijen et al 2005 however when the likelihood function l p d θ d d that is a probabilistic model for the model error gelman et al 1995 and the prior knowledge p θ called prior distribution is given during a monte carlo mc experiment the bayesian rule can be used for updating 2 p θ d p θ d p θ p d 3 p d p θ p θ d d θ c r because p d is a constant p θ d p θ d p θ usually in the frequently used markov chain monte carlo mcmc sampling p d is eliminated by a division after the detailed balance equation in a bayesian workflow it is rather typical that the modeler chooses a uniform prior p θ i u i i 1 i i 2 although this choice can be inadequate because uniform prior is not invariant under reparameterization therefore sensitive to the choice of its defining lower i i 1 and upper bounds i i 2 from the practical point of view choosing the right prior can help to avoid overfitting by regularizing the likelihood functions using uniform prior adds no regularization thus the maximum posterior values will be the maximum likelihoods leading to inappropriately strong conclusions gelman 1996 in the case of high amount of observation data the prior choice is less important and the maximum likelihood estimates are approximately the same as the maximum posterior estimates independently from the choice of the prior function despite the obvious drawbacks uniform priors are frequently used in bayesian frameworks because of their simplicity and because of the fact that it is easier to think about the parameters in a bounded space gelman 1996 stedinger et al 2008 gelman and yao 2020 wallach et al 2021 one other possible way to avoid inappropriately strong conclusions is to use other methods of regularization for example in the generalized likelihood uncertainty estimation method glue prihodko et al 2008 beven and binley 2014 likelihood filtering is applied only the likelihood values above the previously determined quantile 95th percentile is common choice are retained and considered as behavioral and the optimum is considered as the median of the filtered parameters the key advantage of this approach is its simplicity and flexibility and some degree of resistance against overfitting note that the method is sensitive to the choice of the quantile meta parameter this method is not considered as a bayesian one because we cannot interpret the results as probabilities it means that we do not have the proper posterior density function albeit it can be easily extended to be an approximate bayesian computation abc method sadegh and vrugt 2013 in this paper a novel method is introduced that can be used for updating prior distributions here we used the glue method for demonstration purposes and for simplicity exploiting also the typical visualization method for glue in the form of the so called dotty plots where equifinality can easily be recognized based on the marginal distributions in the following section a detailed description of the new method is provided 2 4 2 core logic of the output constraint approach let f o 0 1 be a function where f m categorizes model results into two classes feasible 1 and infeasible 0 output feasible in this context means that o is in accordance with expectations of the modeler we would like to stress that feasibility is not judged directly based on the quantitative comparison of the observation and the simulation but rather it is based on some additional knowledge about the simulated system this kind of knowledge can originate from the scientific literature from almanacs or from the everyday practice of the modeler the model has to be set so that o contains information about the process that is evaluated by f for a given θ parameter 4 θ s is feasible f m θ 1 for simplicity f is called the output conditioning or filtering function as we are interested only in parameter values that provide feasible output hereafter referred as feasible parameters f is used to filter out infeasible parameter combinations constraining the sampling procedure from the initial set although the classifier can help filtering out unrealistic simulations the ratio of the number of good simulations compared to all simulations c r can be low in some cases this ratio is defined as 5 c r f e a s i b l e s i m u l a t i o n s a l l s i m u l a t i o n s this means that the parameter intervals defined by the posterior distribution contain a lot of parameter values that represent good results for wrong reasons which means that we cannot use the whole distribution for gaining scientific knowledge because of the lack of confidence in getting feasible and stable results we need a further inspection of our parameter values and understand why we get a lot of infeasible results note that from this point parameter values associated with infeasible results are referred as infeasible parameters in such cases the researcher has to make an effort by manually looking into the parameter intervals and the input parameters to search for possible mistakes on setting the prior intervals or running more simulations to get more feasible output values in our approach f m is considered as a black box classifier of the input parameter space which is one of the most fundamental recognitions of the presented algorithm if we approximate f m with a white box classifier g the interpretation of g is isomorphic with the interpretation of f m isomorphic in this context means that decisions made by the white box classifier are the same as those provided by the black box classifier thus we can use g to modify the prior input parameter intervals to achieve the high c r the simplest yet still flexible and interpretable white box classifiers are decision trees dt this is the reason why they have recently been used for interpreting the decision making procedure of deep neural networks and support vector machines di castro and bertini 2019 lee and kim 2016 decision trees used for classification can capture complex relationships between the feature space and the output categories i e discriminate feasible and infeasible parameter values as a result they are not much sensitive to collinearities and they can make complex relationships easily interpretable too in our method the so called cart algorithm james et al 2013 was used to generate the trees because of its simplicity note that other algorithms such as id3 c4 5 c5 0 james et al 2013 could have been also used gini impurity index gi was used to quantify the impurity of a split in making the dt while the complexity based pruning was used to avoid overfitting nobel 2002 after creating the dt the accuracy a was determined in the following way 6 a t p t n t p t n f p f n where tp means number of true positive cases fn means number of false negative cases fp means number of cases identified as false positive while tn means number of true negative decision for the f m low accuracy a 0 5 means that the model performance was worse than the random decision in the case of low performance a 0 7 the algorithm is not applicable to parameter interval change if the accuracy is high we can trust our white box model since it behaves similarly as the black box the leaf nodes of a decision tree are continuous domains in the parameter space defined by its corresponding decision nodes if r 1 r 2 r b are the input domains for leaf nodes y 1 y 2 y b we are interested in the domains with the highest number of feasible parameters if there is more than one region with the highest number of feasible parameter values we can have 3 possibilities to decide which one is the region of interest 1 chose the region with the least amount of connected decision nodes and apply the validation to this region 2 chose the region with the lowest fp rate and apply the validation to this region 3 choose both above mentioned regions and apply validation in the next step with this method the selected region ru is defined by the decisions at the decision nodes the region is the maximum volume hyperrectangle inscribed into the constrained region rc sides parallel to the parameter vectors over the original parameter region ro fig 1 the updated parameter intervals are the orthogonal segments which define the hyperrectangle if we have multiple output constraint functions we create multiple dts and go through the updating process sequentially if there is an inconclusive update we roll back the changes and continue the procedure in the next step we can shrink the intervals further if there is more information available for that region the intervals specified by applying individual constraints must overlap to provide updated parameter intervals for the next step note that since in this paper the issue with the similar sized feasible region was not addressed the above mentioned solution is not part of the current implementation 2 4 3 model optimization here we combine the glue frequentist model optimization method prihodko et al 2008 stedinger et al 2008 beven and binley 2014 with the application of the above described dt based classification method we named this procedure as conditional interval refinement method cirm it is easy to extend the f m classifier to support multiple output constraint functions if we have m output constraint functions f i i 1 m we can define f as the following 7 f x i 1 m f i x after the parameter sampling and model simulations behavioral parameter choice can be done with quantile filtering described above with an additional filtering according to the conditioning retaining feasible parameter values where f x 1 it is assumed that the modelization uncertainties are negligible here compared to observational uncertainties and observational uncertainties follow a normal distribution furthermore if the observations are independent from each other the likelihood function is defined as 8 l θ d d i 1 n d 1 σ 2 π e 1 2 m θ d i σ 2 where σ refers to observation uncertainty for practical considerations e g arithmetic underflow we used the loglikelihood instead of the likelihood note that in this case the maxima places are unchanged for the prior function uniform prior on convex polytope was used in order to support successful monte carlo experiment a novel algorithm was developed and applied here based on the hit and run algorithm with mirroring optimization lovász and vempala 2003 meersche et al 2009 note that model optimization with glue or with any other optimization method can be applied at each step of the proposed approach it is mandatory to perform it only for the last step 2 4 4 summary of the proposed method algorithm 1 presents the proposed constrained calibration method the workflow summarizes the methods detailed above also revealing the consecutive steps with all related input and output data in this paper for demonstration purposes we performed glue in each iteration step algorithm 1 workflow of the proposed method including model optimization and parameter interval update meaning of the symbols is defined in the text image 1 2 5 implementation of the method we used the above proposed method to optimize biome bgcmuso v6 3 for maize in a low data situation which means low amount of good quality observation data when only final crop yield data was available supplemented with some additional information about overall crop properties that represent constraints biome bgcmuso has 120 maize related ecophysiological parameters that have to be set by the modeler prior to the simulations some of the parameters are generic plant parameters like c n ratio of plant compartments maximum stomatal conductance maximum rooting depth root distribution parameter canopy water interception coefficient etc while some of them are specific to maize parameters affecting e g heat stress during anthesis germination as the function of soil water content etc hidy et al 2021 among the 120 parameters 42 are affected by some rules see above 28 out of the 42 rule affected parameters are related to allocation similarly to other models that are characterized with a high number of parameters e g bilionis et al 2015 it is impossible to optimize the model for all parameters instead we used previous experience and the outcome of several previous model evaluations for the parameter selection and parameter setting during previous model optimization efforts see methods the selection of the most relevant parameter values were done partly using objective sensitivity analysis and partly by manual parameter adjustment many parameters were fixed because of available observation data like leaf stem and fine root c n ratios and the results of experience at the german and usa experimental sites maximum stomatal conductance canopy light extinction coefficient etc the remaining parameters were found to be variable between the sites and the model showed sensitivity to the proper setting of the parameters see table 1 for a full list for the selected parameters prior to model optimization the upper and lower bounds were set based on literature values stöckle and nelson 2013 white et al 2000 expert knowledge of the co authors and previous experience was also used to set the intervals table 1 note that the complete prior ecophysiological parameterization for the simulations of maize is presented in the supplementary material for the practical implementation of the procedure with biome bgcmuso at the martonvásár ltfe site we used a uniform distribution as prior over convex polytope we used a normal likelihood function eq 8 and we assumed that modelization uncertainties are negligible compared to observational uncertainties we used 0 01 for the complexity based pruning factor for the dt as it was the default value in the rpart package therneau et al 2022 the white box model was the decision tree with cart as described above we used 4 output constraint functions f to evaluate feasible or infeasible simulations the first constraint is related to the annual harvest index hi that is defined by the ratio of the final grain yield and total aboveground biomass at harvest goudriaan et al 2001 the median of the simulated hi values is required to be in the range of 0 40 0 55 defined based on several scientific publications hütsch and schubert 2018 ion et al 2015 li et al 2015 2017 liu et al 2020 the median of the annual maximum leaf area index laimax is requested to be between 2 7 and 5 m2 m2 that is an observation based setting for hungary see e g pokovai and fodor 2019 the long term median value of the rooting depth at the beginning of the flowering phenophase should be larger than 1 4 m but should be less than 1 8 m those values are based on expert knowledge the median of anthesis days expressed in day of year doy should be between 180 and 190 that is a typical range for hungary the output constraint functions are defined by a simple algorithm based on the model outputs for example if annual laimax is a vector containing annual maximum values of the biome bgcmuso simulated lai f is defined as 9 f x 1 m e d l a i m a x 2 7 5 0 o t h e r w i s e where med represents the median of the values from the simulated years during the point simulation biome bgcmuso simulations were performed within the rbbgcmuso software environment https github com hollorol rbbgcmuso rbbgcmuso is an open source r package supporting the easy and user friendly application of the model we used the cirm branch of rbbgcmuso for the case study presented in this paper the output conditioning method was implemented using 10 iterative steps in each step the automated algorithm performed post processing of the decision trees using the method described above i e the interval refinement was done automatically taking into account the overlaps in the calculated thresholds step 4 in algorithm 1 10 000 simulations were done for each iteration step standard r was used mostly in the work r core team 2021 additionally the rpart package was used to construct the decision trees therneau et al 2022 in each iteration step the maximum likelihood parameter set was stored a posteriori parameter intervals were estimated based on behavioral top 5 parameter values that matched the predefined conditioning during the first 9 iteration steps parameter interval reduction was performed based on the decision tree update algorithm glue based a posteriori interval reduction was considered only in the final iteration step optimum parameter set was calculated as the median of the behavioral parameters here we refer to these intervals and optimum parameter set as glue based constrained intervals optimized model performance was evaluated using the square of the linear correlation coefficient r2 bias systematic error root mean square error rmse and nash sutcliffe modelling efficiency me ma et al 2011 sándor et al 2016 the performance indicators were calculated based on the observed and the simulated maize yield time series the optimized model was evaluated based on the training dataset and also on the nuts3 level simulation using independent data in this latter case mean annual maize yield was calculated from the model simulations and the final time series was evaluated against the observed census data we also quantified the percent of change in the final glue parameter intervals relative to the a priori intervals 3 results 3 1 glue and post processing via the dts fig 2 shows selected dotty plots from the first iteration step from the 20 studied parameters table 1 we selected 4 that represent typical patterns characteristic to all cases from the final 10th iteration step see below supplementary material fig s1 shows the complete set of dotty plots for the first step the most prominent feature of the dotty plots is equifinality based on the grey dots that represent all simulations this is the case for almost all other parameters that are presented in fig s1 there are a few exceptions where some pattern can be recognized on the graphs maxlifetime4 leaf allocation 3 leaf allocation 4 stem allocation 3 and rubisco to some extent fig s1 but the distribution of the grey dots support only a slight interval reduction for e g stem allocation 3 leaf allocation 4 for other parameters such as maxlifetime4 even if some pattern can be recognized in the dotty plots posterior intervals based on the behavioral grey dots will not result in parameter interval reduction note that in a typical bayesian calibration this is the final stage of the optimization which is clearly not satisfying and unsuccessful in terms of pursuing an interval reduction after we decided to check the consistency of the results based on the predefined constraints using hi laimax anthesis date and rooting depth the overall picture changed fig 2 fig s1 red dots however likely given the large degree of freedom of the optimization after the first iteration only 596 simulations were feasible which is clearly a low success rate cr 5 96 with the output constraint filtering the dotty plots still show equifinality for most of the cases there are some exceptions like rubisco maxlifetime4 leaf allocation 3 root allocation 3 stem allocation 3 leaf allocation 4 root allocation 4 fruit allocation 6 and stem allocation 6 for which the algorithm based on constraints suggests that they should have narrower ranges fig 2 fig s1 for example in the case of rubisco the parameter range 0 7 0 11 seems to be reasonable based on the red dots we would like to stress here again that in a typical optimization exercise the user neglects some or all above mentioned constraints and conclusions are drawn based on the grey dots only in this sense information extracted from the feasible simulations is already one step forward one can recognize that because of the small c r we do not have a sufficient number of feasible simulations to trust the goodness of the glue optimum values and the glue uncertainty ranges in order to increase c r further refinement of the parameter intervals is a reasonable next step the constructed decision trees provide information about the possible relationships between the parameters and the feasible infeasible simulations thus they can be useful for updating the parameter intervals given the fact that we have 4 output constraints 4 dts were constructed fig 3 shows the dt that was constructed based on the first constraint hi after iteration step 1 the top level of the dt is called the root node in the case of fig 3 this is represented by stem allocation 6 this top level always shows the most important parameter that affects the feasibility of the simulation in this case in terms of hi the lower part of the dt is divided into different layers or levels where internal nodes are located the importance of the parameters associated with the different internal nodes is decreasing with the layer number i e the importance decreases from top to bottom internal nodes represent additional decisions revealing the parameter value that splits the parameter range into two parts depending on the feasible infeasible character of given simulations for example at layer 2 the cutting threshold value for the length of phenophase 4 parameter is 372 if the parameter is less than 372 then we can reach the right branch with the rest of the levels at the bottom of the dt the leaf nodes are located leaf nodes represent the results of the classification due to the homogeneity of the leaf nodes in fig 3 blue leaf nodes i e rounded squares at the bottom marked with 0 which is the result of f m indicate infeasible simulations while green leaf nodes represent feasible simulations marked with 1 the percentage values inside the leaf nodes show the fraction of simulations that is associated with a given branch according to fig 3 due to the applied hi constraint 58 of the sampled parameter combinations were infeasible sum of the percentages in the blue boxes as it was explained in section 2 4 2 in our approach we always focus on the leaf node with the highest percentage of feasible simulations in this sense the dt suggests that the most important parameters associated with the hi constraint in the decreasing order are stem allocation 6 length of phenophase 4 length of phenophase 3 and maxlitefime 4 this is the path from the top to the leaf node with the highest after excluding the repeated occurrence of the parameters in the lower layers of dt those parameters indirectly affect grain allocation in the model and thus hi it is somewhat surprising that fruit allocation 6 parameter is not included in the dt in this path but it is included in other paths leading to other leaf nodes maxlifetime4 affects leaf senescence dynamics prior to and during grain filling thus interacts with hi the lengths of phenophases 3 and 4 affect the leaf dynamics that clearly influence assimilation thus grain allocation during the 6th phenophase the information gained from the dt is essentially useful and provides insights into the complex process of plant growth and final yield that is implemented in biome bgcmuso based on the decisions across the path from top to the leaf node associated with the largest success rate 21 rightmost leaf node we can extract the thresholds and update the original parameter intervals see table 1 stem allocation in the 6th phenophase should be less than 0 31 and greater than 0 16 the original interval was 0 1 0 4 see table 1 the length of the 3rd phenophase should be smaller than 405 and the length of the 4th phenophase should be smaller than 372 maxlifetime4 should be set larger than 693 note that at any time of the procedure a user might select a different leaf node with a lower success rate if the expert knowledge supports an alternative choice in this case manual adjustment might be needed in the parameter ranges and the iteration should be restarted using the adjusted intervals fig 4 shows the dt for the laimax constraint after iteration step 1 the figure shows that 58 of the sampled parameter combinations is infeasible the tree suggests that the most important parameters in a decreasing order are rubisco leaf allocation 4 and length of phenophase3 at lower layers rubisco and leaf allocation 4 appear again as rubisco ultimately controls the maximum photosynthesis rate white et al 2000 its importance is straightforward in terms of leaf development the role of leaf allocation in the 4th phenophase which determines the peak lai is also clear and easily interpretable the length of the 3rd phenophase is less intuitive but it is reasonable since it affects the initial condition of leaf development in the 4th phenophase when lai reaches its maximum using the dt and the path to the rightmost leaf node that is associated with the highest success rate with 16 we can set new intervals for the parameters based on all decision nodes from the dt in fig 4 rubisco should be less than 0 097 and greater than 0 079 leaf allocation 4 should be less than 0 34 and length of phenophase3 should be larger than 296 note that the dt presented in fig 3 already set a new upper limit for length of phenophase3 which is further refined here fig 5 presents the dt for the rooting depth constraint based on the results from the 1st iteration step in this case only 29 of the sampled parameter combinations were infeasible the tree suggests that the most important related parameters are rubisco and length of phenophase 3 this is reasonable considering the determinant role of rubisco in terms of overall productivity and considering the importance of the 3rd phenophase in terms of root establishment rubisco interval can be further refined here it should be larger than 0 09 and according to the previous tree in fig 4 it should be less than 0 097 length of phenophase3 should be greater than 294 which is in fact not used at this stage as it was already set to be larger than 296 fig 4 fig 6 presents the dt for the last anthesis date based constraint using the anthesis date dt 42 of the sampled parameter combinations turned out to be infeasible the tree suggests that the most important parameters are the length of phenophases 3 and 4 this is in perfect agreement with the expectations as the anthesis date is driven by the length of the previous phenophases expressed in gdd note that the length of the first two phenophases is fixed in the simulations the dt provides guidelines for updating the two parameters according to the path leading to the rightmost leaf node length of phenophase3 should be set larger than 318 and length of phenophase4 should be larger than 313 these new settings further constrain these parameters as they were already refined to some extent at this stage the most important recognition is the usefulness of the dts that can be used for manual updating of the parameter intervals this kind of information was hidden so far as the marginal distributions did not reveal parameter interval reduction possibilities after performing the dt analyses of individual constraints the conditions from individual dt are combined and the parameter intervals are modified 3 2 results of the iterations and automatic interpretation of the dts one might recognize at this point that the new interval settings might be used as the new prior of another monte carlo based glue experiment as it was described above a custom procedure was developed to automatically interpret the dts by finding the path to the leaf node that contains the highest percentage of feasible simulations based on this method the above described interval refinement became unattended if the next iterations turn out to provide better success rates and if the parameter interval reduction can be further refined the procedure can be transformed into a multi step iterative method in our case after the first iteration step another nine were performed during the iterations the success rate fig 7 a increased monotonically indicating that the introduced algorithm worked well on improving the success rate as fig 7b shows the white box approximation was also correct because on average the approximation s accuracy monotonically increased although it can be attributed to the large success rate after step 4 supplementary material fig s2 shows the complete set of dotty plots for the 10th iteration step fig 8 shows selected dotty plots that represent typical patterns from the final step the graph is in accordance with fig 7 revealing that almost all simulations were feasible at this stage grey dots are hardly detectable although most of the parameters still show equifinality the parameter intervals are considerably smaller than after the first step fig 2 fig s1 some of the parameters provide well detectable optimum e g max lifetime 4 at fig 8 with typical parameter distribution other parameters show well bounded points like in case of stem allocation 4 parameters controlling the length of the phenophases 3 and 4 show an unusual distribution that is the clear consequence of the cutoff values defined by the dts see above in those plots the behavioral parameters localize the optimum value after the 10th iteration step almost all sampled parameters were feasible 95 45 i e 9545 iterations out of 10 000 it means that almost all simulations satisfied the predefined constraints thus provide results according to the expectations of the user given the large number of successful and meaningful simulations the high sample number clearly improves the confidence of the user about the statistical properties of the results most of all the optimum and the uncertainty ranges considering the optimum values of the parameters represented by vertical lines in fig 8 the maximum likelihood values typically differed from those calculated from the behavioral data i e glue median similarly to step 1 fig 2 fig s1 fig 9 shows the summary of the inversion in the form of a special plot type that is referred to as kitchen sink plot the figure provides easily interpretable information about the multiple step iteration in terms of dt based parameter interval reduction as a function of iteration step number and it also shows the position of the maximum likelihood estimation relative to the parameter ranges note that in the 10th iteration step the glue based interval reduction was also considered the glue based optimum is not indicated but this can be approximated by the mid point of the actual interval fig 9 clearly shows that parameter intervals were significantly reduced in many cases for some parameters such as rubisco and length of phenophase4 interval reduction was more profound in the first iteration step for other parameters such as root depth function shape leaf allocation 3 and stem allocation 6 the reduction was gradual and not necessarily associated with one iteration step this indicates the utility of the multiple step approach in case of some parameters the prior interval remained unchanged e g root allocation 3 stem allocation 3 the plot also shows that in some cases the ml value was out of the final parameter interval i e in step 10 that indicates an infeasible solution as the first iteration step is the one which is performed in a usual bayesian experiment this clearly shows the good results for wrong reasons situation table 2 summarizes the model optimization exercise providing information about the posterior parameter intervals the parameter set representing the maximum likelihood estimation in the final step and the glue based parameter ranges average interval percentage change was 44 for the 20 studied parameters the maximum was 88 associated with rubisco parameter range reduction did not occur for length of phenophase 6 root allocation 3 stem allocation 3 root allocation 6 and maxlifetime 3 stem and root allocation in the 3rd phenophase seem to be less determinant in terms of the final crop yield which is an interesting outcome the interpretation of this parameter behavior from the 6th phenophase is easier estimation of the exact length of this phenophase might be impossible since after leaf senescence during grain filling the final crop yield cannot change anymore so its value cannot be set by any observation or constraint root allocation in the 6th phenophase is small and seems to have no substantial effect on the final crop yield table 2 suggests that additional constraints might support the reduction of the intervals in the parameters where 0 reduction is present note that in some cases this is not a problem as the other dependent parameters already set the value for these parameters like in case of the allocation when the sum of the parameters must sum up to 1 overall the parameter interval reduction have led to 42 3 decrease in the simulated yield uncertainty quantified by the mean of the annually calculated standard deviation of the modeling results based on 1000 simulations performed by monte carlo based sampling from the original and reduced parameter ranges fig s3 in the supplementart material 3 3 performance analysis on the calibration dataset and validation fig 10 shows the model results for the prior parameterization and for the glue based optimized parameter set uncertainty of the observations is high given that the maize yield dataset is a composite of many small experimental plots see methods the figure indicates that in many cases the optimized model estimated yield within the uncertainty ranges improvements were quantified by using statistical indicators table 3 shows the results of the statistical evaluation of the simulations here we include performance metrics from the maximum likelihood simulations as well the table shows considerable improvements of the quality of the simulations in the consecutive iterations steps the explained variance was typically higher for the maximum likelihood parameterization than for glue for maximum likelihood r2 substantially increased already after 1st iteration while this was achieved only after the 6th iteration for glue rmse was lower for the maximum likelihood than for glue exception is step 6 the bias was variable but basically became close to 0 in the case of maximum likelihood while remained positive for glue me was better in the case of the maximum likelihood parameterization than in the case of glue in summary all parameter values show better performance in the case of the maximum likelihood parameterization which might indicate overfitting table 4 shows the performance metrics of the model output for maize yield at martonvásár and for the independent validation experiment nuts3 level model simulation both for the calibration set martonvásár and the validation set the optimized simulation results were significantly closer to the observations than the a priori simulations it means that the calibrated model is more appropriate to apply to the nuts3 level independent dataset maximum likelihood parameterization from the 10th step over performed the glue based simulations in terms of bias and me while the difference between the rmse values was small in terms of r2 the glue based method performed better the explained variance 48 was in fact higher here than in the case of the training dataset for glue note that me was negative in both cases because the magnitude of the yield was different at martonvásár and at the county level likely due to the different n fertilization level and agrotechnology the better performance of glue in the validation experiment in terms of r2 indicates that maximum likelihood was associated with over fitting and the glue based method might be more feasible note that at this stage the modeler might perform a hybrid parameterization using the maximum likelihood values for some parameters that are associated with small or zero interval reduction and glue based more constrained parameters in other cases maximum likelihood values for some parameters might be informative if there is consensus in their values across the multiple iterations steps note that the low explained variance does not necessary mean bad simulation in our case the likelihood optimum function was normal so during the training procedure the main goal was to minimize the error and not to maximize r2 3 4 limitations and outlook like all optimization methods cirm also has limitations the method assumes that the dt white box model is an adequate approximation to the black box model m f similarly to any machine learning classifiers dts have problems with unbalanced datasets because these are heavily biased towards the majority classes hoens and chawla 2013 however quantifying the goodness of the approximation can be accomplished easily by checking the accuracy of the dt while simultaneously checking the success rate if the success rate monotonically increases while the accuracy is above a predefined threshold e g 0 7 is considered high enough the dt approximation is considered adequate through the updating procedure another problem is that the dts are prone to overfitting traditionally the solution for the overfitting is the application of ensemble methods such as random forest or adaboost james et al 2013 however applying these solutions may result in loss of interpretability and might complicate the interval update algorithm additional research should focus on this issue in the future another direction of development can focus on the application of other performance metrics such as recall that uses tp tp fn for the decision trees that are more suitable for unbalanced datasets in this paper dts are trained on the full available dataset because the success rate calculation and the accuracy value in the consecutive iteration can be considered as a simple validation in this study the introduction of the cirm method was done using uniform priors coupled with the widely used glue probabilistic method cirm can be used with other priors and also with other probabilistic model optimization methods in such cases the prior update rule has to be defined based on the dts for example for normal prior distributions μ and σ parameters can be determined in a way that the 0 95 highest density interval s endpoints are generated by the dt update rule as defined earlier this method is applicable for every two parameter unimodal distribution function e g beta distribution it is also important to emphasize that there are alternatives to the interval update algorithm while processing the dt see section 2 4 2 the algorithm presented here selects the maximal volume inner hyperrectangle inscribed into the resulting polytope described by the dt fig 1 alternatively the bounding hyperrectangle could also be used further research is needed to compare these two simple alternatives and see their effect on the results in this study the demonstration of the introduced new method was done at a single site with few available observations in spite of the poor situation the method could provide useful results which means potential in other similar or more data rich cases most importantly the method has to be tested at experimental sites equipped with eddy covariance measurements and ancillary measurements data retrieved from the try database can be included in the model optimization with predetermined ranges for some plant traits kattge et al 2011 according to the introduced method it is possible to handle observations as additional constraints together with another observation data stream that is used for the construction of the likelihood function 4 concluding remarks in this study we presented a novel approach for the inversion of process based models that goes beyond the traditional probabilistic methods although the basic aim is unchanged i e constraining parameter uncertainty the method is markedly different and can be best described as a combination of the traditional probabilistic methods and the application of an interpretable machine learning method up to the knowledge of the authors there is no similar approach published in the literature although the concept of reality constraints as part of the model optimization is already introduced in the literature the previously proposed methods deal exclusively with the input model parameter constraints and no conditioning is present regarding the output data streams note that input data conditioning is an implicit feature of our procedure and is implemented in the rbbgcmuso package that was used to execute the optimization the so called bayesian filtering technique shows some similarity to our method this method implements conditioning of the output streams but as their convergence speed is proportional to the ratio of realistic all simulations they can be very slow additionally this method requires in depth knowledge from the user regarding the underlying inter dependencies of the parameter space interpretability is a remarkable advantage of the cirm method in this regard the output conditioning introduced in this study is a novel technique another advantage of the proposed method is that it has two modes a manual and an automatic one in the manual mode the user can examine the relationships represented by the constructed dts and perform the interval refinement accordingly or the proposed refinement can be accepted or modified based on additional scientific knowledge the automatic interval refinement needs no supervision in this way this method could be a potential candidate for researchers who used the trial and error approach for model inversion so far the new method requires minimal extra knowledge about the technical implementation of the method from the user and as the method is fully automatic inexperienced researchers might still use it successfully concentrating mainly on the scientific questions the focus of the study was a low data situation when the model had to be optimized against maize yield data with relatively large uncertainty the study demonstrated that the traditional probabilistic method glue resulted in unconstrained parameter intervals suggesting that the low data situation leads to a large uncertainty of optimized parameter values this problem was solved by applying the proposed cirm method that eventually led to successful interval reduction with almost 100 realistic simulation score before the calibration only 6 of the simulations were acceptable despite all of the achievements and proposed solutions in the field of model optimization reality is sobering most of the scientists still use trial and error model optimization wallach et al 2021 and the minority who preferred some probabilistic methods typically use only prior knowledge there is a clear and well recognized need for simple easy to use and interpretable software solutions that can be used for parameter estimation of a wide array of process based models cirm might represent a remarkable major step forward to support improved model optimization and application given the fact that cirm is a model independent method it can be easily implemented in any modelling environment software availability the rbbgcmuso package that is used in the study is available at github https github com hollorol rbbgcmuso tree cirm biome bgcmuso is available at the website of the model http nimbus elte hu bbgc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was funded by the széchenyi 2020 programme the european regional development fund and the hungarian government ginop 2 3 2 15 2016 00028 by the national multidisciplinary laboratory for climate change rrf 2 3 1 21 2022 00014 project and by grant advanced research supporting the forestry and wood processing sector s adaptation to global change and the 4th industrial revolution no cz 02 1 01 0 0 0 0 16 019 0000803 financed by op rde also supported by the integrated infrastructure operational programme funded by the erdf project scientific support of climate change adaptation in agriculture and mitigation of soil degradation grant no itms2014 313011w580 the klingenberg ec measurements have been funded by eu project carboeurope ip goce ct 2003 505572 bmbf for icos d implementation 1lk1101a 1lk1101b and freistaat sachsen 1 0230 00 68 192 2016 118972 1 0452 147 158 65 0456 49 10 65 0456 46 28 we thank andy suyker steve evett and bruce kimball for providing us the bushland and mead experimental data for benchmarking the model appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105556 
25501,application of process based models at different spatial scales requires their proper parameterization this task is typically executed using trial and error parameter adjustment or a probabilistic method practical application of the probabilistic methods is hampered by methodological complexity and lack of interpretability here we present a novel approach for the parameterization of process based models that we call as conditional interval refinement method cirm the method can be best described as the combination of a probabilistic approach and the advantages of the expert based parameter adjustment cirm was demonstrated by optimizing the biome bgcmuso biogeochemical model using maize yield observations the proposed approach uses the general likelihood uncertainty estimation glue method with additional expert knowledge supplemented by the construction and interpretation of decision trees it was demonstrated that the iterative fully automatic method successfully constrained the parameter intervals meanwhile our confidence on the parameters increased the algorithm can easily be implemented with other process based models keywords model optimization bayesian calibration parameter constraints decision tree data availability observation data used in the study is available at github within the cirm branch of the rbbgcmuso software package https github com hollorol rbbgcmuso blob cirm docs cirm martonvasar maize obs nuts3 level census data on maize yield is available at the website of the hungarian central statistical office https www ksh hu docs hun xstadat xstadat eves i omn013a html climatic data used in the study have been obtained from the foresee database and is available at github https github com hollorol rbbgcmuso blob cirm docs cirm martonvasar wth 1 introduction process based models are typically associated with many parameters that have to be set by the user in any modeling exercise therond et al 2011 wöhling et al 2013 hararuk et al 2014 bilionis et al 2015 hidy et al 2022 model parameterization is a complex procedure as generally some of the adjustable constants of the model cannot be measured directly e g they are empirical coefficients associated with some process representation another problem is the well recognized uncertainty of the model parameters especially if the model is used at large spatial scales van oijen et al 2005 xiong et al 2008 angulo et al 2013 inverse modeling also referred as parameter estimation or model optimization calibration techniques are widely used to estimate unknown or uncertain model parameters based on observations braswell et al 2005 van oijen et al 2005 sadegh and vrugt 2013 bilionis et al 2015 inverse modelling is considered successful if and only if the model results based on the estimated parameters are reasonably close to the observations that represent the reality with some noise quantified by the measurement error if the uncertainty of the estimation is also low the model is considered appropriate to describe the studied system accurately and precisely the key factor here is the metric for the closeness of the simulations to the observations and the uncertainty of the parameter estimation where the latter can be represented by a probability distribution trudinger et al 2007 the metric is represented by the objective function e g likelihood for probabilistic methods while the latter is quantified by the uncertainty range i e posterior density function for probabilistic case up to recently most of the inverse modelling efforts directed toward only the objective function without estimating the parameter uncertainties e g wallach et al 2021 derivate based optimization techniques such as the levenberg marquardt method are frequently used moré 1978 trudinger et al 2007 brunetti et al 2022 these techniques can provide fast convergence with low stability ideal for deep learning applications where these are most commonly used the success of the procedure depends on the initial values of the parameters and the choice of meta parameters e g the learning rate for gradient descent goodfellow et al 2017 p 101 tarantola 2005 p 76 for complex process based models information about the derivate is usually not available therefore the so called derivate free methods are preferred two subcategories can be distinguished within the derivate free methods the direct search based methods and the probabilistic methods the first one is only suitable for finding the optimum parameter values and more subject to the curse of dimensionality bellman 1957 meaning that these methods are applicable only in low dimensional cases tarantola 2005 p 42 when we only have a few parameters to optimize grid search can be fast simple and stable meaning that after repeating the optimization with the same hyperparameters i e number of dividing points the result would be close to the original optimization output metaheuristic algorithms such as genetic algorithms can be used for addressing the curse of dimensionality but usually by sacrificing the stability or the unicity of the result gogna and tayal 2013 white et al 2022 finding the global optimum is also not guaranteed in such case the goal of the probabilistic methods which is the subject of this study is usually not only to find the optimum of the parameters but also to provide some uncertainty for these van oijen et al 2005 hartig et al 2011 these methods are largely flexible and naturally provide possibilities to calibrate input parameters while considering modelling parametrization and measurement errors at the same time tarantola 2005 however flexibility has its cost the more flexible the system is the more hyperparameters and assumptions have to be used in order to simplify the workflow building up a probabilistic model is generally much harder than finding a good objective function the least complicated probabilistic method that can be used for estimating the models input parameters is the maximum likelihood ml estimation this method relies on an efficient sampling algorithm which effectively samples from the input parameter space in this context effectivity means that the sampling distribution resembles the prior distribution with the minimum number of sampled parameters depending on the effectiveness of the sampling procedure the curse of dimensionality can be a major problem here as well however the biggest problem is that if the number of sampled parameters is low the confidence over the ml value is unreasonably strong and usually the ml values fail on independent data there are two possible directions to address this issue the frequentist and the bayesian approach the frequentist solution is regularization for example in the general likelihood uncertainty estimation glue method not all likelihood values are used for constructing the distribution of the estimated input parameter values but only the so called behavioral values which are usually defined as the top x typically x 5 of the sampled parameters prihodko et al 2008 beven and binley 2014 sexton et al 2016 the behavioral parameter values are all considered as good parameter values among them the parameter value with the ml value is just one of the many the median of the sampled parameter values is frequently considered as the optimum parameter value with the uncertainty interval defined by the empirical cumulative distribution function ecdf if the empirical probability density function is unimodal this approach is more stable and usually gives better results during validation with independent data than the ml estimation the other large family of the probabilistic methods is the bayesian one these methods are generally more flexible than the frequentist approaches and naturally provide interfaces to re use previous knowledge about the parameter space the use of previous knowledge prevents the optimization algorithms to be overly confident about the estimation which is based on small amount of data points the bayes rule is the unifying device with which the prior knowledge can be updated by the likelihood function to gather posterior knowledge gelman et al 1995 the posterior density function provides us with probabilistic information about our parameters for example one can answer questions like what is the probability of a given parameter value present within a certain range with this information we can estimate a parameter with its characteristic points such as a maximum posterior value map while providing information about the uncertainty intervals like highest posterior density interval hpdi however if our only expectation from the method is to produce reliable prediction instead of using the map value bayesian model averaging bma would be the best solution hinne et al 2020 as it uses all information from the posterior distribution to predict the output compared to the simpler ensemble method furthermore the posterior distribution is directly useable for hypothesis testing for example through bayesian factors bf gelman et al 1995 despite all of their strengths if executed properly bayesian methods are undoubtedly complicated thus during the inverse modelling process excessive number of assumptions and numerous choices have to be made first proper likelihood and prior functions have to be chosen the choice is always connected to the problem but not always obvious for example universally applicable uninformative prior distribution does not exist pericchi and walley 1991 although many use a uniform prior distribution for this purpose due to its simplicity pericchi and walley 1991 gelman 1996 gelman and yao 2020 wallach et al 2021 furthermore in spite of the fact that choosing the proper likelihood function is one of the most important decision to be made trudinger et al 2007 dumont et al 2014 modelers still usually use normal likelihood wallach et al 2021 however choosing the likelihood function family is still not sufficient to define the right priors or the global likelihood function there are several additional assumptions such as the independency of measurements and also the independency of the measurement and model errors tarantola 2005 the most frequent assumption by far is that gaining reliable inference about the input parameter space is possible by examining only the marginal posterior distributions there are few attempts to deal with the interdependence of the posterior distribution but these are usually examined pairwise or in a few cases by 3d scatter plots sadegh and vrugt 2013 beven and binley 2014 her and chaubey 2015 which clearly cannot capture higher dimensional relationships the assumptions about the sampling procedure are similar most of the proposed values for the parameters are sampled independently depending on the dimensionality of the parameter space probabilistic calibration is computationally expensive thus the methods which sample less while getting the most informative results are preferable if input parameter dependence is taken into account less simulations are needed bloom and williams 2015 for example if the model has two input parameters and the sum of the parameters must equal 1 the probability of getting the affordable parameter with random uniform sampling is virtually 0 because convex relationships are the most widespread using a sampling method designed specifically for convex cases can have a huge positive impact on the effectivity of the procedure otherwise every concave region can be split into disjoint convex regions where the input parameters can be sampled independently if the convex region is not a polytope it can be approximated with polytopes therefore techniques used to sample convex polytope regions are general enough to be good candidates to sample any parameter region one of these algorithms is the hit and run sampling lovász and vempala 2003 which according to our best knowledge has not been used for process based models yet at this point it is important to note that it is still an open question whether the bayesian method has advantages over the frequentist method even if bayesian methods are not used properly gelman and yao 2020 many studies focus on the practical application of different methods but we are very far from offering best practices or universally applicable solutions to the modeler community there are attempts to improve the success of optimization because of the complexity of the models and the related high dimensionality of the parameterization more and more observations are needed to confidently perform inverse modeling and most of the time the problem is still underdetermined sadegh and vrugt 2013 the modeler can theoretically tackle this problem by introducing additional information about the modelled system providing information about the relationship of the input parameters is a recently proposed new direction richardson et al 2010 bloom and williams 2015 this approach is based on the recognition that not every parameter combination is feasible based on the expert knowledge conditioning the input parameter space is not the only option to improve the results of model optimization process based models are inherently complex and produce more than one output streams calibrating models considering only one of these can result in good results for wrong reasons beven and binley 2014 as a consequence with the resulting parameterization the model can predict the given variable reasonably well on the training data but might fail on the validation data that is the typical case with overfitting the chance of overfitting is the highest if the parameters are associated with equifinality which means that in the predefined interval the likelihood of different parameter values is similar in the case of equifinality we cannot choose objectively from the posterior parameters which means that the parameter uncertainty cannot be constrained beven and freer 2001 equifinality does not necessarily mean failure of the simulation but rather it is a limitation of the inversion method one possible solution to avoid equifinality is the application of multi objective calibration by using multiple objective functions based on multiple observation data streams the result can be more stable with fewer parameters in equifinality her and seong 2018 however multi objective calibration needs good quality observation data streams to ensure a successful inversion because of the output stream interdependence adding one more data stream to the process can lead to a complicated workflow the noise of multiple data streams is not additive c f error covariance matrix in real life cases generally we do not have enough data for multi objective calibration or the data is too noisy in conjunction with multi objective calibration the modeler can use some rejection filtering techniques one of the main advantage of these approaches is the resilience against correlations between the different statistics of the model outputs hartig et al 2011 however applying rejection filtering alone does not shrink the parameter space and these methods cannot be easily interpreted hence the modelers who traditionally use trial and error approach with implicit rejection filtering do not have enough control over the inverse modeling procedure furthermore if the ratio of the filtered acceptable simulations is too low the convergence of the posterior sampling methods can be exceptionally slow or even questionable the complexity of the implementation of probabilistic methods and the possibility to get unusable results i e equifinality might be the main reasons why the majority of the researchers still prefer the trial and error method in model optimization wallach et al 2021 clearly there is a great need to develop interpretable easy to use and effective methods that support modelers to improve the optimization of the models especially in high dimensional cases in this study we introduce a novel method which can resolve some ongoing issues that are described above in probabilistic process based model optimization the method uses observations supplemented by expert knowledge on the simulated system unlike in case of the previously proposed methods richardson et al 2010 bloom and williams 2015 conditioning rejection filtering on the output data streams is introduced in our novel method a decision tree based algorithm is presented that reduces a priori input parameter intervals during the calibration of deterministic models and increases the reliability of simulation results of the calibrated model in this study we apply the glue method for its simplicity supported by an implementation of the hit and run algorithm our method can be easily combined with any bayesian or frequentist inverse modeling method and can contribute to improved and more efficient model optimization results in an array of scientific disciplines 2 material and methods 2 1 experimental data in the present study maize yield observations were used from martonvásár 47 19 56 97 n 18 47 50 61 e hungary central europe the climate of martonvásár is continental with mediterranean and oceanic influences mean annual temperature is 11 2 c while long term mean precipitation is around 550 mm according to the fao wrb classification system iuss working group 2015 the soil type of the area is a haplic chernozem with an average of 51 4 sand 34 silt and 14 6 clay content bulk density is 1 47 g cm 3 ph is 7 3 caco3 content is 0 1 and the mean soil organic matter content in the topsoil is 3 2 the plant available macronutrient supply in the soil is poor for phosphorus and medium to good for potassium based on the proplanta plant nutrition advisory system fodor et al 2011 in the ltfes every treatment is arranged in a random block design with 20 40 m2 plots in four replicates experimental data were collected in long term field experiments ltfe that were set up at the centre for agricultural research in the 1960s maize is grown in several ltfe sites within and around the city of martonvásár some of the experiments focus on the effect of organic and mineral fertilizer application on crop yield others focus on the effect of soil cultivation hybrid selection planting date and plant density on crop physiology and yield in the present study a composite maize yield dataset was used by retrieving the yield data of fao350 400 hybrids in high nitrogen level at least 160 kgn ha 1 year 1 treatments of four ltfes characteristic to the hungarian maize production system in the 1994 2018 period plant density 70 000 plant per hectare planting date second decade of april harvest date mid october the average yield was calculated from 16 yield data of the four selected ltfes with 4 repetitions in each treatment with the parcel size of 20 40 m2 for each year average maize yield calculated from all plots and all years in the experiments was 7 7 t ha 1 with a relatively large uncertainty sd 1 58 t ha 1 min 0 96 t ha 1 max 13 57 t ha 1 note that the observations refer to dry matter of the grain yield nuts3 level eurostat 2021 maize yield data from fejér county where the experimental site is located was also used in the study census data were retrieved from the database of the hungarian central statistical office for the period of 1991 2018 the average maize yield for the study period was 6 36 t ha 1 in fejér county sd 1 92 t ha 1 min 2 89 t ha 1 max 10 06 t ha 1 2 2 biome bgcmuso biogeochemical model the novel method introduced in the present study was linked with the biome bgcmuso process based model biome bgcmuso is a general purpose process based biogeochemical model that simulates the full carbon nitrogen and water budget of terrestrial ecosystems hidy et al 2012 2016 2021 2022 biome bgcmuso is a branch of the well known biome bgc model running and hunt 1993 thornton 1998 thornton et al 2002 churkina et al 2009 di vittorio et al 2010 biome bgc was significantly improved and extended in many terms relative to the original model developments addressed soil processes introduction of management options quantification of disturbance effect on plant physiology and many other processes one major milestone of the model development was the construction of a 10 layer soil submodule with sophisticated soil water balance routine the layer by layer representation of c and n dynamics within the soil and the implementation of detailed nitrification denitrification routine improvement of different stress factors drought stress nitrogen stress heat stress was also a major improvement growing degree day based phenophase specific allocation option was also included that enabled the detailed simulations of crops in cropland simulations detailed management information is essential for proper results including the timing and amount of applied fertilizer planting date soil cultivation type and date harvest date residue management the present biome bgcmuso model can be considered as a combined biogeochemical crop model with state of the art process representation at both scientific fields in this study biome bgcmuso v6 3 was used detailed description about the developments can be found in hidy et al 2012 2016 2022 fodor et al 2021 while additional details are available in the user s guide hidy et al 2021 the parameterization of biome bgcmuso is a complex task not only because of the high number of adjustable ecophysiological parameters but also because of the existence of some rules i e constraints that have to be fulfilled in order to ensure a successful simulation hidy et al 2021 for some parameters these rules are relatively simple e g c n ratio of leaf litter must be greater than the c n ratio of leaves but in some cases they are relatively complex e g the sum of the parameters that control the allocation of carbon to the different plant compartments must sum up to 1 these rules complicate the optimization of the model since in the monte carlo framework random numbers are generated within predefined intervals of selected parameters that drive the model clearly for example in the case of the allocation parameters random numbers will not sum up to 1 in the majority of the cases which will result in a large number of unsuccessful simulations 2 3 parameterization and model setup 2 3 1 a priori parameterization for the construction of the a priori maize ecophysiological parameterization literature search was conducted first for maize related data such as specific leaf area maximum stomatal conductance canopy light interception coefficient etc in the case of the empirical parameters hidy et al 2021 optimization was performed using a multi step procedure parameter adjustment was performed based on eddy covariance data gross primary production gpp and evapotranspiration et from the klingenberg cropland site de kli fluxnet code 50 53 35 n 13 31 20 6 e in germany prescher et al 2010 based on the maize years 2007 2012 and 2018 additionally leaf area index lai data was also used from the site for years when it was available phenological phase dependent allocation parameters were set based on a manual trial and error adjustment parameters for the penman monteith equation based evapotranspiration routine were also set using eddy covariance et data from klingenberg as part of previous modeling exercises maize parameterization was further evaluated previously and adjusted at other data rich experimental sites in the usa bushland lysimeter site in texas and mead eddy covariance site in nebraska us ne2 and us ne3 fluxnet codes it means that maize parameterization was tested for sites with different climatic conditions management types and with different maize cultivars characterized by a diversity of fao numbers the model optimization validation efforts revealed that it is not possible to construct a single universally applicable maize parameterization that is useable in different climatic and agro management conditions some of the parameters were highly site and in some cases year dependent senescence related parameter allocation parameters and plant tissue lifetime related parameters these previous experiences paved the way for the a priori parameterization of the model unfortunately at present there is no maize related eddy covariance data available for hungary in spite of the fact that 3 sites are running at present above croplands it means that optimization of the model is highly needed based on other available data 2 3 2 model setup biome bgcmuso was run at a plot level simulating a single generic maize parcel at martonvásár using 220 kgn ha 1 y 1 mineral fertilizer amount applied at the beginning of april planting and harvest dates were set according the reported dates driving meteorological data was retrieved from the foresee database open database for climate change related impact studies in central europe which is a free meteorological database for central europe with 0 1 0 1 spatial resolution dobor et al 2015 the soil input file of the model was constructed based on observations about the soil texture and soil water retention curve measurements using the pipette method iso 11277 on disturbed and sand kaolin box method iso 11274 on 100 cm3 undisturbed core samples collected from the ltfes for most of the nitrogen cycle parameters values proposed by hidy et al 2021 were used for the nuts3 level simulations the foresee database provided the meteorological data the dosoremi database pásztor et al 2020 was the source for the gridded soil data the simulations for the fejér county were performed on a predefined 0 1 0 1 resolution grid that was used in fodor et al 2021 fertilization was set according to data from the hungarian central statistical office planting data was 15 april and harvest date was 10 october in all simulations we used 51 grid cells covering an area of 4358 km2 the simulated yield data was aggregated at the county level by simple averaging of cell specific yields per year 2 4 description of the conditional interval reduction method 2 4 1 problem overview let m s o be an arbitrary process based model that estimates some output data o based on some input parameters θ s where s r n is the n dimensional input parameter space and o r l k is the matrix of model output where l is the number of time steps and k is the number of output data streams representing the simulated variables note that in this study the word parameter means adjustable input data in some cases convex constraints have to be defined over the s i e to handle dependencies between the input parameters see section 2 2 this can be done by introducing matrices g e and vectors e h for which 1 g θ h e θ e in the present study g controls the parameter ranges and the dependencies between parameters with relations i e one parameter must be smaller than the other one see section 2 2 e in this context is used for the allocation related parameters where the sum of all considered parameters must equal 1 hidy et al 2021 but of course the applicability of the constraint can be more general bloom and williams 2015 supplementary material contains the mathematical representation of the above described matrix based parameter dependencies with two examples inverse modeling has two major objectives tarantola 2005 the first one corresponds to the practical application of the model which needs proper parameterization to achieve reliable simulation results that are in optimal agreement with the observations this aim means that we need point estimation for the parameters single parameter set for all θ s the second one is the estimation of the uncertainty intervals of the parameters for future applications the intervals indicate our imperfect knowledge of the parameter values van oijen et el 2005 uncertainty intervals can be used to define uncertainty ranges of the simulated output variables given the observations d about the modelled system with predetermined uncertainties and the scientific knowledge of the environment the modeler can sample from a probability density function pdf for the model input parameters p θ d d d during the sampling procedure the model is evaluated against d this pdf is the so called posterior function and intuitively if the parameter space can be described by a continuous random variable it is the probability that θ lies in the interval of θ θ ε given the measured data d where ε is an infinitesimally small vector using the posterior distribution we can achieve both objectives defined above usually the posterior distribution cannot be sampled directly because its distribution is not known previous experience might be invalid in new optimization exercises van oijen et al 2005 however when the likelihood function l p d θ d d that is a probabilistic model for the model error gelman et al 1995 and the prior knowledge p θ called prior distribution is given during a monte carlo mc experiment the bayesian rule can be used for updating 2 p θ d p θ d p θ p d 3 p d p θ p θ d d θ c r because p d is a constant p θ d p θ d p θ usually in the frequently used markov chain monte carlo mcmc sampling p d is eliminated by a division after the detailed balance equation in a bayesian workflow it is rather typical that the modeler chooses a uniform prior p θ i u i i 1 i i 2 although this choice can be inadequate because uniform prior is not invariant under reparameterization therefore sensitive to the choice of its defining lower i i 1 and upper bounds i i 2 from the practical point of view choosing the right prior can help to avoid overfitting by regularizing the likelihood functions using uniform prior adds no regularization thus the maximum posterior values will be the maximum likelihoods leading to inappropriately strong conclusions gelman 1996 in the case of high amount of observation data the prior choice is less important and the maximum likelihood estimates are approximately the same as the maximum posterior estimates independently from the choice of the prior function despite the obvious drawbacks uniform priors are frequently used in bayesian frameworks because of their simplicity and because of the fact that it is easier to think about the parameters in a bounded space gelman 1996 stedinger et al 2008 gelman and yao 2020 wallach et al 2021 one other possible way to avoid inappropriately strong conclusions is to use other methods of regularization for example in the generalized likelihood uncertainty estimation method glue prihodko et al 2008 beven and binley 2014 likelihood filtering is applied only the likelihood values above the previously determined quantile 95th percentile is common choice are retained and considered as behavioral and the optimum is considered as the median of the filtered parameters the key advantage of this approach is its simplicity and flexibility and some degree of resistance against overfitting note that the method is sensitive to the choice of the quantile meta parameter this method is not considered as a bayesian one because we cannot interpret the results as probabilities it means that we do not have the proper posterior density function albeit it can be easily extended to be an approximate bayesian computation abc method sadegh and vrugt 2013 in this paper a novel method is introduced that can be used for updating prior distributions here we used the glue method for demonstration purposes and for simplicity exploiting also the typical visualization method for glue in the form of the so called dotty plots where equifinality can easily be recognized based on the marginal distributions in the following section a detailed description of the new method is provided 2 4 2 core logic of the output constraint approach let f o 0 1 be a function where f m categorizes model results into two classes feasible 1 and infeasible 0 output feasible in this context means that o is in accordance with expectations of the modeler we would like to stress that feasibility is not judged directly based on the quantitative comparison of the observation and the simulation but rather it is based on some additional knowledge about the simulated system this kind of knowledge can originate from the scientific literature from almanacs or from the everyday practice of the modeler the model has to be set so that o contains information about the process that is evaluated by f for a given θ parameter 4 θ s is feasible f m θ 1 for simplicity f is called the output conditioning or filtering function as we are interested only in parameter values that provide feasible output hereafter referred as feasible parameters f is used to filter out infeasible parameter combinations constraining the sampling procedure from the initial set although the classifier can help filtering out unrealistic simulations the ratio of the number of good simulations compared to all simulations c r can be low in some cases this ratio is defined as 5 c r f e a s i b l e s i m u l a t i o n s a l l s i m u l a t i o n s this means that the parameter intervals defined by the posterior distribution contain a lot of parameter values that represent good results for wrong reasons which means that we cannot use the whole distribution for gaining scientific knowledge because of the lack of confidence in getting feasible and stable results we need a further inspection of our parameter values and understand why we get a lot of infeasible results note that from this point parameter values associated with infeasible results are referred as infeasible parameters in such cases the researcher has to make an effort by manually looking into the parameter intervals and the input parameters to search for possible mistakes on setting the prior intervals or running more simulations to get more feasible output values in our approach f m is considered as a black box classifier of the input parameter space which is one of the most fundamental recognitions of the presented algorithm if we approximate f m with a white box classifier g the interpretation of g is isomorphic with the interpretation of f m isomorphic in this context means that decisions made by the white box classifier are the same as those provided by the black box classifier thus we can use g to modify the prior input parameter intervals to achieve the high c r the simplest yet still flexible and interpretable white box classifiers are decision trees dt this is the reason why they have recently been used for interpreting the decision making procedure of deep neural networks and support vector machines di castro and bertini 2019 lee and kim 2016 decision trees used for classification can capture complex relationships between the feature space and the output categories i e discriminate feasible and infeasible parameter values as a result they are not much sensitive to collinearities and they can make complex relationships easily interpretable too in our method the so called cart algorithm james et al 2013 was used to generate the trees because of its simplicity note that other algorithms such as id3 c4 5 c5 0 james et al 2013 could have been also used gini impurity index gi was used to quantify the impurity of a split in making the dt while the complexity based pruning was used to avoid overfitting nobel 2002 after creating the dt the accuracy a was determined in the following way 6 a t p t n t p t n f p f n where tp means number of true positive cases fn means number of false negative cases fp means number of cases identified as false positive while tn means number of true negative decision for the f m low accuracy a 0 5 means that the model performance was worse than the random decision in the case of low performance a 0 7 the algorithm is not applicable to parameter interval change if the accuracy is high we can trust our white box model since it behaves similarly as the black box the leaf nodes of a decision tree are continuous domains in the parameter space defined by its corresponding decision nodes if r 1 r 2 r b are the input domains for leaf nodes y 1 y 2 y b we are interested in the domains with the highest number of feasible parameters if there is more than one region with the highest number of feasible parameter values we can have 3 possibilities to decide which one is the region of interest 1 chose the region with the least amount of connected decision nodes and apply the validation to this region 2 chose the region with the lowest fp rate and apply the validation to this region 3 choose both above mentioned regions and apply validation in the next step with this method the selected region ru is defined by the decisions at the decision nodes the region is the maximum volume hyperrectangle inscribed into the constrained region rc sides parallel to the parameter vectors over the original parameter region ro fig 1 the updated parameter intervals are the orthogonal segments which define the hyperrectangle if we have multiple output constraint functions we create multiple dts and go through the updating process sequentially if there is an inconclusive update we roll back the changes and continue the procedure in the next step we can shrink the intervals further if there is more information available for that region the intervals specified by applying individual constraints must overlap to provide updated parameter intervals for the next step note that since in this paper the issue with the similar sized feasible region was not addressed the above mentioned solution is not part of the current implementation 2 4 3 model optimization here we combine the glue frequentist model optimization method prihodko et al 2008 stedinger et al 2008 beven and binley 2014 with the application of the above described dt based classification method we named this procedure as conditional interval refinement method cirm it is easy to extend the f m classifier to support multiple output constraint functions if we have m output constraint functions f i i 1 m we can define f as the following 7 f x i 1 m f i x after the parameter sampling and model simulations behavioral parameter choice can be done with quantile filtering described above with an additional filtering according to the conditioning retaining feasible parameter values where f x 1 it is assumed that the modelization uncertainties are negligible here compared to observational uncertainties and observational uncertainties follow a normal distribution furthermore if the observations are independent from each other the likelihood function is defined as 8 l θ d d i 1 n d 1 σ 2 π e 1 2 m θ d i σ 2 where σ refers to observation uncertainty for practical considerations e g arithmetic underflow we used the loglikelihood instead of the likelihood note that in this case the maxima places are unchanged for the prior function uniform prior on convex polytope was used in order to support successful monte carlo experiment a novel algorithm was developed and applied here based on the hit and run algorithm with mirroring optimization lovász and vempala 2003 meersche et al 2009 note that model optimization with glue or with any other optimization method can be applied at each step of the proposed approach it is mandatory to perform it only for the last step 2 4 4 summary of the proposed method algorithm 1 presents the proposed constrained calibration method the workflow summarizes the methods detailed above also revealing the consecutive steps with all related input and output data in this paper for demonstration purposes we performed glue in each iteration step algorithm 1 workflow of the proposed method including model optimization and parameter interval update meaning of the symbols is defined in the text image 1 2 5 implementation of the method we used the above proposed method to optimize biome bgcmuso v6 3 for maize in a low data situation which means low amount of good quality observation data when only final crop yield data was available supplemented with some additional information about overall crop properties that represent constraints biome bgcmuso has 120 maize related ecophysiological parameters that have to be set by the modeler prior to the simulations some of the parameters are generic plant parameters like c n ratio of plant compartments maximum stomatal conductance maximum rooting depth root distribution parameter canopy water interception coefficient etc while some of them are specific to maize parameters affecting e g heat stress during anthesis germination as the function of soil water content etc hidy et al 2021 among the 120 parameters 42 are affected by some rules see above 28 out of the 42 rule affected parameters are related to allocation similarly to other models that are characterized with a high number of parameters e g bilionis et al 2015 it is impossible to optimize the model for all parameters instead we used previous experience and the outcome of several previous model evaluations for the parameter selection and parameter setting during previous model optimization efforts see methods the selection of the most relevant parameter values were done partly using objective sensitivity analysis and partly by manual parameter adjustment many parameters were fixed because of available observation data like leaf stem and fine root c n ratios and the results of experience at the german and usa experimental sites maximum stomatal conductance canopy light extinction coefficient etc the remaining parameters were found to be variable between the sites and the model showed sensitivity to the proper setting of the parameters see table 1 for a full list for the selected parameters prior to model optimization the upper and lower bounds were set based on literature values stöckle and nelson 2013 white et al 2000 expert knowledge of the co authors and previous experience was also used to set the intervals table 1 note that the complete prior ecophysiological parameterization for the simulations of maize is presented in the supplementary material for the practical implementation of the procedure with biome bgcmuso at the martonvásár ltfe site we used a uniform distribution as prior over convex polytope we used a normal likelihood function eq 8 and we assumed that modelization uncertainties are negligible compared to observational uncertainties we used 0 01 for the complexity based pruning factor for the dt as it was the default value in the rpart package therneau et al 2022 the white box model was the decision tree with cart as described above we used 4 output constraint functions f to evaluate feasible or infeasible simulations the first constraint is related to the annual harvest index hi that is defined by the ratio of the final grain yield and total aboveground biomass at harvest goudriaan et al 2001 the median of the simulated hi values is required to be in the range of 0 40 0 55 defined based on several scientific publications hütsch and schubert 2018 ion et al 2015 li et al 2015 2017 liu et al 2020 the median of the annual maximum leaf area index laimax is requested to be between 2 7 and 5 m2 m2 that is an observation based setting for hungary see e g pokovai and fodor 2019 the long term median value of the rooting depth at the beginning of the flowering phenophase should be larger than 1 4 m but should be less than 1 8 m those values are based on expert knowledge the median of anthesis days expressed in day of year doy should be between 180 and 190 that is a typical range for hungary the output constraint functions are defined by a simple algorithm based on the model outputs for example if annual laimax is a vector containing annual maximum values of the biome bgcmuso simulated lai f is defined as 9 f x 1 m e d l a i m a x 2 7 5 0 o t h e r w i s e where med represents the median of the values from the simulated years during the point simulation biome bgcmuso simulations were performed within the rbbgcmuso software environment https github com hollorol rbbgcmuso rbbgcmuso is an open source r package supporting the easy and user friendly application of the model we used the cirm branch of rbbgcmuso for the case study presented in this paper the output conditioning method was implemented using 10 iterative steps in each step the automated algorithm performed post processing of the decision trees using the method described above i e the interval refinement was done automatically taking into account the overlaps in the calculated thresholds step 4 in algorithm 1 10 000 simulations were done for each iteration step standard r was used mostly in the work r core team 2021 additionally the rpart package was used to construct the decision trees therneau et al 2022 in each iteration step the maximum likelihood parameter set was stored a posteriori parameter intervals were estimated based on behavioral top 5 parameter values that matched the predefined conditioning during the first 9 iteration steps parameter interval reduction was performed based on the decision tree update algorithm glue based a posteriori interval reduction was considered only in the final iteration step optimum parameter set was calculated as the median of the behavioral parameters here we refer to these intervals and optimum parameter set as glue based constrained intervals optimized model performance was evaluated using the square of the linear correlation coefficient r2 bias systematic error root mean square error rmse and nash sutcliffe modelling efficiency me ma et al 2011 sándor et al 2016 the performance indicators were calculated based on the observed and the simulated maize yield time series the optimized model was evaluated based on the training dataset and also on the nuts3 level simulation using independent data in this latter case mean annual maize yield was calculated from the model simulations and the final time series was evaluated against the observed census data we also quantified the percent of change in the final glue parameter intervals relative to the a priori intervals 3 results 3 1 glue and post processing via the dts fig 2 shows selected dotty plots from the first iteration step from the 20 studied parameters table 1 we selected 4 that represent typical patterns characteristic to all cases from the final 10th iteration step see below supplementary material fig s1 shows the complete set of dotty plots for the first step the most prominent feature of the dotty plots is equifinality based on the grey dots that represent all simulations this is the case for almost all other parameters that are presented in fig s1 there are a few exceptions where some pattern can be recognized on the graphs maxlifetime4 leaf allocation 3 leaf allocation 4 stem allocation 3 and rubisco to some extent fig s1 but the distribution of the grey dots support only a slight interval reduction for e g stem allocation 3 leaf allocation 4 for other parameters such as maxlifetime4 even if some pattern can be recognized in the dotty plots posterior intervals based on the behavioral grey dots will not result in parameter interval reduction note that in a typical bayesian calibration this is the final stage of the optimization which is clearly not satisfying and unsuccessful in terms of pursuing an interval reduction after we decided to check the consistency of the results based on the predefined constraints using hi laimax anthesis date and rooting depth the overall picture changed fig 2 fig s1 red dots however likely given the large degree of freedom of the optimization after the first iteration only 596 simulations were feasible which is clearly a low success rate cr 5 96 with the output constraint filtering the dotty plots still show equifinality for most of the cases there are some exceptions like rubisco maxlifetime4 leaf allocation 3 root allocation 3 stem allocation 3 leaf allocation 4 root allocation 4 fruit allocation 6 and stem allocation 6 for which the algorithm based on constraints suggests that they should have narrower ranges fig 2 fig s1 for example in the case of rubisco the parameter range 0 7 0 11 seems to be reasonable based on the red dots we would like to stress here again that in a typical optimization exercise the user neglects some or all above mentioned constraints and conclusions are drawn based on the grey dots only in this sense information extracted from the feasible simulations is already one step forward one can recognize that because of the small c r we do not have a sufficient number of feasible simulations to trust the goodness of the glue optimum values and the glue uncertainty ranges in order to increase c r further refinement of the parameter intervals is a reasonable next step the constructed decision trees provide information about the possible relationships between the parameters and the feasible infeasible simulations thus they can be useful for updating the parameter intervals given the fact that we have 4 output constraints 4 dts were constructed fig 3 shows the dt that was constructed based on the first constraint hi after iteration step 1 the top level of the dt is called the root node in the case of fig 3 this is represented by stem allocation 6 this top level always shows the most important parameter that affects the feasibility of the simulation in this case in terms of hi the lower part of the dt is divided into different layers or levels where internal nodes are located the importance of the parameters associated with the different internal nodes is decreasing with the layer number i e the importance decreases from top to bottom internal nodes represent additional decisions revealing the parameter value that splits the parameter range into two parts depending on the feasible infeasible character of given simulations for example at layer 2 the cutting threshold value for the length of phenophase 4 parameter is 372 if the parameter is less than 372 then we can reach the right branch with the rest of the levels at the bottom of the dt the leaf nodes are located leaf nodes represent the results of the classification due to the homogeneity of the leaf nodes in fig 3 blue leaf nodes i e rounded squares at the bottom marked with 0 which is the result of f m indicate infeasible simulations while green leaf nodes represent feasible simulations marked with 1 the percentage values inside the leaf nodes show the fraction of simulations that is associated with a given branch according to fig 3 due to the applied hi constraint 58 of the sampled parameter combinations were infeasible sum of the percentages in the blue boxes as it was explained in section 2 4 2 in our approach we always focus on the leaf node with the highest percentage of feasible simulations in this sense the dt suggests that the most important parameters associated with the hi constraint in the decreasing order are stem allocation 6 length of phenophase 4 length of phenophase 3 and maxlitefime 4 this is the path from the top to the leaf node with the highest after excluding the repeated occurrence of the parameters in the lower layers of dt those parameters indirectly affect grain allocation in the model and thus hi it is somewhat surprising that fruit allocation 6 parameter is not included in the dt in this path but it is included in other paths leading to other leaf nodes maxlifetime4 affects leaf senescence dynamics prior to and during grain filling thus interacts with hi the lengths of phenophases 3 and 4 affect the leaf dynamics that clearly influence assimilation thus grain allocation during the 6th phenophase the information gained from the dt is essentially useful and provides insights into the complex process of plant growth and final yield that is implemented in biome bgcmuso based on the decisions across the path from top to the leaf node associated with the largest success rate 21 rightmost leaf node we can extract the thresholds and update the original parameter intervals see table 1 stem allocation in the 6th phenophase should be less than 0 31 and greater than 0 16 the original interval was 0 1 0 4 see table 1 the length of the 3rd phenophase should be smaller than 405 and the length of the 4th phenophase should be smaller than 372 maxlifetime4 should be set larger than 693 note that at any time of the procedure a user might select a different leaf node with a lower success rate if the expert knowledge supports an alternative choice in this case manual adjustment might be needed in the parameter ranges and the iteration should be restarted using the adjusted intervals fig 4 shows the dt for the laimax constraint after iteration step 1 the figure shows that 58 of the sampled parameter combinations is infeasible the tree suggests that the most important parameters in a decreasing order are rubisco leaf allocation 4 and length of phenophase3 at lower layers rubisco and leaf allocation 4 appear again as rubisco ultimately controls the maximum photosynthesis rate white et al 2000 its importance is straightforward in terms of leaf development the role of leaf allocation in the 4th phenophase which determines the peak lai is also clear and easily interpretable the length of the 3rd phenophase is less intuitive but it is reasonable since it affects the initial condition of leaf development in the 4th phenophase when lai reaches its maximum using the dt and the path to the rightmost leaf node that is associated with the highest success rate with 16 we can set new intervals for the parameters based on all decision nodes from the dt in fig 4 rubisco should be less than 0 097 and greater than 0 079 leaf allocation 4 should be less than 0 34 and length of phenophase3 should be larger than 296 note that the dt presented in fig 3 already set a new upper limit for length of phenophase3 which is further refined here fig 5 presents the dt for the rooting depth constraint based on the results from the 1st iteration step in this case only 29 of the sampled parameter combinations were infeasible the tree suggests that the most important related parameters are rubisco and length of phenophase 3 this is reasonable considering the determinant role of rubisco in terms of overall productivity and considering the importance of the 3rd phenophase in terms of root establishment rubisco interval can be further refined here it should be larger than 0 09 and according to the previous tree in fig 4 it should be less than 0 097 length of phenophase3 should be greater than 294 which is in fact not used at this stage as it was already set to be larger than 296 fig 4 fig 6 presents the dt for the last anthesis date based constraint using the anthesis date dt 42 of the sampled parameter combinations turned out to be infeasible the tree suggests that the most important parameters are the length of phenophases 3 and 4 this is in perfect agreement with the expectations as the anthesis date is driven by the length of the previous phenophases expressed in gdd note that the length of the first two phenophases is fixed in the simulations the dt provides guidelines for updating the two parameters according to the path leading to the rightmost leaf node length of phenophase3 should be set larger than 318 and length of phenophase4 should be larger than 313 these new settings further constrain these parameters as they were already refined to some extent at this stage the most important recognition is the usefulness of the dts that can be used for manual updating of the parameter intervals this kind of information was hidden so far as the marginal distributions did not reveal parameter interval reduction possibilities after performing the dt analyses of individual constraints the conditions from individual dt are combined and the parameter intervals are modified 3 2 results of the iterations and automatic interpretation of the dts one might recognize at this point that the new interval settings might be used as the new prior of another monte carlo based glue experiment as it was described above a custom procedure was developed to automatically interpret the dts by finding the path to the leaf node that contains the highest percentage of feasible simulations based on this method the above described interval refinement became unattended if the next iterations turn out to provide better success rates and if the parameter interval reduction can be further refined the procedure can be transformed into a multi step iterative method in our case after the first iteration step another nine were performed during the iterations the success rate fig 7 a increased monotonically indicating that the introduced algorithm worked well on improving the success rate as fig 7b shows the white box approximation was also correct because on average the approximation s accuracy monotonically increased although it can be attributed to the large success rate after step 4 supplementary material fig s2 shows the complete set of dotty plots for the 10th iteration step fig 8 shows selected dotty plots that represent typical patterns from the final step the graph is in accordance with fig 7 revealing that almost all simulations were feasible at this stage grey dots are hardly detectable although most of the parameters still show equifinality the parameter intervals are considerably smaller than after the first step fig 2 fig s1 some of the parameters provide well detectable optimum e g max lifetime 4 at fig 8 with typical parameter distribution other parameters show well bounded points like in case of stem allocation 4 parameters controlling the length of the phenophases 3 and 4 show an unusual distribution that is the clear consequence of the cutoff values defined by the dts see above in those plots the behavioral parameters localize the optimum value after the 10th iteration step almost all sampled parameters were feasible 95 45 i e 9545 iterations out of 10 000 it means that almost all simulations satisfied the predefined constraints thus provide results according to the expectations of the user given the large number of successful and meaningful simulations the high sample number clearly improves the confidence of the user about the statistical properties of the results most of all the optimum and the uncertainty ranges considering the optimum values of the parameters represented by vertical lines in fig 8 the maximum likelihood values typically differed from those calculated from the behavioral data i e glue median similarly to step 1 fig 2 fig s1 fig 9 shows the summary of the inversion in the form of a special plot type that is referred to as kitchen sink plot the figure provides easily interpretable information about the multiple step iteration in terms of dt based parameter interval reduction as a function of iteration step number and it also shows the position of the maximum likelihood estimation relative to the parameter ranges note that in the 10th iteration step the glue based interval reduction was also considered the glue based optimum is not indicated but this can be approximated by the mid point of the actual interval fig 9 clearly shows that parameter intervals were significantly reduced in many cases for some parameters such as rubisco and length of phenophase4 interval reduction was more profound in the first iteration step for other parameters such as root depth function shape leaf allocation 3 and stem allocation 6 the reduction was gradual and not necessarily associated with one iteration step this indicates the utility of the multiple step approach in case of some parameters the prior interval remained unchanged e g root allocation 3 stem allocation 3 the plot also shows that in some cases the ml value was out of the final parameter interval i e in step 10 that indicates an infeasible solution as the first iteration step is the one which is performed in a usual bayesian experiment this clearly shows the good results for wrong reasons situation table 2 summarizes the model optimization exercise providing information about the posterior parameter intervals the parameter set representing the maximum likelihood estimation in the final step and the glue based parameter ranges average interval percentage change was 44 for the 20 studied parameters the maximum was 88 associated with rubisco parameter range reduction did not occur for length of phenophase 6 root allocation 3 stem allocation 3 root allocation 6 and maxlifetime 3 stem and root allocation in the 3rd phenophase seem to be less determinant in terms of the final crop yield which is an interesting outcome the interpretation of this parameter behavior from the 6th phenophase is easier estimation of the exact length of this phenophase might be impossible since after leaf senescence during grain filling the final crop yield cannot change anymore so its value cannot be set by any observation or constraint root allocation in the 6th phenophase is small and seems to have no substantial effect on the final crop yield table 2 suggests that additional constraints might support the reduction of the intervals in the parameters where 0 reduction is present note that in some cases this is not a problem as the other dependent parameters already set the value for these parameters like in case of the allocation when the sum of the parameters must sum up to 1 overall the parameter interval reduction have led to 42 3 decrease in the simulated yield uncertainty quantified by the mean of the annually calculated standard deviation of the modeling results based on 1000 simulations performed by monte carlo based sampling from the original and reduced parameter ranges fig s3 in the supplementart material 3 3 performance analysis on the calibration dataset and validation fig 10 shows the model results for the prior parameterization and for the glue based optimized parameter set uncertainty of the observations is high given that the maize yield dataset is a composite of many small experimental plots see methods the figure indicates that in many cases the optimized model estimated yield within the uncertainty ranges improvements were quantified by using statistical indicators table 3 shows the results of the statistical evaluation of the simulations here we include performance metrics from the maximum likelihood simulations as well the table shows considerable improvements of the quality of the simulations in the consecutive iterations steps the explained variance was typically higher for the maximum likelihood parameterization than for glue for maximum likelihood r2 substantially increased already after 1st iteration while this was achieved only after the 6th iteration for glue rmse was lower for the maximum likelihood than for glue exception is step 6 the bias was variable but basically became close to 0 in the case of maximum likelihood while remained positive for glue me was better in the case of the maximum likelihood parameterization than in the case of glue in summary all parameter values show better performance in the case of the maximum likelihood parameterization which might indicate overfitting table 4 shows the performance metrics of the model output for maize yield at martonvásár and for the independent validation experiment nuts3 level model simulation both for the calibration set martonvásár and the validation set the optimized simulation results were significantly closer to the observations than the a priori simulations it means that the calibrated model is more appropriate to apply to the nuts3 level independent dataset maximum likelihood parameterization from the 10th step over performed the glue based simulations in terms of bias and me while the difference between the rmse values was small in terms of r2 the glue based method performed better the explained variance 48 was in fact higher here than in the case of the training dataset for glue note that me was negative in both cases because the magnitude of the yield was different at martonvásár and at the county level likely due to the different n fertilization level and agrotechnology the better performance of glue in the validation experiment in terms of r2 indicates that maximum likelihood was associated with over fitting and the glue based method might be more feasible note that at this stage the modeler might perform a hybrid parameterization using the maximum likelihood values for some parameters that are associated with small or zero interval reduction and glue based more constrained parameters in other cases maximum likelihood values for some parameters might be informative if there is consensus in their values across the multiple iterations steps note that the low explained variance does not necessary mean bad simulation in our case the likelihood optimum function was normal so during the training procedure the main goal was to minimize the error and not to maximize r2 3 4 limitations and outlook like all optimization methods cirm also has limitations the method assumes that the dt white box model is an adequate approximation to the black box model m f similarly to any machine learning classifiers dts have problems with unbalanced datasets because these are heavily biased towards the majority classes hoens and chawla 2013 however quantifying the goodness of the approximation can be accomplished easily by checking the accuracy of the dt while simultaneously checking the success rate if the success rate monotonically increases while the accuracy is above a predefined threshold e g 0 7 is considered high enough the dt approximation is considered adequate through the updating procedure another problem is that the dts are prone to overfitting traditionally the solution for the overfitting is the application of ensemble methods such as random forest or adaboost james et al 2013 however applying these solutions may result in loss of interpretability and might complicate the interval update algorithm additional research should focus on this issue in the future another direction of development can focus on the application of other performance metrics such as recall that uses tp tp fn for the decision trees that are more suitable for unbalanced datasets in this paper dts are trained on the full available dataset because the success rate calculation and the accuracy value in the consecutive iteration can be considered as a simple validation in this study the introduction of the cirm method was done using uniform priors coupled with the widely used glue probabilistic method cirm can be used with other priors and also with other probabilistic model optimization methods in such cases the prior update rule has to be defined based on the dts for example for normal prior distributions μ and σ parameters can be determined in a way that the 0 95 highest density interval s endpoints are generated by the dt update rule as defined earlier this method is applicable for every two parameter unimodal distribution function e g beta distribution it is also important to emphasize that there are alternatives to the interval update algorithm while processing the dt see section 2 4 2 the algorithm presented here selects the maximal volume inner hyperrectangle inscribed into the resulting polytope described by the dt fig 1 alternatively the bounding hyperrectangle could also be used further research is needed to compare these two simple alternatives and see their effect on the results in this study the demonstration of the introduced new method was done at a single site with few available observations in spite of the poor situation the method could provide useful results which means potential in other similar or more data rich cases most importantly the method has to be tested at experimental sites equipped with eddy covariance measurements and ancillary measurements data retrieved from the try database can be included in the model optimization with predetermined ranges for some plant traits kattge et al 2011 according to the introduced method it is possible to handle observations as additional constraints together with another observation data stream that is used for the construction of the likelihood function 4 concluding remarks in this study we presented a novel approach for the inversion of process based models that goes beyond the traditional probabilistic methods although the basic aim is unchanged i e constraining parameter uncertainty the method is markedly different and can be best described as a combination of the traditional probabilistic methods and the application of an interpretable machine learning method up to the knowledge of the authors there is no similar approach published in the literature although the concept of reality constraints as part of the model optimization is already introduced in the literature the previously proposed methods deal exclusively with the input model parameter constraints and no conditioning is present regarding the output data streams note that input data conditioning is an implicit feature of our procedure and is implemented in the rbbgcmuso package that was used to execute the optimization the so called bayesian filtering technique shows some similarity to our method this method implements conditioning of the output streams but as their convergence speed is proportional to the ratio of realistic all simulations they can be very slow additionally this method requires in depth knowledge from the user regarding the underlying inter dependencies of the parameter space interpretability is a remarkable advantage of the cirm method in this regard the output conditioning introduced in this study is a novel technique another advantage of the proposed method is that it has two modes a manual and an automatic one in the manual mode the user can examine the relationships represented by the constructed dts and perform the interval refinement accordingly or the proposed refinement can be accepted or modified based on additional scientific knowledge the automatic interval refinement needs no supervision in this way this method could be a potential candidate for researchers who used the trial and error approach for model inversion so far the new method requires minimal extra knowledge about the technical implementation of the method from the user and as the method is fully automatic inexperienced researchers might still use it successfully concentrating mainly on the scientific questions the focus of the study was a low data situation when the model had to be optimized against maize yield data with relatively large uncertainty the study demonstrated that the traditional probabilistic method glue resulted in unconstrained parameter intervals suggesting that the low data situation leads to a large uncertainty of optimized parameter values this problem was solved by applying the proposed cirm method that eventually led to successful interval reduction with almost 100 realistic simulation score before the calibration only 6 of the simulations were acceptable despite all of the achievements and proposed solutions in the field of model optimization reality is sobering most of the scientists still use trial and error model optimization wallach et al 2021 and the minority who preferred some probabilistic methods typically use only prior knowledge there is a clear and well recognized need for simple easy to use and interpretable software solutions that can be used for parameter estimation of a wide array of process based models cirm might represent a remarkable major step forward to support improved model optimization and application given the fact that cirm is a model independent method it can be easily implemented in any modelling environment software availability the rbbgcmuso package that is used in the study is available at github https github com hollorol rbbgcmuso tree cirm biome bgcmuso is available at the website of the model http nimbus elte hu bbgc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was funded by the széchenyi 2020 programme the european regional development fund and the hungarian government ginop 2 3 2 15 2016 00028 by the national multidisciplinary laboratory for climate change rrf 2 3 1 21 2022 00014 project and by grant advanced research supporting the forestry and wood processing sector s adaptation to global change and the 4th industrial revolution no cz 02 1 01 0 0 0 0 16 019 0000803 financed by op rde also supported by the integrated infrastructure operational programme funded by the erdf project scientific support of climate change adaptation in agriculture and mitigation of soil degradation grant no itms2014 313011w580 the klingenberg ec measurements have been funded by eu project carboeurope ip goce ct 2003 505572 bmbf for icos d implementation 1lk1101a 1lk1101b and freistaat sachsen 1 0230 00 68 192 2016 118972 1 0452 147 158 65 0456 49 10 65 0456 46 28 we thank andy suyker steve evett and bruce kimball for providing us the bushland and mead experimental data for benchmarking the model appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105556 
25502,uncertainties in environmental decisions are large but resolving them is costly we provide a framework for value of information voi analysis to identify key predictive uncertainties in a decision model the approach addresses characteristics that complicate this analysis in environmental management dependencies in the probability distributions of predictions trade offs between multiple objectives and divergent stakeholder perspectives for a coral reef fisheries case we predict ecosystem and fisheries trajectories given different management alternatives with an agent based model we evaluate the uncertain predictions with preference models based on utility theory to find optimal alternatives for stakeholders using the expected value of partially perfect information evppi we measure how relevant resolving uncertainty for various decision attributes is the voi depends on the stakeholder preferences but not directly on the width of an attribute s probability distribution our approach helps reduce costs in structured decision making processes by prioritizing data collection efforts graphical abstract keywords sensitivity analysis multi criteria decision analysis uncertainty agent based modeling coral reef management data and software availability the workflow to reproduce the analysis is available at https doi org 10 5281 zenodo 7156065 it depends on two submodules the seamancore simulation model https doi org 10 5281 zenodo 7155783 and an r package with functions for voi analysis https doi org 10 5281 zenodo 7155948 the dataset of simulation results from seamancore used in the analysis is available at https doi org 10 5281 zenodo 7156015 1 introduction environmental management decisions need to be made in the face of large uncertainties confronted with these uncertainties an intuitive response is to collect additional information on the reasonable assumption that more information will reduce uncertainty this in turn may improve our understanding and ultimately lead to better management decisions however collecting more information requires time and effort which could otherwise be allocated to management actions particularly in countries with limited resources for environmental management when we know enough about a system to make a sensible decision requesting more science rather becomes a technique to delay implementation gregory et al 2006 faced with the trade off between researching and managing an ecosystem it is important to consider to what extent is the use of limited resources to improve our understanding of a system justified by the resulting potential for improved management this question can be approached with the concept of value of information howard 1966 feltham 1968 value of information voi analysis allows us to determine the value in the sense of benefit or utility of additional information for deciding between different alternatives we can think of voi analysis as a form of sensitivity analysis with a focus on the sensitivity of the decision to new information rather than the magnitude of variation in the outcomes borgonovo et al 2016 felli and hazen 1998 razavi et al 2021 the analysis supports us in answering the questions would our decision for a specific management alternative change if we had more information and if so which information would most influence our choice or conversely resolving which of the uncertainties would bring more utility based on the answers we can rationally prioritize between research monitoring and implementation activities for a concrete decision tackling complex decisions in a rational way and conducting voi analysis is facilitated by a quantitative representation of the decision and its uncertainties see section 2 1 this is achieved by a decision model which consists of a predictive system model and a preference model reichert et al 2015 haag et al 2019b the predictive system model also called prediction model system model assessment model or simulation model in the literature provides measures of various system attributes e g coral cover crop yield breeding pairs etc given an input state this allows us to assess the potential consequences if we implemented different management alternatives also called options variants actions strategies or scenarios in the literature the preference model also called evaluation or utility model represents how decision makers stakeholders or society perceive and evaluate the predicted consequences of the system state in this study we focus on quantitative preference models based on multi attribute utility theory keeney and raiffa 1993 value of information analysis is an established approach in health care economics and medical decision making e g felli and hazen 1998 fumie and thompson 2004 jackson et al 2022 there is also growing interest in conservation biology reviewed by bolam et al 2019 and environmental management in general keisler et al 2014 eidsvik et al 2015 however many environmental decision problems share characteristics that complicate voi analysis and at the same time make it especially relevant for these problems in this paper we address two research gaps for voi analysis that emerge from the complexity of environmental decisions first we lack an integrative framework for jointly tackling common characteristics of such decisions including 1 multiple conflicting management objectives management criteria 2 uncertain consequences of management alternatives with continuous distributions rather than discrete states or hypotheses and 3 probability distributions of consequences that are not independent from each other multi criteria decision approaches mcda mcdm have been used with voi analysis before e g bates et al 2014 runge et al 2011 eidsvik et al 2015 but mostly have addressed these aspects separately see bolam et al 2019 voi analysis for continuous probability distribution remains conceptually straightforward but can become challenging for conditionally dependent probability distributions e g eidsvik et al 2015 myklebust et al 2020 second multiple conflicting perspectives on an environmental decision problem commonly exist however the consideration of multiple stakeholder perspectives in voi analysis has not been discussed widely in the literature before we address this issue and investigate the influence of diverging stakeholder perspectives on the results and conclusions from voi analysis the aim of this paper is to develop an integrated approach to conducting voi analysis given the characteristics of environmental decisions and highlight relevant considerations when applying the method the following questions guide our approach 1 how can we estimate the voi based on the correlated output of complex system models 2 how to analyze voi in decision contexts with multiple conflicting objectives 3 what is the influence of diverging stakeholder perspectives on the conclusions of voi analysis to address these questions we create a framework to conduct voi analysis for complex decisions bringing together elements that have been developed previously our voi analysis rests upon a decision model that combines system predictions and stakeholder preferences see above and section 2 1 the predictions can come from one or more arbitrarily complex models and can exhibit dependencies in their distributions to efficiently conduct voi analysis for dependent probability distributions we adapt an algorithm by strong and oakley 2013 as a replacement for traditional double loop monte carlo procedures while different measures of the voi have been proposed see eidsvik et al 2015 and section 2 2 we use the expected value of partially perfect information evppi also called evpxi or evxi this measures the expected gain in utility if we would decide based on perfect information about one or few variables of interest instead of being uncertain about them to implement our framework we investigate a case study on coral reef fisheries management for islands in the indo pacific it is a complex decision problem in a region that requires sound management of precious and fragile ecosystems eddy et al 2021 coral reefs are under intense pressure from various local and global stressors such as climate change pollution fisheries and other impacts burke et al 2012 a reef s ecological complexity poses substantial challenges for predicting the state of the reef under a management alternative while different modeling approaches can be helpful kelly et al 2013 we use a spatially explicit agent based model in our study miñarro et al 2018 decisions about their local management are not only difficult because coral reefs and their fisheries are intricately linked but they also fulfill a variety of needs in often resource constrained socio economic contexts ferse et al 2014 therefore the divergent objectives and perspectives of various stakeholders need to be considered in decision making typically the available data and our ability to collect data is limited requiring iterative and adaptive management approaches that include targeted efforts for the collection of additional information after discussing our framework for voi analysis section 2 we introduce the structure of the reef management decision case section 3 1 we describe how to analyze the evppi for predictions of fisheries management alternatives and discuss design choices for such efforts section 3 based on the analysis results section 4 we discuss the usefulness of voi analysis and suggest desirable extensions to our implementation section 5 2 framework for analyzing value of information for decision models 2 1 decision modeling making a rational decision between several alternatives is facilitated by a quantitative representation of the consequences of implementing an alternative and of the stakeholder preferences regarding these consequences the combination of predictive system models with preference models to evaluate decision alternatives we call a decision model see haag et al 2019b in the following we provide the conceptual background to voi analysis for such models fig 1 a concrete implementation is detailed in section 3 after a comprehensive problem structuring phase e g marttunen et al 2017 the first step of decision modeling is to predict the consequences y a y a 1 y a m for each alternative a on the system attributes 1 m that are relevant for decision making according to the stakeholders these predictions can come from a mathematical model but also from experts e g nicol et al 2019 given that environmental or socio ecological systems can neither be completely understood nor fully observed nor perfectly represented in a model it makes sense to conceptualize these predictive system models as probabilistic models with uncertain parameters or inputs reichert et al 2015 reichert 2020 therefore the consequences for an alternative a are better represented as a random vector y a as we do not obtain a point estimate for the predicted variables but distributions of predictions p y a the distribution of predictions is a more informative description of our knowledge than a point estimate or aggregated value the predicted distributions will often not be independent for instance the biomass of carnivorous fish may be negatively correlated with biomass of prey species the second step is to identify a utility function u that quantifies a stakeholder s preferences with regard to the predicted attribute levels and their uncertainties the utility function represents the trade offs the stakeholder is willing to make between uncertain consequences let y denote the set of all possible consequences for all attributes considered relevant in the decision a multi attribute utility function u y 0 1 maps from a space of predicted consequence regarding system attributes to a utility space it returns the utility of potential consequences with larger utilities representing more preferred system states keeney and raiffa 1993 standard utility theory does not consider uncertainty of preferences in the following we keep to this limitation a more extensive and rigorous treatment of multi attribute utility theory than outlined here can be found in textbooks e g keeney and raiffa 1993 french 1986 once we specified parameters of a utility function based on a stakeholder s preferences we can calculate the resulting multi attribute utility u y a for an individual prediction of consequences y a y a 1 y a m for instance y 1 could be the prediction for the coral cover in and y m the prediction for herbivorous fish biomass per m 2 three years from now however as the predictions are uncertain we need to evaluate their entire distribution given uncertain consequences y a for an alternative a we receive corresponding utilities u a u y a for a given stakeholder preference profile we obtain a ranking of the alternatives based on their expected utility eu 1 eu a e u a y u y p y a y d y with p y a the probability distribution of the consequences of implementing alternative a as measured by all of the system attributes the alternative with the highest eu is then determined by maximizing 2 max a 1 a e u y a utility theory is prescriptive because a rational decision maker should choose the alternative with the highest eu a given their preferences encoded in the function u and the probability distributions of predicted consequences p y a utility theory allows determining the optimal choice for individuals or groups with homogeneous preferences for environmental management decisions the societal utility of an alternative must be considered from conflicting stakeholder perspectives which decision making criterion should be considered rational or fair for a group or society is an unresolved question de jonge 2012 however making a number of contested assumptions such as the possibility for interpersonal comparisons of utility we can construct a group utility function from the individuals utility functions keeney 2013 to obtain a group expected utility eu g a of an alternative a across k stakeholders perspectives we aggregate individual expected utilities eu k using the weighted arithmetic mean the weights w k with 0 w k 1 are scaling factors for the utility between stakeholders that sum to one eq 3 3 eu g a k 1 k w k eu k a 2 2 value of information sensitivity analysis the optimal baseline choice in a decision is the alternative a obtained by maximizing eu given the current state of information and the current preferences however in practice we are usually concerned about the sensitivity or robustness of such a conclusion how bad would it be if we picked this seemingly optimal alternative but some information turned out to be different in reality for illustration let us consider a decision about a system in which only one among all the variables is uncertain following ideas introduced by felli and hazen 1998 we consider three perspectives of a decision s sensitivity to that uncertain variable threshold perspective if we varied the variable from its lowest to highest value is there any threshold level at which the optimal choice changes if not the decision is insensitive to that variable probabilistic perspective given the probability distribution of the variable how likely is it that the threshold is crossed if this is very unlikely the optimal choice remains insensitive to the variable even though a threshold exists utility foregone perspective how large is the expected difference in utility between our baseline choice and the optimal choice if the threshold would be crossed if the difference in utility is small enough the penalty for having chosen a suboptimal alternative will be small enough that we can consider the decision to be insensitive value of information can be thought of as a sensitivity measure that takes both the probabilistic and utility foregone perspectives into account felli and hazen 1998 borgonovo et al 2016 converse to utility forgone it measures the gain in utility we can expect from knowing a decision aspect such as a system attribute or parameter with more certainty with voi we thus measure how sensitive the optimal choice is to potential new information before we acquire it if the voi is small it is unlikely that the baseline optimal choice changes with that new information or the difference in utility that this change would bring is small thus voi supports us in prioritizing which uncertainties should be reduced by further information collection several variants of voi measures have been proposed they share the same concept but differ with regard to the additional information that is considered the expected value of perfect information evpi measures the sensitivity of obtaining perfect information on all modeled aspects of the decision eidsvik et al 2015 the expected value of partially perfect information evppi also abbreviated evxpi or evxi measures the sensitivity of obtaining perfect information on parts of the uncertainties for example one or several parameters eidsvik et al 2015 this is the measure we focus on in the following given that aleatory uncertainty cannot be reduced by more studies or data collection evpi and evppi can be thought of as upper bounds for the actual voi we can expect in practice we will always only obtain imperfect information the expected value of sample information evsi is a measure for the impact of obtaining additional data but not perfect information estimating evsi is still difficult in many contexts but it is an active area of research e g williams and johnson 2018 kunst et al 2020 2 3 the expected value of partially perfect information the idea of voi is to calculate the added value of a piece of information before we actually expend the effort and cost to obtain it in the case of evppi we estimate the expected gain if we knew one or a group of variables of interest parameters inputs or attributes with certainty a formal and extensive treatment as well as algorithms to estimate the evppi is available in the literature brennan et al 2007 eidsvik et al 2015 borgonovo et al 2016 heath et al 2017 here we sketch out the main ideas with the aim to provide an intuitive understanding of the approach in this study we focus on uncertainties of the predictions of the attributes however voi analysis can be conducted for other variables within the models by following the same approach let us assume we have determined the optimal baseline choice a for a coral reef management decision given the current state of data and knowledge we are now interested to find out how sensitive that choice is to one aspect of the decision our prediction of the resulting coral cover cc under a marine protected area mpa alternative we denote this variable of interest y i y cc mpa if we had perfect information about the resulting coral cover if an mpa were implemented what would be the expected added value to the decision the idea of the analysis is that we pretend we could with a certain cost and effort gain clairvoyance on this variable and found for instance that y c c m p a y c c m p a 58 this gives us a better estimate for the variable and can have three effects a the conditional distributions of the other variables y i y i y i change if they are not independent b the distribution of all utilities u a change if they are not independent from the variable of interest and c the optimal alternative might change we now calculate the maximum utility given this new information max a 1 a e u a y c c m p a 58 and determine the best alternative if the best alternative is still the baseline optimal choice a the additional information that coral cover under the mpa alternative will be 58 did not help us make a better choice than we would have made anyway rather there is an opportunity cost if effort was expended towards collecting this information consequently the value of this information would be zero as the eu of the optimal alternative given the information a and the eu of the baseline choice given the information is identical max a 1 a e u a y c c m p a 58 e u a y c c m p a 58 0 however if the new information changed our optimal choice this difference will be positive and indicates the value of collecting this information while this illustrates the voi for one particular value y c c m p a we are interested in the expected voi for y c c m p a over its entire probability distribution of the coral cover within the predicted range we can estimate this by sampling from the predicted probability distribution of coral cover calculating the corresponding eu given that we know this sampled value and tracking the corresponding change in the optimal choice if the optimal choice does not change for any of these values it implies that no matter how much we improve our predictions our conclusions would never change more formally for a risk neutral stakeholder we calculate the evppi about the variable of interest y i with 4 evppi y i e max a 1 a e u a y i e u a y i e max a 1 a e u a y i max a 1 a e u a the evppi can be viewed as the difference between a posterior value of the decision with new information and a prior value without the information as in the second part of eq 4 eidsvik et al 2015 if a stakeholders has an exponential utility function keeney and raiffa 1993 these are the respective certainty equivalents of the situation where information is available for free and where no further information is available eidsvik et al 2015 if a stakeholder is risk neutral the certainty equivalents are the expected values as in eq 4 if we expand our analysis to several variables of interest e g growth rates herbivore fish biomass revenue for fishers etc we can determine a ranking of the variables in terms of their evppi this provides a quantitative and transparent prioritization of the most relevant uncertainties this can serve as the basis for directing study design and deciding about upcoming expenditure of costs and efforts towards collecting data and information 2 4 given data approach to estimating evppi and threshold sensitivity since analytical solutions can rarely be found various simulation based approaches to estimate the evppi have been developed the classical procedure is a two level nested monte carlo simulation e g felli and hazen 1998 brennan et al 2007 however this requires many simulations in addition to account for dependencies in the distributions of predictions markov chain monte carlo mcmc or other conditional resampling procedures with high computational burden are needed with a nested approach strong and oakley 2013 the alternative we describe here is a given data approach that only requires a single loop monte carlo sample as one obtains from a probabilistic sensitivity analysis this procedure has been developed by strong and oakley 2013 and is more generally discussed by borgonovo et al 2016 the method was originally conceived for voi calculations based on the net benefit of alternatives here we adapt it to usage with eu for risk neutral preferences we focus on this approach because it can handle the conditional dependencies in the distributions of variables while being computationally more feasible in connection with complex system models as a single loop monte carlo sample is sufficient a drawback of the method is that it is only efficient for calculating the evppi of single variables to calculate evppi for sets of variables other approaches have been suggested strong et al 2014 heath et al 2017 while we refer the reader to the original publication by strong and oakley 2013 for the details the basic algorithm can be described as follows from a monte carlo simulation we receive s samples of the uncertain variables and of the s corresponding utilities for each alternative the vector of samples y i of the variable of interest y i for which we want to calculate the evppi as well as the corresponding utilities u a for all management alternatives are now both reordered such that y i 1 y i 2 y i s the superscript denotes the reordered position in the vector of variable samples the reordered samples and the corresponding reordered utilities are partitioned into k bins of equal size j with j k s for each bin k we calculate the eu for each alternative this is an approximation of the eu conditional on the value of the variable of interest y i being in this bin we then take the maximum eu across the alternatives the arithmetic mean of these maxima across all k bins is taken as an approximation to the first term in eq 4 e max a 1 a e u a y the second term of eq 4 the eu of the baseline optimal choice max a 1 a e u a can be directly calculated as in eq 2 the corresponding estimator for evvpi can be written as 5 evppi y i 1 k k 1 k max a 1 a 1 j s 1 j k 1 j k u s a max a 1 a 1 s s 1 s u s a we propose that the same algorithm is efficient for threshold sensitivity analysis that takes into account conditional distributions the aim here is to estimate the eu of alternatives given that a variable of interest y takes on different values and then identify threshold values of y where the optimal alternative changes this could be achieved by varying y in a one factor at a time sensitivity analysis however to take a regional view on sensitivity and account for dependencies across attributes and alternatives we need to consider the conditional distributions of all variables given the specific values of the variable of interest to understand the relationship between specific values of y and the eu of the alternatives we can employ the algorithm described above the relationship can be approximated by calculating the arithmetic mean of the variable of interest in each bin and the corresponding eu for each alternative in this bin thresholds can then be determined visually from a scatterplot or by calculating a threshold criterion as it is sample based we may not be able to identify one threshold but rather a range of y as a threshold region 3 implementation for a reef management problem 3 1 case study description 3 1 1 problem background in this case study we investigated the local reef management for an island that can be considered as typical in the spermonde archipelago in indonesia using the framework described in section 2 the spermonde archipelago is a complex of about 70 islands located off southwest sulawesi most of them inhabited and surrounded by coral reefs the region lies in the center of the coral triangle which is the most biodiverse marine region worldwide burke et al 2012 as for many small islands and island nations in the indo pacific it is necessary to find a balance between the exploitation and conservation of their natural resources in the face of local and global changes from livelihoods and ecosystem degradation to climate change and globalized economics major local stressors on the reefs in the area are overfishing and the use of destructive fishing techniques fish are consumed for nutrition locally but mostly sold partly as live fish radjawali 2012 target species are diverse and shift with global demands and local supply ferse et al 2014 a wide range of fishing techniques are employed among them destructive techniques such as bomb fishing or cyanide fishing for fisherfolk of the islands alternative livelihoods are often neither attainable nor desired ferse et al 2014 additionally fisherfolk are often embedded in elaborate systems of patron client relationships that provide benefits such as social protection glaser et al 2015 these relationships can influence fishing behavior and may encourage overfishing and destructive fishing glaser et al 2015 miñarro et al 2016 the effects of fishing pressure are exacerbated by pollution from point and diffuse sources as well as sedimentary run off stemming from the islands and the makassar urban area teichberg et al 2018 larger scale pressures such as global climate change are expected to cause ocean warming and sea level rise resulting in the decline of reef health for this study we investigated an exemplary reef site of a typical island in the region the site is 200 160 m and part of a larger reef area the majority of the seafloor is less than 5 m deep but descends to 15 m depth fig si 1 to improve the local situation we focus on fisheries management as a way to mitigate a primary local stressor for the reef 3 1 2 societal perspectives and objectives different users and interest groups hold varied perspectives on a reef site its value and its relevant services societal evaluation ultimately determines which form of management is optimal therefore we need to consider different stakeholder perspectives on the issue for the reef of the inhabited island we investigate we seek to represent a diversity of views by exploring four archetypal perspectives local livelihoods perspective focused on ensuring food security and economic benefit to local fishers reef conservation perspective focused on ecosystem health and resilience extraction perspective focused on maximizing fishing yield and economic benefits balanced perspective focused on balancing the different aspects and interests the values that are implicit in these perspectives can be expressed in the form of an objectives hierarchy keeney 1992 that we developed based on literature maynard et al 2017 brown et al 2018 mcfield and kramer 2007 as well as our knowledge of the case and context fig 2 we chose these four archetypal perspectives to illustrate a wide range of views on the management issue but they are not meant to represent actual individual stakeholders or groups for real world decision support these perspectives and their objectives need to be elicited and co developed locally we use these perspectives to guide an analysis for multiple stakeholder perspectives which can be adapted to a particular decision context 3 1 3 management alternatives attributes and time scale at the investigated reef site fishing occurs for subsistence and commercial reasons and destructive fishing techniques are employed occasionally see description of the no restrictions alternative in table si 1 to define management alternatives we first developed a strategy generation table gregory et al 2012 we identified three management factors 1 gear and technique restrictions 2 access restrictions and 3 fishing quotas based on the strategy generation table we developed four management alternatives expected to result in decreasing degrees of fishing pressure see table 1 no restrictions no restrictions with continued intense fishing pressure including destructive fishing no destructive fishing enforcing a ban of bomb and cyanide fishing mpa subsistence implementing a marine protected area which allows only subsistence fishing for locals mpa no take implementing a strict marine protected area making it a no take zone the management alternatives if implemented will have consequences for very different aspects of the socio ecological system of the island reef the consequences that are relevant for deciding between the management alternatives are captured by attributes of the system fig 2 and table si 2 the attributes should describe the consequences in a way that is understandable and useful for decision makers and stakeholders keeney and gregory 2005 the degree of fulfillment of the decision objectives can then be quantified based on the attribute levels for instance the attribute total biomass of browsers scrapers and grazers in g m 2 can be used to determine the achievement of the objective of having a high biomass of herbivorous fish in the reef the considered time scale of consequences can make a large difference in decision making short term and long term consequences can diverge for instance an over exploitation of fishing resources can be advantageous in the short term but detrimental in the long term in this study we consider consequences three to six years in the future 3 2 prediction of management consequences predicting the consequences of management alternatives on complex systems is a difficult task to predict the ecological consequences of management alternatives on reef fishes reef benthos and fisheries yield we adapted a previously developed model of the system seamancore miñarro et al 2018 this model combines an agent based model of fish stocks and fishing behavior with a corresponding model for benthic community dynamics the implementation is detailed in the next sections to account for the often neglected cost of implementing and enforcing management alternatives mccrea strub et al 2011 we used the required patrol days as a proxy attribute for these costs brown et al 2018 we used our contextual knowledge to estimate this attribute s distribution for each management alternative these were assumed to be poisson distributions with different rate parameters 3 2 1 model of reef and fishery dynamics under management alternatives seamancore is a spatially explicit 2d model that simulates the dynamics of the benthic and fish populations as well as fisheries in a coral reef through an agent based simulation miñarro et al 2018 we used seamancore to predict the temporal dynamics of the reef benthos and fish community in a 200 160 m area the predicted entities consisted of four benthic functional groups hard coral macroalgae and turf hard substrate and cropped algae non stabilized substrate stocks of three functional fish groups carnivores browsers and grazers scrapers and different fisheries table si 3 the aim for prediction is to capture the inherent stochasticity of the system as well as the directed effects of the management alternatives we achieve this by differentiating between core parameters and processes of the seamancore model that are unaffected by the alternatives and alternative specific parameters and processes if we intend to understand the effect of the alternatives it is only meaningful to compare the set of predictions where the core parameters are shared one model parameterization of the natural processes can be viewed as one potential configuration of the world the four management alternatives then lead to four different futures therefore for one core parameterization we created four simulations with the alternative specific parameters set in addition the difference between the four management alternatives was represented by modified inputs and parameters in the fishing module e g number of boats four types of fisheries were considered 1 bomb fishing 2 cyanide fishing 3 non destructive commercial fishing and 4 non destructive subsistence fishing see table si 2 for their specification in seamancore the different management alternatives allowed specific combinations of these types of fishing to occur in the reef from all types in the no restrictions alternative to none in the no take alternative table 1 for each simulation run we specified the initial conditions in a spatially explicit manner the water depth fig si 1 the benthic cover biomass of fish functional groups and the fishery modes to apply table 1 and table si 3 additionally we set over 100 other core parameters that govern the model behavior miñarro et al 2018 for each simulated time step the model outputs a 10 10 cm resolution map of benthic cover a 20 20 m resolution map of biomass of fish functional groups and the total fishery yield differentiated by depth target species and fishery type we simulated at a temporal resolution of 1 day for both the benthic and the fish grids the management alternatives were assessed against the background of an uncertain environment in which most aspects are beyond management control for instance biological processes such as death rates or feeding rates were assumed to be independent of management alternatives at the considered scale however these processes are usually variable and our knowledge about them is incomplete we represented this with two approaches firstly the rules governing the cellular automaton and model agents have a stochastic element for example rules to change a hard substrate cell to an algae cell are triggered only with a certain probability at each time step secondly we used probability distributions instead of point estimates for the core parameters we used the calibration of miñarro et al 2018 as a basis for the model parameterization we then defined probability distributions for 37 of the models parameters e g reproduction rates feeding rates time for colonization fig si 2 and 15 input variables initial benthic cover and initial fish biomass fig si 2 other parameters such as the probability of transition between benthic substrata remained the same across all simulations parameter distributions were identified based on literature values and judgment by the authors guided by natural constraints e g a death rate cannot be negative plausibility of the simulation results and using maximum entropy distributions distributions of model parameters were assumed to be independent except for initial benthic coverages which were assumed to come from a multivariate normal distribution we assumed all parameters to be constant across the modeled temporal and spatial dimensions by propagating this prior information through the model with monte carlo simulation we received a corresponding distribution of model outputs an external stressor we modeled explicitly were coral bleaching events eddy et al 2021 we included strong bleaching that turn coral cells into hard substrate and mild bleaching that resets the age of coral cells bleaching affects coral cells with a certain probability depending on the depth of their location the effects of other external stressors such as temperature change eutrophication or loss of connectivity to other reefs are captured indirectly by the distributions of the parameters such as colonization rates of coral and algae growth rates of functional groups or external recruitment 3 2 2 obtaining of decision relevant predictions to predict the uncertain consequences of management alternatives we used monte carlo simulations to obtain a population of predictions from independent runs of the seamancore model we drew 1100 samples from the probability distributions of the model core parameters fig si 2 these were combined with the definitions of each alternative the seamancore model was thus run for 1100 samples 4 alternatives 4800 times in total for five of the 1100 parameter combinations the model produced nonsensical results either the model returned an undefined value for biomass of a fish functional group or scraper biomass was continuously above a threshold of 95 g m 2 for more than 60 days these runs were excluded as we focus on the situation three to six years after implementing a management alternative we ran the model for 2280 time steps days and then took the values for years four to six into the future as the basis for the attribute predictions to smooth out short term noisy fluctuations due to asynchronous updating of fish and benthos grids we used a 42 day 6 weeks rolling mean of the time series for the benthos cover and a 14 day 2 weeks rolling mean for the time series of fish biomass the system model outputs are not directly of interest as attributes in the decision we therefore transformed and aggregated model outputs to obtain predictions for attributes that would be understandable to interested stakeholders fig 2 and table si 2 for instance the daily time series of fish biomass for the three functional groups was aggregated to arrive at weekly average total fish biomass the variability in time thus became part of the prediction uncertainty from the perspective of strategic management the spatially explicit output of the model was also not relevant therefore we aggregated the model outputs for the entire area the empirical joint distribution of the aggregated and transformed model outputs was the basis for the decision relevant attribute predictions we aimed for a sample of size s 120 000 for each attribute of each alternative to ensure equal sample size attributes measured on a weekly scale were downsampled from 170820 samples and attributes with monthly scale were upsampled with replacement from 42705 samples for the attribute required patrol days we directly drew s samples from the specified poisson distributions a crucial consideration when creating this sample of attribute predictions is that dependencies exist across attributes and alternatives this should be considered in voi analysis for instance if coral cover correlates with herbivore biomass having better information about coral cover will also inform us about herbivore biomass for an estimation approach of voi that is based on sampling we therefore need to create a sample that retains the relevant conditional dependencies in the predictions given the way we set up our simulations we obtain these dependencies from the seamancore modeling for other modeling approaches this can be less straightforward such dependencies exist within the predictions from a simulation run for one alternative firstly the parts of the ecological system are connected for example high carnivore biomass will often coincide with lower biomass of herbivorous fish secondly predictions are correlated in time as the future system state depends on previous time steps in the resampling of predictions for a management alternative we retained relations between samples for different attributes regarding points in time and simulation parameterization this means for one alternative a particular sample of all attributes comes from the same simulation run and time point dependencies also exist between the consequences of different alternatives because they are predicted based on shared core parameters as described above across the alternatives we retained relations regarding the core parameter samples this represents system properties or shared external influences that are the same for all alternatives however we randomized the resulting predictions regarding time in this way we represent different time points at which the effects of the alternatives are assessed in the future this means a particular attribute sample across the alternatives comes from a model run with the same core parameters but potentially different time points we illustrate the effect of retaining different correlations in fig si 7 3 3 evaluation of management alternatives 3 3 1 preference model structure and parameters to understand the differences in utility that the management alternatives would bring to different societal actors we specified a hierarchical utility model for each of the four archetypal stakeholder perspectives each model encodes an assumed preference profile for a stakeholder perspective and is meant to represent specific interests for that stakeholder based on the evaluations of management alternatives with these models we can then identify areas of conflicts and consensus as outlined in section 2 1 we can also calculate aggregate results across stakeholder perspectives in the following we outline our approach to hierarchical utility models a detailed treatment can be found in haag et al 2019a and reichert et al 2015 the structure of the preference models is given by a hierarchy of objectives fig 2 we assume that all stakeholder perspectives share the same set of objectives but differ in their preferences for instance the trade offs they are willing to make between the objectives to build the model we first specify a marginal value function for each of the seven objectives on the lowest level of the hierarchy v o p y p fig si 3 these map from the attribute space to a relative degree of achievement for each of these objectives then we aggregate these valuations along the hierarchy with nested aggregation functions f k that we specify for each aggregation step we arrive at a multi attribute value function over all attributes v y 1 y m with this function we receive an overall evaluation of each decision alternative lastly as discussed by dyer and sarin 1982 this multi attribute value function is converted to a utility function u v y 1 y m r at the highest objective in the hierarchy given the risk attitude r as we assume stakeholders to be risk neutral the eu of an alternative is its expected value based on the evaluation with the value function the preference model used to evaluate one set of consequences of alternative a can therefore be written as for the indices refer to fig 2 6 u y a f 0 f 1 v o 1 1 y 1 a θ o 1 1 v o 1 2 y 2 a θ o 1 2 w f 1 γ f 1 v o 2 y 3 a θ o 2 v o 3 y 4 a θ o 3 f 4 v o 4 1 y 5 a θ o 4 1 v o 4 2 y 6 a θ o 4 2 w f 4 γ f 4 v o 5 y 7 a θ o 5 w f 0 γ f 0 as aggregation functions f k for the values v on each hierarchical level we chose functions of the family of weighted generalized means also called power means with the form 7 f k v 1 v n w γ i 1 n w i v i γ 1 γ γ r with weight parameters scaling factors 0 w 1 and i 1 n w i 1 the parameters of the preference model eq 6 were changed for each of the stakeholder perspectives based on assumed preferences in line with their concerns see section 3 1 2 these parameters are how they evaluate changes on the attribute scales such as diminishing returns regarding fish catch or coral cover shapes of marginal value functions θ fig si 3 how they trade off changes in one objective relative to the other objectives weight parameters w table si 3 to what degree a poor achievement of objectives can be compensated for example can a high enough fished biomass compensate for a very low coral cover or is a one out all out view appropriate degree of non additivity γ table si 3 as the aim in this study was to explore the space of potential perspectives the preference profiles were designed by the authors according to the archetypes see 3 1 2 for a practical decision problem the parameters should be inferred from stakeholder data these data can be collected by choice experiments hensher et al 2015 or other forms of preference elicitation haag et al 2019a 3 3 2 estimation of expected utility based on the empirical distributions of attribute predictions we first calculated the optimal baseline choice each stakeholder preference profile was treated separately for each sample y y 1 y 7 of the 7 attributes we calculated the utility of each alternative using the respective preference model since we have s 120 000 samples we obtained 120 000 utilities the set of which we denote u a for alternative a by taking the arithmetic mean of u a we received the eu for each alternative assuming risk neutrality e u a the rational imperative is to pick the alternative with the highest eu max a 1 4 e u a this is the optimal baseline choice for a stakeholder preference profiles given our current information about the system attributes based on the evaluation of management alternatives with the four preference profiles associated with the stakeholder perspectives we can identify areas of conflicts and consensus for also providing an aggregated view across the stakeholder perspectives we followed the approach outlined in section 2 1 using eq 3 we calculated the eu of an alternative across perspectives giving all perspectives equal weight 3 4 sensitivity analysis using value of information value of information analysis can be conducted for any uncertain inputs or parameters of a decision model for this case study we focused on exploring the evppi with regard to uncertain attribute predictions the aim was to understand the impact of better knowledge of the predictions of specific attributes to identify whether the uncertainty of an attribute prediction was more relevant for some alternatives than others we focused on individual variables for each of the alternatives and calculated the evppi of 4 alternatives for 7 attributes resulting in 28 variables of interest that is if we had perfect knowledge of some attribute s predictions y i a for alternative a how much additional utility would this be expected to provide to a stakeholder to estimate evppi we implemented the algorithm as described in section 2 4 and ran the analysis separately for each of the 28 variables of interest and each stakeholder perspective we also calculated the evppi across perspectives based on averaging the eus of the perspectives in each bin and on the eu of the baseline optimal alternative across the perspectives for the main analysis we chose values of k 300 and j 400 to mitigate the chance of bias we also investigated the dependency of evppi on these choices fig 7 fig si 9 to understand the added value of evppi over a simpler threshold view on sensitivity see section 2 2 we conducted a threshold sensitivity analysis this means we estimated the eu of alternatives given that a variable of interest y takes on different values as described in section 2 4 we repeated this analysis for all 28 variables of interest and each stakeholder to identify thresholds visually we normalized the eu values to the baseline optimal alternative for a stakeholder and estimated a smoothing spline model for this relationship 4 results 4 1 predicted consequences of reef fishery management the inputs and parameters of the predictive system model were described by probability distributions together with the stochastic processes in the model this led to distributions for the obtained outputs and consequently for the derived predictions of the relevant attributes fig 3 if we would only consider point predictions in our decision making e g median lines in fig 3 we would disregard a lot of relevant information the distributions of attributes that describe the state of the reef coral cover herbivore biomass and total fish biomass are wide this means the predictive uncertainty about their future is high the differences between alternatives appear less pronounced when considering the marginal distributions this suggests the alternative had relatively less effect on the outcome compared to the stochasticity of the system as expected decreased fishing pressure generally leads to increased fish biomass especially of carnivores increases in herbivore biomass are smaller due to the increasing predation pressure from carnivores coral cover increases when destructive fishing is stopped but decreases slightly when fishing is stopped completely as feeding pressure by scrapers increases the distributions of attributes connected with fishery yield carnivorous and herbivorous biomass to sell fish for local consumption exhibit long tails fig 3 with the high fishing pressure in the no restrictions alternative few carnivorous fish can be sold due to over exploitation and hence stock depletion in the considered 3 6 year time frame the carnivorous fishing yield is higher in total with less intense fishing on the other hand when only subsistence fishing occurs there is little surplus of especially herbivorous fish to be sold with the high fishing pressure of the no restrictions alternative it is more likely that not enough fish can be caught for local consumption in comparison to the alternatives with no destructive fishing or with a protected area that allows subsistence fishing with a strict no take zone no fishing is assumed to occur therefore the attributes related to fishery yield are always zero 4 2 optimal baseline choice under uncertainty given the uncertain predictions of the attributes and our preference models for the different perspectives section 3 3 we calculated the utility for each predicted sample of the alternatives distributions in fig 4 the expectation over these utilities the expected utility eu is the criterion that a rational decision should be based on this eu integrates over the predictive uncertainties and is therefore a single number solid markers in fig 4 for the balance and local livelihoods preference profile a ban of destructive fishing practices would be the optimal alternative for the conservation profile a marine protected area mpa with only subsistence fishing and for the extraction profile the alternative with no restrictions would be most desirable fig 4 except for the conservation profile a strict mpa with a no take zone receives the lowest eu in all profiles for the conservation profile the no restrictions alternative results in sightly lower eu this can be explained by the missing fulfillment of any socio economic objectives by an alternative that enforces a no take zone the alternative with no restrictions is not optimal for most preference profiles even a moderate restriction of fisheries can lead to higher fish biomass and also fished biomass especially of carnivorous fish even in the short time frame studied based on these results no clear consensus for a best management alternative emerges between the different perspectives however we can identify two aspects that might help come to such a consensus in an iterative process 1 some reduction in fishing seems beneficial for fishery yield even in a short time horizon and 2 the lack of fulfillment of socio economic objectives due to a complete ban of fisheries can hardly be counterbalanced by better conservation outcomes 4 3 threshold view on decision sensitivity there are large overlaps in the distributions of the alternatives utilities fig 4 this means we might see future system states in which different conclusions about the optimal alternative would be drawn therefore we need to investigate the sensitivity of the decision before considering the voi view of sensitivity analysis we identify thresholds for individual variables of interest here the attribute predictions for one alternative that lead to changes in the optimal alternative in fig 5 we see the resulting eu of the management alternatives relative to the baseline optimal alternative as we vary a variable of interest along its range this relative eu is given as a function of a variable of interest while retaining a probabilistic view and the correlation structure in all other variables based on this we can identify thresholds at which the ordering of the decision alternatives changes since our results are based on simulations the thresholds are small regions rather than exact points as an example the baseline optimal alternative for the conversation perspective is the mpa with subsistence fishing we can now investigate the decision s sensitivity to the predicted coral cover of the mpa with subsistence fishing alternative lower left panel of fig 5 results for all variables and perspectives are given in figs si 5 8 there are two thresholds if we knew the coral cover of the mpa with subsistence fishing alternative would turn out to be below 23 the no restrictions or no destructive fishing alternatives would now provide higher utility if we could be certain that the coral cover of the mpa with subsistence fishing alternative would be between 23 and 62 it would be the optimal choice in this region the voi is zero as the best alternative is not sensitive to the precise value of the coral cover prediction if the coral cover of the mpa with subsistence fishing alternative would be higher than 62 the no take mpa would be optimal this may seem counter intuitive as all else being equal the utility of the mpa with subsistence fishing alternative should increase with increasing coral cover as higher cover is preferred it is however a consequence of the correlation structure in the predictions either the high coral cover for that alternative coincides with less preferred consequences on its other attributes or it coincides with even more preferred consequences for the no take mpa alternative the analysis falls short in two regards first we do not take into account how probable a crossing of a threshold would be how probable would it be that we actually see coral cover greater than 62 under the mpa with subsistence fishing alternative second once we crossed a threshold we disregard how large the potential gain in utility would be from taking the optimal instead of the now sub optimal alternative if coral cover was above 62 how much higher would the utility of deciding for the no take mpa alternative be in comparison to sticking with the mpa with subsistence fishing both aspects are crucial for understanding the sensitivity of a decision this is the point of the analysis of the voi 4 4 results of the value of information analysis to have a more comprehensive measure of decision sensitivity than the threshold view we calculated the evppi of the variables of interest the lower the evppi of a variable is the lower the sensitivity of the decision to it and vice versa if we had perfect information about that variable this either would seldom change the optimal alternative the gain in utility due to choosing the new optimal alternative would be small or both a ranking of the variables of interest based on their expected voi can then support us in identifying the key uncertainties and prioritizing their resolution the evppi varies by variable of interest and stakeholder preference profile fig 6 table si 5 comparing all stakeholder perspectives two commonalities exist the attribute of required patrol days had relatively low evppi while the attribute regarding fish available for local consumption had high evppi otherwise the results are more nuanced across variables the livelihoods perspective often receives lower evppi than the other perspectives the decision is less sensitive for this perspective this demonstrates how the voi depends on the stakeholder preference models and how far away in terms of probability of change the baseline optimal alternative is from the others see fig 4 the evppi is not directly linked to the width of the probability distributions of the variables fig 3 the herbivore biomass of the no restrictions alternative has a markedly narrower distribution than the total fish biomass of this alternative yet for all profiles as well as across profiles both have a similar evppi as expected for variables that are known with certainty for instance fisheries variables in the no take mpa alternative the evppi is zero for the conservation perspective we can summarize our analysis as follows for brevity we do not discuss the other perspectives here but they can be similarly analyzed the optimal baseline choice given our current state of knowledge would be implementing an mpa with subsistence fishing fig 4 however this choice is sensitive to the actual realizations of the predicted attributes thresholds exist that would make a different choice optimal fig 5 and fig si 6 we expect the optimal choice to be sensitive to the days with food for local consumption herbivore biomass in the reef and total fish biomass fig 6 we expect it to be insensitive to the number of patrol days and the actual fished biomass for selling for any variable except the coral cover better knowledge about the true consequences when implementing the no take mpa alternative is hardly relevant for the decision green bars in fig 6 this alternative is unlikely to become the best one for the conservation perspective based on the evppi analysis the conclusion for the conservation preference profile would be that understanding the trajectory of the reef and its organisms better should be a priority however the uncertainty about the fish for local consumption is also relevant further efforts directed at improved understanding of these aspects will be most critical for decision making as the determined baseline choice may not be the best if our knowledge about the respective attributes was improved on the other hand further investigation of the patrolling effort or the sold fish biomass is unlikely to change the conclusions regarding the optimal management alternative for the other preference profiles the list of priorities differs with some commonalities as described above the estimation algorithm for evppi that we propose in section 2 4 has a hyper parameter the size of the bins j the choice of j can have a significant effect on the resulting estimate fig 7 and fig si 9 for small bin sizes the estimator is upwardly biased due to the maximization step as j 1 the estimates converge to the expected value of perfect information across all variables evpi if each sample is placed in its own bin i e j 1 the estimated evppi is equal to the evpi the estimated evppi converges to zero as j s as both terms of eq 5 become equal thus for large sizes of the bins the estimator is downwardly biased in our case most evppi estimates are relatively stable using from 100 bins with 1200 samples each to 12000 bins with 10 samples each fig 7 this confirms our choice of a bin size of 400 for the analyses above however in specific cases estimates can also be sensitive to the bin size e g days with fish for local consumption in fig 7 or fig si 9 5 discussion 5 1 relevance of value of information analysis for the case study in the case study on coral reef fisheries management we found that the attributes measuring consequences related to fish biomass food security and sales of fish groups had the highest evppi for at least one stakeholder preference profile on the other hand better estimates for the required patrolling effort had low evppi for any single and across stakeholder perspectives likewise hard coral cover which is routinely monitored and prominently reported was not among the top three variables in terms of evppi for any perspective the analysis provided a reasoned and prescriptive focus for the design of future investigations for the decision case the effect of the management alternatives on the reef fish their catch and livelihood impacts should be a focus of future data collection efforts to reduce decision uncertainty however this conclusion depends on the stakeholder perspective considered for the conservation perspective better knowledge about the amount of fish sold had low expected informational value whereas for the livelihoods perspective better knowledge about the biomass of fish functional groups in the reef was not very relevant fig 6 importantly the evppi is not directly linked to the extent of uncertainty in the predictions fig 3 even though the coral cover had a wide distribution for each management alternatives having perfect information on it did not have high informational value consequently to improve decision robustness it is not always the largest uncertainties that require addressing rather it is the most decision relevant uncertainties which can be identified through the analysis of voi this is in line with several studies that have investigated the factors that influence the voi in a decision but found it can vary in unexpected and sometimes counter intuitive ways e g eeckhoudt and godfroid 2000 gould 1974 dependencies between alternatives can be one influencing factor while we may be very uncertain about the future coral cover we may be quite certain that one alternative results in higher coral cover than another cf reichert and borsuk 2005 delquié 2008 has shown that under quite general assumptions the voi is highest when a stakeholder s baseline choice is indifferent between two alternatives the voi decreases with increasing utility difference between the alternatives in the baseline case our results show the same pattern as for the livelihoods and extraction preference profiles which have a larger spread among the utility of alternatives fig 4 the evppi is generally lower than for the other two preference profiles fig 6 value of information is specific to the investigated decision if the informational value of a variable e g coral cover is low in a particular decision this does not imply we should stop regular monitoring historical baselines and operating protocols remain important and can be of great value in another decision and for improved understanding of the complex system dynamics 5 2 evaluation and outlook of voi framework this study showed how voi analysis and specifically the evppi is a useful form of sensitivity analysis for decision models based on the approach in this study we highlight three key directions for further development that we consider relevant in the context of environmental management decisions the first direction is extending the approach to uncertainty of preference model parameters standard utility theory e g french 1986 keeney and raiffa 1993 does not consider uncertainty of preferences nor do any voi applications we are aware of however in practice the stakeholder preferences are also uncertain and we have shown in this study that the preference model can make a substantial difference for the voi analysis results the uncertainty of stakeholder preferences could be included in voi analysis by using the expected expected utility concept chajewska et al 2000 haag et al 2019b considering the uncertainty about the consequences of management alternatives and the uncertainty about the societal evaluation of these consequences on equal footing in voi analysis will allow us to differentiate better where further studies are actually needed depending on the case the uncertainty about the social evaluation could be the primary cause for decision uncertainty gregory et al 2006 the second direction for development is improving uncertainty quantification of the variables of interest e g the predictions quantitative voi analysis is only meaningful to the degree that we can specify or infer probability distributions for these variables ideally derived from empirical sources the question of a variable s voi is only ever addressed in the small world savage 1954 of our specified model however the assessment of uncertainties in predictive system models is usually limited especially regarding structure and dependencies we also disregarded crucial structural uncertainties in the predictive reef model missing processes such as changes in fish population structure might entail larger uncertainties than all the included parametric uncertainty more comprehensive uncertainty assessments are a large task for the environmental modeling community but there are many advances in this direction e g uusitalo et al 2015 reichert 2020 a way to address large uncertainties that are difficult to quantify examples for reef systems are crown of thorns starfish outbreaks or powerful storms are scenarios e g walker et al 2003 wright et al 2019 scenarios can be modeled by repeating the analysis for different possible futures in the form of constraints or modified ecosystem processes and qualitatively evaluating the differences in the conclusions this would be possible without fundamental changes to the presented approach the third direction for development is advancing methods to estimate evppi more efficiently especially with environmental models that often entail significant computational effort simulation based approaches that rely on many model runs and resampling from conditional distributions such a nested markov chain monte carlo brennan et al 2007 felli and hazen 1998 are infeasible the algorithm we implemented based on strong and oakley 2013 and borgonovo et al 2016 is fast only requires a given probabilistic sensitivity analysis sample and can handle conditional dependencies on the other hand it can only be sensibly used for investigating the evppi of single variables and it still requires a large sample size an important consideration when using the proposed algorithm is the choice of the bin size j as this can lead to bias as investigated in section 4 4 as strong and oakley 2013 have also shown before the upward and downward biases appeared for extreme values of j with a large region of stability in between these however in our case there were few estimates where such a stable region was small fig si 9 the conditions under which the algorithm can reliably be used thus require further investigation 5 3 value of information analysis to support iterative environmental management voi analysis is most useful if we can take actions to address the key uncertainties and improve the information state before or after decision making thus it fits well with decision contexts that have an iterative aspect such as the regular strategic considerations of environmental monitoring programs for the case study we presented the first steps of such an iterative approach based on the analysis results we can decide to a move forward with implementation of the baseline optimal alternative b gather more information if we deem the decision too sensitive to potential new information or c conduct implementation and information gathering in parallel in the environmental domain adaptive management is a common iterative approach and voi analysis has been used to improve adaptive management decisions e g moore and runge 2012 williams et al 2011 runge et al 2011 in adaptive management we plan monitoring and data collection activities in parallel to implementing a management alternative there will be feedback processes in the system after implementing a management alternative that we need to take into account in long term management therefore the idea of revisiting the same decision context and updating our state of knowledge is key this can include 1 updating changed stakeholder perspectives 2 monitoring how the predicted trajectories have played out 3 updating the future model predictions and 4 coming up with the plan in terms of what to focus on for the next monitoring phase in modeling approaches that optimize over iterative management problems such as markov decision processes voi analysis can also play an important role chadès et al 2017 williams et al 2011 williams and johnson 2018 using voi in environmental management and conservation practice is still at an early stage though its value is increasingly recognized see studies in bolam et al 2019 keisler et al 2014 along structured decision making approaches gregory et al 2012 many opportunities for broader application of voi analysis beyond local management or conservation decisions exist for example voi analysis can be of interest when designing large scale research programs rushing et al 2020 or monitoring programs bal et al 2018 the long term benefits of investing in voi analysis include a well reasoned allocation of resources as it provides a ranked list of the expected benefit of addressing uncertainties for voi measures that are based on utility this relative benefit can be difficult to interpret if costs and benefits of additional information are not measured in the same units we cannot directly determine which uncertainties we should resolve any practical decision about information collection requires making trade offs with the opportunity cost of acquiring this information e g maxwell et al 2015 a straightforward approach to this issue is a cost benefit analysis to find the pareto optimal set of cost efficient information seeking activities marchese et al 2018 6 conclusions difficult environmental decisions can benefit from structured approaches gregory et al 2012 and the conceptual foundation of rational decision making under uncertainty is well established keeney and raiffa 1993 reichert et al 2015 understanding the sensitivity of a decision to uncertainties remains a key challenge to support better decision making our intuitions about the benefit of more information may not be correct but the costs of additional data acquisition are often high we propose that sensitivity analysis known under the umbrella term value of information voi analysis is useful to estimate the robustness of current conclusions and indicate where to focus future data collection the complexities of environmental issues make the practical application of voi analysis challenging in this study we tackled voi analysis in the framework of multi attribute value utility theory mavt maut with 1 a continuous uncertain prediction space 2 dependencies in the distribution of these predictions 3 multidimensional objective functions that include trade offs between objectives and 4 divergent stakeholder perspectives this included adapting a fast algorithm for estimation based on a probabilistic sensitivity analysis sample we analyzed expected value of partially perfect information evppi for a decision model of local coral reef fishery management this led to a ranking of the sensitivity of predictive uncertainty of management alternatives our framework can be used as a template for other decision cases two simple but practically relevant conclusions were corroborated in our case study first the evppi of a variable cannot directly be mapped to the extent of uncertainty in that variable large uncertainties in predictions do not prevent robust decision making per se cf reichert and borsuk 2005 more data collection is not always the answer second the results of voi analysis depend on the preference models used to evaluate the predicted consequences cf delquié 2008 the variables that stakeholders require more information on can differ if we disregard the variety in stakeholder perspectives any voi analysis will give an incomplete picture of the actual value of a piece of information in a specific context value of information analysis fits into many structured and iterative approaches to decision making and assessment such as adaptive management it facilitates identifying and ranking fig 6 key uncertainties and thus key aspects for further investigation and data collection while the extent and intricacy of a quantitative modeling approach as employed in this study will need to be aligned with the concrete needs and resources available any practical decision case can benefit from a deliberation about the value of new information which information if any would likely change our conclusions credit authorship contribution statement fridolin haag conceptualization methodology software formal analysis writing original draft review editing visualization sara miñarro methodology resources writing review editing arjun chennu conceptualization methodology resources writing review editing visualization supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank daniel schürholz for coding advice this project benefited greatly from the resources of the zmt datalab we are thankful to miroslav shaltev and joscha schmiedt for support we also thank the three reviewers and the editor for their excellent comments and suggestions appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105552 appendix a supplementary data the following is the supplementary material related to this article additional information on the case study implementation and results are provided in the supplementary material available at https doi org 10 1016 j envsoft 2022 105552 mmc s1 
25502,uncertainties in environmental decisions are large but resolving them is costly we provide a framework for value of information voi analysis to identify key predictive uncertainties in a decision model the approach addresses characteristics that complicate this analysis in environmental management dependencies in the probability distributions of predictions trade offs between multiple objectives and divergent stakeholder perspectives for a coral reef fisheries case we predict ecosystem and fisheries trajectories given different management alternatives with an agent based model we evaluate the uncertain predictions with preference models based on utility theory to find optimal alternatives for stakeholders using the expected value of partially perfect information evppi we measure how relevant resolving uncertainty for various decision attributes is the voi depends on the stakeholder preferences but not directly on the width of an attribute s probability distribution our approach helps reduce costs in structured decision making processes by prioritizing data collection efforts graphical abstract keywords sensitivity analysis multi criteria decision analysis uncertainty agent based modeling coral reef management data and software availability the workflow to reproduce the analysis is available at https doi org 10 5281 zenodo 7156065 it depends on two submodules the seamancore simulation model https doi org 10 5281 zenodo 7155783 and an r package with functions for voi analysis https doi org 10 5281 zenodo 7155948 the dataset of simulation results from seamancore used in the analysis is available at https doi org 10 5281 zenodo 7156015 1 introduction environmental management decisions need to be made in the face of large uncertainties confronted with these uncertainties an intuitive response is to collect additional information on the reasonable assumption that more information will reduce uncertainty this in turn may improve our understanding and ultimately lead to better management decisions however collecting more information requires time and effort which could otherwise be allocated to management actions particularly in countries with limited resources for environmental management when we know enough about a system to make a sensible decision requesting more science rather becomes a technique to delay implementation gregory et al 2006 faced with the trade off between researching and managing an ecosystem it is important to consider to what extent is the use of limited resources to improve our understanding of a system justified by the resulting potential for improved management this question can be approached with the concept of value of information howard 1966 feltham 1968 value of information voi analysis allows us to determine the value in the sense of benefit or utility of additional information for deciding between different alternatives we can think of voi analysis as a form of sensitivity analysis with a focus on the sensitivity of the decision to new information rather than the magnitude of variation in the outcomes borgonovo et al 2016 felli and hazen 1998 razavi et al 2021 the analysis supports us in answering the questions would our decision for a specific management alternative change if we had more information and if so which information would most influence our choice or conversely resolving which of the uncertainties would bring more utility based on the answers we can rationally prioritize between research monitoring and implementation activities for a concrete decision tackling complex decisions in a rational way and conducting voi analysis is facilitated by a quantitative representation of the decision and its uncertainties see section 2 1 this is achieved by a decision model which consists of a predictive system model and a preference model reichert et al 2015 haag et al 2019b the predictive system model also called prediction model system model assessment model or simulation model in the literature provides measures of various system attributes e g coral cover crop yield breeding pairs etc given an input state this allows us to assess the potential consequences if we implemented different management alternatives also called options variants actions strategies or scenarios in the literature the preference model also called evaluation or utility model represents how decision makers stakeholders or society perceive and evaluate the predicted consequences of the system state in this study we focus on quantitative preference models based on multi attribute utility theory keeney and raiffa 1993 value of information analysis is an established approach in health care economics and medical decision making e g felli and hazen 1998 fumie and thompson 2004 jackson et al 2022 there is also growing interest in conservation biology reviewed by bolam et al 2019 and environmental management in general keisler et al 2014 eidsvik et al 2015 however many environmental decision problems share characteristics that complicate voi analysis and at the same time make it especially relevant for these problems in this paper we address two research gaps for voi analysis that emerge from the complexity of environmental decisions first we lack an integrative framework for jointly tackling common characteristics of such decisions including 1 multiple conflicting management objectives management criteria 2 uncertain consequences of management alternatives with continuous distributions rather than discrete states or hypotheses and 3 probability distributions of consequences that are not independent from each other multi criteria decision approaches mcda mcdm have been used with voi analysis before e g bates et al 2014 runge et al 2011 eidsvik et al 2015 but mostly have addressed these aspects separately see bolam et al 2019 voi analysis for continuous probability distribution remains conceptually straightforward but can become challenging for conditionally dependent probability distributions e g eidsvik et al 2015 myklebust et al 2020 second multiple conflicting perspectives on an environmental decision problem commonly exist however the consideration of multiple stakeholder perspectives in voi analysis has not been discussed widely in the literature before we address this issue and investigate the influence of diverging stakeholder perspectives on the results and conclusions from voi analysis the aim of this paper is to develop an integrated approach to conducting voi analysis given the characteristics of environmental decisions and highlight relevant considerations when applying the method the following questions guide our approach 1 how can we estimate the voi based on the correlated output of complex system models 2 how to analyze voi in decision contexts with multiple conflicting objectives 3 what is the influence of diverging stakeholder perspectives on the conclusions of voi analysis to address these questions we create a framework to conduct voi analysis for complex decisions bringing together elements that have been developed previously our voi analysis rests upon a decision model that combines system predictions and stakeholder preferences see above and section 2 1 the predictions can come from one or more arbitrarily complex models and can exhibit dependencies in their distributions to efficiently conduct voi analysis for dependent probability distributions we adapt an algorithm by strong and oakley 2013 as a replacement for traditional double loop monte carlo procedures while different measures of the voi have been proposed see eidsvik et al 2015 and section 2 2 we use the expected value of partially perfect information evppi also called evpxi or evxi this measures the expected gain in utility if we would decide based on perfect information about one or few variables of interest instead of being uncertain about them to implement our framework we investigate a case study on coral reef fisheries management for islands in the indo pacific it is a complex decision problem in a region that requires sound management of precious and fragile ecosystems eddy et al 2021 coral reefs are under intense pressure from various local and global stressors such as climate change pollution fisheries and other impacts burke et al 2012 a reef s ecological complexity poses substantial challenges for predicting the state of the reef under a management alternative while different modeling approaches can be helpful kelly et al 2013 we use a spatially explicit agent based model in our study miñarro et al 2018 decisions about their local management are not only difficult because coral reefs and their fisheries are intricately linked but they also fulfill a variety of needs in often resource constrained socio economic contexts ferse et al 2014 therefore the divergent objectives and perspectives of various stakeholders need to be considered in decision making typically the available data and our ability to collect data is limited requiring iterative and adaptive management approaches that include targeted efforts for the collection of additional information after discussing our framework for voi analysis section 2 we introduce the structure of the reef management decision case section 3 1 we describe how to analyze the evppi for predictions of fisheries management alternatives and discuss design choices for such efforts section 3 based on the analysis results section 4 we discuss the usefulness of voi analysis and suggest desirable extensions to our implementation section 5 2 framework for analyzing value of information for decision models 2 1 decision modeling making a rational decision between several alternatives is facilitated by a quantitative representation of the consequences of implementing an alternative and of the stakeholder preferences regarding these consequences the combination of predictive system models with preference models to evaluate decision alternatives we call a decision model see haag et al 2019b in the following we provide the conceptual background to voi analysis for such models fig 1 a concrete implementation is detailed in section 3 after a comprehensive problem structuring phase e g marttunen et al 2017 the first step of decision modeling is to predict the consequences y a y a 1 y a m for each alternative a on the system attributes 1 m that are relevant for decision making according to the stakeholders these predictions can come from a mathematical model but also from experts e g nicol et al 2019 given that environmental or socio ecological systems can neither be completely understood nor fully observed nor perfectly represented in a model it makes sense to conceptualize these predictive system models as probabilistic models with uncertain parameters or inputs reichert et al 2015 reichert 2020 therefore the consequences for an alternative a are better represented as a random vector y a as we do not obtain a point estimate for the predicted variables but distributions of predictions p y a the distribution of predictions is a more informative description of our knowledge than a point estimate or aggregated value the predicted distributions will often not be independent for instance the biomass of carnivorous fish may be negatively correlated with biomass of prey species the second step is to identify a utility function u that quantifies a stakeholder s preferences with regard to the predicted attribute levels and their uncertainties the utility function represents the trade offs the stakeholder is willing to make between uncertain consequences let y denote the set of all possible consequences for all attributes considered relevant in the decision a multi attribute utility function u y 0 1 maps from a space of predicted consequence regarding system attributes to a utility space it returns the utility of potential consequences with larger utilities representing more preferred system states keeney and raiffa 1993 standard utility theory does not consider uncertainty of preferences in the following we keep to this limitation a more extensive and rigorous treatment of multi attribute utility theory than outlined here can be found in textbooks e g keeney and raiffa 1993 french 1986 once we specified parameters of a utility function based on a stakeholder s preferences we can calculate the resulting multi attribute utility u y a for an individual prediction of consequences y a y a 1 y a m for instance y 1 could be the prediction for the coral cover in and y m the prediction for herbivorous fish biomass per m 2 three years from now however as the predictions are uncertain we need to evaluate their entire distribution given uncertain consequences y a for an alternative a we receive corresponding utilities u a u y a for a given stakeholder preference profile we obtain a ranking of the alternatives based on their expected utility eu 1 eu a e u a y u y p y a y d y with p y a the probability distribution of the consequences of implementing alternative a as measured by all of the system attributes the alternative with the highest eu is then determined by maximizing 2 max a 1 a e u y a utility theory is prescriptive because a rational decision maker should choose the alternative with the highest eu a given their preferences encoded in the function u and the probability distributions of predicted consequences p y a utility theory allows determining the optimal choice for individuals or groups with homogeneous preferences for environmental management decisions the societal utility of an alternative must be considered from conflicting stakeholder perspectives which decision making criterion should be considered rational or fair for a group or society is an unresolved question de jonge 2012 however making a number of contested assumptions such as the possibility for interpersonal comparisons of utility we can construct a group utility function from the individuals utility functions keeney 2013 to obtain a group expected utility eu g a of an alternative a across k stakeholders perspectives we aggregate individual expected utilities eu k using the weighted arithmetic mean the weights w k with 0 w k 1 are scaling factors for the utility between stakeholders that sum to one eq 3 3 eu g a k 1 k w k eu k a 2 2 value of information sensitivity analysis the optimal baseline choice in a decision is the alternative a obtained by maximizing eu given the current state of information and the current preferences however in practice we are usually concerned about the sensitivity or robustness of such a conclusion how bad would it be if we picked this seemingly optimal alternative but some information turned out to be different in reality for illustration let us consider a decision about a system in which only one among all the variables is uncertain following ideas introduced by felli and hazen 1998 we consider three perspectives of a decision s sensitivity to that uncertain variable threshold perspective if we varied the variable from its lowest to highest value is there any threshold level at which the optimal choice changes if not the decision is insensitive to that variable probabilistic perspective given the probability distribution of the variable how likely is it that the threshold is crossed if this is very unlikely the optimal choice remains insensitive to the variable even though a threshold exists utility foregone perspective how large is the expected difference in utility between our baseline choice and the optimal choice if the threshold would be crossed if the difference in utility is small enough the penalty for having chosen a suboptimal alternative will be small enough that we can consider the decision to be insensitive value of information can be thought of as a sensitivity measure that takes both the probabilistic and utility foregone perspectives into account felli and hazen 1998 borgonovo et al 2016 converse to utility forgone it measures the gain in utility we can expect from knowing a decision aspect such as a system attribute or parameter with more certainty with voi we thus measure how sensitive the optimal choice is to potential new information before we acquire it if the voi is small it is unlikely that the baseline optimal choice changes with that new information or the difference in utility that this change would bring is small thus voi supports us in prioritizing which uncertainties should be reduced by further information collection several variants of voi measures have been proposed they share the same concept but differ with regard to the additional information that is considered the expected value of perfect information evpi measures the sensitivity of obtaining perfect information on all modeled aspects of the decision eidsvik et al 2015 the expected value of partially perfect information evppi also abbreviated evxpi or evxi measures the sensitivity of obtaining perfect information on parts of the uncertainties for example one or several parameters eidsvik et al 2015 this is the measure we focus on in the following given that aleatory uncertainty cannot be reduced by more studies or data collection evpi and evppi can be thought of as upper bounds for the actual voi we can expect in practice we will always only obtain imperfect information the expected value of sample information evsi is a measure for the impact of obtaining additional data but not perfect information estimating evsi is still difficult in many contexts but it is an active area of research e g williams and johnson 2018 kunst et al 2020 2 3 the expected value of partially perfect information the idea of voi is to calculate the added value of a piece of information before we actually expend the effort and cost to obtain it in the case of evppi we estimate the expected gain if we knew one or a group of variables of interest parameters inputs or attributes with certainty a formal and extensive treatment as well as algorithms to estimate the evppi is available in the literature brennan et al 2007 eidsvik et al 2015 borgonovo et al 2016 heath et al 2017 here we sketch out the main ideas with the aim to provide an intuitive understanding of the approach in this study we focus on uncertainties of the predictions of the attributes however voi analysis can be conducted for other variables within the models by following the same approach let us assume we have determined the optimal baseline choice a for a coral reef management decision given the current state of data and knowledge we are now interested to find out how sensitive that choice is to one aspect of the decision our prediction of the resulting coral cover cc under a marine protected area mpa alternative we denote this variable of interest y i y cc mpa if we had perfect information about the resulting coral cover if an mpa were implemented what would be the expected added value to the decision the idea of the analysis is that we pretend we could with a certain cost and effort gain clairvoyance on this variable and found for instance that y c c m p a y c c m p a 58 this gives us a better estimate for the variable and can have three effects a the conditional distributions of the other variables y i y i y i change if they are not independent b the distribution of all utilities u a change if they are not independent from the variable of interest and c the optimal alternative might change we now calculate the maximum utility given this new information max a 1 a e u a y c c m p a 58 and determine the best alternative if the best alternative is still the baseline optimal choice a the additional information that coral cover under the mpa alternative will be 58 did not help us make a better choice than we would have made anyway rather there is an opportunity cost if effort was expended towards collecting this information consequently the value of this information would be zero as the eu of the optimal alternative given the information a and the eu of the baseline choice given the information is identical max a 1 a e u a y c c m p a 58 e u a y c c m p a 58 0 however if the new information changed our optimal choice this difference will be positive and indicates the value of collecting this information while this illustrates the voi for one particular value y c c m p a we are interested in the expected voi for y c c m p a over its entire probability distribution of the coral cover within the predicted range we can estimate this by sampling from the predicted probability distribution of coral cover calculating the corresponding eu given that we know this sampled value and tracking the corresponding change in the optimal choice if the optimal choice does not change for any of these values it implies that no matter how much we improve our predictions our conclusions would never change more formally for a risk neutral stakeholder we calculate the evppi about the variable of interest y i with 4 evppi y i e max a 1 a e u a y i e u a y i e max a 1 a e u a y i max a 1 a e u a the evppi can be viewed as the difference between a posterior value of the decision with new information and a prior value without the information as in the second part of eq 4 eidsvik et al 2015 if a stakeholders has an exponential utility function keeney and raiffa 1993 these are the respective certainty equivalents of the situation where information is available for free and where no further information is available eidsvik et al 2015 if a stakeholder is risk neutral the certainty equivalents are the expected values as in eq 4 if we expand our analysis to several variables of interest e g growth rates herbivore fish biomass revenue for fishers etc we can determine a ranking of the variables in terms of their evppi this provides a quantitative and transparent prioritization of the most relevant uncertainties this can serve as the basis for directing study design and deciding about upcoming expenditure of costs and efforts towards collecting data and information 2 4 given data approach to estimating evppi and threshold sensitivity since analytical solutions can rarely be found various simulation based approaches to estimate the evppi have been developed the classical procedure is a two level nested monte carlo simulation e g felli and hazen 1998 brennan et al 2007 however this requires many simulations in addition to account for dependencies in the distributions of predictions markov chain monte carlo mcmc or other conditional resampling procedures with high computational burden are needed with a nested approach strong and oakley 2013 the alternative we describe here is a given data approach that only requires a single loop monte carlo sample as one obtains from a probabilistic sensitivity analysis this procedure has been developed by strong and oakley 2013 and is more generally discussed by borgonovo et al 2016 the method was originally conceived for voi calculations based on the net benefit of alternatives here we adapt it to usage with eu for risk neutral preferences we focus on this approach because it can handle the conditional dependencies in the distributions of variables while being computationally more feasible in connection with complex system models as a single loop monte carlo sample is sufficient a drawback of the method is that it is only efficient for calculating the evppi of single variables to calculate evppi for sets of variables other approaches have been suggested strong et al 2014 heath et al 2017 while we refer the reader to the original publication by strong and oakley 2013 for the details the basic algorithm can be described as follows from a monte carlo simulation we receive s samples of the uncertain variables and of the s corresponding utilities for each alternative the vector of samples y i of the variable of interest y i for which we want to calculate the evppi as well as the corresponding utilities u a for all management alternatives are now both reordered such that y i 1 y i 2 y i s the superscript denotes the reordered position in the vector of variable samples the reordered samples and the corresponding reordered utilities are partitioned into k bins of equal size j with j k s for each bin k we calculate the eu for each alternative this is an approximation of the eu conditional on the value of the variable of interest y i being in this bin we then take the maximum eu across the alternatives the arithmetic mean of these maxima across all k bins is taken as an approximation to the first term in eq 4 e max a 1 a e u a y the second term of eq 4 the eu of the baseline optimal choice max a 1 a e u a can be directly calculated as in eq 2 the corresponding estimator for evvpi can be written as 5 evppi y i 1 k k 1 k max a 1 a 1 j s 1 j k 1 j k u s a max a 1 a 1 s s 1 s u s a we propose that the same algorithm is efficient for threshold sensitivity analysis that takes into account conditional distributions the aim here is to estimate the eu of alternatives given that a variable of interest y takes on different values and then identify threshold values of y where the optimal alternative changes this could be achieved by varying y in a one factor at a time sensitivity analysis however to take a regional view on sensitivity and account for dependencies across attributes and alternatives we need to consider the conditional distributions of all variables given the specific values of the variable of interest to understand the relationship between specific values of y and the eu of the alternatives we can employ the algorithm described above the relationship can be approximated by calculating the arithmetic mean of the variable of interest in each bin and the corresponding eu for each alternative in this bin thresholds can then be determined visually from a scatterplot or by calculating a threshold criterion as it is sample based we may not be able to identify one threshold but rather a range of y as a threshold region 3 implementation for a reef management problem 3 1 case study description 3 1 1 problem background in this case study we investigated the local reef management for an island that can be considered as typical in the spermonde archipelago in indonesia using the framework described in section 2 the spermonde archipelago is a complex of about 70 islands located off southwest sulawesi most of them inhabited and surrounded by coral reefs the region lies in the center of the coral triangle which is the most biodiverse marine region worldwide burke et al 2012 as for many small islands and island nations in the indo pacific it is necessary to find a balance between the exploitation and conservation of their natural resources in the face of local and global changes from livelihoods and ecosystem degradation to climate change and globalized economics major local stressors on the reefs in the area are overfishing and the use of destructive fishing techniques fish are consumed for nutrition locally but mostly sold partly as live fish radjawali 2012 target species are diverse and shift with global demands and local supply ferse et al 2014 a wide range of fishing techniques are employed among them destructive techniques such as bomb fishing or cyanide fishing for fisherfolk of the islands alternative livelihoods are often neither attainable nor desired ferse et al 2014 additionally fisherfolk are often embedded in elaborate systems of patron client relationships that provide benefits such as social protection glaser et al 2015 these relationships can influence fishing behavior and may encourage overfishing and destructive fishing glaser et al 2015 miñarro et al 2016 the effects of fishing pressure are exacerbated by pollution from point and diffuse sources as well as sedimentary run off stemming from the islands and the makassar urban area teichberg et al 2018 larger scale pressures such as global climate change are expected to cause ocean warming and sea level rise resulting in the decline of reef health for this study we investigated an exemplary reef site of a typical island in the region the site is 200 160 m and part of a larger reef area the majority of the seafloor is less than 5 m deep but descends to 15 m depth fig si 1 to improve the local situation we focus on fisheries management as a way to mitigate a primary local stressor for the reef 3 1 2 societal perspectives and objectives different users and interest groups hold varied perspectives on a reef site its value and its relevant services societal evaluation ultimately determines which form of management is optimal therefore we need to consider different stakeholder perspectives on the issue for the reef of the inhabited island we investigate we seek to represent a diversity of views by exploring four archetypal perspectives local livelihoods perspective focused on ensuring food security and economic benefit to local fishers reef conservation perspective focused on ecosystem health and resilience extraction perspective focused on maximizing fishing yield and economic benefits balanced perspective focused on balancing the different aspects and interests the values that are implicit in these perspectives can be expressed in the form of an objectives hierarchy keeney 1992 that we developed based on literature maynard et al 2017 brown et al 2018 mcfield and kramer 2007 as well as our knowledge of the case and context fig 2 we chose these four archetypal perspectives to illustrate a wide range of views on the management issue but they are not meant to represent actual individual stakeholders or groups for real world decision support these perspectives and their objectives need to be elicited and co developed locally we use these perspectives to guide an analysis for multiple stakeholder perspectives which can be adapted to a particular decision context 3 1 3 management alternatives attributes and time scale at the investigated reef site fishing occurs for subsistence and commercial reasons and destructive fishing techniques are employed occasionally see description of the no restrictions alternative in table si 1 to define management alternatives we first developed a strategy generation table gregory et al 2012 we identified three management factors 1 gear and technique restrictions 2 access restrictions and 3 fishing quotas based on the strategy generation table we developed four management alternatives expected to result in decreasing degrees of fishing pressure see table 1 no restrictions no restrictions with continued intense fishing pressure including destructive fishing no destructive fishing enforcing a ban of bomb and cyanide fishing mpa subsistence implementing a marine protected area which allows only subsistence fishing for locals mpa no take implementing a strict marine protected area making it a no take zone the management alternatives if implemented will have consequences for very different aspects of the socio ecological system of the island reef the consequences that are relevant for deciding between the management alternatives are captured by attributes of the system fig 2 and table si 2 the attributes should describe the consequences in a way that is understandable and useful for decision makers and stakeholders keeney and gregory 2005 the degree of fulfillment of the decision objectives can then be quantified based on the attribute levels for instance the attribute total biomass of browsers scrapers and grazers in g m 2 can be used to determine the achievement of the objective of having a high biomass of herbivorous fish in the reef the considered time scale of consequences can make a large difference in decision making short term and long term consequences can diverge for instance an over exploitation of fishing resources can be advantageous in the short term but detrimental in the long term in this study we consider consequences three to six years in the future 3 2 prediction of management consequences predicting the consequences of management alternatives on complex systems is a difficult task to predict the ecological consequences of management alternatives on reef fishes reef benthos and fisheries yield we adapted a previously developed model of the system seamancore miñarro et al 2018 this model combines an agent based model of fish stocks and fishing behavior with a corresponding model for benthic community dynamics the implementation is detailed in the next sections to account for the often neglected cost of implementing and enforcing management alternatives mccrea strub et al 2011 we used the required patrol days as a proxy attribute for these costs brown et al 2018 we used our contextual knowledge to estimate this attribute s distribution for each management alternative these were assumed to be poisson distributions with different rate parameters 3 2 1 model of reef and fishery dynamics under management alternatives seamancore is a spatially explicit 2d model that simulates the dynamics of the benthic and fish populations as well as fisheries in a coral reef through an agent based simulation miñarro et al 2018 we used seamancore to predict the temporal dynamics of the reef benthos and fish community in a 200 160 m area the predicted entities consisted of four benthic functional groups hard coral macroalgae and turf hard substrate and cropped algae non stabilized substrate stocks of three functional fish groups carnivores browsers and grazers scrapers and different fisheries table si 3 the aim for prediction is to capture the inherent stochasticity of the system as well as the directed effects of the management alternatives we achieve this by differentiating between core parameters and processes of the seamancore model that are unaffected by the alternatives and alternative specific parameters and processes if we intend to understand the effect of the alternatives it is only meaningful to compare the set of predictions where the core parameters are shared one model parameterization of the natural processes can be viewed as one potential configuration of the world the four management alternatives then lead to four different futures therefore for one core parameterization we created four simulations with the alternative specific parameters set in addition the difference between the four management alternatives was represented by modified inputs and parameters in the fishing module e g number of boats four types of fisheries were considered 1 bomb fishing 2 cyanide fishing 3 non destructive commercial fishing and 4 non destructive subsistence fishing see table si 2 for their specification in seamancore the different management alternatives allowed specific combinations of these types of fishing to occur in the reef from all types in the no restrictions alternative to none in the no take alternative table 1 for each simulation run we specified the initial conditions in a spatially explicit manner the water depth fig si 1 the benthic cover biomass of fish functional groups and the fishery modes to apply table 1 and table si 3 additionally we set over 100 other core parameters that govern the model behavior miñarro et al 2018 for each simulated time step the model outputs a 10 10 cm resolution map of benthic cover a 20 20 m resolution map of biomass of fish functional groups and the total fishery yield differentiated by depth target species and fishery type we simulated at a temporal resolution of 1 day for both the benthic and the fish grids the management alternatives were assessed against the background of an uncertain environment in which most aspects are beyond management control for instance biological processes such as death rates or feeding rates were assumed to be independent of management alternatives at the considered scale however these processes are usually variable and our knowledge about them is incomplete we represented this with two approaches firstly the rules governing the cellular automaton and model agents have a stochastic element for example rules to change a hard substrate cell to an algae cell are triggered only with a certain probability at each time step secondly we used probability distributions instead of point estimates for the core parameters we used the calibration of miñarro et al 2018 as a basis for the model parameterization we then defined probability distributions for 37 of the models parameters e g reproduction rates feeding rates time for colonization fig si 2 and 15 input variables initial benthic cover and initial fish biomass fig si 2 other parameters such as the probability of transition between benthic substrata remained the same across all simulations parameter distributions were identified based on literature values and judgment by the authors guided by natural constraints e g a death rate cannot be negative plausibility of the simulation results and using maximum entropy distributions distributions of model parameters were assumed to be independent except for initial benthic coverages which were assumed to come from a multivariate normal distribution we assumed all parameters to be constant across the modeled temporal and spatial dimensions by propagating this prior information through the model with monte carlo simulation we received a corresponding distribution of model outputs an external stressor we modeled explicitly were coral bleaching events eddy et al 2021 we included strong bleaching that turn coral cells into hard substrate and mild bleaching that resets the age of coral cells bleaching affects coral cells with a certain probability depending on the depth of their location the effects of other external stressors such as temperature change eutrophication or loss of connectivity to other reefs are captured indirectly by the distributions of the parameters such as colonization rates of coral and algae growth rates of functional groups or external recruitment 3 2 2 obtaining of decision relevant predictions to predict the uncertain consequences of management alternatives we used monte carlo simulations to obtain a population of predictions from independent runs of the seamancore model we drew 1100 samples from the probability distributions of the model core parameters fig si 2 these were combined with the definitions of each alternative the seamancore model was thus run for 1100 samples 4 alternatives 4800 times in total for five of the 1100 parameter combinations the model produced nonsensical results either the model returned an undefined value for biomass of a fish functional group or scraper biomass was continuously above a threshold of 95 g m 2 for more than 60 days these runs were excluded as we focus on the situation three to six years after implementing a management alternative we ran the model for 2280 time steps days and then took the values for years four to six into the future as the basis for the attribute predictions to smooth out short term noisy fluctuations due to asynchronous updating of fish and benthos grids we used a 42 day 6 weeks rolling mean of the time series for the benthos cover and a 14 day 2 weeks rolling mean for the time series of fish biomass the system model outputs are not directly of interest as attributes in the decision we therefore transformed and aggregated model outputs to obtain predictions for attributes that would be understandable to interested stakeholders fig 2 and table si 2 for instance the daily time series of fish biomass for the three functional groups was aggregated to arrive at weekly average total fish biomass the variability in time thus became part of the prediction uncertainty from the perspective of strategic management the spatially explicit output of the model was also not relevant therefore we aggregated the model outputs for the entire area the empirical joint distribution of the aggregated and transformed model outputs was the basis for the decision relevant attribute predictions we aimed for a sample of size s 120 000 for each attribute of each alternative to ensure equal sample size attributes measured on a weekly scale were downsampled from 170820 samples and attributes with monthly scale were upsampled with replacement from 42705 samples for the attribute required patrol days we directly drew s samples from the specified poisson distributions a crucial consideration when creating this sample of attribute predictions is that dependencies exist across attributes and alternatives this should be considered in voi analysis for instance if coral cover correlates with herbivore biomass having better information about coral cover will also inform us about herbivore biomass for an estimation approach of voi that is based on sampling we therefore need to create a sample that retains the relevant conditional dependencies in the predictions given the way we set up our simulations we obtain these dependencies from the seamancore modeling for other modeling approaches this can be less straightforward such dependencies exist within the predictions from a simulation run for one alternative firstly the parts of the ecological system are connected for example high carnivore biomass will often coincide with lower biomass of herbivorous fish secondly predictions are correlated in time as the future system state depends on previous time steps in the resampling of predictions for a management alternative we retained relations between samples for different attributes regarding points in time and simulation parameterization this means for one alternative a particular sample of all attributes comes from the same simulation run and time point dependencies also exist between the consequences of different alternatives because they are predicted based on shared core parameters as described above across the alternatives we retained relations regarding the core parameter samples this represents system properties or shared external influences that are the same for all alternatives however we randomized the resulting predictions regarding time in this way we represent different time points at which the effects of the alternatives are assessed in the future this means a particular attribute sample across the alternatives comes from a model run with the same core parameters but potentially different time points we illustrate the effect of retaining different correlations in fig si 7 3 3 evaluation of management alternatives 3 3 1 preference model structure and parameters to understand the differences in utility that the management alternatives would bring to different societal actors we specified a hierarchical utility model for each of the four archetypal stakeholder perspectives each model encodes an assumed preference profile for a stakeholder perspective and is meant to represent specific interests for that stakeholder based on the evaluations of management alternatives with these models we can then identify areas of conflicts and consensus as outlined in section 2 1 we can also calculate aggregate results across stakeholder perspectives in the following we outline our approach to hierarchical utility models a detailed treatment can be found in haag et al 2019a and reichert et al 2015 the structure of the preference models is given by a hierarchy of objectives fig 2 we assume that all stakeholder perspectives share the same set of objectives but differ in their preferences for instance the trade offs they are willing to make between the objectives to build the model we first specify a marginal value function for each of the seven objectives on the lowest level of the hierarchy v o p y p fig si 3 these map from the attribute space to a relative degree of achievement for each of these objectives then we aggregate these valuations along the hierarchy with nested aggregation functions f k that we specify for each aggregation step we arrive at a multi attribute value function over all attributes v y 1 y m with this function we receive an overall evaluation of each decision alternative lastly as discussed by dyer and sarin 1982 this multi attribute value function is converted to a utility function u v y 1 y m r at the highest objective in the hierarchy given the risk attitude r as we assume stakeholders to be risk neutral the eu of an alternative is its expected value based on the evaluation with the value function the preference model used to evaluate one set of consequences of alternative a can therefore be written as for the indices refer to fig 2 6 u y a f 0 f 1 v o 1 1 y 1 a θ o 1 1 v o 1 2 y 2 a θ o 1 2 w f 1 γ f 1 v o 2 y 3 a θ o 2 v o 3 y 4 a θ o 3 f 4 v o 4 1 y 5 a θ o 4 1 v o 4 2 y 6 a θ o 4 2 w f 4 γ f 4 v o 5 y 7 a θ o 5 w f 0 γ f 0 as aggregation functions f k for the values v on each hierarchical level we chose functions of the family of weighted generalized means also called power means with the form 7 f k v 1 v n w γ i 1 n w i v i γ 1 γ γ r with weight parameters scaling factors 0 w 1 and i 1 n w i 1 the parameters of the preference model eq 6 were changed for each of the stakeholder perspectives based on assumed preferences in line with their concerns see section 3 1 2 these parameters are how they evaluate changes on the attribute scales such as diminishing returns regarding fish catch or coral cover shapes of marginal value functions θ fig si 3 how they trade off changes in one objective relative to the other objectives weight parameters w table si 3 to what degree a poor achievement of objectives can be compensated for example can a high enough fished biomass compensate for a very low coral cover or is a one out all out view appropriate degree of non additivity γ table si 3 as the aim in this study was to explore the space of potential perspectives the preference profiles were designed by the authors according to the archetypes see 3 1 2 for a practical decision problem the parameters should be inferred from stakeholder data these data can be collected by choice experiments hensher et al 2015 or other forms of preference elicitation haag et al 2019a 3 3 2 estimation of expected utility based on the empirical distributions of attribute predictions we first calculated the optimal baseline choice each stakeholder preference profile was treated separately for each sample y y 1 y 7 of the 7 attributes we calculated the utility of each alternative using the respective preference model since we have s 120 000 samples we obtained 120 000 utilities the set of which we denote u a for alternative a by taking the arithmetic mean of u a we received the eu for each alternative assuming risk neutrality e u a the rational imperative is to pick the alternative with the highest eu max a 1 4 e u a this is the optimal baseline choice for a stakeholder preference profiles given our current information about the system attributes based on the evaluation of management alternatives with the four preference profiles associated with the stakeholder perspectives we can identify areas of conflicts and consensus for also providing an aggregated view across the stakeholder perspectives we followed the approach outlined in section 2 1 using eq 3 we calculated the eu of an alternative across perspectives giving all perspectives equal weight 3 4 sensitivity analysis using value of information value of information analysis can be conducted for any uncertain inputs or parameters of a decision model for this case study we focused on exploring the evppi with regard to uncertain attribute predictions the aim was to understand the impact of better knowledge of the predictions of specific attributes to identify whether the uncertainty of an attribute prediction was more relevant for some alternatives than others we focused on individual variables for each of the alternatives and calculated the evppi of 4 alternatives for 7 attributes resulting in 28 variables of interest that is if we had perfect knowledge of some attribute s predictions y i a for alternative a how much additional utility would this be expected to provide to a stakeholder to estimate evppi we implemented the algorithm as described in section 2 4 and ran the analysis separately for each of the 28 variables of interest and each stakeholder perspective we also calculated the evppi across perspectives based on averaging the eus of the perspectives in each bin and on the eu of the baseline optimal alternative across the perspectives for the main analysis we chose values of k 300 and j 400 to mitigate the chance of bias we also investigated the dependency of evppi on these choices fig 7 fig si 9 to understand the added value of evppi over a simpler threshold view on sensitivity see section 2 2 we conducted a threshold sensitivity analysis this means we estimated the eu of alternatives given that a variable of interest y takes on different values as described in section 2 4 we repeated this analysis for all 28 variables of interest and each stakeholder to identify thresholds visually we normalized the eu values to the baseline optimal alternative for a stakeholder and estimated a smoothing spline model for this relationship 4 results 4 1 predicted consequences of reef fishery management the inputs and parameters of the predictive system model were described by probability distributions together with the stochastic processes in the model this led to distributions for the obtained outputs and consequently for the derived predictions of the relevant attributes fig 3 if we would only consider point predictions in our decision making e g median lines in fig 3 we would disregard a lot of relevant information the distributions of attributes that describe the state of the reef coral cover herbivore biomass and total fish biomass are wide this means the predictive uncertainty about their future is high the differences between alternatives appear less pronounced when considering the marginal distributions this suggests the alternative had relatively less effect on the outcome compared to the stochasticity of the system as expected decreased fishing pressure generally leads to increased fish biomass especially of carnivores increases in herbivore biomass are smaller due to the increasing predation pressure from carnivores coral cover increases when destructive fishing is stopped but decreases slightly when fishing is stopped completely as feeding pressure by scrapers increases the distributions of attributes connected with fishery yield carnivorous and herbivorous biomass to sell fish for local consumption exhibit long tails fig 3 with the high fishing pressure in the no restrictions alternative few carnivorous fish can be sold due to over exploitation and hence stock depletion in the considered 3 6 year time frame the carnivorous fishing yield is higher in total with less intense fishing on the other hand when only subsistence fishing occurs there is little surplus of especially herbivorous fish to be sold with the high fishing pressure of the no restrictions alternative it is more likely that not enough fish can be caught for local consumption in comparison to the alternatives with no destructive fishing or with a protected area that allows subsistence fishing with a strict no take zone no fishing is assumed to occur therefore the attributes related to fishery yield are always zero 4 2 optimal baseline choice under uncertainty given the uncertain predictions of the attributes and our preference models for the different perspectives section 3 3 we calculated the utility for each predicted sample of the alternatives distributions in fig 4 the expectation over these utilities the expected utility eu is the criterion that a rational decision should be based on this eu integrates over the predictive uncertainties and is therefore a single number solid markers in fig 4 for the balance and local livelihoods preference profile a ban of destructive fishing practices would be the optimal alternative for the conservation profile a marine protected area mpa with only subsistence fishing and for the extraction profile the alternative with no restrictions would be most desirable fig 4 except for the conservation profile a strict mpa with a no take zone receives the lowest eu in all profiles for the conservation profile the no restrictions alternative results in sightly lower eu this can be explained by the missing fulfillment of any socio economic objectives by an alternative that enforces a no take zone the alternative with no restrictions is not optimal for most preference profiles even a moderate restriction of fisheries can lead to higher fish biomass and also fished biomass especially of carnivorous fish even in the short time frame studied based on these results no clear consensus for a best management alternative emerges between the different perspectives however we can identify two aspects that might help come to such a consensus in an iterative process 1 some reduction in fishing seems beneficial for fishery yield even in a short time horizon and 2 the lack of fulfillment of socio economic objectives due to a complete ban of fisheries can hardly be counterbalanced by better conservation outcomes 4 3 threshold view on decision sensitivity there are large overlaps in the distributions of the alternatives utilities fig 4 this means we might see future system states in which different conclusions about the optimal alternative would be drawn therefore we need to investigate the sensitivity of the decision before considering the voi view of sensitivity analysis we identify thresholds for individual variables of interest here the attribute predictions for one alternative that lead to changes in the optimal alternative in fig 5 we see the resulting eu of the management alternatives relative to the baseline optimal alternative as we vary a variable of interest along its range this relative eu is given as a function of a variable of interest while retaining a probabilistic view and the correlation structure in all other variables based on this we can identify thresholds at which the ordering of the decision alternatives changes since our results are based on simulations the thresholds are small regions rather than exact points as an example the baseline optimal alternative for the conversation perspective is the mpa with subsistence fishing we can now investigate the decision s sensitivity to the predicted coral cover of the mpa with subsistence fishing alternative lower left panel of fig 5 results for all variables and perspectives are given in figs si 5 8 there are two thresholds if we knew the coral cover of the mpa with subsistence fishing alternative would turn out to be below 23 the no restrictions or no destructive fishing alternatives would now provide higher utility if we could be certain that the coral cover of the mpa with subsistence fishing alternative would be between 23 and 62 it would be the optimal choice in this region the voi is zero as the best alternative is not sensitive to the precise value of the coral cover prediction if the coral cover of the mpa with subsistence fishing alternative would be higher than 62 the no take mpa would be optimal this may seem counter intuitive as all else being equal the utility of the mpa with subsistence fishing alternative should increase with increasing coral cover as higher cover is preferred it is however a consequence of the correlation structure in the predictions either the high coral cover for that alternative coincides with less preferred consequences on its other attributes or it coincides with even more preferred consequences for the no take mpa alternative the analysis falls short in two regards first we do not take into account how probable a crossing of a threshold would be how probable would it be that we actually see coral cover greater than 62 under the mpa with subsistence fishing alternative second once we crossed a threshold we disregard how large the potential gain in utility would be from taking the optimal instead of the now sub optimal alternative if coral cover was above 62 how much higher would the utility of deciding for the no take mpa alternative be in comparison to sticking with the mpa with subsistence fishing both aspects are crucial for understanding the sensitivity of a decision this is the point of the analysis of the voi 4 4 results of the value of information analysis to have a more comprehensive measure of decision sensitivity than the threshold view we calculated the evppi of the variables of interest the lower the evppi of a variable is the lower the sensitivity of the decision to it and vice versa if we had perfect information about that variable this either would seldom change the optimal alternative the gain in utility due to choosing the new optimal alternative would be small or both a ranking of the variables of interest based on their expected voi can then support us in identifying the key uncertainties and prioritizing their resolution the evppi varies by variable of interest and stakeholder preference profile fig 6 table si 5 comparing all stakeholder perspectives two commonalities exist the attribute of required patrol days had relatively low evppi while the attribute regarding fish available for local consumption had high evppi otherwise the results are more nuanced across variables the livelihoods perspective often receives lower evppi than the other perspectives the decision is less sensitive for this perspective this demonstrates how the voi depends on the stakeholder preference models and how far away in terms of probability of change the baseline optimal alternative is from the others see fig 4 the evppi is not directly linked to the width of the probability distributions of the variables fig 3 the herbivore biomass of the no restrictions alternative has a markedly narrower distribution than the total fish biomass of this alternative yet for all profiles as well as across profiles both have a similar evppi as expected for variables that are known with certainty for instance fisheries variables in the no take mpa alternative the evppi is zero for the conservation perspective we can summarize our analysis as follows for brevity we do not discuss the other perspectives here but they can be similarly analyzed the optimal baseline choice given our current state of knowledge would be implementing an mpa with subsistence fishing fig 4 however this choice is sensitive to the actual realizations of the predicted attributes thresholds exist that would make a different choice optimal fig 5 and fig si 6 we expect the optimal choice to be sensitive to the days with food for local consumption herbivore biomass in the reef and total fish biomass fig 6 we expect it to be insensitive to the number of patrol days and the actual fished biomass for selling for any variable except the coral cover better knowledge about the true consequences when implementing the no take mpa alternative is hardly relevant for the decision green bars in fig 6 this alternative is unlikely to become the best one for the conservation perspective based on the evppi analysis the conclusion for the conservation preference profile would be that understanding the trajectory of the reef and its organisms better should be a priority however the uncertainty about the fish for local consumption is also relevant further efforts directed at improved understanding of these aspects will be most critical for decision making as the determined baseline choice may not be the best if our knowledge about the respective attributes was improved on the other hand further investigation of the patrolling effort or the sold fish biomass is unlikely to change the conclusions regarding the optimal management alternative for the other preference profiles the list of priorities differs with some commonalities as described above the estimation algorithm for evppi that we propose in section 2 4 has a hyper parameter the size of the bins j the choice of j can have a significant effect on the resulting estimate fig 7 and fig si 9 for small bin sizes the estimator is upwardly biased due to the maximization step as j 1 the estimates converge to the expected value of perfect information across all variables evpi if each sample is placed in its own bin i e j 1 the estimated evppi is equal to the evpi the estimated evppi converges to zero as j s as both terms of eq 5 become equal thus for large sizes of the bins the estimator is downwardly biased in our case most evppi estimates are relatively stable using from 100 bins with 1200 samples each to 12000 bins with 10 samples each fig 7 this confirms our choice of a bin size of 400 for the analyses above however in specific cases estimates can also be sensitive to the bin size e g days with fish for local consumption in fig 7 or fig si 9 5 discussion 5 1 relevance of value of information analysis for the case study in the case study on coral reef fisheries management we found that the attributes measuring consequences related to fish biomass food security and sales of fish groups had the highest evppi for at least one stakeholder preference profile on the other hand better estimates for the required patrolling effort had low evppi for any single and across stakeholder perspectives likewise hard coral cover which is routinely monitored and prominently reported was not among the top three variables in terms of evppi for any perspective the analysis provided a reasoned and prescriptive focus for the design of future investigations for the decision case the effect of the management alternatives on the reef fish their catch and livelihood impacts should be a focus of future data collection efforts to reduce decision uncertainty however this conclusion depends on the stakeholder perspective considered for the conservation perspective better knowledge about the amount of fish sold had low expected informational value whereas for the livelihoods perspective better knowledge about the biomass of fish functional groups in the reef was not very relevant fig 6 importantly the evppi is not directly linked to the extent of uncertainty in the predictions fig 3 even though the coral cover had a wide distribution for each management alternatives having perfect information on it did not have high informational value consequently to improve decision robustness it is not always the largest uncertainties that require addressing rather it is the most decision relevant uncertainties which can be identified through the analysis of voi this is in line with several studies that have investigated the factors that influence the voi in a decision but found it can vary in unexpected and sometimes counter intuitive ways e g eeckhoudt and godfroid 2000 gould 1974 dependencies between alternatives can be one influencing factor while we may be very uncertain about the future coral cover we may be quite certain that one alternative results in higher coral cover than another cf reichert and borsuk 2005 delquié 2008 has shown that under quite general assumptions the voi is highest when a stakeholder s baseline choice is indifferent between two alternatives the voi decreases with increasing utility difference between the alternatives in the baseline case our results show the same pattern as for the livelihoods and extraction preference profiles which have a larger spread among the utility of alternatives fig 4 the evppi is generally lower than for the other two preference profiles fig 6 value of information is specific to the investigated decision if the informational value of a variable e g coral cover is low in a particular decision this does not imply we should stop regular monitoring historical baselines and operating protocols remain important and can be of great value in another decision and for improved understanding of the complex system dynamics 5 2 evaluation and outlook of voi framework this study showed how voi analysis and specifically the evppi is a useful form of sensitivity analysis for decision models based on the approach in this study we highlight three key directions for further development that we consider relevant in the context of environmental management decisions the first direction is extending the approach to uncertainty of preference model parameters standard utility theory e g french 1986 keeney and raiffa 1993 does not consider uncertainty of preferences nor do any voi applications we are aware of however in practice the stakeholder preferences are also uncertain and we have shown in this study that the preference model can make a substantial difference for the voi analysis results the uncertainty of stakeholder preferences could be included in voi analysis by using the expected expected utility concept chajewska et al 2000 haag et al 2019b considering the uncertainty about the consequences of management alternatives and the uncertainty about the societal evaluation of these consequences on equal footing in voi analysis will allow us to differentiate better where further studies are actually needed depending on the case the uncertainty about the social evaluation could be the primary cause for decision uncertainty gregory et al 2006 the second direction for development is improving uncertainty quantification of the variables of interest e g the predictions quantitative voi analysis is only meaningful to the degree that we can specify or infer probability distributions for these variables ideally derived from empirical sources the question of a variable s voi is only ever addressed in the small world savage 1954 of our specified model however the assessment of uncertainties in predictive system models is usually limited especially regarding structure and dependencies we also disregarded crucial structural uncertainties in the predictive reef model missing processes such as changes in fish population structure might entail larger uncertainties than all the included parametric uncertainty more comprehensive uncertainty assessments are a large task for the environmental modeling community but there are many advances in this direction e g uusitalo et al 2015 reichert 2020 a way to address large uncertainties that are difficult to quantify examples for reef systems are crown of thorns starfish outbreaks or powerful storms are scenarios e g walker et al 2003 wright et al 2019 scenarios can be modeled by repeating the analysis for different possible futures in the form of constraints or modified ecosystem processes and qualitatively evaluating the differences in the conclusions this would be possible without fundamental changes to the presented approach the third direction for development is advancing methods to estimate evppi more efficiently especially with environmental models that often entail significant computational effort simulation based approaches that rely on many model runs and resampling from conditional distributions such a nested markov chain monte carlo brennan et al 2007 felli and hazen 1998 are infeasible the algorithm we implemented based on strong and oakley 2013 and borgonovo et al 2016 is fast only requires a given probabilistic sensitivity analysis sample and can handle conditional dependencies on the other hand it can only be sensibly used for investigating the evppi of single variables and it still requires a large sample size an important consideration when using the proposed algorithm is the choice of the bin size j as this can lead to bias as investigated in section 4 4 as strong and oakley 2013 have also shown before the upward and downward biases appeared for extreme values of j with a large region of stability in between these however in our case there were few estimates where such a stable region was small fig si 9 the conditions under which the algorithm can reliably be used thus require further investigation 5 3 value of information analysis to support iterative environmental management voi analysis is most useful if we can take actions to address the key uncertainties and improve the information state before or after decision making thus it fits well with decision contexts that have an iterative aspect such as the regular strategic considerations of environmental monitoring programs for the case study we presented the first steps of such an iterative approach based on the analysis results we can decide to a move forward with implementation of the baseline optimal alternative b gather more information if we deem the decision too sensitive to potential new information or c conduct implementation and information gathering in parallel in the environmental domain adaptive management is a common iterative approach and voi analysis has been used to improve adaptive management decisions e g moore and runge 2012 williams et al 2011 runge et al 2011 in adaptive management we plan monitoring and data collection activities in parallel to implementing a management alternative there will be feedback processes in the system after implementing a management alternative that we need to take into account in long term management therefore the idea of revisiting the same decision context and updating our state of knowledge is key this can include 1 updating changed stakeholder perspectives 2 monitoring how the predicted trajectories have played out 3 updating the future model predictions and 4 coming up with the plan in terms of what to focus on for the next monitoring phase in modeling approaches that optimize over iterative management problems such as markov decision processes voi analysis can also play an important role chadès et al 2017 williams et al 2011 williams and johnson 2018 using voi in environmental management and conservation practice is still at an early stage though its value is increasingly recognized see studies in bolam et al 2019 keisler et al 2014 along structured decision making approaches gregory et al 2012 many opportunities for broader application of voi analysis beyond local management or conservation decisions exist for example voi analysis can be of interest when designing large scale research programs rushing et al 2020 or monitoring programs bal et al 2018 the long term benefits of investing in voi analysis include a well reasoned allocation of resources as it provides a ranked list of the expected benefit of addressing uncertainties for voi measures that are based on utility this relative benefit can be difficult to interpret if costs and benefits of additional information are not measured in the same units we cannot directly determine which uncertainties we should resolve any practical decision about information collection requires making trade offs with the opportunity cost of acquiring this information e g maxwell et al 2015 a straightforward approach to this issue is a cost benefit analysis to find the pareto optimal set of cost efficient information seeking activities marchese et al 2018 6 conclusions difficult environmental decisions can benefit from structured approaches gregory et al 2012 and the conceptual foundation of rational decision making under uncertainty is well established keeney and raiffa 1993 reichert et al 2015 understanding the sensitivity of a decision to uncertainties remains a key challenge to support better decision making our intuitions about the benefit of more information may not be correct but the costs of additional data acquisition are often high we propose that sensitivity analysis known under the umbrella term value of information voi analysis is useful to estimate the robustness of current conclusions and indicate where to focus future data collection the complexities of environmental issues make the practical application of voi analysis challenging in this study we tackled voi analysis in the framework of multi attribute value utility theory mavt maut with 1 a continuous uncertain prediction space 2 dependencies in the distribution of these predictions 3 multidimensional objective functions that include trade offs between objectives and 4 divergent stakeholder perspectives this included adapting a fast algorithm for estimation based on a probabilistic sensitivity analysis sample we analyzed expected value of partially perfect information evppi for a decision model of local coral reef fishery management this led to a ranking of the sensitivity of predictive uncertainty of management alternatives our framework can be used as a template for other decision cases two simple but practically relevant conclusions were corroborated in our case study first the evppi of a variable cannot directly be mapped to the extent of uncertainty in that variable large uncertainties in predictions do not prevent robust decision making per se cf reichert and borsuk 2005 more data collection is not always the answer second the results of voi analysis depend on the preference models used to evaluate the predicted consequences cf delquié 2008 the variables that stakeholders require more information on can differ if we disregard the variety in stakeholder perspectives any voi analysis will give an incomplete picture of the actual value of a piece of information in a specific context value of information analysis fits into many structured and iterative approaches to decision making and assessment such as adaptive management it facilitates identifying and ranking fig 6 key uncertainties and thus key aspects for further investigation and data collection while the extent and intricacy of a quantitative modeling approach as employed in this study will need to be aligned with the concrete needs and resources available any practical decision case can benefit from a deliberation about the value of new information which information if any would likely change our conclusions credit authorship contribution statement fridolin haag conceptualization methodology software formal analysis writing original draft review editing visualization sara miñarro methodology resources writing review editing arjun chennu conceptualization methodology resources writing review editing visualization supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank daniel schürholz for coding advice this project benefited greatly from the resources of the zmt datalab we are thankful to miroslav shaltev and joscha schmiedt for support we also thank the three reviewers and the editor for their excellent comments and suggestions appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105552 appendix a supplementary data the following is the supplementary material related to this article additional information on the case study implementation and results are provided in the supplementary material available at https doi org 10 1016 j envsoft 2022 105552 mmc s1 
25503,automatic calibration autocalibration of models is a standard practice in hydrologic sciences however hydrologic modelers while performing autocalibrations spend considerable amount of time in data pre processing coding and running simulations rather than focusing on science questions such inefficiency as this paper outlines stems from i platform dependence ii limited computational resource iii limited programming literacy iv limited model structure and source code literacy and v lack of data model interoperability in the so called autocalibration process by expanding and enhancing an existing web based modeling platform swatshare developed for the soil and water assessment tool swat hydrologic model this paper demonstrates a generalizable pathway to making autocalibration efficient via cyberinfrastructure ci solutions swatshare is a collaborative platform for sharing and visualization of swat models model results and metadata online this paper describes the front and back end architectures of swatshare for enabling efficient swat model autocalibration on the web in addition this paper also demonstrates three implementation case studies to validate the autocalibration workflow and results results from these implementations show that swatshare autocalibration can produce streamflow hydrograph and parameters that are comparable with commonly used offline swatcup calibration outputs in some instances the parameter values from swatshare calibration are more physically relevant than those from swatcup although the discussion in this paper is in the context of swat and swatshare the conceptual and technical design presented here can be used as an open science blueprint for similar ci enabled developments in other hydrologic models and more importantly in other domains of earth system sciences keywords calibration high performance computing hydrology swat model swatshare data availability see the software availability section in the manuscript software availability name swatshare 2 0 type web based platform year of first version release 2016 year of second version release 2022 availability openly available at https mygeohub org groups water hub swatshare cyberinfrastructure used for deployment mygeohub https mygeohub org via hubzero platform purdue university software web services programming languages used adobe flash php javascript java rest api redhat linux mysql lamp linux apache mysql php software stack tomcat web services python gdal fiona r dataretrieval hydrogof network file system nsf non sorting genetic algorithm ii high performance computing resources used a purdue campus cluster and the extreme science and engineering discovery environment xsede comet cluster developers i luk kim adnan rajib lan zhao venkatesh merwade carol song 1 0 introduction the predictability of a hydrologic model largely depends on how well its parameters are calibrated because numerous manual iterations to search representative parameter values can be time consuming dawdy and o donnell 1965 hydrologic modelers use an automated iterative process using complex algorithms hereafter autocalibration duan et al 2006 gupta et al 1999 samadi et al 2020 wu et al 2021 over time this autocalibration has become a standard practice in hydrologic model applications leading to the widespread development of open source tools that can link calibration algorithms with respective model source codes e g abbaspour 2015 wi et al 2017 wu and liu 2012 these autocalibration tools when provided with adequate reference datasets produce hydrologic simulations with reasonably high accuracy yet it will not be an overstatement to refer the term autocalibration as a misnomer because many data and model integration tasks required to set up and run the contemporary autocalibration tools heavily rely on modelers manual interventions as a result despite the notable recent improvements in calibration algorithms and open source tools e g baracchini et al 2020 chlumsky et al 2021 femeena et al 2020 sadler et al 2019 wang and brubaker 2015 zhang et al 2013 hydrologic modelers spend considerable time in data pre processing coding and running simulations rather than focusing on science questions therefore having a guided step by step workflow and or an intuitive graphical user interface do not necessarily lead to an efficient autocalibration procedure addressing the root cause s of inefficiency in hydrologic model autocalibration still remains a challenge below we identify five root causes of inefficiency in autocalibration tools and then we compare some of the contemporary hydrologic models according to the efficiency of their respective autocalibration tools table 1 1 platform dependence most of the autocalibration tools recognized across the hydrologic modeling community are developed as offline desktop based black box software e g swat cup for the soil and water assessment tool swat vic assist for the variable infiltration capacity vic model hspexp hspexp for the hydrologic simulation program fortran hspf abbaspour 2015 epa 2015 wi et al 2017 these tools rely on a particular computer operating system i e windows or linux dependencies e g specific program libraries packages and their versions and licensing requirements correspondingly a major issue associated with such platform dependence is maintenance and updating of functions yu et al 2019 over time some autocalibration tools e g hspexp epa 2015 have become obsolete due to incompatibility with current computer operating systems 2 limited computing resource commonly used desktop based autocalibration tools often conduct one calibration job at a time on a single desktop computer the computing resource in a desktop computer may be adequate to run small scale calibrations involving small watershed areas and short simulation periods however such small scale calibrations become computationally intensive when the models undergo multi objective calibrations with spatially distributed data and involves a large parameter set e g kunnath poovakka et al 2016 rajib et al 2018a obviously calibrations involving large watersheds and long simulation periods e g du et al 2018 high spatial resolution e g lin et al 2018 and complex spatial discretization schemes e g evenson et al 2018 demand more computing resources than an average desktop computer alone can provide in any case modelers commonly run multiple instances of the same calibration setup as trial runs to gather priori knowledge of sensitive parameters suitable parameter values and characteristic model bias kuzmin et al 2008 which make the overall computational footprint and run time unmanageable against these needs efficiency of commonly used desktop based autocalibration tools remains limited by the processing power and available storage space in modelers personal computers some of the recent autocalibration tools allow model calibration to be remotely executed in high performance computing hpc clusters e g lcc swat and gswat bashyt for the swat model bacu et al 2011 cau et al 2013 zamani et al 2021 the model independent cloud based parameter estimation pest tool fienen et al 2011 there are also efforts to speed up calibration tools using graphics processing units gpu aided parallel computing ercan et al 2014 freitas et al 2022 kan et al 2019 yet it is widely acknowledged that running autocalibration tools using hpc and or parallel computing also requires a client server interface and standard web services for job management resource monitoring messaging user verification data transfer encrypting and various notification mechanisms hokkanen et al 2021 zhu et al 2016 which may be challenging to sustain unless the tool is part of a large holistic modeling platform 3 programming literacy some of the autocalibration tools do not have a graphical user interface in such cases running the tool requires substantial literacy on programming languages syntax and execution for example use of pywrfhydrocalib for the wrf hydro model lin et al 2018 ncar 2019 r swat fme for the swat model e g wu and liu 2012 2014 and ostrich for the storm water management model swmm behrouz et al 2020 macro et al 2019 require medium to high level skills in python and r codes similarly without knowing how to write in command line interface modelers cannot run the pest tool for the gridded surface subsurface hydrologic analysis gssha model skahill et al 2009 2012 in short it is challenging to run some of the existing autocalibration tools without programming skills which limits their wider acceptability within the hydrologic modeling community 4 model structural literacy besides the knowledge of hydrologic processes and steps to setup and calibrate a model it is necessary for a modeler to clearly understand the model s internal structure including the model s geodatabase source code and parameter definitions because the existing autocalibration tools are not fully coupled with the model structure modelers running these tools with limited model structural literacy need to follow a steep learning curve for example while using pywrfhydrocalib and hspexp hspexp modelers need to manually decide on the spatial scale at which the tool will iterate a parameter at individual subbasin scale or entire basin scale or the nodes the tool will use as calibration sites specific subbasin or river ids epa 2015 ncar 2019 also see the scaling problem discussed by nijzink et al 2018 tsai et al 2021 xu et al 2014 zhang et al 2017 some tools do not offer a function to automatically fit the most optimal parameter combinations back into the model source code after completing a batch of iterations e g swat cup and lcc swat ozdemir and leloglu 2019 zamani et al 2021 nonetheless the reduced need for model structural literacy is the most desired service that an autocalibration tool can offer to educators who often want to avoid the steep learning curve the intuitive graphical user interface in tools like swat cup and vic assist is helpful yet inadequately detailed to substantially reduce the need for model structural literacy 5 data model interoperability use of multiple sources types of reference datasets in model calibration requires a meaningful linking of the data with the model structure which should handle the space time variable continuum without misleading the parameter search procedure dembélé et al 2020 gardner et al 2018 jadidoleslam et al 2020 while there are advances in developing web based platforms to perform spatio temporal query across a region and bulk download available in situ and remotely sensed earth observation datasets ames et al 2012 csiss 2021 gee 2022 none of the existing autocalibration tools can automatically do the data post processing and model linking tasks therefore for a large scale model calibration involving many in situ measurements e g abbaspour 2015 du et al 2018 lin et al 2018 rajib et al 2020a b or for a small watershed scale model calibration involving spatially distributed remotely sensed estimates rajib et al 2018a it may be easy to download required datasets through web based platforms but it remains an excruciatingly labor intensive task for a modeler to process the data let alone link each of those datasets explicitly with the corresponding calibration sites in the model e g river ids unfortunately all the existing calibration tools are designed with an assumption that downloading pre processing and making the reference datasets ready for the calibration tool are auxiliary tasks conducted outside the tool s graphical coding interface scientific cyberinfrastructures cis can address all five root causes of inefficiency in hydrologic model autocalibration through a findable accessible interoperable and reusable fair open science platform bandaragoda et al 2019 chen et al 2020 govindaraju et al 2009 kalyanam et al 2019 2020 maidment 2008 voinov and costanza 1999 wilkinson et al 2016 the fairness or openness of cis is due to their building blocks which generally include hpc resources containerized models code wrappers automated workflows geospatial data analysis gis interface and other tools needed for their interoperability and reproducibility all through an open web based environment e g essawy et al 2020 le et al 2015 wu et al 2013 zhang et al 2019 considering these benefits many contemporary developments in earth system sciences reflect a trend of ci solutions for example recently developed web based gis environments allow running a hydrologic model using hpc resources without having to possess in depth programming and model development skills e g liu et al 2014 lyu et al 2019 similarly inclusion and removal of modules and codes in a simple drag and drop plug and play fashion and thereby enabling the total platform independence of a modeling workflow is becoming increasingly feasible dunlap et al 2013 lodhi et al 2020 peckham and goodall 2013 zeng et al 2020 what has further widened the scope of hydrologic modeling in a ci platform is the ability to link multiple cis that allows greater utilization of available data repositories computational environments and model application programming interfaces apis castronova et al 2013 essawy et al 2018 choi et al 2021 correspondingly modelers are now developing interoperability engines zhang et al 2021 to resolve the heterogeneity in model and data types e g chen et al 2020 gregersen et al 2007 hutton et al 2020 peckham et al 2013 in line with the above efforts swatshare rajib et al 2016 referred here as swatshare 1 0 https mygeohub org groups water hub swatshare was developed as a unique swat modeling platform leveraging the ci capabilities of mygeohub kalyanam et al 2019 2020 https mygeohub org besides the collaborative platform for model and metadata sharing and interoperability with other collaborative cis like hydroshare morsy et al 2017 tarboton et al 2018 swatshare 1 0 offered basic autocalibration functionalities unable to address all five root causes of calibration inefficiency identified earlier see table 1 thus swatshare 1 0 has been substantially modified in swatshare 2 0 by incorporating a new highly efficient autocalibration tool that addresses all root causes of calibration inefficiency by harnessing ci capabilities the overall goal of this paper is to introduce the new swat model autocalibration tool in swatshare 2 0 the specific objectives of this paper are to i describe the ci enabled functionalities in swatshare that make the new autocalibration tool efficient compared to the previous version rajib et al 2016 as well as a widely used desktop based tool called swat cup ii describe the software architecture and simulation workflow of the new tool and iii present three implementation case studies to validate the tool s design concepts as well as physical consistency in terms of hydrologic processes and parameters although the discussion presented in this paper is in the context of swat and swatshare the conceptual and technical design can be used as a blueprint to reproduce similar functionalities in other hydrologic model autocalibration tools 2 0 swat model autocalibration in swatshare 2 1 the genetic algorithm the non sorting genetic algorithm ii nsga ii deb et al 2002 is incorporated into swatshare 2 0 for swat parameter optimization replacing the shuffled complex evolution algorithm sce ua duan et al 1992 van griensven and bauwens 2003 van liew et al 2005 previously used in swatshare 1 0 rajib et al 2016 nsga ii has proven to be effective for multi objective hydrologic model calibrations and complex watershed management decisions e g alam et al 2018 bekele and nicklow 2007 dai et al 2017 ercan et al 2020 monteil et al 2020 besides genetic algorithms ability to mimic natural selection in the physical world using the principles of genetics and thereby produce physically meaningful solutions to parameter optimization problems haupt and haupt 2003 gregory 2009 there are three key factors that make nsga ii a better fit than sce ua for hydrologic modeling cis these factors include i ease of use in a web interface due to minimal user inputs ii rapid convergence to optimal solution and iii adaptability with a parallel computing environment jeon et al 2014 tang et al 2006 zhang et al 2012 2013 a preliminary version of swatshare s nsga ii algorithm written in python programming language has been evaluated by ercan and goodall 2016 a brief description of this algorithm is provided below the nsga ii in swatshare uses latin hypercube sampling lhs method to create the initial parent population ercan and goodall 2016 thus reducing the number of generations significantly to reach the pareto front much quicker than starting with a random parent population ercan and goodall 2016 bekele and nicklow 2007 initial parent population size must be at least twice the population size once the initial parent population objective functions are evaluated within the swat model the non dominated sorting method ranks solutions in groups the best performing groups are selected to create the mate population which has a predetermined constant size population size the crowding distance method is used to select members within the same ranking to reach the exact population size then the mate population goes through crossover and mutation an essential part of the searching process to create a child population once the objective functions through swat runs are calculated for the child population the mate and child populations are combined to create the next generation parent population this process repeats for each generation until the termination criteria are met ercan and goodall 2016 explained these procedures in detail before incorporating into swatshare the original nsga ii source code ercan and goodall 2016 has been modified to facilitate new functionalities and make them compatible with swatshare s workflow within mygeohub 2 2 ci enabled autocalibration functionalities fig 1 summarizes the new autocalibration functionalities in swatshare showing how each of these functionalities is driven by and or benefited from ci in terms of the efficiency matrix introduced in table 1 platform independence access to hpc reduced need for programming literacy reduced need for model structural literacy and data model interoperability additionally fig 1 categorically explains whether and to what extent swatshare s current version leverages ci benefits and improves autocalibration efficiency compared to its predecessor rajib et al 2016 and a contemporary widely used desktop based tool called swat cup 2 2 1 web based interface with hierarchical progression swatshare is equipped with a web based interface to facilitate platform independent calibration of swat models various functions of the interface associated with model calibration are grouped under four graphical control elements or tabs discovery my models simulation and visualization the discovery tab provides an interactive gis environment to search and download existing models previously uploaded and calibrated in swatshare including the models metadata and geographical extent fig 2 a the my models tab enables users to initiate calibration by first uploading the model input files from their personal computers or seamlessly importing them from an external resource in hydroshare upon uploading a new model swatshare automatically maps the watershed on the interface and extracts key metadata e g number of subbasins and hrus in the model watershed drainage area simulation time step and duration following an extended dublin core metadata framework in hydroshare morsy et al 2017 fig 2b finally swatshare creates a dedicated web link for the model which allows modelers to directly view the model metadata and watershed map via a web browser without having to login to swatshare the simulation tab allows modelers to set up the calibration protocol including the number of iterations objective function objective variables reference datasets and a list of parameters along with their minimum maximum ranges figs 3 4 in its current design the simulation interface lets modelers undergo an intuitive hierarchical progression across different tasks and in a specific sequence a critical element of interface design carrillo et al 2006 that reduces or eliminates in some cases the need for both programming and model structural literacy swatshare s previous version had a web based simulation interface as well yet the interface only allowed uploading a pre processed zip file so that modelers had to set up the calibration offline such an interface reduced programming literacy to some extent by running the calibration online but it was implicitly platform dependent with high model structural literacy needs due to the offline setup in modelers local computers the swat cup interface on the other hand is user friendly but not particularly intuitive as the modelers have to manually intervene at critical steps such as defining specific subbasins or rivers as calibration nodes linking the corresponding reference data with the model nodes and feeding the optimized parameters back into the model among others further discussed in the following sections 2 2 2 interoperability for seamless extraction of reference data from the source swatshare offers interoperability so that it can seamlessly and instantaneously extract streamflow and water quality datasets through the united states geological survey web services usgs 2022 while modelers have the option to upload pre processed reference datasets in csv comma separated values format configurable size limit currently set at 10 gb they can activate the interoperability functionality by simply uploading a csv file with a list of usgs gage stations that they want to be included as reference nodes model constraints fig 3 such a high level of automation in data model interoperability correspondingly reduces modelers need for both modeling and programming literacy to the best of authors knowledge a similar data model interoperability functionality is not available in any other contemporary hydrologic model autocalibration tools currently designed for us watersheds limited by the availability of a usgs web service the interoperability function in swatshare can be easily extended to other regions of the world once suitable web services become available nonetheless the current implementation reveals the potential for future development to make swatshare interoperable with emerging remote sensing earth observation platforms 2 2 3 ease of linking spatially distributed reference datasets with the model swatshare lets modelers upload multiple reference datasets in csv file format with each csv file representing a specific variable i e streamflow evapotranspiration regardless of the csv file s with pre processed reference data or the csv file s with location list for seamlessly extracting reference data from the source section 2 2 2 swatshare uses the same csv file s to recognize and link relevant model subbasin and or river ids with corresponding datasets fig 3c there was no such functionality in the previous version of swatshare as it required preparing the calibration setup offline and simply provided a web interface to submit the calibration job to hpc in case of swat cup the graphical interface eases the inclusion of reference datasets but modelers not well versed with swat input output files may still find it challenging to manually link reference datasets with the corresponding subbasin and or river ids the current version of swatshare as noted above offers a parsimonious yet highly efficient way so that modelers can link large spatially distributed datasets with the model in a single step without having to deal with coding to prepare the data in a specific format or being well versed about the model s file structure and nomenclature 2 2 4 multi objective calibration swatshare can calibrate various internal dynamics and signatures across the watershed in addition to calibrating streamflow only at the watershed outlet kunnath poovakka et al 2016 nijzink et al 2018 rajib et al 2018a specifically swatshare offers the option of calibrating up to five hydrology and water quality variables simultaneously and across different spatial scales including streamflow river reach evapotranspiration subbasin sediment river reach phosphorus river reach and nitrate river reach more variables can be added in the future with minimal changes in the overall workflow however the data and computation intensive nature of multi objective calibration is a deterring factor asgari et al 2022 which drives modelers towards the traditional streamflow only calibration and produces right answers for wrong reasons rajib et al 2018a besides the interoperability with data platforms and the ease of linking data with model structure sections 2 2 2 2 2 3 the free access to hpc in swatshare reduces data and computational burden at the modelers end and encourages the use of multi objective calibration to obtain accurate understanding of hydrologic processes executing multi objective calibrations remains challenging in many contemporary autocalibration tools e g high model structural literacy requirements to enable multi objective calibration in pywrfhydrocalib and swat cup no multi objective option in pest gssha but swatshare users can activate multi objective calibration simply by clicking a few check boxes on the interface fig 3a b 2 2 5 multi criteria convergence to the optimal solution model performance criteria measure calibration performance by expressing the agreement between simulation and reference data commonly used criteria e g percent bias pbias correlation r nash sutcliffe efficiency nse kling gupta efficiency kge knoben et al 2019 krause et al 2005 legates and mccabe 1999 are not equally representative benchmarks for different hydrologic regimes knoben et al 2019 and also the choice of criteria during model calibration is often arbitrary see moriasi et al 2007 therefore the so called best parameter set the optimal solution based on a single criterion is hardly the best for all criteria simultaneously considering multiple criteria simultaneously may produce a trade off solution generally known as pareto optimal solutions or non dominated solutions these solutions are optimal in the sense that no other solutions in the parameter space are better than them or can dominate them when all the criteria are considered ercan and goodall 2016 while many existing tools e g swat cup and pywrfhydro use a single criterion to search optimality swatshare lets modelers select a combination of up to three criteria to search pareto optimality fig 3b 2 2 6 spatially distributed parameter search employing spatially distributed reference data in multi objective calibrations e g evapotranspiration from satellites streamflow and water quality from multiple gage stations across the watershed does not automatically guarantee spatially distributed parameter search for example despite being fed with spatially distributed data autocalibration tools often let parameters undergo the same degree of change across the entire watershed during the iteration process e g rajib et al 2016 because such an approach overlooks the relative locations of calibration nodes and the space time variable continuum of the data dembélé et al 2020 gardner et al 2018 jadidoleslam et al 2020 it can mislead the parameter search to an equifinal solution as a remedial solution the new swatshare interface provides an option similar to swat cup so that modelers can divide a watershed into multiple zones according to the spatial distribution proximity of the calibration nodes and also the knowledge of watershed properties and correspondingly apply different degrees of change to the same parameter across these zones fig 4a the increased computational need for such a highly discretized spatially distributed parameter search is supported by swatshare s hpc resource 2 2 7 feedback of optimal parameters into the model as noted above choice of performance criteria in traditional hydrologic model calibration practices is largely arbitrary as such developers of autocalibration tools often automate the iterative parameter search process but decouple the final step from the calibration workflow where the best parameter values are fed into the model source code to produce the optimal model this is to let modelers check which performance criteria shows the best performance select the corresponding best parameter set and do manual parameter adjustments if necessary e g mengistu et al 2019 the contemporary swat autocalibration tools e g swat cup and r swat fme invariably use this approach although such an approach gives modelers some flexibility to evaluate the physical consistency of the calibration results outside the autocalibration workflow abbaspour 2015 it is subjective and highly susceptible to equifinality not to mention the cumbersome tasks of handling model parameters manually and chances of errors therein swatshare on the other hand applies a feedback loop to automatically insert the best parameter set into the model after finishing a batch of iterations the underlying idea here is to trade off the aforesaid flexibility for other opportunities such as use of spatially distributed data multi criteria optimization and in depth visualization of model outputs elaborated in section 2 2 8 which can ultimately reduce subjectivity and equifinality in model calibration and may in fact produce a more robust calibrated model 2 2 8 instantaneous visualization and evaluation of calibration performance when a calibration job is completed swatshare sends an email notification to the modeler thus providing more flexibility in swatshare s remote work environment through an interactive interface similar to swat cup modelers can evaluate calibration performance by visualizing the time series of simulated and reference data along with the performance metrics swatshare allows users to make explainable adjustments to the prior calibration protocol and subsequently re run the calibration as necessary swatshare enables this by displaying the corresponding calibration nodes i e subbasin river in a dynamic map so that the modelers can visually interpret the variation of model performance at different spatial scales e g upstream to downstream gradient and geophysical properties across the watershed e g topography land use climate fig 4b further swatshare lets modelers instantaneously create time series and spatial maps for all the simulated hydrologic processes corresponding to the most optimal parameter set especially for the spatial maps fig 4c swatshare shows a precipitation time series so that modelers can correlate the spatial patterns of water balance components with climatic drivers and sense inconsistencies in their choice of calibration protocol such an instantaneous yet comprehensive evaluation of calibration performance through multiple layers of visualization aids is a feature unique to swatshare currently absent in the contemporary autocalibration tools additionally calibration statistics e g optimal parameter values performance metrics and the calibrated model can be downloaded from swatshare to facilitate offline experiments and or published through swatshare fig 2 for broader community consumption 2 3 the software architecture and simulation workflow swatshare architecture as shown in fig 5 consists of three main structural components front end web interface back end services and external resources although the three main structural components remain the same as in the previous swatshare version rajib et al 2016 the associated software architecture has been modified to better fit the sustainability model and the common geospatial data management and analysis infrastructure mygeohub provides to the other hosted projects kalyanam et al 2019 2 3 1 front end interface the front end of swatshare is an interactive web application deployed through mygeohub kalyanam et al 2019 2020 https mygeohub org a geospatial science gateway in the hubzero ci hubzero is a ci to create and host interactive web portals for scientific research education and outreach activities mclennan and kennell 2010 it provides out of the box data management tools for users such as group project publication ticket tracking wiki forum and an automated process to contribute contents and publish online tools the access control for swatshare is integrated with the hubzero authentication system it enables private model and simulation management for each modeler as well as resource usage monitoring and data sharing the earlier version of the swatshare s graphical user interface was implemented using adobe flash since adobe no longer supports flash player after year 2020 a new version of swatshare as a hubzero component is implemented using php and javascript which are the main programming languages for web application development on the hubzero platform specifically it is a single page application spa providing dynamic content by actively communicating with mygeohub web server eliminating the need to refresh the entire web page this enables a faster transition and thus a better user experience as if they were using a native application 2 3 2 back end services the back end of the swatshare system is built on redhat linux distribution it consists of a set of services that are responsible for handling modelers requests through the web interface in particular the apache php services provide data upload and download functions via http messages the tomcat web services are written in java and support representational state transfer rest web apis for managing models users and simulations they also perform geospatial data processing functions including model output transformation data visualization model validation and metadata extraction using open source python and r software such as gdal warmerdam et al 2022 fiona gillies 2020 dataretrieval de cicco et al 2022 and hydrogof zambrano bigiarini 2020 the metadata for users simulations and models are stored securely in a mysql database this linux apache mysql php lamp software stack is a widely used web development platform having an open source ecosystem and providing cost efficiency lawton 2005 the uncalibrated models uploaded by different modelers and their corresponding calibrated models outputs are stored in a high performance file system 2 3 3 external resources swatshare utilizes and interoperates with several external resources a geoserver is used for rendering interactive maps that allows users to search models by geographic location and metadata geoserver is a standard conforming community based tool that has proved stable and efficient in gis application developments parker et al 2015 swat model autocalibration usually require a large amount of computational resources for instance one calibration job for a swat model normally includes more than 500 iterative simulations in order to get better performance and scalability swatshare connects to the hpc resources at the extreme science and engineering discovery environment xsede comet cluster and a purdue campus cluster this enables users to get simulation results much sooner than using their desktop environments moreover they can run multiple simulations at the same time without performance degradation swatshare also connects to hydroshare tarboton et al 2018 a collaborative ci aiming at enabling the hydrologic user community to share their data models and analysis users from either system can easily access data and tools across the networks without the need to create new accounts or manually import or export models 2 3 4 simulation workflow swatshare runs calibration as well as normal simulations remotely on hpc resources via secure shell ssh the overview of the simulation workflow is described in fig 6 when the modeler submits a job the web front end collects the user input and creates a simulation specification file that describes the input simulation name simulation period and swat executable version see e g fig 3a it then invokes the swatshare job submission web service with the simulation specification additional calibration details are collected and passed to the web service such as the optimization algorithm to use the number of iterations and parameter information and reference data files see e g fig 3b c 4a when the swatshare web service receives the request it prepares a job submission module to be submitted to the hpc resources in particular it parses the specification and creates a simulation module containing the swat model input file a definition file including calibration information and the reference data file s along with the input files the module also includes a simulation execution engine consisting of several automation scripts and a swat executable next the web service sends a job request with the module to the slurm workload manager on the head node of the remote cluster via ssh the job will enter a waiting queue until a suitable compute node is available when a compute node is assigned the automation scripts in the simulation module are executed and the simulation is run iteratively the swatshare job monitoring web service keeps track of the job status when the job completes it fetches the simulation output from the file system of the computational resource and stores it to a network file system nsf mounted to the back end server finally it checks for successful run of a simulation and updates the mysql database accordingly 3 0 implementation case studies in our previous study we conducted modeling experiments to introduce swatshare s model sharing high performance computation and visualization capabilities in a real time multi user environment rajib et al 2016 in the present study we conducted experiments to demonstrate i how the new ci capabilities of swatshare calibration facilitate data model interoperability ii how swatshare allows spatially distributed parameter search as opposed to a conventional approach and iii whether the swatshare calibration results are consistent with those from a widely used desktop based swat calibration tool swat cup 3 1 data model interoperability swatshare s data model interoperability is demonstrated in fig 7 using a swat model originally developed by rajib et al 2018b for the upper wabash river watershed in central indiana united states this watershed has 7 streamflow gage stations that satisfies the data availability criteria and are considered as calibration nodes to activate data model interoperability a csv file listing the gage and the corresponding river ids is uploaded swatshare retrieves other key inputs from the prior steps including the simulation period time step and objective variable see fig 3a b and feeds this information into the dataretreival software de cicco et al 2022 to seamlessly fetch the required time series data from usgs next swatshare uses a python post processor to transform the downloaded time series in a specified format while linking them with the corresponding river ids in addition to the wabash watershed test presented here this functionality has been tested using different watersheds with varying density of gage stations simulation period and objective variables e g water quality for the upper wabash test case a modeler with moderate swat modeling gis and programming experience could save one working day about 8 h in data processing tasks when using swatshare s data model interoperability functionality as opposed to the manual data search download and processing for 7 gaging stations 3 2 spatially distributed parameter search fig 8 shows swatshare s capability to perform spatially distributed parameter search using the same model setup for the upper wabash watershed demonstrated earlier the 7 streamflow gage stations used in the model s calibration are somewhat uniformly distributed across 43 subbasins and numerous hydrologic response units hrus within the subbasins fig 8a by making a 25 change in the default curve number cn2 or replacing the default channel roughness ch n2 with a new value between 0 01 and 0 15 irrespective of subbasins and river reaches fig 8b these parameters are allowed to undergo the same degree of change across the entire watershed without considering the relative locations of calibration nodes i e gage station locations on one hand this spatially uniform optimization limits the ability of a parameter to represent watershed features e g land use and topography on the other hand such an approach makes a distributed model function like a lumped model thus underutilizing the data labor and computational cost of highly resolved process based simulations yang et al 2019 conceptualized this as the calibration density and consistency problem while xie et al 2021 linked this to a dimensionality problem creating a model with high spatial resolutions fenicia et al 2016 kuppel et al 2018 marcé et al 2008 or using spatially distributed reference data in model calibrations rajib et al 2018a alone does not address this problem as a remedial solution the new calibration interface in swatshare uses an approach suggested by abbaspour 2015 to explicitly relate reference data with the corresponding model subbasins and or hrus this functionality is demonstrated in fig 8c that shows how the same parameter e g curver number cn2 can undergo different degrees of optimization across three different watershed zones briefly the sub basins are grouped in three zones according to their proximity to gage stations in an upstream to downstream gradient the parameter optimization is further distributed at hru level in one of the subbasins according to different land use types see fig 8a and c the resulting calibrated model with spatially distributed parameter search option produced notable changes in model outputs for example subbasin level surface runoff after a particular storm event is 30 to 60 different in the spatially distributed option compared to that in the spatially uniform option although this demonstration is based on point observations at a limited number of gage stations the spatially distributed parameter search option shown here makes swatshare a user friendly futuristic tool for calibrating large scale high resolution hydrologic models with increasingly available gridded earth observation datasets 3 3 consistency across swatshare and swat cup calibration results in this experiment separate swatshare and swat cup calibrations of 30 swat models across four different climate zones in the united states see fig 9 a are conducted this is the first calibration experiment at such an extensive scale because prior studies often considered a single watershed to evaluate alternative calibration tools e g paul and negahban azar 2018 yang et al 2008 this extensive calibration experiment facilitates a conclusive understanding of whether and to what extent swatshare and swat cup results are consistent the metadata corresponding to each of these test models can be accessed through the discovery function of swatshare fig 2 using the supplementary information in s1 all 30 models are simulated at a daily time step with a 2 year period of initialization 2001 2002 followed by a 5 year calibration 2003 2007 and a 3 year validation 2008 2010 using gage station streamflow data as reference identical set of calibration parameters n 18 and their respective initial ranges are selected across all 60 setups see supplementary information s1 the models are calibrated with the nsga ii algorithm in swatshare and the sequential uncertainty fitting versions 2 sufi 2 algorithm in swat cup sufi 2 in swat cup is used because of its well documented applications in the literature models in swatshare are set with 20 generations and 100 iterations whereas models in swat cup are set with 1000 iterations each across two successive batches of iterations this is a measure to ensure an equivalent number of total simulations although such a distribution of generation iteration and batch numbers may not translate the same meaning across two conceptually different calibration tools the results indicate that swatshare and swat cup yield nearly consistent calibration performance in terms of streamflow kling gupta efficiency kge knoben et al 2019 values averaged separately for calibration and validation and across all the watersheds located within a climate zone fig 9b these averaged estimates of calibration performance are also supported by individual watershed kge values fig 9c note the adherence of kge values around the swatshare swat cup 1 1 line despite such consistency in overall calibration performance optimal parameter values in swatshare may be inconsistent with those in swat cup as a result the most optimal parameter values in swatshare with a few exceptions showed low correlation with the corresponding parameter values in swat cup fig 10 a importantly a behavioral change is observed revealing how some of these parameters represented hydrologic processes in the two calibration tools for example the optimal value for parameter esco in swatshare is greater than epco across all four climate zones a pattern completely opposite compared to swat cup fig 10b because esco and epco are the key parameters controlling swat s soil moisture accounting and evapotranspiration mechanisms neitsch et al 2011 rajib et al 2016 their opposite behavior shown in fig 10b indicates two potentially different states of water balance in the same model regardless of similar calibration performances although it is hard to disentangle the underlying factors for such behavioral change in parameters responses these findings add new insights into how the choice of calibration tool can cause equifinality 4 conclusions and future directions data for streamflow climate hydrography topography land use and soil over the internet date back to early 2000s naturally development and use of cyberinfrastructure ci for hydrology or water resources in general had long been focused on instantaneous access to data standardizations of data publication and integration and platforms for data storage and sharing therefore cis and open science platforms geared towards hydrologic modeling needs had been limited swatshare 1 0 https mygeohub org groups water hub swatshare partially filled this gap by providing a collaborative platform for sharing simulation and visualization of swat models rajib et al 2016 this paper introduces swatshare 2 0 a substantially improved version incorporating a new autocalibration tool and demonstrating how ci capabilities can be harnessed to solve five root causes of inefficiency in traditional hydrologic model calibration practices including i platform dependence ii limited computing resource iii lack of programming literacy iv lack of model structural literacy and v no data model interoperability the online interface of swatshare ensures platform independence by letting modelers perform complex calibration tasks in a web browser without requiring any specific computer operating system software packages and their versions and licenses the free access to high performance computing hpc resource allows swatshare to lower the computational burden and thereby offer multi objective calibration options involving up to five hydrology and water quality variables and a highly discretized parameter search option involving spatially distributed datasets swatshare s data model interoperability by seamlessly and instantaneously extracting streamflow and water quality datasets through the usgs web services and automatically recognizing and linking those datasets with corresponding model subbasin and or river ids show a unique example of ci capabilities currently absent in any other hydrologic model autocalibration tools searching the optimal solution using a combination of up to three performance criteria direct feedback of optimal parameters into the model and instantaneous visualization and evaluation of calibration performance are the additional functionalities that further increase the efficiency of the autocalibration process in swatshare without the need for excessive pre and post processing tasks swatshare s online interface serves as the one stop platform to let modelers perform all the above calibration steps through an intuitive hierarchical progression thus minimizing the need for a modeler to have high programming and model structural literacy finally and more importantly results from swatshare autocalibration show that its streamflow prediction is comparable with that from the commonly used offline calibration platform swatcup but for some of the study areas parameters estimated from swatshare may be more meaningful to the physical processes while swatshare demonstrates an example application using the swat model except the model most of the workflow is generic and can be adopted for any model for example the swat model itself is evolving and is currently being modified as swat swat can be brought under swatshare via simple code modifications to incorporate the model s new file naming conventions and data structures without having to change swatshare s overall workflow plan to do this is already underway no doubt the above developments of swatshare addressed a critical need of the hydrologic modeling community yet future developments and re developments of swatshare harnessing newer and better ci and open science capabilities to address broader community needs are imminent there is a push and also broader consensus within the scientific community including hydrology for reproducibility reproducibility cannot be accomplished without the ability to run complete scientific workflows which may and in most cases they do demand a more extensive architecture for interoperability and computing for example currently swatshare cannot create a swat model directly from input datasets it relies on modelers to upload their existing models meaning modelers cannot run complete scientific workflows from model creation to model calibration within swatshare interface fortunately besides access to xsede distributed hpc https www xsede org swatshare s current capabilities to interoperate across multiple cis including mygeohub https mygeohub org hydroshare https www hydroshare org and usgs national water information system https waterdata usgs gov nwis have already created the building blocks to achieve that reproducibility goal efforts to link models and enable model interoperability is also booming the generalizable software pieces developed for swatshare can further excel these interoperability efforts by brining other models and myriad open access autocalibration codes under one platform in essence with calls for adopting fair or open science principles and increasing need for convergent approaches to address societal problems involving water and climate platforms like swatshare can serve as a blueprint for new ci enabled developments in hydrology and beyond hydrology in other disciplinary domains of earth system sciences declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was funded by the u s national science foundation nsf grants 1835822 and 1829764 a r s contribution was partially funded by the national aeronautics and space administration nasa grant 80nssc22k1661 the views expressed in this article are those of the authors and do not necessarily represent the views or the policies of the funding agencies appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105561 
25503,automatic calibration autocalibration of models is a standard practice in hydrologic sciences however hydrologic modelers while performing autocalibrations spend considerable amount of time in data pre processing coding and running simulations rather than focusing on science questions such inefficiency as this paper outlines stems from i platform dependence ii limited computational resource iii limited programming literacy iv limited model structure and source code literacy and v lack of data model interoperability in the so called autocalibration process by expanding and enhancing an existing web based modeling platform swatshare developed for the soil and water assessment tool swat hydrologic model this paper demonstrates a generalizable pathway to making autocalibration efficient via cyberinfrastructure ci solutions swatshare is a collaborative platform for sharing and visualization of swat models model results and metadata online this paper describes the front and back end architectures of swatshare for enabling efficient swat model autocalibration on the web in addition this paper also demonstrates three implementation case studies to validate the autocalibration workflow and results results from these implementations show that swatshare autocalibration can produce streamflow hydrograph and parameters that are comparable with commonly used offline swatcup calibration outputs in some instances the parameter values from swatshare calibration are more physically relevant than those from swatcup although the discussion in this paper is in the context of swat and swatshare the conceptual and technical design presented here can be used as an open science blueprint for similar ci enabled developments in other hydrologic models and more importantly in other domains of earth system sciences keywords calibration high performance computing hydrology swat model swatshare data availability see the software availability section in the manuscript software availability name swatshare 2 0 type web based platform year of first version release 2016 year of second version release 2022 availability openly available at https mygeohub org groups water hub swatshare cyberinfrastructure used for deployment mygeohub https mygeohub org via hubzero platform purdue university software web services programming languages used adobe flash php javascript java rest api redhat linux mysql lamp linux apache mysql php software stack tomcat web services python gdal fiona r dataretrieval hydrogof network file system nsf non sorting genetic algorithm ii high performance computing resources used a purdue campus cluster and the extreme science and engineering discovery environment xsede comet cluster developers i luk kim adnan rajib lan zhao venkatesh merwade carol song 1 0 introduction the predictability of a hydrologic model largely depends on how well its parameters are calibrated because numerous manual iterations to search representative parameter values can be time consuming dawdy and o donnell 1965 hydrologic modelers use an automated iterative process using complex algorithms hereafter autocalibration duan et al 2006 gupta et al 1999 samadi et al 2020 wu et al 2021 over time this autocalibration has become a standard practice in hydrologic model applications leading to the widespread development of open source tools that can link calibration algorithms with respective model source codes e g abbaspour 2015 wi et al 2017 wu and liu 2012 these autocalibration tools when provided with adequate reference datasets produce hydrologic simulations with reasonably high accuracy yet it will not be an overstatement to refer the term autocalibration as a misnomer because many data and model integration tasks required to set up and run the contemporary autocalibration tools heavily rely on modelers manual interventions as a result despite the notable recent improvements in calibration algorithms and open source tools e g baracchini et al 2020 chlumsky et al 2021 femeena et al 2020 sadler et al 2019 wang and brubaker 2015 zhang et al 2013 hydrologic modelers spend considerable time in data pre processing coding and running simulations rather than focusing on science questions therefore having a guided step by step workflow and or an intuitive graphical user interface do not necessarily lead to an efficient autocalibration procedure addressing the root cause s of inefficiency in hydrologic model autocalibration still remains a challenge below we identify five root causes of inefficiency in autocalibration tools and then we compare some of the contemporary hydrologic models according to the efficiency of their respective autocalibration tools table 1 1 platform dependence most of the autocalibration tools recognized across the hydrologic modeling community are developed as offline desktop based black box software e g swat cup for the soil and water assessment tool swat vic assist for the variable infiltration capacity vic model hspexp hspexp for the hydrologic simulation program fortran hspf abbaspour 2015 epa 2015 wi et al 2017 these tools rely on a particular computer operating system i e windows or linux dependencies e g specific program libraries packages and their versions and licensing requirements correspondingly a major issue associated with such platform dependence is maintenance and updating of functions yu et al 2019 over time some autocalibration tools e g hspexp epa 2015 have become obsolete due to incompatibility with current computer operating systems 2 limited computing resource commonly used desktop based autocalibration tools often conduct one calibration job at a time on a single desktop computer the computing resource in a desktop computer may be adequate to run small scale calibrations involving small watershed areas and short simulation periods however such small scale calibrations become computationally intensive when the models undergo multi objective calibrations with spatially distributed data and involves a large parameter set e g kunnath poovakka et al 2016 rajib et al 2018a obviously calibrations involving large watersheds and long simulation periods e g du et al 2018 high spatial resolution e g lin et al 2018 and complex spatial discretization schemes e g evenson et al 2018 demand more computing resources than an average desktop computer alone can provide in any case modelers commonly run multiple instances of the same calibration setup as trial runs to gather priori knowledge of sensitive parameters suitable parameter values and characteristic model bias kuzmin et al 2008 which make the overall computational footprint and run time unmanageable against these needs efficiency of commonly used desktop based autocalibration tools remains limited by the processing power and available storage space in modelers personal computers some of the recent autocalibration tools allow model calibration to be remotely executed in high performance computing hpc clusters e g lcc swat and gswat bashyt for the swat model bacu et al 2011 cau et al 2013 zamani et al 2021 the model independent cloud based parameter estimation pest tool fienen et al 2011 there are also efforts to speed up calibration tools using graphics processing units gpu aided parallel computing ercan et al 2014 freitas et al 2022 kan et al 2019 yet it is widely acknowledged that running autocalibration tools using hpc and or parallel computing also requires a client server interface and standard web services for job management resource monitoring messaging user verification data transfer encrypting and various notification mechanisms hokkanen et al 2021 zhu et al 2016 which may be challenging to sustain unless the tool is part of a large holistic modeling platform 3 programming literacy some of the autocalibration tools do not have a graphical user interface in such cases running the tool requires substantial literacy on programming languages syntax and execution for example use of pywrfhydrocalib for the wrf hydro model lin et al 2018 ncar 2019 r swat fme for the swat model e g wu and liu 2012 2014 and ostrich for the storm water management model swmm behrouz et al 2020 macro et al 2019 require medium to high level skills in python and r codes similarly without knowing how to write in command line interface modelers cannot run the pest tool for the gridded surface subsurface hydrologic analysis gssha model skahill et al 2009 2012 in short it is challenging to run some of the existing autocalibration tools without programming skills which limits their wider acceptability within the hydrologic modeling community 4 model structural literacy besides the knowledge of hydrologic processes and steps to setup and calibrate a model it is necessary for a modeler to clearly understand the model s internal structure including the model s geodatabase source code and parameter definitions because the existing autocalibration tools are not fully coupled with the model structure modelers running these tools with limited model structural literacy need to follow a steep learning curve for example while using pywrfhydrocalib and hspexp hspexp modelers need to manually decide on the spatial scale at which the tool will iterate a parameter at individual subbasin scale or entire basin scale or the nodes the tool will use as calibration sites specific subbasin or river ids epa 2015 ncar 2019 also see the scaling problem discussed by nijzink et al 2018 tsai et al 2021 xu et al 2014 zhang et al 2017 some tools do not offer a function to automatically fit the most optimal parameter combinations back into the model source code after completing a batch of iterations e g swat cup and lcc swat ozdemir and leloglu 2019 zamani et al 2021 nonetheless the reduced need for model structural literacy is the most desired service that an autocalibration tool can offer to educators who often want to avoid the steep learning curve the intuitive graphical user interface in tools like swat cup and vic assist is helpful yet inadequately detailed to substantially reduce the need for model structural literacy 5 data model interoperability use of multiple sources types of reference datasets in model calibration requires a meaningful linking of the data with the model structure which should handle the space time variable continuum without misleading the parameter search procedure dembélé et al 2020 gardner et al 2018 jadidoleslam et al 2020 while there are advances in developing web based platforms to perform spatio temporal query across a region and bulk download available in situ and remotely sensed earth observation datasets ames et al 2012 csiss 2021 gee 2022 none of the existing autocalibration tools can automatically do the data post processing and model linking tasks therefore for a large scale model calibration involving many in situ measurements e g abbaspour 2015 du et al 2018 lin et al 2018 rajib et al 2020a b or for a small watershed scale model calibration involving spatially distributed remotely sensed estimates rajib et al 2018a it may be easy to download required datasets through web based platforms but it remains an excruciatingly labor intensive task for a modeler to process the data let alone link each of those datasets explicitly with the corresponding calibration sites in the model e g river ids unfortunately all the existing calibration tools are designed with an assumption that downloading pre processing and making the reference datasets ready for the calibration tool are auxiliary tasks conducted outside the tool s graphical coding interface scientific cyberinfrastructures cis can address all five root causes of inefficiency in hydrologic model autocalibration through a findable accessible interoperable and reusable fair open science platform bandaragoda et al 2019 chen et al 2020 govindaraju et al 2009 kalyanam et al 2019 2020 maidment 2008 voinov and costanza 1999 wilkinson et al 2016 the fairness or openness of cis is due to their building blocks which generally include hpc resources containerized models code wrappers automated workflows geospatial data analysis gis interface and other tools needed for their interoperability and reproducibility all through an open web based environment e g essawy et al 2020 le et al 2015 wu et al 2013 zhang et al 2019 considering these benefits many contemporary developments in earth system sciences reflect a trend of ci solutions for example recently developed web based gis environments allow running a hydrologic model using hpc resources without having to possess in depth programming and model development skills e g liu et al 2014 lyu et al 2019 similarly inclusion and removal of modules and codes in a simple drag and drop plug and play fashion and thereby enabling the total platform independence of a modeling workflow is becoming increasingly feasible dunlap et al 2013 lodhi et al 2020 peckham and goodall 2013 zeng et al 2020 what has further widened the scope of hydrologic modeling in a ci platform is the ability to link multiple cis that allows greater utilization of available data repositories computational environments and model application programming interfaces apis castronova et al 2013 essawy et al 2018 choi et al 2021 correspondingly modelers are now developing interoperability engines zhang et al 2021 to resolve the heterogeneity in model and data types e g chen et al 2020 gregersen et al 2007 hutton et al 2020 peckham et al 2013 in line with the above efforts swatshare rajib et al 2016 referred here as swatshare 1 0 https mygeohub org groups water hub swatshare was developed as a unique swat modeling platform leveraging the ci capabilities of mygeohub kalyanam et al 2019 2020 https mygeohub org besides the collaborative platform for model and metadata sharing and interoperability with other collaborative cis like hydroshare morsy et al 2017 tarboton et al 2018 swatshare 1 0 offered basic autocalibration functionalities unable to address all five root causes of calibration inefficiency identified earlier see table 1 thus swatshare 1 0 has been substantially modified in swatshare 2 0 by incorporating a new highly efficient autocalibration tool that addresses all root causes of calibration inefficiency by harnessing ci capabilities the overall goal of this paper is to introduce the new swat model autocalibration tool in swatshare 2 0 the specific objectives of this paper are to i describe the ci enabled functionalities in swatshare that make the new autocalibration tool efficient compared to the previous version rajib et al 2016 as well as a widely used desktop based tool called swat cup ii describe the software architecture and simulation workflow of the new tool and iii present three implementation case studies to validate the tool s design concepts as well as physical consistency in terms of hydrologic processes and parameters although the discussion presented in this paper is in the context of swat and swatshare the conceptual and technical design can be used as a blueprint to reproduce similar functionalities in other hydrologic model autocalibration tools 2 0 swat model autocalibration in swatshare 2 1 the genetic algorithm the non sorting genetic algorithm ii nsga ii deb et al 2002 is incorporated into swatshare 2 0 for swat parameter optimization replacing the shuffled complex evolution algorithm sce ua duan et al 1992 van griensven and bauwens 2003 van liew et al 2005 previously used in swatshare 1 0 rajib et al 2016 nsga ii has proven to be effective for multi objective hydrologic model calibrations and complex watershed management decisions e g alam et al 2018 bekele and nicklow 2007 dai et al 2017 ercan et al 2020 monteil et al 2020 besides genetic algorithms ability to mimic natural selection in the physical world using the principles of genetics and thereby produce physically meaningful solutions to parameter optimization problems haupt and haupt 2003 gregory 2009 there are three key factors that make nsga ii a better fit than sce ua for hydrologic modeling cis these factors include i ease of use in a web interface due to minimal user inputs ii rapid convergence to optimal solution and iii adaptability with a parallel computing environment jeon et al 2014 tang et al 2006 zhang et al 2012 2013 a preliminary version of swatshare s nsga ii algorithm written in python programming language has been evaluated by ercan and goodall 2016 a brief description of this algorithm is provided below the nsga ii in swatshare uses latin hypercube sampling lhs method to create the initial parent population ercan and goodall 2016 thus reducing the number of generations significantly to reach the pareto front much quicker than starting with a random parent population ercan and goodall 2016 bekele and nicklow 2007 initial parent population size must be at least twice the population size once the initial parent population objective functions are evaluated within the swat model the non dominated sorting method ranks solutions in groups the best performing groups are selected to create the mate population which has a predetermined constant size population size the crowding distance method is used to select members within the same ranking to reach the exact population size then the mate population goes through crossover and mutation an essential part of the searching process to create a child population once the objective functions through swat runs are calculated for the child population the mate and child populations are combined to create the next generation parent population this process repeats for each generation until the termination criteria are met ercan and goodall 2016 explained these procedures in detail before incorporating into swatshare the original nsga ii source code ercan and goodall 2016 has been modified to facilitate new functionalities and make them compatible with swatshare s workflow within mygeohub 2 2 ci enabled autocalibration functionalities fig 1 summarizes the new autocalibration functionalities in swatshare showing how each of these functionalities is driven by and or benefited from ci in terms of the efficiency matrix introduced in table 1 platform independence access to hpc reduced need for programming literacy reduced need for model structural literacy and data model interoperability additionally fig 1 categorically explains whether and to what extent swatshare s current version leverages ci benefits and improves autocalibration efficiency compared to its predecessor rajib et al 2016 and a contemporary widely used desktop based tool called swat cup 2 2 1 web based interface with hierarchical progression swatshare is equipped with a web based interface to facilitate platform independent calibration of swat models various functions of the interface associated with model calibration are grouped under four graphical control elements or tabs discovery my models simulation and visualization the discovery tab provides an interactive gis environment to search and download existing models previously uploaded and calibrated in swatshare including the models metadata and geographical extent fig 2 a the my models tab enables users to initiate calibration by first uploading the model input files from their personal computers or seamlessly importing them from an external resource in hydroshare upon uploading a new model swatshare automatically maps the watershed on the interface and extracts key metadata e g number of subbasins and hrus in the model watershed drainage area simulation time step and duration following an extended dublin core metadata framework in hydroshare morsy et al 2017 fig 2b finally swatshare creates a dedicated web link for the model which allows modelers to directly view the model metadata and watershed map via a web browser without having to login to swatshare the simulation tab allows modelers to set up the calibration protocol including the number of iterations objective function objective variables reference datasets and a list of parameters along with their minimum maximum ranges figs 3 4 in its current design the simulation interface lets modelers undergo an intuitive hierarchical progression across different tasks and in a specific sequence a critical element of interface design carrillo et al 2006 that reduces or eliminates in some cases the need for both programming and model structural literacy swatshare s previous version had a web based simulation interface as well yet the interface only allowed uploading a pre processed zip file so that modelers had to set up the calibration offline such an interface reduced programming literacy to some extent by running the calibration online but it was implicitly platform dependent with high model structural literacy needs due to the offline setup in modelers local computers the swat cup interface on the other hand is user friendly but not particularly intuitive as the modelers have to manually intervene at critical steps such as defining specific subbasins or rivers as calibration nodes linking the corresponding reference data with the model nodes and feeding the optimized parameters back into the model among others further discussed in the following sections 2 2 2 interoperability for seamless extraction of reference data from the source swatshare offers interoperability so that it can seamlessly and instantaneously extract streamflow and water quality datasets through the united states geological survey web services usgs 2022 while modelers have the option to upload pre processed reference datasets in csv comma separated values format configurable size limit currently set at 10 gb they can activate the interoperability functionality by simply uploading a csv file with a list of usgs gage stations that they want to be included as reference nodes model constraints fig 3 such a high level of automation in data model interoperability correspondingly reduces modelers need for both modeling and programming literacy to the best of authors knowledge a similar data model interoperability functionality is not available in any other contemporary hydrologic model autocalibration tools currently designed for us watersheds limited by the availability of a usgs web service the interoperability function in swatshare can be easily extended to other regions of the world once suitable web services become available nonetheless the current implementation reveals the potential for future development to make swatshare interoperable with emerging remote sensing earth observation platforms 2 2 3 ease of linking spatially distributed reference datasets with the model swatshare lets modelers upload multiple reference datasets in csv file format with each csv file representing a specific variable i e streamflow evapotranspiration regardless of the csv file s with pre processed reference data or the csv file s with location list for seamlessly extracting reference data from the source section 2 2 2 swatshare uses the same csv file s to recognize and link relevant model subbasin and or river ids with corresponding datasets fig 3c there was no such functionality in the previous version of swatshare as it required preparing the calibration setup offline and simply provided a web interface to submit the calibration job to hpc in case of swat cup the graphical interface eases the inclusion of reference datasets but modelers not well versed with swat input output files may still find it challenging to manually link reference datasets with the corresponding subbasin and or river ids the current version of swatshare as noted above offers a parsimonious yet highly efficient way so that modelers can link large spatially distributed datasets with the model in a single step without having to deal with coding to prepare the data in a specific format or being well versed about the model s file structure and nomenclature 2 2 4 multi objective calibration swatshare can calibrate various internal dynamics and signatures across the watershed in addition to calibrating streamflow only at the watershed outlet kunnath poovakka et al 2016 nijzink et al 2018 rajib et al 2018a specifically swatshare offers the option of calibrating up to five hydrology and water quality variables simultaneously and across different spatial scales including streamflow river reach evapotranspiration subbasin sediment river reach phosphorus river reach and nitrate river reach more variables can be added in the future with minimal changes in the overall workflow however the data and computation intensive nature of multi objective calibration is a deterring factor asgari et al 2022 which drives modelers towards the traditional streamflow only calibration and produces right answers for wrong reasons rajib et al 2018a besides the interoperability with data platforms and the ease of linking data with model structure sections 2 2 2 2 2 3 the free access to hpc in swatshare reduces data and computational burden at the modelers end and encourages the use of multi objective calibration to obtain accurate understanding of hydrologic processes executing multi objective calibrations remains challenging in many contemporary autocalibration tools e g high model structural literacy requirements to enable multi objective calibration in pywrfhydrocalib and swat cup no multi objective option in pest gssha but swatshare users can activate multi objective calibration simply by clicking a few check boxes on the interface fig 3a b 2 2 5 multi criteria convergence to the optimal solution model performance criteria measure calibration performance by expressing the agreement between simulation and reference data commonly used criteria e g percent bias pbias correlation r nash sutcliffe efficiency nse kling gupta efficiency kge knoben et al 2019 krause et al 2005 legates and mccabe 1999 are not equally representative benchmarks for different hydrologic regimes knoben et al 2019 and also the choice of criteria during model calibration is often arbitrary see moriasi et al 2007 therefore the so called best parameter set the optimal solution based on a single criterion is hardly the best for all criteria simultaneously considering multiple criteria simultaneously may produce a trade off solution generally known as pareto optimal solutions or non dominated solutions these solutions are optimal in the sense that no other solutions in the parameter space are better than them or can dominate them when all the criteria are considered ercan and goodall 2016 while many existing tools e g swat cup and pywrfhydro use a single criterion to search optimality swatshare lets modelers select a combination of up to three criteria to search pareto optimality fig 3b 2 2 6 spatially distributed parameter search employing spatially distributed reference data in multi objective calibrations e g evapotranspiration from satellites streamflow and water quality from multiple gage stations across the watershed does not automatically guarantee spatially distributed parameter search for example despite being fed with spatially distributed data autocalibration tools often let parameters undergo the same degree of change across the entire watershed during the iteration process e g rajib et al 2016 because such an approach overlooks the relative locations of calibration nodes and the space time variable continuum of the data dembélé et al 2020 gardner et al 2018 jadidoleslam et al 2020 it can mislead the parameter search to an equifinal solution as a remedial solution the new swatshare interface provides an option similar to swat cup so that modelers can divide a watershed into multiple zones according to the spatial distribution proximity of the calibration nodes and also the knowledge of watershed properties and correspondingly apply different degrees of change to the same parameter across these zones fig 4a the increased computational need for such a highly discretized spatially distributed parameter search is supported by swatshare s hpc resource 2 2 7 feedback of optimal parameters into the model as noted above choice of performance criteria in traditional hydrologic model calibration practices is largely arbitrary as such developers of autocalibration tools often automate the iterative parameter search process but decouple the final step from the calibration workflow where the best parameter values are fed into the model source code to produce the optimal model this is to let modelers check which performance criteria shows the best performance select the corresponding best parameter set and do manual parameter adjustments if necessary e g mengistu et al 2019 the contemporary swat autocalibration tools e g swat cup and r swat fme invariably use this approach although such an approach gives modelers some flexibility to evaluate the physical consistency of the calibration results outside the autocalibration workflow abbaspour 2015 it is subjective and highly susceptible to equifinality not to mention the cumbersome tasks of handling model parameters manually and chances of errors therein swatshare on the other hand applies a feedback loop to automatically insert the best parameter set into the model after finishing a batch of iterations the underlying idea here is to trade off the aforesaid flexibility for other opportunities such as use of spatially distributed data multi criteria optimization and in depth visualization of model outputs elaborated in section 2 2 8 which can ultimately reduce subjectivity and equifinality in model calibration and may in fact produce a more robust calibrated model 2 2 8 instantaneous visualization and evaluation of calibration performance when a calibration job is completed swatshare sends an email notification to the modeler thus providing more flexibility in swatshare s remote work environment through an interactive interface similar to swat cup modelers can evaluate calibration performance by visualizing the time series of simulated and reference data along with the performance metrics swatshare allows users to make explainable adjustments to the prior calibration protocol and subsequently re run the calibration as necessary swatshare enables this by displaying the corresponding calibration nodes i e subbasin river in a dynamic map so that the modelers can visually interpret the variation of model performance at different spatial scales e g upstream to downstream gradient and geophysical properties across the watershed e g topography land use climate fig 4b further swatshare lets modelers instantaneously create time series and spatial maps for all the simulated hydrologic processes corresponding to the most optimal parameter set especially for the spatial maps fig 4c swatshare shows a precipitation time series so that modelers can correlate the spatial patterns of water balance components with climatic drivers and sense inconsistencies in their choice of calibration protocol such an instantaneous yet comprehensive evaluation of calibration performance through multiple layers of visualization aids is a feature unique to swatshare currently absent in the contemporary autocalibration tools additionally calibration statistics e g optimal parameter values performance metrics and the calibrated model can be downloaded from swatshare to facilitate offline experiments and or published through swatshare fig 2 for broader community consumption 2 3 the software architecture and simulation workflow swatshare architecture as shown in fig 5 consists of three main structural components front end web interface back end services and external resources although the three main structural components remain the same as in the previous swatshare version rajib et al 2016 the associated software architecture has been modified to better fit the sustainability model and the common geospatial data management and analysis infrastructure mygeohub provides to the other hosted projects kalyanam et al 2019 2 3 1 front end interface the front end of swatshare is an interactive web application deployed through mygeohub kalyanam et al 2019 2020 https mygeohub org a geospatial science gateway in the hubzero ci hubzero is a ci to create and host interactive web portals for scientific research education and outreach activities mclennan and kennell 2010 it provides out of the box data management tools for users such as group project publication ticket tracking wiki forum and an automated process to contribute contents and publish online tools the access control for swatshare is integrated with the hubzero authentication system it enables private model and simulation management for each modeler as well as resource usage monitoring and data sharing the earlier version of the swatshare s graphical user interface was implemented using adobe flash since adobe no longer supports flash player after year 2020 a new version of swatshare as a hubzero component is implemented using php and javascript which are the main programming languages for web application development on the hubzero platform specifically it is a single page application spa providing dynamic content by actively communicating with mygeohub web server eliminating the need to refresh the entire web page this enables a faster transition and thus a better user experience as if they were using a native application 2 3 2 back end services the back end of the swatshare system is built on redhat linux distribution it consists of a set of services that are responsible for handling modelers requests through the web interface in particular the apache php services provide data upload and download functions via http messages the tomcat web services are written in java and support representational state transfer rest web apis for managing models users and simulations they also perform geospatial data processing functions including model output transformation data visualization model validation and metadata extraction using open source python and r software such as gdal warmerdam et al 2022 fiona gillies 2020 dataretrieval de cicco et al 2022 and hydrogof zambrano bigiarini 2020 the metadata for users simulations and models are stored securely in a mysql database this linux apache mysql php lamp software stack is a widely used web development platform having an open source ecosystem and providing cost efficiency lawton 2005 the uncalibrated models uploaded by different modelers and their corresponding calibrated models outputs are stored in a high performance file system 2 3 3 external resources swatshare utilizes and interoperates with several external resources a geoserver is used for rendering interactive maps that allows users to search models by geographic location and metadata geoserver is a standard conforming community based tool that has proved stable and efficient in gis application developments parker et al 2015 swat model autocalibration usually require a large amount of computational resources for instance one calibration job for a swat model normally includes more than 500 iterative simulations in order to get better performance and scalability swatshare connects to the hpc resources at the extreme science and engineering discovery environment xsede comet cluster and a purdue campus cluster this enables users to get simulation results much sooner than using their desktop environments moreover they can run multiple simulations at the same time without performance degradation swatshare also connects to hydroshare tarboton et al 2018 a collaborative ci aiming at enabling the hydrologic user community to share their data models and analysis users from either system can easily access data and tools across the networks without the need to create new accounts or manually import or export models 2 3 4 simulation workflow swatshare runs calibration as well as normal simulations remotely on hpc resources via secure shell ssh the overview of the simulation workflow is described in fig 6 when the modeler submits a job the web front end collects the user input and creates a simulation specification file that describes the input simulation name simulation period and swat executable version see e g fig 3a it then invokes the swatshare job submission web service with the simulation specification additional calibration details are collected and passed to the web service such as the optimization algorithm to use the number of iterations and parameter information and reference data files see e g fig 3b c 4a when the swatshare web service receives the request it prepares a job submission module to be submitted to the hpc resources in particular it parses the specification and creates a simulation module containing the swat model input file a definition file including calibration information and the reference data file s along with the input files the module also includes a simulation execution engine consisting of several automation scripts and a swat executable next the web service sends a job request with the module to the slurm workload manager on the head node of the remote cluster via ssh the job will enter a waiting queue until a suitable compute node is available when a compute node is assigned the automation scripts in the simulation module are executed and the simulation is run iteratively the swatshare job monitoring web service keeps track of the job status when the job completes it fetches the simulation output from the file system of the computational resource and stores it to a network file system nsf mounted to the back end server finally it checks for successful run of a simulation and updates the mysql database accordingly 3 0 implementation case studies in our previous study we conducted modeling experiments to introduce swatshare s model sharing high performance computation and visualization capabilities in a real time multi user environment rajib et al 2016 in the present study we conducted experiments to demonstrate i how the new ci capabilities of swatshare calibration facilitate data model interoperability ii how swatshare allows spatially distributed parameter search as opposed to a conventional approach and iii whether the swatshare calibration results are consistent with those from a widely used desktop based swat calibration tool swat cup 3 1 data model interoperability swatshare s data model interoperability is demonstrated in fig 7 using a swat model originally developed by rajib et al 2018b for the upper wabash river watershed in central indiana united states this watershed has 7 streamflow gage stations that satisfies the data availability criteria and are considered as calibration nodes to activate data model interoperability a csv file listing the gage and the corresponding river ids is uploaded swatshare retrieves other key inputs from the prior steps including the simulation period time step and objective variable see fig 3a b and feeds this information into the dataretreival software de cicco et al 2022 to seamlessly fetch the required time series data from usgs next swatshare uses a python post processor to transform the downloaded time series in a specified format while linking them with the corresponding river ids in addition to the wabash watershed test presented here this functionality has been tested using different watersheds with varying density of gage stations simulation period and objective variables e g water quality for the upper wabash test case a modeler with moderate swat modeling gis and programming experience could save one working day about 8 h in data processing tasks when using swatshare s data model interoperability functionality as opposed to the manual data search download and processing for 7 gaging stations 3 2 spatially distributed parameter search fig 8 shows swatshare s capability to perform spatially distributed parameter search using the same model setup for the upper wabash watershed demonstrated earlier the 7 streamflow gage stations used in the model s calibration are somewhat uniformly distributed across 43 subbasins and numerous hydrologic response units hrus within the subbasins fig 8a by making a 25 change in the default curve number cn2 or replacing the default channel roughness ch n2 with a new value between 0 01 and 0 15 irrespective of subbasins and river reaches fig 8b these parameters are allowed to undergo the same degree of change across the entire watershed without considering the relative locations of calibration nodes i e gage station locations on one hand this spatially uniform optimization limits the ability of a parameter to represent watershed features e g land use and topography on the other hand such an approach makes a distributed model function like a lumped model thus underutilizing the data labor and computational cost of highly resolved process based simulations yang et al 2019 conceptualized this as the calibration density and consistency problem while xie et al 2021 linked this to a dimensionality problem creating a model with high spatial resolutions fenicia et al 2016 kuppel et al 2018 marcé et al 2008 or using spatially distributed reference data in model calibrations rajib et al 2018a alone does not address this problem as a remedial solution the new calibration interface in swatshare uses an approach suggested by abbaspour 2015 to explicitly relate reference data with the corresponding model subbasins and or hrus this functionality is demonstrated in fig 8c that shows how the same parameter e g curver number cn2 can undergo different degrees of optimization across three different watershed zones briefly the sub basins are grouped in three zones according to their proximity to gage stations in an upstream to downstream gradient the parameter optimization is further distributed at hru level in one of the subbasins according to different land use types see fig 8a and c the resulting calibrated model with spatially distributed parameter search option produced notable changes in model outputs for example subbasin level surface runoff after a particular storm event is 30 to 60 different in the spatially distributed option compared to that in the spatially uniform option although this demonstration is based on point observations at a limited number of gage stations the spatially distributed parameter search option shown here makes swatshare a user friendly futuristic tool for calibrating large scale high resolution hydrologic models with increasingly available gridded earth observation datasets 3 3 consistency across swatshare and swat cup calibration results in this experiment separate swatshare and swat cup calibrations of 30 swat models across four different climate zones in the united states see fig 9 a are conducted this is the first calibration experiment at such an extensive scale because prior studies often considered a single watershed to evaluate alternative calibration tools e g paul and negahban azar 2018 yang et al 2008 this extensive calibration experiment facilitates a conclusive understanding of whether and to what extent swatshare and swat cup results are consistent the metadata corresponding to each of these test models can be accessed through the discovery function of swatshare fig 2 using the supplementary information in s1 all 30 models are simulated at a daily time step with a 2 year period of initialization 2001 2002 followed by a 5 year calibration 2003 2007 and a 3 year validation 2008 2010 using gage station streamflow data as reference identical set of calibration parameters n 18 and their respective initial ranges are selected across all 60 setups see supplementary information s1 the models are calibrated with the nsga ii algorithm in swatshare and the sequential uncertainty fitting versions 2 sufi 2 algorithm in swat cup sufi 2 in swat cup is used because of its well documented applications in the literature models in swatshare are set with 20 generations and 100 iterations whereas models in swat cup are set with 1000 iterations each across two successive batches of iterations this is a measure to ensure an equivalent number of total simulations although such a distribution of generation iteration and batch numbers may not translate the same meaning across two conceptually different calibration tools the results indicate that swatshare and swat cup yield nearly consistent calibration performance in terms of streamflow kling gupta efficiency kge knoben et al 2019 values averaged separately for calibration and validation and across all the watersheds located within a climate zone fig 9b these averaged estimates of calibration performance are also supported by individual watershed kge values fig 9c note the adherence of kge values around the swatshare swat cup 1 1 line despite such consistency in overall calibration performance optimal parameter values in swatshare may be inconsistent with those in swat cup as a result the most optimal parameter values in swatshare with a few exceptions showed low correlation with the corresponding parameter values in swat cup fig 10 a importantly a behavioral change is observed revealing how some of these parameters represented hydrologic processes in the two calibration tools for example the optimal value for parameter esco in swatshare is greater than epco across all four climate zones a pattern completely opposite compared to swat cup fig 10b because esco and epco are the key parameters controlling swat s soil moisture accounting and evapotranspiration mechanisms neitsch et al 2011 rajib et al 2016 their opposite behavior shown in fig 10b indicates two potentially different states of water balance in the same model regardless of similar calibration performances although it is hard to disentangle the underlying factors for such behavioral change in parameters responses these findings add new insights into how the choice of calibration tool can cause equifinality 4 conclusions and future directions data for streamflow climate hydrography topography land use and soil over the internet date back to early 2000s naturally development and use of cyberinfrastructure ci for hydrology or water resources in general had long been focused on instantaneous access to data standardizations of data publication and integration and platforms for data storage and sharing therefore cis and open science platforms geared towards hydrologic modeling needs had been limited swatshare 1 0 https mygeohub org groups water hub swatshare partially filled this gap by providing a collaborative platform for sharing simulation and visualization of swat models rajib et al 2016 this paper introduces swatshare 2 0 a substantially improved version incorporating a new autocalibration tool and demonstrating how ci capabilities can be harnessed to solve five root causes of inefficiency in traditional hydrologic model calibration practices including i platform dependence ii limited computing resource iii lack of programming literacy iv lack of model structural literacy and v no data model interoperability the online interface of swatshare ensures platform independence by letting modelers perform complex calibration tasks in a web browser without requiring any specific computer operating system software packages and their versions and licenses the free access to high performance computing hpc resource allows swatshare to lower the computational burden and thereby offer multi objective calibration options involving up to five hydrology and water quality variables and a highly discretized parameter search option involving spatially distributed datasets swatshare s data model interoperability by seamlessly and instantaneously extracting streamflow and water quality datasets through the usgs web services and automatically recognizing and linking those datasets with corresponding model subbasin and or river ids show a unique example of ci capabilities currently absent in any other hydrologic model autocalibration tools searching the optimal solution using a combination of up to three performance criteria direct feedback of optimal parameters into the model and instantaneous visualization and evaluation of calibration performance are the additional functionalities that further increase the efficiency of the autocalibration process in swatshare without the need for excessive pre and post processing tasks swatshare s online interface serves as the one stop platform to let modelers perform all the above calibration steps through an intuitive hierarchical progression thus minimizing the need for a modeler to have high programming and model structural literacy finally and more importantly results from swatshare autocalibration show that its streamflow prediction is comparable with that from the commonly used offline calibration platform swatcup but for some of the study areas parameters estimated from swatshare may be more meaningful to the physical processes while swatshare demonstrates an example application using the swat model except the model most of the workflow is generic and can be adopted for any model for example the swat model itself is evolving and is currently being modified as swat swat can be brought under swatshare via simple code modifications to incorporate the model s new file naming conventions and data structures without having to change swatshare s overall workflow plan to do this is already underway no doubt the above developments of swatshare addressed a critical need of the hydrologic modeling community yet future developments and re developments of swatshare harnessing newer and better ci and open science capabilities to address broader community needs are imminent there is a push and also broader consensus within the scientific community including hydrology for reproducibility reproducibility cannot be accomplished without the ability to run complete scientific workflows which may and in most cases they do demand a more extensive architecture for interoperability and computing for example currently swatshare cannot create a swat model directly from input datasets it relies on modelers to upload their existing models meaning modelers cannot run complete scientific workflows from model creation to model calibration within swatshare interface fortunately besides access to xsede distributed hpc https www xsede org swatshare s current capabilities to interoperate across multiple cis including mygeohub https mygeohub org hydroshare https www hydroshare org and usgs national water information system https waterdata usgs gov nwis have already created the building blocks to achieve that reproducibility goal efforts to link models and enable model interoperability is also booming the generalizable software pieces developed for swatshare can further excel these interoperability efforts by brining other models and myriad open access autocalibration codes under one platform in essence with calls for adopting fair or open science principles and increasing need for convergent approaches to address societal problems involving water and climate platforms like swatshare can serve as a blueprint for new ci enabled developments in hydrology and beyond hydrology in other disciplinary domains of earth system sciences declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was funded by the u s national science foundation nsf grants 1835822 and 1829764 a r s contribution was partially funded by the national aeronautics and space administration nasa grant 80nssc22k1661 the views expressed in this article are those of the authors and do not necessarily represent the views or the policies of the funding agencies appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105561 
25504,assessment of long term human exposure to spatiotemporally highly variable air pollution requires accounting for human space time activity individual exposure and space time track data are not available over large populations and for long periods and a modelling approach is required however activity based exposure models face here challenges in setting up the model and overly large computations aiming for long term and large population simulations we propose an activity model which integrates statistical and agent based modelling by treating mobility related variables as random variables probability distributions for these variables are estimated or derived from mobility datasets containing observed activities on top of the activity model we implemented an exposure model a case study of exposure assessment was developed using hourly air pollution maps the activity model can potentially integrate any mobility data and is thus applicable when limited time activity data is available at the individual level keywords agent based modelling statistical modelling activity based sampling exposure air pollution data availability the link to the code is provided in the manuscript 1 introduction estimating the effects of ambient air pollution on health luo et al 2016 chiusolo et al 2011 requires assessing the air pollution exposure of the population studied this is a challenge particularly for air pollutants with considerable spatiotemporal variation at street level such as no 2 as where people are and their activities could greatly determine their exposures for this reason spatiotemporal mobility of individuals is important in exposure assessment several studies have compared the differences between exposures assessed neglecting space time activities i e using pollutant concentration values at the home location as a proxy and exposures assessed accounting for human space time activities duan and mage 1997 lu et al 2019 park and kwan 2017 mölter et al 2012 zenk et al 2011 however if we compare these studies we could observe that different activity modelling methods lead to inconsistent conclusions on the effects of accounting for space time activity in exposure assessment park and kwan 2017 and lu et al 2019 show lower variations in no 2 when mobility is modelled while mölter et al 2012 show the opposite the inconsistency is mainly caused by using different methods calling for further studies in activity based exposure assessment also air pollution maps generated from different air pollution models could contribute to the differences in activity based exposure assessment as shown in yoo et al 2015 2021 who investigate the combined effects of spatiotemporal mapping and space time activities on exposure assessment using respectively simulated and measured activity data activity modelling is needed for exposure assessment as measured activity data on individuals is mostly unavailable in large scale epidemiological studies in transportation studies progress has been made in the development of activity models for simulating transportation patterns for example albatross arentze et al 2000 is a transportation oriented system that simulates activities for the entire population based on activity diary data and dynamic constraints on scheduling decisions matsim w axhausen et al 2016 focuses on large scale one day individual activity simulation based on an activity schedule scoring algorithm and detailed road networks activity models such as matsim that target simulating individual behaviours and the interactions between individuals and the environment fall within the concept of agent based modelling abm crooks and heppenstall 2012 as abm takes a bottom up approach to understand the emerging or aggregated behaviours it allows the integration of individual and population behaviours this property of abm makes it an important tool not only for transportation studies but also in studies dedicated to human mobility simulation čertickỳ et al 2015 rosés et al 2018 more examples include wu et al 2019 who attempt to integrate mobile data in abm for activity simulation and lu et al 2019 who focus on simulating the destination locations and comparing no 2 exposures of homemakers and bicycle commuters activity models contribute to the understanding of human activity patterns miller and roorda 2003 many of the activity models are open source and highly customisable which allows for scenario studies the activity models consider a comprehensive set of mobility related and socioeconomic variables such as travel modes work education leisure activities and traffic w axhausen et al 2016 they are commonly parameterised by mobility microcensus data or diary surveys consisting of locations visited and possibly externally estimated schedules then a continuous time mobility track for each individual is estimated based on general rules of human mobility patterns and space time accessibility nguyen et al 2011 gonzalez et al 2008 yang et al 2010 yu 2006 alessandretti et al 2017 miller 1991 each route simulated can be contingent on for instance distance safety city infrastructure and land use law et al 2014 for example shekarrizfard et al 2017 assign the predictions of a travel demand model to a road network to predict individual hourly trajectories for each person the model selects a path from all possible paths by comparing the assigned travel time and the survey travel time in recent years data revealing space time activity patterns and land use types are becoming increasingly available national mobility microcensus data w axhausen et al 2016 big social media data terroso saenz et al 2022 cellular tower data müller et al 2021 as well as tracking campaign data yoo et al 2021 and the derived information are becoming more prevalent the question is how could they contribute to large scale activity simulations statistical modelling has been intensively studied and applied to take opportunities brought by big data the application of statistical modelling also surged in mobility studies for descriptive predictive and prescriptive analytics torre bastida et al 2018 a prominent example is the analysis or simulation of mobility patterns using geo coded twitter data hawelka et al 2014 huang et al 2020 jurdak et al 2015 statistical modelling has also been integrated into other mobility simulation procedures for example kang et al 2020 design a series of sample analysis to derive statistical features and combine them with the urban context for trajectory generation integrating activity based models with predicted air pollution surfaces allows for considering human space time activities in exposure assessment this has been reflected in several exposure studies shekarrizfard et al 2017 deffner et al 2016 gulliver and briggs 2005 dons et al 2011 beckx et al 2009 propose the use of the albatross model to simulate hourly activities and then combine the resulting space time location of individuals with air pollution dispersion model estimates to assess exposure however these activity based exposure assessment models are not designed to assess long term air pollution exposures and associated uncertainties for large scale epidemiological studies specifically long term simulation means that the model is capable of quantifying exposure averaged over a year or multiple years uncertainty needs to be quantified for each simulated activity including e g time schedules travel modes and possible destination locations a study has focused on long term exposure assessment of a large population and considering uncertainty in the exposure assessment lu et al 2019 but the activity schedules generated are less realistic activity schedules compared to the exposure models integrating a sophisticated activity model w axhausen et al 2016 the need for quantifying long term exposures and the uncertainty call for the development of activity simulation models that are dedicated to exposure assessment this study aims to incorporate statistical modelling of mobility data in a space time activity simulation model to enable long term exposure assessment for large populations and to illustrate the approach with a case study assessing exposure in the area of utrecht the netherlands using the dutch national mobility microcensus dataset our model assumes uncertain mobility related variables such as the travel mode and maximum travel distance as random variables statistical modelling is applied to derive estimate or empirically specify probability distributions of these variables from mobility microcensus datasets or literature for each activity and according to the attribute of a person such as age and occupation this allows a more accurate simulation of activities as people s activity patterns may share similarities according to their socioeconomic statuses these mobility related variables are then used as inputs for the simulation of space time activities the monte carlo approach is applied to sample from each mobility related variable for simulating the activities lastly the exposure is calculated by aggregating the air pollution over the activity tracks and over time the rest of this paper is organised as follows we first describe our activity simulation model following the odd overview design concepts and details protocol railsback and grimm 2019 page 37 then we show how the activity simulation model is used for exposure assessment and demonstrate the modelling process in a case study 2 design concepts and model structure of the activity simulation model 2 1 overview purpose and scales the activity simulation model is developed for large population scale e g the entire population of the netherlands long term personal air quality exposure assessment accounting for human space time activities the model focuses on having land transportation e g cars public transport bikes on foot as major commuting means the activity simulation model is developed with two key features 1 it can simulate long term travel behaviours and 2 the uncertainty of human space time activity is explicitly quantified the activity simulation is based on the statistical modelling of mobility related variables and attributes of a population the model facilitates integrating mobility related information from different sources entities state variables and an overview of the process the activity model treats individuals living at each residential location as entities the attributes of entities that are related to the activity patterns such as age gender education occupation income working status e g full or part time workers having children or not having a car or not and home locations are considered as static state variables mobility related variables are modelled as random or fixed the randomness is assigned to variables whose values we are uncertain about for example travel mode start time and duration they are characterised by a probability distribution the simulation process includes two steps fig 1 the first is to derive or estimate probability distributions of the random variables from mobility data based on static state variables in the next step samples are drawn from these probability distributions to decide sequentially the maximum travel range the origin and destination locations and the travel mode based on the origin and destination locations and the travel mode a route is queried from the road network of openstreetmap boeing 2017 for the generation of the activity schedules and space time tracks the routes may be chosen by different criteria e g based on the shortest distance or the shortest arrival time for auto vehicles the second step is repeated several times each repetition is called an iteration for activity prediction and uncertainty quantification i e each time a different activity schedule and spatial locations are generated note the ensembling effect of averaging assessed exposures over several iterations the precision of the exposure is expected to increase as the variance reduces 2 2 design concepts in exposure assessment the most important activity variables are the destination locations the commuting routes largely determined by the travel mode and the activity schedules the concept is thus to simulate personal activities based on probability sampling of agents departure time travel mode maximum travel range free time activities and possible destinations the duration of an activity can be pre defined or sampled the probability distributions i e probability density function for continuous variables and probability mass function for discrete variables can be empirically specified or derived from mobility surveys with different socioeconomic variables or other attributes that relate to mobility e g age as covariates the duration of working time is specified for different population groups e g 8 h for full time workers 2 3 model details 2 3 1 input and output of the model the input of the activity model consists of 1 home locations of individuals whose exposures are to be assessed 2 all the possible destination locations for each destination type e g all the work locations all the school locations and 3 the mobility data alternatively the mobility data can be replaced by probability distribution functions or probability tables for example the probability of each travel mode for different distance ranges and different attributes of the population e g age can be the input of the model the output of the activity model consists of activity schedules and corresponding spatial locations including geospatial tracks for each iteration and each individual the activity schedule consists for each trip of a start time an end time a travel mode and the activity name and a corresponding code activity code in table 1 which separates different free time activities and links the schedule to the exposure assessment table 1 shows an example 2 3 2 generating activity schedules and spatial locations the most important task of the activity model is to generate activity schedules together with the spatial locations including tracks for each residential location fig 2 in each iteration the model sequentially samples or calculates the travel distance the destination location the travel mode the travel duration a free time activity and the start and end time of each activity more specifically for each residence location the main steps include 1 specify the probability density function of the travel distance which can be estimated based on the mobility survey data in this study or derived from literature 2 sample from the density function of the travel distance and use this distance to refine the sampling space of the destination location specifically from the origin location a buffer is drawn with the travel distance as radius and a sample is taken out of all the candidate destination locations that lie in the buffer fig 4a all the potential destination locations for example all of the school or sport facility locations are assumed known and are input to the model these locations can come from for instance governmental statistics or the openstreetmap 3 if there is no destination location within the travel distance the nearest destination location is used fig 4b 4 the probability distribution of the travel mode depends on the distance travelled and is calculated for different population groups e g elderly people students and travel purposes e g going to work based on the mobility survey data 5 the euclidean distance between the destination and the origin is used to determine the probability of taking a certain travel mode based on this probability a travel mode is sampled the model considers three travel modes walking cycling and driving or taking an auto vehicle the reason that we use euclidean distance instead of the distance along the road network is to reduce computational time this is further discussed in the discussion section 6 a route is queried from the road network for the sampled travel mode for example a walking path is queried if the travel mode is on foot a bicycle path is queried if the travel mode is by bicycle by default the shortest distance road will be chosen for on foot or by bicycle and the fastest route will be chosen for auto vehicles based on openstreetmap 7 based on the travel distance and the travel mode the duration is calculated 8 based on the duration the end time of a trip or the start time of the next activity is calculated 9 if the start time of the current trip is unknown e g the first trip of the day the start time is generated with a distribution the default departure time to work in our model is sampled from a gaussian distribution with mean 8 i e 8 am and standard deviation 0 2 10 a free time activity occurs 1 1 5 h i e a random number is chosen between 1 and 1 5 with equal probability before a person goes to work in the morning and after a person arrives home from work it is chosen by randomly sampling from a set of possible free time activities currently two types of activities are implemented staying at home and taking a walk from the home location taking a walk is implemented with a gaussian kernel specifying the probability that a person visits a location the size and variance of the kernel are user defined 2 3 3 routing based on the transportation mode the travel routes are queried from road networks constructed using the python package osmnx boeing 2017 the osmnx processes routes from openstreetmap openstreetmap contributors 2020 into a network and removes the redundant nodes boeing 2017 this reduces the dataset size and accelerates route querying there are three types of road networks implemented in osmnx auto vehicles bicycle and walk boeing 2017 the road network consists of an attribute travel time for the travel mode auto vehicles the travel time is calculated in osmnx which takes the information of auto vehicle speed on different roads from the openstreetmap for the travel mode bicycle and on foot the speed is assumed to be constant and can be specified by a user by default the speed is set to 14 km hr and 5 km hr for cycling and walking respectively this allows selecting routes either based on the shortest distance or the shortest travel time by default we select a route based on the shortest travel time 3 exposure assessment based on the spatiotemporal tracks of each individual simulated by the activity model and the temporal air pollution maps the personal exposure is calculated as spatial and temporally weighted aggregation of air pollution concentration considering the indoor outdoor ratio the infiltration of pollution if the activity model is run n times to simulate different schedules and spatial locations for each person the exposure is calculated n times i e for each iteration by default we use the mean of exposure calculated in the n iterations as the final exposure assessed the exposure assessment module queries and aggregates air pollution concentrations for each individual and over each activity in the activity schedule we describe the process using the air pollutant no 2 as an example in pseudo code algorithm 1 below the exposure at home and work are calculated as the air pollution concentration at the front door home and work locations multiplied by an indoor infiltration ratio by default this ratio is set to 0 7 based on salonen et al 2019 currently implemented free time activities include a staying at home b in the garden or on the terrace and c taking a walk the free time activity b is implemented as walking randomly within a distance default 0 002 degree about 220 m close to the home location the free time activity c is implemented as using a gaussian kernel of distances away from home to the probability of being a presence at the location default mean 2 degree about 220 km and standard deviation 0 1 degree about 11 km an activity oriented view of the exposure calculated in a single iteration for two individuals is shown in fig 3 which shows the activities starting and end time and the average exposure during the time of a certain activity 4 case study activity modelling using the dutch national microcensus data we demonstrate our model by simulating the exposure for the people with occupation students and age 18 and older to represent the population group university students in utrecht the ovin survey 2010 2017 followed by ovg and mon consists of 33 people in this group living in utrecht at that time ovin is collected by statistics netherlands in dutch centraal bureau voor de statistiek cbs for a one day trip based diary it consists of 0 3 ca 52 000 of the dutch population 17 4 million we processed the text in the original dataset to extract the range of a certain variable and calculate a mean of it for example the variable travel distance is in the form 3 km to 5 km and it was processed into three variables lower limit 3 km upper limit 5 km and mean 4 km travel distances the students are assumed to go to a university or college in the city during the daytime and do free time activities including staying at home in the evening a while after returning home the departure time of going to the university or college is sampled from a gaussian distribution with a mean of 9 for 9 am and a standard deviation of 1 for 1 h the departure time of leaving the university or college is sampled from a gaussian distribution with a mean of 17 for 5 pm and a standard deviation of 1 all the university and college locations queried from openstreetmap are used as possible destination locations the probability distributions of the travel mode and the travel distance are derived from ovin for the travel mode we regrouped the transportation means as they are in the osmnx with auto vehicles including taking cars or taxis or all the other land vehicles e g bus tram the distance range less than 1 km 1 2 5 km 2 5 3 7 km is determined empirically for each distance range the incidence of each travel mode is divided by the total incidence to obtain the probability of the travel mode in each travel distance range for the travel distance the histogram and log normal tests through the qq quantile vs quantile plot and the shapiro test indicate the distribution is log normal 4 1 air pollution prediction temporal air pollution maps are predicted using statistical modelling the air pollution measurements are aggregated into hour of the year e g average no 2 at 12 o clock of the year at each time step the ensemble tree based algorithm lightgbm light gradient boosting machine ke et al 2017 is trained using the annually aggregated air pollution measurements of that hour lightgbm uses histogram based algorithms to bin the continuous values of each feature the hyperparameters of the model are tuned using 5 fold cross validation we combined the official hourly ground station measurements of germany 416 stations and the netherlands 66 stations to predict annual hourly no 2 concentrations in utrecht for 2017 the geospatial predictors include road densities in different buffers 100 300 500 1000 3000 5000 m and of highways primary roads local roads from openstreetmap openstreetmap contributors 2020 monthly wind speed and temperature of 2017 from era5 land model re analysis muñoz sabater et al 2021 elevation of 90 m resolution dai et al 2017 radiation world bank 2022 and sentinel 5p l3 product column density of 2018 1 1 there is an inconsistency in time but we have evaluated the use of sentinel 5p of 2018 vs omi of 2017 when the no 2 observations are of 2017 lu et al 2020a and found that despite the mismatch in time involving sentinel 5p l3 product column density of 2018 led to better prediction results the reason is that the spatial variation is larger than the temporal variation for sentinel 5p data 10 km resolution google earth engine 2019 population from the global human settlement population grid for 2015 schiavina et al 2019 is resampled to 1000 3000 and 5000 m resolutions for each pixel of the earth nightlight satellite data elvidge et al 2017 the mean is calculated in each 450 900 and 1350 m radius window 4 2 results annual hourly no 2 predicted using light gbm are shown in fig 5 the spatiotemporal dynamics of no 2 are visible these 24 maps are the input of the exposure calculation component the no 2 exposure is assessed in 11 iterations and the mean of them is shown in fig 6 exposure assessed using real locations from the survey in an iteration is shown for comparison it should be noted that the activity schedules are also simulated in the situation of using true locations among all the individuals 80 of exposure calculated using the real location is within the exposures calculated with simulated locations in multiple iterations this indicates the uncertainty quantified with exposure assessed in 11 iterations is satisfying in this scenario in practice more iterations lead to more accurate uncertainty estimation as well as exposure prediction e g by taking the mean the r squared between exposure assessed using simulated locations and an iteration of exposures assessed using the real locations is 0 35 besides different destination locations departure times travel modes and free time activities likely cause differences in assessed exposure the exposure assessed using the proposed simulation model for the university students is shown in fig 7 it can be observed that high exposures do not necessarily occur for people with high concentrations at the front door home locations the exposure assessed is in general lower than the home location concentration the reason is the use of a relatively low indoor outdoor ratio 0 7 note that with university students the uncertainty in choosing the destination location is relatively low compared to for example full time workers as the number of university or colleagues in a city is limited 5 discussion we have described an activity simulation method that uses statistical modelling to parameterise the agent based model and showed how it can be used together with temporal air prediction maps for exposure assessment 2 2 the activity model is open sourced on github https github com mengluchu agentmodel tree main activity modeling core a case study using the dutch microcensus data was developed for the assessment of no 2 exposure by university students in utrecht we compared between using the home location concentrations and the exposure assessed using our exposure model with real and simulated destination locations the major novelty of our activity modelling concept lies in that it is based on sampling from the probability distributions of the mobility related variables and the probabilities could be determined for different attributes of a population such as occupation and age the activity modelling process is refocused on the statistical modelling of mobility data statistical modelling is an important mean to characterise the distribution functions of activity model inputs based on the socioeconomic attributes of a population this makes the activity modelling not only a simulation problem but also an approach to integrate different sources of information for optimal estimation of the inputs of the agent based model the prominent role of data integration and big data in gaining us more information and insights about our society is clear our model could potentially integrate mobility data from different sources information from literature as well as social economic and environmental data to facilitate characterising the mobility related variables and reducing the uncertainty compared to the proposed model the approach proposed in lu et al 2019 chooses the destination locations of trips without considering people s travel behaviours also there is no randomness in the activity schedule and mobility related variables in the case study we singled out a population group i e university students in practice we commonly would model for multiple population groups with our model two approaches can be conveniently implemented the first approach is to identify the population groups from the data more specifically we could use socioeconomic attributes as independent variables in a statistical model to predict the distribution of mobility related variables the second approach is to group the population based on between group variability of the activity patterns and then simulate for each group the first approach allows fitting flexible models e g an ensemble tree model as many population attributes such as age are in practice numerical the second approach may include more data in each population group for fitting the mobility related variables the exposure calculation model iterates over each person and then aggregates the air pollution concentrations over the route of each activity of the person with the indoor outdoor ratio accounted for it is highly parallelisable as the exposure calculation is independent for each individual note the individuals could still interact with each other and with the environment when simulating the route and the schedules another way of calculating the exposure is to iterate over each time step i e for each time step the exposure of the entire population is calculated this way can be parallelised for each time step instead of each individual for a population larger than the number of time steps this approach may provide an efficient alternative this study focuses on the new exposure modelling concept and provides a relatively simple model below we listed the limitations of the current model and our envisioned extensions we currently only generate two probability distributions from the mobility data the distribution of travel mode and the maximum travel range as a function of the origin distribution locations and the travel mode the distributions of the departure times and free time activity are empirical the model also only chooses one destination location for the entire schedule i e to and back from work in the future more commuting activities could be implemented an effective way would be to include sophisticated transportation modelling in the activity model to simulate secondary and more commuting activities trips by public transportation and more detailed road conditions such as traffic congestion with our case study we represented the annually averaged diurnal variations in exposure but not variations over a year this would be an interesting next step and it might be feasible in countries where the no 2 observations are available for each day it is possible to predict the no 2 at finer resolutions lu et al 2020b since our activity model is flexible in simulating at different temporal resolutions exposure assessment representing daily variations is possible and the precision could be improved by for example taking into account drivers of seasonal changes in behaviour conditioned by sufficient computational power and data storage a more accurate exposure assessment would likely give different exposure maps how the pattern in fig 7 would change with temporally more detailed exposure assessment is beyond the scope of this study but future studies should confirm if the conclusions derived from fig 7 stay the proposed activity modelling approach can potentially incorporate environmental socio economic or any variables influencing the travel behaviour for example the probability distribution of travel distance could be made dependent on whether the area is urban or rural in addition to the shortest distance or fastest routes the greenest route or least polluted routes could be considered the traffic simulated by the activity model could also be used to update air pollution maps w axhausen et al 2016 the indoor outdoor ratio was set to a fixed value of 0 7 it could be better estimated as a function of temperature müller et al 2021 an infiltration ratio may also be applied when taking vehicles for commuting we used euclidean distance between the departure and arrival locations for selecting the travel mode as the distance is usually shorter than the actual route the approach may be more likely to choose a lower speed traffic mode e g on foot instead of by bicycle the reason for using euclidean distance instead of the road distance is to avoid additional route querying which takes more computation time commonly the euclidean distance becomes closer to the road distance with increased travel distance it is however possible to calculate the road distance instead of the euclidean distance if the computational time is not a concern the uncertainties from statistical modelling and air pollution mapping should be quantified and discussed together with the uncertainty from the activity simulation process we used 11 iterations in our case study as it is shown in lu et al 2019 that this number of iterations is sufficient for quantifying the uncertainty in lu et al 2019 though the destination locations are the only random variable the candidate destination locations are around 500 times more compared to in our case as it included all potential working locations in utrecht and there is no travel distance constraint for this reason we believe 11 iterations are also sufficient for the case study in practice the number of iterations could be determined with the method used in lu et al 2019 the modelling concept can potentially be applied to the exposure assessment of other pollutants such as ozone and particulate matters 6 conclusion an activity simulation model that integrates statistical and agent based modelling is developed for long term large scale exposure assessment the key concept of the activity model is to estimate the probability functions of variables that determine the activity patterns mobility related variables from mobility data to parameterise the agent based model the mobility related variables are repeatedly sampled from the corresponding probability function correspondingly personal exposure is repeatedly assessed in multiple iterations to allow uncertainty quantification and improve the prediction the activity modelling concept could incorporate mobility and environmental information from different sources and makes this process independent of the agent based simulation component of the approach the model is easily extensible and applicable geographically we used the dutch national microcensus survey to demonstrate our activity simulation model the simulated activity then combines with hourly no 2 predicted from the national ground stations of germany and the netherlands to demonstrate the exposure assessment declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study is supported by the health effects institute no 4972 rfa19 1 20 6 the authors are grateful to the editors for handling this manuscript and the reviewers for their constructive comments 
25504,assessment of long term human exposure to spatiotemporally highly variable air pollution requires accounting for human space time activity individual exposure and space time track data are not available over large populations and for long periods and a modelling approach is required however activity based exposure models face here challenges in setting up the model and overly large computations aiming for long term and large population simulations we propose an activity model which integrates statistical and agent based modelling by treating mobility related variables as random variables probability distributions for these variables are estimated or derived from mobility datasets containing observed activities on top of the activity model we implemented an exposure model a case study of exposure assessment was developed using hourly air pollution maps the activity model can potentially integrate any mobility data and is thus applicable when limited time activity data is available at the individual level keywords agent based modelling statistical modelling activity based sampling exposure air pollution data availability the link to the code is provided in the manuscript 1 introduction estimating the effects of ambient air pollution on health luo et al 2016 chiusolo et al 2011 requires assessing the air pollution exposure of the population studied this is a challenge particularly for air pollutants with considerable spatiotemporal variation at street level such as no 2 as where people are and their activities could greatly determine their exposures for this reason spatiotemporal mobility of individuals is important in exposure assessment several studies have compared the differences between exposures assessed neglecting space time activities i e using pollutant concentration values at the home location as a proxy and exposures assessed accounting for human space time activities duan and mage 1997 lu et al 2019 park and kwan 2017 mölter et al 2012 zenk et al 2011 however if we compare these studies we could observe that different activity modelling methods lead to inconsistent conclusions on the effects of accounting for space time activity in exposure assessment park and kwan 2017 and lu et al 2019 show lower variations in no 2 when mobility is modelled while mölter et al 2012 show the opposite the inconsistency is mainly caused by using different methods calling for further studies in activity based exposure assessment also air pollution maps generated from different air pollution models could contribute to the differences in activity based exposure assessment as shown in yoo et al 2015 2021 who investigate the combined effects of spatiotemporal mapping and space time activities on exposure assessment using respectively simulated and measured activity data activity modelling is needed for exposure assessment as measured activity data on individuals is mostly unavailable in large scale epidemiological studies in transportation studies progress has been made in the development of activity models for simulating transportation patterns for example albatross arentze et al 2000 is a transportation oriented system that simulates activities for the entire population based on activity diary data and dynamic constraints on scheduling decisions matsim w axhausen et al 2016 focuses on large scale one day individual activity simulation based on an activity schedule scoring algorithm and detailed road networks activity models such as matsim that target simulating individual behaviours and the interactions between individuals and the environment fall within the concept of agent based modelling abm crooks and heppenstall 2012 as abm takes a bottom up approach to understand the emerging or aggregated behaviours it allows the integration of individual and population behaviours this property of abm makes it an important tool not only for transportation studies but also in studies dedicated to human mobility simulation čertickỳ et al 2015 rosés et al 2018 more examples include wu et al 2019 who attempt to integrate mobile data in abm for activity simulation and lu et al 2019 who focus on simulating the destination locations and comparing no 2 exposures of homemakers and bicycle commuters activity models contribute to the understanding of human activity patterns miller and roorda 2003 many of the activity models are open source and highly customisable which allows for scenario studies the activity models consider a comprehensive set of mobility related and socioeconomic variables such as travel modes work education leisure activities and traffic w axhausen et al 2016 they are commonly parameterised by mobility microcensus data or diary surveys consisting of locations visited and possibly externally estimated schedules then a continuous time mobility track for each individual is estimated based on general rules of human mobility patterns and space time accessibility nguyen et al 2011 gonzalez et al 2008 yang et al 2010 yu 2006 alessandretti et al 2017 miller 1991 each route simulated can be contingent on for instance distance safety city infrastructure and land use law et al 2014 for example shekarrizfard et al 2017 assign the predictions of a travel demand model to a road network to predict individual hourly trajectories for each person the model selects a path from all possible paths by comparing the assigned travel time and the survey travel time in recent years data revealing space time activity patterns and land use types are becoming increasingly available national mobility microcensus data w axhausen et al 2016 big social media data terroso saenz et al 2022 cellular tower data müller et al 2021 as well as tracking campaign data yoo et al 2021 and the derived information are becoming more prevalent the question is how could they contribute to large scale activity simulations statistical modelling has been intensively studied and applied to take opportunities brought by big data the application of statistical modelling also surged in mobility studies for descriptive predictive and prescriptive analytics torre bastida et al 2018 a prominent example is the analysis or simulation of mobility patterns using geo coded twitter data hawelka et al 2014 huang et al 2020 jurdak et al 2015 statistical modelling has also been integrated into other mobility simulation procedures for example kang et al 2020 design a series of sample analysis to derive statistical features and combine them with the urban context for trajectory generation integrating activity based models with predicted air pollution surfaces allows for considering human space time activities in exposure assessment this has been reflected in several exposure studies shekarrizfard et al 2017 deffner et al 2016 gulliver and briggs 2005 dons et al 2011 beckx et al 2009 propose the use of the albatross model to simulate hourly activities and then combine the resulting space time location of individuals with air pollution dispersion model estimates to assess exposure however these activity based exposure assessment models are not designed to assess long term air pollution exposures and associated uncertainties for large scale epidemiological studies specifically long term simulation means that the model is capable of quantifying exposure averaged over a year or multiple years uncertainty needs to be quantified for each simulated activity including e g time schedules travel modes and possible destination locations a study has focused on long term exposure assessment of a large population and considering uncertainty in the exposure assessment lu et al 2019 but the activity schedules generated are less realistic activity schedules compared to the exposure models integrating a sophisticated activity model w axhausen et al 2016 the need for quantifying long term exposures and the uncertainty call for the development of activity simulation models that are dedicated to exposure assessment this study aims to incorporate statistical modelling of mobility data in a space time activity simulation model to enable long term exposure assessment for large populations and to illustrate the approach with a case study assessing exposure in the area of utrecht the netherlands using the dutch national mobility microcensus dataset our model assumes uncertain mobility related variables such as the travel mode and maximum travel distance as random variables statistical modelling is applied to derive estimate or empirically specify probability distributions of these variables from mobility microcensus datasets or literature for each activity and according to the attribute of a person such as age and occupation this allows a more accurate simulation of activities as people s activity patterns may share similarities according to their socioeconomic statuses these mobility related variables are then used as inputs for the simulation of space time activities the monte carlo approach is applied to sample from each mobility related variable for simulating the activities lastly the exposure is calculated by aggregating the air pollution over the activity tracks and over time the rest of this paper is organised as follows we first describe our activity simulation model following the odd overview design concepts and details protocol railsback and grimm 2019 page 37 then we show how the activity simulation model is used for exposure assessment and demonstrate the modelling process in a case study 2 design concepts and model structure of the activity simulation model 2 1 overview purpose and scales the activity simulation model is developed for large population scale e g the entire population of the netherlands long term personal air quality exposure assessment accounting for human space time activities the model focuses on having land transportation e g cars public transport bikes on foot as major commuting means the activity simulation model is developed with two key features 1 it can simulate long term travel behaviours and 2 the uncertainty of human space time activity is explicitly quantified the activity simulation is based on the statistical modelling of mobility related variables and attributes of a population the model facilitates integrating mobility related information from different sources entities state variables and an overview of the process the activity model treats individuals living at each residential location as entities the attributes of entities that are related to the activity patterns such as age gender education occupation income working status e g full or part time workers having children or not having a car or not and home locations are considered as static state variables mobility related variables are modelled as random or fixed the randomness is assigned to variables whose values we are uncertain about for example travel mode start time and duration they are characterised by a probability distribution the simulation process includes two steps fig 1 the first is to derive or estimate probability distributions of the random variables from mobility data based on static state variables in the next step samples are drawn from these probability distributions to decide sequentially the maximum travel range the origin and destination locations and the travel mode based on the origin and destination locations and the travel mode a route is queried from the road network of openstreetmap boeing 2017 for the generation of the activity schedules and space time tracks the routes may be chosen by different criteria e g based on the shortest distance or the shortest arrival time for auto vehicles the second step is repeated several times each repetition is called an iteration for activity prediction and uncertainty quantification i e each time a different activity schedule and spatial locations are generated note the ensembling effect of averaging assessed exposures over several iterations the precision of the exposure is expected to increase as the variance reduces 2 2 design concepts in exposure assessment the most important activity variables are the destination locations the commuting routes largely determined by the travel mode and the activity schedules the concept is thus to simulate personal activities based on probability sampling of agents departure time travel mode maximum travel range free time activities and possible destinations the duration of an activity can be pre defined or sampled the probability distributions i e probability density function for continuous variables and probability mass function for discrete variables can be empirically specified or derived from mobility surveys with different socioeconomic variables or other attributes that relate to mobility e g age as covariates the duration of working time is specified for different population groups e g 8 h for full time workers 2 3 model details 2 3 1 input and output of the model the input of the activity model consists of 1 home locations of individuals whose exposures are to be assessed 2 all the possible destination locations for each destination type e g all the work locations all the school locations and 3 the mobility data alternatively the mobility data can be replaced by probability distribution functions or probability tables for example the probability of each travel mode for different distance ranges and different attributes of the population e g age can be the input of the model the output of the activity model consists of activity schedules and corresponding spatial locations including geospatial tracks for each iteration and each individual the activity schedule consists for each trip of a start time an end time a travel mode and the activity name and a corresponding code activity code in table 1 which separates different free time activities and links the schedule to the exposure assessment table 1 shows an example 2 3 2 generating activity schedules and spatial locations the most important task of the activity model is to generate activity schedules together with the spatial locations including tracks for each residential location fig 2 in each iteration the model sequentially samples or calculates the travel distance the destination location the travel mode the travel duration a free time activity and the start and end time of each activity more specifically for each residence location the main steps include 1 specify the probability density function of the travel distance which can be estimated based on the mobility survey data in this study or derived from literature 2 sample from the density function of the travel distance and use this distance to refine the sampling space of the destination location specifically from the origin location a buffer is drawn with the travel distance as radius and a sample is taken out of all the candidate destination locations that lie in the buffer fig 4a all the potential destination locations for example all of the school or sport facility locations are assumed known and are input to the model these locations can come from for instance governmental statistics or the openstreetmap 3 if there is no destination location within the travel distance the nearest destination location is used fig 4b 4 the probability distribution of the travel mode depends on the distance travelled and is calculated for different population groups e g elderly people students and travel purposes e g going to work based on the mobility survey data 5 the euclidean distance between the destination and the origin is used to determine the probability of taking a certain travel mode based on this probability a travel mode is sampled the model considers three travel modes walking cycling and driving or taking an auto vehicle the reason that we use euclidean distance instead of the distance along the road network is to reduce computational time this is further discussed in the discussion section 6 a route is queried from the road network for the sampled travel mode for example a walking path is queried if the travel mode is on foot a bicycle path is queried if the travel mode is by bicycle by default the shortest distance road will be chosen for on foot or by bicycle and the fastest route will be chosen for auto vehicles based on openstreetmap 7 based on the travel distance and the travel mode the duration is calculated 8 based on the duration the end time of a trip or the start time of the next activity is calculated 9 if the start time of the current trip is unknown e g the first trip of the day the start time is generated with a distribution the default departure time to work in our model is sampled from a gaussian distribution with mean 8 i e 8 am and standard deviation 0 2 10 a free time activity occurs 1 1 5 h i e a random number is chosen between 1 and 1 5 with equal probability before a person goes to work in the morning and after a person arrives home from work it is chosen by randomly sampling from a set of possible free time activities currently two types of activities are implemented staying at home and taking a walk from the home location taking a walk is implemented with a gaussian kernel specifying the probability that a person visits a location the size and variance of the kernel are user defined 2 3 3 routing based on the transportation mode the travel routes are queried from road networks constructed using the python package osmnx boeing 2017 the osmnx processes routes from openstreetmap openstreetmap contributors 2020 into a network and removes the redundant nodes boeing 2017 this reduces the dataset size and accelerates route querying there are three types of road networks implemented in osmnx auto vehicles bicycle and walk boeing 2017 the road network consists of an attribute travel time for the travel mode auto vehicles the travel time is calculated in osmnx which takes the information of auto vehicle speed on different roads from the openstreetmap for the travel mode bicycle and on foot the speed is assumed to be constant and can be specified by a user by default the speed is set to 14 km hr and 5 km hr for cycling and walking respectively this allows selecting routes either based on the shortest distance or the shortest travel time by default we select a route based on the shortest travel time 3 exposure assessment based on the spatiotemporal tracks of each individual simulated by the activity model and the temporal air pollution maps the personal exposure is calculated as spatial and temporally weighted aggregation of air pollution concentration considering the indoor outdoor ratio the infiltration of pollution if the activity model is run n times to simulate different schedules and spatial locations for each person the exposure is calculated n times i e for each iteration by default we use the mean of exposure calculated in the n iterations as the final exposure assessed the exposure assessment module queries and aggregates air pollution concentrations for each individual and over each activity in the activity schedule we describe the process using the air pollutant no 2 as an example in pseudo code algorithm 1 below the exposure at home and work are calculated as the air pollution concentration at the front door home and work locations multiplied by an indoor infiltration ratio by default this ratio is set to 0 7 based on salonen et al 2019 currently implemented free time activities include a staying at home b in the garden or on the terrace and c taking a walk the free time activity b is implemented as walking randomly within a distance default 0 002 degree about 220 m close to the home location the free time activity c is implemented as using a gaussian kernel of distances away from home to the probability of being a presence at the location default mean 2 degree about 220 km and standard deviation 0 1 degree about 11 km an activity oriented view of the exposure calculated in a single iteration for two individuals is shown in fig 3 which shows the activities starting and end time and the average exposure during the time of a certain activity 4 case study activity modelling using the dutch national microcensus data we demonstrate our model by simulating the exposure for the people with occupation students and age 18 and older to represent the population group university students in utrecht the ovin survey 2010 2017 followed by ovg and mon consists of 33 people in this group living in utrecht at that time ovin is collected by statistics netherlands in dutch centraal bureau voor de statistiek cbs for a one day trip based diary it consists of 0 3 ca 52 000 of the dutch population 17 4 million we processed the text in the original dataset to extract the range of a certain variable and calculate a mean of it for example the variable travel distance is in the form 3 km to 5 km and it was processed into three variables lower limit 3 km upper limit 5 km and mean 4 km travel distances the students are assumed to go to a university or college in the city during the daytime and do free time activities including staying at home in the evening a while after returning home the departure time of going to the university or college is sampled from a gaussian distribution with a mean of 9 for 9 am and a standard deviation of 1 for 1 h the departure time of leaving the university or college is sampled from a gaussian distribution with a mean of 17 for 5 pm and a standard deviation of 1 all the university and college locations queried from openstreetmap are used as possible destination locations the probability distributions of the travel mode and the travel distance are derived from ovin for the travel mode we regrouped the transportation means as they are in the osmnx with auto vehicles including taking cars or taxis or all the other land vehicles e g bus tram the distance range less than 1 km 1 2 5 km 2 5 3 7 km is determined empirically for each distance range the incidence of each travel mode is divided by the total incidence to obtain the probability of the travel mode in each travel distance range for the travel distance the histogram and log normal tests through the qq quantile vs quantile plot and the shapiro test indicate the distribution is log normal 4 1 air pollution prediction temporal air pollution maps are predicted using statistical modelling the air pollution measurements are aggregated into hour of the year e g average no 2 at 12 o clock of the year at each time step the ensemble tree based algorithm lightgbm light gradient boosting machine ke et al 2017 is trained using the annually aggregated air pollution measurements of that hour lightgbm uses histogram based algorithms to bin the continuous values of each feature the hyperparameters of the model are tuned using 5 fold cross validation we combined the official hourly ground station measurements of germany 416 stations and the netherlands 66 stations to predict annual hourly no 2 concentrations in utrecht for 2017 the geospatial predictors include road densities in different buffers 100 300 500 1000 3000 5000 m and of highways primary roads local roads from openstreetmap openstreetmap contributors 2020 monthly wind speed and temperature of 2017 from era5 land model re analysis muñoz sabater et al 2021 elevation of 90 m resolution dai et al 2017 radiation world bank 2022 and sentinel 5p l3 product column density of 2018 1 1 there is an inconsistency in time but we have evaluated the use of sentinel 5p of 2018 vs omi of 2017 when the no 2 observations are of 2017 lu et al 2020a and found that despite the mismatch in time involving sentinel 5p l3 product column density of 2018 led to better prediction results the reason is that the spatial variation is larger than the temporal variation for sentinel 5p data 10 km resolution google earth engine 2019 population from the global human settlement population grid for 2015 schiavina et al 2019 is resampled to 1000 3000 and 5000 m resolutions for each pixel of the earth nightlight satellite data elvidge et al 2017 the mean is calculated in each 450 900 and 1350 m radius window 4 2 results annual hourly no 2 predicted using light gbm are shown in fig 5 the spatiotemporal dynamics of no 2 are visible these 24 maps are the input of the exposure calculation component the no 2 exposure is assessed in 11 iterations and the mean of them is shown in fig 6 exposure assessed using real locations from the survey in an iteration is shown for comparison it should be noted that the activity schedules are also simulated in the situation of using true locations among all the individuals 80 of exposure calculated using the real location is within the exposures calculated with simulated locations in multiple iterations this indicates the uncertainty quantified with exposure assessed in 11 iterations is satisfying in this scenario in practice more iterations lead to more accurate uncertainty estimation as well as exposure prediction e g by taking the mean the r squared between exposure assessed using simulated locations and an iteration of exposures assessed using the real locations is 0 35 besides different destination locations departure times travel modes and free time activities likely cause differences in assessed exposure the exposure assessed using the proposed simulation model for the university students is shown in fig 7 it can be observed that high exposures do not necessarily occur for people with high concentrations at the front door home locations the exposure assessed is in general lower than the home location concentration the reason is the use of a relatively low indoor outdoor ratio 0 7 note that with university students the uncertainty in choosing the destination location is relatively low compared to for example full time workers as the number of university or colleagues in a city is limited 5 discussion we have described an activity simulation method that uses statistical modelling to parameterise the agent based model and showed how it can be used together with temporal air prediction maps for exposure assessment 2 2 the activity model is open sourced on github https github com mengluchu agentmodel tree main activity modeling core a case study using the dutch microcensus data was developed for the assessment of no 2 exposure by university students in utrecht we compared between using the home location concentrations and the exposure assessed using our exposure model with real and simulated destination locations the major novelty of our activity modelling concept lies in that it is based on sampling from the probability distributions of the mobility related variables and the probabilities could be determined for different attributes of a population such as occupation and age the activity modelling process is refocused on the statistical modelling of mobility data statistical modelling is an important mean to characterise the distribution functions of activity model inputs based on the socioeconomic attributes of a population this makes the activity modelling not only a simulation problem but also an approach to integrate different sources of information for optimal estimation of the inputs of the agent based model the prominent role of data integration and big data in gaining us more information and insights about our society is clear our model could potentially integrate mobility data from different sources information from literature as well as social economic and environmental data to facilitate characterising the mobility related variables and reducing the uncertainty compared to the proposed model the approach proposed in lu et al 2019 chooses the destination locations of trips without considering people s travel behaviours also there is no randomness in the activity schedule and mobility related variables in the case study we singled out a population group i e university students in practice we commonly would model for multiple population groups with our model two approaches can be conveniently implemented the first approach is to identify the population groups from the data more specifically we could use socioeconomic attributes as independent variables in a statistical model to predict the distribution of mobility related variables the second approach is to group the population based on between group variability of the activity patterns and then simulate for each group the first approach allows fitting flexible models e g an ensemble tree model as many population attributes such as age are in practice numerical the second approach may include more data in each population group for fitting the mobility related variables the exposure calculation model iterates over each person and then aggregates the air pollution concentrations over the route of each activity of the person with the indoor outdoor ratio accounted for it is highly parallelisable as the exposure calculation is independent for each individual note the individuals could still interact with each other and with the environment when simulating the route and the schedules another way of calculating the exposure is to iterate over each time step i e for each time step the exposure of the entire population is calculated this way can be parallelised for each time step instead of each individual for a population larger than the number of time steps this approach may provide an efficient alternative this study focuses on the new exposure modelling concept and provides a relatively simple model below we listed the limitations of the current model and our envisioned extensions we currently only generate two probability distributions from the mobility data the distribution of travel mode and the maximum travel range as a function of the origin distribution locations and the travel mode the distributions of the departure times and free time activity are empirical the model also only chooses one destination location for the entire schedule i e to and back from work in the future more commuting activities could be implemented an effective way would be to include sophisticated transportation modelling in the activity model to simulate secondary and more commuting activities trips by public transportation and more detailed road conditions such as traffic congestion with our case study we represented the annually averaged diurnal variations in exposure but not variations over a year this would be an interesting next step and it might be feasible in countries where the no 2 observations are available for each day it is possible to predict the no 2 at finer resolutions lu et al 2020b since our activity model is flexible in simulating at different temporal resolutions exposure assessment representing daily variations is possible and the precision could be improved by for example taking into account drivers of seasonal changes in behaviour conditioned by sufficient computational power and data storage a more accurate exposure assessment would likely give different exposure maps how the pattern in fig 7 would change with temporally more detailed exposure assessment is beyond the scope of this study but future studies should confirm if the conclusions derived from fig 7 stay the proposed activity modelling approach can potentially incorporate environmental socio economic or any variables influencing the travel behaviour for example the probability distribution of travel distance could be made dependent on whether the area is urban or rural in addition to the shortest distance or fastest routes the greenest route or least polluted routes could be considered the traffic simulated by the activity model could also be used to update air pollution maps w axhausen et al 2016 the indoor outdoor ratio was set to a fixed value of 0 7 it could be better estimated as a function of temperature müller et al 2021 an infiltration ratio may also be applied when taking vehicles for commuting we used euclidean distance between the departure and arrival locations for selecting the travel mode as the distance is usually shorter than the actual route the approach may be more likely to choose a lower speed traffic mode e g on foot instead of by bicycle the reason for using euclidean distance instead of the road distance is to avoid additional route querying which takes more computation time commonly the euclidean distance becomes closer to the road distance with increased travel distance it is however possible to calculate the road distance instead of the euclidean distance if the computational time is not a concern the uncertainties from statistical modelling and air pollution mapping should be quantified and discussed together with the uncertainty from the activity simulation process we used 11 iterations in our case study as it is shown in lu et al 2019 that this number of iterations is sufficient for quantifying the uncertainty in lu et al 2019 though the destination locations are the only random variable the candidate destination locations are around 500 times more compared to in our case as it included all potential working locations in utrecht and there is no travel distance constraint for this reason we believe 11 iterations are also sufficient for the case study in practice the number of iterations could be determined with the method used in lu et al 2019 the modelling concept can potentially be applied to the exposure assessment of other pollutants such as ozone and particulate matters 6 conclusion an activity simulation model that integrates statistical and agent based modelling is developed for long term large scale exposure assessment the key concept of the activity model is to estimate the probability functions of variables that determine the activity patterns mobility related variables from mobility data to parameterise the agent based model the mobility related variables are repeatedly sampled from the corresponding probability function correspondingly personal exposure is repeatedly assessed in multiple iterations to allow uncertainty quantification and improve the prediction the activity modelling concept could incorporate mobility and environmental information from different sources and makes this process independent of the agent based simulation component of the approach the model is easily extensible and applicable geographically we used the dutch national microcensus survey to demonstrate our activity simulation model the simulated activity then combines with hourly no 2 predicted from the national ground stations of germany and the netherlands to demonstrate the exposure assessment declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study is supported by the health effects institute no 4972 rfa19 1 20 6 the authors are grateful to the editors for handling this manuscript and the reviewers for their constructive comments 
