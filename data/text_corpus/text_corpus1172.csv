index,text
5860,an evaluation of published laboratory studies of flow through single rough fractures was conducted to establish a relationship between aperture 2b and critical reynolds number rec under controlled conditions for physically measured fractures of the relevant 47 published laboratory studies only six reported physically measured apertures along with adequate hydraulic testing information to determine the corresponding rec value and show that the fractures follow the cubic law reasonably well our analysis indicates a logarithmic relationship between 2b and rec for cubic law fractures in the reported aperture range 100 500 µm consistent with theoretical and modeling studies discrete fracture network models used for simulating flow and transport in fractured rock aquifers require reliable values for 2b usually obtained from the cubic law using t values from straddle packer hydraulic tests the cubic law requires specification of the number of permeable fractures n in each test interval but distinguishing the permeable from the impermeable fractures is problematic the relationship between 2b and rec determined from the laboratory tests provides a fluid mechanics basis for estimating the number of permeable fractures in straddle packer tests constant head step tests carefully done with many steps provide precise identification of the flow rate at the onset of nonlinear flow and numerous tests have shown that flow deviates from linearity at larger flow rates for larger values of transmissivity t a procedure was previously developed to relate n 2b and rec using hydraulic test data but this approach requires knowledge of the relation between 2b and rec thereby motivating this work keywords critical reynolds number forchheimer equation nonlinear flow laboratory flow test on single fractures hydraulic aperture packer testing 1 introduction discrete fracture network dfn models based on powerful numerical methods have long been available to simulate groundwater flow and contaminant transport in fractured rock such as fractran e g sudicky and mclaren 1992 fracman e g dershowitz and miller 1995 feflow e g diersch 1996 2002 and hydrogeosphere e g therrien and sudicky 1996 however these models are rarely applied to field situations because many of the input parameters are difficult to measure such as fracture aperture length and spacing simulations are strongly sensitive to these parameter inputs and in particular there is need to decrease errors and uncertainty in aperture estimation and fracture frequency to improve contaminant transport predictions hydraulic apertures 2bh derived using the cubic law with transmissivity t values from straddle packer tests in boreholes are the most practical inputs for apertures in dfn numerical models in a series of papers quinn et al 2011a 2012 2013 2015 we describe improvements in the straddle packer testing equipment design and test procedures to better understand the fluid mechanics of the water passing through the test equipment and in the fractures intersecting the tested interval the aim is to minimize the uncertainty and errors in measured t values to obtain more reliable cubic law hydraulic apertures however hydraulic apertures are also dependent on the number of permeable fractures which is most difficult to specify through observation or measurement development of a framework based on fluid mechanics for selecting the number of active fractures is the motivation for this paper the cubic law a simplified form of the navier stokes equations was introduced into groundwater investigations of fractured rock by snow 1965 smooth parallel plate fractures are assumed with all flow parallel to the fracture axis i e vz 0 snow 1968 was the first to demonstrate application of the cubic law to obtain hydraulic aperture values from hydraulic tests using straddle packers in rock boreholes 1 t ρ g 2 b h 3 12 μ n where 2bh is the average hydraulic aperture for assumed smooth parallel plate fractures l μis the absolute viscosity of water m lt ρ is the density of water m l3 and n is the number of permeable fractures in the test interval this form of the cubic law is commonly applied in rock where bedding plane fractures govern flow and are mainly orthogonal to vertical boreholes e g snow 1970 maini 1971 novakowski 1988 quinn et al 2011b all fractures are assumed identical and therefore 2bh represents an averaged value biased towards the larger fractures hydraulic apertures in both laboratory and field studies are calculated from a measured t value but they are not exactly equal to a physically measured aperture 2bm however when they are shown to be similar enough we refer to them as cubic law fractures this comparison can only be made for laboratory studies where the aperture is physically measured by appropriate methods but no suitable method exists for such field measurements from a field perspective inaccuracy in the estimation of hydraulic apertures calculated using the cubic law is derived from two sources the measured t value and the estimation of the number of permeable fractures n that intersect the borehole wall in the test interval in addition to these two main sources of error there is also error inherent in the cubic law assumption that all fractures are the same size in each interval because experience has shown that fracture sizes and their distribution vary greatly throughout boreholes in fractured rock this supports the need for a framework based on fluid mechanics to connect measured t values to individual fracture sizes to obtain insight into the number of permeable fractures present the two major assumptions inherent in most analytical models used to obtain t values from field hydraulic tests are that all flow is horizontal radial and darcian also known as linear flow where flow q is directionally proportional to applied head dh constant head ch step injection tests commonly conducted in straddle packer tests in fractured rock are best suited for validating the darcian flow assumption quinn et al 2011a b because only q and dh are measured in a field test the fundamental analysis plot for the identification of the darcian regime is q vs dh from this graph the full darcian flow range can be accurately identified i e the linear trend of the data that project through the origin many researchers have observed that the deviation from darcian flow in ch tests begins at low to moderate flow rates and show that when the darcian assumption is not met t values can be underestimated by as much as an order of magnitude e g maini 1971 ziegler 1976 mackie 1982 atkinson et al 1994 quinn et al 2011a which results in substantial error in calculated hydraulic apertures for tests conducted within the darcian flow range quinn et al 2013 showed that constant head step tests and slug tests provide similar t values usually within 10 15 which provides confidence in the t values short circuiting from the test interval through the fracture network to the open hole above and or below the packers during the test can also occur causing violation of the radial flow assumption however when the pressure is measured in the open hole above and below the tested zone quinn et al 2015 outlined a procedure to correct for this non ideality to obtain better t values using a model developed for flow in multi aquifer wells e g sokol 1963 overall advances in test equipment and procedures for straddle packer hydraulic tests result in more reliable t values however little work has been directed toward reducing the uncertainty of n to obtain more representative cubic law hydraulic apertures from these hydraulic tests quinn et al 2011b identified nonlinear flow in ch step tests conducted in a fractured dolostone and observed that the onset of nonlinear flow identified by the critical reynolds number rec is consistently larger for the high t zones than the low t zones because t is directly related to aperture for cubic law fractures they proposed that the onset of nonlinear flow may give insight into the number and size of fractures participating in the hydraulic response rec is typically calculated at the borehole wall where the velocity in the fractures is largest with rapid decline radially away from the well iwai 1976 quinn et al 2011a the reynolds number originally developed for flow through pipes is defined as 2 re ρ v d μ where v is the average velocity l t and d is a characteristic length for the flow system l for flow through non circular conduits theory shows that the hydraulic diameter 4a p where a is the cross sectional flow area and p is the wetted perimeter is the characteristic length for a non circular duct when the aspect ratio width height is large such as flow through a thin slit i e slit width slit gap the hydraulic diameter reduces to 2 the gap between the slit surfaces however bird et al 1960 and 2nd edition 2002 advise that the hydraulic diameter concept should only be used for turbulent flow through non circular flow channels for laminar flow through a slit they recommend the slit gap i e distance between surfaces for the characteristic length d this approach has been adopted by many researchers using 2bm as the characteristic length for rock fractures jones et al 1988 hakami 1989 hakami and larsson 1996 nicholl et al 1999 zimmerman et al 2004 quinn et al 2011b however rec can vary by a factor of two depending on which characteristic length used e g 2bm or 2 2bm and cause disagreement of rec values from different studies yu et al 2017 conducted a review of the critical conditions for the onset of nonlinear flow in rock fractures and reported rec values ranging from 3 8 to 340 this large range of rec for flow through fractures is not currently supported by theory as reported in the literature and therefore we must attribute this to either the fluid mechanics varying according to fracture size or to flaws in the methodologies used to determine rec more recently some researchers use hydraulic parameters such as 2bh to represent the characteristic length in re e g zhang and nemcik 2013 or blended parameters so that re ρq wμ where q is the flow rate and w is the fracture horizontal width e g javadi et al 2014 zhou et al 2015 xia et al 2017 theoretically these methods that do not use a physically measured characteristic length are only valid for cubic law fractures if this methodology is used for fractures that do not follow the cubic law it will introduce a larger range in rec values another contributor to the large range of rec derives from the use of different metrics to specify the point where rec is calculated in some studies rec is specified when t decreases by 10 whereas others calculate rec when the nonlinear head loss is 10 of the total head loss this 10 metric is too large and results in rec values that are well within the nonlinear range thereby increasing the upper rec range in our study we use 2bm as the characteristic length for re and strive to use the smallest practical nonlinear head loss 1 3 for the rec calculation to minimize overestimation of rec nevertheless the majority of rec values reported in the literature for the onset of nonlinear flow in rough fractures are 20 which implies that inertial forces do not dominate over viscous forces influences of the fracture geometry i e fracture roughness contact area and aperture variability are most likely the main causes of the onset of nonlinearity at low flow rates when inertial effects are still relatively small with a gradual transition to turbulent flow therefore accurate rec values may give insight into the geometry of fractures in tests on single fractures and may supply important information related to the number of fractures participating in a straddle packer hydraulic test as proposed by quinn et al 2011b vertical boreholes in fractured rock intersect all horizontal fractures present in the fracture network of the rock mass and some but not all of the high angled fractures core inspection and image logs are the two main methods for identifying fractures in boreholes but neither can distinguish permeable fractures from impermeable ones e g baker 1955 when rock is cored the number of visually apparent fractures in the core are identified however uncertainty arises concerning the number of fractures and their depth when core recovery is 100 which is common mechanical breaks in the core caused by the drilling add to this uncertainty examination of the fracture surface in the core may provide evidence of flow e g oxidation staining smoothing from dissolution sediment deposits mineral precipitation but this evidence forms over geologic timescales and gives no indication of present day flow borehole image logs including acoustic and optical televiewer or electrical microresistivity identify fractures at the borehole wall and provide their strike and dip but these logs also do not distinguish permeable from impermeable fractures because of the inability to identify permeable fractures in the field quinn et al 2011b proposed that the hydraulic behavior of a borehole segment isolated by packers may aid in the selection of the number of permeable fractures the similarity of the shape of q vs dh plots from straddle packer hydraulic field tests and laboratory flow tests through single fractures suggests that the fluid mechanics are similar for both field and laboratory conditions where the onset of nonlinear flow occurs at low flow rates with a gradual nonlinear transition toward turbulence however short interval straddle packer tests e g 1 2 m long commonly include more than one fracture and therefore clearer linkages must be developed between the flow mechanics in single fractures and flow through multiple fractures common in straddle packer hydraulic tests during a straddle packer ch step test conducted in the field the applied head differential measured in each test interval is reasonably assumed to be the same for all fractures intersecting the borehole between the packers therefore the same driving force pushes water from the borehole into each of the permeable fractures in the test interval and the flow through each fracture is expected to be proportional to the fracture aperture cubed the measured q and dh at the onset of nonlinear flow can be used to partition the flow through each of the fractures present assuming parallel flow though all fractures which results in nearly all injected water moving through the largest permeable fractures furthermore in radial flow away from a well the flow area increases with distance from the well and therefore the nonlinear flow regime only occurs nearest to the borehole transitioning to linear flow as the velocity decreases away from the borehole because the applied head is small in carefully conducted ch step tests and the largest fractures carry the majority of the flow it is likely that only the largest fractures intersecting the borehole in the tested zone have nonlinear flow and that this nonlinear condition subsides as the velocity diminishes away from the borehole therefore if the rec vs 2b relationship can be deduced from laboratory tests on single cubic law fractures it is reasonable to expect that this relationship is applicable to the flow through the largest fractures present in a straddle packer test interval the fact that the fractures at the borehole wall connect to other fractures further away from the borehole is generally expected to have minimal effect on the fluid mechanics in the most permeable fractures at the borehole wall because the nonlinear condition is so close to the borehole the premise that measured rec values are related to measured fracture geometry i e 2bm is best explored through laboratory flow studies through single rough fractures that offer the advantages of discerning the fracture geometry i e measured aperture distribution and controlled conditions for conducting hydraulic tests i e n 1 to provide a clear relationship between 2bm and rec this study examines in detail the published literature concerning theoretical numerical and laboratory studies of flow through single fractures to identify a quantitative relationship between rec and 2bm for cubic law fractures best supported by the literature the theoretical and numerical studies provide a conceptual basis for better understanding the causes of the onset of nonlinear flow the data from laboratory studies presented in the literature were re analyzed using consistent methodology based on stated criteria for rec determination and supported by a rigorous application of the forchheimer equation e g forchheimer 1901 the relationship between rec and the measured 2b established in this thorough assessment of laboratory results is expected to be applicable to the largest fractures present in a straddle packer field test 2 background of nonlinear flow this section provides a basis for understanding the history behind the observation of nonlinear flow and the insights that have been gained regarding the causes of the onset of nonlinear flow through fractures that include the effect of surface roughness aperture variation and contact area examples of direct and indirect evidence are presented that support the premise that 2b and rec are inter related theoretical estimations of expected maximum values of rec for flow through single fractures reported in the literature are also presented 2 1 nonlinear flow equations the most well established equations used for modeling nonlinear flow are the quadratic equation e g prony 1804 dupuit 1863 forchheimer 1901 and the power law e g smreker 1879 izbash 1931 missbach 1937 which relate the hydraulic gradient and the seepage velocity however forchheimer 1901 was the first to introduce the quadratic equation as an extension of darcy s law darcy 1856 forchheimer acknowledged the work of prony using the quadratic function for calculating river and canal velocities as well as darcy s linear flow law but he focused on the deviation from linear flow where the hydraulic conductivity decreases as the seepage velocity increases in sands of different grain size he presented the four equations shown below including a form of darcy s law the quadratic equation the power law and a cubic polynomial 3 α a v α a v b v 2 α m v n α a v b v 2 c v 3 where α is the gradient v is the darcy flux l t and a b c m n are constants forchheimer stated that a 1 k where k is the hydraulic conductivity l t therefore a is equal to the slope of the linear data projected through the origin in a plot of v vs α in the text of his paper forchheimer defines alpha α as the head loss per unit length of the line of motion in this context α must be positive and therefore a 0 using data from other researchers who collected both linear and nonlinear data in sand and gravel forchheimer stated that the quadratic equation described the behavior better than the power law but in some cases the cubic polynomial fit the data best based on forchheimer 1901 it is evident that data in the linear and nonlinear range must be collected to accurately apply a quadratic equation using a two step approach i the linear data are measured first and used as the linear constant in the quadratic equation and ii then the nonlinear constant is empirically determined to best fit the nonlinear data thereby completely separating the determination of these two constants more recently researchers have mathematically derived the forchheimer equation and confirmed its use of as an extension of darcy s law e g irmay 1958 audu et al 2018 a detailed summary table of the literature outlining the developments of the forchheimer equation for both unconsolidated porous media and fractured rockis presented by takhanov 2011 for porous media lage et al 1997 showed that the two step forchheimer equation represented their data well up to a certain flow rate but a cubic term described all of their data with better accuracy they used a three step approach to determine the three constants needed for the cubic polynomial in order to fit the data at the largest flow rates benedikt et al 2018 provides a more complete history of the evolution of all of these nonlinear equations 2 2 nonlinear flow observed in laboratory studies on single fractures laboratory studies of flow in single fractures have been conducted with a focus on many important flow processes but only rarely to determine the rec vs 2b relationship early laboratory studies of flow through single naturally rough fractures in rock showed that the transition from laminar to turbulent flow is gradual with the onset of non linearity beginning in the laminar range at low flow rates sharp 1970 maini 1971 louis 1972 atkinson 1986 this is different from flow in smooth pipes where the transition from laminar to turbulent flow is abrupt the early onset of non linear flow which is well before the onset of turbulence has been attributed to various combinations of factors related to fracture geometry including surface roughness contact area obstructions and aperture variations e g sharp 1970 maini 1971 iwai 1976 konzuk and kueper 2004 sharp 1970 observed that over the same range of gradients larger fractures showed more nonlinearity than smaller fractures i e greater deviation from linear flow maini 1971 used dye experiments to identify stagnant or dead water volumes and active flow paths in a single fracture he observed complex flow through a fracture when the hydraulic gradient was small and the largest volume moved through the largest aperture regions in the fracture e g the path of least resistance as the gradient increased the active flow field expanded into smaller aperture domains resulting in proportionately more friction acting on the bulk flow this behavior was further supported by the work of many later researchers e g neuzil and tracy 1981 pyrak nolte et al 1988 tsang 1984 tsang and tsang 1989 brown et al 1998 javadi et al 2014 pyrak nolte et al 1988 concluded that flow occurs mostly within the largest aperture domain which is expected but is governed by the restrictions imposed by the smallest aperture regions along the flow path collectively these laboratory studies illustrated why fracture geometry e g roughness contact area aperture variation causes nonlinear flow beginning when inertial forces are relatively small and well before the onset of turbulent flow more recent laboratory studies on flow through single fractures have focused on effects of surface roughness and contact area on nonlinear flow and the validity of the cubic law for flow through fractures many conducted flow tests on the same fracture exposed to increasing normal stress e g indraratna et al 1999 pyrak nolte and morris 2000 lee and cho 2002 ranjith and darlington 2007 kulatilake et al 2008 ranjith 2010 ranjith and viete 2011 develi and babadagli 2015 singh at al 2015 zhang et al 2015 chen et al 2015 zhou et al 2015 others conducted flow tests on the same fracture with increasing fracture surface offset or shearing e g lee and cho 2002 xiong et al 2011 javadi et al 2014 zou et al 2017 zhang et al 2017 xia et al 2017 a few studies created fractures in various types of rock resulting in differences in roughness to examine the effects of roughness e g radilla et al 2013 zhang and nemcik 2013 liu et al 2017 while some investigators created surfaces of different roughness using different materials or used computer generated surfaces to examine their impact on fracture flow e g zoorabadi et al 2015 tzelepis et al 2015 qian et al 2010 qian et al 2015 ni et al 2018 unfortunately the majority of these studies had to be excluded from our rec analysis for reasons outlined in section 3 however in general these studies indicated that the rec decreased as the normal stress increased and rec increased as the shear stress increased this is indirect evidence that the magnitude of rec is closely linked to the size of the aperture and the overall geometry of the fracture lee et al 2014 investigated the validity of the stokes and reynolds equations stokes 1851 reynolds 1886 common simplifications of the navier stokes equations for describing flow through an acrylic transparent replica of a single fracture with an average measured aperture of 1030 μm and a standard deviation of 411 μm videos of flow through part of the fracture at different re 0 029 2 87 8 6 17 2 are included in their supplementary material the videos show the development of an eddy as the flow rate increases forming a slow moving eddy at re 8 6 and a very fast moving eddy at re 17 2 their study also shows that the onset of nonlinear flow occurs while viscous forces are the same order of magnitude as the inertial forces and further supports the importance of fracture geometry causing nonlinear flow long before the onset of turbulence 2 3 theory and modeling of nonlinear flow much work has been done using numerical models for simulating flow through realistically shaped irregular rough fractures many of which used the data collected from real fractures to create numerical fractures e g skjetne et al 1999 skjetne and auriault 1999 lucas et al 2007 zou et al 2015 briggs et al 2017 lee et al 2014 to illustrate the dependence of a hydraulic aperture on the flow regime fig 1 shows a schematic of a vertical plane through a fracture with flow fields at different applied gradients based on these numerical results in fig 1a all flow is laminar and the active flow field blue extends throughout the fracture volume between asperities with the largest velocity in the center of the fracture outside of this active flow field water movement is minimal which is referred to as the dead water volume white area this depiction of flow through fractures exemplified in the numerical results presented by skjetne et al 1999a b at re 0 by the results from lucas et al 2007 and zou et al 2015 at re 1 by the results of briggs et al 2017 at re 10 and by the video presented by lee et al 2014 at re 0 03 is consistent with laminar flow through smooth parallel plates fig 1b shows the active flow zone encroaching into some of the previous dead water volume thereby expanding the active flow field without producing eddies i e no flow reversal this behavior is shown in the video presented by lee et al 2014 at re 3 it is unclear whether flow and pressure measurements could distinguish between 1a and 1b but flow depicted in 1b can reasonably be deemed to be laminar without appreciable error fig 1c shows that at higher re the effective width of the through going flow field decreases and the largest velocity is displaced from the centerline of the fracture also flow invades the previously dead water volume indicated by the red shading as eddies near the fracture walls in directions perpendicular or oblique to the thinner through going flow path the formation of eddies are seen in the lee et al 2014 video at re 8 6 where flow reversal begins to form a slow moving eddy that subtly decreases the effective width of the through going flow field at re 17 a fully developed fast moving eddy is formed decreasing the width of the through going flow field to a greater degree eddies are also observed in the numerical modeling by lucas et al 2007 at re 8 in zou et al 2015 at re 100 and in briggs et al 2017 at re 10 these modeling studies illustrate the fluid mechanics behind nonlinear flow through single fractures flow through a rough fracture has more pressure loss than flow through a smooth parallel plate however when the flow is linear 2bh calculated with the cubic law using linear flow data depicted in 1a 1b is similar to the measured aperture for cubic law fractures e g within 30 of 2bm for the experiments examined in this study at larger flow rates eddies form thereby creating more pressure loss as flow becomes nonlinear 2bh calculated from this nonlinear data is underestimated from 10 to 30 more than 2bh determined from linear data therefore cubic law fractures appears as if they do not follow the cubic law i e 40 60 2bm when the hydraulic aperture is calculated from nonlinear data many researchers have studied the role of contact area which is the area of total closure within a fracture on the development of nonlinear flow e g tsang and witherspoon 1983 tsang 1984 brown 1987 zimmerman and yeo 2000 berkowitz 2002 xia et al 2016 brown 1987 used numerical methods to simulate flow through rough fractures using the reynolds equation he digitally created 3 d fracture surfaces and placed the two surfaces parallel to each other and moved them closer together until part of the surfaces overlapped these overlaps were then deleted from each fracture surface so that at these locations there was zero separation e g full closure brown then defined a mechanical aperture analogous to our term for the physically measured aperture 2bm as the mean vertical separation distance between two horizontal fracture surfaces however he recognized that these surfaces are not welded together so some small flow may still occur through the areas of zero surface separations witherspoon et al 1980 also recognized this when they found they could not close a fracture completely using normal stress at the largest normal stress there was still some flow therefore brown created a conceptual term for a flow aperture more related to fluid flow than 2bm to be used for comparison to hydraulic apertures the numerical results showed that the effects of surface roughness and contact area on calculated hydraulic apertures are more substantial for smaller apertures the ratio of the hydraulic aperture to brown s theoretical flow aperture varied from 45 to greater than 95 over the range of standard surface separation defined as 2bm divided by the standard deviation 2bm σ the graphs presented by brown 1987 showed the aperture ratio increased toward 1 asymptotically as the 2bm σ ratio increased i e the hydraulic aperture becomes more similar to brown s theoretical flow aperture at larger 2bm σ ratios other researchers have indirectly validated this theory through the observation that the smallest aperture fractures often do not follow the cubic law e g tsang 1984 indraratna et al 1999 drazer and koplik 2002 develi and babadagli 2015 this deviation from the cubic law occurs due to the fracture geometry primarily when the magnitude of irregularities in aperture throughout the fracture is similar or greater than the average measured aperture this is relevant to the current study because it implies that fractures can be grouped based on whether or not they follow the cubic law and therefore we restrict our analysis to cubic law fractures it is reasonable to expect the relation between 2bm and rec to be different for cubic law fractures than for non cubic law fractures but insufficient data is available to assess this fig 2 illustrates these effects of contact area and variable aperture on nonlinear flow fig 2a and b shows flow behavior in the horizontal fracture plane with regions of fracture closure represented by randomly placed impermeable cylinders dark circles scattered throughout the fracture a common assumption for flow through hele shaw cells e g batchelor 1967 under small gradients fig 2a the flow path goes around the cylinders without causing eddies or excessive head loss as the gradient is increased eddies form behind the cylinders as shown in fig 2b causing more head loss aperture variation causes similar behavior as shown in fig 2c where the majority of flow is channeled through the larger fracture regions light color while lesser flow moves through the smaller aperture regions darker grey these figures illustrate the concepts developed from laboratory tests and numerical modeling regarding how the fracture geometry in the horizontal and vertical planes cause flow to change direction and because of this fracture geometry flow becomes nonlinear even when the inertial forces are still relatively small the role of fracture geometry causing nonlinear flow shown in figs 1 and 2 are consistent with the observations from straddle packer ch step tests in boreholes in that they show the onset of nonlinear flow at small flow rates berkowitz 2002 conducted a comprehensive literature review on flow and transport in fractured rock that provided insights from both lab and field studies he noted that in the majority of laboratory studies only the fracture surface is measured and not the aperture distribution he attributed this to the difficulty of measuring the actual aperture opening under flow test conditions he also recognized that many fractures can be visually identified in the field but it is difficult to identify those actually transmitting fluid he recommended using percolation theory to characterize fracture connectivity to obtain fracture properties that are independent of scale when fracture networks are dense but not hydraulically connected they are near but below the percolation threshold defined as the density of fractures required to allow flow through part of the network below and near this threshold power law relationships are valid however highly connected fractures are well above this threshold and the power law relationship becomes invalid this suggests that there may be different hydraulic behavior between fractures with very small apertures with limited connectivity and larger fractures with much open space between surfaces further supporting the need to use cubic law fractures for a correlation between 2b and rec 2 4 theoretical maximum rec values a few studies used mathematical theory with the simplifying assumptions inherent in the cubic law to understand the upper limit of linear flow through single fractures zimmerman and yeo 2000 used an order of magnitude analysis of the simplifications of the navier stokes equations necessary for appropriate use of the stokes and reynolds equations for flow through a single fracture they used the ratio of aperture to wavelength 2b σ for sinusoidal fractures to focus on how abruptly the aperture changes with distance they showed that as long as the 2b variations occur gradually flow remains linear even when aperture variations are large however they noted that many studies show that 2b actually varies over short distances overall for fractures in rock zimmerman and yeo propose that for flow to remain linear an upper limit of rec of about 10 15 is required oron and berkowitz 1998 examined the local cubic law lcl assumptions to illustrate the necessity of limited surface roughness to ensure that the lcl flow area is representative they developed three relationships that defined lcl conditions based on flow aperture and roughness and state that the most representative lcl aperture is an average value over a segment rather than a point measurement a local scale analysis identified the interdependence of surface roughness and aperture variation suggesting that when re 10 the linear flow assumption is no longer a good approximation for flow these theoretical maximum re values in the range of 10 15 serve as an indicator of the reasonableness of rec values obtained from the literature 3 approach and methodology 3 1 selection of laboratory studies for rec analysis to assess data from well controlled laboratory experiments of flow through single fractures we identified 47 publications as possibly providing useful results to determine the relationship between a physically measured 2b and rec for cubic law fractures however we are forced to exclude most of these 47 studies from the rec analysis for one or more of the following reasons 1 the physical aperture was not measured so that comparison to the hydraulic aperture was not possible e g rangith 2010 ju et al 2013 javadi et al 2014 zhou et al 2015 zoorabadi et al 2015 zhang et al 2015 chen et al 2015 develi and babadagli 2015 singh et al 2015 yu et al 2017 2 the flow and pressure data were not provided e g van genabeek and rothman 1999 lee and chow 2002 kulatilake et al 2008 wang et al 2015 3 only linear data were collected and all flow rates were excessively small e g nazridoust et al 2006 zheng et al 2008 4 all data were nonlinear and the linear constant could not be determined e g xiong et al 2011 zhang and nemcik 2013 radilla et al 2013 qian et al 2015 tzelepis et al 2015 huang et al 2016 xia et al 2017 zhang et al 2017 ni et al 2018 xiong et al 2018 qian et al 2018 5 the study was focused on networks rather than single fractures e g zhang et al 2007 cherubini et al 2012 liu et al 2016a liu et al 2016b li et al 2016 6 the analysis of the published data indicated non cubic law behavior as 2bh was a small fraction of the measured 2b e g qian et al 2005 ranjith and darlington 2007 qian et al 2010 7 the study involved gas flow through fractures where the viscosity density compressibility and molecular nature is different from water e g skjetne et al 1999a skjetne and auriault 1999 kløv et al 2001 rangith and viete 2011 8 the study only conducted modeling with no experimentation e g zou et al 2015 2017 liu et al 2017 key aspects of these excluded studies are summarized in the supplemental material table s1 although these papers did not provide data directly useful for our quantitative evaluation many provide innovative testing design aperture measurement techniques and some provide qualitative insights that corroborate the results presented from our analysis physically measured apertures 2bm defined by brown 1987 as the average vertical separation of the opposing fracture surfaces are a requirement for inclusion of laboratory results in our analysis to obtain the 2b vs rec relationship physical aperture measurements serve as a reference regarding the distribution of the aperture throughout the fracture being studied and the average is used to assess the calculated hydraulic apertures derived from the linear data collected in a flow test a number of studies focused solely on measuring apertures accurately e g detwiller et al 1999 renshaw et al 2000 chen et al 2000 bertels et al 2001 tatone and grasselli 2012 detwiller et al 1999 used light transmission techniques to accurately measure fracture apertures for transparent fracture surfaces e g glass plates transparent replicas however most researchers first measure the topography of both fracture surfaces individually with a mechanical laser profiler to produce digitized fracture surfaces these digital surfaces are then referenced to each other based on measurements of side and or end views of the fracture after it is placed in the flow apparatus then the aperture distribution and statistics are determined digitally others measure the fracture volume using a water to incrementally fill the fracture e g hakami 1989 to obtain volumes over different areas of the fracture or b a known volume of a gas at a measured pressure and temperature is used to pressurize a known volume of tubing and the unknown volume of the fracture while monitoring the pressure and temperature of the gas e g chen et al 2000 the unknown fracture volume is then determined using the ideal gas law and 2bm is calculated by dividing the volume by the fracture planar area although the volumetric method for measuring apertures is accurate this method has the disadvantage of not obtaining an aperture distribution of the fracture and therefore the aperture statistics remain unknown table 1 summarizes the six laboratory studies retained from the 47 papers examined that we used for determining rec based on the criteria outlined in the next section e g jones et al 1988 hakami 1989 hakami and larsson 1996 nicholl et al 1999 zimmerman et al 2004 konzuk and kueper 2004 listing the study purpose fracture and rock type the specifics of the experiment and the aperture measurement technique some of these studies conducted tests on real rock fractures in core samples of different rock types some on acrylic or epoxy replicas of rock fractures some on blocks or cores with an induced fracture tensile fracture and others on fracture analogs i e artificially roughened plates different aperture measurement techniques were used but the estimated accuracy presented for all methods are similar none of these studies as originally published were focused on determining values for rec and only two reported specific re values for their tests these studies are re examined with a consistent methodology to determine rec for the different size fractures reported in these laboratory tests where the fracture is shown to be hydraulically well behaved i e for the laboratory tests examined in this study the hydraulic apertures are within 30 of the physically measured aperture 3 2 data analysis methodology many studies in the rock mechanics literature e g al yaarubi a h b 2003 zimmerman et al 2004 did not supply the measured flow and pressure data but instead provide a semi log plot of re vs a t ratio t measured from a hydraulic test toto calculated from a physically measured aperture t to and report rec when the t ratio decreases by 10 these plots have limited accuracy due to excess scatter at lower flow rates resulting from the calculation of t at each measured q and dh dl when no other information was presented these plots were used to obtain rec using smaller changes of the t to values e g 1 5 providing a lower degree of accuracy the most basic plot for the identification of the flow regime within the fracture for laboratory tests is q vs gradient dh dl from this plot the critical flow rate at the onset of nonlinear flow is accurately identified and then used to calculate rec one of the studies selected for this analysis reported flow and pressure data in tabular form thereby making it convenient to re examine the data directly e g jones et al 1988 however for the other five studies the data had to be extracted from graphs using digitizing software to estimate the numerical values for each data point then a plot of the digitized data was superimposed on the published graph to ensure accuracy in the majority of the six laboratory studies examined in detail the pressure drop dp across the fractures was measured and when the flow is horizontal the pressure head drop dp dh because the elevation is constant in the two studies where flow was vertical and dp was measured it can also be rationalized that dp dh in the first such study e g jones et al 1988 a differential pressure transducer was used to measure the pressure drop across the sample and because the differential pressure is measured at one elevation the elevation head is included in the measurements in the second study e g hakami and larsson 1996 a constant pressure head was applied at the inlet at the base of the fracture and the flow was measured from overflow at the top of the fracture they did not describe the system in detail but there are a variety of ways the experiment could have been set up to use these two measurements to obtain a hydraulic aperture they report a hydraulic aperture of 250 μm which can only be obtained using the data they present if they measured dh therefore we make this assumption for each study data within the linear range were first selected from the q vs dh dl plot and the linear trend of these data projected through the origin was used to calculate the hydraulic aperture as outlined by witherspoon et al 1980 4 2 b h q dh dl 12 μ ρ g w 1 3 where dh dl is the gradient w is the horizontal width of the fracture sample l and q dh dl is the inverse of the slope of the linear data on a q vs dh dl plot i e the slope of this plot is dh dl q because a fracture is not a smooth parallel plate the hydraulic aperture values from flow tests on a single fracture are typically less than the physically measured average aperture because flow does not occur through the entire fracture volume and there are more frictional losses than flow between smooth parallel plates comparison of 2bh obtained from the flow test to the physically measured aperture provides the evidence necessary for assessing the validity of the cubic law for the studied fracture this is important for ensuring that all fractures are hydraulically similar when determining the 2b vs rec relationship we used the forchheimer equation applied using a two step process to describe the nonlinear behavior for the laboratory flow tests examined using the linear constant as previously identified 5 dh d l a q b q 2 where dh dl is the gradient as defined by forchheimer 1901 a is the slope on a q vs dh dl plot equal to 1 k flow area t l3 and b is empirically derived t2 l6 we used equation 5 for each dataset that included both linear and nonlinear data the linear coefficient is determined as the slope of the linear data through the origin on a q vs dh dl plot during the calculation of 2bh and this value is set as a in equation 5 then the nonlinear constant b is varied so that the equation accurately fits the nonlinear data the nonlinear constant b can easily be obtained from clean data sets with a least squares analysis on only the nonlinear data using equation 5 rearranged to solve for bq2 however many of the data sets in this examination are less than ideal so we use the best data sets e g nicholl et al 1999 konzuk and kueper 2004 as a guide when determining the nonlinear constant for less ideal data sets once the forchheimer equation is established the largest flow rate in the linear regime qc estimated as the flow rate at the point where the forchheimer curve deviates from the linear trend line is used to calculate rec this sometimes requires calculation of dh dl at a larger or smaller flow rate using the linear trend in order to quantitatively verify the onset of nonlinear flow the percent deviation from linear flow at this flow rate was calculated as outlined by others e g zhou et al 2015 wang et al 2016 xiong et al 2018 using the two step forchheimer equation constants 6 d e v i a t i o n b q 2 aq b q 2 the deviation from linear flow is essentially the ratio of the nonlinear head losses to the total head loss ideally at the onset of nonlinear flow the deviation would be 1 or less however because the data used in this study varied in quality we increased this range up to 3 for comparison to the forchheimer equation fe outlined above an arbitrary quadratic equation aqe forced through the origin is also fit to the data and instead of using the measured linear data the constants in the aqe are used to represent the linear and nonlinear constants in eq 6 4 results and discussion we present three examples of data analysis from these studies to illustrate the procedure for identifying the linear data and determining rec the forchheimer equation fe is a powerful expression for describing nonlinear flow when the linear constant is measured the two step process used to establish the fe effectively removes all influence of nonlinear flow from the linear constant and allows precise analysis of clean data commonly reported in the literature e g nicholl et al 1999 ranjith and darlington 2007 radilla et al 2013 develi and babadagli 2015 zhou et al 2015 zhang et al 2017 xia et al 2017 xiong et al 2018 and also allows consistent analysis of less ideal data sets e g hakami 1989 konzuk and kueper 2004 xiong et al 2011 zhang and nemcik 2013 zoorabadi et al 2015 zhang et al 2015 when the linear data is measured in these examples we compare the results using the two step fe with an aqe where the constants in the aqe are used for calculating the deviation from linear flow using eq 6 and the largest linear flow rate fig 3 shows an example of the analysis of a clean data set from nicholl et al 1999 on a fracture analog made from two textured glass plates 2bm 226 μm the filled circles identify the linear data range with an accurate slope solid red line selected from the entire data set open and filled circles 2bh calculated using the linear data is 199 μm 12 less than the physically measured 2bm indicating that this is a cubic law fracture using the measured linear constant in the two step fe the nonlinear constant is varied to best fit the nonlinear data at larger flow rates in this case the aqe results in an equation with a slightly smaller linear constant green dotted line and a nonlinear constant twice as large as that given in the two step fe the fit of the aqe and the fe are indistinguishable black dashed at the larger flow rates but the largest linear flow rate of the aqe 2 7x10 7 m3 s is much less than the largest measured flow rate of the linear data 5 3x10 7 m3 s used in the fe therefore using the largest measured linear flow rate in eq 6 the aqe constants result in a 2 5 deviation from linear flow while the fe constants calculate a 1 deviation this example of a very clean data set illustrates the importance of measuring the linear data when determining rec rec values calculated at 1 deviation from linear flow are very different i e 3 9 for the fe and 2 0 for the aqe fig 4 shows an example of the data analysis of a slightly less ideal data set from konzuk and kueper 2004 on a tensile fracture created in dolostone 417 μm using the same format and procedures as explained for fig 3 the linear data are used to determine 2bh 307 mm 26 less than the physically measured 2bm indicating that this is also a cubic law fracture the aqe fit to all of the data results in an equation with a slightly larger linear constant green dashed line and a smaller nonlinear constant than the constants in the two step fe the fit of the aqe and the fe are indistinguishable black dashed at larger flow rates but the aqe becomes linear at a higher flow rate than the full extent of the measured linear data resulting in a different rec value calculated at the same 2 1 deviation from linear flow e g rec 6 8 for fe and 7 8 for aqe in these examples the difference between the linear constants is trivial but it serves to show that the arbitrary application of a quadratic equation can underestimate fig 3 and overestimate fig 4 the extent of the linear data causing error in the rec determination fig 5 shows an example of the data analysis of a less than ideal data set from hakami 1989 on an epoxy cast of a natural fracture 313 μm in leptite a fine grained metamorphic rock using the same formatting as the previous fig 2bh calculated from the liner data is 236 μm 25 less than the physically measured 2bm showing this to be a cubic law fracture using the measured linear constant in the fe the nonlinear constant is varied to best describe the deviation from linear flow using the behavior of the more ideal data as a guide which indicates that the largest flow data point is aberrant an aqe fit to all of the data results in an equation with a 20 smaller linear constant green dotted line and a nonlinear constant over an order of magnitude larger than the fe nonlinear constant the fit of the aqe grey dashed and the two step fe black dashed are very different the aqe underestimates the gradient at low flow rates and the linear constant does not represent the measured linear data in contrast the fe behavior is more similar to the fe for the cleaner data sets shown in figs 3 and 4 in this case at the largest linear flow rate the aqe calculates 27 deviation from linear flow while the fe calculates 2 the aqe becomes linear at a much lower flow rate than the fe resulting in very different rec values calculated at 2 deviation e g rec 5 7 for fe and 2 for aqe this example of a less than ideal data set clearly shows the value of using the two step fe to control the effects of aberrant data points and determine rec more consistently when grounding the fe to the linear data both linear and nonlinear data were collected in each of these examples and the linear constant in an arbitrary quadratic equation fit to all of the data was consistently in error regardless of the quality of the dataset and this error is shown to carry over into rec determination all data used in this study were analyzed in a similar manner and examples are shown in the supplemental material for the other tests examined examples of data analysis providing the identified linear data the critical flow rate used to obtain the rec values and comparison of the aqe and fe fits throughout this evaluation we also consistently observed that for tests using excessively large head differentials fracture dilation effects also introduces errors in an aqe resulting in negative linear constants and underestimating head at lower flow rates when only nonlinear data are collected this error is further compounded because of the end effects of the largest and smallest data points examples of these non idealities are shown in the supporting information arbitrary application of quadratic equation tables s2 and s3 table 2 list the results of the rec analysis of the eleven flow tests from six laboratory studies where rec was determined with a high degree of confidence this table compares the physically measured 2bm with the calculated 2bh and lists the 2b σ values to assess the expected accuracy of 2bh the rec values determined for each study are also listed and the nonlinearity at qc is used as an indicator of consistency of the flow regime at the rec flow rate the quality of the rec value is considered excellent only when more than one data point was measured in the linear regime and nonlinear data were also measured when no nonlinear data were collected there is more uncertainty in the rec value i e the value may be biased low in general the expected accuracy of 2bh based on the standard surface separation outlined by brown 1987 shows that the accuracy of the calculated hydraulic aperture generally improves as 2b σ increases the deviation from linear flow at the critical flow rate obtained using the two step forchheimer constants is 1 2 for all cases where linear data were collected indicating consistent flow conditions at the onset of nonlinear flow and the calculated rec values the only data set that deviated at 3 did not have linear data and therefore has less certainty fig 6 shows the plot of both measured open and filled dark squares and hydraulic apertures open and filled green circles vs rec based on data from the 11 flow experiments conducted in the six selected laboratory studies with fractures ranging from 100 to 500 μm the open symbols identify the three tests with fair results the rec values for this aperture range varies from 1 to about 8 which is lower than the theoretical upper limit i e 10 15 obtained from the literature as discussed previously all of the fractures used in fig 6 are shown to be cubic law fractures using the linear data i e all were within 30 of the average physically measured value which indirectly indicates laminar flow conditions and the overall assembled results show a logarithmic relationship between rec and 2b the three tests that resulted in fair rec values are marked in red to show that they still fit well with the other data and if they were removed it would not change the overall logarithmic relationship the range of t values calculated from 2bm for these tests was in a narrow range from 1 7 10 6 to 8 2 10 5 m2 s however most field equipment for straddle packer testing in fractured rock has a measurement range for t from 10 9 to 10 3 m2 s indicating the need for more laboratory tests over a larger range of t at both ends of the t spectrum to further evaluate the relevant 2b vs rec relationship the plot presented in fig 6 is similar to the plots presented by brown 1987 based on numerical modeling where 2b σ shows a logarithmic type relationship with the ratio of hydraulic aperture to theoretical flow aperture indicating that surface roughness aperture variation is more important for small apertures which makes intuitive sense brown s work is also consistent with the results showing that the relationship of the measured aperture vs rec has better correlation than the hydraulic aperture vs rec for the laboratory studies all physical apertures were measured with similar accuracy but 2b σ varied between fractures 2b σ varied from 1 to 42 therefore the hydraulic aperture relationship shows more scatter in general the standard surface separation 2bm σ is an insightful parameter useful for assessing the potential accuracy of hydraulic apertures 4 1 implications for the full rec range the logarithmic relationship we obtained for the 100 500 μm range using the results of the eleven flow tests is clearly defined and is consistent with theory and modeling however it is unclear how far this relationship should be projected beyond the 100 500 μm range fig 7 shows the relationship between 2bm vs rec taken from fig 6 over a larger range for consideration of relationships outside of the 100 500 μm range based on the standard surface separations reported for the 100 500 μm range i e 1 5 2bm σ 4 these fractures likely have some through going open space but are still very small so that asperities project into the open space however the low end of the logarithmic relation does not project through the origin indicating that there must be a different relation for smaller fractures in this smaller aperture domain 100 μm it is unclear whether fractures will follow the cubic law because the fracture surfaces are so close together with highly constrained tortuous flow through very small openings and therefore the effects of surface roughness and aperture variations are greatest for a relation that projects through the origin 0 0 a power law is most reasonable from a fluid mechanics standpoint to connect the logarithmic relation to the origin a power law relation has been used by others for fractures 100 μm e g zhou et al 2015 at the high end of the logarithmic relation there is no fixed constraint for larger fractures however this larger fracture domain begins when fracture surfaces are still close together with asperities projecting into open space and this space must continually increase as the aperture increases until 2bm becomes large enough such that the open space in the center of the fracture where most of the flow is occurring is larger than the asperities at this point the fracture flow may begin to be approximated as flow through rough walled pipes e g moody 1944 this suggests that at some point rec likely increases proportionally greater than the 100 500 μm range according to either a power law logarithmic function or some combination thereof this reasoning can account for the extremely large rec values reported in the literature e g 340 given that the effect of surface roughness and aperture variation becomes minimal for very large fractures laboratory flow test on physically measured fractures are needed to establish the best relations for the larger and smaller fractures 5 conclusions and recommendations although much literature exists concerning laboratory studies of flow through single rough walled fractures we identified only six papers with a total of 11 flow tests that provided suitable data to produce values of 2b and rec for cubic law fractures identified by physically measuring the aperture distribution in the fracture for both physically measured and calculated hydraulic apertures the results indicate that a strong logarithmic relationship exists between 2b and rec for the range of 100 500 μm covered by the laboratory studies the flow tests were conducted in laboratory studies on fractures in different materials ranging from igneous and sedimentary rock samples epoxy acrylic replicas of fractures and artificial fractures thereby suggesting that this relationship has general applicability for the 100 500 μm range independent of rock type or fracture origin this range of apertures represents transmissivities ranging from 1 7 10 6 to 8 2 10 5 m2 s which is within the typical range of t values reported for short interval straddle packer tests at many fractured rock sites e g novakowski and lapcevic 1994 pailliet 1995 novakowski et al 2006 quinn et al 2011a b therefore for t values within this range the 2b vs rec relation presented in this paper provides a fluid mechanics basis for estimating the flow conditions based on the reynolds number i e rec of the largest permeable fractures participating in the hydraulic response of straddle packer tests conducted in rock boreholes all of the observed nonlinear flow occurs in the largest fractures at and near the borehole wall and therefore even though the fractures intersecting the borehole are connected to other fractures in the fracture network the laboratory 2b vs rec relationship is expected to be applicable as long as the t value is within this general range the range of rec values obtained from the laboratory tests is from 1 to 8 for the 100 500 μm aperture range which is at the low end of the much larger range reported in the literature 3 8 to 340 we attribute this large reported rec range partially to inconsistencies in the characteristic length used in re i e d the hydraulic radius the hydraulic diameter or the average aperture however inadequate controls in the experimental methodologies for determining rec most likely strongly contribute to this extreme rec range the data analysis methodology used in this study to develop the 2b vs rec relationship shows that both linear and nonlinear data are required to most accurately identify rec for each test the two step forchheimer equation based on the measured linear constant is more appropriate than an arbitrary quadratic equation fit to the data because it provides consistency for the determination of rec from different studies by grounding the forchheimer equation to the measured linear data if more laboratory studies are conducted with this aim the use of the measured linear data as a control on the forchheimer equation will offer a viable approach for reducing the large range of rec values reported in the literature using a consistent metric for quantitatively identifying the onset of nonlinear flow for rec calculations will also contribute to reducing the size of the rec range a 10 deviation from linear flow is unreasonably large especially for the larger fractures where small aperture changes result in large flow differences the 1 3 deviation criteria we have used is more reasonable and would lessen the over estimation of rec for the larger fractures finally our study shows that to obtain a strong correlation between measured 2b and rec close similarity in the values of 2bh with 2bm is essential for ensuring that all fractures are cubic law fractures although the 2bm vs rec relation we have presented for the 100 500 mm range is strong it is not suitable for projecting very far above or below this this range it is likely that there are at least three distinct rec vs 2b relationships for cubic law fractures based on categories of fracture size including a relationship for fractures smaller than 100 μm the relationship we have presented and another relation for fractures greater than 500 μm accurate physical aperture measurements are required for all studies on flow through single fractures in order to identify cubic law fractures this is most often done digitally where both fracture surfaces are measured and referenced to each other when set in the flow apparatus to obtain good measured aperture statistics volumetric fracture measurements could be used as a check on the 2b physical measurement and may provide a reference when determining the digital 2b distribution from physically measured fracture surfaces ideally a single natural fracture of sufficient area lxw would be mounted so that the aperture of the rock specimen is allowed to vary over a large range of apertures some of the studies listed in table s1 in the supplemental material developed equipment that could accomplish this versatility for example a recent laboratory study ni et al 2018 accomplished this using rubber gaskets of varying thickness to effectively change the separation of the fracture surfaces between 2 and 8 mm allowing a strong normal stress to be used to seal the fracture without closing it and qian et al 2018 sealed the sides of the fracture thereby allowing a device to raise the top fracture surface with a high degree of accuracy carefully conducting flow tests between the same fracture surfaces at different apertures would be the most rigorous approach for increasing the range of the rec vs 2b relationship laboratory tests over the entire range of apertures i e 10 s of microns to a few mm are needed to identify the 2b and rec relationship s for cubic law fractures to determine whether the same relationship applies to all fracture sizes or whether the relationship changes beyond the 100 500 μm range however for tests through very small fractures precise exceptionally low flow measurements will likely be needed while for tests through larger fractures very precise pressure measurements will be necessary there remain other important unanswered questions regarding flow through single fractures that need to be addressed through carefully controlled laboratory studies the six laboratory studies examined conducted flow tests on samples ranging from a few centimeters to greater than 20 cm in length and width yet it is unclear as to whether or not a representative elemental volume of a fracture exists where the fracture properties i e aperture distribution standard deviation etc no longer significantly change with larger scale other important questions include is there an optimal width and length of a tested fracture that would compare best across studies what are the physical differences between cubic law and non cubic law fractures does the difference in small scale surface roughness i e surface smoothness between a fracture replica i e epoxy acrylic etc and a natural fracture change the hydraulic behavior at larger flow rates credit authorship contribution statement p m quinn conceptualization formal analysis methodology validation visualization writing original draft writing review editing j a cherry conceptualization supervision writing review editing b l parker conceptualization funding acquisition investigation project administration resources supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank all of the researchers involved in conducting careful flow tests through single rock fractures in the laboratory their hard work was indispensable for our analysis and we anticipate that results from future tests will further our understanding of the relationship between rec and 2b all data used to support the conclusions can be obtained from the previously published papers listed in table 2 funding for this investigation was provided by a natural sciences and engineering research council of canada nserc senior industrial research chair grant no ircpj 363783 11 held by dr b l parker appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124384 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 supplementary data 3 supplementary data 4 supplementary data 5 supplementary data 6 
5860,an evaluation of published laboratory studies of flow through single rough fractures was conducted to establish a relationship between aperture 2b and critical reynolds number rec under controlled conditions for physically measured fractures of the relevant 47 published laboratory studies only six reported physically measured apertures along with adequate hydraulic testing information to determine the corresponding rec value and show that the fractures follow the cubic law reasonably well our analysis indicates a logarithmic relationship between 2b and rec for cubic law fractures in the reported aperture range 100 500 µm consistent with theoretical and modeling studies discrete fracture network models used for simulating flow and transport in fractured rock aquifers require reliable values for 2b usually obtained from the cubic law using t values from straddle packer hydraulic tests the cubic law requires specification of the number of permeable fractures n in each test interval but distinguishing the permeable from the impermeable fractures is problematic the relationship between 2b and rec determined from the laboratory tests provides a fluid mechanics basis for estimating the number of permeable fractures in straddle packer tests constant head step tests carefully done with many steps provide precise identification of the flow rate at the onset of nonlinear flow and numerous tests have shown that flow deviates from linearity at larger flow rates for larger values of transmissivity t a procedure was previously developed to relate n 2b and rec using hydraulic test data but this approach requires knowledge of the relation between 2b and rec thereby motivating this work keywords critical reynolds number forchheimer equation nonlinear flow laboratory flow test on single fractures hydraulic aperture packer testing 1 introduction discrete fracture network dfn models based on powerful numerical methods have long been available to simulate groundwater flow and contaminant transport in fractured rock such as fractran e g sudicky and mclaren 1992 fracman e g dershowitz and miller 1995 feflow e g diersch 1996 2002 and hydrogeosphere e g therrien and sudicky 1996 however these models are rarely applied to field situations because many of the input parameters are difficult to measure such as fracture aperture length and spacing simulations are strongly sensitive to these parameter inputs and in particular there is need to decrease errors and uncertainty in aperture estimation and fracture frequency to improve contaminant transport predictions hydraulic apertures 2bh derived using the cubic law with transmissivity t values from straddle packer tests in boreholes are the most practical inputs for apertures in dfn numerical models in a series of papers quinn et al 2011a 2012 2013 2015 we describe improvements in the straddle packer testing equipment design and test procedures to better understand the fluid mechanics of the water passing through the test equipment and in the fractures intersecting the tested interval the aim is to minimize the uncertainty and errors in measured t values to obtain more reliable cubic law hydraulic apertures however hydraulic apertures are also dependent on the number of permeable fractures which is most difficult to specify through observation or measurement development of a framework based on fluid mechanics for selecting the number of active fractures is the motivation for this paper the cubic law a simplified form of the navier stokes equations was introduced into groundwater investigations of fractured rock by snow 1965 smooth parallel plate fractures are assumed with all flow parallel to the fracture axis i e vz 0 snow 1968 was the first to demonstrate application of the cubic law to obtain hydraulic aperture values from hydraulic tests using straddle packers in rock boreholes 1 t ρ g 2 b h 3 12 μ n where 2bh is the average hydraulic aperture for assumed smooth parallel plate fractures l μis the absolute viscosity of water m lt ρ is the density of water m l3 and n is the number of permeable fractures in the test interval this form of the cubic law is commonly applied in rock where bedding plane fractures govern flow and are mainly orthogonal to vertical boreholes e g snow 1970 maini 1971 novakowski 1988 quinn et al 2011b all fractures are assumed identical and therefore 2bh represents an averaged value biased towards the larger fractures hydraulic apertures in both laboratory and field studies are calculated from a measured t value but they are not exactly equal to a physically measured aperture 2bm however when they are shown to be similar enough we refer to them as cubic law fractures this comparison can only be made for laboratory studies where the aperture is physically measured by appropriate methods but no suitable method exists for such field measurements from a field perspective inaccuracy in the estimation of hydraulic apertures calculated using the cubic law is derived from two sources the measured t value and the estimation of the number of permeable fractures n that intersect the borehole wall in the test interval in addition to these two main sources of error there is also error inherent in the cubic law assumption that all fractures are the same size in each interval because experience has shown that fracture sizes and their distribution vary greatly throughout boreholes in fractured rock this supports the need for a framework based on fluid mechanics to connect measured t values to individual fracture sizes to obtain insight into the number of permeable fractures present the two major assumptions inherent in most analytical models used to obtain t values from field hydraulic tests are that all flow is horizontal radial and darcian also known as linear flow where flow q is directionally proportional to applied head dh constant head ch step injection tests commonly conducted in straddle packer tests in fractured rock are best suited for validating the darcian flow assumption quinn et al 2011a b because only q and dh are measured in a field test the fundamental analysis plot for the identification of the darcian regime is q vs dh from this graph the full darcian flow range can be accurately identified i e the linear trend of the data that project through the origin many researchers have observed that the deviation from darcian flow in ch tests begins at low to moderate flow rates and show that when the darcian assumption is not met t values can be underestimated by as much as an order of magnitude e g maini 1971 ziegler 1976 mackie 1982 atkinson et al 1994 quinn et al 2011a which results in substantial error in calculated hydraulic apertures for tests conducted within the darcian flow range quinn et al 2013 showed that constant head step tests and slug tests provide similar t values usually within 10 15 which provides confidence in the t values short circuiting from the test interval through the fracture network to the open hole above and or below the packers during the test can also occur causing violation of the radial flow assumption however when the pressure is measured in the open hole above and below the tested zone quinn et al 2015 outlined a procedure to correct for this non ideality to obtain better t values using a model developed for flow in multi aquifer wells e g sokol 1963 overall advances in test equipment and procedures for straddle packer hydraulic tests result in more reliable t values however little work has been directed toward reducing the uncertainty of n to obtain more representative cubic law hydraulic apertures from these hydraulic tests quinn et al 2011b identified nonlinear flow in ch step tests conducted in a fractured dolostone and observed that the onset of nonlinear flow identified by the critical reynolds number rec is consistently larger for the high t zones than the low t zones because t is directly related to aperture for cubic law fractures they proposed that the onset of nonlinear flow may give insight into the number and size of fractures participating in the hydraulic response rec is typically calculated at the borehole wall where the velocity in the fractures is largest with rapid decline radially away from the well iwai 1976 quinn et al 2011a the reynolds number originally developed for flow through pipes is defined as 2 re ρ v d μ where v is the average velocity l t and d is a characteristic length for the flow system l for flow through non circular conduits theory shows that the hydraulic diameter 4a p where a is the cross sectional flow area and p is the wetted perimeter is the characteristic length for a non circular duct when the aspect ratio width height is large such as flow through a thin slit i e slit width slit gap the hydraulic diameter reduces to 2 the gap between the slit surfaces however bird et al 1960 and 2nd edition 2002 advise that the hydraulic diameter concept should only be used for turbulent flow through non circular flow channels for laminar flow through a slit they recommend the slit gap i e distance between surfaces for the characteristic length d this approach has been adopted by many researchers using 2bm as the characteristic length for rock fractures jones et al 1988 hakami 1989 hakami and larsson 1996 nicholl et al 1999 zimmerman et al 2004 quinn et al 2011b however rec can vary by a factor of two depending on which characteristic length used e g 2bm or 2 2bm and cause disagreement of rec values from different studies yu et al 2017 conducted a review of the critical conditions for the onset of nonlinear flow in rock fractures and reported rec values ranging from 3 8 to 340 this large range of rec for flow through fractures is not currently supported by theory as reported in the literature and therefore we must attribute this to either the fluid mechanics varying according to fracture size or to flaws in the methodologies used to determine rec more recently some researchers use hydraulic parameters such as 2bh to represent the characteristic length in re e g zhang and nemcik 2013 or blended parameters so that re ρq wμ where q is the flow rate and w is the fracture horizontal width e g javadi et al 2014 zhou et al 2015 xia et al 2017 theoretically these methods that do not use a physically measured characteristic length are only valid for cubic law fractures if this methodology is used for fractures that do not follow the cubic law it will introduce a larger range in rec values another contributor to the large range of rec derives from the use of different metrics to specify the point where rec is calculated in some studies rec is specified when t decreases by 10 whereas others calculate rec when the nonlinear head loss is 10 of the total head loss this 10 metric is too large and results in rec values that are well within the nonlinear range thereby increasing the upper rec range in our study we use 2bm as the characteristic length for re and strive to use the smallest practical nonlinear head loss 1 3 for the rec calculation to minimize overestimation of rec nevertheless the majority of rec values reported in the literature for the onset of nonlinear flow in rough fractures are 20 which implies that inertial forces do not dominate over viscous forces influences of the fracture geometry i e fracture roughness contact area and aperture variability are most likely the main causes of the onset of nonlinearity at low flow rates when inertial effects are still relatively small with a gradual transition to turbulent flow therefore accurate rec values may give insight into the geometry of fractures in tests on single fractures and may supply important information related to the number of fractures participating in a straddle packer hydraulic test as proposed by quinn et al 2011b vertical boreholes in fractured rock intersect all horizontal fractures present in the fracture network of the rock mass and some but not all of the high angled fractures core inspection and image logs are the two main methods for identifying fractures in boreholes but neither can distinguish permeable fractures from impermeable ones e g baker 1955 when rock is cored the number of visually apparent fractures in the core are identified however uncertainty arises concerning the number of fractures and their depth when core recovery is 100 which is common mechanical breaks in the core caused by the drilling add to this uncertainty examination of the fracture surface in the core may provide evidence of flow e g oxidation staining smoothing from dissolution sediment deposits mineral precipitation but this evidence forms over geologic timescales and gives no indication of present day flow borehole image logs including acoustic and optical televiewer or electrical microresistivity identify fractures at the borehole wall and provide their strike and dip but these logs also do not distinguish permeable from impermeable fractures because of the inability to identify permeable fractures in the field quinn et al 2011b proposed that the hydraulic behavior of a borehole segment isolated by packers may aid in the selection of the number of permeable fractures the similarity of the shape of q vs dh plots from straddle packer hydraulic field tests and laboratory flow tests through single fractures suggests that the fluid mechanics are similar for both field and laboratory conditions where the onset of nonlinear flow occurs at low flow rates with a gradual nonlinear transition toward turbulence however short interval straddle packer tests e g 1 2 m long commonly include more than one fracture and therefore clearer linkages must be developed between the flow mechanics in single fractures and flow through multiple fractures common in straddle packer hydraulic tests during a straddle packer ch step test conducted in the field the applied head differential measured in each test interval is reasonably assumed to be the same for all fractures intersecting the borehole between the packers therefore the same driving force pushes water from the borehole into each of the permeable fractures in the test interval and the flow through each fracture is expected to be proportional to the fracture aperture cubed the measured q and dh at the onset of nonlinear flow can be used to partition the flow through each of the fractures present assuming parallel flow though all fractures which results in nearly all injected water moving through the largest permeable fractures furthermore in radial flow away from a well the flow area increases with distance from the well and therefore the nonlinear flow regime only occurs nearest to the borehole transitioning to linear flow as the velocity decreases away from the borehole because the applied head is small in carefully conducted ch step tests and the largest fractures carry the majority of the flow it is likely that only the largest fractures intersecting the borehole in the tested zone have nonlinear flow and that this nonlinear condition subsides as the velocity diminishes away from the borehole therefore if the rec vs 2b relationship can be deduced from laboratory tests on single cubic law fractures it is reasonable to expect that this relationship is applicable to the flow through the largest fractures present in a straddle packer test interval the fact that the fractures at the borehole wall connect to other fractures further away from the borehole is generally expected to have minimal effect on the fluid mechanics in the most permeable fractures at the borehole wall because the nonlinear condition is so close to the borehole the premise that measured rec values are related to measured fracture geometry i e 2bm is best explored through laboratory flow studies through single rough fractures that offer the advantages of discerning the fracture geometry i e measured aperture distribution and controlled conditions for conducting hydraulic tests i e n 1 to provide a clear relationship between 2bm and rec this study examines in detail the published literature concerning theoretical numerical and laboratory studies of flow through single fractures to identify a quantitative relationship between rec and 2bm for cubic law fractures best supported by the literature the theoretical and numerical studies provide a conceptual basis for better understanding the causes of the onset of nonlinear flow the data from laboratory studies presented in the literature were re analyzed using consistent methodology based on stated criteria for rec determination and supported by a rigorous application of the forchheimer equation e g forchheimer 1901 the relationship between rec and the measured 2b established in this thorough assessment of laboratory results is expected to be applicable to the largest fractures present in a straddle packer field test 2 background of nonlinear flow this section provides a basis for understanding the history behind the observation of nonlinear flow and the insights that have been gained regarding the causes of the onset of nonlinear flow through fractures that include the effect of surface roughness aperture variation and contact area examples of direct and indirect evidence are presented that support the premise that 2b and rec are inter related theoretical estimations of expected maximum values of rec for flow through single fractures reported in the literature are also presented 2 1 nonlinear flow equations the most well established equations used for modeling nonlinear flow are the quadratic equation e g prony 1804 dupuit 1863 forchheimer 1901 and the power law e g smreker 1879 izbash 1931 missbach 1937 which relate the hydraulic gradient and the seepage velocity however forchheimer 1901 was the first to introduce the quadratic equation as an extension of darcy s law darcy 1856 forchheimer acknowledged the work of prony using the quadratic function for calculating river and canal velocities as well as darcy s linear flow law but he focused on the deviation from linear flow where the hydraulic conductivity decreases as the seepage velocity increases in sands of different grain size he presented the four equations shown below including a form of darcy s law the quadratic equation the power law and a cubic polynomial 3 α a v α a v b v 2 α m v n α a v b v 2 c v 3 where α is the gradient v is the darcy flux l t and a b c m n are constants forchheimer stated that a 1 k where k is the hydraulic conductivity l t therefore a is equal to the slope of the linear data projected through the origin in a plot of v vs α in the text of his paper forchheimer defines alpha α as the head loss per unit length of the line of motion in this context α must be positive and therefore a 0 using data from other researchers who collected both linear and nonlinear data in sand and gravel forchheimer stated that the quadratic equation described the behavior better than the power law but in some cases the cubic polynomial fit the data best based on forchheimer 1901 it is evident that data in the linear and nonlinear range must be collected to accurately apply a quadratic equation using a two step approach i the linear data are measured first and used as the linear constant in the quadratic equation and ii then the nonlinear constant is empirically determined to best fit the nonlinear data thereby completely separating the determination of these two constants more recently researchers have mathematically derived the forchheimer equation and confirmed its use of as an extension of darcy s law e g irmay 1958 audu et al 2018 a detailed summary table of the literature outlining the developments of the forchheimer equation for both unconsolidated porous media and fractured rockis presented by takhanov 2011 for porous media lage et al 1997 showed that the two step forchheimer equation represented their data well up to a certain flow rate but a cubic term described all of their data with better accuracy they used a three step approach to determine the three constants needed for the cubic polynomial in order to fit the data at the largest flow rates benedikt et al 2018 provides a more complete history of the evolution of all of these nonlinear equations 2 2 nonlinear flow observed in laboratory studies on single fractures laboratory studies of flow in single fractures have been conducted with a focus on many important flow processes but only rarely to determine the rec vs 2b relationship early laboratory studies of flow through single naturally rough fractures in rock showed that the transition from laminar to turbulent flow is gradual with the onset of non linearity beginning in the laminar range at low flow rates sharp 1970 maini 1971 louis 1972 atkinson 1986 this is different from flow in smooth pipes where the transition from laminar to turbulent flow is abrupt the early onset of non linear flow which is well before the onset of turbulence has been attributed to various combinations of factors related to fracture geometry including surface roughness contact area obstructions and aperture variations e g sharp 1970 maini 1971 iwai 1976 konzuk and kueper 2004 sharp 1970 observed that over the same range of gradients larger fractures showed more nonlinearity than smaller fractures i e greater deviation from linear flow maini 1971 used dye experiments to identify stagnant or dead water volumes and active flow paths in a single fracture he observed complex flow through a fracture when the hydraulic gradient was small and the largest volume moved through the largest aperture regions in the fracture e g the path of least resistance as the gradient increased the active flow field expanded into smaller aperture domains resulting in proportionately more friction acting on the bulk flow this behavior was further supported by the work of many later researchers e g neuzil and tracy 1981 pyrak nolte et al 1988 tsang 1984 tsang and tsang 1989 brown et al 1998 javadi et al 2014 pyrak nolte et al 1988 concluded that flow occurs mostly within the largest aperture domain which is expected but is governed by the restrictions imposed by the smallest aperture regions along the flow path collectively these laboratory studies illustrated why fracture geometry e g roughness contact area aperture variation causes nonlinear flow beginning when inertial forces are relatively small and well before the onset of turbulent flow more recent laboratory studies on flow through single fractures have focused on effects of surface roughness and contact area on nonlinear flow and the validity of the cubic law for flow through fractures many conducted flow tests on the same fracture exposed to increasing normal stress e g indraratna et al 1999 pyrak nolte and morris 2000 lee and cho 2002 ranjith and darlington 2007 kulatilake et al 2008 ranjith 2010 ranjith and viete 2011 develi and babadagli 2015 singh at al 2015 zhang et al 2015 chen et al 2015 zhou et al 2015 others conducted flow tests on the same fracture with increasing fracture surface offset or shearing e g lee and cho 2002 xiong et al 2011 javadi et al 2014 zou et al 2017 zhang et al 2017 xia et al 2017 a few studies created fractures in various types of rock resulting in differences in roughness to examine the effects of roughness e g radilla et al 2013 zhang and nemcik 2013 liu et al 2017 while some investigators created surfaces of different roughness using different materials or used computer generated surfaces to examine their impact on fracture flow e g zoorabadi et al 2015 tzelepis et al 2015 qian et al 2010 qian et al 2015 ni et al 2018 unfortunately the majority of these studies had to be excluded from our rec analysis for reasons outlined in section 3 however in general these studies indicated that the rec decreased as the normal stress increased and rec increased as the shear stress increased this is indirect evidence that the magnitude of rec is closely linked to the size of the aperture and the overall geometry of the fracture lee et al 2014 investigated the validity of the stokes and reynolds equations stokes 1851 reynolds 1886 common simplifications of the navier stokes equations for describing flow through an acrylic transparent replica of a single fracture with an average measured aperture of 1030 μm and a standard deviation of 411 μm videos of flow through part of the fracture at different re 0 029 2 87 8 6 17 2 are included in their supplementary material the videos show the development of an eddy as the flow rate increases forming a slow moving eddy at re 8 6 and a very fast moving eddy at re 17 2 their study also shows that the onset of nonlinear flow occurs while viscous forces are the same order of magnitude as the inertial forces and further supports the importance of fracture geometry causing nonlinear flow long before the onset of turbulence 2 3 theory and modeling of nonlinear flow much work has been done using numerical models for simulating flow through realistically shaped irregular rough fractures many of which used the data collected from real fractures to create numerical fractures e g skjetne et al 1999 skjetne and auriault 1999 lucas et al 2007 zou et al 2015 briggs et al 2017 lee et al 2014 to illustrate the dependence of a hydraulic aperture on the flow regime fig 1 shows a schematic of a vertical plane through a fracture with flow fields at different applied gradients based on these numerical results in fig 1a all flow is laminar and the active flow field blue extends throughout the fracture volume between asperities with the largest velocity in the center of the fracture outside of this active flow field water movement is minimal which is referred to as the dead water volume white area this depiction of flow through fractures exemplified in the numerical results presented by skjetne et al 1999a b at re 0 by the results from lucas et al 2007 and zou et al 2015 at re 1 by the results of briggs et al 2017 at re 10 and by the video presented by lee et al 2014 at re 0 03 is consistent with laminar flow through smooth parallel plates fig 1b shows the active flow zone encroaching into some of the previous dead water volume thereby expanding the active flow field without producing eddies i e no flow reversal this behavior is shown in the video presented by lee et al 2014 at re 3 it is unclear whether flow and pressure measurements could distinguish between 1a and 1b but flow depicted in 1b can reasonably be deemed to be laminar without appreciable error fig 1c shows that at higher re the effective width of the through going flow field decreases and the largest velocity is displaced from the centerline of the fracture also flow invades the previously dead water volume indicated by the red shading as eddies near the fracture walls in directions perpendicular or oblique to the thinner through going flow path the formation of eddies are seen in the lee et al 2014 video at re 8 6 where flow reversal begins to form a slow moving eddy that subtly decreases the effective width of the through going flow field at re 17 a fully developed fast moving eddy is formed decreasing the width of the through going flow field to a greater degree eddies are also observed in the numerical modeling by lucas et al 2007 at re 8 in zou et al 2015 at re 100 and in briggs et al 2017 at re 10 these modeling studies illustrate the fluid mechanics behind nonlinear flow through single fractures flow through a rough fracture has more pressure loss than flow through a smooth parallel plate however when the flow is linear 2bh calculated with the cubic law using linear flow data depicted in 1a 1b is similar to the measured aperture for cubic law fractures e g within 30 of 2bm for the experiments examined in this study at larger flow rates eddies form thereby creating more pressure loss as flow becomes nonlinear 2bh calculated from this nonlinear data is underestimated from 10 to 30 more than 2bh determined from linear data therefore cubic law fractures appears as if they do not follow the cubic law i e 40 60 2bm when the hydraulic aperture is calculated from nonlinear data many researchers have studied the role of contact area which is the area of total closure within a fracture on the development of nonlinear flow e g tsang and witherspoon 1983 tsang 1984 brown 1987 zimmerman and yeo 2000 berkowitz 2002 xia et al 2016 brown 1987 used numerical methods to simulate flow through rough fractures using the reynolds equation he digitally created 3 d fracture surfaces and placed the two surfaces parallel to each other and moved them closer together until part of the surfaces overlapped these overlaps were then deleted from each fracture surface so that at these locations there was zero separation e g full closure brown then defined a mechanical aperture analogous to our term for the physically measured aperture 2bm as the mean vertical separation distance between two horizontal fracture surfaces however he recognized that these surfaces are not welded together so some small flow may still occur through the areas of zero surface separations witherspoon et al 1980 also recognized this when they found they could not close a fracture completely using normal stress at the largest normal stress there was still some flow therefore brown created a conceptual term for a flow aperture more related to fluid flow than 2bm to be used for comparison to hydraulic apertures the numerical results showed that the effects of surface roughness and contact area on calculated hydraulic apertures are more substantial for smaller apertures the ratio of the hydraulic aperture to brown s theoretical flow aperture varied from 45 to greater than 95 over the range of standard surface separation defined as 2bm divided by the standard deviation 2bm σ the graphs presented by brown 1987 showed the aperture ratio increased toward 1 asymptotically as the 2bm σ ratio increased i e the hydraulic aperture becomes more similar to brown s theoretical flow aperture at larger 2bm σ ratios other researchers have indirectly validated this theory through the observation that the smallest aperture fractures often do not follow the cubic law e g tsang 1984 indraratna et al 1999 drazer and koplik 2002 develi and babadagli 2015 this deviation from the cubic law occurs due to the fracture geometry primarily when the magnitude of irregularities in aperture throughout the fracture is similar or greater than the average measured aperture this is relevant to the current study because it implies that fractures can be grouped based on whether or not they follow the cubic law and therefore we restrict our analysis to cubic law fractures it is reasonable to expect the relation between 2bm and rec to be different for cubic law fractures than for non cubic law fractures but insufficient data is available to assess this fig 2 illustrates these effects of contact area and variable aperture on nonlinear flow fig 2a and b shows flow behavior in the horizontal fracture plane with regions of fracture closure represented by randomly placed impermeable cylinders dark circles scattered throughout the fracture a common assumption for flow through hele shaw cells e g batchelor 1967 under small gradients fig 2a the flow path goes around the cylinders without causing eddies or excessive head loss as the gradient is increased eddies form behind the cylinders as shown in fig 2b causing more head loss aperture variation causes similar behavior as shown in fig 2c where the majority of flow is channeled through the larger fracture regions light color while lesser flow moves through the smaller aperture regions darker grey these figures illustrate the concepts developed from laboratory tests and numerical modeling regarding how the fracture geometry in the horizontal and vertical planes cause flow to change direction and because of this fracture geometry flow becomes nonlinear even when the inertial forces are still relatively small the role of fracture geometry causing nonlinear flow shown in figs 1 and 2 are consistent with the observations from straddle packer ch step tests in boreholes in that they show the onset of nonlinear flow at small flow rates berkowitz 2002 conducted a comprehensive literature review on flow and transport in fractured rock that provided insights from both lab and field studies he noted that in the majority of laboratory studies only the fracture surface is measured and not the aperture distribution he attributed this to the difficulty of measuring the actual aperture opening under flow test conditions he also recognized that many fractures can be visually identified in the field but it is difficult to identify those actually transmitting fluid he recommended using percolation theory to characterize fracture connectivity to obtain fracture properties that are independent of scale when fracture networks are dense but not hydraulically connected they are near but below the percolation threshold defined as the density of fractures required to allow flow through part of the network below and near this threshold power law relationships are valid however highly connected fractures are well above this threshold and the power law relationship becomes invalid this suggests that there may be different hydraulic behavior between fractures with very small apertures with limited connectivity and larger fractures with much open space between surfaces further supporting the need to use cubic law fractures for a correlation between 2b and rec 2 4 theoretical maximum rec values a few studies used mathematical theory with the simplifying assumptions inherent in the cubic law to understand the upper limit of linear flow through single fractures zimmerman and yeo 2000 used an order of magnitude analysis of the simplifications of the navier stokes equations necessary for appropriate use of the stokes and reynolds equations for flow through a single fracture they used the ratio of aperture to wavelength 2b σ for sinusoidal fractures to focus on how abruptly the aperture changes with distance they showed that as long as the 2b variations occur gradually flow remains linear even when aperture variations are large however they noted that many studies show that 2b actually varies over short distances overall for fractures in rock zimmerman and yeo propose that for flow to remain linear an upper limit of rec of about 10 15 is required oron and berkowitz 1998 examined the local cubic law lcl assumptions to illustrate the necessity of limited surface roughness to ensure that the lcl flow area is representative they developed three relationships that defined lcl conditions based on flow aperture and roughness and state that the most representative lcl aperture is an average value over a segment rather than a point measurement a local scale analysis identified the interdependence of surface roughness and aperture variation suggesting that when re 10 the linear flow assumption is no longer a good approximation for flow these theoretical maximum re values in the range of 10 15 serve as an indicator of the reasonableness of rec values obtained from the literature 3 approach and methodology 3 1 selection of laboratory studies for rec analysis to assess data from well controlled laboratory experiments of flow through single fractures we identified 47 publications as possibly providing useful results to determine the relationship between a physically measured 2b and rec for cubic law fractures however we are forced to exclude most of these 47 studies from the rec analysis for one or more of the following reasons 1 the physical aperture was not measured so that comparison to the hydraulic aperture was not possible e g rangith 2010 ju et al 2013 javadi et al 2014 zhou et al 2015 zoorabadi et al 2015 zhang et al 2015 chen et al 2015 develi and babadagli 2015 singh et al 2015 yu et al 2017 2 the flow and pressure data were not provided e g van genabeek and rothman 1999 lee and chow 2002 kulatilake et al 2008 wang et al 2015 3 only linear data were collected and all flow rates were excessively small e g nazridoust et al 2006 zheng et al 2008 4 all data were nonlinear and the linear constant could not be determined e g xiong et al 2011 zhang and nemcik 2013 radilla et al 2013 qian et al 2015 tzelepis et al 2015 huang et al 2016 xia et al 2017 zhang et al 2017 ni et al 2018 xiong et al 2018 qian et al 2018 5 the study was focused on networks rather than single fractures e g zhang et al 2007 cherubini et al 2012 liu et al 2016a liu et al 2016b li et al 2016 6 the analysis of the published data indicated non cubic law behavior as 2bh was a small fraction of the measured 2b e g qian et al 2005 ranjith and darlington 2007 qian et al 2010 7 the study involved gas flow through fractures where the viscosity density compressibility and molecular nature is different from water e g skjetne et al 1999a skjetne and auriault 1999 kløv et al 2001 rangith and viete 2011 8 the study only conducted modeling with no experimentation e g zou et al 2015 2017 liu et al 2017 key aspects of these excluded studies are summarized in the supplemental material table s1 although these papers did not provide data directly useful for our quantitative evaluation many provide innovative testing design aperture measurement techniques and some provide qualitative insights that corroborate the results presented from our analysis physically measured apertures 2bm defined by brown 1987 as the average vertical separation of the opposing fracture surfaces are a requirement for inclusion of laboratory results in our analysis to obtain the 2b vs rec relationship physical aperture measurements serve as a reference regarding the distribution of the aperture throughout the fracture being studied and the average is used to assess the calculated hydraulic apertures derived from the linear data collected in a flow test a number of studies focused solely on measuring apertures accurately e g detwiller et al 1999 renshaw et al 2000 chen et al 2000 bertels et al 2001 tatone and grasselli 2012 detwiller et al 1999 used light transmission techniques to accurately measure fracture apertures for transparent fracture surfaces e g glass plates transparent replicas however most researchers first measure the topography of both fracture surfaces individually with a mechanical laser profiler to produce digitized fracture surfaces these digital surfaces are then referenced to each other based on measurements of side and or end views of the fracture after it is placed in the flow apparatus then the aperture distribution and statistics are determined digitally others measure the fracture volume using a water to incrementally fill the fracture e g hakami 1989 to obtain volumes over different areas of the fracture or b a known volume of a gas at a measured pressure and temperature is used to pressurize a known volume of tubing and the unknown volume of the fracture while monitoring the pressure and temperature of the gas e g chen et al 2000 the unknown fracture volume is then determined using the ideal gas law and 2bm is calculated by dividing the volume by the fracture planar area although the volumetric method for measuring apertures is accurate this method has the disadvantage of not obtaining an aperture distribution of the fracture and therefore the aperture statistics remain unknown table 1 summarizes the six laboratory studies retained from the 47 papers examined that we used for determining rec based on the criteria outlined in the next section e g jones et al 1988 hakami 1989 hakami and larsson 1996 nicholl et al 1999 zimmerman et al 2004 konzuk and kueper 2004 listing the study purpose fracture and rock type the specifics of the experiment and the aperture measurement technique some of these studies conducted tests on real rock fractures in core samples of different rock types some on acrylic or epoxy replicas of rock fractures some on blocks or cores with an induced fracture tensile fracture and others on fracture analogs i e artificially roughened plates different aperture measurement techniques were used but the estimated accuracy presented for all methods are similar none of these studies as originally published were focused on determining values for rec and only two reported specific re values for their tests these studies are re examined with a consistent methodology to determine rec for the different size fractures reported in these laboratory tests where the fracture is shown to be hydraulically well behaved i e for the laboratory tests examined in this study the hydraulic apertures are within 30 of the physically measured aperture 3 2 data analysis methodology many studies in the rock mechanics literature e g al yaarubi a h b 2003 zimmerman et al 2004 did not supply the measured flow and pressure data but instead provide a semi log plot of re vs a t ratio t measured from a hydraulic test toto calculated from a physically measured aperture t to and report rec when the t ratio decreases by 10 these plots have limited accuracy due to excess scatter at lower flow rates resulting from the calculation of t at each measured q and dh dl when no other information was presented these plots were used to obtain rec using smaller changes of the t to values e g 1 5 providing a lower degree of accuracy the most basic plot for the identification of the flow regime within the fracture for laboratory tests is q vs gradient dh dl from this plot the critical flow rate at the onset of nonlinear flow is accurately identified and then used to calculate rec one of the studies selected for this analysis reported flow and pressure data in tabular form thereby making it convenient to re examine the data directly e g jones et al 1988 however for the other five studies the data had to be extracted from graphs using digitizing software to estimate the numerical values for each data point then a plot of the digitized data was superimposed on the published graph to ensure accuracy in the majority of the six laboratory studies examined in detail the pressure drop dp across the fractures was measured and when the flow is horizontal the pressure head drop dp dh because the elevation is constant in the two studies where flow was vertical and dp was measured it can also be rationalized that dp dh in the first such study e g jones et al 1988 a differential pressure transducer was used to measure the pressure drop across the sample and because the differential pressure is measured at one elevation the elevation head is included in the measurements in the second study e g hakami and larsson 1996 a constant pressure head was applied at the inlet at the base of the fracture and the flow was measured from overflow at the top of the fracture they did not describe the system in detail but there are a variety of ways the experiment could have been set up to use these two measurements to obtain a hydraulic aperture they report a hydraulic aperture of 250 μm which can only be obtained using the data they present if they measured dh therefore we make this assumption for each study data within the linear range were first selected from the q vs dh dl plot and the linear trend of these data projected through the origin was used to calculate the hydraulic aperture as outlined by witherspoon et al 1980 4 2 b h q dh dl 12 μ ρ g w 1 3 where dh dl is the gradient w is the horizontal width of the fracture sample l and q dh dl is the inverse of the slope of the linear data on a q vs dh dl plot i e the slope of this plot is dh dl q because a fracture is not a smooth parallel plate the hydraulic aperture values from flow tests on a single fracture are typically less than the physically measured average aperture because flow does not occur through the entire fracture volume and there are more frictional losses than flow between smooth parallel plates comparison of 2bh obtained from the flow test to the physically measured aperture provides the evidence necessary for assessing the validity of the cubic law for the studied fracture this is important for ensuring that all fractures are hydraulically similar when determining the 2b vs rec relationship we used the forchheimer equation applied using a two step process to describe the nonlinear behavior for the laboratory flow tests examined using the linear constant as previously identified 5 dh d l a q b q 2 where dh dl is the gradient as defined by forchheimer 1901 a is the slope on a q vs dh dl plot equal to 1 k flow area t l3 and b is empirically derived t2 l6 we used equation 5 for each dataset that included both linear and nonlinear data the linear coefficient is determined as the slope of the linear data through the origin on a q vs dh dl plot during the calculation of 2bh and this value is set as a in equation 5 then the nonlinear constant b is varied so that the equation accurately fits the nonlinear data the nonlinear constant b can easily be obtained from clean data sets with a least squares analysis on only the nonlinear data using equation 5 rearranged to solve for bq2 however many of the data sets in this examination are less than ideal so we use the best data sets e g nicholl et al 1999 konzuk and kueper 2004 as a guide when determining the nonlinear constant for less ideal data sets once the forchheimer equation is established the largest flow rate in the linear regime qc estimated as the flow rate at the point where the forchheimer curve deviates from the linear trend line is used to calculate rec this sometimes requires calculation of dh dl at a larger or smaller flow rate using the linear trend in order to quantitatively verify the onset of nonlinear flow the percent deviation from linear flow at this flow rate was calculated as outlined by others e g zhou et al 2015 wang et al 2016 xiong et al 2018 using the two step forchheimer equation constants 6 d e v i a t i o n b q 2 aq b q 2 the deviation from linear flow is essentially the ratio of the nonlinear head losses to the total head loss ideally at the onset of nonlinear flow the deviation would be 1 or less however because the data used in this study varied in quality we increased this range up to 3 for comparison to the forchheimer equation fe outlined above an arbitrary quadratic equation aqe forced through the origin is also fit to the data and instead of using the measured linear data the constants in the aqe are used to represent the linear and nonlinear constants in eq 6 4 results and discussion we present three examples of data analysis from these studies to illustrate the procedure for identifying the linear data and determining rec the forchheimer equation fe is a powerful expression for describing nonlinear flow when the linear constant is measured the two step process used to establish the fe effectively removes all influence of nonlinear flow from the linear constant and allows precise analysis of clean data commonly reported in the literature e g nicholl et al 1999 ranjith and darlington 2007 radilla et al 2013 develi and babadagli 2015 zhou et al 2015 zhang et al 2017 xia et al 2017 xiong et al 2018 and also allows consistent analysis of less ideal data sets e g hakami 1989 konzuk and kueper 2004 xiong et al 2011 zhang and nemcik 2013 zoorabadi et al 2015 zhang et al 2015 when the linear data is measured in these examples we compare the results using the two step fe with an aqe where the constants in the aqe are used for calculating the deviation from linear flow using eq 6 and the largest linear flow rate fig 3 shows an example of the analysis of a clean data set from nicholl et al 1999 on a fracture analog made from two textured glass plates 2bm 226 μm the filled circles identify the linear data range with an accurate slope solid red line selected from the entire data set open and filled circles 2bh calculated using the linear data is 199 μm 12 less than the physically measured 2bm indicating that this is a cubic law fracture using the measured linear constant in the two step fe the nonlinear constant is varied to best fit the nonlinear data at larger flow rates in this case the aqe results in an equation with a slightly smaller linear constant green dotted line and a nonlinear constant twice as large as that given in the two step fe the fit of the aqe and the fe are indistinguishable black dashed at the larger flow rates but the largest linear flow rate of the aqe 2 7x10 7 m3 s is much less than the largest measured flow rate of the linear data 5 3x10 7 m3 s used in the fe therefore using the largest measured linear flow rate in eq 6 the aqe constants result in a 2 5 deviation from linear flow while the fe constants calculate a 1 deviation this example of a very clean data set illustrates the importance of measuring the linear data when determining rec rec values calculated at 1 deviation from linear flow are very different i e 3 9 for the fe and 2 0 for the aqe fig 4 shows an example of the data analysis of a slightly less ideal data set from konzuk and kueper 2004 on a tensile fracture created in dolostone 417 μm using the same format and procedures as explained for fig 3 the linear data are used to determine 2bh 307 mm 26 less than the physically measured 2bm indicating that this is also a cubic law fracture the aqe fit to all of the data results in an equation with a slightly larger linear constant green dashed line and a smaller nonlinear constant than the constants in the two step fe the fit of the aqe and the fe are indistinguishable black dashed at larger flow rates but the aqe becomes linear at a higher flow rate than the full extent of the measured linear data resulting in a different rec value calculated at the same 2 1 deviation from linear flow e g rec 6 8 for fe and 7 8 for aqe in these examples the difference between the linear constants is trivial but it serves to show that the arbitrary application of a quadratic equation can underestimate fig 3 and overestimate fig 4 the extent of the linear data causing error in the rec determination fig 5 shows an example of the data analysis of a less than ideal data set from hakami 1989 on an epoxy cast of a natural fracture 313 μm in leptite a fine grained metamorphic rock using the same formatting as the previous fig 2bh calculated from the liner data is 236 μm 25 less than the physically measured 2bm showing this to be a cubic law fracture using the measured linear constant in the fe the nonlinear constant is varied to best describe the deviation from linear flow using the behavior of the more ideal data as a guide which indicates that the largest flow data point is aberrant an aqe fit to all of the data results in an equation with a 20 smaller linear constant green dotted line and a nonlinear constant over an order of magnitude larger than the fe nonlinear constant the fit of the aqe grey dashed and the two step fe black dashed are very different the aqe underestimates the gradient at low flow rates and the linear constant does not represent the measured linear data in contrast the fe behavior is more similar to the fe for the cleaner data sets shown in figs 3 and 4 in this case at the largest linear flow rate the aqe calculates 27 deviation from linear flow while the fe calculates 2 the aqe becomes linear at a much lower flow rate than the fe resulting in very different rec values calculated at 2 deviation e g rec 5 7 for fe and 2 for aqe this example of a less than ideal data set clearly shows the value of using the two step fe to control the effects of aberrant data points and determine rec more consistently when grounding the fe to the linear data both linear and nonlinear data were collected in each of these examples and the linear constant in an arbitrary quadratic equation fit to all of the data was consistently in error regardless of the quality of the dataset and this error is shown to carry over into rec determination all data used in this study were analyzed in a similar manner and examples are shown in the supplemental material for the other tests examined examples of data analysis providing the identified linear data the critical flow rate used to obtain the rec values and comparison of the aqe and fe fits throughout this evaluation we also consistently observed that for tests using excessively large head differentials fracture dilation effects also introduces errors in an aqe resulting in negative linear constants and underestimating head at lower flow rates when only nonlinear data are collected this error is further compounded because of the end effects of the largest and smallest data points examples of these non idealities are shown in the supporting information arbitrary application of quadratic equation tables s2 and s3 table 2 list the results of the rec analysis of the eleven flow tests from six laboratory studies where rec was determined with a high degree of confidence this table compares the physically measured 2bm with the calculated 2bh and lists the 2b σ values to assess the expected accuracy of 2bh the rec values determined for each study are also listed and the nonlinearity at qc is used as an indicator of consistency of the flow regime at the rec flow rate the quality of the rec value is considered excellent only when more than one data point was measured in the linear regime and nonlinear data were also measured when no nonlinear data were collected there is more uncertainty in the rec value i e the value may be biased low in general the expected accuracy of 2bh based on the standard surface separation outlined by brown 1987 shows that the accuracy of the calculated hydraulic aperture generally improves as 2b σ increases the deviation from linear flow at the critical flow rate obtained using the two step forchheimer constants is 1 2 for all cases where linear data were collected indicating consistent flow conditions at the onset of nonlinear flow and the calculated rec values the only data set that deviated at 3 did not have linear data and therefore has less certainty fig 6 shows the plot of both measured open and filled dark squares and hydraulic apertures open and filled green circles vs rec based on data from the 11 flow experiments conducted in the six selected laboratory studies with fractures ranging from 100 to 500 μm the open symbols identify the three tests with fair results the rec values for this aperture range varies from 1 to about 8 which is lower than the theoretical upper limit i e 10 15 obtained from the literature as discussed previously all of the fractures used in fig 6 are shown to be cubic law fractures using the linear data i e all were within 30 of the average physically measured value which indirectly indicates laminar flow conditions and the overall assembled results show a logarithmic relationship between rec and 2b the three tests that resulted in fair rec values are marked in red to show that they still fit well with the other data and if they were removed it would not change the overall logarithmic relationship the range of t values calculated from 2bm for these tests was in a narrow range from 1 7 10 6 to 8 2 10 5 m2 s however most field equipment for straddle packer testing in fractured rock has a measurement range for t from 10 9 to 10 3 m2 s indicating the need for more laboratory tests over a larger range of t at both ends of the t spectrum to further evaluate the relevant 2b vs rec relationship the plot presented in fig 6 is similar to the plots presented by brown 1987 based on numerical modeling where 2b σ shows a logarithmic type relationship with the ratio of hydraulic aperture to theoretical flow aperture indicating that surface roughness aperture variation is more important for small apertures which makes intuitive sense brown s work is also consistent with the results showing that the relationship of the measured aperture vs rec has better correlation than the hydraulic aperture vs rec for the laboratory studies all physical apertures were measured with similar accuracy but 2b σ varied between fractures 2b σ varied from 1 to 42 therefore the hydraulic aperture relationship shows more scatter in general the standard surface separation 2bm σ is an insightful parameter useful for assessing the potential accuracy of hydraulic apertures 4 1 implications for the full rec range the logarithmic relationship we obtained for the 100 500 μm range using the results of the eleven flow tests is clearly defined and is consistent with theory and modeling however it is unclear how far this relationship should be projected beyond the 100 500 μm range fig 7 shows the relationship between 2bm vs rec taken from fig 6 over a larger range for consideration of relationships outside of the 100 500 μm range based on the standard surface separations reported for the 100 500 μm range i e 1 5 2bm σ 4 these fractures likely have some through going open space but are still very small so that asperities project into the open space however the low end of the logarithmic relation does not project through the origin indicating that there must be a different relation for smaller fractures in this smaller aperture domain 100 μm it is unclear whether fractures will follow the cubic law because the fracture surfaces are so close together with highly constrained tortuous flow through very small openings and therefore the effects of surface roughness and aperture variations are greatest for a relation that projects through the origin 0 0 a power law is most reasonable from a fluid mechanics standpoint to connect the logarithmic relation to the origin a power law relation has been used by others for fractures 100 μm e g zhou et al 2015 at the high end of the logarithmic relation there is no fixed constraint for larger fractures however this larger fracture domain begins when fracture surfaces are still close together with asperities projecting into open space and this space must continually increase as the aperture increases until 2bm becomes large enough such that the open space in the center of the fracture where most of the flow is occurring is larger than the asperities at this point the fracture flow may begin to be approximated as flow through rough walled pipes e g moody 1944 this suggests that at some point rec likely increases proportionally greater than the 100 500 μm range according to either a power law logarithmic function or some combination thereof this reasoning can account for the extremely large rec values reported in the literature e g 340 given that the effect of surface roughness and aperture variation becomes minimal for very large fractures laboratory flow test on physically measured fractures are needed to establish the best relations for the larger and smaller fractures 5 conclusions and recommendations although much literature exists concerning laboratory studies of flow through single rough walled fractures we identified only six papers with a total of 11 flow tests that provided suitable data to produce values of 2b and rec for cubic law fractures identified by physically measuring the aperture distribution in the fracture for both physically measured and calculated hydraulic apertures the results indicate that a strong logarithmic relationship exists between 2b and rec for the range of 100 500 μm covered by the laboratory studies the flow tests were conducted in laboratory studies on fractures in different materials ranging from igneous and sedimentary rock samples epoxy acrylic replicas of fractures and artificial fractures thereby suggesting that this relationship has general applicability for the 100 500 μm range independent of rock type or fracture origin this range of apertures represents transmissivities ranging from 1 7 10 6 to 8 2 10 5 m2 s which is within the typical range of t values reported for short interval straddle packer tests at many fractured rock sites e g novakowski and lapcevic 1994 pailliet 1995 novakowski et al 2006 quinn et al 2011a b therefore for t values within this range the 2b vs rec relation presented in this paper provides a fluid mechanics basis for estimating the flow conditions based on the reynolds number i e rec of the largest permeable fractures participating in the hydraulic response of straddle packer tests conducted in rock boreholes all of the observed nonlinear flow occurs in the largest fractures at and near the borehole wall and therefore even though the fractures intersecting the borehole are connected to other fractures in the fracture network the laboratory 2b vs rec relationship is expected to be applicable as long as the t value is within this general range the range of rec values obtained from the laboratory tests is from 1 to 8 for the 100 500 μm aperture range which is at the low end of the much larger range reported in the literature 3 8 to 340 we attribute this large reported rec range partially to inconsistencies in the characteristic length used in re i e d the hydraulic radius the hydraulic diameter or the average aperture however inadequate controls in the experimental methodologies for determining rec most likely strongly contribute to this extreme rec range the data analysis methodology used in this study to develop the 2b vs rec relationship shows that both linear and nonlinear data are required to most accurately identify rec for each test the two step forchheimer equation based on the measured linear constant is more appropriate than an arbitrary quadratic equation fit to the data because it provides consistency for the determination of rec from different studies by grounding the forchheimer equation to the measured linear data if more laboratory studies are conducted with this aim the use of the measured linear data as a control on the forchheimer equation will offer a viable approach for reducing the large range of rec values reported in the literature using a consistent metric for quantitatively identifying the onset of nonlinear flow for rec calculations will also contribute to reducing the size of the rec range a 10 deviation from linear flow is unreasonably large especially for the larger fractures where small aperture changes result in large flow differences the 1 3 deviation criteria we have used is more reasonable and would lessen the over estimation of rec for the larger fractures finally our study shows that to obtain a strong correlation between measured 2b and rec close similarity in the values of 2bh with 2bm is essential for ensuring that all fractures are cubic law fractures although the 2bm vs rec relation we have presented for the 100 500 mm range is strong it is not suitable for projecting very far above or below this this range it is likely that there are at least three distinct rec vs 2b relationships for cubic law fractures based on categories of fracture size including a relationship for fractures smaller than 100 μm the relationship we have presented and another relation for fractures greater than 500 μm accurate physical aperture measurements are required for all studies on flow through single fractures in order to identify cubic law fractures this is most often done digitally where both fracture surfaces are measured and referenced to each other when set in the flow apparatus to obtain good measured aperture statistics volumetric fracture measurements could be used as a check on the 2b physical measurement and may provide a reference when determining the digital 2b distribution from physically measured fracture surfaces ideally a single natural fracture of sufficient area lxw would be mounted so that the aperture of the rock specimen is allowed to vary over a large range of apertures some of the studies listed in table s1 in the supplemental material developed equipment that could accomplish this versatility for example a recent laboratory study ni et al 2018 accomplished this using rubber gaskets of varying thickness to effectively change the separation of the fracture surfaces between 2 and 8 mm allowing a strong normal stress to be used to seal the fracture without closing it and qian et al 2018 sealed the sides of the fracture thereby allowing a device to raise the top fracture surface with a high degree of accuracy carefully conducting flow tests between the same fracture surfaces at different apertures would be the most rigorous approach for increasing the range of the rec vs 2b relationship laboratory tests over the entire range of apertures i e 10 s of microns to a few mm are needed to identify the 2b and rec relationship s for cubic law fractures to determine whether the same relationship applies to all fracture sizes or whether the relationship changes beyond the 100 500 μm range however for tests through very small fractures precise exceptionally low flow measurements will likely be needed while for tests through larger fractures very precise pressure measurements will be necessary there remain other important unanswered questions regarding flow through single fractures that need to be addressed through carefully controlled laboratory studies the six laboratory studies examined conducted flow tests on samples ranging from a few centimeters to greater than 20 cm in length and width yet it is unclear as to whether or not a representative elemental volume of a fracture exists where the fracture properties i e aperture distribution standard deviation etc no longer significantly change with larger scale other important questions include is there an optimal width and length of a tested fracture that would compare best across studies what are the physical differences between cubic law and non cubic law fractures does the difference in small scale surface roughness i e surface smoothness between a fracture replica i e epoxy acrylic etc and a natural fracture change the hydraulic behavior at larger flow rates credit authorship contribution statement p m quinn conceptualization formal analysis methodology validation visualization writing original draft writing review editing j a cherry conceptualization supervision writing review editing b l parker conceptualization funding acquisition investigation project administration resources supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank all of the researchers involved in conducting careful flow tests through single rock fractures in the laboratory their hard work was indispensable for our analysis and we anticipate that results from future tests will further our understanding of the relationship between rec and 2b all data used to support the conclusions can be obtained from the previously published papers listed in table 2 funding for this investigation was provided by a natural sciences and engineering research council of canada nserc senior industrial research chair grant no ircpj 363783 11 held by dr b l parker appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124384 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 supplementary data 3 supplementary data 4 supplementary data 5 supplementary data 6 
5861,the fate of fluid borne entities depends on flow and transport processes in geological environments the classical theories for describing flow cubic law and transport taylor dispersion theory processes within single fractures are based on the poiseuille flow model i e flow through the parallel plates and its modifications with more complex surfaces nonetheless the poiseuille flow model assumes no slip boundary condition while some natural environments show the otherwise e g fracture walls are slippery with non zero flow velocity to better understand the effects of slippery boundaries on transport within poiseuille flow we develop a closed form expression for the longitudinal dispersion coefficient dl based on the corrected flow field that considers homogeneous slip boundary condition moreover the reliable direct numerical simulations were implemented to further validate our proposed theory on dl both theory and numerical experiments suggest that homogeneous slip boundary condition unsignificantly alters dl although slippery boundaries can significantly change the mean velocity of poiseuille flow our theory based on mechanistic albeit simplified model might shed light on predicting the fate of fluid borne entities in complex geological environments keywords longitudinal dispersion coefficient poiseuille flow slip boundary condition fracture cubic law taylor dispersion 1 introduction macrodispersion is mainly caused by the spatial velocity contrast that leads to enhanced mixing and spreading of solute bouquain et al 2012 the enhanced mixing and spreading process i e taylor dispersion was first proposed and quantified as the asymptotic longitudinal dispersion coefficient dl within hagen poiseuille flow in tubes taylor 1953 in addition to the idealized hagen poiseuille flow model for porous media previous studies dentz and carrera 2007 fischer et al 1979 rigorously derived dl for describing taylor dispersion within poiseuille flow which is regarded as a prototype for rivers fractures and microchannels indeed the taylor dispersion theory has been thereafter generalized for quantifying transport process where spatial velocity contrast is present bolster et al 2011 note that the taylor dispersion theory is valid only when asymptotic dispersion is achieved prior to that preasymptotic dispersion i e dynamic dispersion would be more appropriate koch and brady 1987 wang et al 2012 the determination of dl is vital for developing a macroscopic transport macrodispersion model many studies revealed that different concentration boundaries webster et al 2007 the solute source conditions meng and yang 2016 different transport regimes detwiler et al 2000 wang et al 2018 development of recirculation zones zhou et al 2019 temperature effects zheng and wang 2018 transient flow field zheng et al 2019 and flow regimes qian et al 2011b can alter the magnitude of dl moreover many models have been proposed to quantify the macrodispersion process where fluid and solute exchanges between the main flow channel and surrounding matrix are included for example the transient storage model gooseff et al 2005 mattia et al 2003 the multirate mass transfer model haggerty 2013 haggerty and gorelick 1995 and the mobile immobile model gao et al 2010 roubinet et al 2012 all above mentioned scenarios and models assume that the flow field remains unchanged however this assumption might not hold in some natural environments for example slip boundary condition can enhance the velocity field across the poiseuille flow channel for fractures mohais et al 2012 indeed slippery boundaries are commonly observed in microchannels and fractures when mineral surface is hydrophobic ahmad et al 2017 and for the cases when the knudsen number is between 0 01 and 0 1 nagayama et al 2017 nazari moghaddam and jamiolahmady 2016 wang et al 2019 zaouter et al 2018 the knudsen number is the ratio of the mean free path to the flow characteristic length although a large number of studies have investigated the effects of slip boundary condition on fluid flow mohais et al 2011 mohais et al 2012 nagayama et al 2017 nazari moghaddam and jamiolahmady 2016 the effect of slippery boundary on dispersion i e dl remains unclear this largely limits a wide application of the macrodispersion model that requires the information of both fluid velocity and dl our study addresses this open question by quantifying dl for the poiseuille flow model we theoretically develop a closed form expression for dl with the corrected shear flow field considering slip boundary condition following previous studies mohais et al 2011 mohais et al 2012 the reliable direct numerical simulations further support the proposed theory i e slip boundary condition hardly changes dl 2 theory of dl in single fracture considering homogeneous slip boundary condition we first briefly introduce the shear velocity profile within poiseuille flow considering slip boundary condition fig 1 that was derived by mohais et al 2012 following the volume averaging method fischer et al 1979 wood and valdés parada 2013 we derive the mathematical expression for dl based on the corrected flow field 2 1 the corrected flow field within poiseuille flow with slippery boundaries although our study simply focuses on single fracture where mass exchange between fracture and matrix is excluded here the theoretical derivation of dl can be traced back to the fracture matrix system that is the navier stokes equations nse and darcy s law govern fluid flow in the poiseuille flow and surrounding matrix fig 1 respectively the associated governing equations are described by 1 1 ρ u u p μ 2 u 1 2 u 0 1 3 u m k μ p where u is the velocity vector in poiseuille flow with the longitudinal component u fig 1 um is the longitudinal velocity in the surrounding matrix p is pressure gradient μ 1 10 3pa s is fluid dynamic viscosity ρ 1 103 kg m3 is fluid density and k is intrinsic permeability of matrix note that the transverse flow in the matrix is not considered here unlike previous study mohais et al 2012 here we only considered slip boundary condition rather than fluid exchange fig 1 this is because slip boundary condition is commonly present in natural environments for example the hydrophobic mineral surface ahmad et al 2017 slip boundary condition without mass exchange can be mathematically described after beavers and joseph 1967 2 du dy α k u u m where y represents the transverse direction α is a dimensionless slip coefficient that represents the slip length or encapsulates the structural characteristics of the surrounding matrix i e fracture surface roughness which controls the characteristics of moment dissipation across the interface substituting equation 2 into equation 1 eliminated um mohais et al 2011 mohais et al 2012 which left equation 1 with a dependent variable u therefore the matrix fracture system can be reduced to a single fracture system based on the stream function mohais et al 2012 rigorously derived a mathematic expression for u via the similarity solution for the single fracture system 3 u 1 3 ϕ h 2 3 μ y 3 1 2 1 3 ϕ y 3 6 ϕ 2 6 ϕ p x where h is the half height of the poiseuille flow channel fig 1 and ϕ is dimensionless slip coefficient ϕ k α h thus the effects of homogeneous slip boundary are represented by ϕ more in depth procedures in deriving equation 3 can be found in previous studies mohais et al 2011 mohais et al 2012 2 2 the closed form expression for the longitudinal dispersion coefficient based on the corrected shear velocity profile equation 3 we applied the volume averaging method to obtain dl following fischer et al 1979 wang et al 2012 wood and valdés parada 2013 4 d l d m 1 2 h d m h h u h m h n u d y d n d m where m and n are spatial variables equivalent to y and u is the fluctuation term about the mean velocity u u 1 2 h h h u d y across the poiseuille flow channel fig 1 u is mathematically described as 5 u u 1 2 h h h u d y moreover substituting equations 3 into 5 yielded u which was further used to estimate dl based on equation 4 the closed form expression for dl is 6 d l d m 1 2 h d m 1 3 ϕ h 2 3 μ p 2 f h ϕ where f is a function of h and ϕ 7 f h ϕ 2 g 1 2 84 h 7 7 g 1 g 2 30 h 5 2 3 g 1 g 3 g 2 2 2 h 3 2 g 2 g 3 h where g 1 g 3 are functions of ϕ and or h 8 1 g 1 g 2 3 h 2 8 2 g 2 1 2 1 3 ϕ 8 3 g 3 g 1 4 h 4 g 2 2 h 2 3 direct numerical simulations to validate our theory on dispersion coefficient to support the theoretical development on dl we further assessed the effects of homogeneous slip boundary condition on solute transport via direct flow and transport simulations these were achieved by imposing no slip case 1 and slip boundary cases 2 6 conditions with varying slip lengths 5 10 6 5 10 5 m for the poiseuille flow model with h 5 10 4 m length l 0 02 m and a unit width w 1 m table 1 and fig 2 a here the pressure driven flow with a given pressure gradient 1 pa m is governed by the navier stokes equations nse where the inlet and outlet were prescribed as fixed pressure boundaries the fluid properties were μ 1 10 3 pa s and ρ 1 103 kg m3 the case 1 was specified with a no slip boundary condition for the top and bottom walls fig 2a for the cases 2 6 a uniform slip length increases from 5 10 6 5 10 5 m for the slip boundary was set to induce a noticeable change in the mean velocity to ensure accurate numerical results we gradually imposed a finer mesh around the boundaries and a relative coarse mesh in the middle of domain fig 2a until numerical solutions were not sensitive to the mesh size the computational fluid dynamics were implemented in comsol multiphysics the resultant flow fields for six cases were used to simulate solute transport via the advection diffusion equation 9 c t u c d m c where c is concentration dm 2 03 10 9 is molecular diffusion coefficient c was zero everywhere in the domain when t 0 the inlet was imposed as a step injection with c 0 1 the outlet was an open boundary where dispersive flux was zero the effluent concentration over time at the outlet constituted the breakthrough curves btcs which were used to estimate dl by fitting the advection dispersion equation to the btcs the transport simulations were also implemented in comsol multiphysics generally the direct numerical simulations are able to produce the most accurate results wang and cardenas 2014 wang and cardenas 2015 4 results and discussion of the longitudinal dispersion coefficient 4 1 constrains of dimensionless parameter ϕ for the theory the mean velocity u and dl are only functions of h and p as traditionally described by the cubic law witherspoon et al 1980 and by the taylor dispersion theory fischer et al 1979 respectively for a no slip boundary case nonetheless consideration of slip boundary condition additionally introduces a dimensionless number i e ϕ note that the generic forms of the shear velocity profile equation 3 and dl equations 6 8 considering slippery boundaries effect can be reduced to their counterparts figs 3 and 4 with no slip boundary condition i e ϕ 0 to assess the effects of homogeneous slippery boundaries the lower and upper bounds of ϕ are required for a material that has a cellular structure i e irregular interconnected pores α ranges from 0 78 to 4 0 beavers and joseph 1967 moreover k ranges from 10 15 m2 for fine sand silt and loam to 10 8 m2 for well sorted gravel bear 1972 h often spans from 10 5 m to 10 2 m for fractures berkowitz 2002 according to the ranges of above mentioned parameters we confined our study to the cases where 0 ϕ 0 1 this range is conservative as recommended by mohais et al 2012 4 2 negligible effect of homogeneous slip boundary condition on dl according to the theory for the theoretical development the variations of ϕ significantly affect u and discharge figs 3 and 4 of poiseuille flow which is consistent with previous studies mohais et al 2011 mohais et al 2012 compared to the no slip boundary case with ϕ 0 an increase in u is attributed to the overall enhanced velocity component in the longitudinal x direction across the entire poiseuille flow channel figs 1 and 3 this is particularly demonstrated by the non zero velocity at the slippery boundaries because velocity is expected to be zero at the no slip boundaries generally the larger ϕ is the greater enhancement of u and discharge q would be fig 3 however the enhancement of u and q caused by increasing ϕ unnecessarily leads to any changes in dl as suggested by our theory that is q increases with ϕ while dl maintains constant fig 4 where q q cl q c l 8 h 3 w 12 μ p based on the cubic law and dl d taylor d t a y l o r d m q w 2 210 d m based on the taylor dispersion theory the insensitivity of dl to u with increasing ϕ disagrees with the conventional view i e dl is assumed to monotonically increase with u bear 1972 this is because in most circumstances for porous media the enhancement of u implies the greater spatial velocity contrast while this is not the case in poiseuille flow by only varying ϕ fig 3 since only the spatial velocity contrast rather than the magnitude of u alters dl taylor 1953 the variations in ϕ consequently play a trivial role in affecting dl 4 3 direct numerical simulations support the proposed theory regarding dl the results from numerical simulations were found to be consistent with our theory derived results as shown by the velocity profile across the poiseuille flow channel fig 3b which further support the reliability of direct numerical simulations moreover the slippery boundaries in the cases 2 6 resulted in a larger u than that for the case 1 with no slip boundary with an increase in enhancement factor in u with an increase in the slip length table 1 the enhancement is also demonstrated in terms of q fig 4 where the slip length and ϕ are interchangeable given the same amount of q due to the enhancement of u and q at a given time see t 108 s or t 120 s in fig 2b the plume advanced faster take the case 4 with the slip length 2 10 5 m for example than that for the case 1 with no slip boundary fig 2b although homogeneous slippery boundaries facilitated the fluid velocity in the dimensionless time space t pv i e the physical time was nondimensionalized by pore volume pv w area q where area is the area of poiseuille flow channel the detailed description of pv can be found at wang and cardenas 2014 the plume patterns for both two cases with slip case 4 and no slip boundary conditions were fairly identical t pv 0 5 in fig 2b this suggests that with a correction in the flow field considering slippery boundaries effect the solute transport process is essentially not affected by a slip boundary condition an alternative metric to measure the effects of slip boundary condition on macrodispersion is to compare the btcs for the six cases the btcs plotted the dimensionless concentration against the dimensionless time t pv fig 5 a there were no discernable at least not significant differences between six cases in the btcs fig 5a this is inarguably consistent to the snapshots of plume pattern t pv 0 5 in fig 2b and to our proposed theory moreover the calculated dl s by fitting the advection dispersion equation to the numerically derived btcs fig 5b were identical i e dl 1 75 10 8 m2 s for all cases table 1 which again supports our theory that dl maintains constant regardless of variations in ϕ and the slip length fig 4 note that the slip length and ϕ are interchangeable given the same q for cases 2 6 table 1 in short all aforementioned facts suggest that homogeneous slip boundary condition with varying slip lengths table 1 imposes negligible impacts on dl 5 implications and limitations of this study the poiseuille flow model serves as a prototype for fundamentally understanding flow and transport processes in many geological and engineered environments nagayama et al 2017 wang et al 2012 witherspoon et al 1980 while numerous studies have made advancement in revising the poiseuille flow model mohais et al 2012 qian et al 2011a roubinet et al 2012 the effects of commonly observed slip boundary condition on transport were poorly understood the theory and numerical experiments developed here suggest that unlike significant impact on the flow field homogeneous slip boundary condition exerts negligible effects on macrodispersion i e dl admittedly our study only focused on the effects of velocity alteration caused by homogeneous slip boundary condition this neglects fluid and solute exchanges between the poiseuille flow channel and surrounding permeable matrix that of course influence macrodispersion mattia et al 2003 roubinet et al 2012 regardless of these simplifications we established a mechanistic framework to properly assess macrodispersion when the velocity field was significantly altered due to slippery boundaries this improves the shortcoming of previous models where the velocity field is assumed to be constant gooseff et al 2005 mattia et al 2003 6 conclusions the effects of slip boundary condition on the transport process within poiseuille flow remain less clear in this study we developed a closed form expression for the longitudinal dispersion coefficient dl considering the effects of homogeneous slippery boundaries on modifying the shear velocity field moreover the direct numerical simulations on flow and transport processes were implemented to assess and validate our proposed theory regarding dl the simulations considered six cases using either slip or no slip boundary conditions the theory and numerical experiments consistently showed that homogeneous slip boundary condition barely alters dl albeit it significantly affects the mean velocity and discharge within poiseuille flow our study suggests that the fate and transport of fluid borne entities within poiseuille flow is loosely dependent on homogeneous slip boundary condition via the dispersive process credit authorship contribution statement lizhi zheng data curation formal analysis writing original draft writing review editing lichun wang conceptualization investigation methodology resources writing review editing tiejun wang methodology software writing review editing kuldeep singh methodology software zhong liang wang visualization validation xi chen methodology validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is financially supported by national natural science foundation of china grant no 41977161 and grant no 41907171 additional financial support is provided by tianjin university and tianjin normal university all theoretical and numerical results are presented in figures and table the authors thank editor associate editor and three anonymous reviewers for their constructive comments that significantly improved the quality of this technical note 
5861,the fate of fluid borne entities depends on flow and transport processes in geological environments the classical theories for describing flow cubic law and transport taylor dispersion theory processes within single fractures are based on the poiseuille flow model i e flow through the parallel plates and its modifications with more complex surfaces nonetheless the poiseuille flow model assumes no slip boundary condition while some natural environments show the otherwise e g fracture walls are slippery with non zero flow velocity to better understand the effects of slippery boundaries on transport within poiseuille flow we develop a closed form expression for the longitudinal dispersion coefficient dl based on the corrected flow field that considers homogeneous slip boundary condition moreover the reliable direct numerical simulations were implemented to further validate our proposed theory on dl both theory and numerical experiments suggest that homogeneous slip boundary condition unsignificantly alters dl although slippery boundaries can significantly change the mean velocity of poiseuille flow our theory based on mechanistic albeit simplified model might shed light on predicting the fate of fluid borne entities in complex geological environments keywords longitudinal dispersion coefficient poiseuille flow slip boundary condition fracture cubic law taylor dispersion 1 introduction macrodispersion is mainly caused by the spatial velocity contrast that leads to enhanced mixing and spreading of solute bouquain et al 2012 the enhanced mixing and spreading process i e taylor dispersion was first proposed and quantified as the asymptotic longitudinal dispersion coefficient dl within hagen poiseuille flow in tubes taylor 1953 in addition to the idealized hagen poiseuille flow model for porous media previous studies dentz and carrera 2007 fischer et al 1979 rigorously derived dl for describing taylor dispersion within poiseuille flow which is regarded as a prototype for rivers fractures and microchannels indeed the taylor dispersion theory has been thereafter generalized for quantifying transport process where spatial velocity contrast is present bolster et al 2011 note that the taylor dispersion theory is valid only when asymptotic dispersion is achieved prior to that preasymptotic dispersion i e dynamic dispersion would be more appropriate koch and brady 1987 wang et al 2012 the determination of dl is vital for developing a macroscopic transport macrodispersion model many studies revealed that different concentration boundaries webster et al 2007 the solute source conditions meng and yang 2016 different transport regimes detwiler et al 2000 wang et al 2018 development of recirculation zones zhou et al 2019 temperature effects zheng and wang 2018 transient flow field zheng et al 2019 and flow regimes qian et al 2011b can alter the magnitude of dl moreover many models have been proposed to quantify the macrodispersion process where fluid and solute exchanges between the main flow channel and surrounding matrix are included for example the transient storage model gooseff et al 2005 mattia et al 2003 the multirate mass transfer model haggerty 2013 haggerty and gorelick 1995 and the mobile immobile model gao et al 2010 roubinet et al 2012 all above mentioned scenarios and models assume that the flow field remains unchanged however this assumption might not hold in some natural environments for example slip boundary condition can enhance the velocity field across the poiseuille flow channel for fractures mohais et al 2012 indeed slippery boundaries are commonly observed in microchannels and fractures when mineral surface is hydrophobic ahmad et al 2017 and for the cases when the knudsen number is between 0 01 and 0 1 nagayama et al 2017 nazari moghaddam and jamiolahmady 2016 wang et al 2019 zaouter et al 2018 the knudsen number is the ratio of the mean free path to the flow characteristic length although a large number of studies have investigated the effects of slip boundary condition on fluid flow mohais et al 2011 mohais et al 2012 nagayama et al 2017 nazari moghaddam and jamiolahmady 2016 the effect of slippery boundary on dispersion i e dl remains unclear this largely limits a wide application of the macrodispersion model that requires the information of both fluid velocity and dl our study addresses this open question by quantifying dl for the poiseuille flow model we theoretically develop a closed form expression for dl with the corrected shear flow field considering slip boundary condition following previous studies mohais et al 2011 mohais et al 2012 the reliable direct numerical simulations further support the proposed theory i e slip boundary condition hardly changes dl 2 theory of dl in single fracture considering homogeneous slip boundary condition we first briefly introduce the shear velocity profile within poiseuille flow considering slip boundary condition fig 1 that was derived by mohais et al 2012 following the volume averaging method fischer et al 1979 wood and valdés parada 2013 we derive the mathematical expression for dl based on the corrected flow field 2 1 the corrected flow field within poiseuille flow with slippery boundaries although our study simply focuses on single fracture where mass exchange between fracture and matrix is excluded here the theoretical derivation of dl can be traced back to the fracture matrix system that is the navier stokes equations nse and darcy s law govern fluid flow in the poiseuille flow and surrounding matrix fig 1 respectively the associated governing equations are described by 1 1 ρ u u p μ 2 u 1 2 u 0 1 3 u m k μ p where u is the velocity vector in poiseuille flow with the longitudinal component u fig 1 um is the longitudinal velocity in the surrounding matrix p is pressure gradient μ 1 10 3pa s is fluid dynamic viscosity ρ 1 103 kg m3 is fluid density and k is intrinsic permeability of matrix note that the transverse flow in the matrix is not considered here unlike previous study mohais et al 2012 here we only considered slip boundary condition rather than fluid exchange fig 1 this is because slip boundary condition is commonly present in natural environments for example the hydrophobic mineral surface ahmad et al 2017 slip boundary condition without mass exchange can be mathematically described after beavers and joseph 1967 2 du dy α k u u m where y represents the transverse direction α is a dimensionless slip coefficient that represents the slip length or encapsulates the structural characteristics of the surrounding matrix i e fracture surface roughness which controls the characteristics of moment dissipation across the interface substituting equation 2 into equation 1 eliminated um mohais et al 2011 mohais et al 2012 which left equation 1 with a dependent variable u therefore the matrix fracture system can be reduced to a single fracture system based on the stream function mohais et al 2012 rigorously derived a mathematic expression for u via the similarity solution for the single fracture system 3 u 1 3 ϕ h 2 3 μ y 3 1 2 1 3 ϕ y 3 6 ϕ 2 6 ϕ p x where h is the half height of the poiseuille flow channel fig 1 and ϕ is dimensionless slip coefficient ϕ k α h thus the effects of homogeneous slip boundary are represented by ϕ more in depth procedures in deriving equation 3 can be found in previous studies mohais et al 2011 mohais et al 2012 2 2 the closed form expression for the longitudinal dispersion coefficient based on the corrected shear velocity profile equation 3 we applied the volume averaging method to obtain dl following fischer et al 1979 wang et al 2012 wood and valdés parada 2013 4 d l d m 1 2 h d m h h u h m h n u d y d n d m where m and n are spatial variables equivalent to y and u is the fluctuation term about the mean velocity u u 1 2 h h h u d y across the poiseuille flow channel fig 1 u is mathematically described as 5 u u 1 2 h h h u d y moreover substituting equations 3 into 5 yielded u which was further used to estimate dl based on equation 4 the closed form expression for dl is 6 d l d m 1 2 h d m 1 3 ϕ h 2 3 μ p 2 f h ϕ where f is a function of h and ϕ 7 f h ϕ 2 g 1 2 84 h 7 7 g 1 g 2 30 h 5 2 3 g 1 g 3 g 2 2 2 h 3 2 g 2 g 3 h where g 1 g 3 are functions of ϕ and or h 8 1 g 1 g 2 3 h 2 8 2 g 2 1 2 1 3 ϕ 8 3 g 3 g 1 4 h 4 g 2 2 h 2 3 direct numerical simulations to validate our theory on dispersion coefficient to support the theoretical development on dl we further assessed the effects of homogeneous slip boundary condition on solute transport via direct flow and transport simulations these were achieved by imposing no slip case 1 and slip boundary cases 2 6 conditions with varying slip lengths 5 10 6 5 10 5 m for the poiseuille flow model with h 5 10 4 m length l 0 02 m and a unit width w 1 m table 1 and fig 2 a here the pressure driven flow with a given pressure gradient 1 pa m is governed by the navier stokes equations nse where the inlet and outlet were prescribed as fixed pressure boundaries the fluid properties were μ 1 10 3 pa s and ρ 1 103 kg m3 the case 1 was specified with a no slip boundary condition for the top and bottom walls fig 2a for the cases 2 6 a uniform slip length increases from 5 10 6 5 10 5 m for the slip boundary was set to induce a noticeable change in the mean velocity to ensure accurate numerical results we gradually imposed a finer mesh around the boundaries and a relative coarse mesh in the middle of domain fig 2a until numerical solutions were not sensitive to the mesh size the computational fluid dynamics were implemented in comsol multiphysics the resultant flow fields for six cases were used to simulate solute transport via the advection diffusion equation 9 c t u c d m c where c is concentration dm 2 03 10 9 is molecular diffusion coefficient c was zero everywhere in the domain when t 0 the inlet was imposed as a step injection with c 0 1 the outlet was an open boundary where dispersive flux was zero the effluent concentration over time at the outlet constituted the breakthrough curves btcs which were used to estimate dl by fitting the advection dispersion equation to the btcs the transport simulations were also implemented in comsol multiphysics generally the direct numerical simulations are able to produce the most accurate results wang and cardenas 2014 wang and cardenas 2015 4 results and discussion of the longitudinal dispersion coefficient 4 1 constrains of dimensionless parameter ϕ for the theory the mean velocity u and dl are only functions of h and p as traditionally described by the cubic law witherspoon et al 1980 and by the taylor dispersion theory fischer et al 1979 respectively for a no slip boundary case nonetheless consideration of slip boundary condition additionally introduces a dimensionless number i e ϕ note that the generic forms of the shear velocity profile equation 3 and dl equations 6 8 considering slippery boundaries effect can be reduced to their counterparts figs 3 and 4 with no slip boundary condition i e ϕ 0 to assess the effects of homogeneous slippery boundaries the lower and upper bounds of ϕ are required for a material that has a cellular structure i e irregular interconnected pores α ranges from 0 78 to 4 0 beavers and joseph 1967 moreover k ranges from 10 15 m2 for fine sand silt and loam to 10 8 m2 for well sorted gravel bear 1972 h often spans from 10 5 m to 10 2 m for fractures berkowitz 2002 according to the ranges of above mentioned parameters we confined our study to the cases where 0 ϕ 0 1 this range is conservative as recommended by mohais et al 2012 4 2 negligible effect of homogeneous slip boundary condition on dl according to the theory for the theoretical development the variations of ϕ significantly affect u and discharge figs 3 and 4 of poiseuille flow which is consistent with previous studies mohais et al 2011 mohais et al 2012 compared to the no slip boundary case with ϕ 0 an increase in u is attributed to the overall enhanced velocity component in the longitudinal x direction across the entire poiseuille flow channel figs 1 and 3 this is particularly demonstrated by the non zero velocity at the slippery boundaries because velocity is expected to be zero at the no slip boundaries generally the larger ϕ is the greater enhancement of u and discharge q would be fig 3 however the enhancement of u and q caused by increasing ϕ unnecessarily leads to any changes in dl as suggested by our theory that is q increases with ϕ while dl maintains constant fig 4 where q q cl q c l 8 h 3 w 12 μ p based on the cubic law and dl d taylor d t a y l o r d m q w 2 210 d m based on the taylor dispersion theory the insensitivity of dl to u with increasing ϕ disagrees with the conventional view i e dl is assumed to monotonically increase with u bear 1972 this is because in most circumstances for porous media the enhancement of u implies the greater spatial velocity contrast while this is not the case in poiseuille flow by only varying ϕ fig 3 since only the spatial velocity contrast rather than the magnitude of u alters dl taylor 1953 the variations in ϕ consequently play a trivial role in affecting dl 4 3 direct numerical simulations support the proposed theory regarding dl the results from numerical simulations were found to be consistent with our theory derived results as shown by the velocity profile across the poiseuille flow channel fig 3b which further support the reliability of direct numerical simulations moreover the slippery boundaries in the cases 2 6 resulted in a larger u than that for the case 1 with no slip boundary with an increase in enhancement factor in u with an increase in the slip length table 1 the enhancement is also demonstrated in terms of q fig 4 where the slip length and ϕ are interchangeable given the same amount of q due to the enhancement of u and q at a given time see t 108 s or t 120 s in fig 2b the plume advanced faster take the case 4 with the slip length 2 10 5 m for example than that for the case 1 with no slip boundary fig 2b although homogeneous slippery boundaries facilitated the fluid velocity in the dimensionless time space t pv i e the physical time was nondimensionalized by pore volume pv w area q where area is the area of poiseuille flow channel the detailed description of pv can be found at wang and cardenas 2014 the plume patterns for both two cases with slip case 4 and no slip boundary conditions were fairly identical t pv 0 5 in fig 2b this suggests that with a correction in the flow field considering slippery boundaries effect the solute transport process is essentially not affected by a slip boundary condition an alternative metric to measure the effects of slip boundary condition on macrodispersion is to compare the btcs for the six cases the btcs plotted the dimensionless concentration against the dimensionless time t pv fig 5 a there were no discernable at least not significant differences between six cases in the btcs fig 5a this is inarguably consistent to the snapshots of plume pattern t pv 0 5 in fig 2b and to our proposed theory moreover the calculated dl s by fitting the advection dispersion equation to the numerically derived btcs fig 5b were identical i e dl 1 75 10 8 m2 s for all cases table 1 which again supports our theory that dl maintains constant regardless of variations in ϕ and the slip length fig 4 note that the slip length and ϕ are interchangeable given the same q for cases 2 6 table 1 in short all aforementioned facts suggest that homogeneous slip boundary condition with varying slip lengths table 1 imposes negligible impacts on dl 5 implications and limitations of this study the poiseuille flow model serves as a prototype for fundamentally understanding flow and transport processes in many geological and engineered environments nagayama et al 2017 wang et al 2012 witherspoon et al 1980 while numerous studies have made advancement in revising the poiseuille flow model mohais et al 2012 qian et al 2011a roubinet et al 2012 the effects of commonly observed slip boundary condition on transport were poorly understood the theory and numerical experiments developed here suggest that unlike significant impact on the flow field homogeneous slip boundary condition exerts negligible effects on macrodispersion i e dl admittedly our study only focused on the effects of velocity alteration caused by homogeneous slip boundary condition this neglects fluid and solute exchanges between the poiseuille flow channel and surrounding permeable matrix that of course influence macrodispersion mattia et al 2003 roubinet et al 2012 regardless of these simplifications we established a mechanistic framework to properly assess macrodispersion when the velocity field was significantly altered due to slippery boundaries this improves the shortcoming of previous models where the velocity field is assumed to be constant gooseff et al 2005 mattia et al 2003 6 conclusions the effects of slip boundary condition on the transport process within poiseuille flow remain less clear in this study we developed a closed form expression for the longitudinal dispersion coefficient dl considering the effects of homogeneous slippery boundaries on modifying the shear velocity field moreover the direct numerical simulations on flow and transport processes were implemented to assess and validate our proposed theory regarding dl the simulations considered six cases using either slip or no slip boundary conditions the theory and numerical experiments consistently showed that homogeneous slip boundary condition barely alters dl albeit it significantly affects the mean velocity and discharge within poiseuille flow our study suggests that the fate and transport of fluid borne entities within poiseuille flow is loosely dependent on homogeneous slip boundary condition via the dispersive process credit authorship contribution statement lizhi zheng data curation formal analysis writing original draft writing review editing lichun wang conceptualization investigation methodology resources writing review editing tiejun wang methodology software writing review editing kuldeep singh methodology software zhong liang wang visualization validation xi chen methodology validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is financially supported by national natural science foundation of china grant no 41977161 and grant no 41907171 additional financial support is provided by tianjin university and tianjin normal university all theoretical and numerical results are presented in figures and table the authors thank editor associate editor and three anonymous reviewers for their constructive comments that significantly improved the quality of this technical note 
5862,this study proposed and compared several novel hybrid models that combined swarm intelligence algorithms and deep learning neural network for flood susceptibility mapping lai chau a province in the northwest mountainous region of vietnam was chosen as a case study since it had recently undergone severe flashflood in 2018 for this purpose numerical predictor variables such as topographically derived factors digital elevation model aspect slope curvature topographic wetness index climatic variables rain and hydrological variables stream density stream power index distance to river and multiple remote sensing indices normalized difference vegetation index normalized difference buildup index were used these predictor variables were selected because they are globally collectible and reproducible the performances of these models were evaluated by using common statistical indicators namely root mean square error mean absolute error overall accuracy and area under receiving operating characteristics and the statistical test of differences the results showed that the proposed swarm intelligence models outperformed benchmarked methods namely particle swarm optimization support vector machine random forest in almost all comparing indicators it is suggested that proposed models are more robust than the classifiers which were used for benchmarking and they are good alternatives for flood susceptibility mapping given the availability of dataset keywords swarm intelligence optimization algorithm deep learning neural network flash flood 1 introduction tropical countries are among the most vulnerable regions to the impact of climate change induced disasters that happen in unprecedented magnitude and frequency the prevention of these events is strenuous in the short term but the negative impacts can be mitigated through risk preparedness plans among devastating hazards flashflood is one of the most dangerous events in mountainous areas especially in vietnam where mountain encounters three quarters of the country area flood causes severe damages to people and makes adverse impacts on social economic development across the country particularly to the ethnic communities the management of flood including flood risk prediction requires accurate spatial data and temporal information ouma and tateishi 2014 and effective preparedness plans to minimize loses herold and sawada 2012 jonkman and dawson 2012 the identification of vulnerable areas to the flood significantly contributes to the reduction of damages to human settlements agriculture and livelihood by avoiding more construction and developments in the prone areas the crucial component of the vulnerability assessment model is a susceptible map that predicts the probability of an area to be exposed to the flood driving forces of this hazard are mainly depended on climatic factors physical conditions as well as anthropogenic activities bui et al 2016 ngo et al 2018 zhao et al 2018 currently the relationship between these factors and flood occurrences has been investigated with the support of the development of geo information technology and machine learning techniques most recent research focused on the application of machine learning classifiers and their customized versions of which neural network family support vector machine decision rules and frequency ratios are perhaps among the most usually used techniques examples could be found in the study of cao et al 2016 rahmati et al 2016 rsamanta et al 2018a samanta et al 2018b or in ngo et al 2018 zhao et al 2018 moreover the performances of these classifiers can be improved with the implementation of optimizers for tuning the classifier s parameters in general physical based evolutionary swarm intelligence are three conventional optimization techniques that have been found in numerous applications such as land cover classification bui et al 2019b bui et al 2019c bui et al 2018 landslide risk detection pham et al 2019 forest fire analysis bui 2019 community disease assessment bui et al 2019a for flood study the evolutional approach was mentioned in the studies of bui et al 2018 conventional swarm algorithms such as firefly or particle swarm optimization algorithms were used in the works of ngo et al 2018 razavi termeh et al 2018 sachdeva bhatia and verma 2017 or genetic algorithm in bui et al 2016 with good results however no solution fits to solve all problems bui et al 2018a bui et al 2018b because of the variation of training dataset or the complexity of different geographic regions in this regard every single study tries to find the most appropriate algorithm by contrasting its performance to conventional benchmarked methods in this regard this study verified several novel hybrid models that are based on the recently developed swarm intelligence optimization algorithms mimicking behaviors of grey wolfs gwo mirjalili et al 2014 social spiders sso yu and li 2015 and grasshopper goa saremi et al 2017 for tuning a deep learning neural network for flood susceptibility mapping these algorithms had been investigated to solve global optimization problems of the theoretical benchmarked functions but a few applications to solve real problems is found the diverse properties of real problems require considerable verifications of algorithms to find out the suitable methods at present to solve those problems to the best knowledge of the authors no such examinations of these algorithms have been applied for flood susceptibility mapping therefore the verifications these algorithms are scientifically sound and practically useful for prone countries to natural hazards lai chau a province in the northwest mountainous region of vietnam was chosen as a case study since it had recently undergone severe flash flood in 2018 from literature review eleven numerical predictor variables such as topographically derived factors digital elevation model dem aspect slope curvature topographic wetness index twi climatic variables rain river network stream density stream power index spi distance to river and several multispectral satellite image index such as ndvi ndbi were used these factors were selected because they are globally collectible and reproducible if similar conditions are met the input data was standardized by using two conventional methods namely 0 to 1 normalization and reclassification based on the natural break algorithm the performance of these models was evaluated by using common statistical indicators namely root mean square error rmse mean absolute error mae overall accuracy oa and area under receiving operating characteristics auc and was statistically tested for differences the final result is a susceptible map that was generated from the most optimal model 2 study area and data 2 1 study area lai chau province is located on the northwestern mountainous region of vietnam between latitudes 21 51 and 22 49 and longitudes 102 19 to 103 59 the province topology ranges from 120 m more than 3000 m above the sea level with complex spatial variations several dense river systems run across the province providing a relatively high density at 5 5 6 km km2 typically the province is influenced by tropical monsoon with temperatures at 0 c 33 c and high humidity at around 86 the climate is divided into two distinct seasons rainy and dry seasons in which the rainy season usually begins from may to september when 80 of annual precipitation is recorded and the dry season begins from november to may of the next year in lai chau approximately 35 of natural land is forested including natural forest and plantation forest agricultural land covers about 7 and the rest are for other types deforestation and forest degradation continue to be critical threats in the province forest cover consequently the loss of forest cover leads to an increased risk of hazards such as flashflood and landslides recently the study area had undergone severe impacts of these double disasters in 2018 that caused significant loss of humans and massive loss of economic properties therefore a flood susceptible map is necessary for disaster prevention or quick response programs among all districts in the province a border district in the northwestern part of lai chau was selected to verify the proposed models fig 1 2 2 flood inventory mapping the historical flash flood locations could be determined by comparing two images that had been captured before and after the investigated events in 2018 unprecedented rains had caused severe floods in the vast areas in the northwestern mountainous regions of vietnam the local authorities of lai chau province had reported a peak of the flood on june 24th 2018 when vast damages of community infrastructures were reported based on this information two sentinel 1 scenes before and after the flood events were selected to determine flood areas by comparing backscattering between two dates the level1 ground range detected grd products interferometric wide swath mode were downloaded from the copernicus open access hub the data cover an area of 250 km swath with a resolution of 5 m 20 m detail specification of sentinel data is described in table 1 the processing was implemented by using snap open source software that is downloadable at http step esa int the procedure started with the calibration for vv band to convert pixel values to backscatter coefficient values followed by the speckle filtering to reduce noise then the determination of flooded area was implemented by defining a threshold to distinguish water land values the conversion of results from previous steps to binary raster 1 for water and 0 for non water was carried out the results were further processed with terrain correction by using dem from 1 50 000 topographic map which was produced edited in 2017 and georeferenced to utm wgs 84 projection the final results were compared to derive the inundated areas in muongte district additionally historical flood locations from field surveys were also added to build up the input database 2 3 predictor variables from literature there is no specific requirement on which predictor variables should be used and the selection of the variables depends on data availability and specific local conditions bui et al 2018a in general the risk of flood hazard is subject to changes of physical topography in mountainous regions and variation of elevation in built up areas in cities in simulating water flow in elevated areas it is assumed that water runs to steepest slopes which are calculated from terrain models in this sense the topography is the first element to be mentioned and it is represented by a 1 digital elevation model dem and dem derivable maps such as 2 slope 3 aspect 4 curvature and 5 topographic wetness index twi twi quantitatively measure the soil moisture and is a good indicator of land cover conditions typically floods are caused by the trajectory of water flow in rivers several river related indexes were proposed namely 6 river density 7 distance to river 8 stream power index spi to measure the influence of mountain stream network cao et al 2016 products from remote sensing are another useful source to monitor the periodical surface conditions over an area particularly flood related variables such as land cover some commonly used indexes have been investigated and were proposed to use in this study including 9 normalized difference vegetation index ndvi which is one of the most common indicators to measure spatial variation of vegetated areas and 10 normalized difference built up index ndbi which defines the status of build up and bare soil areas those indexes were calculated from the surface reflectance product of the landsat 8 oli operational land imager scene on feb 12th 2017 which was downloaded from https earthexplorer usgs gov in practical there are several other indices that can be used for this study such as normalized difference water index or normalized difference moisture index but there is a certain level of correlation between these indices and ndvi ndbi therefore ndvi and ndbi were kept for further processes the measurement units are different among predictor variables and in some cases normalization might be applied indeed there is no requirement for normalization of input data but sometimes model performances get boosted if proper rescaling methods are applied the two most common techniques are 1 natural break algorithms that had been used to reclassify variables into a limited number of classes in flood risk mapping bui et al 2016 ngo et al 2018 this method determines the best arrangement of input data and assigns value ranges to classes 2 conversion of input data into 0 1 value range bui et al 2018b new values are calculated by the equation x n e w x x m i n x m a x x m i n and this conversion keeps the original distribution of input datasets for the verification of proposed models two normalized datasets by using these methods were applied in which each variable in the first dataset was classified into classes as shown in table 2 3 methods the susceptible map is generated with an assumption that events that happened in the past will happen in the future if similar conditions come this type of map calculates the probability of a location to be exposed to flash floods the study area is represented by a raster 30 m 30 m and each pixel is assigned the likelihood from 0 to 100 or 0 1 in which areas that are not inundated are given the probability of 0 and inundated areas are assigned to 100 additionally apart from historically flooded areas as described in flood inventory mapping section the same amount of points in non inundated areas were randomly chosen to make up a whole set of 4812 point layer for the training and validation dataset this point layer was overlaid to 11 predictor variable layers and the attributes were extracted the overall workflow is shown in fig 2 and can be explained as follows step 1 this step includes the pre processing of satellite data calculating satellite index images estimating landuse derived maps and interpolating climatic maps from meteorological data the historical flood points are identified by using sentinel 1a images in combination with field surveys including flood and non flood and are assigned values from eleven variables all data are produced converted resampled into a raster format of similar spatial resolution 30 m 30 m in utm wgs 84 projection step 2 the input data are randomly divided into a training dataset 70 to train the proposed models and the remaining is used to validate test the performance of the model in this step deep learning neural network dnn acts as a classifier and swarm intelligence algorithms which are described in the following sections are used to search for connecting weights of dnn step 3 by running the validated models for the entire study area the flood susceptible map is generated by calculating the probability of each location to be exposed to the risk finally the value range is reclassified by using natural break algorithm to the qualitative classes such as very low low moderate high and very high 3 1 deep learning neural network dnn belongs to the artificial neural network family and is characterized to have more than a single hidden layer it is the feed forward network and usually trained by using the back propagation method but the increasing of hidden layers makes the network hard to train because of different learning speeds in hidden layers dnn has been successfully implemented in a variety of applications mainly in automatic image recognition speech recognition language processing and several applications in remote sensing were found in the studies of kemker et al 2018 lanaras et al 2018 li et al 2018 technically there is no rule of thumb on how many hidden layers and the number of neurons in each layer because the decision is subject to the complexity of problems and condition of the training dataset since the relation between flood occurrences and diving forces is complex bui et al 2016 zhao et al 2018 three hidden layers were proposed the structure of a dnn is shown in fig 3 having one input layer of eleven predictor variables three hidden layers and one output layer representing exposure probabilities the output values are reclassified into qualitative classes basing on predefined value ranges technically this dnn is characterized by four weight matrixes that have the sizes of n 11 n n n n and n 1 in the sequential order respectively the weights are trained by the proposed optimization algorithms for each training step the sigmoid activation function equation 1 was employed and rmse as shown in equation 2 was used as the objective function 1 f x 1 1 e x 2 rmse 1 n 1 n predicted i observed i 3 2 swarm intelligence algorithms the meta heuristic algorithm is a fast growing and active research area to catch up with changes in the nature of problems particularly with increasing data size swarm intelligence optimization algorithms are developed basing on the behaviors of insect swarm in the social arrangement or hunting strategies the fast growing of this type of algorithm helps in solving many science and engineering applications and there is a considerable need to verify the potential uses of new algorithms in real complex problems social spider optimization sso grasshopper optimization algorithm goa grey wolf optimization gwo are three newly developed algorithms and had been theoretically examined but the practical application of those are still limited this study investigated the potential uses of these algorithms in spatial distribution earth surface occurrences in general the pseudocode for all of those are as follows initialize the swarm in d dimensional space these dimensions are predictor variables initialize algorithm parameters e g swarm population maximum iteration calculate fitness value for each search agent and randomly define the preliminary best position t while t than maximum iteration update the position of all swarm agents by each algorithm specific equation recalculate fitness for each agent update best position t if there is a better value t t 1 break if desirable rmse is met end while loop 3 2 1 grey wolf optimization grey wolf society has a hierarchical structure forming a pyramid with the most dominant powerful wolfs at the top and remaining wolfs are in descending importance at lower positions gwo was firstly introduced by mirjalili et al 2014 and was successfully implemented in the forest fire study of bui 2019 grey wolf behaviors are mathematically modeled by mimicking the social hierarchy prey searching exploration process prey encircling and hunting and prey attacking activities exploitation process like other swarm based algorithms search agents wolfs are randomly positioned in the d dimensional search space and updated their positioned and the fitness values after each iteration the final location of the top wolf is the optimal solution for solving the raised problem 3 2 2 grasshopper optimization algorithm goa was developed by saremi et al 2017 and was verified by neve 2017 for solving both constrained and unconstrained problems it had also been tested with a shallow neural network in the study of bui et al 2018b for land cover classification grasshopper behavior is modeled in both nymph and adulthood stages to form the population grasshoppers search agents move in a swarm in searching for food fitness value starting with abrupt movement in the exploration stage and locally focus on exploitation search for each change grasshoppers interact with others in the swarm and the combination of the current position global best and position of other agent s influences on how the next position is updated this searching procedure is how goa is found different from conventional pso and probably makes goa more powerful than pso in solving the non linear problems 3 2 3 social spider optimization algorithm sso is a new meta heuristic optimization algorithm that has been proposed by yu and li 2015 by investigating the food foraging behaviors of spiders or so called agents in nature spiders live in groups and interact with each other for searching for food a web is described as a d dimensional search space and each node on the web is considered a feasible solution to the optimization problem on the other hand spiders act as search agents and move on the web from node to node vibrations at sources are generated after each movement of spiders and they are propagated over the web based on the attenuated intensity of vibration spiders receive from the other the fitness values are recalculated technically this foraging strategy makes sso different from conventional swarm algorithms in searching strategy spiders are influenced by current positions of all other and their historical positions rather than solely sticking to their positions and the global best positions as explained in pso also information attenuation by distance is another aspect that shapes how information is propagated 3 3 accuracy assessment the comparison between the performances of classification algorithms is carried out through confronting conventional statistical indicators of the proposed model with similar values conducted by benchmark methods these suggested methods were verified against shallow neural networks ngo et al 2018 dnn optimized by gradient descent algorithm gd and particle swarm optimization pso sachdeva et al 2017 which have some specified level of similarity to proposed methods besides two conventional classifiers such as support vector machine svm and random forest rf were also introduced as they had been widely used in solving non linear problems such as susceptible mapping of disasters bui et al 2018a bui et al 2016 these three methods run with the similar dataset and the results were compared by using several indicators namely 1 root mean square error rmse is used as the objective function 2 mean absolute error mae measures the error that is similar to rmse but takes some aspects of proportional distribution 3 overall accuracy oa to reveal the percentage of correct classification over total number of samples 4 receiver operation characteristics roc plots true positive rates against false positive rates and area under roc auc auc value ranges from 0 1 in which random value is equal to 0 5 the higher the auc is the more robust the model performs but regularly auc much exceeds 0 5 to be considered the proper method 4 experiments and discussion 4 1 initialization of model conventionally in running classifiers with data in multiple or hyper dimensional space the feature selection procedure is indeed a crucial step q t bui pham et al 2019 data redundancy increases the cost of data collection and manipulation and might also influence the performance of the predictive models classifiers are unable to separate classes with few variables but usually fall into over fitting situation if predictor variables are large even though there were only 11 predictor variables in this study but we did evaluate them by using the method as implemented by hong et al 2016 through assessing mean decrease accuracy mda and mean decrease gini mdg with random forest algorithm in which out of bag error oob is used to rank the importance of variables this method is simple but informative and can be a good start before moving to the more complicated step fig 4 shows mda on the left and mdg on the right hand side from fig 4 it can be seen that ndvi had the highest mdg value and was ranked as the most critical factor ndvi provides the surface condition of the study areas as most of the flood locations were found in the bare land dem and distance to river also have essential contributions to the model as they are one of the most dominant factors in determining how water flows in the river basin the importance levels of the remaining predictor variables slowly decreased and the precipitation was found having fewer influences on the determination of flooded area it could be understood that the rainfall was almost homogeneous due to the sparse distribution of metrological stations in the study areas and there was no significant difference between regions of the highest precipitation and regions with the lowest quantity statistically all variables have higher values than zero which means all can be used to train the proposed model other crucial steps are the identification of model parameters to ensure the replicability of the experiments in different locations general settings are listed in the first column of table 3 and they are specified per each algorithm the search for optimal values was done from the literature review and through trial and error with the training dataset svm performance was characterized by two standard parameters namely kernel width γ and the regularization c and grid search was used to determine those parameters rf is an ensemble method and does not have tunable parameters except the predefined number of trees 500 trees are large enough with the current training dataset 4 2 results from experiments dnn structure is differenced from a shallow network by the depth of hidden layers and the number of neurons in each layer the more layers dnn has the harder it is to train the network a network of three hidden layers was proposed and several choices of neuron nodes were examined usually the neurons should be smaller than the inputs and higher than the expected outputs in this sense the experiments were run with 10 9 8 7 and 6 neurons in each hidden layer respectively and the statistical errors were used to compare the results the experiments were implemented with datasets from two conventional normalization methods table 4 showed the rmses using eq 2 of the validation dataset which was used as the termination for the iteration process after 3000 iterations of the proposed methods in which the dataset with natural break standardization methods almost generated smaller values than those in the other dataset it could also be seen that sso seemed to perform better rmses 0 224 for the training dataset and 0 227 for validation dataset with ten neurons in every hidden layer and there is no clear correlation between the number of neurons and rmses other performance indicators are also shown in table 5 besides examples of rmse variations of goa sso and gwo shaded cells in table 4 were plotted in fig 5 that shows the search patterns of proposed algorithms it could be seen that goa and gwo had a similar searching pattern that rapidly jumped from the beginning to the 500th iteration and gradually decreased until reaching maximum repetition this strategy begins with the exploration stage aiming to search for promising regions and then is followed by exploitation search to find an accurate approximation of global best value on the other hand sso took aggregated signals from all other searching agents spiders and had small jumps in constant combination changes during the search the results from the experiments were compared with three other benchmarked classifiers namely svm rf and gd dnn optimized by gradient descend algorithms by using several statistical indicators svm and rf were chosen for benchmarking since they had been widely used in studies for disaster susceptible mappings bui et al 2018a bui et al 2016 ngo et al 2018 and optimal parameters were presented in table 3 the results showed that swarm intelligence algorithms outperformed the other in all comparing indicators and sso ended up with the highest overall accuracy 96 539 with 10 neurons and natural breal normalization method in this study oa was found high in either training or validation datasets since the inundated areas usually are clustered along with the stream network and the predictor variables predict well the locations of flood hotspots technically the susceptible probability varies between 0 and 100 or between 0 and 1 and 0 5 was defined as the specific cut point to classify instances to either flood higher than 0 5 or non flood smaller than 0 5 fig 6 plotted the validation samples from the sso algorithm with ten neurons in each of the hidden layers in which samples on the right side of the y axis are observed flood locations and those on the left side of the y axis are non flood samples plots in quarters i iv showed true classified samples with almost predicted probabilities are close to 1 or 0 on the other hand there are small numbers of miss classified samples in quarters ii iii one of the challenging issues that all data driven machine learning models face is to minimize or avoid over fitting traditional back propagation is found limitation in coping with local minima and over fitting problem chiroma et al 2020 so that several techniques are being investigated the recent introduction of meta heuristic algorithms had been proposed as alternative optimizers for tuning deep neural networks chiroma et al 2020 fong et al 2018 rere et al 2016 to avoid overfitting parameters of algorithms are adjusted such as velocity and searching space in pso tian and fong 2016 in this study parameters of the verified algorithms were predefined and adjusted through trial and error processes the differences between rmse values from both train data and validation data which were shown in table 4 were small for each algorithm and by each determination of neurons the learning curves from training datasets fig 7 have similar patterns with rmse curves from the validation dataset the unbiased evaluation by the held back validation data showed that over fitting has been significantly overcome and the validated models were ready for mapping flood susceptibility for entire study area since the objective of this study is to verify the potential application of swarm intelligence methods in optimizing parameters of dnn for flood susceptible mapping sdditional statistical test of difference between model performance is required in this regard a wilcoxon signed rank test was carried out for the rmses as showed in table 4 table 6 showed that the rmses from gwo goa sso are statistically differenced from those generated by pso and we failed to reject the null hypothesis that the differences between sso goa and sso gwo equal to zero indeed swarm algorithms have similar searching algorithms mirjalili et al 2014 saremi et al 2017 and with the specific dataset the operation might result in approximated rmses however in comparison to benchmark methods all three models perform better and can be used as alternative methods to generate flood susceptible maps 4 3 model comparison and flood susceptibility map generation the performance of examined models are shown in table 5 in which sso goa and gwo were investigated with two different training datasets by using two normalization methods the significane test table 6 showed that the performances are statistical significant in which the examined algorithms are better than benchmark methods and are suitable for the dataset of this study the best results from sso auc 97 003 goa auc 96 798 and gwo auc 96 751 are satisfactory in comparison to previous studies razavi termeh et al 2018 by using the combination of adaptive neural fuzzy inference system adaptive nfis and pso with auc 94 5 bui et al 2016 by itegrating metaheuristic optimization into nfis with auc 0 962 bui et al 2018a with auc 94 7 from adaptive nfis optimized imperialistic competitive algorithm and some others rahmati et al 2016 s samanta et al 2018 in fact the power of any model is subject to specific problems or specific training datasets wolpert and macready 1997 and it could be seen that the proposed model in this study perform well over referenced flashflood susceptibility mapping studies the application of meta heuristic algorithms in optimimizing parameters of classifiers for flood modelling or for natural hazard analysis in broader perspective is increasing with the fast growing development of new algorithms because there is no algorithm to solve all optimization problems in this study the verification of several algorithms that are inspired from animal behaviors enrich modelling methods for similar works in other study areas from the analysis of experiment results and model comparision sso was chosen to produce susceptible for the visualization of spatial distribution of areas with potential flooded risk in muongte district fig 8 the output values were reclassified by using the natural break algorithm to assign value range to the verbal representations of flood susceptible levels the qualitative classes are very low 0 0 091 low 0 092 0 26 moderate 0 27 0 49 high 0 50 0 76 very high 0 77 1 the hotspot of flood risk clustered along the river from the practical experiments it is well noted that the frequency of landslide occurrences in a similar location is rare but the frequency that areas get inundated is high since the influential factors are unchanged from time to time the final susceptible map provides a scientifically based reference for the disaster preparedness plan for the local communities when rainy seasons come 5 conclusion swarm intelligence is a powerful optimization group to solve global minimization or maximization problem in real applications in this study three newly developed algorithms were examined with a dataset of 11 predictor variables and the results were compared to several benchmarked methods with common statistical indicators the proposed methods significantly improve the performance of dnn by generating smaller rmse as the objective function than those produced by svm rf pso or gradient descent algorithm however the goodness of these hybrid methods was determined through the proper identification of flood areas by using sentinel data adequate processing of predictor variables and initialization of model parameters these processes have a significant contribution to the success of this research flash flood is a destructive natural event that destroys society s infrastructure and causes enormous loss to humanity in the context of climate change the situation is worsening when extreme climatic events happen in an unprecedented way accurate flood susceptible maps are good sources to support disaster preparedness plan to eliminate potential damages to flood prone areas therefore the search for more precise mapping methods is either scientifically sound or practical in reality this study proposed a verified novel combination of swarm intelligence algorithms and deep learning neural networks in mapping the flood susceptible levels in a case study province of vietnam the experiments were done with data that are collectible from open geoportal and can be reproduced in another area with a similar physical condition in general this study has a similar approach to numerous works on the susceptibility mapping of other natural hazards such as forest fire landslide in which historical hazard occurrences are to model the relations with potential physical natural or socio economic factors technically the proposed method is applicable to other hazards if input datasets for such events are relevant and available however the proper determination of driving forces and data collection procedures have decisive power on the overall performance of this model therefore more effort should be spent with diverse datasets from different geographic areas this method is a useful contribution of knowledge in the field of integrating machine learning into the geospatial analysis in risk mapping of natural disasters this trend is promising with the fast growing of optimization algorithms and the vast emergence of geospatial data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research is funded by vietnam national foundation for science and technology development nafosted under grant number 105 99 2016 05 author contributions all authors contribute to the work quang thanh bui conceived and designed the experiments quang thanh bui quoc huy nguyen xuan linh nguyen vu dong pham huu duy nguyen van manh pham performed the experiments quang thanh bui huu duy nguyen van manh pham analyzed the data quang thanh bui huu duy nguyen van manh pham discussed the results quang thanh bui supervised the project and commented on the manuscript 
5862,this study proposed and compared several novel hybrid models that combined swarm intelligence algorithms and deep learning neural network for flood susceptibility mapping lai chau a province in the northwest mountainous region of vietnam was chosen as a case study since it had recently undergone severe flashflood in 2018 for this purpose numerical predictor variables such as topographically derived factors digital elevation model aspect slope curvature topographic wetness index climatic variables rain and hydrological variables stream density stream power index distance to river and multiple remote sensing indices normalized difference vegetation index normalized difference buildup index were used these predictor variables were selected because they are globally collectible and reproducible the performances of these models were evaluated by using common statistical indicators namely root mean square error mean absolute error overall accuracy and area under receiving operating characteristics and the statistical test of differences the results showed that the proposed swarm intelligence models outperformed benchmarked methods namely particle swarm optimization support vector machine random forest in almost all comparing indicators it is suggested that proposed models are more robust than the classifiers which were used for benchmarking and they are good alternatives for flood susceptibility mapping given the availability of dataset keywords swarm intelligence optimization algorithm deep learning neural network flash flood 1 introduction tropical countries are among the most vulnerable regions to the impact of climate change induced disasters that happen in unprecedented magnitude and frequency the prevention of these events is strenuous in the short term but the negative impacts can be mitigated through risk preparedness plans among devastating hazards flashflood is one of the most dangerous events in mountainous areas especially in vietnam where mountain encounters three quarters of the country area flood causes severe damages to people and makes adverse impacts on social economic development across the country particularly to the ethnic communities the management of flood including flood risk prediction requires accurate spatial data and temporal information ouma and tateishi 2014 and effective preparedness plans to minimize loses herold and sawada 2012 jonkman and dawson 2012 the identification of vulnerable areas to the flood significantly contributes to the reduction of damages to human settlements agriculture and livelihood by avoiding more construction and developments in the prone areas the crucial component of the vulnerability assessment model is a susceptible map that predicts the probability of an area to be exposed to the flood driving forces of this hazard are mainly depended on climatic factors physical conditions as well as anthropogenic activities bui et al 2016 ngo et al 2018 zhao et al 2018 currently the relationship between these factors and flood occurrences has been investigated with the support of the development of geo information technology and machine learning techniques most recent research focused on the application of machine learning classifiers and their customized versions of which neural network family support vector machine decision rules and frequency ratios are perhaps among the most usually used techniques examples could be found in the study of cao et al 2016 rahmati et al 2016 rsamanta et al 2018a samanta et al 2018b or in ngo et al 2018 zhao et al 2018 moreover the performances of these classifiers can be improved with the implementation of optimizers for tuning the classifier s parameters in general physical based evolutionary swarm intelligence are three conventional optimization techniques that have been found in numerous applications such as land cover classification bui et al 2019b bui et al 2019c bui et al 2018 landslide risk detection pham et al 2019 forest fire analysis bui 2019 community disease assessment bui et al 2019a for flood study the evolutional approach was mentioned in the studies of bui et al 2018 conventional swarm algorithms such as firefly or particle swarm optimization algorithms were used in the works of ngo et al 2018 razavi termeh et al 2018 sachdeva bhatia and verma 2017 or genetic algorithm in bui et al 2016 with good results however no solution fits to solve all problems bui et al 2018a bui et al 2018b because of the variation of training dataset or the complexity of different geographic regions in this regard every single study tries to find the most appropriate algorithm by contrasting its performance to conventional benchmarked methods in this regard this study verified several novel hybrid models that are based on the recently developed swarm intelligence optimization algorithms mimicking behaviors of grey wolfs gwo mirjalili et al 2014 social spiders sso yu and li 2015 and grasshopper goa saremi et al 2017 for tuning a deep learning neural network for flood susceptibility mapping these algorithms had been investigated to solve global optimization problems of the theoretical benchmarked functions but a few applications to solve real problems is found the diverse properties of real problems require considerable verifications of algorithms to find out the suitable methods at present to solve those problems to the best knowledge of the authors no such examinations of these algorithms have been applied for flood susceptibility mapping therefore the verifications these algorithms are scientifically sound and practically useful for prone countries to natural hazards lai chau a province in the northwest mountainous region of vietnam was chosen as a case study since it had recently undergone severe flash flood in 2018 from literature review eleven numerical predictor variables such as topographically derived factors digital elevation model dem aspect slope curvature topographic wetness index twi climatic variables rain river network stream density stream power index spi distance to river and several multispectral satellite image index such as ndvi ndbi were used these factors were selected because they are globally collectible and reproducible if similar conditions are met the input data was standardized by using two conventional methods namely 0 to 1 normalization and reclassification based on the natural break algorithm the performance of these models was evaluated by using common statistical indicators namely root mean square error rmse mean absolute error mae overall accuracy oa and area under receiving operating characteristics auc and was statistically tested for differences the final result is a susceptible map that was generated from the most optimal model 2 study area and data 2 1 study area lai chau province is located on the northwestern mountainous region of vietnam between latitudes 21 51 and 22 49 and longitudes 102 19 to 103 59 the province topology ranges from 120 m more than 3000 m above the sea level with complex spatial variations several dense river systems run across the province providing a relatively high density at 5 5 6 km km2 typically the province is influenced by tropical monsoon with temperatures at 0 c 33 c and high humidity at around 86 the climate is divided into two distinct seasons rainy and dry seasons in which the rainy season usually begins from may to september when 80 of annual precipitation is recorded and the dry season begins from november to may of the next year in lai chau approximately 35 of natural land is forested including natural forest and plantation forest agricultural land covers about 7 and the rest are for other types deforestation and forest degradation continue to be critical threats in the province forest cover consequently the loss of forest cover leads to an increased risk of hazards such as flashflood and landslides recently the study area had undergone severe impacts of these double disasters in 2018 that caused significant loss of humans and massive loss of economic properties therefore a flood susceptible map is necessary for disaster prevention or quick response programs among all districts in the province a border district in the northwestern part of lai chau was selected to verify the proposed models fig 1 2 2 flood inventory mapping the historical flash flood locations could be determined by comparing two images that had been captured before and after the investigated events in 2018 unprecedented rains had caused severe floods in the vast areas in the northwestern mountainous regions of vietnam the local authorities of lai chau province had reported a peak of the flood on june 24th 2018 when vast damages of community infrastructures were reported based on this information two sentinel 1 scenes before and after the flood events were selected to determine flood areas by comparing backscattering between two dates the level1 ground range detected grd products interferometric wide swath mode were downloaded from the copernicus open access hub the data cover an area of 250 km swath with a resolution of 5 m 20 m detail specification of sentinel data is described in table 1 the processing was implemented by using snap open source software that is downloadable at http step esa int the procedure started with the calibration for vv band to convert pixel values to backscatter coefficient values followed by the speckle filtering to reduce noise then the determination of flooded area was implemented by defining a threshold to distinguish water land values the conversion of results from previous steps to binary raster 1 for water and 0 for non water was carried out the results were further processed with terrain correction by using dem from 1 50 000 topographic map which was produced edited in 2017 and georeferenced to utm wgs 84 projection the final results were compared to derive the inundated areas in muongte district additionally historical flood locations from field surveys were also added to build up the input database 2 3 predictor variables from literature there is no specific requirement on which predictor variables should be used and the selection of the variables depends on data availability and specific local conditions bui et al 2018a in general the risk of flood hazard is subject to changes of physical topography in mountainous regions and variation of elevation in built up areas in cities in simulating water flow in elevated areas it is assumed that water runs to steepest slopes which are calculated from terrain models in this sense the topography is the first element to be mentioned and it is represented by a 1 digital elevation model dem and dem derivable maps such as 2 slope 3 aspect 4 curvature and 5 topographic wetness index twi twi quantitatively measure the soil moisture and is a good indicator of land cover conditions typically floods are caused by the trajectory of water flow in rivers several river related indexes were proposed namely 6 river density 7 distance to river 8 stream power index spi to measure the influence of mountain stream network cao et al 2016 products from remote sensing are another useful source to monitor the periodical surface conditions over an area particularly flood related variables such as land cover some commonly used indexes have been investigated and were proposed to use in this study including 9 normalized difference vegetation index ndvi which is one of the most common indicators to measure spatial variation of vegetated areas and 10 normalized difference built up index ndbi which defines the status of build up and bare soil areas those indexes were calculated from the surface reflectance product of the landsat 8 oli operational land imager scene on feb 12th 2017 which was downloaded from https earthexplorer usgs gov in practical there are several other indices that can be used for this study such as normalized difference water index or normalized difference moisture index but there is a certain level of correlation between these indices and ndvi ndbi therefore ndvi and ndbi were kept for further processes the measurement units are different among predictor variables and in some cases normalization might be applied indeed there is no requirement for normalization of input data but sometimes model performances get boosted if proper rescaling methods are applied the two most common techniques are 1 natural break algorithms that had been used to reclassify variables into a limited number of classes in flood risk mapping bui et al 2016 ngo et al 2018 this method determines the best arrangement of input data and assigns value ranges to classes 2 conversion of input data into 0 1 value range bui et al 2018b new values are calculated by the equation x n e w x x m i n x m a x x m i n and this conversion keeps the original distribution of input datasets for the verification of proposed models two normalized datasets by using these methods were applied in which each variable in the first dataset was classified into classes as shown in table 2 3 methods the susceptible map is generated with an assumption that events that happened in the past will happen in the future if similar conditions come this type of map calculates the probability of a location to be exposed to flash floods the study area is represented by a raster 30 m 30 m and each pixel is assigned the likelihood from 0 to 100 or 0 1 in which areas that are not inundated are given the probability of 0 and inundated areas are assigned to 100 additionally apart from historically flooded areas as described in flood inventory mapping section the same amount of points in non inundated areas were randomly chosen to make up a whole set of 4812 point layer for the training and validation dataset this point layer was overlaid to 11 predictor variable layers and the attributes were extracted the overall workflow is shown in fig 2 and can be explained as follows step 1 this step includes the pre processing of satellite data calculating satellite index images estimating landuse derived maps and interpolating climatic maps from meteorological data the historical flood points are identified by using sentinel 1a images in combination with field surveys including flood and non flood and are assigned values from eleven variables all data are produced converted resampled into a raster format of similar spatial resolution 30 m 30 m in utm wgs 84 projection step 2 the input data are randomly divided into a training dataset 70 to train the proposed models and the remaining is used to validate test the performance of the model in this step deep learning neural network dnn acts as a classifier and swarm intelligence algorithms which are described in the following sections are used to search for connecting weights of dnn step 3 by running the validated models for the entire study area the flood susceptible map is generated by calculating the probability of each location to be exposed to the risk finally the value range is reclassified by using natural break algorithm to the qualitative classes such as very low low moderate high and very high 3 1 deep learning neural network dnn belongs to the artificial neural network family and is characterized to have more than a single hidden layer it is the feed forward network and usually trained by using the back propagation method but the increasing of hidden layers makes the network hard to train because of different learning speeds in hidden layers dnn has been successfully implemented in a variety of applications mainly in automatic image recognition speech recognition language processing and several applications in remote sensing were found in the studies of kemker et al 2018 lanaras et al 2018 li et al 2018 technically there is no rule of thumb on how many hidden layers and the number of neurons in each layer because the decision is subject to the complexity of problems and condition of the training dataset since the relation between flood occurrences and diving forces is complex bui et al 2016 zhao et al 2018 three hidden layers were proposed the structure of a dnn is shown in fig 3 having one input layer of eleven predictor variables three hidden layers and one output layer representing exposure probabilities the output values are reclassified into qualitative classes basing on predefined value ranges technically this dnn is characterized by four weight matrixes that have the sizes of n 11 n n n n and n 1 in the sequential order respectively the weights are trained by the proposed optimization algorithms for each training step the sigmoid activation function equation 1 was employed and rmse as shown in equation 2 was used as the objective function 1 f x 1 1 e x 2 rmse 1 n 1 n predicted i observed i 3 2 swarm intelligence algorithms the meta heuristic algorithm is a fast growing and active research area to catch up with changes in the nature of problems particularly with increasing data size swarm intelligence optimization algorithms are developed basing on the behaviors of insect swarm in the social arrangement or hunting strategies the fast growing of this type of algorithm helps in solving many science and engineering applications and there is a considerable need to verify the potential uses of new algorithms in real complex problems social spider optimization sso grasshopper optimization algorithm goa grey wolf optimization gwo are three newly developed algorithms and had been theoretically examined but the practical application of those are still limited this study investigated the potential uses of these algorithms in spatial distribution earth surface occurrences in general the pseudocode for all of those are as follows initialize the swarm in d dimensional space these dimensions are predictor variables initialize algorithm parameters e g swarm population maximum iteration calculate fitness value for each search agent and randomly define the preliminary best position t while t than maximum iteration update the position of all swarm agents by each algorithm specific equation recalculate fitness for each agent update best position t if there is a better value t t 1 break if desirable rmse is met end while loop 3 2 1 grey wolf optimization grey wolf society has a hierarchical structure forming a pyramid with the most dominant powerful wolfs at the top and remaining wolfs are in descending importance at lower positions gwo was firstly introduced by mirjalili et al 2014 and was successfully implemented in the forest fire study of bui 2019 grey wolf behaviors are mathematically modeled by mimicking the social hierarchy prey searching exploration process prey encircling and hunting and prey attacking activities exploitation process like other swarm based algorithms search agents wolfs are randomly positioned in the d dimensional search space and updated their positioned and the fitness values after each iteration the final location of the top wolf is the optimal solution for solving the raised problem 3 2 2 grasshopper optimization algorithm goa was developed by saremi et al 2017 and was verified by neve 2017 for solving both constrained and unconstrained problems it had also been tested with a shallow neural network in the study of bui et al 2018b for land cover classification grasshopper behavior is modeled in both nymph and adulthood stages to form the population grasshoppers search agents move in a swarm in searching for food fitness value starting with abrupt movement in the exploration stage and locally focus on exploitation search for each change grasshoppers interact with others in the swarm and the combination of the current position global best and position of other agent s influences on how the next position is updated this searching procedure is how goa is found different from conventional pso and probably makes goa more powerful than pso in solving the non linear problems 3 2 3 social spider optimization algorithm sso is a new meta heuristic optimization algorithm that has been proposed by yu and li 2015 by investigating the food foraging behaviors of spiders or so called agents in nature spiders live in groups and interact with each other for searching for food a web is described as a d dimensional search space and each node on the web is considered a feasible solution to the optimization problem on the other hand spiders act as search agents and move on the web from node to node vibrations at sources are generated after each movement of spiders and they are propagated over the web based on the attenuated intensity of vibration spiders receive from the other the fitness values are recalculated technically this foraging strategy makes sso different from conventional swarm algorithms in searching strategy spiders are influenced by current positions of all other and their historical positions rather than solely sticking to their positions and the global best positions as explained in pso also information attenuation by distance is another aspect that shapes how information is propagated 3 3 accuracy assessment the comparison between the performances of classification algorithms is carried out through confronting conventional statistical indicators of the proposed model with similar values conducted by benchmark methods these suggested methods were verified against shallow neural networks ngo et al 2018 dnn optimized by gradient descent algorithm gd and particle swarm optimization pso sachdeva et al 2017 which have some specified level of similarity to proposed methods besides two conventional classifiers such as support vector machine svm and random forest rf were also introduced as they had been widely used in solving non linear problems such as susceptible mapping of disasters bui et al 2018a bui et al 2016 these three methods run with the similar dataset and the results were compared by using several indicators namely 1 root mean square error rmse is used as the objective function 2 mean absolute error mae measures the error that is similar to rmse but takes some aspects of proportional distribution 3 overall accuracy oa to reveal the percentage of correct classification over total number of samples 4 receiver operation characteristics roc plots true positive rates against false positive rates and area under roc auc auc value ranges from 0 1 in which random value is equal to 0 5 the higher the auc is the more robust the model performs but regularly auc much exceeds 0 5 to be considered the proper method 4 experiments and discussion 4 1 initialization of model conventionally in running classifiers with data in multiple or hyper dimensional space the feature selection procedure is indeed a crucial step q t bui pham et al 2019 data redundancy increases the cost of data collection and manipulation and might also influence the performance of the predictive models classifiers are unable to separate classes with few variables but usually fall into over fitting situation if predictor variables are large even though there were only 11 predictor variables in this study but we did evaluate them by using the method as implemented by hong et al 2016 through assessing mean decrease accuracy mda and mean decrease gini mdg with random forest algorithm in which out of bag error oob is used to rank the importance of variables this method is simple but informative and can be a good start before moving to the more complicated step fig 4 shows mda on the left and mdg on the right hand side from fig 4 it can be seen that ndvi had the highest mdg value and was ranked as the most critical factor ndvi provides the surface condition of the study areas as most of the flood locations were found in the bare land dem and distance to river also have essential contributions to the model as they are one of the most dominant factors in determining how water flows in the river basin the importance levels of the remaining predictor variables slowly decreased and the precipitation was found having fewer influences on the determination of flooded area it could be understood that the rainfall was almost homogeneous due to the sparse distribution of metrological stations in the study areas and there was no significant difference between regions of the highest precipitation and regions with the lowest quantity statistically all variables have higher values than zero which means all can be used to train the proposed model other crucial steps are the identification of model parameters to ensure the replicability of the experiments in different locations general settings are listed in the first column of table 3 and they are specified per each algorithm the search for optimal values was done from the literature review and through trial and error with the training dataset svm performance was characterized by two standard parameters namely kernel width γ and the regularization c and grid search was used to determine those parameters rf is an ensemble method and does not have tunable parameters except the predefined number of trees 500 trees are large enough with the current training dataset 4 2 results from experiments dnn structure is differenced from a shallow network by the depth of hidden layers and the number of neurons in each layer the more layers dnn has the harder it is to train the network a network of three hidden layers was proposed and several choices of neuron nodes were examined usually the neurons should be smaller than the inputs and higher than the expected outputs in this sense the experiments were run with 10 9 8 7 and 6 neurons in each hidden layer respectively and the statistical errors were used to compare the results the experiments were implemented with datasets from two conventional normalization methods table 4 showed the rmses using eq 2 of the validation dataset which was used as the termination for the iteration process after 3000 iterations of the proposed methods in which the dataset with natural break standardization methods almost generated smaller values than those in the other dataset it could also be seen that sso seemed to perform better rmses 0 224 for the training dataset and 0 227 for validation dataset with ten neurons in every hidden layer and there is no clear correlation between the number of neurons and rmses other performance indicators are also shown in table 5 besides examples of rmse variations of goa sso and gwo shaded cells in table 4 were plotted in fig 5 that shows the search patterns of proposed algorithms it could be seen that goa and gwo had a similar searching pattern that rapidly jumped from the beginning to the 500th iteration and gradually decreased until reaching maximum repetition this strategy begins with the exploration stage aiming to search for promising regions and then is followed by exploitation search to find an accurate approximation of global best value on the other hand sso took aggregated signals from all other searching agents spiders and had small jumps in constant combination changes during the search the results from the experiments were compared with three other benchmarked classifiers namely svm rf and gd dnn optimized by gradient descend algorithms by using several statistical indicators svm and rf were chosen for benchmarking since they had been widely used in studies for disaster susceptible mappings bui et al 2018a bui et al 2016 ngo et al 2018 and optimal parameters were presented in table 3 the results showed that swarm intelligence algorithms outperformed the other in all comparing indicators and sso ended up with the highest overall accuracy 96 539 with 10 neurons and natural breal normalization method in this study oa was found high in either training or validation datasets since the inundated areas usually are clustered along with the stream network and the predictor variables predict well the locations of flood hotspots technically the susceptible probability varies between 0 and 100 or between 0 and 1 and 0 5 was defined as the specific cut point to classify instances to either flood higher than 0 5 or non flood smaller than 0 5 fig 6 plotted the validation samples from the sso algorithm with ten neurons in each of the hidden layers in which samples on the right side of the y axis are observed flood locations and those on the left side of the y axis are non flood samples plots in quarters i iv showed true classified samples with almost predicted probabilities are close to 1 or 0 on the other hand there are small numbers of miss classified samples in quarters ii iii one of the challenging issues that all data driven machine learning models face is to minimize or avoid over fitting traditional back propagation is found limitation in coping with local minima and over fitting problem chiroma et al 2020 so that several techniques are being investigated the recent introduction of meta heuristic algorithms had been proposed as alternative optimizers for tuning deep neural networks chiroma et al 2020 fong et al 2018 rere et al 2016 to avoid overfitting parameters of algorithms are adjusted such as velocity and searching space in pso tian and fong 2016 in this study parameters of the verified algorithms were predefined and adjusted through trial and error processes the differences between rmse values from both train data and validation data which were shown in table 4 were small for each algorithm and by each determination of neurons the learning curves from training datasets fig 7 have similar patterns with rmse curves from the validation dataset the unbiased evaluation by the held back validation data showed that over fitting has been significantly overcome and the validated models were ready for mapping flood susceptibility for entire study area since the objective of this study is to verify the potential application of swarm intelligence methods in optimizing parameters of dnn for flood susceptible mapping sdditional statistical test of difference between model performance is required in this regard a wilcoxon signed rank test was carried out for the rmses as showed in table 4 table 6 showed that the rmses from gwo goa sso are statistically differenced from those generated by pso and we failed to reject the null hypothesis that the differences between sso goa and sso gwo equal to zero indeed swarm algorithms have similar searching algorithms mirjalili et al 2014 saremi et al 2017 and with the specific dataset the operation might result in approximated rmses however in comparison to benchmark methods all three models perform better and can be used as alternative methods to generate flood susceptible maps 4 3 model comparison and flood susceptibility map generation the performance of examined models are shown in table 5 in which sso goa and gwo were investigated with two different training datasets by using two normalization methods the significane test table 6 showed that the performances are statistical significant in which the examined algorithms are better than benchmark methods and are suitable for the dataset of this study the best results from sso auc 97 003 goa auc 96 798 and gwo auc 96 751 are satisfactory in comparison to previous studies razavi termeh et al 2018 by using the combination of adaptive neural fuzzy inference system adaptive nfis and pso with auc 94 5 bui et al 2016 by itegrating metaheuristic optimization into nfis with auc 0 962 bui et al 2018a with auc 94 7 from adaptive nfis optimized imperialistic competitive algorithm and some others rahmati et al 2016 s samanta et al 2018 in fact the power of any model is subject to specific problems or specific training datasets wolpert and macready 1997 and it could be seen that the proposed model in this study perform well over referenced flashflood susceptibility mapping studies the application of meta heuristic algorithms in optimimizing parameters of classifiers for flood modelling or for natural hazard analysis in broader perspective is increasing with the fast growing development of new algorithms because there is no algorithm to solve all optimization problems in this study the verification of several algorithms that are inspired from animal behaviors enrich modelling methods for similar works in other study areas from the analysis of experiment results and model comparision sso was chosen to produce susceptible for the visualization of spatial distribution of areas with potential flooded risk in muongte district fig 8 the output values were reclassified by using the natural break algorithm to assign value range to the verbal representations of flood susceptible levels the qualitative classes are very low 0 0 091 low 0 092 0 26 moderate 0 27 0 49 high 0 50 0 76 very high 0 77 1 the hotspot of flood risk clustered along the river from the practical experiments it is well noted that the frequency of landslide occurrences in a similar location is rare but the frequency that areas get inundated is high since the influential factors are unchanged from time to time the final susceptible map provides a scientifically based reference for the disaster preparedness plan for the local communities when rainy seasons come 5 conclusion swarm intelligence is a powerful optimization group to solve global minimization or maximization problem in real applications in this study three newly developed algorithms were examined with a dataset of 11 predictor variables and the results were compared to several benchmarked methods with common statistical indicators the proposed methods significantly improve the performance of dnn by generating smaller rmse as the objective function than those produced by svm rf pso or gradient descent algorithm however the goodness of these hybrid methods was determined through the proper identification of flood areas by using sentinel data adequate processing of predictor variables and initialization of model parameters these processes have a significant contribution to the success of this research flash flood is a destructive natural event that destroys society s infrastructure and causes enormous loss to humanity in the context of climate change the situation is worsening when extreme climatic events happen in an unprecedented way accurate flood susceptible maps are good sources to support disaster preparedness plan to eliminate potential damages to flood prone areas therefore the search for more precise mapping methods is either scientifically sound or practical in reality this study proposed a verified novel combination of swarm intelligence algorithms and deep learning neural networks in mapping the flood susceptible levels in a case study province of vietnam the experiments were done with data that are collectible from open geoportal and can be reproduced in another area with a similar physical condition in general this study has a similar approach to numerous works on the susceptibility mapping of other natural hazards such as forest fire landslide in which historical hazard occurrences are to model the relations with potential physical natural or socio economic factors technically the proposed method is applicable to other hazards if input datasets for such events are relevant and available however the proper determination of driving forces and data collection procedures have decisive power on the overall performance of this model therefore more effort should be spent with diverse datasets from different geographic areas this method is a useful contribution of knowledge in the field of integrating machine learning into the geospatial analysis in risk mapping of natural disasters this trend is promising with the fast growing of optimization algorithms and the vast emergence of geospatial data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research is funded by vietnam national foundation for science and technology development nafosted under grant number 105 99 2016 05 author contributions all authors contribute to the work quang thanh bui conceived and designed the experiments quang thanh bui quoc huy nguyen xuan linh nguyen vu dong pham huu duy nguyen van manh pham performed the experiments quang thanh bui huu duy nguyen van manh pham analyzed the data quang thanh bui huu duy nguyen van manh pham discussed the results quang thanh bui supervised the project and commented on the manuscript 
5863,the performance of the noah land surface model lsm with multi parameterization options noah mp in simulating snow depth was evaluated in northern xinjiang china a total number of 13 824 noah mp physics ensemble simulations were conducted at the altay site by combining different parameterization schemes of physical processes while disregarding the uncertainties of forcing data and model parameters the natural selection approach and tukey s test which are two sensitivity analysis methods were used to analyze the sensitivity of snow to parameterization schemes then the uncertainty intervals of the ensemble simulation experiments were compared according to the results of the sensitivity and uncertainty experiments snow depth could be simulated by three typical combination schemes at the regional scale the longest snow melting time scheme lt the shortest snow melting time scheme st and the default combination scheme dt observation data of snow depth from thirty nine meteorological stations in northern xinjiang were used to evaluate the snow simulation performance of typical combination schemes the simulation performances of the three typical combination schemes were examined and compared in groups that were divided according to elevation and land cover the results demonstrated that the simulation results of snow depth and snow water equivalent swe were sensitive to four of the eleven physics options within noah mp the exclusion of the parameterization schemes that notably reduced the simulation performance in the sensitive physical processes can significantly reduce uncertainty snow simulation performances of three typical combination schemes were diverse in northern xinjiang china no single scheme performed best at all sites but the length of the snow melting phase exhibited the best performance keywords multi parameterization ensemble simulation sensitivity analysis typical combination scheme noah mp 1 introduction snow cover is one of the most active land surface types and has a great influence on the energy balance and water circulation processes the high albedo of snow cover areas results in increased reflection of solar radiation and makes the heat flux interactions between the land surface and the lower atmosphere unique chen et al 2017 wrzesien et al 2015 moreover snow cover can also strongly influence the climate and human living environment and melt water is a significant component of the annual water budget in terms of both soil moisture and runoff which plays a critical role in flood generation in snow dominated basins dai et al 2018 durand and margulis 2006 verbunt et al 2003 in addition excessive or untimely snow cover may result in hazards such as spring flooding and bursts of glacier meltwater fed lakes han et al 2019 barnett et al 2005 piazzi et al 2018 xu et al 2017 therefore appropriate descriptions and correct simulations of snow processes have become extremely important and accurate predictions of the temporal and spatial variations in snow cover and snowmelt runoff processes have become increasingly important as a crucial variable in snow processes snow depth is an indicator of climate change and can have a positive feedback effect on climate systems chio et al 2010 snow depth is also an important parameter for studying the water balance in watersheds and simulating snowmelt runoff and snow depth has received much attention in the fields of hydrological and meteorological research regonda et al 2005 boniface et al 2015 xu et al 2017 however snow depth observation data obtained from meteorological sites cannot satisfy the data requirements of quantitative hydrology studies in addition it is difficult to correctly depict the spatial distribution of snow cover using observation data at the site scale because the scope of situ observations is always limited andreadis and lettenmaier 2006 liu et al 2013 although the observation data of snow depth that are obtained using remote sensing technology can cover large areas especially in regions that have no human footprint temporally continuous observation data cannot be obtained and the accuracy is often constrained by the algorithms required to fill in cloud covered areas thirel et al 2013 zhang et al 2014 land surface models lsms have continuously evolved according to the requirements of atmospheric and hydrological disciplines seneviratne et al 2010 niu et al 2011 and can effectively simulate snow processes compared with observation methods the spatial and temporal distributions of snow cover can be widely and easily determined by lsms in the past several decades models have often been employed to reconstruct snowpack patterns and explore the variations in snow cover for example wrzesien et al 2015 combined the weather research and forecasting wrf regional climate model with the noah mp lsm to simulate the snow cover fraction scf and snow water equivalent swe in a 3 km domain over the central sierra nevada mountains and the results indicated that wrf with the noah mp lsm can simulate the scf in mountainous environments to within 22 26 uncertainty several studies have explored the interactions between snow and vegetation with models used to reconstruct snow cover and vegetation essery 2013 thackeray et al 2014 loranty et al 2014 the analysis results revealed that snow albedo can be highly impacted by vegetation the canadian land surface scheme class was run over a domain centered over eastern canada for the period from 1990 to 2011 and the snow simulation performance of class was assessed at a large scale verseghy et al 2017 which indicated that the scf can be well simulated and that the modeled albedo has low bias all of these studies demonstrated that models can be an efficient approach to reconstruct snow cover and evaluations of the snow simulation performances of models can promote regional climate research noah mp has been widely used to simulate various land surface variables gao et al 2015 zhang et al 2016 cai et al 2014 and the simulation performance of noah mp has been evaluated at both point and watershed scales niu 2011 yang et al 2011 in a study on the snow process niu et al 2011 reported that noah mp considers a three layer snowpack and thus provides better estimates of observed snow depth snow density and swe compared to the noah lsm kuribayashi et al 2013 estimated the swe of snowpack in central japan from september 2006 to august 2008 using a 3 3 km mesh regional climate model with the noah and noah mp models the results indicated that noah mp could simulate the spatiotemporal variations in swe better than noah lsm which underestimated swe and the difference in swe between the lsms was particularly high under warm conditions the noah mp model was also systematically assessed over the continental united states for the simulation of water and energy fluxes and the results revealed that noah mp generally better captured the observed seasonal and interregional variability in net radiation scf and runoff compared to other variables ma et al 2017 although there are many examples of using noah mp to reconstruct snow cover minder et al 2016 xia et al 2017 tomasi et al 2017 the performance of noah mp in simulating snow depth has not been evaluated in snow rich areas in china such as the northeast region and northern xinjiang in this study we attempt to assess the performance of noah mp in simulating snow depth and reconstructing snow cover over northern xinjiang in china first the sensitivity of snow to the parameterization schemes within noah mp was explored using a meteorological observation dataset from the altay site then two extreme combination schemes obtained from sensitivity and uncertainty experiments and the default parameterization scheme combination were selected to simulate snow depth at the regional scale in northern xinjiang third the effectiveness of the conclusion of the sensitivity experiment was examined and the snow simulation performances of the three typical combination schemes were evaluated this article is organized as follows section 2 describes the materials and methods including the study area forcing data observation data and the introduction of the parameterization scheme combinations section 3 explains the results of snow depth simulations using the typical combination schemes section 4 summarizes the findings of this study 2 materials and methods 2 1 study area the study area is located in northern xinjiang 79 0 92 0 e 42 0 50 0 n and comprises the northwest border area of china the altay mountains are the northernmost region in this area with the highest elevation of 4370 m and the tianshan mountains are the southernmost region in this area with the highest elevation of 7440 m the central area is the zhunger er basin with an average elevation of 400 m the terrain of northern xinjiang is shown in fig 1 located in the hinterland of the eurasian continent this area is characterized by a typical temperate continental climate xu and shu 2014 due to the influence from siberian circulation this area has a short warm and rainy summer and a long cold and snowy winter the duration of snow cover in this region is approximately 120 days from november to march huang et al 2011 xu and shu 2014 because this region has rich snow resources it is well suited for the implementation of snow simulation experiments the altay site investigated in this study 47 44 n 88 05 e is located in northern xinjiang in northwestern china fig 1 the elevation of the meteorological observation field is 735 3 m and this field is less than 5 km from the altay mountains this site is a typical meteorological observation station in northern xinjiang and has a complete and eligible high quality meteorological observation dataset in addition this site has less wind in the winter season therefore a sensitivity experiment on snow under various parameterization schemes was conducted at this site 2 2 atmospheric forcing and observation data at the altay site meteorological observation data from october 1 2014 to september 30 2015 including wind speed and direction air temperature relative humidity precipitation pressure snow depth and swe were obtained from the china meteorological administration cma according to the necessities of model forcing four components of radiation downward and upward longwave and shortwave radiation were obtained using a four component net radiometer snow depth was manually obtained with daily resolution and swe was obtained with six hourly resolution by using a gmon gamma monitor detector however swe observation data from november 15 2014 first snowfall during the observation period to january 20 2015 are missing because the gmon detector broke down at the beginning of the snow period for the regional snow depth simulation the forcing data from october 1 2012 to september 30 2013 in this study developed by the hydrometeorological research group at the institute of tibetan plateau research at the chinese academy of sciences itpcas were used to drive noah mp at a regional scale in northern xinjiang including seven essential meteorological variables pressure precipitation wind speed specific humidity near surface air temperature downward shortwave radiation and longwave radiation this dataset was produced by merging the observations collected at 740 operational stations of the cma with the corresponding princeton meteorological forcing data the global land data assimilation systems gldas reanalysis forcing data the tropical rainfall measuring mission trmm 3b42 precipitation products and the global energy and water cycle experiment surface radiation budget gewexsrb radiation data chen et al 2011 the itpcas forcing data completely cover the geographic scope of china with a spatial resolution of 0 1 and a temporal resolution of 3 h he and yang 2011 to meet the demands of experimental accuracy the micromet approach liston and elder 2006 was adopted to downscale the atmospheric forcing data and is mainly based on the relationships between meteorological variables and the surrounding landscape primarily topography finally a set of forcing data with high spatial resolution 0 05 and high temporal resolution 1 h was obtained for this study area the observations of snow depth from october 1 2012 to september 30 2013 with a daily resolution were from thirty nine meteorological stations in northern xinjiang the spatial distribution of meteorological stations is shown in fig 1 the land cover at the stations is also shown in fig 1 there are five land cover types at these sites cropland grassland urban and built up land barren or sparsely vegetated land and sparse shrubland validation data from these sites were used to evaluate the simulation performance of the three typical combination schemes 2 3 physics ensemble numerical experiment noah mp was developed based on noah v3 0 chen et al 1996 1997 chen and dudhia 2001 ek et al 2003 where vegetation and groundwater dynamics were first incorporated and then multiple parameterization schemes were introduced for various physics options such as runoff radiation transfer and snow albedo niu 2011 yang 2011 every physics option has two to four parameterization schemes for selection table 1 compared with noah noah mp considers a three layer snowpack depending on the total snow depth which allows snow variables to be simulated more accurately the sensitivity of snow to parameterization schemes was studied while disregarding the uncertainties from forcing data and model parameters meteorological measurements with a temporal resolution of 1 h were used as forcing data and the simulation results were evaluated by observation data because noah mp requires a long time to reach soil state equilibrium chen et al 2014 cai et al 2014 gao et al 2015 before it can be used for initialization the forcing data from october 1 2013 to september 30 2014 were used repeatedly to drive noah mp to ensure the equilibrium soil state the equilibrium criterion is defined as the time when the difference in annual means between two consecutive single year simulations was less than 0 1 of the mean cai et al 2014 gao et al 2015 the dynamic vegetation model within the noah mp model was developed mainly to simulate natural vegetation dynamics for vegetation without human interference and due to simplified phenology and assimilated portioning the model provides less reliable simulations of vegetation affected by human activities xia et al 2014 zhang et al 2016 vegetation at the altay site was impacted by human activities such as irrigation and weeding therefore in this study the dynamic vegetation scheme was turned off in the ensemble simulation experiments the physics options for the 11 remaining physical processes table 1 were all included and possibly combined and 13 824 scheme combinations were designed to analyze the sensitivity and uncertainty hereafter ens1 table 2 based on the analysis results of the sensitivity experiment the ensemble simulation experiment ens2 was conducted by possibly combining the schemes of 4 sensitive physical processes there are 24 members in total in ens2 ens3 ens6 were designed to reduce uncertainties in the ensemble simulations and analyze the sources of the uncertainties 2 4 analysis and evaluation methods for the sensitivity experiment at the altay site snow depth and swe were recorded as simulation variables two sensitivity analysis methods were chosen to analyze sensitivity in ens1 natural selection and tukey s test both methods use the root mean square error rmse between the simulations and observations to evaluate the performance of the model simulations the sensitivity of snow to parameterization schemes can be determined and crucial physical processes were extracted based on the analysis results from these two methods the main steps of the two sensitivity analysis methods are described below more details can be found in the work of zhang et al 2016 natural selection first the rmse of each member in ens1 was computed and all rmses were sorted in ascending order then the members with rmses concentrated below the 5th percentile for snow depth and swe were denoted as the best members 692 members and those above the 95th percentile were denoted as the worst members 692 members third the times when a given scheme for each physical process occurred with the best and worst members were counted obviously for a given parameterization scheme a large number of selections with the best members has an advantage in terms of model accuracy similarly a large number of selections with the worst members has a lower advantage in terms of model accuracy the sensitivity of snow to a given physical process can be judged on the macro level by applying this method tukey s test approach as a hypothesis test this method can effectively examine the differences between parameterization schemes in one physical process at the micro level a statistical hypothesis test could be used to compare each pair of population means u i and u j for all i j where the null and alternative hypotheses are 1 h 0 u i u j h 1 u i u j the distribution of the studentized range statistic was used in tukey s procedure 2 q y max y min m s e 2 1 n i 1 n j y max y min s s e 2 n a 1 n i 1 n j where y max is the maximum value of the two parameterization means being compared y min is their minimum value n i and n j are the sample sizes for the ith and jth parameterization schemes respectively m s e is the mean square error s s e k 1 a l 1 n k y kl y k 2 is the sum of the square error n is the total sample size for all schemes and n a is the degrees of freedom associated with m s e according to tukey s test two population means are significantly different if 3 y i y j m s e 2 1 n i 1 n j q α a n a where α is the 0 05 significance level in this study and q α a n a was obtained from the table with the studentized range distribution 2 5 typical combination schemes for snow depth simulation according to the analysis results of the sensitivity and uncertainty experiments in section 2 4 two extreme combination schemes can be identified one has the longest snow melting time the lt scheme and the other has the shortest snow melting time the st scheme as expected the difference between the two combination schemes was mainly focused on the parameterization scheme options for four sensitive physical processes obviously the uncertainty interval of multi parameterization ensemble simulation results can be determined from extreme combination schemes although the st scheme was closer to the observations at the altay site than the lt scheme the simulation performances of the two combination schemes at the other sites are unknown therefore the simulation performance of extreme combination schemes at the regional scale should be further examined the default combination scheme dt within noah mp was the scheme recommended by the model and has been applied in many studies the two extreme combination schemes obtained by the sensitivity experiment and the dt schemes recommended by the model are typical schemes in all 13 824 combination schemes to examine the simulation performance of these three typical combination schemes in this section three were selected to simulate snow depth at the regional scale in northern xinjiang the parameterization scheme options of lt st and dt are listed in table 1 the difference between the st and dt combination schemes is relatively small only the parameterization scheme option of physical process pcp is different however the difference between the st and lt schemes is obvious and the scheme options of all sensitive physical processes are different the calculation equations of the different parameterization schemes for the sensitive physical processes are listed below a surface exchange coefficient for heat sfc 4 c h κ 2 ln z d 0 z 0 m ψ m z d 0 l ln z d 0 z 0 h ψ h z d 0 l m o 5 c h κ 2 ln z z 0 m ψ m z l ψ m z 0 m l ln z z 0 h ψ h z l ψ h z 0 h l original noah chen 97 where c h is the surface heat exchange coefficient κ is the von karmon constant l is the monin obukhov length and z is the reference height z 0 h and z 0 m are the roughness lengths for heat and momentum respectively d 0 is the zero displacement height for the m o scheme both the m o and chen97 schemes take the same stability correction functions ψ m and ψ h for stable and unstable conditions respectively the m o scheme accounts for the zero displacement height d 0 but the chen97 scheme accounts for the difference between roughness lengths for heat and momentum which makes the values of c h significantly different b partitioning precipitation into rainfall and snowfall pcp 6 f p i c e 0 t sfc t frz 1 t sfc t frz 0 5 1 54 632 0 2 t sfc t sfc t frz 2 0 6 t sfc t frz 0 5 jordan 91 7 f p i c e 0 t sfc t frz 2 2 1 t sfc t frz bats 8 f p i c e 0 t sfc t frz 1 t sfc t frz noah where f p i c e is the proportion of snow in precipitation t sfc and t frz are the surface air temperature and freezing point respectively jordan 91 may be the most complicated of the three schemes because of the different ways of dealing with temperature and the results calculated by the above three schemes vary greatly c lower boundary condition of soil temperature tbot 9 e 0 zero flux scheme 10 e e 0 noah in the equation e is the heat flux from the bottom of soil column the zero flux scheme assumes no heat flux into the soil column when describing the physical process of soil temperature boundary conditions however the noah scheme is based on the assumption that there is an existing heat flux at the 8 m soil depth e 0 was caculated according to soil temperature at the depth of the lower boundary condition and the bottom temperature of soil column the caculation result of heat flux was also decided by soil thermal conductivity the bottom depth of soil column and depth of lower boundary condition these two assumptions make the calculation for soil temperature different which influences the snow cover melting process d snow and soil temperature time scheme temp semi implicit and fully implicit are two options in the temp physical process that were used to solve the thermal diffusion equation in snow and soil layer eq 11 is the thermal diffusion equation used in the noah mp model where t is the time z is the soil snow depth t is the soil snow temperature c is the soil snow heat capacity k t is the soil snow thermal conductivity coefficient at time t although both semi implicit and fully implicit are discretization methods for solving the thermal diffusion equation within the noah mp model the impact of the two options on the snow simulation result is marked 11 c t t z k t t z 2 6 evaluation methods for typical combination schemes the model performance was quantified using statistical analysis based on the daily mean values of simulations and observations certainly the observations are the snow depths measured at thirty nine meteorological stations from october 1 2012 to september 30 2013 model performance was evaluated using the rmse and nash sutcliffe efficiency nse coefficient 12 rmse 1 n i 1 n o b s i s i m i 2 13 nse 1 i 1 n o b s i s i m i 2 i 1 n o b s i obs 2 where n is the total number of observations s i m i is the simulated value at time i o b s i is the observed value at time i and obs is the mean of the observed data the rmse ranges from 0 to the smaller the rmse value is the closer the simulation is to the observation which means the simulation has high precision the nse indicates how well the scatter plot of the observed versus simulated data fits the 1 1 line and ranges between and 1 a nse 1 indicates a perfect fit according to previous studies nse 0 65 indicates unsatisfactory model performance 0 65 n s e 0 8 indicates acceptable model performance 0 8 n s e 0 9 indicates good model performance and nse 0 9 indicates very good performance ritter and munoz carpena 2013 moriasi et al 2007 3 results and discussion 3 1 sensitivity analysis of snow to physics options 3 1 1 natural selection results the rmses for snow depth and swe by 13 824 combination schemes at the altay site were calculated and sorted in ascending order members concentrated below the 5th percentile of the rmse for snow depth and swe were considered the best members and those above the 95th percentile were considered the worst members where the frequency of each scheme was counted for the two groups above the horizontal axis in fig 2 a and b the frequency of each parameterization scheme is shown for the best members and the frequency of the worst members is below the horizontal axis using the canopy resistance process crs table 1 in fig 2 a as an example the selected frequency of crs scheme 1 hereafter crs 1 for the best members is 0 43 and the frequency of crs 2 is 0 57 indicating a 43 combination scheme for all 692 members using crs 1 and a 57 combination scheme using crs 2 with the best members however the selected frequencies of crs 1 and crs 2 with the worst members are 0 47 and 0 53 respectively indicating that 47 of the 692 worst members use crs 1 and 53 of those use crs 2 the difference in the selected frequency with the best members implies that using crs 2 provides a greater chance of producing favorable simulations than using crs 1 however the selected frequency of the two schemes with the worst members ball berry 0 47 jarvis 0 53 implies that crs 2 has a much better chance of producing worse simulations than crs 1 although it is difficult to identify the better scheme from the analysis results the frequencies of crs 1 and crs 2 imply that the simulation results are not sensitive to crs fig 2 shows that the difference in the selected frequencies of the parameterization schemes in the physical processes of crs btr run inf rad and alb are not significant for either the best or worst members consequently the simulation results of snow depth and swe are not sensitive to these physical processes the differences in the selected frequency of the parameterization schemes in sfc frz pcp tbot and temp are significant particularly in the parameterization schemes for sfc and temp the selected frequencies of sfc 1 and temp 1 are 100 and the selected frequencies of sfc 2 and temp 2 are zero with the best members however with the worst members the selected frequencies of sfc 1 and temp 1 are zero and the selected frequencies of sfc 2 and temp 2 are 100 this result implies that sfc 1 and temp 1 have a much better chance of producing favorable simulations while sfc 2 and temp 2 are more likely to produce unfavorable simulations in the ensemble simulation experiments of both snow depth and swe the difference between parameterization schemes in these physical processes is significant the simulation results are more sensitive to sfc and temp than pcp and tbot sfc 1 and sfc 2 used the same stability correction function ψ m and ψ h for stable and unstable conditions to calculate the surface heat exchange coefficient c h chen et al 1997 niu et al 2011 sfc 1 is based on the more general monin obukhov similarity theory and sfc 2 selects the calculation method from noah v3 as seen in eqs 4 and 5 respectively the most important difference between these two schemes is that sfc 1 accounts for the zero displacement height d 0 but sfc 2 accounts for the difference between roughness lengths for heat and momentum the difference between the two schemes greatly impacts the c h value energy and water balance the results of the snow depth and swe show that the performance of sfc 1 is better than that of sfc 2 which is for the most part consistent with the conclusions in hong et al 2014 and zhang et al 2016 when soil is frozen the temperature is below the freezing point but water near soil particles remains in liquid form due to soil capillarity in the snow depth ensemble simulation experiment the selected frequency of frz 1 is 0 83 with the best members and frz 2 is 0 78 with the worst members in the swe ensemble simulation experiment the selected frequency of frz 1 is 0 72 with the best members and frz 2 is 0 84 with the worst members the statistical results imply that the performance of frz 1 is better than that of frz 2 in both the snow depth and swe ensemble simulation experiments frz 1 takes a more general form of the freezing point depression equation niu and yang 2006 while frz 2 exhibits a variant of the freezing point depression equation including an extra term 1 8 θ ice 2 niu et al 2011 compared to frz 1 frz 2 produces more liquid water in soil the use of the appropriate parameterization scheme to divide precipitation into two parts rainfall and snowfall can directly influence the snow depth simulation results the difference between the selected frequencies of pcp 1 and pcp 2 is not obvious but the selected frequency of pcp 3 is much larger than those of pcp 1 and pcp 2 with the best members in both the snow depth and swe ensemble simulations pcp 3 is implied to have a chance to produce a better simulation than pcp 1 and pcp 2 this physical process uses the surface air temperature t air as a criterion where pcp 3 simply assumes that all precipitation is snowfall when t air t frz otherwise it is rainfall when t air frequently varies around the freezing point the modeled snow accumulation is very sensitive to the parameterization scheme of this process niu et al 2011 at the altay site temperatures vary widely around the freezing point in the stage of snow melting therefore the simulation results are sensitive to these parameterizations tbot 1 assumes no heat flux into the soil column when describing the physical process of the soil temperature boundary conditions tbot 2 is based on the assumption that there is an existing heat flux at the 8 m soil depth these two assumptions make the calculation of soil temperature different which influences the snow cover melting process the selected frequencies of tbot 1 and tbot 2 are very close with the best members additionally the selected frequency of tbot 1 is much higher than that of tbot 2 with the worst members in both the snow depth and swe experiments although the difference between the two parameterizations is obvious in fig 2 we did not identify the ultimately better scheme the differences between temp 1 and temp 2 are significant in both the snow depth and swe ensemble simulations the frequencies of the two parameterizations are opposite when the best members and worst members are used similar to sfc snow simulaions are also highly sensitive to physical process temp 3 1 2 tukey s test results in this section tukey s test was used to examine the differences between parameterization schemes first the rmse of each combination scheme for the snow depth and swe ensemble simulation groups was calculated resulting in a total of 13 824 rmses for both the snow depth and swe groups all 13 824 of the rmses were independent from one another and the assumptions of normality and equality of variances were checked before adopting tukey s test for example physical process btr has 3 parameterization schemes and each parameterization scheme was used in 4608 combination schemes therefore 4608 rmse values correspond to each scheme the quartile distribution of the rmse for each scheme for the 11 physical processes is shown in the subfigures in fig 3 from top to bottom in fig 3 schemes that do not share the letter a behave significantly different and those with the letter b outperformed those with the letter a because their mean rmse values were much smaller obviously the smaller the mean value of the rmse is the better the scheme performance the simulation results are sensitive to a physical process depending on whether the difference between parameterization schemes is significant consequently a physical process can be regarded as insensitive when all of its schemes share a common letter meanwhile when different letters are assigned to any two schemes it is sensitive fig 3 for example sfc 1 was labeled with the letter b and sfc 2 was labeled with a in both the snow depth and swe ensemble simulation experiments the mean value of the rmse for sfc 1 was smaller than that for sfc 2 therefore the difference between sfc 1 and sfc 2 was significant and sfc 1 was more likely to produce advantageous simulations than sfc 2 in the snow depth and swe ensemble simulation experiments this result is consistent with the conclusion of natural selection the simulation results of the snow depth and swe deemed that the sfc was sensitive the results also indicate that the differences between the parameterization schemes of crs btr run inf rad frz and alb are not significant but the differences between the parameterization schemes of sfc tbot and temp are significant in both the snow depth and swe ensemble simulation experiments the difference between the parameterization schemes of pcp is significant in the snow depth ensemble simulation experiment but not in the swe experiment the distributions of the rmse values of the three parameterization schemes are roughly consistent in fig 3 we concluded that snow depth is sensitive to pcp and swe is insensitive to pcp but the sensitivity is relatively weak the results of tukey s test are roughly equal to the results of natural selection however the selection frequencies of frz 1 and frz 2 have large differences in both the best members and worst members the simulation results of snow depth and swe should be sensitive to frz based on convention but there is no evidence that shows existing significant differences between frz 1 and frz 2 according to tukey s test the rmse distributions of the two parameterization schemes are very similar indicating that the results had almost no differences additionally the sensitivity of pcp is relatively weak compared with those of sfc tbot and temp the rmse distribution difference between the three parameterization schemes is not obvious however the difference in the frequency selected by the natural selection method is significant as shown in fig 2 which is partially due to the natural selection method considering only the best and worst members approximately 692 and because it is only a part of the total simulation results not the results of all of the members in contrast tukey s test considers the means of all 13 824 members therefore this method is more convincing for these reasons we concluded that the simulation results of snow depth and swe were insensitive to frz and sensitive to pcp the mean rmse values have obscure differences suggest that a majority of the physical processes are not significant as shown in fig 3 nevertheless the difference between the parameterization schemes of sfc and temp is still highly significant and the mean rmse values of the schemes are significantly different paired with these factors we declared that the simulation results of snow depth and swe were highly sensitive to sfc and temp compared to other physical processes and possibly the main factors causing uncertainties in the ensemble simulation results based on the above analysis results we concluded that the simulation results of snow depth and swe are sensitive to the physical processes of sfc pcp tbot and temp where the sensitivities of sfc tbot and temp are relatively significant and that of pcp is relatively weak 3 2 uncertainties in physical parameterization schemes the sensitivities of the parameterization schemes were analyzed by natural selection and tukey s test in section 3 1 the simulations are highly sensitive to sfc and temp as shown in figs 2 and 3 respectively and could serve as the main sources of uncertainty based on the analysis results of these two methods this section further studies the uncertainties in the ensemble simulation experiments of snow depth and swe ens2 was conducted to explore the uncertainty interval of the sensitive physical process ensemble simulation experiment in this experiment forcing data that were the same as ens1 and the schemes of four sensitive physical processes sfc pcp tbot and temp were possibly combined for a total of 24 combination schemes the uncertainty interval of the simulations based on the sensitive physical processes 24 simulations was compared with the uncertainty interval of the simulations based on all combination schemes 13 824 simulations as shown in fig 4 during the snow accumulation period the uncertainty interval is relatively small and the variations in the different combination schemes are almost equal during the snowmelt period the uncertainty interval is relatively large beginning in late march and the differences are mainly correlated with the length of melting time the uncertainty interval of the sensitive physical process ensemble simulations is roughly equivalent to the total ensemble simulations in fig 4 and both the uncertainties of snow depth and swe are concentrated during the snow melting period this result implies that the uncertainties in the ensemble simulation results are mostly from sensitive physical processes thus the selection of appropriate parameterization schemes for sensitive physical processes is crucial for the simulation of the snow melting process to reduce uncertainties in the ensemble simulation experiments further study of the uncertainty sources in the sensitivity process is needed according to the conclusion in section 3 1 sfc pcp tbot and temp are sensitive physical processes in the snow depth and swe ensemble simulations especially sfc and temp in addition the results of the two sensitivity analysis methods indicate that the first parameterization scheme is better than the second in both sfc and temp figs 2 and 3 then the members of ens2 were divided into four groups according to the specific parameterization schemes of sfc and temp ① both sfc and temp select favorable schemes where sfc 1 and temp 1 ② sfc selects a favorable scheme and temp selects an unfavorable scheme ③ sfc selects an unfavorable scheme and temp chooses a favorable scheme where sfc 2 and temp 1 and ④ both sfc and temp select unfavorable schemes where sfc 2 and temp 2 the four groups correspond to ens3 ens6 and the uncertainties for these groups were further analyzed the uncertainty interval of ens3 ens6 is shown in fig 5 the uncertainty interval of each group is relatively small and is mainly concentrated during the snow melting period and overlaps during the accumulation period the differences in the simulations of the four groups were also concentrated during snow melting period the length of the melting time is the shortest in the four groups when sfc 1 and temp 1 and longest when sfc 2 and temp 2 a relatively narrow uncertainty interval for the four groups indicates fewer uncertainties from pcp and tbot the parameterization schemes of sfc and temp determine the length of melting time for instance when sfc 2 and temp 2 were selected the length of snow melt was the longest if sfc 1 and temp 1 were selected the length of snow melt was the shortest the length of snow melt of the group where sfc 2 and temp 1 were selected ranked second after the group where sfc 1 and temp 1 were selected and the third group was where sfc 1 and temp 2 were selected overall the choice of parameterization schemes in the physical processes of sfc and temp can have a great influence on the snow depth and swe simulations the lt scheme can be determined in the group where sfc 2 and temp 2 were selected at the same time and the st scheme can be determined in the group where sfc 1 and temp 1 were selected at the same time thus extreme combination schemes are mainly due to a parameterization scheme selection of sensitive physical processes 3 3 performance of three typical combination schemes at the regional scale in section 3 2 the lt and st combination scheme can be identified according to the analysis results of the sensitivity and uncertainty experiments the difference between lt and st is highly significant and mainly concentrated during the snowmelt phase in this section lt st and dt were applied to simulate snow depth at regional scale and the simulation performances of the three typical combination schemes were examined using snow depth observations the snow depth of each site was extracted from the regional simulation results based on geographic position and then evaluated by the observations from 39 sites in northern xinjiang comparisons between the observed and simulated daily snow depth in table 3 provide an overview of the performances among three typical combination schemes over the entirety of the available period of data collected for each site the mean rmse values of the three combination schemes are similar the mean rmse values of st and dt are nearly equal and the rmse value of lt is relatively larger the mean nse value of lt is the smallest of the three indicating unsatisfactory model performance and the mean nse values of st and dt are higher by 0 16 0 18 and nearly equal which indicates an acceptable model performance according to the nse threshold values in section 2 6 the performances of st and dt are approximately the same from a macro perspective and the performance of the lt combination scheme is relatively poor from table 1 we know that the difference between st and dt is the parameterization scheme option in physical process pcp although snow depth is sensitive to pcp the sensitivity of pcp is weak therefore the simulation performances of the st and dt combination schemes are similar 3 3 1 differences in performance due to elevation although the performances of st and dt have advantages over lt in macro statistics it is worth exploring the differences in simulation performance due to elevation and land cover the sites were divided into those below 500 m between 501 and 1000 m between 1001 and 1500 m and above 1501 m according to elevation and into cropland grassland urban and built up land and barren or sparsely vegetated according to land cover because only two sites were categorized as sparse shrubland they were merged into the barren or sparsely vegetated group for a total of five sites to explore the effects of elevation on the simulations of snow depth thirty nine sites were divided into four groups according to elevation certainly the number of sites in each group was different the standardized deviations and correlations of the three typical combination schemes in each group are shown in fig 6 the simulation performances of the st and dt combination schemes are almost identical at all sites and st has a slight advantage over dt the main difference between st and dt is the parameterization scheme selection in pcp indicating that the noah scheme has a greater advantage than the jordan scheme in northern xinjiang this result was consistent with the analysis results of the sensitivity experiment as shown in fig 6 the simulation performance of lt was significantly worse than those of st and dt especially at sites above 1000 m according to the analysis results of the sensitivity experiment an unfavorable parameterization scheme of physical processes was selected to comprise the lt combination scheme the unfavorable schemes in the sensitive physical process were unsuitable in northern xinjiang and this phenomenon became more significant as elevation increased 3 3 2 differences in performance due to land cover sites were divided into four groups according to land cover and the nse coefficients of three combination schemes at each site are compared in fig 7 at most sites the nse values of the st and dt combination schemes were larger than those of the lt scheme and had acceptable or good model performance however at a small number of sites such as manas and shawan the nse value of lt was larger than those of st and dt this result suggested that the simulation performance of lt was better than those of st and dt at these sites at a few sites both of the three combination schemes exhibited unsatisfactory model performance nse 0 65 in areas such as shawan and shihezi according to the analysis results of the sensitivity and uncertainty experiments at the altay site the main difference between st and lt is the focus on the snow ablation mechanism snow in lt usually melts slower but it melts faster in dt and st certainly this is mainly due to the difference in the selection of the parameterization scheme of sensitive physics options in the four groups fifteen sites were croplands and the simulation performance of lt was better than those of st and dt at nine of these sites the sites where the simulation performance of lt is better than those of st and dt are marked with a dashed box in fig 1 there are fourteen sites in total and the nse value of lt has an obvious advantage over those of st and dt at these sites these numbers are in bold in table 3 these sites were located north of the tianshan mountains within 100 kilometers and three sites were located south of the tianshan mountains within 50 kilometers the conclusions of prior sections indicate that the difference between the st and lt combination schemes is mainly due to the melting stage of the snow process which is specifically reflected in the temporal length of the snow melt period therefore we considered that the temporal length of the snow melting stage is longer than that at other sites and that the lt combination scheme is the most suitable for these sites 4 conclusions in this study a multi parameterization ensemble simulation of snow depth and swe was conducted using meteorological observations from the altay site the sensitivity of snow to parameterization schemes was explored and the uncertainty interval of ensemble simulations was defined on this basis three typical combination schemes obtained from sensitivity analysis results were applied to simulate snow depth at the regional scale in northern xinjiang and the simulation results were evaluated the main findings are as follows 1 snow depth and swe are sensitive to the physical processes of sfc pcp tbot and temp both sfc and temp show high sensitivity the parameterization scheme selection in sfc and temp greatly influences the snow depth and swe simulations 2 uncertainties in the multi parameterization ensemble simulation experiments are mainly from sensitive physical processes and the uncertainty is mainly concentrated during the snow melting period the snow melting phase is more sensitive than the snow accumulation period to parameterization schemes especially the parameterization schemes of sensitive physics options 3 there is a substantial difference between the simulation results produced by ideal schemes and those produced by unfavorable parameterization schemes at most sites st and dt have an absolute advantage over the lt combination scheme as the elevation increases the simulation performance of lt worsens however the lt shows a better simulation performance in some regions where the snow melts slowly parameterization schemes of physical processes especially sensitive physical processes have a great impact on simulation results certainly a multi parameterization scheme can increase the applicability of a model the sensitivity experiment conducted in this study disregarded the uncertainty from the forcing field and model parameters and future studies are needed to examine uncertainties in these two fields in addition the gridded simulation results were directly compared with in situ observations and the spatial heterogeneity was ignored however the spatial resolution used in noah mp was downscaled to a fine scale of 0 05 degrees the findings from this study will provide guidance for simulating snow using the noah mp model in our next investigation data assimilation methods will be used to combine remote sensing data such as remotely sensed snow cover area data and generate more accurate and reliable predictions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the editors and anonymous reviewers for their constructive comments that significantly improved the quality and clarity of this manuscript this work was supported by the strategic priority research program of the chinese academy of sciences grant number xda19040504 the national natural science foundation of china grant number 41871251 41671375 and 41971326 
5863,the performance of the noah land surface model lsm with multi parameterization options noah mp in simulating snow depth was evaluated in northern xinjiang china a total number of 13 824 noah mp physics ensemble simulations were conducted at the altay site by combining different parameterization schemes of physical processes while disregarding the uncertainties of forcing data and model parameters the natural selection approach and tukey s test which are two sensitivity analysis methods were used to analyze the sensitivity of snow to parameterization schemes then the uncertainty intervals of the ensemble simulation experiments were compared according to the results of the sensitivity and uncertainty experiments snow depth could be simulated by three typical combination schemes at the regional scale the longest snow melting time scheme lt the shortest snow melting time scheme st and the default combination scheme dt observation data of snow depth from thirty nine meteorological stations in northern xinjiang were used to evaluate the snow simulation performance of typical combination schemes the simulation performances of the three typical combination schemes were examined and compared in groups that were divided according to elevation and land cover the results demonstrated that the simulation results of snow depth and snow water equivalent swe were sensitive to four of the eleven physics options within noah mp the exclusion of the parameterization schemes that notably reduced the simulation performance in the sensitive physical processes can significantly reduce uncertainty snow simulation performances of three typical combination schemes were diverse in northern xinjiang china no single scheme performed best at all sites but the length of the snow melting phase exhibited the best performance keywords multi parameterization ensemble simulation sensitivity analysis typical combination scheme noah mp 1 introduction snow cover is one of the most active land surface types and has a great influence on the energy balance and water circulation processes the high albedo of snow cover areas results in increased reflection of solar radiation and makes the heat flux interactions between the land surface and the lower atmosphere unique chen et al 2017 wrzesien et al 2015 moreover snow cover can also strongly influence the climate and human living environment and melt water is a significant component of the annual water budget in terms of both soil moisture and runoff which plays a critical role in flood generation in snow dominated basins dai et al 2018 durand and margulis 2006 verbunt et al 2003 in addition excessive or untimely snow cover may result in hazards such as spring flooding and bursts of glacier meltwater fed lakes han et al 2019 barnett et al 2005 piazzi et al 2018 xu et al 2017 therefore appropriate descriptions and correct simulations of snow processes have become extremely important and accurate predictions of the temporal and spatial variations in snow cover and snowmelt runoff processes have become increasingly important as a crucial variable in snow processes snow depth is an indicator of climate change and can have a positive feedback effect on climate systems chio et al 2010 snow depth is also an important parameter for studying the water balance in watersheds and simulating snowmelt runoff and snow depth has received much attention in the fields of hydrological and meteorological research regonda et al 2005 boniface et al 2015 xu et al 2017 however snow depth observation data obtained from meteorological sites cannot satisfy the data requirements of quantitative hydrology studies in addition it is difficult to correctly depict the spatial distribution of snow cover using observation data at the site scale because the scope of situ observations is always limited andreadis and lettenmaier 2006 liu et al 2013 although the observation data of snow depth that are obtained using remote sensing technology can cover large areas especially in regions that have no human footprint temporally continuous observation data cannot be obtained and the accuracy is often constrained by the algorithms required to fill in cloud covered areas thirel et al 2013 zhang et al 2014 land surface models lsms have continuously evolved according to the requirements of atmospheric and hydrological disciplines seneviratne et al 2010 niu et al 2011 and can effectively simulate snow processes compared with observation methods the spatial and temporal distributions of snow cover can be widely and easily determined by lsms in the past several decades models have often been employed to reconstruct snowpack patterns and explore the variations in snow cover for example wrzesien et al 2015 combined the weather research and forecasting wrf regional climate model with the noah mp lsm to simulate the snow cover fraction scf and snow water equivalent swe in a 3 km domain over the central sierra nevada mountains and the results indicated that wrf with the noah mp lsm can simulate the scf in mountainous environments to within 22 26 uncertainty several studies have explored the interactions between snow and vegetation with models used to reconstruct snow cover and vegetation essery 2013 thackeray et al 2014 loranty et al 2014 the analysis results revealed that snow albedo can be highly impacted by vegetation the canadian land surface scheme class was run over a domain centered over eastern canada for the period from 1990 to 2011 and the snow simulation performance of class was assessed at a large scale verseghy et al 2017 which indicated that the scf can be well simulated and that the modeled albedo has low bias all of these studies demonstrated that models can be an efficient approach to reconstruct snow cover and evaluations of the snow simulation performances of models can promote regional climate research noah mp has been widely used to simulate various land surface variables gao et al 2015 zhang et al 2016 cai et al 2014 and the simulation performance of noah mp has been evaluated at both point and watershed scales niu 2011 yang et al 2011 in a study on the snow process niu et al 2011 reported that noah mp considers a three layer snowpack and thus provides better estimates of observed snow depth snow density and swe compared to the noah lsm kuribayashi et al 2013 estimated the swe of snowpack in central japan from september 2006 to august 2008 using a 3 3 km mesh regional climate model with the noah and noah mp models the results indicated that noah mp could simulate the spatiotemporal variations in swe better than noah lsm which underestimated swe and the difference in swe between the lsms was particularly high under warm conditions the noah mp model was also systematically assessed over the continental united states for the simulation of water and energy fluxes and the results revealed that noah mp generally better captured the observed seasonal and interregional variability in net radiation scf and runoff compared to other variables ma et al 2017 although there are many examples of using noah mp to reconstruct snow cover minder et al 2016 xia et al 2017 tomasi et al 2017 the performance of noah mp in simulating snow depth has not been evaluated in snow rich areas in china such as the northeast region and northern xinjiang in this study we attempt to assess the performance of noah mp in simulating snow depth and reconstructing snow cover over northern xinjiang in china first the sensitivity of snow to the parameterization schemes within noah mp was explored using a meteorological observation dataset from the altay site then two extreme combination schemes obtained from sensitivity and uncertainty experiments and the default parameterization scheme combination were selected to simulate snow depth at the regional scale in northern xinjiang third the effectiveness of the conclusion of the sensitivity experiment was examined and the snow simulation performances of the three typical combination schemes were evaluated this article is organized as follows section 2 describes the materials and methods including the study area forcing data observation data and the introduction of the parameterization scheme combinations section 3 explains the results of snow depth simulations using the typical combination schemes section 4 summarizes the findings of this study 2 materials and methods 2 1 study area the study area is located in northern xinjiang 79 0 92 0 e 42 0 50 0 n and comprises the northwest border area of china the altay mountains are the northernmost region in this area with the highest elevation of 4370 m and the tianshan mountains are the southernmost region in this area with the highest elevation of 7440 m the central area is the zhunger er basin with an average elevation of 400 m the terrain of northern xinjiang is shown in fig 1 located in the hinterland of the eurasian continent this area is characterized by a typical temperate continental climate xu and shu 2014 due to the influence from siberian circulation this area has a short warm and rainy summer and a long cold and snowy winter the duration of snow cover in this region is approximately 120 days from november to march huang et al 2011 xu and shu 2014 because this region has rich snow resources it is well suited for the implementation of snow simulation experiments the altay site investigated in this study 47 44 n 88 05 e is located in northern xinjiang in northwestern china fig 1 the elevation of the meteorological observation field is 735 3 m and this field is less than 5 km from the altay mountains this site is a typical meteorological observation station in northern xinjiang and has a complete and eligible high quality meteorological observation dataset in addition this site has less wind in the winter season therefore a sensitivity experiment on snow under various parameterization schemes was conducted at this site 2 2 atmospheric forcing and observation data at the altay site meteorological observation data from october 1 2014 to september 30 2015 including wind speed and direction air temperature relative humidity precipitation pressure snow depth and swe were obtained from the china meteorological administration cma according to the necessities of model forcing four components of radiation downward and upward longwave and shortwave radiation were obtained using a four component net radiometer snow depth was manually obtained with daily resolution and swe was obtained with six hourly resolution by using a gmon gamma monitor detector however swe observation data from november 15 2014 first snowfall during the observation period to january 20 2015 are missing because the gmon detector broke down at the beginning of the snow period for the regional snow depth simulation the forcing data from october 1 2012 to september 30 2013 in this study developed by the hydrometeorological research group at the institute of tibetan plateau research at the chinese academy of sciences itpcas were used to drive noah mp at a regional scale in northern xinjiang including seven essential meteorological variables pressure precipitation wind speed specific humidity near surface air temperature downward shortwave radiation and longwave radiation this dataset was produced by merging the observations collected at 740 operational stations of the cma with the corresponding princeton meteorological forcing data the global land data assimilation systems gldas reanalysis forcing data the tropical rainfall measuring mission trmm 3b42 precipitation products and the global energy and water cycle experiment surface radiation budget gewexsrb radiation data chen et al 2011 the itpcas forcing data completely cover the geographic scope of china with a spatial resolution of 0 1 and a temporal resolution of 3 h he and yang 2011 to meet the demands of experimental accuracy the micromet approach liston and elder 2006 was adopted to downscale the atmospheric forcing data and is mainly based on the relationships between meteorological variables and the surrounding landscape primarily topography finally a set of forcing data with high spatial resolution 0 05 and high temporal resolution 1 h was obtained for this study area the observations of snow depth from october 1 2012 to september 30 2013 with a daily resolution were from thirty nine meteorological stations in northern xinjiang the spatial distribution of meteorological stations is shown in fig 1 the land cover at the stations is also shown in fig 1 there are five land cover types at these sites cropland grassland urban and built up land barren or sparsely vegetated land and sparse shrubland validation data from these sites were used to evaluate the simulation performance of the three typical combination schemes 2 3 physics ensemble numerical experiment noah mp was developed based on noah v3 0 chen et al 1996 1997 chen and dudhia 2001 ek et al 2003 where vegetation and groundwater dynamics were first incorporated and then multiple parameterization schemes were introduced for various physics options such as runoff radiation transfer and snow albedo niu 2011 yang 2011 every physics option has two to four parameterization schemes for selection table 1 compared with noah noah mp considers a three layer snowpack depending on the total snow depth which allows snow variables to be simulated more accurately the sensitivity of snow to parameterization schemes was studied while disregarding the uncertainties from forcing data and model parameters meteorological measurements with a temporal resolution of 1 h were used as forcing data and the simulation results were evaluated by observation data because noah mp requires a long time to reach soil state equilibrium chen et al 2014 cai et al 2014 gao et al 2015 before it can be used for initialization the forcing data from october 1 2013 to september 30 2014 were used repeatedly to drive noah mp to ensure the equilibrium soil state the equilibrium criterion is defined as the time when the difference in annual means between two consecutive single year simulations was less than 0 1 of the mean cai et al 2014 gao et al 2015 the dynamic vegetation model within the noah mp model was developed mainly to simulate natural vegetation dynamics for vegetation without human interference and due to simplified phenology and assimilated portioning the model provides less reliable simulations of vegetation affected by human activities xia et al 2014 zhang et al 2016 vegetation at the altay site was impacted by human activities such as irrigation and weeding therefore in this study the dynamic vegetation scheme was turned off in the ensemble simulation experiments the physics options for the 11 remaining physical processes table 1 were all included and possibly combined and 13 824 scheme combinations were designed to analyze the sensitivity and uncertainty hereafter ens1 table 2 based on the analysis results of the sensitivity experiment the ensemble simulation experiment ens2 was conducted by possibly combining the schemes of 4 sensitive physical processes there are 24 members in total in ens2 ens3 ens6 were designed to reduce uncertainties in the ensemble simulations and analyze the sources of the uncertainties 2 4 analysis and evaluation methods for the sensitivity experiment at the altay site snow depth and swe were recorded as simulation variables two sensitivity analysis methods were chosen to analyze sensitivity in ens1 natural selection and tukey s test both methods use the root mean square error rmse between the simulations and observations to evaluate the performance of the model simulations the sensitivity of snow to parameterization schemes can be determined and crucial physical processes were extracted based on the analysis results from these two methods the main steps of the two sensitivity analysis methods are described below more details can be found in the work of zhang et al 2016 natural selection first the rmse of each member in ens1 was computed and all rmses were sorted in ascending order then the members with rmses concentrated below the 5th percentile for snow depth and swe were denoted as the best members 692 members and those above the 95th percentile were denoted as the worst members 692 members third the times when a given scheme for each physical process occurred with the best and worst members were counted obviously for a given parameterization scheme a large number of selections with the best members has an advantage in terms of model accuracy similarly a large number of selections with the worst members has a lower advantage in terms of model accuracy the sensitivity of snow to a given physical process can be judged on the macro level by applying this method tukey s test approach as a hypothesis test this method can effectively examine the differences between parameterization schemes in one physical process at the micro level a statistical hypothesis test could be used to compare each pair of population means u i and u j for all i j where the null and alternative hypotheses are 1 h 0 u i u j h 1 u i u j the distribution of the studentized range statistic was used in tukey s procedure 2 q y max y min m s e 2 1 n i 1 n j y max y min s s e 2 n a 1 n i 1 n j where y max is the maximum value of the two parameterization means being compared y min is their minimum value n i and n j are the sample sizes for the ith and jth parameterization schemes respectively m s e is the mean square error s s e k 1 a l 1 n k y kl y k 2 is the sum of the square error n is the total sample size for all schemes and n a is the degrees of freedom associated with m s e according to tukey s test two population means are significantly different if 3 y i y j m s e 2 1 n i 1 n j q α a n a where α is the 0 05 significance level in this study and q α a n a was obtained from the table with the studentized range distribution 2 5 typical combination schemes for snow depth simulation according to the analysis results of the sensitivity and uncertainty experiments in section 2 4 two extreme combination schemes can be identified one has the longest snow melting time the lt scheme and the other has the shortest snow melting time the st scheme as expected the difference between the two combination schemes was mainly focused on the parameterization scheme options for four sensitive physical processes obviously the uncertainty interval of multi parameterization ensemble simulation results can be determined from extreme combination schemes although the st scheme was closer to the observations at the altay site than the lt scheme the simulation performances of the two combination schemes at the other sites are unknown therefore the simulation performance of extreme combination schemes at the regional scale should be further examined the default combination scheme dt within noah mp was the scheme recommended by the model and has been applied in many studies the two extreme combination schemes obtained by the sensitivity experiment and the dt schemes recommended by the model are typical schemes in all 13 824 combination schemes to examine the simulation performance of these three typical combination schemes in this section three were selected to simulate snow depth at the regional scale in northern xinjiang the parameterization scheme options of lt st and dt are listed in table 1 the difference between the st and dt combination schemes is relatively small only the parameterization scheme option of physical process pcp is different however the difference between the st and lt schemes is obvious and the scheme options of all sensitive physical processes are different the calculation equations of the different parameterization schemes for the sensitive physical processes are listed below a surface exchange coefficient for heat sfc 4 c h κ 2 ln z d 0 z 0 m ψ m z d 0 l ln z d 0 z 0 h ψ h z d 0 l m o 5 c h κ 2 ln z z 0 m ψ m z l ψ m z 0 m l ln z z 0 h ψ h z l ψ h z 0 h l original noah chen 97 where c h is the surface heat exchange coefficient κ is the von karmon constant l is the monin obukhov length and z is the reference height z 0 h and z 0 m are the roughness lengths for heat and momentum respectively d 0 is the zero displacement height for the m o scheme both the m o and chen97 schemes take the same stability correction functions ψ m and ψ h for stable and unstable conditions respectively the m o scheme accounts for the zero displacement height d 0 but the chen97 scheme accounts for the difference between roughness lengths for heat and momentum which makes the values of c h significantly different b partitioning precipitation into rainfall and snowfall pcp 6 f p i c e 0 t sfc t frz 1 t sfc t frz 0 5 1 54 632 0 2 t sfc t sfc t frz 2 0 6 t sfc t frz 0 5 jordan 91 7 f p i c e 0 t sfc t frz 2 2 1 t sfc t frz bats 8 f p i c e 0 t sfc t frz 1 t sfc t frz noah where f p i c e is the proportion of snow in precipitation t sfc and t frz are the surface air temperature and freezing point respectively jordan 91 may be the most complicated of the three schemes because of the different ways of dealing with temperature and the results calculated by the above three schemes vary greatly c lower boundary condition of soil temperature tbot 9 e 0 zero flux scheme 10 e e 0 noah in the equation e is the heat flux from the bottom of soil column the zero flux scheme assumes no heat flux into the soil column when describing the physical process of soil temperature boundary conditions however the noah scheme is based on the assumption that there is an existing heat flux at the 8 m soil depth e 0 was caculated according to soil temperature at the depth of the lower boundary condition and the bottom temperature of soil column the caculation result of heat flux was also decided by soil thermal conductivity the bottom depth of soil column and depth of lower boundary condition these two assumptions make the calculation for soil temperature different which influences the snow cover melting process d snow and soil temperature time scheme temp semi implicit and fully implicit are two options in the temp physical process that were used to solve the thermal diffusion equation in snow and soil layer eq 11 is the thermal diffusion equation used in the noah mp model where t is the time z is the soil snow depth t is the soil snow temperature c is the soil snow heat capacity k t is the soil snow thermal conductivity coefficient at time t although both semi implicit and fully implicit are discretization methods for solving the thermal diffusion equation within the noah mp model the impact of the two options on the snow simulation result is marked 11 c t t z k t t z 2 6 evaluation methods for typical combination schemes the model performance was quantified using statistical analysis based on the daily mean values of simulations and observations certainly the observations are the snow depths measured at thirty nine meteorological stations from october 1 2012 to september 30 2013 model performance was evaluated using the rmse and nash sutcliffe efficiency nse coefficient 12 rmse 1 n i 1 n o b s i s i m i 2 13 nse 1 i 1 n o b s i s i m i 2 i 1 n o b s i obs 2 where n is the total number of observations s i m i is the simulated value at time i o b s i is the observed value at time i and obs is the mean of the observed data the rmse ranges from 0 to the smaller the rmse value is the closer the simulation is to the observation which means the simulation has high precision the nse indicates how well the scatter plot of the observed versus simulated data fits the 1 1 line and ranges between and 1 a nse 1 indicates a perfect fit according to previous studies nse 0 65 indicates unsatisfactory model performance 0 65 n s e 0 8 indicates acceptable model performance 0 8 n s e 0 9 indicates good model performance and nse 0 9 indicates very good performance ritter and munoz carpena 2013 moriasi et al 2007 3 results and discussion 3 1 sensitivity analysis of snow to physics options 3 1 1 natural selection results the rmses for snow depth and swe by 13 824 combination schemes at the altay site were calculated and sorted in ascending order members concentrated below the 5th percentile of the rmse for snow depth and swe were considered the best members and those above the 95th percentile were considered the worst members where the frequency of each scheme was counted for the two groups above the horizontal axis in fig 2 a and b the frequency of each parameterization scheme is shown for the best members and the frequency of the worst members is below the horizontal axis using the canopy resistance process crs table 1 in fig 2 a as an example the selected frequency of crs scheme 1 hereafter crs 1 for the best members is 0 43 and the frequency of crs 2 is 0 57 indicating a 43 combination scheme for all 692 members using crs 1 and a 57 combination scheme using crs 2 with the best members however the selected frequencies of crs 1 and crs 2 with the worst members are 0 47 and 0 53 respectively indicating that 47 of the 692 worst members use crs 1 and 53 of those use crs 2 the difference in the selected frequency with the best members implies that using crs 2 provides a greater chance of producing favorable simulations than using crs 1 however the selected frequency of the two schemes with the worst members ball berry 0 47 jarvis 0 53 implies that crs 2 has a much better chance of producing worse simulations than crs 1 although it is difficult to identify the better scheme from the analysis results the frequencies of crs 1 and crs 2 imply that the simulation results are not sensitive to crs fig 2 shows that the difference in the selected frequencies of the parameterization schemes in the physical processes of crs btr run inf rad and alb are not significant for either the best or worst members consequently the simulation results of snow depth and swe are not sensitive to these physical processes the differences in the selected frequency of the parameterization schemes in sfc frz pcp tbot and temp are significant particularly in the parameterization schemes for sfc and temp the selected frequencies of sfc 1 and temp 1 are 100 and the selected frequencies of sfc 2 and temp 2 are zero with the best members however with the worst members the selected frequencies of sfc 1 and temp 1 are zero and the selected frequencies of sfc 2 and temp 2 are 100 this result implies that sfc 1 and temp 1 have a much better chance of producing favorable simulations while sfc 2 and temp 2 are more likely to produce unfavorable simulations in the ensemble simulation experiments of both snow depth and swe the difference between parameterization schemes in these physical processes is significant the simulation results are more sensitive to sfc and temp than pcp and tbot sfc 1 and sfc 2 used the same stability correction function ψ m and ψ h for stable and unstable conditions to calculate the surface heat exchange coefficient c h chen et al 1997 niu et al 2011 sfc 1 is based on the more general monin obukhov similarity theory and sfc 2 selects the calculation method from noah v3 as seen in eqs 4 and 5 respectively the most important difference between these two schemes is that sfc 1 accounts for the zero displacement height d 0 but sfc 2 accounts for the difference between roughness lengths for heat and momentum the difference between the two schemes greatly impacts the c h value energy and water balance the results of the snow depth and swe show that the performance of sfc 1 is better than that of sfc 2 which is for the most part consistent with the conclusions in hong et al 2014 and zhang et al 2016 when soil is frozen the temperature is below the freezing point but water near soil particles remains in liquid form due to soil capillarity in the snow depth ensemble simulation experiment the selected frequency of frz 1 is 0 83 with the best members and frz 2 is 0 78 with the worst members in the swe ensemble simulation experiment the selected frequency of frz 1 is 0 72 with the best members and frz 2 is 0 84 with the worst members the statistical results imply that the performance of frz 1 is better than that of frz 2 in both the snow depth and swe ensemble simulation experiments frz 1 takes a more general form of the freezing point depression equation niu and yang 2006 while frz 2 exhibits a variant of the freezing point depression equation including an extra term 1 8 θ ice 2 niu et al 2011 compared to frz 1 frz 2 produces more liquid water in soil the use of the appropriate parameterization scheme to divide precipitation into two parts rainfall and snowfall can directly influence the snow depth simulation results the difference between the selected frequencies of pcp 1 and pcp 2 is not obvious but the selected frequency of pcp 3 is much larger than those of pcp 1 and pcp 2 with the best members in both the snow depth and swe ensemble simulations pcp 3 is implied to have a chance to produce a better simulation than pcp 1 and pcp 2 this physical process uses the surface air temperature t air as a criterion where pcp 3 simply assumes that all precipitation is snowfall when t air t frz otherwise it is rainfall when t air frequently varies around the freezing point the modeled snow accumulation is very sensitive to the parameterization scheme of this process niu et al 2011 at the altay site temperatures vary widely around the freezing point in the stage of snow melting therefore the simulation results are sensitive to these parameterizations tbot 1 assumes no heat flux into the soil column when describing the physical process of the soil temperature boundary conditions tbot 2 is based on the assumption that there is an existing heat flux at the 8 m soil depth these two assumptions make the calculation of soil temperature different which influences the snow cover melting process the selected frequencies of tbot 1 and tbot 2 are very close with the best members additionally the selected frequency of tbot 1 is much higher than that of tbot 2 with the worst members in both the snow depth and swe experiments although the difference between the two parameterizations is obvious in fig 2 we did not identify the ultimately better scheme the differences between temp 1 and temp 2 are significant in both the snow depth and swe ensemble simulations the frequencies of the two parameterizations are opposite when the best members and worst members are used similar to sfc snow simulaions are also highly sensitive to physical process temp 3 1 2 tukey s test results in this section tukey s test was used to examine the differences between parameterization schemes first the rmse of each combination scheme for the snow depth and swe ensemble simulation groups was calculated resulting in a total of 13 824 rmses for both the snow depth and swe groups all 13 824 of the rmses were independent from one another and the assumptions of normality and equality of variances were checked before adopting tukey s test for example physical process btr has 3 parameterization schemes and each parameterization scheme was used in 4608 combination schemes therefore 4608 rmse values correspond to each scheme the quartile distribution of the rmse for each scheme for the 11 physical processes is shown in the subfigures in fig 3 from top to bottom in fig 3 schemes that do not share the letter a behave significantly different and those with the letter b outperformed those with the letter a because their mean rmse values were much smaller obviously the smaller the mean value of the rmse is the better the scheme performance the simulation results are sensitive to a physical process depending on whether the difference between parameterization schemes is significant consequently a physical process can be regarded as insensitive when all of its schemes share a common letter meanwhile when different letters are assigned to any two schemes it is sensitive fig 3 for example sfc 1 was labeled with the letter b and sfc 2 was labeled with a in both the snow depth and swe ensemble simulation experiments the mean value of the rmse for sfc 1 was smaller than that for sfc 2 therefore the difference between sfc 1 and sfc 2 was significant and sfc 1 was more likely to produce advantageous simulations than sfc 2 in the snow depth and swe ensemble simulation experiments this result is consistent with the conclusion of natural selection the simulation results of the snow depth and swe deemed that the sfc was sensitive the results also indicate that the differences between the parameterization schemes of crs btr run inf rad frz and alb are not significant but the differences between the parameterization schemes of sfc tbot and temp are significant in both the snow depth and swe ensemble simulation experiments the difference between the parameterization schemes of pcp is significant in the snow depth ensemble simulation experiment but not in the swe experiment the distributions of the rmse values of the three parameterization schemes are roughly consistent in fig 3 we concluded that snow depth is sensitive to pcp and swe is insensitive to pcp but the sensitivity is relatively weak the results of tukey s test are roughly equal to the results of natural selection however the selection frequencies of frz 1 and frz 2 have large differences in both the best members and worst members the simulation results of snow depth and swe should be sensitive to frz based on convention but there is no evidence that shows existing significant differences between frz 1 and frz 2 according to tukey s test the rmse distributions of the two parameterization schemes are very similar indicating that the results had almost no differences additionally the sensitivity of pcp is relatively weak compared with those of sfc tbot and temp the rmse distribution difference between the three parameterization schemes is not obvious however the difference in the frequency selected by the natural selection method is significant as shown in fig 2 which is partially due to the natural selection method considering only the best and worst members approximately 692 and because it is only a part of the total simulation results not the results of all of the members in contrast tukey s test considers the means of all 13 824 members therefore this method is more convincing for these reasons we concluded that the simulation results of snow depth and swe were insensitive to frz and sensitive to pcp the mean rmse values have obscure differences suggest that a majority of the physical processes are not significant as shown in fig 3 nevertheless the difference between the parameterization schemes of sfc and temp is still highly significant and the mean rmse values of the schemes are significantly different paired with these factors we declared that the simulation results of snow depth and swe were highly sensitive to sfc and temp compared to other physical processes and possibly the main factors causing uncertainties in the ensemble simulation results based on the above analysis results we concluded that the simulation results of snow depth and swe are sensitive to the physical processes of sfc pcp tbot and temp where the sensitivities of sfc tbot and temp are relatively significant and that of pcp is relatively weak 3 2 uncertainties in physical parameterization schemes the sensitivities of the parameterization schemes were analyzed by natural selection and tukey s test in section 3 1 the simulations are highly sensitive to sfc and temp as shown in figs 2 and 3 respectively and could serve as the main sources of uncertainty based on the analysis results of these two methods this section further studies the uncertainties in the ensemble simulation experiments of snow depth and swe ens2 was conducted to explore the uncertainty interval of the sensitive physical process ensemble simulation experiment in this experiment forcing data that were the same as ens1 and the schemes of four sensitive physical processes sfc pcp tbot and temp were possibly combined for a total of 24 combination schemes the uncertainty interval of the simulations based on the sensitive physical processes 24 simulations was compared with the uncertainty interval of the simulations based on all combination schemes 13 824 simulations as shown in fig 4 during the snow accumulation period the uncertainty interval is relatively small and the variations in the different combination schemes are almost equal during the snowmelt period the uncertainty interval is relatively large beginning in late march and the differences are mainly correlated with the length of melting time the uncertainty interval of the sensitive physical process ensemble simulations is roughly equivalent to the total ensemble simulations in fig 4 and both the uncertainties of snow depth and swe are concentrated during the snow melting period this result implies that the uncertainties in the ensemble simulation results are mostly from sensitive physical processes thus the selection of appropriate parameterization schemes for sensitive physical processes is crucial for the simulation of the snow melting process to reduce uncertainties in the ensemble simulation experiments further study of the uncertainty sources in the sensitivity process is needed according to the conclusion in section 3 1 sfc pcp tbot and temp are sensitive physical processes in the snow depth and swe ensemble simulations especially sfc and temp in addition the results of the two sensitivity analysis methods indicate that the first parameterization scheme is better than the second in both sfc and temp figs 2 and 3 then the members of ens2 were divided into four groups according to the specific parameterization schemes of sfc and temp ① both sfc and temp select favorable schemes where sfc 1 and temp 1 ② sfc selects a favorable scheme and temp selects an unfavorable scheme ③ sfc selects an unfavorable scheme and temp chooses a favorable scheme where sfc 2 and temp 1 and ④ both sfc and temp select unfavorable schemes where sfc 2 and temp 2 the four groups correspond to ens3 ens6 and the uncertainties for these groups were further analyzed the uncertainty interval of ens3 ens6 is shown in fig 5 the uncertainty interval of each group is relatively small and is mainly concentrated during the snow melting period and overlaps during the accumulation period the differences in the simulations of the four groups were also concentrated during snow melting period the length of the melting time is the shortest in the four groups when sfc 1 and temp 1 and longest when sfc 2 and temp 2 a relatively narrow uncertainty interval for the four groups indicates fewer uncertainties from pcp and tbot the parameterization schemes of sfc and temp determine the length of melting time for instance when sfc 2 and temp 2 were selected the length of snow melt was the longest if sfc 1 and temp 1 were selected the length of snow melt was the shortest the length of snow melt of the group where sfc 2 and temp 1 were selected ranked second after the group where sfc 1 and temp 1 were selected and the third group was where sfc 1 and temp 2 were selected overall the choice of parameterization schemes in the physical processes of sfc and temp can have a great influence on the snow depth and swe simulations the lt scheme can be determined in the group where sfc 2 and temp 2 were selected at the same time and the st scheme can be determined in the group where sfc 1 and temp 1 were selected at the same time thus extreme combination schemes are mainly due to a parameterization scheme selection of sensitive physical processes 3 3 performance of three typical combination schemes at the regional scale in section 3 2 the lt and st combination scheme can be identified according to the analysis results of the sensitivity and uncertainty experiments the difference between lt and st is highly significant and mainly concentrated during the snowmelt phase in this section lt st and dt were applied to simulate snow depth at regional scale and the simulation performances of the three typical combination schemes were examined using snow depth observations the snow depth of each site was extracted from the regional simulation results based on geographic position and then evaluated by the observations from 39 sites in northern xinjiang comparisons between the observed and simulated daily snow depth in table 3 provide an overview of the performances among three typical combination schemes over the entirety of the available period of data collected for each site the mean rmse values of the three combination schemes are similar the mean rmse values of st and dt are nearly equal and the rmse value of lt is relatively larger the mean nse value of lt is the smallest of the three indicating unsatisfactory model performance and the mean nse values of st and dt are higher by 0 16 0 18 and nearly equal which indicates an acceptable model performance according to the nse threshold values in section 2 6 the performances of st and dt are approximately the same from a macro perspective and the performance of the lt combination scheme is relatively poor from table 1 we know that the difference between st and dt is the parameterization scheme option in physical process pcp although snow depth is sensitive to pcp the sensitivity of pcp is weak therefore the simulation performances of the st and dt combination schemes are similar 3 3 1 differences in performance due to elevation although the performances of st and dt have advantages over lt in macro statistics it is worth exploring the differences in simulation performance due to elevation and land cover the sites were divided into those below 500 m between 501 and 1000 m between 1001 and 1500 m and above 1501 m according to elevation and into cropland grassland urban and built up land and barren or sparsely vegetated according to land cover because only two sites were categorized as sparse shrubland they were merged into the barren or sparsely vegetated group for a total of five sites to explore the effects of elevation on the simulations of snow depth thirty nine sites were divided into four groups according to elevation certainly the number of sites in each group was different the standardized deviations and correlations of the three typical combination schemes in each group are shown in fig 6 the simulation performances of the st and dt combination schemes are almost identical at all sites and st has a slight advantage over dt the main difference between st and dt is the parameterization scheme selection in pcp indicating that the noah scheme has a greater advantage than the jordan scheme in northern xinjiang this result was consistent with the analysis results of the sensitivity experiment as shown in fig 6 the simulation performance of lt was significantly worse than those of st and dt especially at sites above 1000 m according to the analysis results of the sensitivity experiment an unfavorable parameterization scheme of physical processes was selected to comprise the lt combination scheme the unfavorable schemes in the sensitive physical process were unsuitable in northern xinjiang and this phenomenon became more significant as elevation increased 3 3 2 differences in performance due to land cover sites were divided into four groups according to land cover and the nse coefficients of three combination schemes at each site are compared in fig 7 at most sites the nse values of the st and dt combination schemes were larger than those of the lt scheme and had acceptable or good model performance however at a small number of sites such as manas and shawan the nse value of lt was larger than those of st and dt this result suggested that the simulation performance of lt was better than those of st and dt at these sites at a few sites both of the three combination schemes exhibited unsatisfactory model performance nse 0 65 in areas such as shawan and shihezi according to the analysis results of the sensitivity and uncertainty experiments at the altay site the main difference between st and lt is the focus on the snow ablation mechanism snow in lt usually melts slower but it melts faster in dt and st certainly this is mainly due to the difference in the selection of the parameterization scheme of sensitive physics options in the four groups fifteen sites were croplands and the simulation performance of lt was better than those of st and dt at nine of these sites the sites where the simulation performance of lt is better than those of st and dt are marked with a dashed box in fig 1 there are fourteen sites in total and the nse value of lt has an obvious advantage over those of st and dt at these sites these numbers are in bold in table 3 these sites were located north of the tianshan mountains within 100 kilometers and three sites were located south of the tianshan mountains within 50 kilometers the conclusions of prior sections indicate that the difference between the st and lt combination schemes is mainly due to the melting stage of the snow process which is specifically reflected in the temporal length of the snow melt period therefore we considered that the temporal length of the snow melting stage is longer than that at other sites and that the lt combination scheme is the most suitable for these sites 4 conclusions in this study a multi parameterization ensemble simulation of snow depth and swe was conducted using meteorological observations from the altay site the sensitivity of snow to parameterization schemes was explored and the uncertainty interval of ensemble simulations was defined on this basis three typical combination schemes obtained from sensitivity analysis results were applied to simulate snow depth at the regional scale in northern xinjiang and the simulation results were evaluated the main findings are as follows 1 snow depth and swe are sensitive to the physical processes of sfc pcp tbot and temp both sfc and temp show high sensitivity the parameterization scheme selection in sfc and temp greatly influences the snow depth and swe simulations 2 uncertainties in the multi parameterization ensemble simulation experiments are mainly from sensitive physical processes and the uncertainty is mainly concentrated during the snow melting period the snow melting phase is more sensitive than the snow accumulation period to parameterization schemes especially the parameterization schemes of sensitive physics options 3 there is a substantial difference between the simulation results produced by ideal schemes and those produced by unfavorable parameterization schemes at most sites st and dt have an absolute advantage over the lt combination scheme as the elevation increases the simulation performance of lt worsens however the lt shows a better simulation performance in some regions where the snow melts slowly parameterization schemes of physical processes especially sensitive physical processes have a great impact on simulation results certainly a multi parameterization scheme can increase the applicability of a model the sensitivity experiment conducted in this study disregarded the uncertainty from the forcing field and model parameters and future studies are needed to examine uncertainties in these two fields in addition the gridded simulation results were directly compared with in situ observations and the spatial heterogeneity was ignored however the spatial resolution used in noah mp was downscaled to a fine scale of 0 05 degrees the findings from this study will provide guidance for simulating snow using the noah mp model in our next investigation data assimilation methods will be used to combine remote sensing data such as remotely sensed snow cover area data and generate more accurate and reliable predictions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the editors and anonymous reviewers for their constructive comments that significantly improved the quality and clarity of this manuscript this work was supported by the strategic priority research program of the chinese academy of sciences grant number xda19040504 the national natural science foundation of china grant number 41871251 41671375 and 41971326 
5864,porewater fluxes including fresh groundwater discharge and circulation of surface waters through sediments are increasingly documented to play an important role in hydrological and biogeochemical cycles of coastal water bodies in most studies the magnitude of porewater fluxes is inferred from geochemical tracers but a detailed understanding of the underlying physical forces driving these fluxes remains limited in this study we evaluate the mechanisms driving porewater fluxes in the shallow coastal la palme lagoon france we combined measurements of variations of salinity and temperature in the subsurface with 1 dimensional fluid salt and heat transport models to evaluate the dynamics of porewater fluxes across the sediment water interface in response to temporally variable forcings two main processes were identified as major drivers of porewater fluxes i temporal variations of lagoon water depths forcing porewater fluxes up to 25 cm d 1 and ii locally generated wind waves porewater fluxes of 50 cm d 1 these processes operate over different spatial and temporal scales wind driven waves force the shallow circulation of surface lagoon waters through sediments mostly 0 2 m but are restricted to strong wind events typically lasting for 1 3 days in contrast porewater fluxes driven by variations of lagoon water depths flush a much greater depth of sediment 1 m the spatial and temporal scales of driving forces will largely determine the significance of porewater fluxes as well as their chemical composition thus an appropriate evaluation of the magnitude of porewater driven solute fluxes and their consequences for coastal ecosystems requires a solid and site specific understanding of the underlying physical forces keywords porewater exchange submarine groundwater discharge coastal lagoon driving forces waves salinity temperature 1 introduction water fluxes circulating through permeable sediments are increasingly being recognized as an important source of dissolved solutes e g nutrients metals pollutants to surface water bodies anschutz et al 2009 liefer et al 2013 rodellas et al 2015 in coastal settings these fluxes across the sediment water interface are commonly referred to as submarine groundwater discharge sgd or porewater exchange pex depending on the scale of the circulation process moore 2010 santos et al 2012 in this study we use the term porewater fluxes to refer to the total efflux of water and solutes across the permeable sediments to surface waters thus including both sgd and pex porewater driven fluxes of solutes may exert a major control on the biogeochemistry water quality and ecological functioning of receiving water bodies e g contributing to sustaining the primary production and community composition of phytoplankton in coastal areas andrisoa et al 2019 garcés et al 2011 valiela et al 1990 promoting eutrophication of surface waters hwang et al 2005 paerl 1997 and leading to recurrent harmful algal blooms gobler and sañudo wilhelmy 2001 lee et al 2010 the physical mechanisms driving porewater fluxes strongly affect the residence time of waters within sediments or the coastal aquifer determining the extent and rates of biogeochemical reactions and therefore the composition of discharging fluids santos et al 2012 weinstein et al 2011 an appropriate understanding of the magnitude of solute fluxes driven by groundwater and porewater discharge requires thus identifying the mechanism forcing these inputs many physical processes produce pressure gradients at the sediment water interface that can force advective porewater fluxes the main driving forces include the terrestrial hydraulic gradient and its seasonal oscillations wave and tidal pumping the interaction of currents and seafloor topography convection driven by density inversions or pumping activities of benthic fauna huettel et al 2014 santos et al 2012 these different forcing mechanisms which are of both marine e g wave and tidal setup and terrestrial e g hydraulic gradient origin are highly dynamic and irregular span a wide range of exchange lengths and timescales and are frequently superimposed robinson et al 2017 santos et al 2012 2009 a large number of studies highlight the overall magnitude and the significance of porewater fluxes e g cho et al 2018 kwon et al 2014 moore et al 2008 rodellas et al 2015 but there is still little information about their driving forces robinson et al 2017 studies conducted to date have evaluated the effect of individual driving forces in isolation and have been mainly focused on regular and short term forces e g semi diurnal diurnal tides density driven flows mainly as a consequence of the difficulties inherent in investigating irregular and longer period forcing via field experiments and in unraveling the various forcing effects robinson et al 2017 irregular forcings such as episodic high intensity events may have a great impact on fluxes of water and solutes driven by porewater fluxes sawyer et al 2013 smith et al 2008 the understanding of these forcing is thus required to better predict the effects of increasing stressors in the system e g climate change anthropogenic pressure and to better identify settings where specific forcings may dominate over the others this study is aimed at characterizing porewater fluxes in a shallow coastal lagoon la palme lagoon france where the circulation of significant volumes of surface water through sediments have been previously documented cook et al 2018a rodellas et al 2018 stieglitz et al 2013 tamborski et al 2018 these previous studies have estimated the average magnitude of porewater fluxes to the lagoon but they provided little insight into their temporal variations and the mechanisms driving these fluxes the current paper examines variations of subsurface temperature and porewater salinity in la palme lagoon to evaluate the dynamics of porewater fluxes and to provide some insight into controlling forcings in this study we focus on two mechanisms that can control porewater fluxes in the lagoon and that operate over different temporal and spatial scales i the variations of lagoon water depths which can influence the terrestrial hydraulic gradient and drive long scale 1 m porewater fluxes and ii wave pumping produced by the strong winds of the region which forces the flushing of shallow sediments short scale porewater fluxes other active mechanisms are likely significantly contributing to total porewater fluxes e g bioirrigation or current topography interactions but they are not specifically evaluated in this study 2 methods 2 1 study site la palme lagoon france la palme is a small 500 ha surface area shallow coastal lagoon with mean and maximum water depths of 0 7 and 2 m respectively fig 1 it is connected with the mediterranean sea through a small opening in the coastal sand spit which may be seasonally closed and it receives continuous fresh groundwater inputs 0 01 0 04 m3 s 1 mainly from a regional karst aquifer constituted by karstified jurassic and lower cretaceous limestones stieglitz et al 2013 wilke and boutière 2000 the lagoon is also connected with a shallow alluvial aquifer alluvial aquifer of aude and berre rivers but little information is available on the aquifer lagoon interaction the internal mixing of the lagoon and its exchange with coastal waters is driven primarily by the strong north westerly winds characteristic of the region regularly exceeding 10 m s 1 given that tidal variations in the mediterranean sea are usually small and the exchange between la palme lagoon and the sea is highly restricted by three physical barriers railway dike road dike and sandy barrier fig 1 tidal forcing plays a minor role on the hydrodynamic functioning of this lagoon tidal range in the lagoon 1 cm fiandrino et al 2012 most of the lagoon is covered by fine to coarse grained sands 100 500 µm and only the northern part of the lagoon is dominated by fine grained sediments 50 µm the eastern part of the lagoon is surrounded by evaporation ponds but there is no visual or chemical evidence of a connection between the lagoon and the salt pond rodellas et al 2018 tamborski et al 2018 a study conducted by stieglitz et al 2013 hypothesized that strong winds produced circulation of large amounts of lagoon water through surface sediments different studies have estimated porewater inputs to the entire lagoon at 0 4 2 1 m3 s 1 0 8 4 1 cm d 1 which is the equivalent of the volume of the entire lagoon circulating through the sediments every 20 90 days rodellas et al 2018 stieglitz et al 2013 tamborski et al 2018 however to date the forces driving these fluxes have not been evaluated in detail 2 2 sampling and analysis four different stations pz1 pz2 pz3 pz4 were established in areas considered representative of the different sediment types of the lagoon fig 1 in may 2017 a sediment core up to 50 cm depth was collected at each one of these locations and sliced every 5 cm the grain size distribution of each sediment sample was determined through a coulter ls230 laser diffraction particle size analyzer average sediment porosities for each location were obtained from tamborski et al 2018 who collected sediment cores at the same locations sensors for measuring temperature salinity lagoon water depths and wave parameters were also installed at these sites and porewater samples were collected as discussed in the following sections hourly rainfall temperature wind speed and direction and atmospheric pressure data at the nearby meteorological station leucate was extracted from the database of the french meteorological service météo france additional monthly data on lagoon water depths and salinity at three sites in the northern lagoon pn stations in fig 1 was obtained from the database of parc naturel régional de la narbonnaise en méditerranée pnrnm data on daily piezometric levels of the alluvial aquifer connected to the lagoon alluvial aquifer of aude and berre rivers was obtained from the french groundwater national portal piezometer code bss002lrh ades eaufrance fr 2 2 1 subsurface salinity time series porewater samples for salinity analysis were collected during 7 different sampling campaigns between march 2016 and june 2017 march april june october and november 2016 april and june 2017 during each campaign porewater samples were collected from 3 different locations pz1 pz2 and pz3 using a direct push shielded screen well point piezometer charette and allen 2006 porewater samples for salinity analysis 10 ml were collected at depths ranging from 5 to 140 cm below the sediment water interface including surface water and measured using a pre calibrated wtw multiparameter sonde wtw multi 3430 m with tetracon 925 probe a ctd logger ltc levelogger from solinst was installed at pz1 from 1st april 2017 to 31st december 2017 at 10 cm above the sediment water interface to monitor water level measurements were corrected for atmospheric pressure and salinity variations in surface waters additionally a ctd logger ltc levelogger from solinst was placed at 30 cm below the sediment water interface to record changes in porewater salinities at this depth for the same period this logger was driven into sediments by using a plastic rod with a shielded protection to avoid clogging of the conductivity cell which was also protected with a membrane during installation and to minimize the disturbance of sediments changes in pressure were also recorded at 30 cm below the sediment water interface but water level gradients between this depth and surface water were too small to be measured 2 2 2 subsurface temperature time series in situ temperature data was acquired by a string of thermochron ibutton thermistors measuring systems ltd which are small size stand alone and inexpensive temperature loggers with a reported accuracy of 0 2 c and a resolution of 0 06 c johnson et al 2005 the sensors were placed at depths of 5 10 15 25 and 40 cm below the sediment water interface and at 10 cm above the seafloor by vertically driving a 2 cm diameter wooden rod with the thermistors inserted into the sediments these strings of thermistors were installed simultaneously at each of the above 4 locations in la palme lagoon pz1 pz2 pz3 pz4 during 2 periods of 1 month between may 9 and may 30 and between june 9 and july 5 the strings of thermistors were also installed between november and december 2017 but the data obtained from this deployment is not included in this manuscript because it was a period of abrupt changes in lagoon water depths which makes interpretation of the data difficult once recovered the thermistors were intercalibrated in a calibration bath during the deployment periods pressure sensors nke sp2t10 were installed at stations pz1 pz2 and pz3 measuring water depths for 5 min per hour at a frequency of 4 hz to monitor the variability of wave parameters significant wave height and period and water depths a barometer barologger edge from solinst was also installed in la palme lagoon to correct water pressures for changes in atmospheric pressure 2 3 numerical modeling 2 3 1 numerical modeling of subsurface salinities to estimate deep porewater fluxes models of salt transport have been used to estimate the exchange of water and solutes across the sediment water interface where surface and porewaters have distinctive salt concentrations martin et al 2007 2004 morris 1995 rapaglia and bokuniewicz 2009 a vertical one dimensional finite element model was developed to investigate porewater fluxes from the subsurface to the lagoon based on the equations of simmons et al 2001 voss and souza 1987 the fluid mass balance equation is 1 ρ s op p t θ ρ c c t z θ ρ v q p where ρ is the fluid density kg m3 sop is the compressibility of the saturated sediment pa 1 p is pressure pa θ is the porosity dimensionless c is the concentration of the chemical species salt kg m 3 qp is the water source or sink kg m 3 s 1 and v is the fluid velocity m s 1 defined as 2 v k μ θ p z ρ g where k is the permeability of the sediment m2 μ is the viscosity of the fluid pa s and g is the gravitational constant 9 8 m s 2 to simulate the movement of the solute species salt in this case eq 1 is coupled to the transport equation simmons et al 2001 voss and souza 1987 3 θ ρ c t z θ ρ v c z θ ρ d c z q p c p c where d is the dispersion coefficient m2 s 1 and cp is the concentration of solute species salt in the fluid source kg m 3 the equations were solved with a galerkin finite element numerical technique using one dimensional linear element which was implemented in python using the numpy and scipy libraries oliphant 2007 van der walt et al 2011 the fluid eq 1 and solute eq 3 transport equations were solved iteratively until the residuals for both pressure and concentration were 10 9 the term ρ c was assumed to be a constant value parameter values used in the one dimensional model are shown in table 1 the model was assumed to be homogeneous with uniform properties for permeability viscosity porosity and dispersity the model implemented boundary conditions of pressure and concentration at both the top and bottom node of the model with pressure approximated as p ρgh the water flux exchanged across the sediment water interface cm d 1 or cm3 cm 2 d 1 was assessed by determining the darcy flux v θ in the uppermost element the model was set with element length of 0 05 m between the lagoon bed and 1 m depth 0 1 m between 1 and 2 m and 0 2 m between 2 and 4 m depth these depths were chosen so that nodes were coincident with the location of porewater observations the upper pressure and salinity boundary conditions were taken from measured values in the lagoon see section 2 2 1 between 21st january 2016 and 29th june 2017 boundary conditions were obtained by linearly interpolating between monthly measurements in station pn1 between 29th june 2017 and 31st december 2017 the salinity and pressure values were obtained from the ctd logger installed in the surface water at station pz1 see section 2 2 1 the lower boundary condition was fixed at a constant salinity for the duration of the simulation however the lower pressure boundary was linearly varied over 6 month periods as part of model calibration notice that the variations in lower pressure boundary essentially represent variations in the inland groundwater head that are transmitted to lagoon sediments the model was implemented with a 4 hours time step with a total simulation period of two years calibration was undertaken by fitting the model results to the porewater depth profiles and the ctd logger data at 0 3 m depth all of the parameters were fixed for the calibration with the exception of hbot c bot k θ and α calibration was undertaken using the truncated newton method nash 1984 implemented in scipy oliphant 2007 the adjustable parameters hbot c bot k θ and α were modified to reduce the misfit between the modeled and observed values of salinity at depth this model was only implemented at station pz1 because it was the only station where all the input data needed for the model was collected e g porewater depth profiles surface salinities and water depths continuous data at 0 3 m the initial concentration profile was determined by linearly interpolating between the measured concentrations and the lower boundary conditions to produce a continuous concentration profile this initial concentration profile was used to generate a steady state pressure distribution in the profile to use as the starting conditions for the transient model simulation this was achieved by solving eq 1 where the variation of concentration and pressure with time were set to zero 2 3 2 numerical modeling of subsurface temperatures to estimate shallow porewater fluxes heat has been used as an environmental tracer for investigating groundwater porewater and surface water interactions in a range of hydrogeologic settings boano et al 2014 cranswick et al 2014 martin et al 2006 savidge et al 2016 its application is based on temperature differences between surface water bodies which are subject to diel or seasonal temperature variations and porewater or groundwaters which typically display reduced temperature variation cranswick et al 2014 most of the studies have applied the heat transport equation in thermal porewater records to estimate groundwater advection however it can also be applied to estimate shallow rapid porewater exchange by using a 1 d enhanced dispersion term that includes aside from thermal conductivity an effective dispersion term accounting for the increase of heat transport driven by porewater exchange bhaskar et al 2012 wilson et al 2016 in a system without net groundwater advection the enhanced dispersion coefficient can be obtained using 4 t t d e 2 t z 2 where t is temperature t is time z is depth and de is the enhanced dispersion coefficient a finite difference model was written in fortran 95 to solve eq 4 rather than calibrating the model to observed temperatures at all depths simultaneously we chose to calibrate temperature at each depth separately for discrete 48 hour periods with relatively constant wave conditions a period of 48 h was chosen because periods of high wind of much longer duration did not occur during our periods of measurement and shorter periods are less likely to induce significant temperature changes in the subsurface to evaluate the relevance of wave pumping as a driver of porewater fluxes several 48 hour periods during the different monitoring periods were selected to represent both high and low wind wave conditions although shallow porewater fluxes are likely to produce an effective dispersion coefficient that decreases with depth qian et al 2009 wilson et al 2016 we model the data using a constant dispersion coefficient but model the temperature at each depth separately the best fit dispersion coefficient de therefore represents a combination of conduction and the apparent dispersion coefficient due to porewater exchange fluxes to the relevant depth the model was run for each of the selected 48 hour periods and each piezometer and depth using different values of de in increments of 3 5 10 4 m2 d 1 the lowest rmse root mean square error value in each case identified the best fit value of de uncertainties associated with dispersion coefficients were estimated based on the shape of the rmse versus de plot for each piezometer and each 48 hour period considering the accuracy of the temperature sensors 0 2 c upper and lower bounds were defined by rmse values 0 1 c greater than the minimum rmse in each case considering the little dependence of thermal conductivity on salinity caldwell 1974 we assume that variations of porewater salinities have a negligible influence on the computations the time for temperature changes in surface water to propagate into the subsurface can be expressed as 5 t z 2 4 d e thus for de 4 10 2 m2 d 1 a typical value for enhanced dispersion coefficient due to wind and wave action see below the time for surface water temperature changes to propagate to depths z of 5 10 15 25 and 40 cm depths at which sensors were installed is 0 4 1 5 3 3 9 3 and 24 h respectively we thus chose to focus on depths of 10 and 15 cm as temperatures at greater depths do not respond sufficiently to wave conditions within the 48 hour period we also discarded the sensors at 5 cm because of uncertainties in the depth of installation and the potential effects of artifacts associated with the installation of the wooden rod e g alteration of sediment water interface to calibrate the model the upper boundary condition was specified as the measured surface water temperature sensor at 10 cm above the sediment water interface and a constant temperature 20 c was specified at a depth of 20 m by using temperature in surface water as upper boundary condition we are assuming that potential solar heating of sediments has a minor influence on the heat balance the initial condition was specified to be the measured temperatures at the start of each period with linear interpolation between observation depths initial temperatures between the deepest sensor and the model lower boundary at 20 m were also determined by linear interpolation between the deepest measurement and the specified lower boundary temperature varying the temperature value of the lower boundary confirmed that this did not affect simulated temperatures at the observation depths depth discretisation was 0 005 m and temporal discretisation was 3 5 10 5 d 0 05 min fig 2 shows how different values of de affect time series of temperature at 15 cm depth lower de values result in reduced diurnal variations in the subsurface and increased lag between temperature minimum and maximum values in the surface water and in the subsurface simulation of subsurface temperature and comparison with measured values hence allows de to be estimated enhanced dispersion coefficient de estimated following this approach include both thermal conductivity de cond and dispersion due to advective porewater exchange de adv de cond at each station can be derived from the following equation irvine et al 2015 wilson et al 2016 6 d e c o n d k b 1 θ ρ s c s θ ρ w c w k s 1 θ k w θ 1 θ ρ s c s θ ρ w c w where kb is the bulk thermal conductivity of sediments ks cs and ρs are the thermal conductivity specific heat capacity and density of the solid phase respectively kw cw and ρw are the corresponding terms for the water phase and θ is the sediment porosity 3 results 3 1 sediment analysis the grain size distributions of the 4 sediment cores collected in la palme lagoon are shown in supplementary information table s1 the contents of silt and clay in sediments from pz1 pz2 and pz3 were low generally below 10 15 sediments from these three sites were mainly composed of fine and medium size sands in contrast sediments from pz4 mainly comprised silt 40 and had a significant clay content 15 the grain size was relatively constant with depth with the only exception being pz3 which included a layer from 30 to 40 cm with a higher content of silts and clays 20 estimated sediment permeabilities from the grain size distribution and sorting following berg 1970 were on the order of 10 10 10 11 m2 for pz1 pz2 and pz3 and 10 15 m2 for pz4 sediments from pz1 pz2 and pz3 were thus characterized by a relatively high permeability huettel et al 2014 whereas pz4 were lower permeability sediments average porosities θ were 0 47 0 43 0 39 and 0 70 for pz1 pz2 pz3 and pz4 respectively tamborski et al 2018 3 2 wind wave and lagoon water depth dynamics the region is characterized by frequent strong winds 10 m s 1 generally blowing from the n w locally called tramontane and sporadic winds from the sea s e that can also reach high speeds and that are usually linked to storms indeed in 2017 most events where wind speed exceeded 10 m s 1 were blowing either from the nw 59 or the se 37 the time series of wind speeds during the main period of samplings april 2017 december 2017 is shown in fig 3 a water depths in the lagoon decreased progressively from april to september 2017 from 0 9 to 0 4 m in pz1 fig 3b mainly as a consequence of an increase in temperatures and a reduction of precipitation and groundwater inputs that resulted in evaporative losses exceeding water inputs rodellas et al 2018 during this period wind dynamics exerted a minor control on the water depths of the lagoon and were only responsible for water depth oscillations 0 5 m that lasted for 24 h significant changes in lagoon water depths were measured between 15th and 20th october predominantly se winds and 6th and 15th november nw winds as a consequence of strong wind events that opened the sandy barrier that separates the lagoon from the sea which remained open for a few days after the wind event lagoon water in the northern basin increased by 0 2 m in october as a consequence of the se wind event which brought water from the central and southern basins and the mediterranean sea and it decreased by 0 15 m in november due to nw winds after these events the sandy barrier became less consolidated and thus more permeable to water exchange and most of the subsequent wind events produced significant changes in lagoon water depths the generation of waves in the lagoon is highly controlled by the wind regime direction speed and duration as evidenced by the similar wind and wave patterns fig 4 during calm periods wave height remained below 0 02 m strong wind events produced rapid increases of wave heights wave height up to 0 10 m wave period of 1 2 s which remained elevated for the duration of the event no major differences in wave height and period were observed between the different sampling stations pz1 pz2 and pz3 which were located in different areas of the lagoon the spectral analysis of lagoon water depths revealed that the influence of seiches and tides was negligible at la palme lagoon for the studied periods we thus exclude them as drivers of porewater fluxes for la palme lagoon 3 3 porewater salinities porewater could be easily sampled with a push point piezometer from most of the depths at the locations pz1 pz2 and pz3 indicating a relatively high hydraulic permeability for the sandy sediments at these locations the only exception was a low permeability layer found at pz3 extending from 30 to 40 cm below the sediment water interface see section 3 1 porewater samples could not be collected at pz4 due to the low hydraulic conductivities which is consistent with the low permeabilities derived from sediment core particle size analysis see section 3 1 salinities in porewater mainly reflect a mixing between two endmembers fig 5 i lagoon waters with varying salinities depending on the season and the location salinities usually between 20 and 40 and ii deep hypersaline porewaters with salinities above 80 most likely from an evaporative origin fig 5 as a consequence porewater salinities generally increased downwards although these trends depend on the dynamics of this 2 endmember mixing which varies significantly depending on the sampling time and location 3 3 1 estimation of advective vertical velocities from subsurface salinities the significantly greater salinities measured in deep porewater than in shallow porewaters produce deep porewaters being significantly denser than overlying fluids notice that temperature differences between surface and deep waters differences 10 c have a minor influence on fluid density differences in comparison to the controls played by salinity differences these conditions produce stable density profiles that prevent gravitational convection or salt fingering bokuniewicz et al 2004 simmons et al 2001 however the variability of salinity in porewaters observed at each site during the different sampling periods fig 5 suggests that porewater advection driven by hydraulic head gradients exerts a major control on the vertical profiles subsurface salinity variations can be used to assess the magnitude of porewater advection i e deep porewater fluxes and their temporal variability by applying the fluid and salt transport one dimensional model described in section 2 3 1 which accounts for both density and hydraulic gradient differences the results of the observed and modeled subsurface salinities for station pz1 are shown in figs 6 and 7 and include both the porewater profiles collected at different periods fig 6 and the continuous measurements at 30 cm below the sediment water interface fig 7 the model reproduces the observed subsurface salinities remarkably well particularly for the observations at 30 cm below the sediment water interface some differences between observed and model salinities at the shallow area of the porewater profiles might be related to mechanisms driving shallow and rapid lagoon water porewater exchange e g increase porewater fluxes driven by wave pumping or bioirrigation that are not accounted for in the advection dispersion model it should also be noted that these results are also limited by the boundary conditions which were assumed i to vary linearly between monthly measurements at the top continuously measured for the last 6 months and ii to be constant for salinity and vary linearly over 6 months periods for pressure at the lower boundary the modeled vertical porewater fluxes needed to reproduce the observed subsurface salinities using the lagoon water depths and salinities measured in surface water are shown in fig 8 estimated porewater advection darcy fluxes range from 11 to 25 cm d 1 with positive fluxes representing porewater fluxes to the lagoon and negatives values the infiltration of lagoon water to the sediments these modeled porewater advection rates for station pz1 show a correspondence with lagoon water depths fig 8 negligible or negative porewater fluxes to the lagoon occurring during periods of relatively constant and high water depths e g from april to october 2017 and high upward advection rates occurring as a consequence of decreases of lagoon water depths e g from july to october 2016 this pattern is consistent with the advection of deep hypersaline porewaters driven by the hydraulic gradient largely controlled by changes on lagoon water depths steep hydraulic gradients occur in periods of shallow lagoon water depths or after the rapid drop of lagoon water levels leading to increased upward advection of porewaters it should be noted that not only the absolute lagoon water depth but also the rate of change of lagoon water depths are determining the magnitude of porewater fluxes 3 4 subsurface temperatures and derived enhanced dispersion coefficients for all locations pz1 pz2 pz3 and pz4 and deployment periods may june and november temperature records clearly show large amplitude daily fluctuations in surface waters typically 3 5 c example in fig 9 a damping in the amplitude of diurnal temperature cycles at increasing depths is immediately apparent as it is the phase shifting with increasing depth of measurement separate calibration of the numerical model within the discrete low and high wind and wave 48 hour periods was therefore performed to determine whether changes in wave regime induced changes in porewater exchange rate as reflected by values of the enhanced dispersion coefficient 3 4 1 estimation of enhanced dispersion coefficients the enhanced dispersion coefficient de for each of the selected 48 hour periods is determined by selecting the de that best fits the lowest rmse the subsurface temperature records at a given depth as an example fig 10 shows the variation in rmse vs de for the different 48 hour periods selected in june 2017 temperatures at 15 cm depth at pz1 for the two periods of low winds best fit values of de are 2 6 10 2 m2 d 1 rmse values of 0 15 and 0 12 c respectively for the three periods of high wind best fit values of de are 4 4 10 2 3 9 10 2 and 3 2 10 2 m2 d 1 rmse values of 0 22 0 19 and 0 02 c respectively variations in the best fit rmse value are probably related partly to the uniformity of the wave conditions and thus porewater exchanges and de within the chosen 48 hour periods in many cases minimum rmse values are close or lower than the accuracy of the sensors 0 2 c the best fit values of de for each profile and each of the discrete periods calm and windy periods for may and june 2017 deployments together with their uncertainties are shown in table 2 in some cases modeled temperature could not fit properly the observed temperature lowest rmse higher than temperature sensor accuracy and the de values derived from these cases are not reported the approach followed here to estimate dispersion coefficients from temperature time series is based on the assumption that there is no net porewater advection no advection term in eq 4 the relative importance of heat transport by advective to conductive heat flux can be assessed using the dimensionless thermal peclet number pe anderson 2005 bhaskar et al 2012 7 p e vl d where v is porewater velocity m d 1 l is the scale length m and d is the dispersion coefficient m2 d 1 using the maximum vertical porewater fluxes derived from the fluid salt transport model for the periods of temperature subsurface measurements v of 5 cm d 1 an average value of the estimated enhanced dispersion coefficient under calm conditions 0 03 m2 d 1 and using the mean grain diameter as the representative length l 2 10 4 m as suggested by bhaskar et al 2012 gives a thermal peclet number of 10 3 this value suggests a clear dominance of conductive heat transport over advective transport anderson 2005 a qualitative comparison can also be performed considering the time for surface water temperature changes to propagate to the subsurface few hours would be required to propagate the surface temperature signal to the depths at which sensors were installed if heat transport was dominated by dispersion e g 3 h to 15 cm below the sediment water interface see eq 5 whereas few days would be required if advection was the dominant transport mechanism 3 days we thus assume that the advective heat transfer will not significantly affect the interpretation of subsurface temperature data it should be noted that both thermal conductivity de cond and dispersion due to advective porewater exchange de adv are included within the calculated values of enhanced dispersion coefficient de the parameters used to estimate de cond following eq 6 are summarized in table 3 note that none of these parameters are constant since all of them depend on sediment or water specific properties e g water salinity and temperature sediment composition grain size and they might be highly variable duque et al 2016 thus calculated de cond should only be used as an approximation estimated de cond range from 1 8 10 2 m2 d 1 at pz4 θ 0 70 to 3 0 10 2 m2 d 1 at pz3 θ 0 70 the enhanced dispersion coefficients de derived from temperature profiles for the calm periods in may and june 2017 are in general good agreement with the theoretically calculated thermal conductivities de cond in eq 6 particularly for the stations pz1 pz2 and pz3 fig 11 this suggests that the porewater temperature records for calm periods are mainly governed by thermal conductivity notice that there is a significant disagreement between the calculated thermal conductivity and the estimated de for calm periods for pz4 but these differences could be related to the used of literature based thermal parameters instead of specific measurements for the clayey sediments of pz4 3 4 2 comparison of dispersion coefficients for calm and windy conditions as shown in table 2 and fig 11 enhanced dispersion coefficients de obtained for windy 48 hour periods for a given location are generally higher than those obtained for calm periods a kruskal wallis test was applied to compare modeled enhanced dispersion coefficients for windy and calm periods evaluating together the results from the 10 cm and 15 cm sensors for the different deployments confirming that modeled de for windy periods are significantly higher than those modeled for calm periods for the deployments of may and june 2017 kruskal wallis p 0 01 when the results are clustered by locations de for windy periods are consistently higher than those modeled for calm periods in all the stations fig 11 the difference in de at each site between calm and windy periods reveals an increase of the rate of heat transport in windy periods likely driven by enhanced porewater exchange fluxes assuming that the modeled de for the calm periods represents mainly heat transport due to thermal conductivity the effective dispersion driven by porewater exchange de adv can be estimated as the difference between de in calm and windy periods estimated de adv during the wind periods for 15 cm temperature sensors are 1 7 0 6 10 2 2 5 0 8 10 2 1 4 0 7 10 2 and 0 6 0 6 10 2 m2 d 1 for stations pz1 pz2 pz3 and pz4 in may 2017 respectively and 1 2 0 6 10 2 1 0 1 1 10 2 1 5 1 2 10 2 and 0 4 0 4 10 2 m2 d 1 for stations pz1 pz2 pz3 and pz4 in june 2017 slightly lower but comparable coefficients are estimated when using the temperature sensors installed at 10 cm below the sediment water interface the only station where dispersion driven by porewater fluxes i e differences in de modeled for calm and windy periods is not statistically significant is pz4 where the presence of low permeability sediments permeabilities 10 12 m2 likely results in a significant reduction of porewater fluxes huettel et al 2014 4 discussion 4 1 deep porewater fluxes to la palme lagoon driven by oscillations of lagoon water depths insights from porewater salinities the hydraulic gradient between the aquifer and coastal water bodies and its seasonal variations is commonly a major force driving groundwater or porewater fluxes santos et al 2012 many studies have focused on the influence and variability of inland groundwater head which is driven by the aquifer recharge anderson and emanuel 2010 michael et al 2005 sugimoto et al 2015 yu et al 2017 and only a limited number of studies have evaluated how changes on surface water levels in receiving water bodies alter the hydraulic gradient and consequently the water and solute fluxes across the land ocean interface gonneea et al 2013 lee et al 2013 michael et al 2013 in the case of la palme lagoon water depths in the lagoon are controlled by both i seasonal changes on the balance between water inputs and evaporative losses e g higher evaporation and lower water inputs in dry summer months resulting in lower water levels in summer and ii wind events that control the opening of the sandy barrier and the exchange of water between the lagoon and the open sea changes on lagoon water depths are thus occurring over relatively short time scales few days weeks when the inland hydraulic head can be assumed to be constant maximum variations in coastal piezometric levels from the alluvial aquifer are indeed on the order of 30 40 cm piezometer code bss002lrh ades eaufrance fr occurring over annual cycles in addition the relatively large size of the lagoon limits the effect of variations in inland groundwater head on porewater fluxes lagoon water depth is thus expected to contribute more to the variability in the hydraulic gradient than variation in groundwater head does as a consequence in periods of decreases of lagoon water depths the increased hydraulic gradient favors the upward advection of deep hypersaline porewaters porewater fluxes up to 25 cm d 1 as derived from the results of the fluid salt transport model and the measured subsurface salinities fig 8 similarly increases of lagoon water depths may force the infiltration of lagoon waters into the sediments driven by the reduced hydraulic gradient and density convection 4 2 shallow porewater fluxes to la palme lagoon driven by wind waves insights from temperature time series 4 2 1 drivers of increased heat transport during windy periods a number of driving forces have been identified to produce transient porewater fluxes across the sediment water interface including hydraulic gradients wave and tidal pumping interaction of bottom currents and seafloor topography density instabilities and pumping activities of benthic fauna huettel et al 2014 santos et al 2012 among all the potential drivers short term wind driven wave forcing is the only mechanism that can explain the highly dynamic nature of the observed porewater fluxes with systematically higher fluxes during windy periods as detailed in section 3 2 the strong se and mainly nw winds in the area produce locally generated wind waves that can reach significant wave heights of 5 10 cm lasting for some hours to few days wave action can drive large volume of water to circulate under the swash zone but this mechanism is only acting in the shoreline li and barry 2000 robinson et al 2014 sous et al 2016 in submerged areas waves can also induce advective shallow porewater exchange fluxes either through pressure gradients generated by the different hydrostatic pressures between wave crests and troughs or through wave induced oscillatory currents that interact with sediment topography cardenas and jiang 2011 li et al 2017 precht and huettel 2003 wind driven waves and currents in shallow areas can also produce shear stress inducing resuspension of sediments and increasing the magnitude of porewater exchange fluxes almroth rosell et al 2012 whipple et al 2018 the magnitude of wind wave driven porewater fluxes will depend on both the physical characteristics of the water body e g hydraulic conductivities of the sediments water depths and the magnitude of the forcing itself i e wave frequency wave amplitude duration of the events robinson et al 2017 qian et al 2009 developed a model to examine the effect of wave action on porewater solute profiles which related the enhanced dispersion coefficient at the sediment surface with wave and sediment parameters 8 d e 5 α k a l θ where α is the hydrodynamic dispersivity m k is the sediment hydraulic conductivity m d 1 a is the half wave amplitude and l is the wavelength m using previously derived parameters for la palme lagoon α 0 005 m l 1 m k 2 4 m d 1 cook et al 2018a and a 0 03 0 05 m derived from wave measurements a de at the sediment water interface of 0 4 1 3 10 2 m2 d 1 is calculated this range is comparable with the modeled thermal dispersion driven by porewater fluxes de adv suggesting that the increase in porewater flux in windy wave periods is consistent with wave pumping being the principal driver of porewater fluxes in la palme lagoon 4 2 2 magnitude of wave driven shallow porewater fluxes estimating the porewater flux required to create these modeled de adv is not a straightforward step rau et al 2014 for solute transport as opposed to heat transport the dispersion coefficient de can be related to porewater flux through the hydrodynamic dispersivity α 9 d e 2 q v α θ where q v is the mean upward or downward flux averaged across the upwelling and downwelling phases anderson 2005 cook et al 2018a the calculated q v is thus a function of the selected hydrodynamic dispersivity α which is a scale dependent parameter difficult to constrain for short scale porewater fluxes cook et al 2018a whereas solute dispersion depends linearly on fluid velocity the linear dependence of thermal dispersion and fluid velocity is under debate bhaskar et al 2012 molina giraldo et al 2011 rau et al 2012 assuming that the dispersion of heat is analogous to dispersion of a conservatively transported solute tracer in water calculated median porewater exchange rates during windy periods would be on the order of 50 cm d 1 derived from eq 9 using a hydrodynamic dispersivity of 0 005 m cook et al 2018a gelhar et al 1992 considering that the average water depths of la palme lagoon usually ranges from 0 5 to 1 5 m the porewater exchange rates estimated in this study would imply that the entire water volume of the lagoon would circulate through its sediments every 1 3 days i e during a multi day wind event king et al king et al 2009 used a generalized analytical model to estimate wave driven porewater rates on the order of 10 cm d 1 for a setting with characteristics similar to those from la palme lagoon wave amplitude of 5 cm wave period of 1 s water depth of 0 5 m permeability of 1011 m2 even though these estimates from king et al 2009 do not consider the porewater fluxes caused by the interaction of oscillatory flows and bottom topography which may exceed those fluxes from wave pumping alone precht and huettel 2003 these rates are comparable with the advection rates roughly estimated for wind periods in la palme lagoon thus temperature derived porewater fluxes estimated for strong wind events in la palme lagoon are likely a good order of magnitude approximation of wave driven porewater fluxes for the studied site 4 3 magnitudes and temporal scales of driving forces and porewater fluxes recent studies conducted in la palme lagoon have estimated average porewater fluxes to the entire lagoon to be on the order of 0 8 4 1 cm d 1 bejannin et al 2017 rodellas et al 2018 stieglitz et al 2013 tamborski et al 2018 these fluxes which were estimated from whole of lagoon radionuclide mass balances are in good agreement with the porewater fluxes driven by oscillations of lagoon water depths estimated in this study from the fluid salt transport model yearly averaged porewater fluxes of 1 2 cm d 1 interquartile range q1 q3 of 3 0 3 2 cm d 1 some of the whole of lagoon studies were conducted in calm periods with relatively high and constant lagoon waters depths e g april and june 2017 rodellas et al 2018 when porewater fluxes driven by oscillations of lagoon water depths and wind driven waves are expected to be low as inferred from subsurface salinities and temperatures we thus cannot exclude the existence of a porewater base flux to la palme lagoon driven by other mechanisms e g bioirrigation current topography interactions etc however results of this study provide evidence that porewater fluxes increase significantly during periods of decreases of lagoon water depths or during strong wind events as a consequence of increased hydraulic gradients and increased wave pumping respectively fig 12 importantly the two mechanisms evaluated in this study occur over different spatial and temporal scales at the larger scale variations of the lagoon water depth drive deep porewater fluxes at the scale of meters at the smaller scale wind driven waves force surface water to move in and out of the shallow sediments i e shallow porewater fluxes the length of the porewater flowpath have a large influence on the biogeochemical processes occurring within sediments and on the chemical composition of porewaters discharging across the sediment water interface heiss et al 2017 lamontagne et al 2018 weinstein et al 2011 consequently the spatial scale of porewater fluxes needs to be considered to evaluate the overall magnitude of solute inputs driven by porewater fluxes from a temporal perspective porewater fluxes driven by wave pumping will only occur during important wind events typically over periods of 1 3 days fig 4 contrarily reduced lagoon water depths occur mainly as a consequence of the high evaporative loss in summer and or strong wind events that control the opening of the sandy barrier and force the export of water towards the mediterranean sea periods of shallow water depths are typically extending from several days to few months fig 3b and thus the duration of porewater fluxes forced by reduced lagoon water depths can be far larger than that of wave induced fluxes a proper evaluation of the magnitude of porewater fluxes and their relevance for water systems thus requires understanding their temporal and spatial scales as detailed in wilson et al 2015 most of the studies conducted elsewhere evaluating porewater fluxes are focused on specific short term 1 5 days samplings that only provide snap shot observations and that are generally biased towards the summer field season and periods with calm conditions when some of the driving forces e g wind waves might not operate long term observations are thus required to capture all the potential mechanisms driving porewater fluxes including those forcings operating in sporadic intense events e g storms heavy rainfalls sawyer et al 2013 smith et al 2008 in addition the driving force that is captured will also depend on the tracer technique or approach used to estimate porewater fluxes cook et al 2018b king 2012 rodellas et al 2017 future studies in lagoons and coastal environments should focus on long term observations and combine different tracers to capture and differentiate the fluxes produced by the diverse driving mechanisms long term studies also allow isolating the driving mechanism based on temporal variations of porewater fluxes considering periods when one forcing dominates over the other cook et al 2018b as done in this study it should additionally be noted that the interaction between different forcings is generally nonlinear and porewater fluxes cannot be estimated simply as a sum of independent drivers king 2012 yu et al 2017 rather a thorough understanding of the different drivers and their interactions is required 5 conclusions this study documents the role of lagoon water depth variations and wind driven waves as drivers of porewater fluxes in a coastal lagoon the dynamics of these physical driving forces are evaluated in isolation through measurements of variations of salinity and temperature in the subsurface the temporal and vertical variability of porewater salinity profiles coupled with a fluid and salt transport model suggests that oscillations of lagoon water depth act as a major control on the fluxes of deep 1 m porewaters in periods of shallow lagoon water depths or when sudden decreases of lagoon water depths occur the increased hydraulic gradient favors the upward advection of deep hypersaline porewaters whereas porewater inputs are restricted or reversed in periods of constant and high lagoon water depths the temperature records in the lagoon subsurface coupled with a heat transport model reveal that porewater fluxes are significantly higher in windy periods as a consequence of locally generated wind waves that force the circulation of lagoon waters through sediments wave pumping and the hydraulic gradient contribute to significantly increase porewater fluxes to the lagoon during wind events and in periods with shallow lagoon water depths respectively whereas the large fluxes driven by wave pumping only flush relatively shallow sediments and are restricted to the duration of strong wind events porewater fluxes driven by the hydraulic gradient involve deeper sediments 1 m and their relevance may extend for longer periods up to few months the temporal and spatial scale of porewater fluxes will largely determine the overall magnitude of solute inputs driven by porewater fluxes an appropriate evaluation of not only the magnitude of porewater fluxes but also their underlying physical forces is thus required to fully understand the significance of these fluxes and their implications for coastal water bodies declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is a contribution to the anr raction chair anr 14 achn 0007 01 t stieglitz and labex ot med anr 11 labex 0061 part of the investissements d avenir program through the a midex project anr 11 idex 0001 02 funded by the french national research agency anr this project has received funding from the european union s horizon 2020 research and innovation programme under the marie skłodowska curie grant agreement no 748896 v rodellas acknowledges financial support from the beatriu de pinós postdoctoral programme of the catalan government 2017 bp 00334 p g cook acknowledges support from iméra institute of advanced studies aix marseille université labex rfiea and anr investissements d avenir we thank c fleger and k fortune from the parc naturel régional de la narbonnaise en méditerranée pnrnm france m david ifremer brgm cerege france v bailly comte brgm p dussouillez and j fleury cerege for their help in sampling field trips and experimentation as well as a wilson and c george university of south carolina usa for their recommendations on heat transport modeling we thank a calafat and m guart universitat de barcelona for the analysis of sediment grain size distribution we are also grateful to gladys research group www gladys littoral org who supported the experimentation appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124363 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5864,porewater fluxes including fresh groundwater discharge and circulation of surface waters through sediments are increasingly documented to play an important role in hydrological and biogeochemical cycles of coastal water bodies in most studies the magnitude of porewater fluxes is inferred from geochemical tracers but a detailed understanding of the underlying physical forces driving these fluxes remains limited in this study we evaluate the mechanisms driving porewater fluxes in the shallow coastal la palme lagoon france we combined measurements of variations of salinity and temperature in the subsurface with 1 dimensional fluid salt and heat transport models to evaluate the dynamics of porewater fluxes across the sediment water interface in response to temporally variable forcings two main processes were identified as major drivers of porewater fluxes i temporal variations of lagoon water depths forcing porewater fluxes up to 25 cm d 1 and ii locally generated wind waves porewater fluxes of 50 cm d 1 these processes operate over different spatial and temporal scales wind driven waves force the shallow circulation of surface lagoon waters through sediments mostly 0 2 m but are restricted to strong wind events typically lasting for 1 3 days in contrast porewater fluxes driven by variations of lagoon water depths flush a much greater depth of sediment 1 m the spatial and temporal scales of driving forces will largely determine the significance of porewater fluxes as well as their chemical composition thus an appropriate evaluation of the magnitude of porewater driven solute fluxes and their consequences for coastal ecosystems requires a solid and site specific understanding of the underlying physical forces keywords porewater exchange submarine groundwater discharge coastal lagoon driving forces waves salinity temperature 1 introduction water fluxes circulating through permeable sediments are increasingly being recognized as an important source of dissolved solutes e g nutrients metals pollutants to surface water bodies anschutz et al 2009 liefer et al 2013 rodellas et al 2015 in coastal settings these fluxes across the sediment water interface are commonly referred to as submarine groundwater discharge sgd or porewater exchange pex depending on the scale of the circulation process moore 2010 santos et al 2012 in this study we use the term porewater fluxes to refer to the total efflux of water and solutes across the permeable sediments to surface waters thus including both sgd and pex porewater driven fluxes of solutes may exert a major control on the biogeochemistry water quality and ecological functioning of receiving water bodies e g contributing to sustaining the primary production and community composition of phytoplankton in coastal areas andrisoa et al 2019 garcés et al 2011 valiela et al 1990 promoting eutrophication of surface waters hwang et al 2005 paerl 1997 and leading to recurrent harmful algal blooms gobler and sañudo wilhelmy 2001 lee et al 2010 the physical mechanisms driving porewater fluxes strongly affect the residence time of waters within sediments or the coastal aquifer determining the extent and rates of biogeochemical reactions and therefore the composition of discharging fluids santos et al 2012 weinstein et al 2011 an appropriate understanding of the magnitude of solute fluxes driven by groundwater and porewater discharge requires thus identifying the mechanism forcing these inputs many physical processes produce pressure gradients at the sediment water interface that can force advective porewater fluxes the main driving forces include the terrestrial hydraulic gradient and its seasonal oscillations wave and tidal pumping the interaction of currents and seafloor topography convection driven by density inversions or pumping activities of benthic fauna huettel et al 2014 santos et al 2012 these different forcing mechanisms which are of both marine e g wave and tidal setup and terrestrial e g hydraulic gradient origin are highly dynamic and irregular span a wide range of exchange lengths and timescales and are frequently superimposed robinson et al 2017 santos et al 2012 2009 a large number of studies highlight the overall magnitude and the significance of porewater fluxes e g cho et al 2018 kwon et al 2014 moore et al 2008 rodellas et al 2015 but there is still little information about their driving forces robinson et al 2017 studies conducted to date have evaluated the effect of individual driving forces in isolation and have been mainly focused on regular and short term forces e g semi diurnal diurnal tides density driven flows mainly as a consequence of the difficulties inherent in investigating irregular and longer period forcing via field experiments and in unraveling the various forcing effects robinson et al 2017 irregular forcings such as episodic high intensity events may have a great impact on fluxes of water and solutes driven by porewater fluxes sawyer et al 2013 smith et al 2008 the understanding of these forcing is thus required to better predict the effects of increasing stressors in the system e g climate change anthropogenic pressure and to better identify settings where specific forcings may dominate over the others this study is aimed at characterizing porewater fluxes in a shallow coastal lagoon la palme lagoon france where the circulation of significant volumes of surface water through sediments have been previously documented cook et al 2018a rodellas et al 2018 stieglitz et al 2013 tamborski et al 2018 these previous studies have estimated the average magnitude of porewater fluxes to the lagoon but they provided little insight into their temporal variations and the mechanisms driving these fluxes the current paper examines variations of subsurface temperature and porewater salinity in la palme lagoon to evaluate the dynamics of porewater fluxes and to provide some insight into controlling forcings in this study we focus on two mechanisms that can control porewater fluxes in the lagoon and that operate over different temporal and spatial scales i the variations of lagoon water depths which can influence the terrestrial hydraulic gradient and drive long scale 1 m porewater fluxes and ii wave pumping produced by the strong winds of the region which forces the flushing of shallow sediments short scale porewater fluxes other active mechanisms are likely significantly contributing to total porewater fluxes e g bioirrigation or current topography interactions but they are not specifically evaluated in this study 2 methods 2 1 study site la palme lagoon france la palme is a small 500 ha surface area shallow coastal lagoon with mean and maximum water depths of 0 7 and 2 m respectively fig 1 it is connected with the mediterranean sea through a small opening in the coastal sand spit which may be seasonally closed and it receives continuous fresh groundwater inputs 0 01 0 04 m3 s 1 mainly from a regional karst aquifer constituted by karstified jurassic and lower cretaceous limestones stieglitz et al 2013 wilke and boutière 2000 the lagoon is also connected with a shallow alluvial aquifer alluvial aquifer of aude and berre rivers but little information is available on the aquifer lagoon interaction the internal mixing of the lagoon and its exchange with coastal waters is driven primarily by the strong north westerly winds characteristic of the region regularly exceeding 10 m s 1 given that tidal variations in the mediterranean sea are usually small and the exchange between la palme lagoon and the sea is highly restricted by three physical barriers railway dike road dike and sandy barrier fig 1 tidal forcing plays a minor role on the hydrodynamic functioning of this lagoon tidal range in the lagoon 1 cm fiandrino et al 2012 most of the lagoon is covered by fine to coarse grained sands 100 500 µm and only the northern part of the lagoon is dominated by fine grained sediments 50 µm the eastern part of the lagoon is surrounded by evaporation ponds but there is no visual or chemical evidence of a connection between the lagoon and the salt pond rodellas et al 2018 tamborski et al 2018 a study conducted by stieglitz et al 2013 hypothesized that strong winds produced circulation of large amounts of lagoon water through surface sediments different studies have estimated porewater inputs to the entire lagoon at 0 4 2 1 m3 s 1 0 8 4 1 cm d 1 which is the equivalent of the volume of the entire lagoon circulating through the sediments every 20 90 days rodellas et al 2018 stieglitz et al 2013 tamborski et al 2018 however to date the forces driving these fluxes have not been evaluated in detail 2 2 sampling and analysis four different stations pz1 pz2 pz3 pz4 were established in areas considered representative of the different sediment types of the lagoon fig 1 in may 2017 a sediment core up to 50 cm depth was collected at each one of these locations and sliced every 5 cm the grain size distribution of each sediment sample was determined through a coulter ls230 laser diffraction particle size analyzer average sediment porosities for each location were obtained from tamborski et al 2018 who collected sediment cores at the same locations sensors for measuring temperature salinity lagoon water depths and wave parameters were also installed at these sites and porewater samples were collected as discussed in the following sections hourly rainfall temperature wind speed and direction and atmospheric pressure data at the nearby meteorological station leucate was extracted from the database of the french meteorological service météo france additional monthly data on lagoon water depths and salinity at three sites in the northern lagoon pn stations in fig 1 was obtained from the database of parc naturel régional de la narbonnaise en méditerranée pnrnm data on daily piezometric levels of the alluvial aquifer connected to the lagoon alluvial aquifer of aude and berre rivers was obtained from the french groundwater national portal piezometer code bss002lrh ades eaufrance fr 2 2 1 subsurface salinity time series porewater samples for salinity analysis were collected during 7 different sampling campaigns between march 2016 and june 2017 march april june october and november 2016 april and june 2017 during each campaign porewater samples were collected from 3 different locations pz1 pz2 and pz3 using a direct push shielded screen well point piezometer charette and allen 2006 porewater samples for salinity analysis 10 ml were collected at depths ranging from 5 to 140 cm below the sediment water interface including surface water and measured using a pre calibrated wtw multiparameter sonde wtw multi 3430 m with tetracon 925 probe a ctd logger ltc levelogger from solinst was installed at pz1 from 1st april 2017 to 31st december 2017 at 10 cm above the sediment water interface to monitor water level measurements were corrected for atmospheric pressure and salinity variations in surface waters additionally a ctd logger ltc levelogger from solinst was placed at 30 cm below the sediment water interface to record changes in porewater salinities at this depth for the same period this logger was driven into sediments by using a plastic rod with a shielded protection to avoid clogging of the conductivity cell which was also protected with a membrane during installation and to minimize the disturbance of sediments changes in pressure were also recorded at 30 cm below the sediment water interface but water level gradients between this depth and surface water were too small to be measured 2 2 2 subsurface temperature time series in situ temperature data was acquired by a string of thermochron ibutton thermistors measuring systems ltd which are small size stand alone and inexpensive temperature loggers with a reported accuracy of 0 2 c and a resolution of 0 06 c johnson et al 2005 the sensors were placed at depths of 5 10 15 25 and 40 cm below the sediment water interface and at 10 cm above the seafloor by vertically driving a 2 cm diameter wooden rod with the thermistors inserted into the sediments these strings of thermistors were installed simultaneously at each of the above 4 locations in la palme lagoon pz1 pz2 pz3 pz4 during 2 periods of 1 month between may 9 and may 30 and between june 9 and july 5 the strings of thermistors were also installed between november and december 2017 but the data obtained from this deployment is not included in this manuscript because it was a period of abrupt changes in lagoon water depths which makes interpretation of the data difficult once recovered the thermistors were intercalibrated in a calibration bath during the deployment periods pressure sensors nke sp2t10 were installed at stations pz1 pz2 and pz3 measuring water depths for 5 min per hour at a frequency of 4 hz to monitor the variability of wave parameters significant wave height and period and water depths a barometer barologger edge from solinst was also installed in la palme lagoon to correct water pressures for changes in atmospheric pressure 2 3 numerical modeling 2 3 1 numerical modeling of subsurface salinities to estimate deep porewater fluxes models of salt transport have been used to estimate the exchange of water and solutes across the sediment water interface where surface and porewaters have distinctive salt concentrations martin et al 2007 2004 morris 1995 rapaglia and bokuniewicz 2009 a vertical one dimensional finite element model was developed to investigate porewater fluxes from the subsurface to the lagoon based on the equations of simmons et al 2001 voss and souza 1987 the fluid mass balance equation is 1 ρ s op p t θ ρ c c t z θ ρ v q p where ρ is the fluid density kg m3 sop is the compressibility of the saturated sediment pa 1 p is pressure pa θ is the porosity dimensionless c is the concentration of the chemical species salt kg m 3 qp is the water source or sink kg m 3 s 1 and v is the fluid velocity m s 1 defined as 2 v k μ θ p z ρ g where k is the permeability of the sediment m2 μ is the viscosity of the fluid pa s and g is the gravitational constant 9 8 m s 2 to simulate the movement of the solute species salt in this case eq 1 is coupled to the transport equation simmons et al 2001 voss and souza 1987 3 θ ρ c t z θ ρ v c z θ ρ d c z q p c p c where d is the dispersion coefficient m2 s 1 and cp is the concentration of solute species salt in the fluid source kg m 3 the equations were solved with a galerkin finite element numerical technique using one dimensional linear element which was implemented in python using the numpy and scipy libraries oliphant 2007 van der walt et al 2011 the fluid eq 1 and solute eq 3 transport equations were solved iteratively until the residuals for both pressure and concentration were 10 9 the term ρ c was assumed to be a constant value parameter values used in the one dimensional model are shown in table 1 the model was assumed to be homogeneous with uniform properties for permeability viscosity porosity and dispersity the model implemented boundary conditions of pressure and concentration at both the top and bottom node of the model with pressure approximated as p ρgh the water flux exchanged across the sediment water interface cm d 1 or cm3 cm 2 d 1 was assessed by determining the darcy flux v θ in the uppermost element the model was set with element length of 0 05 m between the lagoon bed and 1 m depth 0 1 m between 1 and 2 m and 0 2 m between 2 and 4 m depth these depths were chosen so that nodes were coincident with the location of porewater observations the upper pressure and salinity boundary conditions were taken from measured values in the lagoon see section 2 2 1 between 21st january 2016 and 29th june 2017 boundary conditions were obtained by linearly interpolating between monthly measurements in station pn1 between 29th june 2017 and 31st december 2017 the salinity and pressure values were obtained from the ctd logger installed in the surface water at station pz1 see section 2 2 1 the lower boundary condition was fixed at a constant salinity for the duration of the simulation however the lower pressure boundary was linearly varied over 6 month periods as part of model calibration notice that the variations in lower pressure boundary essentially represent variations in the inland groundwater head that are transmitted to lagoon sediments the model was implemented with a 4 hours time step with a total simulation period of two years calibration was undertaken by fitting the model results to the porewater depth profiles and the ctd logger data at 0 3 m depth all of the parameters were fixed for the calibration with the exception of hbot c bot k θ and α calibration was undertaken using the truncated newton method nash 1984 implemented in scipy oliphant 2007 the adjustable parameters hbot c bot k θ and α were modified to reduce the misfit between the modeled and observed values of salinity at depth this model was only implemented at station pz1 because it was the only station where all the input data needed for the model was collected e g porewater depth profiles surface salinities and water depths continuous data at 0 3 m the initial concentration profile was determined by linearly interpolating between the measured concentrations and the lower boundary conditions to produce a continuous concentration profile this initial concentration profile was used to generate a steady state pressure distribution in the profile to use as the starting conditions for the transient model simulation this was achieved by solving eq 1 where the variation of concentration and pressure with time were set to zero 2 3 2 numerical modeling of subsurface temperatures to estimate shallow porewater fluxes heat has been used as an environmental tracer for investigating groundwater porewater and surface water interactions in a range of hydrogeologic settings boano et al 2014 cranswick et al 2014 martin et al 2006 savidge et al 2016 its application is based on temperature differences between surface water bodies which are subject to diel or seasonal temperature variations and porewater or groundwaters which typically display reduced temperature variation cranswick et al 2014 most of the studies have applied the heat transport equation in thermal porewater records to estimate groundwater advection however it can also be applied to estimate shallow rapid porewater exchange by using a 1 d enhanced dispersion term that includes aside from thermal conductivity an effective dispersion term accounting for the increase of heat transport driven by porewater exchange bhaskar et al 2012 wilson et al 2016 in a system without net groundwater advection the enhanced dispersion coefficient can be obtained using 4 t t d e 2 t z 2 where t is temperature t is time z is depth and de is the enhanced dispersion coefficient a finite difference model was written in fortran 95 to solve eq 4 rather than calibrating the model to observed temperatures at all depths simultaneously we chose to calibrate temperature at each depth separately for discrete 48 hour periods with relatively constant wave conditions a period of 48 h was chosen because periods of high wind of much longer duration did not occur during our periods of measurement and shorter periods are less likely to induce significant temperature changes in the subsurface to evaluate the relevance of wave pumping as a driver of porewater fluxes several 48 hour periods during the different monitoring periods were selected to represent both high and low wind wave conditions although shallow porewater fluxes are likely to produce an effective dispersion coefficient that decreases with depth qian et al 2009 wilson et al 2016 we model the data using a constant dispersion coefficient but model the temperature at each depth separately the best fit dispersion coefficient de therefore represents a combination of conduction and the apparent dispersion coefficient due to porewater exchange fluxes to the relevant depth the model was run for each of the selected 48 hour periods and each piezometer and depth using different values of de in increments of 3 5 10 4 m2 d 1 the lowest rmse root mean square error value in each case identified the best fit value of de uncertainties associated with dispersion coefficients were estimated based on the shape of the rmse versus de plot for each piezometer and each 48 hour period considering the accuracy of the temperature sensors 0 2 c upper and lower bounds were defined by rmse values 0 1 c greater than the minimum rmse in each case considering the little dependence of thermal conductivity on salinity caldwell 1974 we assume that variations of porewater salinities have a negligible influence on the computations the time for temperature changes in surface water to propagate into the subsurface can be expressed as 5 t z 2 4 d e thus for de 4 10 2 m2 d 1 a typical value for enhanced dispersion coefficient due to wind and wave action see below the time for surface water temperature changes to propagate to depths z of 5 10 15 25 and 40 cm depths at which sensors were installed is 0 4 1 5 3 3 9 3 and 24 h respectively we thus chose to focus on depths of 10 and 15 cm as temperatures at greater depths do not respond sufficiently to wave conditions within the 48 hour period we also discarded the sensors at 5 cm because of uncertainties in the depth of installation and the potential effects of artifacts associated with the installation of the wooden rod e g alteration of sediment water interface to calibrate the model the upper boundary condition was specified as the measured surface water temperature sensor at 10 cm above the sediment water interface and a constant temperature 20 c was specified at a depth of 20 m by using temperature in surface water as upper boundary condition we are assuming that potential solar heating of sediments has a minor influence on the heat balance the initial condition was specified to be the measured temperatures at the start of each period with linear interpolation between observation depths initial temperatures between the deepest sensor and the model lower boundary at 20 m were also determined by linear interpolation between the deepest measurement and the specified lower boundary temperature varying the temperature value of the lower boundary confirmed that this did not affect simulated temperatures at the observation depths depth discretisation was 0 005 m and temporal discretisation was 3 5 10 5 d 0 05 min fig 2 shows how different values of de affect time series of temperature at 15 cm depth lower de values result in reduced diurnal variations in the subsurface and increased lag between temperature minimum and maximum values in the surface water and in the subsurface simulation of subsurface temperature and comparison with measured values hence allows de to be estimated enhanced dispersion coefficient de estimated following this approach include both thermal conductivity de cond and dispersion due to advective porewater exchange de adv de cond at each station can be derived from the following equation irvine et al 2015 wilson et al 2016 6 d e c o n d k b 1 θ ρ s c s θ ρ w c w k s 1 θ k w θ 1 θ ρ s c s θ ρ w c w where kb is the bulk thermal conductivity of sediments ks cs and ρs are the thermal conductivity specific heat capacity and density of the solid phase respectively kw cw and ρw are the corresponding terms for the water phase and θ is the sediment porosity 3 results 3 1 sediment analysis the grain size distributions of the 4 sediment cores collected in la palme lagoon are shown in supplementary information table s1 the contents of silt and clay in sediments from pz1 pz2 and pz3 were low generally below 10 15 sediments from these three sites were mainly composed of fine and medium size sands in contrast sediments from pz4 mainly comprised silt 40 and had a significant clay content 15 the grain size was relatively constant with depth with the only exception being pz3 which included a layer from 30 to 40 cm with a higher content of silts and clays 20 estimated sediment permeabilities from the grain size distribution and sorting following berg 1970 were on the order of 10 10 10 11 m2 for pz1 pz2 and pz3 and 10 15 m2 for pz4 sediments from pz1 pz2 and pz3 were thus characterized by a relatively high permeability huettel et al 2014 whereas pz4 were lower permeability sediments average porosities θ were 0 47 0 43 0 39 and 0 70 for pz1 pz2 pz3 and pz4 respectively tamborski et al 2018 3 2 wind wave and lagoon water depth dynamics the region is characterized by frequent strong winds 10 m s 1 generally blowing from the n w locally called tramontane and sporadic winds from the sea s e that can also reach high speeds and that are usually linked to storms indeed in 2017 most events where wind speed exceeded 10 m s 1 were blowing either from the nw 59 or the se 37 the time series of wind speeds during the main period of samplings april 2017 december 2017 is shown in fig 3 a water depths in the lagoon decreased progressively from april to september 2017 from 0 9 to 0 4 m in pz1 fig 3b mainly as a consequence of an increase in temperatures and a reduction of precipitation and groundwater inputs that resulted in evaporative losses exceeding water inputs rodellas et al 2018 during this period wind dynamics exerted a minor control on the water depths of the lagoon and were only responsible for water depth oscillations 0 5 m that lasted for 24 h significant changes in lagoon water depths were measured between 15th and 20th october predominantly se winds and 6th and 15th november nw winds as a consequence of strong wind events that opened the sandy barrier that separates the lagoon from the sea which remained open for a few days after the wind event lagoon water in the northern basin increased by 0 2 m in october as a consequence of the se wind event which brought water from the central and southern basins and the mediterranean sea and it decreased by 0 15 m in november due to nw winds after these events the sandy barrier became less consolidated and thus more permeable to water exchange and most of the subsequent wind events produced significant changes in lagoon water depths the generation of waves in the lagoon is highly controlled by the wind regime direction speed and duration as evidenced by the similar wind and wave patterns fig 4 during calm periods wave height remained below 0 02 m strong wind events produced rapid increases of wave heights wave height up to 0 10 m wave period of 1 2 s which remained elevated for the duration of the event no major differences in wave height and period were observed between the different sampling stations pz1 pz2 and pz3 which were located in different areas of the lagoon the spectral analysis of lagoon water depths revealed that the influence of seiches and tides was negligible at la palme lagoon for the studied periods we thus exclude them as drivers of porewater fluxes for la palme lagoon 3 3 porewater salinities porewater could be easily sampled with a push point piezometer from most of the depths at the locations pz1 pz2 and pz3 indicating a relatively high hydraulic permeability for the sandy sediments at these locations the only exception was a low permeability layer found at pz3 extending from 30 to 40 cm below the sediment water interface see section 3 1 porewater samples could not be collected at pz4 due to the low hydraulic conductivities which is consistent with the low permeabilities derived from sediment core particle size analysis see section 3 1 salinities in porewater mainly reflect a mixing between two endmembers fig 5 i lagoon waters with varying salinities depending on the season and the location salinities usually between 20 and 40 and ii deep hypersaline porewaters with salinities above 80 most likely from an evaporative origin fig 5 as a consequence porewater salinities generally increased downwards although these trends depend on the dynamics of this 2 endmember mixing which varies significantly depending on the sampling time and location 3 3 1 estimation of advective vertical velocities from subsurface salinities the significantly greater salinities measured in deep porewater than in shallow porewaters produce deep porewaters being significantly denser than overlying fluids notice that temperature differences between surface and deep waters differences 10 c have a minor influence on fluid density differences in comparison to the controls played by salinity differences these conditions produce stable density profiles that prevent gravitational convection or salt fingering bokuniewicz et al 2004 simmons et al 2001 however the variability of salinity in porewaters observed at each site during the different sampling periods fig 5 suggests that porewater advection driven by hydraulic head gradients exerts a major control on the vertical profiles subsurface salinity variations can be used to assess the magnitude of porewater advection i e deep porewater fluxes and their temporal variability by applying the fluid and salt transport one dimensional model described in section 2 3 1 which accounts for both density and hydraulic gradient differences the results of the observed and modeled subsurface salinities for station pz1 are shown in figs 6 and 7 and include both the porewater profiles collected at different periods fig 6 and the continuous measurements at 30 cm below the sediment water interface fig 7 the model reproduces the observed subsurface salinities remarkably well particularly for the observations at 30 cm below the sediment water interface some differences between observed and model salinities at the shallow area of the porewater profiles might be related to mechanisms driving shallow and rapid lagoon water porewater exchange e g increase porewater fluxes driven by wave pumping or bioirrigation that are not accounted for in the advection dispersion model it should also be noted that these results are also limited by the boundary conditions which were assumed i to vary linearly between monthly measurements at the top continuously measured for the last 6 months and ii to be constant for salinity and vary linearly over 6 months periods for pressure at the lower boundary the modeled vertical porewater fluxes needed to reproduce the observed subsurface salinities using the lagoon water depths and salinities measured in surface water are shown in fig 8 estimated porewater advection darcy fluxes range from 11 to 25 cm d 1 with positive fluxes representing porewater fluxes to the lagoon and negatives values the infiltration of lagoon water to the sediments these modeled porewater advection rates for station pz1 show a correspondence with lagoon water depths fig 8 negligible or negative porewater fluxes to the lagoon occurring during periods of relatively constant and high water depths e g from april to october 2017 and high upward advection rates occurring as a consequence of decreases of lagoon water depths e g from july to october 2016 this pattern is consistent with the advection of deep hypersaline porewaters driven by the hydraulic gradient largely controlled by changes on lagoon water depths steep hydraulic gradients occur in periods of shallow lagoon water depths or after the rapid drop of lagoon water levels leading to increased upward advection of porewaters it should be noted that not only the absolute lagoon water depth but also the rate of change of lagoon water depths are determining the magnitude of porewater fluxes 3 4 subsurface temperatures and derived enhanced dispersion coefficients for all locations pz1 pz2 pz3 and pz4 and deployment periods may june and november temperature records clearly show large amplitude daily fluctuations in surface waters typically 3 5 c example in fig 9 a damping in the amplitude of diurnal temperature cycles at increasing depths is immediately apparent as it is the phase shifting with increasing depth of measurement separate calibration of the numerical model within the discrete low and high wind and wave 48 hour periods was therefore performed to determine whether changes in wave regime induced changes in porewater exchange rate as reflected by values of the enhanced dispersion coefficient 3 4 1 estimation of enhanced dispersion coefficients the enhanced dispersion coefficient de for each of the selected 48 hour periods is determined by selecting the de that best fits the lowest rmse the subsurface temperature records at a given depth as an example fig 10 shows the variation in rmse vs de for the different 48 hour periods selected in june 2017 temperatures at 15 cm depth at pz1 for the two periods of low winds best fit values of de are 2 6 10 2 m2 d 1 rmse values of 0 15 and 0 12 c respectively for the three periods of high wind best fit values of de are 4 4 10 2 3 9 10 2 and 3 2 10 2 m2 d 1 rmse values of 0 22 0 19 and 0 02 c respectively variations in the best fit rmse value are probably related partly to the uniformity of the wave conditions and thus porewater exchanges and de within the chosen 48 hour periods in many cases minimum rmse values are close or lower than the accuracy of the sensors 0 2 c the best fit values of de for each profile and each of the discrete periods calm and windy periods for may and june 2017 deployments together with their uncertainties are shown in table 2 in some cases modeled temperature could not fit properly the observed temperature lowest rmse higher than temperature sensor accuracy and the de values derived from these cases are not reported the approach followed here to estimate dispersion coefficients from temperature time series is based on the assumption that there is no net porewater advection no advection term in eq 4 the relative importance of heat transport by advective to conductive heat flux can be assessed using the dimensionless thermal peclet number pe anderson 2005 bhaskar et al 2012 7 p e vl d where v is porewater velocity m d 1 l is the scale length m and d is the dispersion coefficient m2 d 1 using the maximum vertical porewater fluxes derived from the fluid salt transport model for the periods of temperature subsurface measurements v of 5 cm d 1 an average value of the estimated enhanced dispersion coefficient under calm conditions 0 03 m2 d 1 and using the mean grain diameter as the representative length l 2 10 4 m as suggested by bhaskar et al 2012 gives a thermal peclet number of 10 3 this value suggests a clear dominance of conductive heat transport over advective transport anderson 2005 a qualitative comparison can also be performed considering the time for surface water temperature changes to propagate to the subsurface few hours would be required to propagate the surface temperature signal to the depths at which sensors were installed if heat transport was dominated by dispersion e g 3 h to 15 cm below the sediment water interface see eq 5 whereas few days would be required if advection was the dominant transport mechanism 3 days we thus assume that the advective heat transfer will not significantly affect the interpretation of subsurface temperature data it should be noted that both thermal conductivity de cond and dispersion due to advective porewater exchange de adv are included within the calculated values of enhanced dispersion coefficient de the parameters used to estimate de cond following eq 6 are summarized in table 3 note that none of these parameters are constant since all of them depend on sediment or water specific properties e g water salinity and temperature sediment composition grain size and they might be highly variable duque et al 2016 thus calculated de cond should only be used as an approximation estimated de cond range from 1 8 10 2 m2 d 1 at pz4 θ 0 70 to 3 0 10 2 m2 d 1 at pz3 θ 0 70 the enhanced dispersion coefficients de derived from temperature profiles for the calm periods in may and june 2017 are in general good agreement with the theoretically calculated thermal conductivities de cond in eq 6 particularly for the stations pz1 pz2 and pz3 fig 11 this suggests that the porewater temperature records for calm periods are mainly governed by thermal conductivity notice that there is a significant disagreement between the calculated thermal conductivity and the estimated de for calm periods for pz4 but these differences could be related to the used of literature based thermal parameters instead of specific measurements for the clayey sediments of pz4 3 4 2 comparison of dispersion coefficients for calm and windy conditions as shown in table 2 and fig 11 enhanced dispersion coefficients de obtained for windy 48 hour periods for a given location are generally higher than those obtained for calm periods a kruskal wallis test was applied to compare modeled enhanced dispersion coefficients for windy and calm periods evaluating together the results from the 10 cm and 15 cm sensors for the different deployments confirming that modeled de for windy periods are significantly higher than those modeled for calm periods for the deployments of may and june 2017 kruskal wallis p 0 01 when the results are clustered by locations de for windy periods are consistently higher than those modeled for calm periods in all the stations fig 11 the difference in de at each site between calm and windy periods reveals an increase of the rate of heat transport in windy periods likely driven by enhanced porewater exchange fluxes assuming that the modeled de for the calm periods represents mainly heat transport due to thermal conductivity the effective dispersion driven by porewater exchange de adv can be estimated as the difference between de in calm and windy periods estimated de adv during the wind periods for 15 cm temperature sensors are 1 7 0 6 10 2 2 5 0 8 10 2 1 4 0 7 10 2 and 0 6 0 6 10 2 m2 d 1 for stations pz1 pz2 pz3 and pz4 in may 2017 respectively and 1 2 0 6 10 2 1 0 1 1 10 2 1 5 1 2 10 2 and 0 4 0 4 10 2 m2 d 1 for stations pz1 pz2 pz3 and pz4 in june 2017 slightly lower but comparable coefficients are estimated when using the temperature sensors installed at 10 cm below the sediment water interface the only station where dispersion driven by porewater fluxes i e differences in de modeled for calm and windy periods is not statistically significant is pz4 where the presence of low permeability sediments permeabilities 10 12 m2 likely results in a significant reduction of porewater fluxes huettel et al 2014 4 discussion 4 1 deep porewater fluxes to la palme lagoon driven by oscillations of lagoon water depths insights from porewater salinities the hydraulic gradient between the aquifer and coastal water bodies and its seasonal variations is commonly a major force driving groundwater or porewater fluxes santos et al 2012 many studies have focused on the influence and variability of inland groundwater head which is driven by the aquifer recharge anderson and emanuel 2010 michael et al 2005 sugimoto et al 2015 yu et al 2017 and only a limited number of studies have evaluated how changes on surface water levels in receiving water bodies alter the hydraulic gradient and consequently the water and solute fluxes across the land ocean interface gonneea et al 2013 lee et al 2013 michael et al 2013 in the case of la palme lagoon water depths in the lagoon are controlled by both i seasonal changes on the balance between water inputs and evaporative losses e g higher evaporation and lower water inputs in dry summer months resulting in lower water levels in summer and ii wind events that control the opening of the sandy barrier and the exchange of water between the lagoon and the open sea changes on lagoon water depths are thus occurring over relatively short time scales few days weeks when the inland hydraulic head can be assumed to be constant maximum variations in coastal piezometric levels from the alluvial aquifer are indeed on the order of 30 40 cm piezometer code bss002lrh ades eaufrance fr occurring over annual cycles in addition the relatively large size of the lagoon limits the effect of variations in inland groundwater head on porewater fluxes lagoon water depth is thus expected to contribute more to the variability in the hydraulic gradient than variation in groundwater head does as a consequence in periods of decreases of lagoon water depths the increased hydraulic gradient favors the upward advection of deep hypersaline porewaters porewater fluxes up to 25 cm d 1 as derived from the results of the fluid salt transport model and the measured subsurface salinities fig 8 similarly increases of lagoon water depths may force the infiltration of lagoon waters into the sediments driven by the reduced hydraulic gradient and density convection 4 2 shallow porewater fluxes to la palme lagoon driven by wind waves insights from temperature time series 4 2 1 drivers of increased heat transport during windy periods a number of driving forces have been identified to produce transient porewater fluxes across the sediment water interface including hydraulic gradients wave and tidal pumping interaction of bottom currents and seafloor topography density instabilities and pumping activities of benthic fauna huettel et al 2014 santos et al 2012 among all the potential drivers short term wind driven wave forcing is the only mechanism that can explain the highly dynamic nature of the observed porewater fluxes with systematically higher fluxes during windy periods as detailed in section 3 2 the strong se and mainly nw winds in the area produce locally generated wind waves that can reach significant wave heights of 5 10 cm lasting for some hours to few days wave action can drive large volume of water to circulate under the swash zone but this mechanism is only acting in the shoreline li and barry 2000 robinson et al 2014 sous et al 2016 in submerged areas waves can also induce advective shallow porewater exchange fluxes either through pressure gradients generated by the different hydrostatic pressures between wave crests and troughs or through wave induced oscillatory currents that interact with sediment topography cardenas and jiang 2011 li et al 2017 precht and huettel 2003 wind driven waves and currents in shallow areas can also produce shear stress inducing resuspension of sediments and increasing the magnitude of porewater exchange fluxes almroth rosell et al 2012 whipple et al 2018 the magnitude of wind wave driven porewater fluxes will depend on both the physical characteristics of the water body e g hydraulic conductivities of the sediments water depths and the magnitude of the forcing itself i e wave frequency wave amplitude duration of the events robinson et al 2017 qian et al 2009 developed a model to examine the effect of wave action on porewater solute profiles which related the enhanced dispersion coefficient at the sediment surface with wave and sediment parameters 8 d e 5 α k a l θ where α is the hydrodynamic dispersivity m k is the sediment hydraulic conductivity m d 1 a is the half wave amplitude and l is the wavelength m using previously derived parameters for la palme lagoon α 0 005 m l 1 m k 2 4 m d 1 cook et al 2018a and a 0 03 0 05 m derived from wave measurements a de at the sediment water interface of 0 4 1 3 10 2 m2 d 1 is calculated this range is comparable with the modeled thermal dispersion driven by porewater fluxes de adv suggesting that the increase in porewater flux in windy wave periods is consistent with wave pumping being the principal driver of porewater fluxes in la palme lagoon 4 2 2 magnitude of wave driven shallow porewater fluxes estimating the porewater flux required to create these modeled de adv is not a straightforward step rau et al 2014 for solute transport as opposed to heat transport the dispersion coefficient de can be related to porewater flux through the hydrodynamic dispersivity α 9 d e 2 q v α θ where q v is the mean upward or downward flux averaged across the upwelling and downwelling phases anderson 2005 cook et al 2018a the calculated q v is thus a function of the selected hydrodynamic dispersivity α which is a scale dependent parameter difficult to constrain for short scale porewater fluxes cook et al 2018a whereas solute dispersion depends linearly on fluid velocity the linear dependence of thermal dispersion and fluid velocity is under debate bhaskar et al 2012 molina giraldo et al 2011 rau et al 2012 assuming that the dispersion of heat is analogous to dispersion of a conservatively transported solute tracer in water calculated median porewater exchange rates during windy periods would be on the order of 50 cm d 1 derived from eq 9 using a hydrodynamic dispersivity of 0 005 m cook et al 2018a gelhar et al 1992 considering that the average water depths of la palme lagoon usually ranges from 0 5 to 1 5 m the porewater exchange rates estimated in this study would imply that the entire water volume of the lagoon would circulate through its sediments every 1 3 days i e during a multi day wind event king et al king et al 2009 used a generalized analytical model to estimate wave driven porewater rates on the order of 10 cm d 1 for a setting with characteristics similar to those from la palme lagoon wave amplitude of 5 cm wave period of 1 s water depth of 0 5 m permeability of 1011 m2 even though these estimates from king et al 2009 do not consider the porewater fluxes caused by the interaction of oscillatory flows and bottom topography which may exceed those fluxes from wave pumping alone precht and huettel 2003 these rates are comparable with the advection rates roughly estimated for wind periods in la palme lagoon thus temperature derived porewater fluxes estimated for strong wind events in la palme lagoon are likely a good order of magnitude approximation of wave driven porewater fluxes for the studied site 4 3 magnitudes and temporal scales of driving forces and porewater fluxes recent studies conducted in la palme lagoon have estimated average porewater fluxes to the entire lagoon to be on the order of 0 8 4 1 cm d 1 bejannin et al 2017 rodellas et al 2018 stieglitz et al 2013 tamborski et al 2018 these fluxes which were estimated from whole of lagoon radionuclide mass balances are in good agreement with the porewater fluxes driven by oscillations of lagoon water depths estimated in this study from the fluid salt transport model yearly averaged porewater fluxes of 1 2 cm d 1 interquartile range q1 q3 of 3 0 3 2 cm d 1 some of the whole of lagoon studies were conducted in calm periods with relatively high and constant lagoon waters depths e g april and june 2017 rodellas et al 2018 when porewater fluxes driven by oscillations of lagoon water depths and wind driven waves are expected to be low as inferred from subsurface salinities and temperatures we thus cannot exclude the existence of a porewater base flux to la palme lagoon driven by other mechanisms e g bioirrigation current topography interactions etc however results of this study provide evidence that porewater fluxes increase significantly during periods of decreases of lagoon water depths or during strong wind events as a consequence of increased hydraulic gradients and increased wave pumping respectively fig 12 importantly the two mechanisms evaluated in this study occur over different spatial and temporal scales at the larger scale variations of the lagoon water depth drive deep porewater fluxes at the scale of meters at the smaller scale wind driven waves force surface water to move in and out of the shallow sediments i e shallow porewater fluxes the length of the porewater flowpath have a large influence on the biogeochemical processes occurring within sediments and on the chemical composition of porewaters discharging across the sediment water interface heiss et al 2017 lamontagne et al 2018 weinstein et al 2011 consequently the spatial scale of porewater fluxes needs to be considered to evaluate the overall magnitude of solute inputs driven by porewater fluxes from a temporal perspective porewater fluxes driven by wave pumping will only occur during important wind events typically over periods of 1 3 days fig 4 contrarily reduced lagoon water depths occur mainly as a consequence of the high evaporative loss in summer and or strong wind events that control the opening of the sandy barrier and force the export of water towards the mediterranean sea periods of shallow water depths are typically extending from several days to few months fig 3b and thus the duration of porewater fluxes forced by reduced lagoon water depths can be far larger than that of wave induced fluxes a proper evaluation of the magnitude of porewater fluxes and their relevance for water systems thus requires understanding their temporal and spatial scales as detailed in wilson et al 2015 most of the studies conducted elsewhere evaluating porewater fluxes are focused on specific short term 1 5 days samplings that only provide snap shot observations and that are generally biased towards the summer field season and periods with calm conditions when some of the driving forces e g wind waves might not operate long term observations are thus required to capture all the potential mechanisms driving porewater fluxes including those forcings operating in sporadic intense events e g storms heavy rainfalls sawyer et al 2013 smith et al 2008 in addition the driving force that is captured will also depend on the tracer technique or approach used to estimate porewater fluxes cook et al 2018b king 2012 rodellas et al 2017 future studies in lagoons and coastal environments should focus on long term observations and combine different tracers to capture and differentiate the fluxes produced by the diverse driving mechanisms long term studies also allow isolating the driving mechanism based on temporal variations of porewater fluxes considering periods when one forcing dominates over the other cook et al 2018b as done in this study it should additionally be noted that the interaction between different forcings is generally nonlinear and porewater fluxes cannot be estimated simply as a sum of independent drivers king 2012 yu et al 2017 rather a thorough understanding of the different drivers and their interactions is required 5 conclusions this study documents the role of lagoon water depth variations and wind driven waves as drivers of porewater fluxes in a coastal lagoon the dynamics of these physical driving forces are evaluated in isolation through measurements of variations of salinity and temperature in the subsurface the temporal and vertical variability of porewater salinity profiles coupled with a fluid and salt transport model suggests that oscillations of lagoon water depth act as a major control on the fluxes of deep 1 m porewaters in periods of shallow lagoon water depths or when sudden decreases of lagoon water depths occur the increased hydraulic gradient favors the upward advection of deep hypersaline porewaters whereas porewater inputs are restricted or reversed in periods of constant and high lagoon water depths the temperature records in the lagoon subsurface coupled with a heat transport model reveal that porewater fluxes are significantly higher in windy periods as a consequence of locally generated wind waves that force the circulation of lagoon waters through sediments wave pumping and the hydraulic gradient contribute to significantly increase porewater fluxes to the lagoon during wind events and in periods with shallow lagoon water depths respectively whereas the large fluxes driven by wave pumping only flush relatively shallow sediments and are restricted to the duration of strong wind events porewater fluxes driven by the hydraulic gradient involve deeper sediments 1 m and their relevance may extend for longer periods up to few months the temporal and spatial scale of porewater fluxes will largely determine the overall magnitude of solute inputs driven by porewater fluxes an appropriate evaluation of not only the magnitude of porewater fluxes but also their underlying physical forces is thus required to fully understand the significance of these fluxes and their implications for coastal water bodies declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is a contribution to the anr raction chair anr 14 achn 0007 01 t stieglitz and labex ot med anr 11 labex 0061 part of the investissements d avenir program through the a midex project anr 11 idex 0001 02 funded by the french national research agency anr this project has received funding from the european union s horizon 2020 research and innovation programme under the marie skłodowska curie grant agreement no 748896 v rodellas acknowledges financial support from the beatriu de pinós postdoctoral programme of the catalan government 2017 bp 00334 p g cook acknowledges support from iméra institute of advanced studies aix marseille université labex rfiea and anr investissements d avenir we thank c fleger and k fortune from the parc naturel régional de la narbonnaise en méditerranée pnrnm france m david ifremer brgm cerege france v bailly comte brgm p dussouillez and j fleury cerege for their help in sampling field trips and experimentation as well as a wilson and c george university of south carolina usa for their recommendations on heat transport modeling we thank a calafat and m guart universitat de barcelona for the analysis of sediment grain size distribution we are also grateful to gladys research group www gladys littoral org who supported the experimentation appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124363 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
