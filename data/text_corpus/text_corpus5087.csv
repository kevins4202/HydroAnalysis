index,text
25435,discrete global grid systems with quadrangular cells as reference frameworks for the current generation of earth observation data cubes rubén béjar javier lacasta francisco j lopez pellicer javier nogueras iso aragon institute of engineering research i3a universidad zaragoza spain aragon institute of engineering research i3a universidad zaragoza spain aragon institute of engineering research i3a universidad zaragoza spain corresponding author discrete global grid systems are spatial reference frameworks that associate information to multi resolution grids of uniquely identified cells they are proposed as mechanisms to facilitate the efficient integration of heterogeneous spatial data they could provide an excellent reference system for earth observation data cubes technological infrastructures that provide analysis ready access to earth observation big data as long as they can be made compatible with them in this paper we demonstrate that this is currently feasible without requiring new technological developments we show how a discrete global grid system with quadrangular cells rhealpix and an existing data cube platform open data cube can be integrated without loosing the advantages of having all the data in a discrete global grid system while keeping a straightforward access to all of the analysis tools provided by an earth observation data cube keywords discrete global grid system dggs earth observation open data cube rhealpix data availability code and data created and or used are available in a github repository referenced in the text 1 introduction the development of earth observation data cubes eo data cubes has been fueled by the increasingly pressing necessity to monitor the global environment there is a growing volume variety and velocity of big earth observation data due to an increasing amount of earth observation satellites equipped with instruments which have higher and higher spatial and spectral resolutions these spatio temporal data are made available faster and faster to the environmental research community through the implementation of better automatic processing pipelines in order to cope with this increasing amount of information the eo data cubes have been proposed as a new paradigm where the access to big earth observation data is facilitated by an infrastructure the data cube which provides analysis ready spatio temporal data to the data scientists who can focus on their research and not on the technical issues of managing and efficiently accessing to those big data giuliani et al 2019 discrete global grid systems dggss are a relatively new framework for spatial referencing focused on associating information to well known and well identified areas cells on earth their main focus and perhaps their strongest advantage is also in the integration of heterogeneous spatial big data eo data cubes can be understood as a new technological paradigm their development has been focused on facilitating an efficient access to big earth observation data while allowing the use of existing well known tools for the processing of the data the research presented in this paper addresses the problem of extending the technological capabilities of existing eo data cubes by considering other areas of improvement beyond data access performance and integration of common tools more specifically we demonstrate an original way to integrate dggss with eo data cubes this integration has been proposed before but in a way that would require major technological developments we show that this integration is feasible with the current generation of data cubes without new technological developments in the particular case of quadrangular dggss if dggs datasets are pre processed within certain constraints ingesting and processing those datasets in an existing data cube can be done without losing the characteristics of a dggs such as the well known uniquely identified and equal area cells associated to some information among the existing dggss rhealpix is a good choice for this work as its projected planar square grid is a good fit for the array oriented tools that data cubes provide and for the raster tools that most gis packages have we will demonstrate that you can take an existing raster dataset pre process it producing another raster dataset that most common gis applications and current data cubes can ingest and process and that this new raster dataset is strictly under the framework provided by the dggs each pixel in the raster dataset corresponds exactly with one cell in the dggs and matching pixels to cells does not require reprojecting or resampling in order to prove these points we have carried out an experiment first of all we have transformed some datasets both raster and vector to rhealpix rasters being careful to preserve their dggs characteristics then we have configured an instance of open data cube loading it with both the original versions of these datasets and the ones transformed to rhealpix and finally we have designed and implemented a simple geoprocess that combines those datasets in two different ways one using the original datasets and the other one using the rhealpix versions we have compared the results quantifying their differences to show that both versions of the workflow can be implemented with the same geoprocessing tools that the results are quantitatively very similar and that the data projected to rhealpix does not loose its dggs characteristics after this processing this proves that at least for some problems and with the appropriate procedures some dggss can be used with the current generation of eo data cubes without requiring any major changes to existing software systems we have found some issues that need to be addressed before this is immediately applicable at a large scale but they are small and definitely fixable the code used to run these experiments is available in a github repository https github com iaaa lab rhealpix opendatacube docker in the following section we review some related work focused on dggss open data cube rhealpix and how data cubes and dggss are being currently combined in section 3 we describe the method and tools used to carry out our experiment the datasets used as inputs the transformation of those datasets to rhealpix the indexing of those datasets in open data cube and the geoprocess that calculates some results using them in section 4 we analyze the results produced by the geoprocess both using a common spatial reference system and using rhealpix in order to quantify the differences section 5 discusses some of the decisions we have taken and the main issues which have arisen as well as some proposals for addressing or improving some of them to finish this paper section 6 summarizes the main results and proposes some research lines for future work 2 related work discrete global grid systems dggss divide the surface of the earth into tessellations of cells organized hierarchically in multi resolution grids sahr et al 2003 these grids are designed to contain and process information associated to those cells which are fixed areas and not as systems to support repeatable navigation purss 2017 every dggs must also provide an algorithm to generate a unique identification an index for each of its cells among the advantages of discrete global grids over traditional gis projections we can point out that there are not any singularities neither at the poles nor elsewhere that the spatial resolution of data is always made explicit and that their multi resolution nature makes them good candidates for combining datasets with different spatial resolutions goodchild 2018 an additional advantage is that equal area dggss allow to carry on spatial analyses which can be replicated consistently anywhere on earth purss 2017 even in higher latitudes where equal size pixels would create distortions for large areas hojati et al 2022 regarding its potential to integrate heterogeneous big data mentioned in section 1 that would be facilitated by their always congruent multi resolution cells and the hierarchical indexing schemes of those cells goodchild 2018 we are already starting to see interesting cases of this in the case of big environmental data robertson et al 2020 and indeed this kind of data is being proposed as one of the main drivers that justify the necessity for dggss hojati et al 2022 the rhealpix is a cubic geodesic dggs compatible with the ogc proposal gibb 2016 with cells that are squares once they are projected this choice square cells makes it a better at least simpler fit for some problems than other dggs which are based on hexagonal or triangular cells for example the integration of existing gridded datasets to hexagons may require different sampling and aggregation strategies for different resolutions of the dggs bousquin 2021 there are other advantages of quadrangular cells too such as the perfect congruency between adjacent levels of the dggs and the widespread use of some data structures which match them perfectly amiri et al 2015 on the other hand quadrilateral cells such as those in rhealpix would be less adequate to model dynamical systems where inter cell distances are involved as they lack uniform adjacency bowater and stefanakis 2018 in our paper we have focused on a problem that uses existing gridded datasets requires area calculations and does not use inter cell distances at all which makes it a good case for rhealpix the rhealpix dggs has other positive attributes such as low average angular and linear distortions and a perfectly congruent cell structure where every cell at one resolution level is fully contained into a cell at the previous resolution level which is something that hexagon based grids do not support there is a python library that implements the rhealpix dggs gibb et al 2013a and its projection is supported by the well known proj library proj contributors 2022 which makes it automatically available in many gis packages and applications the support provided by the proj library has allowed us to use common gis libraries such as rasterio gillies et al 2013 more easily in our work earth observation satellites and improved scientific instruments are delivering growing amounts of better quality earth observation data and the need to transform that increasing volume of data in information in a timely fashion has encouraged the development of data processing infrastructures in many cases under the paradigm of the spatial data cubes where the australian geoscience data cube is one of its main examples lewis et al 2017 this project is where the open data cube open software project killough 2018 was born a software that we are using in this paper the open data cube intends to provide a scalable open and free tool to exploit satellite data and is already been used to support data cubes in switzerland chatenoux et al 2021 several african countries mubea et al 2020 and other regions data cubes in general with different technologies and implementations are getting more and more attention as platforms for the efficient modeling and analysis of grid coverages that model multi dimensional spatio temporal data baumann 2021 regarding the relationship among data cubes and dggss there are some advantages in using a dggs as a base to implement a data cube infrastructure as its data integration engine layer and there are a number of initiatives where this combination is being explored purss et al 2019 how that is related to the results in this paper is discussed in section 5 3 a workflow to process data in open data cube using rhealpix this section describes the method and tools that we have used to validate the hypothesis of this paper which is the feasibility of processing rhealpix based datasets under a data cube paradigm with standard geoprocessing tools while keeping the advantages provided by the use of this dggs and while producing results which are accurate when compared to the ones produced with more common reference systems this method is a geodata processing workflow which uses datasets loaded into an instance of open data cube based on the cube in a box docker container open data cube 2022 and is implemented using a jupyter notebook kluyver et al 2016 this workflow produces its results in two different ways using the source datasets in their native reference systems and resolutions and using versions of those datasets previously reprojected and resampled to rhealpix the workflow solves a simple geospatial processing task because its purpose as a validation tool for the hypothesis mentioned above does not require it to be more complex both alternatives of the workflow calculate the same results the average snow cover extension in a given date april 26 2022 in two given areas the pyrenees and the part of the pyrenees that belong to the aragón region in spain a nuts level 2 territorial unit the total surface for each of these study areas with a snow cover of 75 or more in a more realistic context these or similar results would be calculated and accumulated in a daily basis and used in some decision making process or analysis for example snow cover is a variable used in forage production simulation he et al 2019 or in catchment nutrient models in cold areas costa et al 2020 to delimit the study areas we have used a bounding box of the pyrenees and kept only those parts above 1500 m in altitude the aragón boundary as a polygon has also been necessary this workflow is designed as an experiment to test not only the feasibility of using rhealpix with standard data cube and geoprocessing tools but also the accuracy of the produced results so we have also made a quantitative comparison of the results described in section 4 to keep the workflow simple and focused on testing our problem we are making our calculations with data for a single date but with the same tools that would be used for processing time series or data cubes with more dimensions fig 1 provides a graphical view of this workflow in this figure we see that the original datasets downloaded from their online sources are transformed to rhealpix using a jupyter notebook then all datasets the original ones and the rhealpix versions plus their product and dataset metadata yaml files required by open data cube which have been created by hand for this example are indexed in the open data cube finally there is another jupyter notebook which implements the rest of the workflow produces the intended results with both versions of the datasets and calculates the differences among those results all this is available in a github repository https github com iaaa lab rhealpix opendatacube docker implemented using a docker container to facilitate its deployment and testing in different environments in the following subsections we will describe the steps of the workflow roughly following the pattern suggested by apicella et al 2022 data selection download pre processing processing data integration and results presentation 3 1 data selection download and transformation to rhealpix the workflow combines datasets extracted from three products the nasadem hgt v001 nasa et al 2000 the snow cover extent 500 m europe copernicus service information 2022 and the boundary of aragón derived from the administrative units of spain instituto geográfico nacional ign es 2021 these datasets and the yaml metadata associated to them and their product definitions are indexed into the open data cube using its command line tools the rhealpix system supports a number of parameters that can be used to produce different geodesic grids we have used the wgs84 ellipsoid and made n s i d e 3 so each cell is subdivided into 9 at each new resolution level and we have positioned the north and south squares at positions 1 and 0 1 0 rhealpix these parameters are not very significant given our problem and study area so other choices could have been made another parameter is more relevant and that is the planar origin this can be shifted if for instance a study area is divided between two faces of the rhealpix cube and you prefer it contained in just one although software which is aware of the rhealpix system should be able to deal with data distributed in several faces of the cube we have found some issues with this see section 5 for more details so we have had to use 10 as our prime meridian fig 2 shows the resolution 1 cells for the 1 0 rhealpix with n s i d e 3 and prime meridian at 10 the choice of n s i d e 3 establishes the available resolutions and the cell sizes at those resolutions we have chosen to work at the resolution level which is closest to the one that we will be using for resampling the original datasets 500 m and this turns out to be level 9 for a cell width and height of 508 72 m finally we have decided that resampling is made with the nearest neighbor strategy for the rhealpix transformation and everywhere else in this paper although other resampling strategies could have produced slightly better results we wanted to focus on comparing rhealpix vs non rhealpix and thus we have kept other parameters and choices as simple as possible to transform the raster datasets to rhealpix we have used the rasterio gillies et al 2013 library to warp i e reproject and resample the datasets from their original coordinate reference systems to rhealpix and the rhealpixdggs library gibb et al 2013a to calculate the size of the cells to use the proper pixel size before the actual warping we have made the transformation matrix perfectly aligned with the rhealpix grid so each pixel center corresponds with an rhealpix cell centroid for the transformation of the polygon vector dataset aragón boundaries we have first rasterized its features using rasterio in its original reference system and then we have followed the same process as with the other raster datasets the python code used for these transformations is in a notebook named rhealpix data transformation although the rhealpix datasets are already included in the github repository so they can be immediately indexed without using this notebook at all it can be run to verify that the produced rhealpix datasets are identical to the included ones we could have included this in the data pre processing step but as this is done in a separate notebook we think that it is clearer to consider it an additional task within the data download step 3 2 pre processing processing data integration and results presentation once the datasets are indexed in the open data cube the rest of the workflow is implemented in the snow cover workflow notebook which has the following steps pre processing in the part that uses the original datasets we establish a common reference system and a common resolution to work with and then reproject and resample as needed given the datasets area of study and objectives we have chosen to use the universal transverse mercator coordinate system zone 30n and with the etrs89 datum epsg 25830 for the common resolution we have chosen the highest resolution of the datasets we are using which is 500 m we also have to create a raster mask from the aragón vector dataset so we can use it in the processing step pre processing using the rhealpix datasets is simpler given that some steps have already implemented when loading the datasets into the open data cube reprojection and resampling we do not even have to do anything special regarding the aragón geometry as at this point it is just another rhealpix raster dataset available in our open data cube that can immediately be used as mask processing data integration and results presentation these steps are simple as the calculations themselves are not complex we mask the snow cover dataset to keep only the cells inside our study areas calculate the values we are looking for in our study with those masked datasets using the tools provided by open data cube the xarray library hoyer et al 2022 and other python geoprocessing libraries and make a simple visualization of the results the comparison of the results using the original datasets and the rhealpix ones is also included in the notebook and described in the next section 4 results we have compared the results from the two variants of this workflow with and without using a dggs and quantified the differences between them there are three sources of systematic errors that would explain these differences clipping raster data with a vector geometry the aragón boundary resampling and reprojecting these processes are common in geodata processing workflows so we were not expecting large differences due to them our main focus is to make sure that rhealpix and the software libraries that implement it do not introduce large systematic errors in our results because it is not a commonly used reference system and also to test the code that we have implemented for processing the datasets we summarize the differences in two tables in table 1 we see that the data we are calculating in this study total surface with a snow cover of 75 or more and the mean and standard deviation of the snow cover percentage in the given study areas are very similar using both approaches although in absolute numbers it seems clear that the difference between the means is small we have also calculated a standardized difference between them the cohen s d to confirm that point fig 3 shows the areas on the pyrenees with a snow cover of 75 or more as a raster dataset at the top and as a vector dataset at the bottom both datasets are in rhealpix though they have been projected to mercator just to show them on a map the vector dataset has been produced by taking the raster dataset we have created with the open data cube tools reading each pixel translating the coordinates of that pixel to a resolution 9 cell of the 1 0 rhealpix with n s i d e 3 and writing the centroid of that cell with its snow cover and the unique identifier of the cell to a vector file a sample of the contents of the vector dataset is shown in table 2 in the provided notebook there is code to produce both datasets the raster and the vector ones as a geotiff and a geopackage respectively so that they can be fully explored and compared with any desktop gis application the point we want to highlight here is that we have been working with normal raster datasets with the standard tools provided by open data cube but the dggs nature of the data has not been lost because the input datasets were carefully reprojected resampled and aligned before indexing them in the open data cube and thus in our raster results each pixel corresponds exactly with a dggs cell and producing a vector version of those cells that can be processed with any vector gis application is simple and direct in the sense that it does not require any reprojection resampling or any other operation that could introduce additional systematic errors table 3 shows the results of a different validation of the results we have taken the resulting datasets from the two different approaches utm 30n and rhealpix and we have compared them on a pixel by pixel basis calculating the mean absolute error mae the bias and their correlation in these datasets the pixels with nodata appear as nan not a number so we have made our calculations taking this into account to make the datasets comparable pixel by pixel we have reprojected and resampled with the nearest neighbor strategy the dataset resulting from the rhealpix process to the crs and resolution of the dataset produced with the other geoprocess to calculate the mean absolute error mae we have subtracted the value of the pixels in one dataset from the value of the corresponding pixel in the other one taken the absolute value of this difference and obtained a nan aware mean the calculation of the bias is similar but without taking the absolute value we have calculated a correlation coefficient the spearman s ρ between both datasets finally fig 4 plots the absolute difference between both datasets on a map for the whole pyrenees to show the spatial distribution of the differences 5 discussion the work by purss et al 2019 points out that data cubes can coexist with and benefit from an underlying dggs compliant data tiling and integration scheme in this paper we are contributing to prove that point by actually integrating an existing data cube with a dggs however purss et al 2019 also point out that this integration would normally require to replace the tiling and query processing in the data cubes by dggs dggs like technology and also that a dggss structure does not need to employ spatial analytics operations to perform routine search aggregation and decomposition tasks we fully agree on the second point with the proper dggs structure and query tools you do not need spatial analytics operations nevertheless even if you do not need them you can still use them our solution does use spatial analytics operations those provided by the open data cube and this makes the integration possible without new tiling and query processing technology there are many spatial analytics raster data processing tools and libraries which expect or at least are very optimized for the case of regular grids with rectangular usually square cells discrete global grids based on hexagonal or triangular cells do not make easy cases for these tools and libraries but those global grids which are based on quadrangular cells are a different case they could offer the opportunity to make use of those tools and libraries with little if any changes the results presented in this paper do that and thus they would confirm that this opportunity is real if other research confirms this for other use cases and also for other dggss and tools we can have ahead of us a path for the adoption of dggss in different raster data processing communities these communities could continue working with their current tools while having the possibility to easily make profit from some advantages of the dggss as they introduce them in their work this path could also lead to increase the adoption of dggss based on hexagonal and triangular cells as the quadrangular dggss become better known and more used the interest in the applications where hexagonal and triangular dggss might be better options could be increased finally this path could also facilitate the transition towards data cube implementations which are more dggs aware as proposed by purss et al 2019 nevertheless there are some relatively minor issues that could make it more difficult to follow the path proposed in the previous paragraph although we have shown that we can calculate results which are very close to those obtained with common projections using the same tools we have found a number of problems specific to rhealpix and open data cube that have required solutions and workarounds that deserve some discussion first of all rhealpix accepts a number of parameters that modify both the grids and the projection of the data as briefly mentioned in section 3 1 we have shifted the prime meridian to 10 so our area of study falls in adjacent faces of the rhealpix cube this can be seen in fig 5 where the area of study is approximately depicted in red with the default prime meridian on the left the study area intersects the n p and q faces and worse although it is a bounding box that can be expressed as two pairs of geodetic coordinates once projected to the plane with rhealpix it is not a rectangle anymore because this area intersects non contiguous faces on the right side we can see that with the prime meridian at 10 the study area is contiguous and a rectangle after projecting to the plane this should not be a major issue for a software which is developed to deal with rhealpix datasets but the load operation of open data cube failed with the default prime meridian shifting the prime meridian solved the problem in our case but a long term solution would require a more robust rhealpix data selection by extent in open data cube this brings another issue to the table the proj software library supports shifting the prime meridian for rhealpix projections with the lon 0 parameter but not the prime parallel in our case shifting the prime meridian has been enough to be able to work with open data cube and rasterio as required but the capability of shifting the planar origin in both axes could be necessary or at least convenient besides this minor thing the support provided by proj to the rhealpix projection has allowed us to use common gis software such as rasterio easily in this work however with a dggs the projection is not everything you need to take into account at least the allowed resolutions and cell identifiers so the rhealpixdggs library was also necessary for this paper we have combined both libraries plus fiona gillies et al 2011 to deal with vector geometries to reproject and resample the used datasets this code besides supporting our experiment can be useful for others working with this dggs but it lacks the completeness robustness and documentation that a library to reproject datasets to rhealpix should have we propose this as future work in section 6 our rhealpix datasets are produced as rasters in the geotiff format before indexing them into open data cube and processing them with the array oriented tools provided there nevertheless as described in section 4 implemented in the jupyter notebook and shown in table 2 they can be easily transformed into a vector dataset where each pixel corresponds to a single cell in the dggs without requiring any reprojection or resampling and without losing any information at all neither spatial nor thematic as a collection of unique cell identifiers where each one corresponds to one fixed area on earth with some associated variables e g snow cover value the results produced within the data cube can be immediately exported to a csv file processed as a dataframe loaded into a sql or non sql database with or without spatial capabilities etc for the small example shown in this paper this result proves mainly that dggss and data cubes can be made compatible at least for some dggss right now however in a big data context where this is done in a consistent way for many datasets with many different workflows all of them updated often and with many other kinds of datasets spatial and non spatial available this becomes much more interesting because it underlines the major expected advantage of using dggss they facilitate the integration of heterogeneous spatial big data in section 6 we propose as future work the design of spatial data processing pipelines that integrate data cubes sql and non sql databases big data frameworks and other data storage and processing infrastructures all of them mediated by dgggs we have already seen some steps towards this kind of infrastructures robertson et al 2020 and we expect that incorporating data cubes in the way we propose here can provide further advances the validation of the proposal presented in this paper is done with a specific variable snow cover and comparing rhealpix with a utm projection on a specific study area there is not anything special about any of those choices a dggs defines a spatial structure with uniquely identified well known cells but the contents of those cells can be any kind of information and the dggss are focused on the information associated to the cells so designing them with small area distortions is important an information grid not a navigation grid as succinctly put in purss 2017 utm has been chosen because it is a common choice for the chosen study area and the rhealpix is a global projection with a small areal distortion see gibb et al 2013b appendix b so it should perform properly in any other study area to end this section there are two technical decisions of our work that deserve some discussion first as described in section 4 when we have compared cell by cell the spatial datasets resulting from working in utm 30n and in rhealpix we have calculated the spearman s ρ instead of the perhaps more common pearson correlation coefficient pearson s r this has been done because the spearman s ρ makes less assumptions on the distribution of the data it only requires a monotonic relationship so it is often less biased than the pearson s r the monotonicity condition has been checked with a scatterplot of the data which is included in the jupyter notebook and second to index a product and its datasets in open data cube there are certain metadata files in the yaml format which are required for this paper these files have been created manually as we just needed to show that it was feasible to index the rhealpix datasets in open data cube in a more realistic scenario these files should be produced automatically at the same time that the reprojection and resampling of the original datasets is done something that could also be done by that library that we are proposing as future work 6 conclusion and future work in this paper we have demonstrated that a dggs based on quadrilateral cells rhealpix can be used to process raster datasets with current data cube technology in this case open data cube without any changes to that software and with the usual array oriented tools which are commonly used in data cubes and producing results which are very similar to those produced using other projection systems a careful pre processing of the raster datasets taking into account the characteristics of rhealpix allows us to process the raster datasets as if they were projected with any common projection system without losing their dggs based structure this way we can produce easily and without losing any spatial or thematic information the final results as values associated to uniquely identified cells in larger information infrastructures these uniquely identified cells can be associated to values produced in other completely unrelated spatial processes helping thus to create a view of the earth where all kind of information is associated to a congruent multi resolution grid of cells each corresponding a well known fixed area on the earth surface and thus getting closer to that digital earth vision that dggss intend to facilitate we have discussed some issues that we have found in our work the rhealpix dggs has a number of characteristics which are still not completely supported by current spatial software libraries we have shown that some workarounds have been necessary and that we have needed to combine a number of different libraries to achieve the intended results although some improvements in the management of rhealpix and surely other dggss too are required in common spatial libraries we think that a robust generic implementation of some of the code developed for the experiments in this paper well documented and easy to install would be a useful addition to the toolbox of any spatial data analyst willing to work with rhealpix the experiment conducted in this paper suggests that a deeper integration of rhealpix and the open data cube is feasible without too much work the indexing of rhealpix products and datasets in an instance of the odc just requires a proper configuration regarding the coordinate reference system definition and the valid cell resolutions and regarding the python api that allows the processing of the datasets a first step is using it carefully for instance avoiding resampling operations if they would produce cell resolutions outside of the allowed resolutions of the dggs and including external libraries for rhealpix specific operations when needed this is what we have done in this experiment a next step that we propose as future work would be to design and implement a set of rhealpix aware and rhealpix safe operations and integrate them in the odc python api this paper has presented a small example intended to prove the feasibility of our approach and to quantify potential systematic errors however most benefit from dggss will come when they are used to facilitate integrating heterogeneous big spatial data introducing dggs processing steps in big data processing pipelines and finding out how to make those steps as efficient and automatic as possible is necessary to validate this point if we expect dggss to develop their full potential we intend to work on this problem in our next steps declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this paper is part of the r d project pid2020 113353rb i00 supported by the spanish ministry of science and innovation mcin aei 10 13039 501100011033 and the project t59 20r supported by the aragon regional government we also want to acknowledge the suggestions provided by the reviewers and the editors of the journal as they have undoubtedly helped us to improve this paper 
25435,discrete global grid systems with quadrangular cells as reference frameworks for the current generation of earth observation data cubes rubén béjar javier lacasta francisco j lopez pellicer javier nogueras iso aragon institute of engineering research i3a universidad zaragoza spain aragon institute of engineering research i3a universidad zaragoza spain aragon institute of engineering research i3a universidad zaragoza spain corresponding author discrete global grid systems are spatial reference frameworks that associate information to multi resolution grids of uniquely identified cells they are proposed as mechanisms to facilitate the efficient integration of heterogeneous spatial data they could provide an excellent reference system for earth observation data cubes technological infrastructures that provide analysis ready access to earth observation big data as long as they can be made compatible with them in this paper we demonstrate that this is currently feasible without requiring new technological developments we show how a discrete global grid system with quadrangular cells rhealpix and an existing data cube platform open data cube can be integrated without loosing the advantages of having all the data in a discrete global grid system while keeping a straightforward access to all of the analysis tools provided by an earth observation data cube keywords discrete global grid system dggs earth observation open data cube rhealpix data availability code and data created and or used are available in a github repository referenced in the text 1 introduction the development of earth observation data cubes eo data cubes has been fueled by the increasingly pressing necessity to monitor the global environment there is a growing volume variety and velocity of big earth observation data due to an increasing amount of earth observation satellites equipped with instruments which have higher and higher spatial and spectral resolutions these spatio temporal data are made available faster and faster to the environmental research community through the implementation of better automatic processing pipelines in order to cope with this increasing amount of information the eo data cubes have been proposed as a new paradigm where the access to big earth observation data is facilitated by an infrastructure the data cube which provides analysis ready spatio temporal data to the data scientists who can focus on their research and not on the technical issues of managing and efficiently accessing to those big data giuliani et al 2019 discrete global grid systems dggss are a relatively new framework for spatial referencing focused on associating information to well known and well identified areas cells on earth their main focus and perhaps their strongest advantage is also in the integration of heterogeneous spatial big data eo data cubes can be understood as a new technological paradigm their development has been focused on facilitating an efficient access to big earth observation data while allowing the use of existing well known tools for the processing of the data the research presented in this paper addresses the problem of extending the technological capabilities of existing eo data cubes by considering other areas of improvement beyond data access performance and integration of common tools more specifically we demonstrate an original way to integrate dggss with eo data cubes this integration has been proposed before but in a way that would require major technological developments we show that this integration is feasible with the current generation of data cubes without new technological developments in the particular case of quadrangular dggss if dggs datasets are pre processed within certain constraints ingesting and processing those datasets in an existing data cube can be done without losing the characteristics of a dggs such as the well known uniquely identified and equal area cells associated to some information among the existing dggss rhealpix is a good choice for this work as its projected planar square grid is a good fit for the array oriented tools that data cubes provide and for the raster tools that most gis packages have we will demonstrate that you can take an existing raster dataset pre process it producing another raster dataset that most common gis applications and current data cubes can ingest and process and that this new raster dataset is strictly under the framework provided by the dggs each pixel in the raster dataset corresponds exactly with one cell in the dggs and matching pixels to cells does not require reprojecting or resampling in order to prove these points we have carried out an experiment first of all we have transformed some datasets both raster and vector to rhealpix rasters being careful to preserve their dggs characteristics then we have configured an instance of open data cube loading it with both the original versions of these datasets and the ones transformed to rhealpix and finally we have designed and implemented a simple geoprocess that combines those datasets in two different ways one using the original datasets and the other one using the rhealpix versions we have compared the results quantifying their differences to show that both versions of the workflow can be implemented with the same geoprocessing tools that the results are quantitatively very similar and that the data projected to rhealpix does not loose its dggs characteristics after this processing this proves that at least for some problems and with the appropriate procedures some dggss can be used with the current generation of eo data cubes without requiring any major changes to existing software systems we have found some issues that need to be addressed before this is immediately applicable at a large scale but they are small and definitely fixable the code used to run these experiments is available in a github repository https github com iaaa lab rhealpix opendatacube docker in the following section we review some related work focused on dggss open data cube rhealpix and how data cubes and dggss are being currently combined in section 3 we describe the method and tools used to carry out our experiment the datasets used as inputs the transformation of those datasets to rhealpix the indexing of those datasets in open data cube and the geoprocess that calculates some results using them in section 4 we analyze the results produced by the geoprocess both using a common spatial reference system and using rhealpix in order to quantify the differences section 5 discusses some of the decisions we have taken and the main issues which have arisen as well as some proposals for addressing or improving some of them to finish this paper section 6 summarizes the main results and proposes some research lines for future work 2 related work discrete global grid systems dggss divide the surface of the earth into tessellations of cells organized hierarchically in multi resolution grids sahr et al 2003 these grids are designed to contain and process information associated to those cells which are fixed areas and not as systems to support repeatable navigation purss 2017 every dggs must also provide an algorithm to generate a unique identification an index for each of its cells among the advantages of discrete global grids over traditional gis projections we can point out that there are not any singularities neither at the poles nor elsewhere that the spatial resolution of data is always made explicit and that their multi resolution nature makes them good candidates for combining datasets with different spatial resolutions goodchild 2018 an additional advantage is that equal area dggss allow to carry on spatial analyses which can be replicated consistently anywhere on earth purss 2017 even in higher latitudes where equal size pixels would create distortions for large areas hojati et al 2022 regarding its potential to integrate heterogeneous big data mentioned in section 1 that would be facilitated by their always congruent multi resolution cells and the hierarchical indexing schemes of those cells goodchild 2018 we are already starting to see interesting cases of this in the case of big environmental data robertson et al 2020 and indeed this kind of data is being proposed as one of the main drivers that justify the necessity for dggss hojati et al 2022 the rhealpix is a cubic geodesic dggs compatible with the ogc proposal gibb 2016 with cells that are squares once they are projected this choice square cells makes it a better at least simpler fit for some problems than other dggs which are based on hexagonal or triangular cells for example the integration of existing gridded datasets to hexagons may require different sampling and aggregation strategies for different resolutions of the dggs bousquin 2021 there are other advantages of quadrangular cells too such as the perfect congruency between adjacent levels of the dggs and the widespread use of some data structures which match them perfectly amiri et al 2015 on the other hand quadrilateral cells such as those in rhealpix would be less adequate to model dynamical systems where inter cell distances are involved as they lack uniform adjacency bowater and stefanakis 2018 in our paper we have focused on a problem that uses existing gridded datasets requires area calculations and does not use inter cell distances at all which makes it a good case for rhealpix the rhealpix dggs has other positive attributes such as low average angular and linear distortions and a perfectly congruent cell structure where every cell at one resolution level is fully contained into a cell at the previous resolution level which is something that hexagon based grids do not support there is a python library that implements the rhealpix dggs gibb et al 2013a and its projection is supported by the well known proj library proj contributors 2022 which makes it automatically available in many gis packages and applications the support provided by the proj library has allowed us to use common gis libraries such as rasterio gillies et al 2013 more easily in our work earth observation satellites and improved scientific instruments are delivering growing amounts of better quality earth observation data and the need to transform that increasing volume of data in information in a timely fashion has encouraged the development of data processing infrastructures in many cases under the paradigm of the spatial data cubes where the australian geoscience data cube is one of its main examples lewis et al 2017 this project is where the open data cube open software project killough 2018 was born a software that we are using in this paper the open data cube intends to provide a scalable open and free tool to exploit satellite data and is already been used to support data cubes in switzerland chatenoux et al 2021 several african countries mubea et al 2020 and other regions data cubes in general with different technologies and implementations are getting more and more attention as platforms for the efficient modeling and analysis of grid coverages that model multi dimensional spatio temporal data baumann 2021 regarding the relationship among data cubes and dggss there are some advantages in using a dggs as a base to implement a data cube infrastructure as its data integration engine layer and there are a number of initiatives where this combination is being explored purss et al 2019 how that is related to the results in this paper is discussed in section 5 3 a workflow to process data in open data cube using rhealpix this section describes the method and tools that we have used to validate the hypothesis of this paper which is the feasibility of processing rhealpix based datasets under a data cube paradigm with standard geoprocessing tools while keeping the advantages provided by the use of this dggs and while producing results which are accurate when compared to the ones produced with more common reference systems this method is a geodata processing workflow which uses datasets loaded into an instance of open data cube based on the cube in a box docker container open data cube 2022 and is implemented using a jupyter notebook kluyver et al 2016 this workflow produces its results in two different ways using the source datasets in their native reference systems and resolutions and using versions of those datasets previously reprojected and resampled to rhealpix the workflow solves a simple geospatial processing task because its purpose as a validation tool for the hypothesis mentioned above does not require it to be more complex both alternatives of the workflow calculate the same results the average snow cover extension in a given date april 26 2022 in two given areas the pyrenees and the part of the pyrenees that belong to the aragón region in spain a nuts level 2 territorial unit the total surface for each of these study areas with a snow cover of 75 or more in a more realistic context these or similar results would be calculated and accumulated in a daily basis and used in some decision making process or analysis for example snow cover is a variable used in forage production simulation he et al 2019 or in catchment nutrient models in cold areas costa et al 2020 to delimit the study areas we have used a bounding box of the pyrenees and kept only those parts above 1500 m in altitude the aragón boundary as a polygon has also been necessary this workflow is designed as an experiment to test not only the feasibility of using rhealpix with standard data cube and geoprocessing tools but also the accuracy of the produced results so we have also made a quantitative comparison of the results described in section 4 to keep the workflow simple and focused on testing our problem we are making our calculations with data for a single date but with the same tools that would be used for processing time series or data cubes with more dimensions fig 1 provides a graphical view of this workflow in this figure we see that the original datasets downloaded from their online sources are transformed to rhealpix using a jupyter notebook then all datasets the original ones and the rhealpix versions plus their product and dataset metadata yaml files required by open data cube which have been created by hand for this example are indexed in the open data cube finally there is another jupyter notebook which implements the rest of the workflow produces the intended results with both versions of the datasets and calculates the differences among those results all this is available in a github repository https github com iaaa lab rhealpix opendatacube docker implemented using a docker container to facilitate its deployment and testing in different environments in the following subsections we will describe the steps of the workflow roughly following the pattern suggested by apicella et al 2022 data selection download pre processing processing data integration and results presentation 3 1 data selection download and transformation to rhealpix the workflow combines datasets extracted from three products the nasadem hgt v001 nasa et al 2000 the snow cover extent 500 m europe copernicus service information 2022 and the boundary of aragón derived from the administrative units of spain instituto geográfico nacional ign es 2021 these datasets and the yaml metadata associated to them and their product definitions are indexed into the open data cube using its command line tools the rhealpix system supports a number of parameters that can be used to produce different geodesic grids we have used the wgs84 ellipsoid and made n s i d e 3 so each cell is subdivided into 9 at each new resolution level and we have positioned the north and south squares at positions 1 and 0 1 0 rhealpix these parameters are not very significant given our problem and study area so other choices could have been made another parameter is more relevant and that is the planar origin this can be shifted if for instance a study area is divided between two faces of the rhealpix cube and you prefer it contained in just one although software which is aware of the rhealpix system should be able to deal with data distributed in several faces of the cube we have found some issues with this see section 5 for more details so we have had to use 10 as our prime meridian fig 2 shows the resolution 1 cells for the 1 0 rhealpix with n s i d e 3 and prime meridian at 10 the choice of n s i d e 3 establishes the available resolutions and the cell sizes at those resolutions we have chosen to work at the resolution level which is closest to the one that we will be using for resampling the original datasets 500 m and this turns out to be level 9 for a cell width and height of 508 72 m finally we have decided that resampling is made with the nearest neighbor strategy for the rhealpix transformation and everywhere else in this paper although other resampling strategies could have produced slightly better results we wanted to focus on comparing rhealpix vs non rhealpix and thus we have kept other parameters and choices as simple as possible to transform the raster datasets to rhealpix we have used the rasterio gillies et al 2013 library to warp i e reproject and resample the datasets from their original coordinate reference systems to rhealpix and the rhealpixdggs library gibb et al 2013a to calculate the size of the cells to use the proper pixel size before the actual warping we have made the transformation matrix perfectly aligned with the rhealpix grid so each pixel center corresponds with an rhealpix cell centroid for the transformation of the polygon vector dataset aragón boundaries we have first rasterized its features using rasterio in its original reference system and then we have followed the same process as with the other raster datasets the python code used for these transformations is in a notebook named rhealpix data transformation although the rhealpix datasets are already included in the github repository so they can be immediately indexed without using this notebook at all it can be run to verify that the produced rhealpix datasets are identical to the included ones we could have included this in the data pre processing step but as this is done in a separate notebook we think that it is clearer to consider it an additional task within the data download step 3 2 pre processing processing data integration and results presentation once the datasets are indexed in the open data cube the rest of the workflow is implemented in the snow cover workflow notebook which has the following steps pre processing in the part that uses the original datasets we establish a common reference system and a common resolution to work with and then reproject and resample as needed given the datasets area of study and objectives we have chosen to use the universal transverse mercator coordinate system zone 30n and with the etrs89 datum epsg 25830 for the common resolution we have chosen the highest resolution of the datasets we are using which is 500 m we also have to create a raster mask from the aragón vector dataset so we can use it in the processing step pre processing using the rhealpix datasets is simpler given that some steps have already implemented when loading the datasets into the open data cube reprojection and resampling we do not even have to do anything special regarding the aragón geometry as at this point it is just another rhealpix raster dataset available in our open data cube that can immediately be used as mask processing data integration and results presentation these steps are simple as the calculations themselves are not complex we mask the snow cover dataset to keep only the cells inside our study areas calculate the values we are looking for in our study with those masked datasets using the tools provided by open data cube the xarray library hoyer et al 2022 and other python geoprocessing libraries and make a simple visualization of the results the comparison of the results using the original datasets and the rhealpix ones is also included in the notebook and described in the next section 4 results we have compared the results from the two variants of this workflow with and without using a dggs and quantified the differences between them there are three sources of systematic errors that would explain these differences clipping raster data with a vector geometry the aragón boundary resampling and reprojecting these processes are common in geodata processing workflows so we were not expecting large differences due to them our main focus is to make sure that rhealpix and the software libraries that implement it do not introduce large systematic errors in our results because it is not a commonly used reference system and also to test the code that we have implemented for processing the datasets we summarize the differences in two tables in table 1 we see that the data we are calculating in this study total surface with a snow cover of 75 or more and the mean and standard deviation of the snow cover percentage in the given study areas are very similar using both approaches although in absolute numbers it seems clear that the difference between the means is small we have also calculated a standardized difference between them the cohen s d to confirm that point fig 3 shows the areas on the pyrenees with a snow cover of 75 or more as a raster dataset at the top and as a vector dataset at the bottom both datasets are in rhealpix though they have been projected to mercator just to show them on a map the vector dataset has been produced by taking the raster dataset we have created with the open data cube tools reading each pixel translating the coordinates of that pixel to a resolution 9 cell of the 1 0 rhealpix with n s i d e 3 and writing the centroid of that cell with its snow cover and the unique identifier of the cell to a vector file a sample of the contents of the vector dataset is shown in table 2 in the provided notebook there is code to produce both datasets the raster and the vector ones as a geotiff and a geopackage respectively so that they can be fully explored and compared with any desktop gis application the point we want to highlight here is that we have been working with normal raster datasets with the standard tools provided by open data cube but the dggs nature of the data has not been lost because the input datasets were carefully reprojected resampled and aligned before indexing them in the open data cube and thus in our raster results each pixel corresponds exactly with a dggs cell and producing a vector version of those cells that can be processed with any vector gis application is simple and direct in the sense that it does not require any reprojection resampling or any other operation that could introduce additional systematic errors table 3 shows the results of a different validation of the results we have taken the resulting datasets from the two different approaches utm 30n and rhealpix and we have compared them on a pixel by pixel basis calculating the mean absolute error mae the bias and their correlation in these datasets the pixels with nodata appear as nan not a number so we have made our calculations taking this into account to make the datasets comparable pixel by pixel we have reprojected and resampled with the nearest neighbor strategy the dataset resulting from the rhealpix process to the crs and resolution of the dataset produced with the other geoprocess to calculate the mean absolute error mae we have subtracted the value of the pixels in one dataset from the value of the corresponding pixel in the other one taken the absolute value of this difference and obtained a nan aware mean the calculation of the bias is similar but without taking the absolute value we have calculated a correlation coefficient the spearman s ρ between both datasets finally fig 4 plots the absolute difference between both datasets on a map for the whole pyrenees to show the spatial distribution of the differences 5 discussion the work by purss et al 2019 points out that data cubes can coexist with and benefit from an underlying dggs compliant data tiling and integration scheme in this paper we are contributing to prove that point by actually integrating an existing data cube with a dggs however purss et al 2019 also point out that this integration would normally require to replace the tiling and query processing in the data cubes by dggs dggs like technology and also that a dggss structure does not need to employ spatial analytics operations to perform routine search aggregation and decomposition tasks we fully agree on the second point with the proper dggs structure and query tools you do not need spatial analytics operations nevertheless even if you do not need them you can still use them our solution does use spatial analytics operations those provided by the open data cube and this makes the integration possible without new tiling and query processing technology there are many spatial analytics raster data processing tools and libraries which expect or at least are very optimized for the case of regular grids with rectangular usually square cells discrete global grids based on hexagonal or triangular cells do not make easy cases for these tools and libraries but those global grids which are based on quadrangular cells are a different case they could offer the opportunity to make use of those tools and libraries with little if any changes the results presented in this paper do that and thus they would confirm that this opportunity is real if other research confirms this for other use cases and also for other dggss and tools we can have ahead of us a path for the adoption of dggss in different raster data processing communities these communities could continue working with their current tools while having the possibility to easily make profit from some advantages of the dggss as they introduce them in their work this path could also lead to increase the adoption of dggss based on hexagonal and triangular cells as the quadrangular dggss become better known and more used the interest in the applications where hexagonal and triangular dggss might be better options could be increased finally this path could also facilitate the transition towards data cube implementations which are more dggs aware as proposed by purss et al 2019 nevertheless there are some relatively minor issues that could make it more difficult to follow the path proposed in the previous paragraph although we have shown that we can calculate results which are very close to those obtained with common projections using the same tools we have found a number of problems specific to rhealpix and open data cube that have required solutions and workarounds that deserve some discussion first of all rhealpix accepts a number of parameters that modify both the grids and the projection of the data as briefly mentioned in section 3 1 we have shifted the prime meridian to 10 so our area of study falls in adjacent faces of the rhealpix cube this can be seen in fig 5 where the area of study is approximately depicted in red with the default prime meridian on the left the study area intersects the n p and q faces and worse although it is a bounding box that can be expressed as two pairs of geodetic coordinates once projected to the plane with rhealpix it is not a rectangle anymore because this area intersects non contiguous faces on the right side we can see that with the prime meridian at 10 the study area is contiguous and a rectangle after projecting to the plane this should not be a major issue for a software which is developed to deal with rhealpix datasets but the load operation of open data cube failed with the default prime meridian shifting the prime meridian solved the problem in our case but a long term solution would require a more robust rhealpix data selection by extent in open data cube this brings another issue to the table the proj software library supports shifting the prime meridian for rhealpix projections with the lon 0 parameter but not the prime parallel in our case shifting the prime meridian has been enough to be able to work with open data cube and rasterio as required but the capability of shifting the planar origin in both axes could be necessary or at least convenient besides this minor thing the support provided by proj to the rhealpix projection has allowed us to use common gis software such as rasterio easily in this work however with a dggs the projection is not everything you need to take into account at least the allowed resolutions and cell identifiers so the rhealpixdggs library was also necessary for this paper we have combined both libraries plus fiona gillies et al 2011 to deal with vector geometries to reproject and resample the used datasets this code besides supporting our experiment can be useful for others working with this dggs but it lacks the completeness robustness and documentation that a library to reproject datasets to rhealpix should have we propose this as future work in section 6 our rhealpix datasets are produced as rasters in the geotiff format before indexing them into open data cube and processing them with the array oriented tools provided there nevertheless as described in section 4 implemented in the jupyter notebook and shown in table 2 they can be easily transformed into a vector dataset where each pixel corresponds to a single cell in the dggs without requiring any reprojection or resampling and without losing any information at all neither spatial nor thematic as a collection of unique cell identifiers where each one corresponds to one fixed area on earth with some associated variables e g snow cover value the results produced within the data cube can be immediately exported to a csv file processed as a dataframe loaded into a sql or non sql database with or without spatial capabilities etc for the small example shown in this paper this result proves mainly that dggss and data cubes can be made compatible at least for some dggss right now however in a big data context where this is done in a consistent way for many datasets with many different workflows all of them updated often and with many other kinds of datasets spatial and non spatial available this becomes much more interesting because it underlines the major expected advantage of using dggss they facilitate the integration of heterogeneous spatial big data in section 6 we propose as future work the design of spatial data processing pipelines that integrate data cubes sql and non sql databases big data frameworks and other data storage and processing infrastructures all of them mediated by dgggs we have already seen some steps towards this kind of infrastructures robertson et al 2020 and we expect that incorporating data cubes in the way we propose here can provide further advances the validation of the proposal presented in this paper is done with a specific variable snow cover and comparing rhealpix with a utm projection on a specific study area there is not anything special about any of those choices a dggs defines a spatial structure with uniquely identified well known cells but the contents of those cells can be any kind of information and the dggss are focused on the information associated to the cells so designing them with small area distortions is important an information grid not a navigation grid as succinctly put in purss 2017 utm has been chosen because it is a common choice for the chosen study area and the rhealpix is a global projection with a small areal distortion see gibb et al 2013b appendix b so it should perform properly in any other study area to end this section there are two technical decisions of our work that deserve some discussion first as described in section 4 when we have compared cell by cell the spatial datasets resulting from working in utm 30n and in rhealpix we have calculated the spearman s ρ instead of the perhaps more common pearson correlation coefficient pearson s r this has been done because the spearman s ρ makes less assumptions on the distribution of the data it only requires a monotonic relationship so it is often less biased than the pearson s r the monotonicity condition has been checked with a scatterplot of the data which is included in the jupyter notebook and second to index a product and its datasets in open data cube there are certain metadata files in the yaml format which are required for this paper these files have been created manually as we just needed to show that it was feasible to index the rhealpix datasets in open data cube in a more realistic scenario these files should be produced automatically at the same time that the reprojection and resampling of the original datasets is done something that could also be done by that library that we are proposing as future work 6 conclusion and future work in this paper we have demonstrated that a dggs based on quadrilateral cells rhealpix can be used to process raster datasets with current data cube technology in this case open data cube without any changes to that software and with the usual array oriented tools which are commonly used in data cubes and producing results which are very similar to those produced using other projection systems a careful pre processing of the raster datasets taking into account the characteristics of rhealpix allows us to process the raster datasets as if they were projected with any common projection system without losing their dggs based structure this way we can produce easily and without losing any spatial or thematic information the final results as values associated to uniquely identified cells in larger information infrastructures these uniquely identified cells can be associated to values produced in other completely unrelated spatial processes helping thus to create a view of the earth where all kind of information is associated to a congruent multi resolution grid of cells each corresponding a well known fixed area on the earth surface and thus getting closer to that digital earth vision that dggss intend to facilitate we have discussed some issues that we have found in our work the rhealpix dggs has a number of characteristics which are still not completely supported by current spatial software libraries we have shown that some workarounds have been necessary and that we have needed to combine a number of different libraries to achieve the intended results although some improvements in the management of rhealpix and surely other dggss too are required in common spatial libraries we think that a robust generic implementation of some of the code developed for the experiments in this paper well documented and easy to install would be a useful addition to the toolbox of any spatial data analyst willing to work with rhealpix the experiment conducted in this paper suggests that a deeper integration of rhealpix and the open data cube is feasible without too much work the indexing of rhealpix products and datasets in an instance of the odc just requires a proper configuration regarding the coordinate reference system definition and the valid cell resolutions and regarding the python api that allows the processing of the datasets a first step is using it carefully for instance avoiding resampling operations if they would produce cell resolutions outside of the allowed resolutions of the dggs and including external libraries for rhealpix specific operations when needed this is what we have done in this experiment a next step that we propose as future work would be to design and implement a set of rhealpix aware and rhealpix safe operations and integrate them in the odc python api this paper has presented a small example intended to prove the feasibility of our approach and to quantify potential systematic errors however most benefit from dggss will come when they are used to facilitate integrating heterogeneous big spatial data introducing dggs processing steps in big data processing pipelines and finding out how to make those steps as efficient and automatic as possible is necessary to validate this point if we expect dggss to develop their full potential we intend to work on this problem in our next steps declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this paper is part of the r d project pid2020 113353rb i00 supported by the spanish ministry of science and innovation mcin aei 10 13039 501100011033 and the project t59 20r supported by the aragon regional government we also want to acknowledge the suggestions provided by the reviewers and the editors of the journal as they have undoubtedly helped us to improve this paper 
25436,lake erie has experienced a re emergence of cyanobacterial harmful algal blooms chabs since the early 2000s posing significant socioeconomic and ecological consequences that impact drinking water human health fisheries tourism and water quality as predicting chab intensity and spatial distribution is critical to lake erie ecosystem management this study focuses on a comprehensive evaluation of lagrangian and eulerian transport models for lake erie chab forecasts including 1 a lagrangian particle model lpm 2 an eulerian tracer model etm and 3 a property carrying particle model pcpm that utilizes the hybrid eulerian lagrangian approach we evaluated the models performance against the latest high resolution satellite product from the european space agency s sentinel 3 olci sensor over 24 to 240 h hindcasts for each chab occurrence in three consecutive chab seasons 2017 2019 we examined the relative contributions of horizontal transport vertical turbulent mixing and algal buoyancy on the chab inter and intra day variability in the short term forecast we emphasize the highly dynamic reaction of currents to weather scale wind events that are crucial to chab transport while statistical skill assessments show that these three transport models attain comparable levels of hindcast accuracy we explore the advantages and disadvantages of each model in the context of general biophysical modeling in particular the fact that the etm and pcpm perform as well as or better than the lpm sets up a promising path to developing more biological realism in future operational forecast models using eulerian or hybrid approaches keywords harmful algal bloom hydrodynamics transport model lake erie the great lakes data availability data will be made available on request 1 introduction lake erie is the most productive lake among the five laurentian great lakes of north america the re emergence of cyanobacterial harmful algal blooms chabs since the early 2000s has been known as one of the most severe environmental problems in the great lakes watson et al 2016 liu et al 2020 as a result of eutrophication conditions caused by the excessive nutrient input primarily from agricultural use baker et al 2014 the most severe chabs originate in the shallow western basin of lake erie fig 1 associated with nutrient loading from the maumee river in the southwest of the basin kane et al 2014 chabs in lake erie are dominated by microcystis aeruginosa which produces a family of algal toxins known as microcystins that are dangerous to humans and animals microcystis aeruginosa is a species of freshwater cyanobacteria that can regulate the buoyancy of its colonies with most of the colonies being positively buoyant in lake erie den uyl et al 2021 chabs have high socioeconomic and ecological costs in lake erie impacting drinking water human health fisheries tourism and water quality for example on august 2nd 2014 the city of toledo issued a do not drink notice for their drinking water system that affected 0 5 million people for three days as a result of contamination of treated drinking water with microcystins associated with a severe chab event stumpf et al 2016 steffen et al 2017 the severe impact caused by chabs promoted the development of an improved short term forecast system for chab distribution and transport rowe et al 2016 as physical processes play a dominant role in explaining short term bloom variability rowe et al 2016 short term chab forecasts often employ a mass conserving advection diffusion model such a model is typically forced by a hydrodynamic forecast with chab spatial distribution initialized from satellite remote sensing to predict the bloom location and intensity several days into the future the partial differential equations governing algae transport can be solved using a lagrangian approach an eulerian approach or a combination of the two i e a hybrid eulerian lagrangian approach in lake erie both the lagrangian and eulerian approaches have been tested for chab hindcast simulations wynne et al 2013 rowe et al 2016 soontiens et al 2019 the lagrangian particle model lpm describes motion by following individual particle trajectories as the particles move through space and time each particle represents a certain amount of planktonic or chlorophyll mass and is transported by three dimensional velocity components and turbulent diffusivity output from a hydrodynamic model the chab condition is therefore expressed as an accumulative behavior of phytoplanktonic particles as each individual particle can contain a set of attributes e g colony size buoyant velocity with corresponding physiological traits and behavioral traits this approach allows for considering the colony variability of chabs henrichs et al 2015 the lpm is limited in its ability to represent continuous concentration fields because of the need to count particles within a control volume to represent a concentration unlike the lpm in which mass is transported as discrete particles the eulerian tracer model etm expresses planktonic biomass or chlorophyll as tracer state variables and calculates their concentrations in each model grid cell the eulerian specification of a field calculates the local rate of change at fixed locations grid cells determined by advection and turbulent diffusion as well as its biophysical source and sink terms while a few previous studies have assessed short term hindcast simulations of lake erie chabs using lagrangian or eulerian approaches drawing an insightful conclusion on model performances across these approaches remains challenging this is due to the fact that these models were evaluated using different skill measurements different biophysical settings different forecast periods and or different model versions for example wynne et al 2013 focused on a two dimensional 2d lpm that was initialized with satellite remote sensing data and driven by a hydrodynamic model output to hindcast the chabs for 2008 2010 rowe et al 2016 took a similar approach but applied a 3d lpm for each chab event in the 2011 chab season for up to 240 h hindcasts by taking into account the vertical distribution and buoyancy of microcystis colonies the 3d version of the lpm delivered more skillful hindcasts than its 2d version more recently a 3d etm was compared to a 2d lpm rather than a 3d lpm that limits particles to the lake surface by soontiens et al 2019 the comparison revealed that the 3d etm outperformed the 2d lpm by a wide margin however a 3d etm and 3d lpm were not directly compared in contrast to the 120 h and 240 h hindcasts from wynne et al 2013 and rowe et al 2016 soontiens et al 2019 assessed shorter 48 h hindcasts from july 27th to october 8th 2017 therefore this study aims for a comprehensive model skill assessment directly comparing 3d versions of eulerian and lagrangian chab forecast approaches including assessing a hybrid eulerian lagrangian approach see section 2 4 for the model description these models were run in the latest 3d version at the time of the writing and were assessed under identical biophysical conditions and statistical skill metrics for each chab occurrence in three consecutive chab seasons 2017 2019 we evaluated model performance against the latest high resolution satellite product from the european space agency s sentinel 3 olci sensor over 24 to 240 h hindcasts then we established a statistical skill assessment based on all 43 chab events to identify each model s strengths and limitations 2 methods 2 1 hydrodynamic model the finite volume community ocean model fvcom is a three dimensional free surface primitive equation hydrodynamic model that solves the integral form of the governing equations on an unstructured sigma coordinate mesh the advantage of an unstructured grid mesh for shoreline fitting and local mesh refinement makes the model particularly popular in applications to coastal waters xue et al 2014 2020 2022 ibrahim et al 2020 huang et al 2021a b fvcom has been applied in many coastal systems characterized by geometric complexities and highly variable flow patterns including various applications to lake erie kelley et al 2018 rowe et al 2019 ye et al 2020 the lake erie le fvcom has an unstructured grid mesh of 6106 nodes and 11509 elements with the water column divided into 20 uniform thickness terrain following sigma layers fig 2 a the mesh has a grid resolution of 2 5 km in the central basin 1 5 km in the western basin and 0 5 km in maumee bay and the islands the open boundary conditions include primary inflow from the detroit river and outflow through the niagara river with specified hourly water levels using the noaa nos gauges at gibraltar 9044020 and buffalo 9063020 the le fvcom is driven by hourly atmospheric forcing from the high resolution rapid refresh hrrr model a cloud resolving and convection allowing weather forecast and data assimilation system running in real time at a 3 km grid resolution benjamin et al 2016 2 2 lagrangian particle model lpm the lpm described by rowe et al 2016 is used for the lake erie harmful algal bloom forecast system https coastalscience noaa gov research stressor impacts mitigation hab forecasts lake erie accessed august 2nd 2021 which was originally described by huret et al 2007 and churchill et al 2011 the experimental version was sometimes referred to as the chab tracker the same lpm was used to hindcast the probability of microcystin concentration exceeding public health advisory guidelines liu et al 2020 in the lpm chab extent and intensity described as cyanobacterial chlorophyll concentration are represented using lagrangian particles each particle represents a specific chlorophyll mass 1 0 10 μ g p a r t i c l e and chlorophyll concentration in each grid is determined by counting the number of particles in each fvcom tracer control element the lpm calculates concentration using the same horizontal mesh as fvcom but using constant thickness 1 m z layers instead of sigma layers advection of particles is governed by the following equation 1 d x t d t v x t t where x t is the three dimensional particle position at time t v x t t is a three dimensional time varying velocity field linear interpolation in space and time was used to obtain v x t t from hourly fvcom output the contribution of advection to the particle position was updated by integrating equation 1 using an explicit fourth order runge kutta scheme with a time step of 600 s the lpm uses a random walk process milstein scheme gräwe 2011 gräwe et al 2012 rowe et al 2016 to account for vertical turbulent mixing and microcystis buoyant movement eq 2 this is done after the calculation of particles 3 d advective movement with ambient currents eq 1 2 z t δ t z t w b δ t 0 5 d k d z z t δ w 2 δ t δ w 2 k z t where z t is the vertical position of the particle at time t δ t is the vertical random walk time step w b is buoyant velocity k is vertical turbulent diffusivity δ w is a random variable drawn from a gaussian distribution with zero mean and standard deviation δ t the detailed description and evaluation of the milstein scheme are presented in rowe et al 2016 note that an option exists in the lpm code to apply a spatially uniform horizontal random walk diffusion process however we applied the lpm as described by rowe et al 2016 for the chab forecast in which horizontal diffusion was turned off an advantage of the lagrangian approach is that each particle can have individual properties which allows for taking into account varying buoyant velocities among microcystis colonies therefore particles microcystis colonies were assigned with buoyant velocities randomly sampled from a buoyant velocity distribution estimated from a measured microcystis colony size distribution and reported relationship between colony size and buoyant velocity described in rowe et al 2016 2 3 eulerian tracer model etm the etm resolves advective transport and turbulent mixing of tracer concentration c in the following advection diffusion equation 3 c t u c x v c y w c z z k h c z x a m c x y a m c y w b c z c s o u r c e c s i n k where u v and w are the x y and z components of the water velocity k h is the vertical eddy diffusivity calculated using the mellor and yamada level 2 5 my 2 5 turbulent closure scheme mellor and yamada 1982 a m is the horizontal diffusion coefficient calculated with the smagorinsky eddy parameterization method smagorinsky 1963 w b is the buoyant velocity of microcystis colonies and c s o u r c e and c s i n k represent the sources and sinks of c associated with biological processes to represent the physical transport only model and quantify the impact of physical processes on the bloom variability the source and sink terms were turned off in this study in contrast to the lpm where each particle can have an individual buoyant velocity randomly sampled from an estimated buoyant velocity distribution of microcystis colonies the eulerian approach represents characteristics of the population mean instead of describing intrapopulation variability hence a representative buoyant velocity of 9 10 5 m s was used in etm which represented 70 of measured buoyant velocities based on the frequency distribution histogram fig s1 this provided the best model performance in the sensitivity analysis of buoyant velocities supplementary material 2 4 property carrying particle model pcpm etm is designed to solve the physical and biological processes in one equation eq 3 this may become inefficient when running ensemble models with the need to test different configurations for biological components this is because in the etm framework any changes regardless of physical or biological components would require a time integration of the entire eq 3 even if the changes in biological terms do not impact physical processes e g water movement and mixing this creates an opportunity to save computing load by using an alternative approach to calculate advection and diffusion the particles in lpm represent chlorophyll mass while the particles in property carrying particle model pcpm describe flow conditions and record associated environmental properties along the particle trajectories due to the water movement namely properties carrying particles these trajectories and recorded physical properties become the linking mechanism between the hydrodynamic and biological processes in the next step it should be pointed out that the particles in lpm only appear in areas where algae are present because the particles represent chlorophyll mass in contrast the particles in pcpm cover the entire model domain to represent the water mass movement and associated environmental variables in the region without algae the recorded value of algae concentration is simply zero therefore it is necessary to continue releasing particles according to the flow rates of the detroit river in the pcpm to track the water entering the western basin of lake erie from the river on the contrary particles should not be released from the river in lpm as it is assumed the river carried no algae into the lake second the pcpm employs its own grid cell system with a rectilinear grid mesh fig 2b to calculate local average values of the particle based environmental properties within each pcpm grid cell then the cell based state variables within a given pcpm grid cell are predicted in response to various biophysical processes if they are included by using the stored physical conditions and the initial conditions of the state variables which is the information brought by particles from ambient locations in the previous step this step is an eulerian approach to simulating the biophysical process after that the cell based state variables are reassigned to the particles within each pcpm grid cell and these particles carry the updated properties of state variables to the next location note that no calculations are needed to calculate a particle s next step destination as particle trajectories have already been computed at the beginning such a cycle repeats at each time step pcpm differs from lpm as tracer particles in pcpm characterize 3 d fields of concentration encountered by the particles instead of representing chlorophyll mass associated with microcystis colonies as such the pcpm separates the hydrodynamic transport and the remaining biophysical processes hence the pcpm combines the pre computed particle trajectories lagrangian approach with local biophysical in this case vertical mixing and buoyancy processes eulerian approach to predict the spatial distribution and temporal variation of chabs in this pcpm implementation 1 4 million initial particles were randomly distributed throughout the western basin of lake erie within a total water volume of 38 6 km3 the pcpm grid has a resolution of 1 km 1 km with 20 uniform sigma layers in vertical each pcpm cell contains 15 particles on average if no particles are present in a particular cell pcpm uses the values from the previous time step additional particles are continuously introduced according to the flow rates of the detroit river and the maumee river to keep the same particle density as in the initial distribution xue et al 2017 2018 in this study the pcpm first uses pre recorded physical properties carried by particles to determine the pcpm grid cell based average chlorophyll concentration c h l g r i d n in a pcpm grid cell n as follows 4 c h l g r i d n j 1 l c h l p a r t i c l e m j l where the summation includes all l particles m 1 m 2 m l currently within cell n l is the total number of particles within that cell n and c h l p a r t i c l e m j is the concentration of chlorophyll associated with particle m j then the pcpm cell based average chlorophyll concentration is updated through local processes buoyant velocity and vertical mixing in the local water column the updated chlorophyll concentration is then redistributed from cells to particles m 1 m 2 m l to carry forward the development validation and application of the pcpm are documented in xue et al 2017 2018 similar to the etm the pcpm does not resolve intrapopulation variability among microcystis colonies thus the pcpm used the same buoyant velocity as in the etm 2 5 model initialization and experiment design to provide model initial conditions we calculated cyanobacterial chlorophyll concentration based on the satellite derived cyanobacteria index ci one of the european space agency s sentinel 3 olci sensor products https www ncei noaa gov access metadata landing page bin iso id gov noaa nodc nos habofs lakeerie accessed nov 8th 2022 the ci has been used for detecting surface algal blooms and for quantitative mapping of cyanobacterial chlorophyll concentrations in coastal oceans and inland waters binding et al 2019 chlorophyll was converted from the ci by an empirical relationship rowe et al 2016 tomlinson et al 2016 as follows 5 c h l 12570 c i 10 we evaluated daily satellite images of ci for the three chab seasons in 2017 2019 and identified 134 images of chab occurrence to initialize the transport models based on cloud cover conditions table 1 see below for details each 10 day model simulation was initialized from each image by assigning satellite derived chlorophyll concentration to the transport model grids as the initial surface field using the nearest neighbor interpolation the missing data for those model grids under cloud cover were filled by model predicted results on that day from the simulation initialized from the earlier image chlorophyll concentration can be directly used as the initial surface field for the etm and pcpm for the lpm the chlorophyll concentration must be converted to an amount of lagrangian particles by specifying a chlorophyll mass per particle and placing the corresponding number of particles in each model grid to represent the initial chlorophyll concentration the initial vertical distribution of chlorophyll was determined by applying the surface chlorophyll concentration to the surface mixed layer sml depth as described in rowe et al 2016 the satellite images used in the model simulations were divided into four categories no chab event chab event with clear sky chab event with low cloud coverage chab event with high cloud coverage the no chab event category is defined as the case with no more than 5 of the no cloud region having chab in the western basin of lake erie the high and low cloud coverage categories are distinguished by whether more than 50 of the western basin is covered by clouds when there is a chab event finally the clear sky category is those satellite images with no cloud cover that clearly show chab events in the western basin of lake erie model results of three transport models were evaluated against observations for the clear sky category and the low cloud coverage category to characterize the competition between vertical mixing vertical advection and buoyant velocity of microcystis colonies we used the dimensionless péclet number p e which is defined as the ratio of the advective transport rate due to the algal buoyant velocity and water vertical advection to the turbulent transport rate due to vertical eddy diffusivity 6 p e w b w w h k where w b is the buoyant velocity m s h is the water column depth m w w is the vertical advection velocity m s and k is the column mean eddy diffusivity m 2 s the péclet number in this study varied by orders of magnitude 10 3 10 3 pe 1 10 0 indicates that vertical turbulent mixing dominates over buoyant velocity and vertical water advection and vice versa 2 6 skill assessments following the methods of rowe et al 2016 and soontiens et al 2019 we evaluated three transport models using the skill metrics of binary categorical variables and the mean absolute error mae for a statistical assessment of model performance based on all 43 bloom events in three chab seasons the binary categorical variables test whether models can correctly simulate the chab occurrence at each grid cell comparisons at each model grid cell were divided into four categories depending on whether or not a chab event is presented in a given grid a chab event occurs on a grid cell if chlorophyll concentration is greater than 12 u g l which is a level 2 alert defined by the world health organization for short term responses to toxic cyanobacteria in drinking water supplies and a level 1 alert for monitoring and managing cyanobacteria in water bodies used for recreation chorus and welker 2021 four categories were defined a true positive the model correctly predicts an observed chab event b false positive the model predicts an unobserved event c false negative the model fails to predict an observed event d true negative both model and observation show no chab event two metrics including frequency bias fb and pierce skill score pss were used to evaluate the model performance the fb is calculated as 7 f b a b a c it gives the ratio of the number of grid cells over which the model predicts a chab event to the number of grid cells over which a chab event has actually been observed fb 1 indicates an overestimation of the chab area represented by the total number of grids where a chab event occurs from the model and vice versa the pss is defined as 8 p s s a a c b b d pss compares the true positive and false alarm rates pss 0 suggests the model predictions have equivalent rates of true positives and false alarms which is the expectation of a random forecast therefore a positive pss score indicates that the model outperforms a random forecast and a negative pss score indicates that the model performs worse than a random forecast mae measures the absolute difference between modeled chlorophyll concentration and observation 9 m a e 1 n i 1 n m i o i where n is the total number of grid cells m i and o i are the modeled and observed chlorophyll concentration in each grid cell respectively in addition we evaluated the model performance against a persistence forecast which assumes an observed chab pattern remains unchanged over time the persistence forecast represents the best available information to forecast users in the absence of applicable models by comparing the difference in pss and mae between transport models and against persistence forecasts we characterized the quality of the additional information provided by the process based transport models 3 results 3 1 inter day chab transport and variability to examine the impacts of wind driven transport turbulent mixing and buoyancy effect on chab surface variability we presented detailed bloom evolution during two major chab events in this section additional simulation results for low and medium size chab events are also provided in supplementary materials in figs s4 s9 a statistical assessment of model performance based on all 43 bloom events in three chab seasons is presented in section 3 3 the most significant chab event occurred on september 22nd 23rd 2017 on the 15th an intense bloom originated from maumee bay and extended throughout the center of the western basin as seen in the satellite image fig 3a1 in the next eight days the bloom was transported northeastward and reached the northern shore of western lake erie fig 3a2 a3 during this period the surface bloom significantly intensified in the following days the surface chlorophyll concentration decreased rapidly from september 26th through october 1st fig 3a4 a5 correspondingly we conducted a continuous 15 day model simulation from september 15th to october 1st the simulations were initialized from the satellite image on september 15th and then the three models ran continuously until october 1st so that the other satellite images represent independent observations that could be used for model assessment all three transport models successfully captured the northeastward transport pattern of chab as well as the bloom intensification during the first 8 days fig 3a3 d3 all three transport models also successfully reproduced the following diminishing surface bloom with remnants near the northern shore on october 1st fig 3b4 d4 b5 d5 the fact that the models performed well in simulating not only how the bloom moved but also its intensification and reduction reinforces the predominant role of highly variable meteorological and hydrodynamic processes in short term bloom evolution southeasterly wind prevailed during september 16th 23rd fig 4a it generated northwestward currents in the shallow water along the south coast that primarily followed the wind direction fig 4b while in the deeper region of the western basin the ekman flow surface currents turn right with respect to the wind forcing due to the coriolis force and the constraint of the shoreline boundary led to the northeastward transport fig 4b as a result the bloom originated from maumee bay was transported to the northern shore of the western basin during this period the intensification of the bloom on september 23rd was associated with the reduced vertical turbulent mixing as reflected in the increase in the péclet number from september 16th to 23rd the péclet number in the center of western lake erie increased by more than an order of magnitude fig 4b d due to the decreased wind speed from 4 to 6 m s to 2 4 m s fig 4a c as a result the algal buoyancy and water vertical advection gradually dominated over the vertical mixing leading to upward vertical transport of algae from deeper water to the surface that intensified the surface bloom the disappearance of blooms after september 23rd was primarily controlled by the enhanced vertical mixing due to the strong wind fig 4e the péclet number decreased by three orders of magnitude throughout the western basin of lake erie during september 23rd 28th fig 4d f reflecting the dominance of vertical mixing over buoyant velocity and water vertical advection which mixed the algae throughout the water column and resulted in reduced surface chlorophyll concentration fig 5a d the time evolution of bloom intensity was explained by the evolution of the péclet number fig 4g furthermore we separated the impact of algae buoyant velocity and water vertical advection on péclet number at all grid points across the entire model domain the results fig s2 show that 95 of the vertical water velocities are below the buoyant velocity 9 10 5 m s used by the etm and pcpm and more importantly 60 of the vertical water velocities are at least 10 times smaller than the buoyant velocity therefore the péclet number is primarily determined by the competition of the buoyant velocity and vertical turbulent mixing as a result we simplified the péclet number p e s by excluding the impact of water vertical advection 10 p e s w b h k a comparison of p e and p e s shows a very similar pattern fig s3 which confirms that the buoyant velocity has the dominant influence over the vertical advection in péclet number calculation this is due to the fact that upward or downward water velocity as resolved on a km scale horizontal grid has a much smaller magnitude compared to the buoyant velocity of microcystis colonies fig s1 in numerical fluid dynamic models the sub grid scale vertical velocities are represented by the turbulent diffusivity which drives vertical mixing therefore the péclet number primarily represents buoyant velocity competing against turbulent mixing and provides a useful indicator of conditions under which buoyant cyanobacterial colonies are likely to concentrate near the surface versus being mixed through the water column similarly in 2019 a severe chab event occurred from july 29th to august 5th fig 6 on july 29th the bloom with a surface chlorophyll concentration of 40 60 μ g l occurred near the west shore fig 6a1 surface chlorophyll concentration increased to more than 100 μ g l on july 30th the bloom moved eastward fig 6a2 with two branches one branch extending to the center of the western basin and the other one moving along the south coast on august 2nd the central branch of the bloom moved further eastward leaving a chab finger pointing to the northern shore fig 6a3 the finger extended to the north shoreline of the western basin forming a semi circular shaped front when it met with the water mass from the detroit river outflow from august 2nd to 5th fig 6a4 all three transport models captured the northeastward propagation of the bloom and the semi circular shaped front around the detroit river mouth the three transport models also successfully reproduced the increase in surface chlorophyll concentration on july 30th followed by a decrease of surface chlorophyll concentration on august 2nd and a re intensification of chlorophyll concentration on august 5th this again highlights the significant impact of buoyancy and vertical mixing on surface chab intensity the formation of the chab finger and the semi circular shaped front resulted from wind driven surface currents and the outflow from the detroit river a southwesterly wind of 6 8 m s on july 29th at the beginning of the event fig 7a and southeasterly wind of 2 4 m s fig 7c on august 5th at the end of the event favored the northeastward flow carrying high concentration of chlorophyll fig 7b d meanwhile the water mass from the detroit river flowed southward in the northern part of the basin with a counter clockwise turn to the northeast to exit the western basin correspondingly the chlorophyll front was formed between the two water masses the modeled vertical distribution of chlorophyll varied along with the changing péclet number figs 7 and 8 offshore where surface chlorophyll concentration varied significantly the chlorophyll concentration of 50 μ g l was uniformly distributed from the surface to the sml depth of 5 m fig 8a on july 29th the rapidly elevated péclet number on july 30th resulted in the first intensification of surface bloom figs 7e and 6a2 d2 on august 2nd buoyancy driven surface bloom intensification extended to the adjacent area 83 1 w to 83 3 w with horizontal transport and diffusion fig 8b a continued increase in péclet number due to reduced wind and mixing since august 2nd led to a further decrease in sml depth and intensification of the surface bloom with the chlorophyll concentration exceeding 100 μ g l fig 8c and d the three transport models simulated the vertical structure of chlorophyll similarly with some noticeable differences between the lpm and the other two models this is mainly because the lpm used a distribution of buoyant velocity while the etm and pcpm applied a uniform buoyant velocity as discussed in section 2 2 2 4 3 2 diel variability of chab concentration model results also revealed significant intraday variation of surface chlorophyll concentration in a diel cycle fig 9 presents the variation on august 5th 2019 as a typical example at 4 00 a m surface chlorophyll concentration was around 40 60 μ g l in the region fig 9a1 b1 c1 at 1 00 p m the surface chlorophyll concentration increased significantly to 100 μ g l with the chab area nearly unchanged fig 9a2 b2 c2 at 8 00 p m surface chlorophyll concentration decreased again to 30 40 μ g l fig 9a3 b3 c3 such a cycle was clearly shown in the deeper places water depth 4 m where water is more influenced by convective cooling surface chlorophyll concentration was low during the nighttime due to surface cooling induced mixing and increased significantly when water was re stratified mixing weakened during the daytime correspondingly fig 10 shows that the péclet number was greater than one from 4 00 a m to 12 00 p m leading to algae floating upward from the deep layer to the surface and reaching its peak surface concentration at noon the péclet number became less than one afterward consistent with the time when surface chlorophyll concentration started to decrease all three transport models simulated the diel cycle although pcpm and etm showed a higher peak value and reached peak value slightly earlier than the lpm due to their different buoyancy configurations 3 3 model skill statistics the two bloom events we presented in detail in previous sections are aimed at identifying the impact of physical processes on chab evolution in this section we focus on a statistical assessment of model performance based on all 43 bloom events in the three chab seasons 2017 2019 to evaluate the models overall performance we evaluated model performance from 24 to 240 h hindcasts for each chab event in the three consecutive chab seasons and established model skill statistics to identify each model s strengths and limitations the model skill assessments were grouped into two day intervals based on model forecast length table 2 there were 103 model satellite matchups for each simulation with more than 10 for each group all three transport models had positive pss scores indicating that they have greater skill in capturing the occurrence of chab non chab events than a random forecast fig 11 all three models pss scores decreased from 0 75 0 6 as the number of simulation days increased which shows that the models prediction accuracy decreased over longer prediction periods as expected the fb values for all three transport models were less than 1 0 0 7 0 9 indicating that all the models underestimated the chab area among these three models the etm had the best performance based on pss fb metrics in addition fig 12 evaluates the transport models performance against a persistence forecast as persistence forecast assumes a steady chab pattern over time it performed well in 1 2 day prediction capturing both chab non chab events and chlorophyll concentrations this is because the chabs in lake erie have several persistent features which contribute to the skill of the persistence forecast for example chabs often persist in the southern and western nearshore zones due to relatively weak currents and long residence time also the bloom is rarely present in the detroit river plume or beyond 82 7 w as indicated by 13 years of lake erie chab spatial patterns compiled by wynne and stumpf 2015 the three transport models outperformed the persistence forecast in the following days demonstrating the cumulative impacts of transport and mixing on bloom spatiotemporal distribution among the three transport models the etm performs the best in terms of the pss and fb metrics with respect to the mae metric the etm and pcpm show similar performances which are slightly better than the performance of the lpm considering the differences in pss and mae scores among the three transport models are insignificant it suggests that all three transport models have a similar level of skill and the etm performs the best in the overall evaluation note that we also tested model sensitivity to buoyant velocities of microcystis colonies including the second experiment with a high buoyancy velocity of 18 10 5 m s representing microcystis colonies with large diameters and the third experiment excluding buoyant velocities supplementary materials the buoyant velocity of 9 10 5 m s used here in the etm and pcpm represented 70 of measured buoyant velocities based on the frequency distribution histogram of the measured buoyant velocities fig s1 and provided the best model performance in our sensitivity analysis fig s4 4 discussion and conclusion 4 1 importance of physical processes on chab forecast for forecasting the short term evolution of chab events this study reveals the importance of highly dynamic flow patterns and associated transport and mixing in response to weather scale wind events while in the context of long term mean circulation beletsky et al 2013 summarized that the western basin circulation is driven by the detroit river inflow moving eastward out of the basin with wind modulating the circulation to a certain extent however short term responses of flow patterns to weather scale winds have a significant impact on chab forecasts as chabs are persistently located in maumee bay wynne and stumpf 2015 wind driven currents play the most critical role in the bloom movement to locations that are less commonly affected which is the ultimate goal of what the transport models are intended to forecast our results show that depending on wind direction wind events can cause nearshore currents that transport chabs eastward from maumee bay toward the toledo water intake northward toward the monroe water intake and long distance northeastward transport to the canadian shore figs 3 and 6 focusing on how the flow patterns quickly respond to wind events our results provide additional insights in comparison to the previous understanding of the detroit river as the main driver of currents in the western basin this study also demonstrated that the competition between vertical mixing and buoyant velocity with a much less significant impact from vertical advection is a key factor in determining the surface intensity and vertical distribution of chabs when the colony s buoyancy is strong enough to keep microcystis concentrated within the sml the chlorophyll concentration changes with sml depth rowe et al 2016 therefore strong turbulent mixing keeps the algae distributed homogeneously within deeper sml while surface algae can quickly intensify when vertical mixing is reduced figs 5 and 8 such a competition between vertical mixing and buoyant velocity is well expressed by the simplified péclet number defined in this study figs 10 and 11 the péclet number serves as a relatively simple index that can predict the surface intensity and vertical distribution of chab previous studies also support these results wynne et al 2010 show that the chabs increased in both area and intensity for wind stress 0 05 pa and decreased for wind stress 0 1 pa and fang et al 2019 found that the average surface chlorophyll decreased by about 6 2 with a wind speed increase by 1 m s in our study buoyant velocity was held constant as a simplifying assumption so variation in péclet number was largely driven by varying turbulent diffusivity however other studies have shown that microcystis colony buoyant velocity can vary across lake systems den uyl et al 2021 and due to diel variation in cell carbohydrate content medrano et al 2013 thus by accounting for the main drivers of microcystis vertical distribution buoyancy and turbulent diffusivity péclet number regarded as a more general indicator of microcystis vertical distribution across systems than for example the effects of wind speed which would depend on local variables such as the exposure of a lake to wind and the buoyancy of the local microcystis population 4 2 strengths and limitations of the transport models as predicting chab intensity and spatial distribution is critical to lake erie ecosystem management this study presents a comprehensive evaluation of eulerian lagrangian and hybrid transport modeling approaches for lake erie chab hindcasts to ensure an objective assessment these models were run in the most recent 3 d version and evaluated under identical biophysical conditions and with the same statistical skill measures the skill assessments show that three transport models could achieve similar levels of prediction accuracy based on transport only simulations in this study however in the context of general biophysical modeling each of these three models has its own advantages in different situations while a known benefit of lagrangian models is that they may be less susceptible to numerical diffusion than eulerian models recent studies show that when modeling chab with a high level of spatial detail the spatial mismatch between modeled and observed fields can cause a significant penalty in skill and smoothing the spatial fields can improve skill gill et al 2017 thus smoother chab distributions in the eulerian models e g etm and pcpm which resolve spatiotemporally varying horizontal eddy diffusivity may have contributed to greater skill incorporating horizontal diffusion using the random walk method in lpm has the potential to improve the model skill however we elected to implement the lpm in this study as it had previously been applied in the lake erie hab forecast even so spatially varying horizontal diffusivity is more difficult to apply in an lpm due to potential numerical artifacts ross and sharples 2004 and etm can represent more realistic spatially varying horizontal diffusivity smagorinsky 1963 etm can better represent continuous fields of concentration than lpm nearly all lower food web biological models used in the great lakes represent nutrients phytoplankton zooplankton and detritus compartments as continuous fields and describe the biological process in the water column in the eulerian framework therefore the lower food web biological models can be directly coupled to etm due to their compatibility xue et al 2014 rowe et al 2017 on the other hand lpm can represent properties that vary over a population such as buoyant velocity and track exposure to environmental conditions over time which is essential for individual based models of organisms li et al 2014 pcpm to some extent possesses the advantages of etm but with greater computational efficiency pcpm is 15 times faster than etm using the same cpus pcpm is also 30 faster than lpm this can be a critical factor depending on how much computing power is available and how many simulation scenarios an application requires finally although the present model configurations focus on simulating the impacts of physical processes on chab forecast it is critical to include biological processes to resolve the source and sink processes of algae biomass in long term biophysical simulations the fact that etm and pcpm performed as well as or better than the lpm sets up an alternative path to developing more biological realism in future models using eulerian or hybrid approaches declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this is the contribution 96 of the great lakes research center at michigan technological university and noaa glerl contribution number xx this research was funded by the national oceanic and atmospheric administration s national centers for coastal ocean science under award na17nos4780186 liu was funded by the award to the university of north carolina at wilmington through noaa 1305m320pnrma0357 the michigan tech high performance computing cluster superior was used in obtaining the modeling results presented in this publication appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105641 
25436,lake erie has experienced a re emergence of cyanobacterial harmful algal blooms chabs since the early 2000s posing significant socioeconomic and ecological consequences that impact drinking water human health fisheries tourism and water quality as predicting chab intensity and spatial distribution is critical to lake erie ecosystem management this study focuses on a comprehensive evaluation of lagrangian and eulerian transport models for lake erie chab forecasts including 1 a lagrangian particle model lpm 2 an eulerian tracer model etm and 3 a property carrying particle model pcpm that utilizes the hybrid eulerian lagrangian approach we evaluated the models performance against the latest high resolution satellite product from the european space agency s sentinel 3 olci sensor over 24 to 240 h hindcasts for each chab occurrence in three consecutive chab seasons 2017 2019 we examined the relative contributions of horizontal transport vertical turbulent mixing and algal buoyancy on the chab inter and intra day variability in the short term forecast we emphasize the highly dynamic reaction of currents to weather scale wind events that are crucial to chab transport while statistical skill assessments show that these three transport models attain comparable levels of hindcast accuracy we explore the advantages and disadvantages of each model in the context of general biophysical modeling in particular the fact that the etm and pcpm perform as well as or better than the lpm sets up a promising path to developing more biological realism in future operational forecast models using eulerian or hybrid approaches keywords harmful algal bloom hydrodynamics transport model lake erie the great lakes data availability data will be made available on request 1 introduction lake erie is the most productive lake among the five laurentian great lakes of north america the re emergence of cyanobacterial harmful algal blooms chabs since the early 2000s has been known as one of the most severe environmental problems in the great lakes watson et al 2016 liu et al 2020 as a result of eutrophication conditions caused by the excessive nutrient input primarily from agricultural use baker et al 2014 the most severe chabs originate in the shallow western basin of lake erie fig 1 associated with nutrient loading from the maumee river in the southwest of the basin kane et al 2014 chabs in lake erie are dominated by microcystis aeruginosa which produces a family of algal toxins known as microcystins that are dangerous to humans and animals microcystis aeruginosa is a species of freshwater cyanobacteria that can regulate the buoyancy of its colonies with most of the colonies being positively buoyant in lake erie den uyl et al 2021 chabs have high socioeconomic and ecological costs in lake erie impacting drinking water human health fisheries tourism and water quality for example on august 2nd 2014 the city of toledo issued a do not drink notice for their drinking water system that affected 0 5 million people for three days as a result of contamination of treated drinking water with microcystins associated with a severe chab event stumpf et al 2016 steffen et al 2017 the severe impact caused by chabs promoted the development of an improved short term forecast system for chab distribution and transport rowe et al 2016 as physical processes play a dominant role in explaining short term bloom variability rowe et al 2016 short term chab forecasts often employ a mass conserving advection diffusion model such a model is typically forced by a hydrodynamic forecast with chab spatial distribution initialized from satellite remote sensing to predict the bloom location and intensity several days into the future the partial differential equations governing algae transport can be solved using a lagrangian approach an eulerian approach or a combination of the two i e a hybrid eulerian lagrangian approach in lake erie both the lagrangian and eulerian approaches have been tested for chab hindcast simulations wynne et al 2013 rowe et al 2016 soontiens et al 2019 the lagrangian particle model lpm describes motion by following individual particle trajectories as the particles move through space and time each particle represents a certain amount of planktonic or chlorophyll mass and is transported by three dimensional velocity components and turbulent diffusivity output from a hydrodynamic model the chab condition is therefore expressed as an accumulative behavior of phytoplanktonic particles as each individual particle can contain a set of attributes e g colony size buoyant velocity with corresponding physiological traits and behavioral traits this approach allows for considering the colony variability of chabs henrichs et al 2015 the lpm is limited in its ability to represent continuous concentration fields because of the need to count particles within a control volume to represent a concentration unlike the lpm in which mass is transported as discrete particles the eulerian tracer model etm expresses planktonic biomass or chlorophyll as tracer state variables and calculates their concentrations in each model grid cell the eulerian specification of a field calculates the local rate of change at fixed locations grid cells determined by advection and turbulent diffusion as well as its biophysical source and sink terms while a few previous studies have assessed short term hindcast simulations of lake erie chabs using lagrangian or eulerian approaches drawing an insightful conclusion on model performances across these approaches remains challenging this is due to the fact that these models were evaluated using different skill measurements different biophysical settings different forecast periods and or different model versions for example wynne et al 2013 focused on a two dimensional 2d lpm that was initialized with satellite remote sensing data and driven by a hydrodynamic model output to hindcast the chabs for 2008 2010 rowe et al 2016 took a similar approach but applied a 3d lpm for each chab event in the 2011 chab season for up to 240 h hindcasts by taking into account the vertical distribution and buoyancy of microcystis colonies the 3d version of the lpm delivered more skillful hindcasts than its 2d version more recently a 3d etm was compared to a 2d lpm rather than a 3d lpm that limits particles to the lake surface by soontiens et al 2019 the comparison revealed that the 3d etm outperformed the 2d lpm by a wide margin however a 3d etm and 3d lpm were not directly compared in contrast to the 120 h and 240 h hindcasts from wynne et al 2013 and rowe et al 2016 soontiens et al 2019 assessed shorter 48 h hindcasts from july 27th to october 8th 2017 therefore this study aims for a comprehensive model skill assessment directly comparing 3d versions of eulerian and lagrangian chab forecast approaches including assessing a hybrid eulerian lagrangian approach see section 2 4 for the model description these models were run in the latest 3d version at the time of the writing and were assessed under identical biophysical conditions and statistical skill metrics for each chab occurrence in three consecutive chab seasons 2017 2019 we evaluated model performance against the latest high resolution satellite product from the european space agency s sentinel 3 olci sensor over 24 to 240 h hindcasts then we established a statistical skill assessment based on all 43 chab events to identify each model s strengths and limitations 2 methods 2 1 hydrodynamic model the finite volume community ocean model fvcom is a three dimensional free surface primitive equation hydrodynamic model that solves the integral form of the governing equations on an unstructured sigma coordinate mesh the advantage of an unstructured grid mesh for shoreline fitting and local mesh refinement makes the model particularly popular in applications to coastal waters xue et al 2014 2020 2022 ibrahim et al 2020 huang et al 2021a b fvcom has been applied in many coastal systems characterized by geometric complexities and highly variable flow patterns including various applications to lake erie kelley et al 2018 rowe et al 2019 ye et al 2020 the lake erie le fvcom has an unstructured grid mesh of 6106 nodes and 11509 elements with the water column divided into 20 uniform thickness terrain following sigma layers fig 2 a the mesh has a grid resolution of 2 5 km in the central basin 1 5 km in the western basin and 0 5 km in maumee bay and the islands the open boundary conditions include primary inflow from the detroit river and outflow through the niagara river with specified hourly water levels using the noaa nos gauges at gibraltar 9044020 and buffalo 9063020 the le fvcom is driven by hourly atmospheric forcing from the high resolution rapid refresh hrrr model a cloud resolving and convection allowing weather forecast and data assimilation system running in real time at a 3 km grid resolution benjamin et al 2016 2 2 lagrangian particle model lpm the lpm described by rowe et al 2016 is used for the lake erie harmful algal bloom forecast system https coastalscience noaa gov research stressor impacts mitigation hab forecasts lake erie accessed august 2nd 2021 which was originally described by huret et al 2007 and churchill et al 2011 the experimental version was sometimes referred to as the chab tracker the same lpm was used to hindcast the probability of microcystin concentration exceeding public health advisory guidelines liu et al 2020 in the lpm chab extent and intensity described as cyanobacterial chlorophyll concentration are represented using lagrangian particles each particle represents a specific chlorophyll mass 1 0 10 μ g p a r t i c l e and chlorophyll concentration in each grid is determined by counting the number of particles in each fvcom tracer control element the lpm calculates concentration using the same horizontal mesh as fvcom but using constant thickness 1 m z layers instead of sigma layers advection of particles is governed by the following equation 1 d x t d t v x t t where x t is the three dimensional particle position at time t v x t t is a three dimensional time varying velocity field linear interpolation in space and time was used to obtain v x t t from hourly fvcom output the contribution of advection to the particle position was updated by integrating equation 1 using an explicit fourth order runge kutta scheme with a time step of 600 s the lpm uses a random walk process milstein scheme gräwe 2011 gräwe et al 2012 rowe et al 2016 to account for vertical turbulent mixing and microcystis buoyant movement eq 2 this is done after the calculation of particles 3 d advective movement with ambient currents eq 1 2 z t δ t z t w b δ t 0 5 d k d z z t δ w 2 δ t δ w 2 k z t where z t is the vertical position of the particle at time t δ t is the vertical random walk time step w b is buoyant velocity k is vertical turbulent diffusivity δ w is a random variable drawn from a gaussian distribution with zero mean and standard deviation δ t the detailed description and evaluation of the milstein scheme are presented in rowe et al 2016 note that an option exists in the lpm code to apply a spatially uniform horizontal random walk diffusion process however we applied the lpm as described by rowe et al 2016 for the chab forecast in which horizontal diffusion was turned off an advantage of the lagrangian approach is that each particle can have individual properties which allows for taking into account varying buoyant velocities among microcystis colonies therefore particles microcystis colonies were assigned with buoyant velocities randomly sampled from a buoyant velocity distribution estimated from a measured microcystis colony size distribution and reported relationship between colony size and buoyant velocity described in rowe et al 2016 2 3 eulerian tracer model etm the etm resolves advective transport and turbulent mixing of tracer concentration c in the following advection diffusion equation 3 c t u c x v c y w c z z k h c z x a m c x y a m c y w b c z c s o u r c e c s i n k where u v and w are the x y and z components of the water velocity k h is the vertical eddy diffusivity calculated using the mellor and yamada level 2 5 my 2 5 turbulent closure scheme mellor and yamada 1982 a m is the horizontal diffusion coefficient calculated with the smagorinsky eddy parameterization method smagorinsky 1963 w b is the buoyant velocity of microcystis colonies and c s o u r c e and c s i n k represent the sources and sinks of c associated with biological processes to represent the physical transport only model and quantify the impact of physical processes on the bloom variability the source and sink terms were turned off in this study in contrast to the lpm where each particle can have an individual buoyant velocity randomly sampled from an estimated buoyant velocity distribution of microcystis colonies the eulerian approach represents characteristics of the population mean instead of describing intrapopulation variability hence a representative buoyant velocity of 9 10 5 m s was used in etm which represented 70 of measured buoyant velocities based on the frequency distribution histogram fig s1 this provided the best model performance in the sensitivity analysis of buoyant velocities supplementary material 2 4 property carrying particle model pcpm etm is designed to solve the physical and biological processes in one equation eq 3 this may become inefficient when running ensemble models with the need to test different configurations for biological components this is because in the etm framework any changes regardless of physical or biological components would require a time integration of the entire eq 3 even if the changes in biological terms do not impact physical processes e g water movement and mixing this creates an opportunity to save computing load by using an alternative approach to calculate advection and diffusion the particles in lpm represent chlorophyll mass while the particles in property carrying particle model pcpm describe flow conditions and record associated environmental properties along the particle trajectories due to the water movement namely properties carrying particles these trajectories and recorded physical properties become the linking mechanism between the hydrodynamic and biological processes in the next step it should be pointed out that the particles in lpm only appear in areas where algae are present because the particles represent chlorophyll mass in contrast the particles in pcpm cover the entire model domain to represent the water mass movement and associated environmental variables in the region without algae the recorded value of algae concentration is simply zero therefore it is necessary to continue releasing particles according to the flow rates of the detroit river in the pcpm to track the water entering the western basin of lake erie from the river on the contrary particles should not be released from the river in lpm as it is assumed the river carried no algae into the lake second the pcpm employs its own grid cell system with a rectilinear grid mesh fig 2b to calculate local average values of the particle based environmental properties within each pcpm grid cell then the cell based state variables within a given pcpm grid cell are predicted in response to various biophysical processes if they are included by using the stored physical conditions and the initial conditions of the state variables which is the information brought by particles from ambient locations in the previous step this step is an eulerian approach to simulating the biophysical process after that the cell based state variables are reassigned to the particles within each pcpm grid cell and these particles carry the updated properties of state variables to the next location note that no calculations are needed to calculate a particle s next step destination as particle trajectories have already been computed at the beginning such a cycle repeats at each time step pcpm differs from lpm as tracer particles in pcpm characterize 3 d fields of concentration encountered by the particles instead of representing chlorophyll mass associated with microcystis colonies as such the pcpm separates the hydrodynamic transport and the remaining biophysical processes hence the pcpm combines the pre computed particle trajectories lagrangian approach with local biophysical in this case vertical mixing and buoyancy processes eulerian approach to predict the spatial distribution and temporal variation of chabs in this pcpm implementation 1 4 million initial particles were randomly distributed throughout the western basin of lake erie within a total water volume of 38 6 km3 the pcpm grid has a resolution of 1 km 1 km with 20 uniform sigma layers in vertical each pcpm cell contains 15 particles on average if no particles are present in a particular cell pcpm uses the values from the previous time step additional particles are continuously introduced according to the flow rates of the detroit river and the maumee river to keep the same particle density as in the initial distribution xue et al 2017 2018 in this study the pcpm first uses pre recorded physical properties carried by particles to determine the pcpm grid cell based average chlorophyll concentration c h l g r i d n in a pcpm grid cell n as follows 4 c h l g r i d n j 1 l c h l p a r t i c l e m j l where the summation includes all l particles m 1 m 2 m l currently within cell n l is the total number of particles within that cell n and c h l p a r t i c l e m j is the concentration of chlorophyll associated with particle m j then the pcpm cell based average chlorophyll concentration is updated through local processes buoyant velocity and vertical mixing in the local water column the updated chlorophyll concentration is then redistributed from cells to particles m 1 m 2 m l to carry forward the development validation and application of the pcpm are documented in xue et al 2017 2018 similar to the etm the pcpm does not resolve intrapopulation variability among microcystis colonies thus the pcpm used the same buoyant velocity as in the etm 2 5 model initialization and experiment design to provide model initial conditions we calculated cyanobacterial chlorophyll concentration based on the satellite derived cyanobacteria index ci one of the european space agency s sentinel 3 olci sensor products https www ncei noaa gov access metadata landing page bin iso id gov noaa nodc nos habofs lakeerie accessed nov 8th 2022 the ci has been used for detecting surface algal blooms and for quantitative mapping of cyanobacterial chlorophyll concentrations in coastal oceans and inland waters binding et al 2019 chlorophyll was converted from the ci by an empirical relationship rowe et al 2016 tomlinson et al 2016 as follows 5 c h l 12570 c i 10 we evaluated daily satellite images of ci for the three chab seasons in 2017 2019 and identified 134 images of chab occurrence to initialize the transport models based on cloud cover conditions table 1 see below for details each 10 day model simulation was initialized from each image by assigning satellite derived chlorophyll concentration to the transport model grids as the initial surface field using the nearest neighbor interpolation the missing data for those model grids under cloud cover were filled by model predicted results on that day from the simulation initialized from the earlier image chlorophyll concentration can be directly used as the initial surface field for the etm and pcpm for the lpm the chlorophyll concentration must be converted to an amount of lagrangian particles by specifying a chlorophyll mass per particle and placing the corresponding number of particles in each model grid to represent the initial chlorophyll concentration the initial vertical distribution of chlorophyll was determined by applying the surface chlorophyll concentration to the surface mixed layer sml depth as described in rowe et al 2016 the satellite images used in the model simulations were divided into four categories no chab event chab event with clear sky chab event with low cloud coverage chab event with high cloud coverage the no chab event category is defined as the case with no more than 5 of the no cloud region having chab in the western basin of lake erie the high and low cloud coverage categories are distinguished by whether more than 50 of the western basin is covered by clouds when there is a chab event finally the clear sky category is those satellite images with no cloud cover that clearly show chab events in the western basin of lake erie model results of three transport models were evaluated against observations for the clear sky category and the low cloud coverage category to characterize the competition between vertical mixing vertical advection and buoyant velocity of microcystis colonies we used the dimensionless péclet number p e which is defined as the ratio of the advective transport rate due to the algal buoyant velocity and water vertical advection to the turbulent transport rate due to vertical eddy diffusivity 6 p e w b w w h k where w b is the buoyant velocity m s h is the water column depth m w w is the vertical advection velocity m s and k is the column mean eddy diffusivity m 2 s the péclet number in this study varied by orders of magnitude 10 3 10 3 pe 1 10 0 indicates that vertical turbulent mixing dominates over buoyant velocity and vertical water advection and vice versa 2 6 skill assessments following the methods of rowe et al 2016 and soontiens et al 2019 we evaluated three transport models using the skill metrics of binary categorical variables and the mean absolute error mae for a statistical assessment of model performance based on all 43 bloom events in three chab seasons the binary categorical variables test whether models can correctly simulate the chab occurrence at each grid cell comparisons at each model grid cell were divided into four categories depending on whether or not a chab event is presented in a given grid a chab event occurs on a grid cell if chlorophyll concentration is greater than 12 u g l which is a level 2 alert defined by the world health organization for short term responses to toxic cyanobacteria in drinking water supplies and a level 1 alert for monitoring and managing cyanobacteria in water bodies used for recreation chorus and welker 2021 four categories were defined a true positive the model correctly predicts an observed chab event b false positive the model predicts an unobserved event c false negative the model fails to predict an observed event d true negative both model and observation show no chab event two metrics including frequency bias fb and pierce skill score pss were used to evaluate the model performance the fb is calculated as 7 f b a b a c it gives the ratio of the number of grid cells over which the model predicts a chab event to the number of grid cells over which a chab event has actually been observed fb 1 indicates an overestimation of the chab area represented by the total number of grids where a chab event occurs from the model and vice versa the pss is defined as 8 p s s a a c b b d pss compares the true positive and false alarm rates pss 0 suggests the model predictions have equivalent rates of true positives and false alarms which is the expectation of a random forecast therefore a positive pss score indicates that the model outperforms a random forecast and a negative pss score indicates that the model performs worse than a random forecast mae measures the absolute difference between modeled chlorophyll concentration and observation 9 m a e 1 n i 1 n m i o i where n is the total number of grid cells m i and o i are the modeled and observed chlorophyll concentration in each grid cell respectively in addition we evaluated the model performance against a persistence forecast which assumes an observed chab pattern remains unchanged over time the persistence forecast represents the best available information to forecast users in the absence of applicable models by comparing the difference in pss and mae between transport models and against persistence forecasts we characterized the quality of the additional information provided by the process based transport models 3 results 3 1 inter day chab transport and variability to examine the impacts of wind driven transport turbulent mixing and buoyancy effect on chab surface variability we presented detailed bloom evolution during two major chab events in this section additional simulation results for low and medium size chab events are also provided in supplementary materials in figs s4 s9 a statistical assessment of model performance based on all 43 bloom events in three chab seasons is presented in section 3 3 the most significant chab event occurred on september 22nd 23rd 2017 on the 15th an intense bloom originated from maumee bay and extended throughout the center of the western basin as seen in the satellite image fig 3a1 in the next eight days the bloom was transported northeastward and reached the northern shore of western lake erie fig 3a2 a3 during this period the surface bloom significantly intensified in the following days the surface chlorophyll concentration decreased rapidly from september 26th through october 1st fig 3a4 a5 correspondingly we conducted a continuous 15 day model simulation from september 15th to october 1st the simulations were initialized from the satellite image on september 15th and then the three models ran continuously until october 1st so that the other satellite images represent independent observations that could be used for model assessment all three transport models successfully captured the northeastward transport pattern of chab as well as the bloom intensification during the first 8 days fig 3a3 d3 all three transport models also successfully reproduced the following diminishing surface bloom with remnants near the northern shore on october 1st fig 3b4 d4 b5 d5 the fact that the models performed well in simulating not only how the bloom moved but also its intensification and reduction reinforces the predominant role of highly variable meteorological and hydrodynamic processes in short term bloom evolution southeasterly wind prevailed during september 16th 23rd fig 4a it generated northwestward currents in the shallow water along the south coast that primarily followed the wind direction fig 4b while in the deeper region of the western basin the ekman flow surface currents turn right with respect to the wind forcing due to the coriolis force and the constraint of the shoreline boundary led to the northeastward transport fig 4b as a result the bloom originated from maumee bay was transported to the northern shore of the western basin during this period the intensification of the bloom on september 23rd was associated with the reduced vertical turbulent mixing as reflected in the increase in the péclet number from september 16th to 23rd the péclet number in the center of western lake erie increased by more than an order of magnitude fig 4b d due to the decreased wind speed from 4 to 6 m s to 2 4 m s fig 4a c as a result the algal buoyancy and water vertical advection gradually dominated over the vertical mixing leading to upward vertical transport of algae from deeper water to the surface that intensified the surface bloom the disappearance of blooms after september 23rd was primarily controlled by the enhanced vertical mixing due to the strong wind fig 4e the péclet number decreased by three orders of magnitude throughout the western basin of lake erie during september 23rd 28th fig 4d f reflecting the dominance of vertical mixing over buoyant velocity and water vertical advection which mixed the algae throughout the water column and resulted in reduced surface chlorophyll concentration fig 5a d the time evolution of bloom intensity was explained by the evolution of the péclet number fig 4g furthermore we separated the impact of algae buoyant velocity and water vertical advection on péclet number at all grid points across the entire model domain the results fig s2 show that 95 of the vertical water velocities are below the buoyant velocity 9 10 5 m s used by the etm and pcpm and more importantly 60 of the vertical water velocities are at least 10 times smaller than the buoyant velocity therefore the péclet number is primarily determined by the competition of the buoyant velocity and vertical turbulent mixing as a result we simplified the péclet number p e s by excluding the impact of water vertical advection 10 p e s w b h k a comparison of p e and p e s shows a very similar pattern fig s3 which confirms that the buoyant velocity has the dominant influence over the vertical advection in péclet number calculation this is due to the fact that upward or downward water velocity as resolved on a km scale horizontal grid has a much smaller magnitude compared to the buoyant velocity of microcystis colonies fig s1 in numerical fluid dynamic models the sub grid scale vertical velocities are represented by the turbulent diffusivity which drives vertical mixing therefore the péclet number primarily represents buoyant velocity competing against turbulent mixing and provides a useful indicator of conditions under which buoyant cyanobacterial colonies are likely to concentrate near the surface versus being mixed through the water column similarly in 2019 a severe chab event occurred from july 29th to august 5th fig 6 on july 29th the bloom with a surface chlorophyll concentration of 40 60 μ g l occurred near the west shore fig 6a1 surface chlorophyll concentration increased to more than 100 μ g l on july 30th the bloom moved eastward fig 6a2 with two branches one branch extending to the center of the western basin and the other one moving along the south coast on august 2nd the central branch of the bloom moved further eastward leaving a chab finger pointing to the northern shore fig 6a3 the finger extended to the north shoreline of the western basin forming a semi circular shaped front when it met with the water mass from the detroit river outflow from august 2nd to 5th fig 6a4 all three transport models captured the northeastward propagation of the bloom and the semi circular shaped front around the detroit river mouth the three transport models also successfully reproduced the increase in surface chlorophyll concentration on july 30th followed by a decrease of surface chlorophyll concentration on august 2nd and a re intensification of chlorophyll concentration on august 5th this again highlights the significant impact of buoyancy and vertical mixing on surface chab intensity the formation of the chab finger and the semi circular shaped front resulted from wind driven surface currents and the outflow from the detroit river a southwesterly wind of 6 8 m s on july 29th at the beginning of the event fig 7a and southeasterly wind of 2 4 m s fig 7c on august 5th at the end of the event favored the northeastward flow carrying high concentration of chlorophyll fig 7b d meanwhile the water mass from the detroit river flowed southward in the northern part of the basin with a counter clockwise turn to the northeast to exit the western basin correspondingly the chlorophyll front was formed between the two water masses the modeled vertical distribution of chlorophyll varied along with the changing péclet number figs 7 and 8 offshore where surface chlorophyll concentration varied significantly the chlorophyll concentration of 50 μ g l was uniformly distributed from the surface to the sml depth of 5 m fig 8a on july 29th the rapidly elevated péclet number on july 30th resulted in the first intensification of surface bloom figs 7e and 6a2 d2 on august 2nd buoyancy driven surface bloom intensification extended to the adjacent area 83 1 w to 83 3 w with horizontal transport and diffusion fig 8b a continued increase in péclet number due to reduced wind and mixing since august 2nd led to a further decrease in sml depth and intensification of the surface bloom with the chlorophyll concentration exceeding 100 μ g l fig 8c and d the three transport models simulated the vertical structure of chlorophyll similarly with some noticeable differences between the lpm and the other two models this is mainly because the lpm used a distribution of buoyant velocity while the etm and pcpm applied a uniform buoyant velocity as discussed in section 2 2 2 4 3 2 diel variability of chab concentration model results also revealed significant intraday variation of surface chlorophyll concentration in a diel cycle fig 9 presents the variation on august 5th 2019 as a typical example at 4 00 a m surface chlorophyll concentration was around 40 60 μ g l in the region fig 9a1 b1 c1 at 1 00 p m the surface chlorophyll concentration increased significantly to 100 μ g l with the chab area nearly unchanged fig 9a2 b2 c2 at 8 00 p m surface chlorophyll concentration decreased again to 30 40 μ g l fig 9a3 b3 c3 such a cycle was clearly shown in the deeper places water depth 4 m where water is more influenced by convective cooling surface chlorophyll concentration was low during the nighttime due to surface cooling induced mixing and increased significantly when water was re stratified mixing weakened during the daytime correspondingly fig 10 shows that the péclet number was greater than one from 4 00 a m to 12 00 p m leading to algae floating upward from the deep layer to the surface and reaching its peak surface concentration at noon the péclet number became less than one afterward consistent with the time when surface chlorophyll concentration started to decrease all three transport models simulated the diel cycle although pcpm and etm showed a higher peak value and reached peak value slightly earlier than the lpm due to their different buoyancy configurations 3 3 model skill statistics the two bloom events we presented in detail in previous sections are aimed at identifying the impact of physical processes on chab evolution in this section we focus on a statistical assessment of model performance based on all 43 bloom events in the three chab seasons 2017 2019 to evaluate the models overall performance we evaluated model performance from 24 to 240 h hindcasts for each chab event in the three consecutive chab seasons and established model skill statistics to identify each model s strengths and limitations the model skill assessments were grouped into two day intervals based on model forecast length table 2 there were 103 model satellite matchups for each simulation with more than 10 for each group all three transport models had positive pss scores indicating that they have greater skill in capturing the occurrence of chab non chab events than a random forecast fig 11 all three models pss scores decreased from 0 75 0 6 as the number of simulation days increased which shows that the models prediction accuracy decreased over longer prediction periods as expected the fb values for all three transport models were less than 1 0 0 7 0 9 indicating that all the models underestimated the chab area among these three models the etm had the best performance based on pss fb metrics in addition fig 12 evaluates the transport models performance against a persistence forecast as persistence forecast assumes a steady chab pattern over time it performed well in 1 2 day prediction capturing both chab non chab events and chlorophyll concentrations this is because the chabs in lake erie have several persistent features which contribute to the skill of the persistence forecast for example chabs often persist in the southern and western nearshore zones due to relatively weak currents and long residence time also the bloom is rarely present in the detroit river plume or beyond 82 7 w as indicated by 13 years of lake erie chab spatial patterns compiled by wynne and stumpf 2015 the three transport models outperformed the persistence forecast in the following days demonstrating the cumulative impacts of transport and mixing on bloom spatiotemporal distribution among the three transport models the etm performs the best in terms of the pss and fb metrics with respect to the mae metric the etm and pcpm show similar performances which are slightly better than the performance of the lpm considering the differences in pss and mae scores among the three transport models are insignificant it suggests that all three transport models have a similar level of skill and the etm performs the best in the overall evaluation note that we also tested model sensitivity to buoyant velocities of microcystis colonies including the second experiment with a high buoyancy velocity of 18 10 5 m s representing microcystis colonies with large diameters and the third experiment excluding buoyant velocities supplementary materials the buoyant velocity of 9 10 5 m s used here in the etm and pcpm represented 70 of measured buoyant velocities based on the frequency distribution histogram of the measured buoyant velocities fig s1 and provided the best model performance in our sensitivity analysis fig s4 4 discussion and conclusion 4 1 importance of physical processes on chab forecast for forecasting the short term evolution of chab events this study reveals the importance of highly dynamic flow patterns and associated transport and mixing in response to weather scale wind events while in the context of long term mean circulation beletsky et al 2013 summarized that the western basin circulation is driven by the detroit river inflow moving eastward out of the basin with wind modulating the circulation to a certain extent however short term responses of flow patterns to weather scale winds have a significant impact on chab forecasts as chabs are persistently located in maumee bay wynne and stumpf 2015 wind driven currents play the most critical role in the bloom movement to locations that are less commonly affected which is the ultimate goal of what the transport models are intended to forecast our results show that depending on wind direction wind events can cause nearshore currents that transport chabs eastward from maumee bay toward the toledo water intake northward toward the monroe water intake and long distance northeastward transport to the canadian shore figs 3 and 6 focusing on how the flow patterns quickly respond to wind events our results provide additional insights in comparison to the previous understanding of the detroit river as the main driver of currents in the western basin this study also demonstrated that the competition between vertical mixing and buoyant velocity with a much less significant impact from vertical advection is a key factor in determining the surface intensity and vertical distribution of chabs when the colony s buoyancy is strong enough to keep microcystis concentrated within the sml the chlorophyll concentration changes with sml depth rowe et al 2016 therefore strong turbulent mixing keeps the algae distributed homogeneously within deeper sml while surface algae can quickly intensify when vertical mixing is reduced figs 5 and 8 such a competition between vertical mixing and buoyant velocity is well expressed by the simplified péclet number defined in this study figs 10 and 11 the péclet number serves as a relatively simple index that can predict the surface intensity and vertical distribution of chab previous studies also support these results wynne et al 2010 show that the chabs increased in both area and intensity for wind stress 0 05 pa and decreased for wind stress 0 1 pa and fang et al 2019 found that the average surface chlorophyll decreased by about 6 2 with a wind speed increase by 1 m s in our study buoyant velocity was held constant as a simplifying assumption so variation in péclet number was largely driven by varying turbulent diffusivity however other studies have shown that microcystis colony buoyant velocity can vary across lake systems den uyl et al 2021 and due to diel variation in cell carbohydrate content medrano et al 2013 thus by accounting for the main drivers of microcystis vertical distribution buoyancy and turbulent diffusivity péclet number regarded as a more general indicator of microcystis vertical distribution across systems than for example the effects of wind speed which would depend on local variables such as the exposure of a lake to wind and the buoyancy of the local microcystis population 4 2 strengths and limitations of the transport models as predicting chab intensity and spatial distribution is critical to lake erie ecosystem management this study presents a comprehensive evaluation of eulerian lagrangian and hybrid transport modeling approaches for lake erie chab hindcasts to ensure an objective assessment these models were run in the most recent 3 d version and evaluated under identical biophysical conditions and with the same statistical skill measures the skill assessments show that three transport models could achieve similar levels of prediction accuracy based on transport only simulations in this study however in the context of general biophysical modeling each of these three models has its own advantages in different situations while a known benefit of lagrangian models is that they may be less susceptible to numerical diffusion than eulerian models recent studies show that when modeling chab with a high level of spatial detail the spatial mismatch between modeled and observed fields can cause a significant penalty in skill and smoothing the spatial fields can improve skill gill et al 2017 thus smoother chab distributions in the eulerian models e g etm and pcpm which resolve spatiotemporally varying horizontal eddy diffusivity may have contributed to greater skill incorporating horizontal diffusion using the random walk method in lpm has the potential to improve the model skill however we elected to implement the lpm in this study as it had previously been applied in the lake erie hab forecast even so spatially varying horizontal diffusivity is more difficult to apply in an lpm due to potential numerical artifacts ross and sharples 2004 and etm can represent more realistic spatially varying horizontal diffusivity smagorinsky 1963 etm can better represent continuous fields of concentration than lpm nearly all lower food web biological models used in the great lakes represent nutrients phytoplankton zooplankton and detritus compartments as continuous fields and describe the biological process in the water column in the eulerian framework therefore the lower food web biological models can be directly coupled to etm due to their compatibility xue et al 2014 rowe et al 2017 on the other hand lpm can represent properties that vary over a population such as buoyant velocity and track exposure to environmental conditions over time which is essential for individual based models of organisms li et al 2014 pcpm to some extent possesses the advantages of etm but with greater computational efficiency pcpm is 15 times faster than etm using the same cpus pcpm is also 30 faster than lpm this can be a critical factor depending on how much computing power is available and how many simulation scenarios an application requires finally although the present model configurations focus on simulating the impacts of physical processes on chab forecast it is critical to include biological processes to resolve the source and sink processes of algae biomass in long term biophysical simulations the fact that etm and pcpm performed as well as or better than the lpm sets up an alternative path to developing more biological realism in future models using eulerian or hybrid approaches declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this is the contribution 96 of the great lakes research center at michigan technological university and noaa glerl contribution number xx this research was funded by the national oceanic and atmospheric administration s national centers for coastal ocean science under award na17nos4780186 liu was funded by the award to the university of north carolina at wilmington through noaa 1305m320pnrma0357 the michigan tech high performance computing cluster superior was used in obtaining the modeling results presented in this publication appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105641 
25437,land use regression is a popular method for predicting ambient pollutant concentrations at points of interest where no measurements are taken however the model building process is complicated and systematically understanding when and how the process works is difficult to overcome these limitations we reformulate the existing land use regression method as a sign constrained regression problem with an explicit objective function to be minimized this novel formulation always leads to estimated regression coefficients that satisfy the predefined direction based on subject matter knowledge while simultaneously substantially improving the prediction performance of the existing land use regression method the advantages of the proposed sign constrained regression method are confirmed through a numerical study and real data analysis keywords land use regression sign constraints penalized regression methods prediction interpretability data availability we can share data and code on github or website 1 introduction the two main aspects when building exposure prediction models are accurate predictions and high interpretability if we focus on only the first aspect variables should be selected to maximize the prediction performance regardless of the correctness of the sign of the regression coefficient crouse et al 2009 in contrast for the second aspect the explanation view has been emphasized to determine how factors affect the amount of exposure amini et al 2014 beelen et al 2013 de hoogh et al 2013 eeftens et al 2012 gonzales et al 2012 gulliver and de hoogh 2015 henderson et al 2007 hoek et al 2011 tang et al 2013 land use regression lur based analysis focuses on the second aspect and retains variables only if their coefficients are consistent with prior assumptions regarding the direction of the effect acknowledging the importance of the high interpretability required for environmental sciences we note that the current lur model building process is far from the optimal way for achieving accurate predictions therefore we propose an alternative method that overcomes the limitations of existing lur methods by reformulating the lur method as a sign constrained regression model lur has been used to interpolate ambient pollutant concentrations at points of interest where no measurements are available using measurements from several monitoring sites for example it was used to predict outdoor concentrations at the home addresses of the study participants in more than 30 cohort studies beelen et al 2013 the small area variation in air quality and health saviah study was the first to use lur to model small scale variations in air pollution briggs et al 1997 the aforementioned study aimed to generate individual level indicators of long term average exposure to air pollution to assess the risk of respiratory diseases in children it included variables such as the traffic volume population density land use and altitude which were compiled using geographic information systems gis the lur model was constrained by requiring that all the regression coefficients have the a priori definition of the required signs for regression coefficients e g positive for traffic and negative for altitude beelen et al 2013 developed an lur model in a standardized way described in the european study of cohorts for air pollution effects escape exposure manual using the air pollutant and gis based values for potential predictor variables when predefined signs of variables are utilized in a model the model becomes more easily interpretable and its risk of overfitting is limited beelen et al 2013 hastie et al 2009 james et al 2013 kato et al 2019 wolf et al 2017 in the lur model building process typically a large number of covariates are considered the existing lur model building process is complicated and renders difficulty in determining when and how the procedure works because of the complexity in implementing the algorithm the key reason is that it does not explicitly stipulate the type of objective function that is minimized therefore this lack of transparency regarding the algorithm s objective function should be avoided to better understand the properties of the lur algorithm for high dimensional problems penalized likelihood regression methods such as the least absolute shrinkage and selection operator lasso tibshirani 1996 and ridge hoerl and kennard 1970 methods seem to be appropriate choices because these methods are theoretically guaranteed to improve the prediction performance by regularizing the regression coefficients the lasso and ridge methods shrink the regression coefficients by penalizing the sums of their absolute and squared values respectively lasso performs automatic variable selection and ridge regression is effective at handling a large number of covariates with high multicollinearity elastic net zou and hastie 2005 was developed to take advantage of the lasso and ridge methods by mixing their penalty functions however applying regularized methods such as lasso ridge and elastic net in ordinary lur models is not suitable because the signs of the regression coefficients from penalized regression methods are not guaranteed to match the domain knowledge essentially a conflict between the estimated regression coefficients and their predefined signs can cause conventional penalized likelihood methods to lose their high interpretability therefore we propose a modification of the penalized regression methods to satisfy domain knowledge based sign constraints recently a sign constrained lasso was developed by gaines et al 2018 and is well suited for our research however to overcome the limitation of lasso in coping with the high multicollinearity that often appears in environmental science we develop new sign constrained regression methods based on the ridge and elastic net methods as alternative solutions for the existing lur first we explain why the sign constrained lasso ridge and elastic net regressions were well suited to the purpose of the present study their objective functions are clearly defined and we examine what kinds of optimization problems are solved to provide estimated regression coefficients that always match the domain knowledge because this new lur formulation is designed to maximize the prediction performance under the sign constraint a significant improvement in the prediction performance can be expected compared to the existing lur method which will be numerically confirmed later the remainder of this study is organized as follows section 2 begins with a brief review of lur focusing on variable selection subsequently the proposed methods including lur with the constrained lasso ridge and elastic net methods are explained section 3 presents the results of a simulation study to assess the prediction performance and sign accuracy of the proposed methods and illustrates their application in real data analysis finally section 4 concludes the paper the supplementary material contains an r function for implementing the proposed sign constrained lur method with some examples 2 method beelen et al 2013 developed an algorithm that used predefined effect directions based on fundamental physical principles for lur variable selection in the escape study first we start with the algorithmic details and propose a new formulation 2 1 formulating lur as sign constrained linear regression the detailed steps for variable selection in lur are as follows first we determined the regression coefficients for each covariate against the response variable subsequently the model with a single covariate with the highest adjusted r 2 becomes the start model next the remaining variables were sequentially input to this start model the candidate models exhibited a change in the adjusted r 2 the model retains the input predictor if the following three conditions for the candidate models are satisfied 1 the increase in adjusted r 2 is greater than 1 2 the coefficient conforms to the pre specified direction and 3 the direction of effect for the predictors already included in the model does not change the fitting process repeats the addition of variables until there are no predictor variables that add more than 1 to the adjusted r 2 of the previous regression model note that when the variable selection criterion uses the adjusted r 2 some variables may be statistically non significant therefore the fitted model removes variables with p values 0 1 hence the final model includes predictor variables with p values 0 1 although its rationale is understandable the detailed implementation steps of the lur are complex moreover understanding the ultimate goals of the procedure is challenging considering the sign constraints we pursue a more systematic and transparent approach to lur variable selection to do this we begin with the following linear regression 1 y i β 0 β 1 x i 1 β 2 x i 2 β 3 x i 3 β p x i p ϵ i ϵ i n 0 σ 2 i 1 n where x i 1 x i 2 x i 3 x i p are p predictor or independent variables y i is the dependent variable and β i denotes the regression coefficient associated with the i th predictor variable the optimization problem of the linear regression model can be written as follows 2 minimize β 0 β 1 β p i 1 n y i β 0 j 1 p β j x i j 2 the ordinary least squares ols method is often used to estimate β 0 β p by minimizing the sum of the squares in 2 because the sign of the estimated β i may not satisfy the corresponding domain knowledge sign constraints need to be imposed on 2 suppose that c k is 1 if the k th predictor variable x k is known to have a positive effect on dependent variable y c k is 1 if it has a negative effect and 0 otherwise in particular c k 0 corresponds to that there is no strong domain knowledge for the sign of β k we define the set of regression coefficients matching the predefined signs using the domain knowledge as c β 1 β 2 β p c k β k 0 for some k which is called a feasible set note that the feasible set does not require the sign constraints for all the explanatory variables the sign constrained ols method corresponds to determining the coefficients that solve 2 in the feasible set based on this the optimization problem of the proposed sign constrained lur model has the following form 3 minimize β 0 β 1 β p i 1 n y i β 0 j 1 p β j x i j 2 subject to β 1 β 2 β p c to explain the proposed objective function concretely we consider four specific covariates to predict pm 2 5 we denote the area of industrial facilities as x 1 the area of agriculture as x 2 the area of forest meadows as x 3 and the population as x 4 regarding pm 2 5 x 1 and x 4 are known to have positive effects whereas x 2 and x 3 have negative effects thus the sign constrained regression model is expressed as follows y i β 0 β 1 x i 1 β 2 x i 2 β 3 x i 3 β 4 x i 4 ϵ i ϵ i n 0 σ 2 i 1 n with the feasible set c β 1 β 2 β 3 β 4 c k β k 0 c 1 0 c 2 0 c 3 0 c 4 0 k 1 2 3 4 for example considering β 2 since c 2 0 is fixed first the estimate of β 2 should be negative or zero to satisfy the condition of c 2 β 2 0 the scope of the sign constraints can be straightforwardly extended to linear combinations of regression coefficients β a general feasible set can be expressed as follows c β 1 β 2 β p c 11 β 11 c 12 β 12 c 1 p β 1 p d 1 c 21 β 21 c 22 β 22 c 2 p β 2 p d 2 c m 1 β m 1 c m 2 β m 2 c m p β m p d m m p the proposed formulation had several advantages first the model can achieve high interpretability because the estimated regression coefficients always satisfy the predefined direction of the regression coefficients second the proposed optimization formulation determines regression coefficients to maximize the prediction performance under sign constraints therefore the proposed constrained optimization formulation is expected to provide an improved prediction performance compared with the original lur method 2 2 sign constrained lur with a large number of covariates in environmental epidemiology lur models typically deal with a large number of predictor variables p compared with the sample size n this situation is a high dimensional regression problem in the statistics literature as is well known both the prediction performance and interpretability can deteriorate when unnecessary covariates are included or when high correlations between covariates are not appropriately handled hastie et al 1995 principal component analysis pca abdi and williams 2010 is a popular dimension reduction method for handling the issue several regularization methods have been developed to prevent such deterioration out of these we consider three popular methods lasso tibshirani 1996 ridge hoerl and kennard 1970 and elastic net zou and hastie 2005 the following section explains how sign constraints can be incorporated into these three regularization methods 2 2 1 lur with constrained lasso model lasso employs the sum of the absolute values of the regression coefficients as a penalty function which is called l 1 regularization this l 1 regularization can lead to some coefficients being exactly zero i e estimations and variable selections are simultaneously performed recently james et al 2013 and gaines et al 2018 developed an algorithm that incorporates sign constraints for lasso we can use the sign constrained lasso algorithm to determine the regression coefficients in the sign constrained lur model the optimization problem for sign constrained lasso is written as follows 4 minimize β 0 r β c i 1 n y i β 0 j 1 p β j x i j 2 λ j 1 p β j where the feasible set is c β β 1 β 2 β p c k β k 0 for some k the tuning parameter λ 0 controls the amount of regularization unlike the objective function of lasso the condition in 4 requires that the estimates of β 1 β p belong to the feasible set c which automatically satisfies the predefined sign constraints for example the solution of λ 0 is the same as that of ols whereas a large λ can result in numerous coefficients having values of exactly zero in particular unlike ols objective function in 4 can be well defined even when p n under regularity conditions in the sense that it allows a unique solution tibshirani 2013 for completeness the details regarding minimizing the objective function in 4 for the high dimensional case are provided in the supplementary material 2 2 2 lur with constrained ridge and elastic net models lasso exhibits good performance under the sparsity condition i e most of the true regression coefficients are zero however the sparsity condition may not hold for environmental epidemiological problems for example various ecological covariates are associated with pm 10 concentrations although the strength of the association may be small dockery et al 1992 stafoggia et al 2016 zeka et al 2005 therefore lasso may not be always the best choice for lur models we need to consider ridge or elastic net models because they are known to handle such non sparse situations better than lasso in this section first we consider the objective functions of the sign constrained ridge and elastic net methods ridge regression employs the sum of the squared values of the regression coefficients as a penalty function which is called l 2 regularization ridge regression shrinks the regression coefficients toward zero this shrinkage does not result in variable selection but often leads to a better prediction performance than that of ols because the predicted value has smaller variance hastie et al 2009 james et al 2013 similar to the manner in which objective function 4 was constructed the optimization problem for the sign constrained ridge regression is written as follows 5 minimize β 0 r β c i 1 n y i β 0 j 1 p β j x i j 2 λ j 1 p β j 2 subject to feasible set c β 1 β 2 β p c k β k 0 for some k objective function in 5 is well defined even when p n due to the penalty function when λ 0 this objective function is the same as that of the ols whereas a positive λ causes the regression coefficients to shrink toward zero unlike ridge regression without sign constraints the sign constrained ridge regression can perform a type of variable selection when the sign of the estimated regression coefficient is the opposite to that of the predefined value the sign constraint should be satisfied however if the objective function increases continuously because of the change in sign the estimated regression coefficient can become zero consequently the sign constrained ridge regression automatically enhances the interpretability of the regression results by performing variable selection in other words by eliminating variables that present results opposite to their corresponding predefined signs an elastic net model was proposed to circumvent the instability of the lasso solution when predictors are highly correlated while maintaining variable selection such as lasso it employs a linear combination of l 1 and l 2 penalty functions as its penalty function the optimization problem for the sign constrained elastic net regression is given by the following 6 minimize β 0 r β c i 1 n y i β 0 j 1 p β j x i j 2 λ 1 j 1 p β j λ 2 j 1 p β j 2 where λ 1 λ 1 1 λ 2 n and λ 2 λ 2 1 λ 2 n when λ 1 0 the objective function in 6 is reduced to that of ridge regression and when λ 2 0 the objective function is reduced to that of lasso unlike lasso the elastic net method can select all the highly correlated variables while maintaining a sparse solution for β j 3 numerical results we performed a numerical study to investigate the increases in the prediction performance and sign accuracy when the three sign constrained methods constrained ridge constrained lasso constrained elastic net are used as compared with the ordinary lur ridge lasso and elastic net methods without sign constraints the models were called c ridge c lasso c en lur ridge lasso and en respectively 3 1 comparison of prediction performance and sign accuracy we considered six simulation settings and each setting had different conditions depending on four parameters ρ was the correlation between covariates n was the number of samples p was the number of variables and m was the number of sign constraints it is noteworthy that m could be less than p in total 24 different conditions were investigated setting 1 used the sparsity condition in which lasso regression was preferred and all the nonzero regression coefficients were positive setting 2 was similar to setting 1 but allowed both negative and positive coefficients in setting 3 the sparsity condition did not hold therefore ridge regression was preferred and all the predictors had nonzero positive regression coefficients setting 4 was similar to setting 3 but allowed both negative and positive coefficients setting 5 was similar to setting 1 but the number of sign constraints was lower than that of setting 1 setting 6 was similar to setting 3 but the number of sign constraints was lower than that of setting 3 therefore the effect of the number of sign constraints could be examined using settings 5 and 6 in addition both low dimensional n p and high dimensional cases n p were addressed in the numerical study the detailed values of these parameters are presented in the supplementary material tuning parameters were selected using 5 fold cross validation cv the root mean square error rmse was calculated based on a test set consisting of 10000 observations to evaluate the predictive performance we replicated all the simulation results 200 times independently the data listed in table 1 reveals that with setting 1 c lasso performed the best in terms of the rmse in the low dimensional case n 100 p 50 m 5 however in the high dimensional case n 50 p 100 m 60 c ridge and c en were superior to c lasso lasso and lur these results seemed somewhat strange at first glance because setting 1 was designed to favor lasso and c lasso by imposing the sparsity condition however we observed that the use of the sign constraint reinforced the prediction performance of c ridge through automatic variable selection by the sign constraints the c ridge result revealed that numerous coefficients were forced to be zero when they violated the predefined sign constraints thus c ridge had an improved prediction performance even under the sparsity condition from table 1 the rmse of c ridge is 1 to 13 lower than ridge c lasso has 1 to 8 lower rmse than lasso c en has 2 to 40 lower rmse than en in addition it was especially notable that the performance of the ordinary lur method deteriorated enormously in high dimensional cases setting 2 exhibited similar results whereas for setting 3 c ridge was the best and was superior to the ordinary lur except for table 1 all the details are provided in the supplementary material for brevity in addition to the rmse based on the test set to evaluate whether the fitted regression model was interpretable to researchers we considered the sign accuracy this was defined as the proportion for which the estimated coefficients had the correct sign a higher sign accuracy value indicated that more coefficients satisfied the predefined sign constraints the results presented in table 2 reveal that with setting 5 c ridge was superior to c lasso lasso and lur in terms of sign accuracy c lasso has 6 to 80 higher sign accuracy than lasso c en has 26 to 86 higher sign accuracy than en in case of ρ 0 the sign accuracy of c ridge is 1 to 7 higher than ridge in addition the sign accuracy of c ridge improved as the number of sign constraints m increased when ρ 0 5 and m was relatively small ridge exhibited a higher sign accuracy than c ridge because c ridge automatically removed a subset of variables owing to the sign constraints however at the cost of a lower sign accuracy c ridge outperformed ridge in terms of the rmse in these cases additionally setting 6 yielded similar results c ridge and c en outperformed c lasso lasso and lur in terms of the rmse and sign accuracy values as the number of constraints changed in the high dimensional case n 50 p 100 except for table 2 all the details are provided in the supplementary material for brevity 3 2 analysis of real air pollution data we analyzed the annual averages of no2 pm 10 and pm 2 5 measurements in 2020 for the entire area and capital of south korea which are available at www airkorea or kr regarding the entire area the sample sizes were 492 483 and 481 for no2 pm 10 and pm 2 5 respectively which corresponded to the number of air pollution monitoring stations the numbers of predictors were 237 236 and 236 for no2 pm 10 and pm 2 5 respectively regarding the capital area the sample sizes were 169 166 and 166 for no2 pm 10 and pm 2 5 respectively and the number of predictors was 212 for the three cases the predictors included variables related to the land cover industrial characteristics housing characteristics population characteristics traffic volume and amount of discharge sign constraints were created by an experienced medical doctor for example the regression coefficients associated with the traffic volume of the nearest road and traffic volume of major roads were determined to be non negative regardless of the buffer size all the details of the predictors and their sign constraints are provided in the supplementary material regression models with or without the sign constraints and lur were fitted to the six datasets to compare the prediction performances and check the ease of interpretability in particular we investigated the prediction performance changes based on the number of sign constraints m for each dataset 80 was used as the training set and 20 as the test set the tuning parameter λ was selected using 10 fold cv in the training set and all the performance measures were calculated using the test set table 3 summarizes the prediction performances of the seven methods in terms of the adjusted r 2 r a d j 2 and rmse based on the test data except for pm 2 5 with m 40 the rmse of c ridge is 8 to 28 lower than ridge in the entire area c ridge exhibited 5 to 74 higher r a d j 2 and 3 to 60 lower rmse than ridge in the capital area c lasso has 1 to 5 lower rmse than lasso in the entire area and 2 to 3 lower rmse than lasso for no2 outcome in the capital area c en was significantly better than en in terms of both r a d j 2 and the rmse c ridge c lasso and c en performed better than lur in most cases among the sign constrained regression models c ridge was the best for no2 and pm 10 and c lasso was the best for pm 2 5 in both the entire and the capital area to summarize the prediction performances with the six real datasets the sign constrained regression models often exhibited superior performances compared to lur and the regression models without the sign constraints furthermore the results were robust to varying numbers of sign constraints furthermore we checked the ease of interpretation of the fitted regression models as a representative result consider the result from c ridge which had the highest r a d j 2 and lowest rmse for no2 in the capital area where the number of sign constraints was 66 we note the variable sum of major road lengths because different fitting methods exhibited different results as expected its associated the predefined regression coefficient was non negative and the estimated coefficient was non negative unlike the sign constrained method ridge exhibited the opposite sign which might have been a source of misleading interpretations lur did not select the variable in this case the use of the sign constrained models had an important advantage in terms of the interpretability because the estimated nonzero coefficients from the sign constrained models always satisfied the predefined condition all the other details regarding the estimated coefficients are provided in the supplementary material 4 result and discussion the lur method has broad applicability in the prediction of air pollution concentrations fast prediction and ease of interpretation are known to be the main advantages of this method however its implementation consists of several complex steps therefore it does not have a transparent estimation procedure consequently understanding how the lur method behaves in various environmental science situations is difficult considering this point we proposed a novel formulation of the lur model using sign constrained regression the proposed sign constrained regression methods showed superior prediction performances in various simulation settings compared with the existing lur method furthermore the sign constraint reflected domain knowledge and its use could enhance the interpretability we developed a novel algorithm for fitting sign constrained lur models and made an r package freely available for researchers the current study has some limitations first all the lur models in this study assumes independent observations but this assumption can be violated because the covariates may not fully capture the spatial dependence of the outcome variable thus the sign constrained lur model may be extended to allow spatial correlation in this situation second the proposed sign constrained lur algorithm is slow when the data size is large therefore accelerating the steps such as through parallel computing or employing other fast optimization techniques can be helpful third because the confidence interval indicates more informatively how uncertain the regression coefficient estimates are than the p value it may be useful to report the confidence interval for the regression coefficients associated with the variables selected by our proposed sign constrained lur methods however its construction is not as straightforward as that of the ordinary least squares and reporting the confidence interval for the ordinary least squares without taking into account the selection process is not recommended hastie et al 2009 2015 we think that this difficulty in establishing an accurate confidence interval can be a major obstacle to the widespread use of the proposed method in some fields of epidemiology recently when regularization techniques such as lasso and elastic net have been used two principle ways of obtaining confidence intervals have been developed under the name of selective inference the first approach is known as post selection inference and aims to construct simultaneous confidence intervals that do not depend on the selection procedure berk et al 2013 to do so the least squares method must be implemented using only the selected variables in our application one concern with this post selection inference is that the least square estimates may not satisfy the sign constraints thus the specific sign pattern would not be fully reflected when constructing the confidence interval the second approach is to determine the confidence interval conditioned on the selected model and the sign of the estimated regression coefficients lee et al 2016 this method seems more suitable for our purpose because it considers the sign constraints properly however the current theory is limited in that it is not applicable to sign constrained lasso therefore further research is necessary in this area hastie et al 2015 fourth the proposed model has a low capacity to capture nonlinear behaviors and complex interactions between the outcome variables and predictors this is particularly true in large scale studies in this case we recommend machine learning ml methods to explore the heterogeneous nonlinear relationships and complex interactions of the variables considered ren et al 2020 in recent studies ml methods have been widely used to predict air pollution concentrations brokamp et al 2017 karimian et al 2019 li et al 2017 pan 2018 however ml based prediction models have low interpretability recently wong et al 2021 suggested a combination of lur and ml to boost the prediction performance they applied a stepwise variable selection procedure to the lur model building process in the first stage they choose variables matched with a predefined direction of effect on air pollutant concentrations subsequently they applied various ml algorithms to the selected variables in the second stage however their final ml result was suitable only for prediction purposes and not for interpretability an interesting methodological research topic in environmental epidemiology would be to develop a general ml method that allows complex higher order interactions while maintaining interpretability software and data availability name of software scrr sign constrained regularized regression for lur developers and contact information hosik choi woojoo lee soon sun kwon qrio1010 ajou ac kr available since 2022 program language r availability this software is open source and freely downloadable from https sites google com view lwj221 data is provided https sites google com view lwj221 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments woojoo lee was supported by the national research foundation of korea nrf grant funded by the korea government msit no 2021r1a2c1014409 soon sun kwon was supported by the basic science research program of the national research foundation of korea nrf funded by the ministry of science and ict 2017r1e1a1a030 70345 2021r1a6a1a10044950 appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105653 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25437,land use regression is a popular method for predicting ambient pollutant concentrations at points of interest where no measurements are taken however the model building process is complicated and systematically understanding when and how the process works is difficult to overcome these limitations we reformulate the existing land use regression method as a sign constrained regression problem with an explicit objective function to be minimized this novel formulation always leads to estimated regression coefficients that satisfy the predefined direction based on subject matter knowledge while simultaneously substantially improving the prediction performance of the existing land use regression method the advantages of the proposed sign constrained regression method are confirmed through a numerical study and real data analysis keywords land use regression sign constraints penalized regression methods prediction interpretability data availability we can share data and code on github or website 1 introduction the two main aspects when building exposure prediction models are accurate predictions and high interpretability if we focus on only the first aspect variables should be selected to maximize the prediction performance regardless of the correctness of the sign of the regression coefficient crouse et al 2009 in contrast for the second aspect the explanation view has been emphasized to determine how factors affect the amount of exposure amini et al 2014 beelen et al 2013 de hoogh et al 2013 eeftens et al 2012 gonzales et al 2012 gulliver and de hoogh 2015 henderson et al 2007 hoek et al 2011 tang et al 2013 land use regression lur based analysis focuses on the second aspect and retains variables only if their coefficients are consistent with prior assumptions regarding the direction of the effect acknowledging the importance of the high interpretability required for environmental sciences we note that the current lur model building process is far from the optimal way for achieving accurate predictions therefore we propose an alternative method that overcomes the limitations of existing lur methods by reformulating the lur method as a sign constrained regression model lur has been used to interpolate ambient pollutant concentrations at points of interest where no measurements are available using measurements from several monitoring sites for example it was used to predict outdoor concentrations at the home addresses of the study participants in more than 30 cohort studies beelen et al 2013 the small area variation in air quality and health saviah study was the first to use lur to model small scale variations in air pollution briggs et al 1997 the aforementioned study aimed to generate individual level indicators of long term average exposure to air pollution to assess the risk of respiratory diseases in children it included variables such as the traffic volume population density land use and altitude which were compiled using geographic information systems gis the lur model was constrained by requiring that all the regression coefficients have the a priori definition of the required signs for regression coefficients e g positive for traffic and negative for altitude beelen et al 2013 developed an lur model in a standardized way described in the european study of cohorts for air pollution effects escape exposure manual using the air pollutant and gis based values for potential predictor variables when predefined signs of variables are utilized in a model the model becomes more easily interpretable and its risk of overfitting is limited beelen et al 2013 hastie et al 2009 james et al 2013 kato et al 2019 wolf et al 2017 in the lur model building process typically a large number of covariates are considered the existing lur model building process is complicated and renders difficulty in determining when and how the procedure works because of the complexity in implementing the algorithm the key reason is that it does not explicitly stipulate the type of objective function that is minimized therefore this lack of transparency regarding the algorithm s objective function should be avoided to better understand the properties of the lur algorithm for high dimensional problems penalized likelihood regression methods such as the least absolute shrinkage and selection operator lasso tibshirani 1996 and ridge hoerl and kennard 1970 methods seem to be appropriate choices because these methods are theoretically guaranteed to improve the prediction performance by regularizing the regression coefficients the lasso and ridge methods shrink the regression coefficients by penalizing the sums of their absolute and squared values respectively lasso performs automatic variable selection and ridge regression is effective at handling a large number of covariates with high multicollinearity elastic net zou and hastie 2005 was developed to take advantage of the lasso and ridge methods by mixing their penalty functions however applying regularized methods such as lasso ridge and elastic net in ordinary lur models is not suitable because the signs of the regression coefficients from penalized regression methods are not guaranteed to match the domain knowledge essentially a conflict between the estimated regression coefficients and their predefined signs can cause conventional penalized likelihood methods to lose their high interpretability therefore we propose a modification of the penalized regression methods to satisfy domain knowledge based sign constraints recently a sign constrained lasso was developed by gaines et al 2018 and is well suited for our research however to overcome the limitation of lasso in coping with the high multicollinearity that often appears in environmental science we develop new sign constrained regression methods based on the ridge and elastic net methods as alternative solutions for the existing lur first we explain why the sign constrained lasso ridge and elastic net regressions were well suited to the purpose of the present study their objective functions are clearly defined and we examine what kinds of optimization problems are solved to provide estimated regression coefficients that always match the domain knowledge because this new lur formulation is designed to maximize the prediction performance under the sign constraint a significant improvement in the prediction performance can be expected compared to the existing lur method which will be numerically confirmed later the remainder of this study is organized as follows section 2 begins with a brief review of lur focusing on variable selection subsequently the proposed methods including lur with the constrained lasso ridge and elastic net methods are explained section 3 presents the results of a simulation study to assess the prediction performance and sign accuracy of the proposed methods and illustrates their application in real data analysis finally section 4 concludes the paper the supplementary material contains an r function for implementing the proposed sign constrained lur method with some examples 2 method beelen et al 2013 developed an algorithm that used predefined effect directions based on fundamental physical principles for lur variable selection in the escape study first we start with the algorithmic details and propose a new formulation 2 1 formulating lur as sign constrained linear regression the detailed steps for variable selection in lur are as follows first we determined the regression coefficients for each covariate against the response variable subsequently the model with a single covariate with the highest adjusted r 2 becomes the start model next the remaining variables were sequentially input to this start model the candidate models exhibited a change in the adjusted r 2 the model retains the input predictor if the following three conditions for the candidate models are satisfied 1 the increase in adjusted r 2 is greater than 1 2 the coefficient conforms to the pre specified direction and 3 the direction of effect for the predictors already included in the model does not change the fitting process repeats the addition of variables until there are no predictor variables that add more than 1 to the adjusted r 2 of the previous regression model note that when the variable selection criterion uses the adjusted r 2 some variables may be statistically non significant therefore the fitted model removes variables with p values 0 1 hence the final model includes predictor variables with p values 0 1 although its rationale is understandable the detailed implementation steps of the lur are complex moreover understanding the ultimate goals of the procedure is challenging considering the sign constraints we pursue a more systematic and transparent approach to lur variable selection to do this we begin with the following linear regression 1 y i β 0 β 1 x i 1 β 2 x i 2 β 3 x i 3 β p x i p ϵ i ϵ i n 0 σ 2 i 1 n where x i 1 x i 2 x i 3 x i p are p predictor or independent variables y i is the dependent variable and β i denotes the regression coefficient associated with the i th predictor variable the optimization problem of the linear regression model can be written as follows 2 minimize β 0 β 1 β p i 1 n y i β 0 j 1 p β j x i j 2 the ordinary least squares ols method is often used to estimate β 0 β p by minimizing the sum of the squares in 2 because the sign of the estimated β i may not satisfy the corresponding domain knowledge sign constraints need to be imposed on 2 suppose that c k is 1 if the k th predictor variable x k is known to have a positive effect on dependent variable y c k is 1 if it has a negative effect and 0 otherwise in particular c k 0 corresponds to that there is no strong domain knowledge for the sign of β k we define the set of regression coefficients matching the predefined signs using the domain knowledge as c β 1 β 2 β p c k β k 0 for some k which is called a feasible set note that the feasible set does not require the sign constraints for all the explanatory variables the sign constrained ols method corresponds to determining the coefficients that solve 2 in the feasible set based on this the optimization problem of the proposed sign constrained lur model has the following form 3 minimize β 0 β 1 β p i 1 n y i β 0 j 1 p β j x i j 2 subject to β 1 β 2 β p c to explain the proposed objective function concretely we consider four specific covariates to predict pm 2 5 we denote the area of industrial facilities as x 1 the area of agriculture as x 2 the area of forest meadows as x 3 and the population as x 4 regarding pm 2 5 x 1 and x 4 are known to have positive effects whereas x 2 and x 3 have negative effects thus the sign constrained regression model is expressed as follows y i β 0 β 1 x i 1 β 2 x i 2 β 3 x i 3 β 4 x i 4 ϵ i ϵ i n 0 σ 2 i 1 n with the feasible set c β 1 β 2 β 3 β 4 c k β k 0 c 1 0 c 2 0 c 3 0 c 4 0 k 1 2 3 4 for example considering β 2 since c 2 0 is fixed first the estimate of β 2 should be negative or zero to satisfy the condition of c 2 β 2 0 the scope of the sign constraints can be straightforwardly extended to linear combinations of regression coefficients β a general feasible set can be expressed as follows c β 1 β 2 β p c 11 β 11 c 12 β 12 c 1 p β 1 p d 1 c 21 β 21 c 22 β 22 c 2 p β 2 p d 2 c m 1 β m 1 c m 2 β m 2 c m p β m p d m m p the proposed formulation had several advantages first the model can achieve high interpretability because the estimated regression coefficients always satisfy the predefined direction of the regression coefficients second the proposed optimization formulation determines regression coefficients to maximize the prediction performance under sign constraints therefore the proposed constrained optimization formulation is expected to provide an improved prediction performance compared with the original lur method 2 2 sign constrained lur with a large number of covariates in environmental epidemiology lur models typically deal with a large number of predictor variables p compared with the sample size n this situation is a high dimensional regression problem in the statistics literature as is well known both the prediction performance and interpretability can deteriorate when unnecessary covariates are included or when high correlations between covariates are not appropriately handled hastie et al 1995 principal component analysis pca abdi and williams 2010 is a popular dimension reduction method for handling the issue several regularization methods have been developed to prevent such deterioration out of these we consider three popular methods lasso tibshirani 1996 ridge hoerl and kennard 1970 and elastic net zou and hastie 2005 the following section explains how sign constraints can be incorporated into these three regularization methods 2 2 1 lur with constrained lasso model lasso employs the sum of the absolute values of the regression coefficients as a penalty function which is called l 1 regularization this l 1 regularization can lead to some coefficients being exactly zero i e estimations and variable selections are simultaneously performed recently james et al 2013 and gaines et al 2018 developed an algorithm that incorporates sign constraints for lasso we can use the sign constrained lasso algorithm to determine the regression coefficients in the sign constrained lur model the optimization problem for sign constrained lasso is written as follows 4 minimize β 0 r β c i 1 n y i β 0 j 1 p β j x i j 2 λ j 1 p β j where the feasible set is c β β 1 β 2 β p c k β k 0 for some k the tuning parameter λ 0 controls the amount of regularization unlike the objective function of lasso the condition in 4 requires that the estimates of β 1 β p belong to the feasible set c which automatically satisfies the predefined sign constraints for example the solution of λ 0 is the same as that of ols whereas a large λ can result in numerous coefficients having values of exactly zero in particular unlike ols objective function in 4 can be well defined even when p n under regularity conditions in the sense that it allows a unique solution tibshirani 2013 for completeness the details regarding minimizing the objective function in 4 for the high dimensional case are provided in the supplementary material 2 2 2 lur with constrained ridge and elastic net models lasso exhibits good performance under the sparsity condition i e most of the true regression coefficients are zero however the sparsity condition may not hold for environmental epidemiological problems for example various ecological covariates are associated with pm 10 concentrations although the strength of the association may be small dockery et al 1992 stafoggia et al 2016 zeka et al 2005 therefore lasso may not be always the best choice for lur models we need to consider ridge or elastic net models because they are known to handle such non sparse situations better than lasso in this section first we consider the objective functions of the sign constrained ridge and elastic net methods ridge regression employs the sum of the squared values of the regression coefficients as a penalty function which is called l 2 regularization ridge regression shrinks the regression coefficients toward zero this shrinkage does not result in variable selection but often leads to a better prediction performance than that of ols because the predicted value has smaller variance hastie et al 2009 james et al 2013 similar to the manner in which objective function 4 was constructed the optimization problem for the sign constrained ridge regression is written as follows 5 minimize β 0 r β c i 1 n y i β 0 j 1 p β j x i j 2 λ j 1 p β j 2 subject to feasible set c β 1 β 2 β p c k β k 0 for some k objective function in 5 is well defined even when p n due to the penalty function when λ 0 this objective function is the same as that of the ols whereas a positive λ causes the regression coefficients to shrink toward zero unlike ridge regression without sign constraints the sign constrained ridge regression can perform a type of variable selection when the sign of the estimated regression coefficient is the opposite to that of the predefined value the sign constraint should be satisfied however if the objective function increases continuously because of the change in sign the estimated regression coefficient can become zero consequently the sign constrained ridge regression automatically enhances the interpretability of the regression results by performing variable selection in other words by eliminating variables that present results opposite to their corresponding predefined signs an elastic net model was proposed to circumvent the instability of the lasso solution when predictors are highly correlated while maintaining variable selection such as lasso it employs a linear combination of l 1 and l 2 penalty functions as its penalty function the optimization problem for the sign constrained elastic net regression is given by the following 6 minimize β 0 r β c i 1 n y i β 0 j 1 p β j x i j 2 λ 1 j 1 p β j λ 2 j 1 p β j 2 where λ 1 λ 1 1 λ 2 n and λ 2 λ 2 1 λ 2 n when λ 1 0 the objective function in 6 is reduced to that of ridge regression and when λ 2 0 the objective function is reduced to that of lasso unlike lasso the elastic net method can select all the highly correlated variables while maintaining a sparse solution for β j 3 numerical results we performed a numerical study to investigate the increases in the prediction performance and sign accuracy when the three sign constrained methods constrained ridge constrained lasso constrained elastic net are used as compared with the ordinary lur ridge lasso and elastic net methods without sign constraints the models were called c ridge c lasso c en lur ridge lasso and en respectively 3 1 comparison of prediction performance and sign accuracy we considered six simulation settings and each setting had different conditions depending on four parameters ρ was the correlation between covariates n was the number of samples p was the number of variables and m was the number of sign constraints it is noteworthy that m could be less than p in total 24 different conditions were investigated setting 1 used the sparsity condition in which lasso regression was preferred and all the nonzero regression coefficients were positive setting 2 was similar to setting 1 but allowed both negative and positive coefficients in setting 3 the sparsity condition did not hold therefore ridge regression was preferred and all the predictors had nonzero positive regression coefficients setting 4 was similar to setting 3 but allowed both negative and positive coefficients setting 5 was similar to setting 1 but the number of sign constraints was lower than that of setting 1 setting 6 was similar to setting 3 but the number of sign constraints was lower than that of setting 3 therefore the effect of the number of sign constraints could be examined using settings 5 and 6 in addition both low dimensional n p and high dimensional cases n p were addressed in the numerical study the detailed values of these parameters are presented in the supplementary material tuning parameters were selected using 5 fold cross validation cv the root mean square error rmse was calculated based on a test set consisting of 10000 observations to evaluate the predictive performance we replicated all the simulation results 200 times independently the data listed in table 1 reveals that with setting 1 c lasso performed the best in terms of the rmse in the low dimensional case n 100 p 50 m 5 however in the high dimensional case n 50 p 100 m 60 c ridge and c en were superior to c lasso lasso and lur these results seemed somewhat strange at first glance because setting 1 was designed to favor lasso and c lasso by imposing the sparsity condition however we observed that the use of the sign constraint reinforced the prediction performance of c ridge through automatic variable selection by the sign constraints the c ridge result revealed that numerous coefficients were forced to be zero when they violated the predefined sign constraints thus c ridge had an improved prediction performance even under the sparsity condition from table 1 the rmse of c ridge is 1 to 13 lower than ridge c lasso has 1 to 8 lower rmse than lasso c en has 2 to 40 lower rmse than en in addition it was especially notable that the performance of the ordinary lur method deteriorated enormously in high dimensional cases setting 2 exhibited similar results whereas for setting 3 c ridge was the best and was superior to the ordinary lur except for table 1 all the details are provided in the supplementary material for brevity in addition to the rmse based on the test set to evaluate whether the fitted regression model was interpretable to researchers we considered the sign accuracy this was defined as the proportion for which the estimated coefficients had the correct sign a higher sign accuracy value indicated that more coefficients satisfied the predefined sign constraints the results presented in table 2 reveal that with setting 5 c ridge was superior to c lasso lasso and lur in terms of sign accuracy c lasso has 6 to 80 higher sign accuracy than lasso c en has 26 to 86 higher sign accuracy than en in case of ρ 0 the sign accuracy of c ridge is 1 to 7 higher than ridge in addition the sign accuracy of c ridge improved as the number of sign constraints m increased when ρ 0 5 and m was relatively small ridge exhibited a higher sign accuracy than c ridge because c ridge automatically removed a subset of variables owing to the sign constraints however at the cost of a lower sign accuracy c ridge outperformed ridge in terms of the rmse in these cases additionally setting 6 yielded similar results c ridge and c en outperformed c lasso lasso and lur in terms of the rmse and sign accuracy values as the number of constraints changed in the high dimensional case n 50 p 100 except for table 2 all the details are provided in the supplementary material for brevity 3 2 analysis of real air pollution data we analyzed the annual averages of no2 pm 10 and pm 2 5 measurements in 2020 for the entire area and capital of south korea which are available at www airkorea or kr regarding the entire area the sample sizes were 492 483 and 481 for no2 pm 10 and pm 2 5 respectively which corresponded to the number of air pollution monitoring stations the numbers of predictors were 237 236 and 236 for no2 pm 10 and pm 2 5 respectively regarding the capital area the sample sizes were 169 166 and 166 for no2 pm 10 and pm 2 5 respectively and the number of predictors was 212 for the three cases the predictors included variables related to the land cover industrial characteristics housing characteristics population characteristics traffic volume and amount of discharge sign constraints were created by an experienced medical doctor for example the regression coefficients associated with the traffic volume of the nearest road and traffic volume of major roads were determined to be non negative regardless of the buffer size all the details of the predictors and their sign constraints are provided in the supplementary material regression models with or without the sign constraints and lur were fitted to the six datasets to compare the prediction performances and check the ease of interpretability in particular we investigated the prediction performance changes based on the number of sign constraints m for each dataset 80 was used as the training set and 20 as the test set the tuning parameter λ was selected using 10 fold cv in the training set and all the performance measures were calculated using the test set table 3 summarizes the prediction performances of the seven methods in terms of the adjusted r 2 r a d j 2 and rmse based on the test data except for pm 2 5 with m 40 the rmse of c ridge is 8 to 28 lower than ridge in the entire area c ridge exhibited 5 to 74 higher r a d j 2 and 3 to 60 lower rmse than ridge in the capital area c lasso has 1 to 5 lower rmse than lasso in the entire area and 2 to 3 lower rmse than lasso for no2 outcome in the capital area c en was significantly better than en in terms of both r a d j 2 and the rmse c ridge c lasso and c en performed better than lur in most cases among the sign constrained regression models c ridge was the best for no2 and pm 10 and c lasso was the best for pm 2 5 in both the entire and the capital area to summarize the prediction performances with the six real datasets the sign constrained regression models often exhibited superior performances compared to lur and the regression models without the sign constraints furthermore the results were robust to varying numbers of sign constraints furthermore we checked the ease of interpretation of the fitted regression models as a representative result consider the result from c ridge which had the highest r a d j 2 and lowest rmse for no2 in the capital area where the number of sign constraints was 66 we note the variable sum of major road lengths because different fitting methods exhibited different results as expected its associated the predefined regression coefficient was non negative and the estimated coefficient was non negative unlike the sign constrained method ridge exhibited the opposite sign which might have been a source of misleading interpretations lur did not select the variable in this case the use of the sign constrained models had an important advantage in terms of the interpretability because the estimated nonzero coefficients from the sign constrained models always satisfied the predefined condition all the other details regarding the estimated coefficients are provided in the supplementary material 4 result and discussion the lur method has broad applicability in the prediction of air pollution concentrations fast prediction and ease of interpretation are known to be the main advantages of this method however its implementation consists of several complex steps therefore it does not have a transparent estimation procedure consequently understanding how the lur method behaves in various environmental science situations is difficult considering this point we proposed a novel formulation of the lur model using sign constrained regression the proposed sign constrained regression methods showed superior prediction performances in various simulation settings compared with the existing lur method furthermore the sign constraint reflected domain knowledge and its use could enhance the interpretability we developed a novel algorithm for fitting sign constrained lur models and made an r package freely available for researchers the current study has some limitations first all the lur models in this study assumes independent observations but this assumption can be violated because the covariates may not fully capture the spatial dependence of the outcome variable thus the sign constrained lur model may be extended to allow spatial correlation in this situation second the proposed sign constrained lur algorithm is slow when the data size is large therefore accelerating the steps such as through parallel computing or employing other fast optimization techniques can be helpful third because the confidence interval indicates more informatively how uncertain the regression coefficient estimates are than the p value it may be useful to report the confidence interval for the regression coefficients associated with the variables selected by our proposed sign constrained lur methods however its construction is not as straightforward as that of the ordinary least squares and reporting the confidence interval for the ordinary least squares without taking into account the selection process is not recommended hastie et al 2009 2015 we think that this difficulty in establishing an accurate confidence interval can be a major obstacle to the widespread use of the proposed method in some fields of epidemiology recently when regularization techniques such as lasso and elastic net have been used two principle ways of obtaining confidence intervals have been developed under the name of selective inference the first approach is known as post selection inference and aims to construct simultaneous confidence intervals that do not depend on the selection procedure berk et al 2013 to do so the least squares method must be implemented using only the selected variables in our application one concern with this post selection inference is that the least square estimates may not satisfy the sign constraints thus the specific sign pattern would not be fully reflected when constructing the confidence interval the second approach is to determine the confidence interval conditioned on the selected model and the sign of the estimated regression coefficients lee et al 2016 this method seems more suitable for our purpose because it considers the sign constraints properly however the current theory is limited in that it is not applicable to sign constrained lasso therefore further research is necessary in this area hastie et al 2015 fourth the proposed model has a low capacity to capture nonlinear behaviors and complex interactions between the outcome variables and predictors this is particularly true in large scale studies in this case we recommend machine learning ml methods to explore the heterogeneous nonlinear relationships and complex interactions of the variables considered ren et al 2020 in recent studies ml methods have been widely used to predict air pollution concentrations brokamp et al 2017 karimian et al 2019 li et al 2017 pan 2018 however ml based prediction models have low interpretability recently wong et al 2021 suggested a combination of lur and ml to boost the prediction performance they applied a stepwise variable selection procedure to the lur model building process in the first stage they choose variables matched with a predefined direction of effect on air pollutant concentrations subsequently they applied various ml algorithms to the selected variables in the second stage however their final ml result was suitable only for prediction purposes and not for interpretability an interesting methodological research topic in environmental epidemiology would be to develop a general ml method that allows complex higher order interactions while maintaining interpretability software and data availability name of software scrr sign constrained regularized regression for lur developers and contact information hosik choi woojoo lee soon sun kwon qrio1010 ajou ac kr available since 2022 program language r availability this software is open source and freely downloadable from https sites google com view lwj221 data is provided https sites google com view lwj221 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments woojoo lee was supported by the national research foundation of korea nrf grant funded by the korea government msit no 2021r1a2c1014409 soon sun kwon was supported by the basic science research program of the national research foundation of korea nrf funded by the ministry of science and ict 2017r1e1a1a030 70345 2021r1a6a1a10044950 appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105653 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25438,carbon and water cycles in forest ecosystems are tightly coupled but global warming induced soil and atmospheric droughts alter the coupling thereby greatly increasing uncertainty in predicting carbon and water cycles therefore a carbon water coupled model triplex cw flux was developed and an r package rtriplexcwflux was created to facilitate model application triplex cw flux integrates vapor pressure deficit and soil moisture into a stomatal conductance submodule to estimate forest carbon and water fluxes prediction accuracy of triplex cw flux and rtriplexcwflux application were evaluated in a chinese fir cunninghamia lanceolata plantation simulated net ecosystem production nep and evapotranspiration et were in good agreement with flux observations r 2 0 76 for nep 0 71 for et thus the triplex cw flux model can be used to predict and quantify effects of global warming induced droughts on forest carbon and water cycles the open access rtriplexcwflux package facilitates estimations of carbon sequestration and water consumption in forest ecosystems using the observed flux data keywords carbon water flux coupling model drought stress fluxnet tower observation r package data availability data will be made available on request software availability name of software rtriplexcwflux version 0 3 0 developer shulan sun xiaolu zhou wenhua xiang authors shulan sun wenhua xiang shuai ouyang xiaolu zhou changhui peng contact email sslhhxx 163 com year of first release 2022 software required r program language r program size 2 85 mb license mit file license availability and cost the rtriplexcwflux source code and data are published under mit file license the version described in this paper v0 3 0 can be accessed via github https github com shulansun rtriplex cw flux download free of charge 1 introduction carbon and water cycles in forest ecosystems are coupled ecological processes regulated by leaf stomata which control carbon dioxide co2 absorbed during photosynthesis and water vapor loss during transpiration cowan and farquhar 1977 global forests cover 31 of the total land area and store 662 gt of carbon fao 2020 plant transpiration accounts for approximately 65 of terrestrial evapotranspiration et good et al 2015 as the main pathway of gas exchange between plant and atmosphere stomatal behavior reflects the effect of changes in environmental factors i e solar radiation temperature precipitation and relative humidity etc on forest carbon and water cycle hetherington and woodward 2003 global climate change is currently increasing drought frequency and severity which seriously affects cycling and coupling of carbon and water in forest ecosystems such effects greatly increase the uncertainty in predicting forest carbon and water cycles kang et al 2014 thus carbon water coupling relations need to be further explored to effectively manage forest ecosystems and mitigate global climate change atmospheric aridity and soil drought are closely interacting key environmental stressors that affect the stomata and thereby simultaneously alter carbon uptake and water transport in the soil plant atmosphere continuum gentine et al 2019 high vapor pressure deficit vpd increases atmospheric evaporative demand and leads to soil moisture depletion will et al 2013 land atmosphere feedback exacerbates the frequency and severity of drought and plant water stress especially in low rainfall and high vpd areas zhou et al 2019 control of leaf stomata is the primary mechanism of plant s response to drought thus soil and atmospheric moisture stress can regulate the carbon water coupling in forest ecosystems but drought effects are complicated and remain relatively unexplored net ecosystem production nep represents the net land atmosphere co2 flux reichstein et al 2005 and is estimated as the difference between gross primary production and total ecosystem respiration wu et al 2013 similarly et is a key process of water cycle and a major pathway for water loss in the ecosystems including plant transpiration canopy interception and soil evaporation mitchell et al 2009 schlesinger and jasechko 2014 to understand the complex processes and mechanisms in forest carbon and water cycles fluxnet tower observations and models are widely used to investigate real time changes in carbon and water fluxes more than 500 flux towers are in long term operation worldwide providing reliable ground observation data for model calibration and validation however how to extrapolate fluxnet observations to large scales is a challenge armstrong et al 2022 alternatively models have good spatiotemporal extrapolation and thus offer the advantage of estimating gas exchange between forest and atmosphere without spatial and equipment limitations various process based models have been developed to simulate carbon and water fluxes including beps intec chen et al 1999 and triplex flux zhou et al 2008a for carbon flux the penman monteith model for et monteith 1965 the coupmodel jansson and moon 2001 senapati et al 2016 for water and carbon fluxes however those models do not consider effects of atmospheric and soil droughts on photosynthesis and et models of stomatal conductance and et with hydraulic limitation can improve estimates under drought conditions bonan et al 2014 liu et al 2020 therefore the aim of this study was to develop a coupled model that integrates vpd and soil moisture to estimate and predict carbon and water fluxes this model can reveal how the carbon water coupling will be affected by drought stress under climate change in this study a new carbon water coupling model triplex cw flux was developed the triplex cw flux model introduces the modification functions of vpd and soil moisture into a stomatal model to better capture the dynamics of carbon uptake and et at a 30 min scale under water stress to facilitate model use a new open access r package rtriplexcwflux was created in r v4 1 3 r development core team 2022 in which users need to input only relevant environmental variables and parameters to run the model in r the main objectives of this study were to test the prediction accuracy of the triplex cw flux model and to assess the application of the r package rtriplexcwflux following descriptions of triplex cw flux model structure and rtriplexcwflux package basic functions model accuracy and package application were examined using a case study of a chinese fir cunninghamia lanceolata lamb hook plantation chinese fir had the largest plantation area 9 9 million ha in china national forestry and grassland administration 2019 the large amounts of observed flux data effectively tested the prediction accuracy of the triplex cw flux model and also provided evidence for sustainable management to enhance ecosystem services of chinese fir plantations under the future climate change 2 description of the triplex cw flux model 2 1 model structure the triplex cw flux model is an integrated model based on two well established models triplex flux model zhou et al 2008a and penman monteith model monteith 1965 the triplex cw flux model links photosynthesis and et through stomatal conductance and considers the effects of vpd and soil moisture stress on stomatal resistance fig 1 the triplex flux model is based on the two leaf model sun shade model which divides the forest canopy into sunlit leaves that are exposed to both direct and diffusive solar radiation and shaded leaves that are only exposed to diffusive solar radiation photosynthetic rates of shaded and sunlit leaves are calculated separately de pury and farquhar 1997 the triplex flux model has been tested in canadian boreal forests sun et al 2008 and bamboo forests zhang et al 2020 at a 30 min scale and has showed good performance for simple structure few input parameters and variables required the model contains three submodules 1 leaf photosynthesis submodule 2 canopy photosynthesis submodule and 3 ecosystem carbon flux submodule the leaf photosynthesis submodule is based on farquhar s biochemical model farquhar et al 1980 and calculates the instantaneous leaf gross photosynthetic rate using the minimum rate of the rubisco limited gross photosynthesis rate v c and the light limited gross photosynthesis rate v j the net co2 assimilation rate a n for sunlit a sun and shaded a shaded leaves is derived from the difference between leaf gross photosynthetic rate and leaf dark respiration r d baldocchi and meyers 1998 the equation for a n is the following 1 a n min v c v j r d the equation is further expressed as follows leuning 1990 2 a n g s c a c i 1 6 where g s is the stomatal conductance to water vapor c a is the atmospheric co2 concentration c i is the intracellular co2 concentration 1 6 is the ratio of diffusivities for water vapor and co2 in air boyer et al 1997 the canopy photosynthesis submodule is based on the two leaf model and scales up a n from leaf to canopy by summing up photosynthetic rate of shaded and sunlit leaves norman 1993 the canopy gross photosynthesis a canopy was calculated using the following formula 3 a canopy a sun l a i sun a shaded l a i shaded where lai sun and lai shaded are the leaf area index for sunlit and shaded leaves respectively the lai sun and lai shaded calculations are detailed in de pury and farquhar 1997 nep is defined as the difference between gross primary production gpp which is equivalent to a canopy and ecosystem respiration r e lloyd and taylor 1994 4 n e p g p p r e where r e is estimated by summing up growth respiration r g maintenance respiration r m and heterotrophic respiration r h zhou et al 2008a 5 r e r g r m r h detailed formulas of v c v j r d r g r m r h and other related variables are described in zhou et al 2008a et is derived from latent heat flux le w m 2 based on the equations of the penman monteith model liu et al 2006 monteith 1965 6 l e δ r n g ρ c p v p d 1 r a δ γ 1 r c r a 7 e t 0 43 l e 597 0 564 t a where δ is the slope of the saturation vapor pressure against temperature curve kpa c 1 r n is the net radiation at the canopy surface w m 2 g is the soil heat flux w m 2 ρ is the air density kg m 3 c p is the specific heat of the air j kg 1 c 1 vpd is the vapor pressure deficit kpa γ is the psychrometric constant kpa c 1 r c is the canopy resistance and r a is the aerodynamic resistance the equation for r a is as follows maidment 1996 8 r a ln z d z o m k 2 ln z h d z o h u z where z is the height m at wind measurement z h is the height m at air temperature and humidity measurements d is the zero plane displacement height m z om is the roughness height m for momentum transfer z oh is the roughness height m for vapor and heat transfer k is the von karman s constant 0 41 and u z is the wind speed at height z m s 1 the d and z om are derived based on the average canopy height h c m using the following widely used empirical equations brutsaert 1979 9 d 0 7 h c 10 z o m 0 1 h c 11 z o h 0 5 z o m the canopy resistance r c in eq 6 is derived from the canopy conductance g c the stomatal conductance g s in the triplex flux model does not consider the effects of drought on stomata and is estimated by using a n relative humidity rh and c a as input variables ball et al 1988 to consider the effects of vpd and soil water deficit on canopy resistance buckley 2019 stomatal conductance and canopy resistance are modified by the following functions 12 g s g 0 m a n r h c a max 0 min θ i θ w θ s θ w 1 max 0 min v p d c l o s e v p d v p d c l o s e v p d o p e n 1 13 r c 1 g c 1 g s l a i e where g 0 is the initial stomatal conductance at night mmol m 2 s 1 m is an empirical coefficient θ i is the volumetric soil moisture at a 30 cm depth and θ w and θ s are the wilting point and saturated soil moisture respectively the vpd open and vpd close are the vpd at stomatal opening and closure respectively the lai e is the effective leaf area index and can be set to the actually measured lai when lai 2 to 2 when 2 lai 4 and to lai 2 when lai 4 zhou et al 2008b 2 2 input data and parameters meteorological data used in the triplex cw flux model include atmospheric temperature t a c rh vpd kpa c a μmol mol 1 photosynthetic photon flux density ppfd μmol m 2 s 1 net radiation r n w m 2 soil heat flux g w m 2 wind speed v m s 1 and soil volumetric water content at a 30 cm depth swc table s1 the nep and le at a 30 min scale observed by flux tower are used to calibrate and validate the model the input parameters are presented in table s2 according to vegetation type climate and the geographical conditions of the flux tower site stomatal parameters m and g 0 are manually optimized 3 rtriplexcwflux package the triplex cw flux model is encoded in the rtriplexcwflux package users can install and library this package and then input the variables and parameters of the model to run the package in r software https github com shulansun rtriplex cw flux image 1 the overyear true means that the input data consists of full year data and the outputs of the triplex cw flux function include a table of long format data frame five graphs of simulated nep and et at 30 min scale during the entire studied period and in four seasons and one graph of monthly variations in input environmental factors when overyear false the outputs include a table of simulated nep and et and a graph of simulated nep and et at 30 min scale during the entire studied period before running the rtriplexcwflux package users is required to prepare the input variables and parameters and to check in advance whether the column names are consistent with those in internal data after running data inputpara and data inputvariable users can use view inputpara and view inputvariable to see the information and format of input variables and parameters tables respectively the description of column names definitions and units for the input variables was presented in table s1 the parameters of the model were listed in table s2 and the model output data frame was presented in table s3 the package contains a vignette file to provide an overview of its functions and use 4 case study 4 1 site description and data source the study site is located at the huitong national forest ecosystem research station 26 50 n 109 45 e huitong county hunan province china detailed information about the site and flux tower is provided in wen et al 2014 data used in the study were collected from january 2016 to december 2020 in 2016 chinese fir stand age was 20 years and stand density was 3394 stems ha 1 with an average stem diameter at breast height of 14 2 cm and an average tree height of 13 05 m xiang et al 2022 4 2 calibration and validation input parameters affect model performance and prediction accuracy in simulating nep and et input referenced values of major parameters including general parameters and photosynthesis and respiration parameters were summarized in table s4 three years of data from january 2016 to december 2018 were selected for model calibration the coefficient of determination r 2 root mean square error rmse were use to calibrate the model as shown in fig s1 r 2 and rmse indicated simulated nep and et were highly consistent with those of observations in addition the index of agreement ia was used to examine the model performance and was calculated using the following formula 14 i a 1 i 1 n s i o i 2 i 1 n s i o o i o 2 where s i is the i th simulated value o i is the i th observed value o is the average observed value ia ranges from 0 to 1 with high value indicating good performance of the model willmott 1981 4 3 results and discussion 4 3 1 diurnal variations in observed and simulated net ecosystem production and evapotranspiration the 30 min nep and et of model simulations and flux observations from january to december in 2019 and 2020 are shown in figs s2 s5 nep and et reached their peak at noon and valley at night during the day in winter from december to february nep and et values were low and their daily fluctuation was relatively small nep and et gradually increased in spring from march to may and reached the maximums in summer from june to august but started to decrease in autumn from september to november figs s2 s5 the diurnal and seasonal variations of nep and et showed consistent change patterns of r n and t a fig 2 suggesting that solar radiation and temperature are the key drivers of carbon and water fluxes tong et al 2014 yu et al 2008 nep and et simulated by triplex cw flux model could largely capture the diurnal variations in observed values but have some bias to peaks and valleys figs s2 s5 this result is consistent with other studies amthor et al 2001 sun et al 2008 zhang et al 2020 the deviation of peak and valley simulation might be due to 1 systematic and random errors in respiratory flux observation caused by lacking energy balance closure at night falge et al 2001 goulden et al 1996 grant et al 2005 2 the sensitivity of input parameters of the model in response to the changes of radiation and temperature at noon and night for example temperature sensitivity factor of soil respiration q10 could affect respiration within the ecosystem and the distribution between root respiration and heterotrophic respiration zhou et al 2008a therefore the data of soil respiration and its components root respiration and soil microbial respiration should be observed as a supplementary for tower flux data to optimize the input parameters and reduce the errors of simulated results 4 3 2 model validation to test the performance of the triplex cw flux model and display the basic functions of the rtriplexcwflux package the nep and et were simulated at a 30 min scale in a chinese fir plantation simulated nep and et were in good agreement with observed values in four seasons figs 3 and 4 and during entire period fig 5 the r 2 values ranged from 0 70 to 0 84 for nep and from 0 63 to 0 73 for et in four seasons and during the entire period 2019 2020 the rmse ranged from 0 03 to 0 07 g c m 2 for nep and from 0 02 to 0 04 mm for et in four seasons and during the entire period 2019 2020 the ia ranged from 0 90 to 0 95 g c m 2 for nep and from 0 87 to 0 92 mm for et in four seasons and during the entire period 2019 2020 the r 2 values in this study are higher than that in a study carried out in an old black spruce in canada r 2 0 62 sun et al 2008 and within the range of a bamboo forest r 2 0 78 0 91 zhang et al 2020 this indicates that the triplex cw flux model could be used to predict carbon flux in subtropical forests the simulation performance between triplex cw flux model and triplex flux model was compared using the flux data observed in the chinese fir plantation the results showed that nep simulated by the two models were in good agreement with observations in four seasons fig s6 however because et could not be estimated directly by triplex flux model we used gs calculated by triplex flux model and penman monteith equation to estimate et the prediction accuracy of triplex cw flux model was improved in terms of higher rmse and lower ia compared with the estimations by triplex flux model fig s7 this implies that the optimized stomatal conductance submodule by introducing vpd and soil moisture could improve predictive accuracy of the model for water flux nep simulated by triplex cw flux model was 278 87 gc m 2 yr 1 in 2019 and 340 30 gc m 2 yr 1 in 2020 which were within the range of global forests 246 875 gc m 2 yr 1 and were higher than the average value 101 200 gc m 2 yr 1 of subtropical forests in china zhao et al 2019 this indicates that chinese fir plantations have great capacity of carbon sequestration but nep in this study was lower than the value 362 39 g c m 2 yr 1 reported by yu et al 2014 in east asian monsoon subtropical forests stand age 100 years between 20 n and 40 n based on the eddy covariance technique this could be explained by the fact that old natural forests have higher carbon sequestration capacity than plantation forests liao et al 2010 the simulated et by triplex cw flux model was 581 31 mm 39 of annual precipitation in 2019 and 560 05 mm 33 of annual precipitation in 2020 which were lower than other study 63 88 sun et al 2018 this is mainly because penman monteith model estimates forest et by combining various meteorological data and the meteorological data observed by eddy covariance method is easy to be affected by meteorological conditions such as rainfall berbigier et al 2001 furthermore the eddy covariance has energy closure and the observed energy output including sensible heat and latent heat is 10 30 less than the energy input net radiation wilson et al 2002 4 3 3 model limitations and future directions our results showed that the triplex cw flux model had good performance to predict nep and et and could capture the diurnal and seasonal variations of forest carbon and forest fluxes however this model still has some limitations firstly some input parameters in this model are fixed leading to errors in simulation zhou et al 2008a more observed data about input parameters is required to examine their changes with environmental factors secondly precipitation is key source of water in forest ecosystems which affects the rates of photosynthesis and evapotranspiration tong et al 2014 thirdly the effects of soil nutrients on tree growth and carbon allocation are not considered in triplex cw flux model schönbeck et al 2021 these factors should be considered in future model development 5 conclusions to integrate carbon sequestration and et in forest ecosystems a new carbon water coupling model triplex cw flux was developed based on leaf stomata and incorporation of vpd and swc in a stomatal submodule the model effectively simulated nep and et dynamics under changing environmental conditions demonstrating the central role of drought stress in carbon water coupling process thus the triplex cw flux model can predict carbon sequestration and et in forest ecosystems under drought stress due to future global climate change the rtriplexcwflux package is successfully tested in a case study of chinese fir plantations and provides a convenient tool for model simulation although approximately 4 min may be required to run one year of data at 30 min scale 17 520 entries in the package parameters of studied tree species can be adjusted quickly and accurately moreover advanced statistical tools are easily used to further analyze simulated results in the r environment the approaches presented in this study can help to sustainably manage forests and mitigate climate change author contributions w h x c h p and x l z conceived the idea s l s w h x c h p and x l z designed the software and s l s and w h x wrote the manuscript all other authors contributed critically and approved the manuscript for publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the key research and development program of hunan province 2021nk2031 the national key research and development program of china 2021yfd22004 the science and technology innovation program of hunan province 2021rc3104 and the huitong forest ecological station funded by the state forestry and grassland administration of china 2021132078 appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105661 
25438,carbon and water cycles in forest ecosystems are tightly coupled but global warming induced soil and atmospheric droughts alter the coupling thereby greatly increasing uncertainty in predicting carbon and water cycles therefore a carbon water coupled model triplex cw flux was developed and an r package rtriplexcwflux was created to facilitate model application triplex cw flux integrates vapor pressure deficit and soil moisture into a stomatal conductance submodule to estimate forest carbon and water fluxes prediction accuracy of triplex cw flux and rtriplexcwflux application were evaluated in a chinese fir cunninghamia lanceolata plantation simulated net ecosystem production nep and evapotranspiration et were in good agreement with flux observations r 2 0 76 for nep 0 71 for et thus the triplex cw flux model can be used to predict and quantify effects of global warming induced droughts on forest carbon and water cycles the open access rtriplexcwflux package facilitates estimations of carbon sequestration and water consumption in forest ecosystems using the observed flux data keywords carbon water flux coupling model drought stress fluxnet tower observation r package data availability data will be made available on request software availability name of software rtriplexcwflux version 0 3 0 developer shulan sun xiaolu zhou wenhua xiang authors shulan sun wenhua xiang shuai ouyang xiaolu zhou changhui peng contact email sslhhxx 163 com year of first release 2022 software required r program language r program size 2 85 mb license mit file license availability and cost the rtriplexcwflux source code and data are published under mit file license the version described in this paper v0 3 0 can be accessed via github https github com shulansun rtriplex cw flux download free of charge 1 introduction carbon and water cycles in forest ecosystems are coupled ecological processes regulated by leaf stomata which control carbon dioxide co2 absorbed during photosynthesis and water vapor loss during transpiration cowan and farquhar 1977 global forests cover 31 of the total land area and store 662 gt of carbon fao 2020 plant transpiration accounts for approximately 65 of terrestrial evapotranspiration et good et al 2015 as the main pathway of gas exchange between plant and atmosphere stomatal behavior reflects the effect of changes in environmental factors i e solar radiation temperature precipitation and relative humidity etc on forest carbon and water cycle hetherington and woodward 2003 global climate change is currently increasing drought frequency and severity which seriously affects cycling and coupling of carbon and water in forest ecosystems such effects greatly increase the uncertainty in predicting forest carbon and water cycles kang et al 2014 thus carbon water coupling relations need to be further explored to effectively manage forest ecosystems and mitigate global climate change atmospheric aridity and soil drought are closely interacting key environmental stressors that affect the stomata and thereby simultaneously alter carbon uptake and water transport in the soil plant atmosphere continuum gentine et al 2019 high vapor pressure deficit vpd increases atmospheric evaporative demand and leads to soil moisture depletion will et al 2013 land atmosphere feedback exacerbates the frequency and severity of drought and plant water stress especially in low rainfall and high vpd areas zhou et al 2019 control of leaf stomata is the primary mechanism of plant s response to drought thus soil and atmospheric moisture stress can regulate the carbon water coupling in forest ecosystems but drought effects are complicated and remain relatively unexplored net ecosystem production nep represents the net land atmosphere co2 flux reichstein et al 2005 and is estimated as the difference between gross primary production and total ecosystem respiration wu et al 2013 similarly et is a key process of water cycle and a major pathway for water loss in the ecosystems including plant transpiration canopy interception and soil evaporation mitchell et al 2009 schlesinger and jasechko 2014 to understand the complex processes and mechanisms in forest carbon and water cycles fluxnet tower observations and models are widely used to investigate real time changes in carbon and water fluxes more than 500 flux towers are in long term operation worldwide providing reliable ground observation data for model calibration and validation however how to extrapolate fluxnet observations to large scales is a challenge armstrong et al 2022 alternatively models have good spatiotemporal extrapolation and thus offer the advantage of estimating gas exchange between forest and atmosphere without spatial and equipment limitations various process based models have been developed to simulate carbon and water fluxes including beps intec chen et al 1999 and triplex flux zhou et al 2008a for carbon flux the penman monteith model for et monteith 1965 the coupmodel jansson and moon 2001 senapati et al 2016 for water and carbon fluxes however those models do not consider effects of atmospheric and soil droughts on photosynthesis and et models of stomatal conductance and et with hydraulic limitation can improve estimates under drought conditions bonan et al 2014 liu et al 2020 therefore the aim of this study was to develop a coupled model that integrates vpd and soil moisture to estimate and predict carbon and water fluxes this model can reveal how the carbon water coupling will be affected by drought stress under climate change in this study a new carbon water coupling model triplex cw flux was developed the triplex cw flux model introduces the modification functions of vpd and soil moisture into a stomatal model to better capture the dynamics of carbon uptake and et at a 30 min scale under water stress to facilitate model use a new open access r package rtriplexcwflux was created in r v4 1 3 r development core team 2022 in which users need to input only relevant environmental variables and parameters to run the model in r the main objectives of this study were to test the prediction accuracy of the triplex cw flux model and to assess the application of the r package rtriplexcwflux following descriptions of triplex cw flux model structure and rtriplexcwflux package basic functions model accuracy and package application were examined using a case study of a chinese fir cunninghamia lanceolata lamb hook plantation chinese fir had the largest plantation area 9 9 million ha in china national forestry and grassland administration 2019 the large amounts of observed flux data effectively tested the prediction accuracy of the triplex cw flux model and also provided evidence for sustainable management to enhance ecosystem services of chinese fir plantations under the future climate change 2 description of the triplex cw flux model 2 1 model structure the triplex cw flux model is an integrated model based on two well established models triplex flux model zhou et al 2008a and penman monteith model monteith 1965 the triplex cw flux model links photosynthesis and et through stomatal conductance and considers the effects of vpd and soil moisture stress on stomatal resistance fig 1 the triplex flux model is based on the two leaf model sun shade model which divides the forest canopy into sunlit leaves that are exposed to both direct and diffusive solar radiation and shaded leaves that are only exposed to diffusive solar radiation photosynthetic rates of shaded and sunlit leaves are calculated separately de pury and farquhar 1997 the triplex flux model has been tested in canadian boreal forests sun et al 2008 and bamboo forests zhang et al 2020 at a 30 min scale and has showed good performance for simple structure few input parameters and variables required the model contains three submodules 1 leaf photosynthesis submodule 2 canopy photosynthesis submodule and 3 ecosystem carbon flux submodule the leaf photosynthesis submodule is based on farquhar s biochemical model farquhar et al 1980 and calculates the instantaneous leaf gross photosynthetic rate using the minimum rate of the rubisco limited gross photosynthesis rate v c and the light limited gross photosynthesis rate v j the net co2 assimilation rate a n for sunlit a sun and shaded a shaded leaves is derived from the difference between leaf gross photosynthetic rate and leaf dark respiration r d baldocchi and meyers 1998 the equation for a n is the following 1 a n min v c v j r d the equation is further expressed as follows leuning 1990 2 a n g s c a c i 1 6 where g s is the stomatal conductance to water vapor c a is the atmospheric co2 concentration c i is the intracellular co2 concentration 1 6 is the ratio of diffusivities for water vapor and co2 in air boyer et al 1997 the canopy photosynthesis submodule is based on the two leaf model and scales up a n from leaf to canopy by summing up photosynthetic rate of shaded and sunlit leaves norman 1993 the canopy gross photosynthesis a canopy was calculated using the following formula 3 a canopy a sun l a i sun a shaded l a i shaded where lai sun and lai shaded are the leaf area index for sunlit and shaded leaves respectively the lai sun and lai shaded calculations are detailed in de pury and farquhar 1997 nep is defined as the difference between gross primary production gpp which is equivalent to a canopy and ecosystem respiration r e lloyd and taylor 1994 4 n e p g p p r e where r e is estimated by summing up growth respiration r g maintenance respiration r m and heterotrophic respiration r h zhou et al 2008a 5 r e r g r m r h detailed formulas of v c v j r d r g r m r h and other related variables are described in zhou et al 2008a et is derived from latent heat flux le w m 2 based on the equations of the penman monteith model liu et al 2006 monteith 1965 6 l e δ r n g ρ c p v p d 1 r a δ γ 1 r c r a 7 e t 0 43 l e 597 0 564 t a where δ is the slope of the saturation vapor pressure against temperature curve kpa c 1 r n is the net radiation at the canopy surface w m 2 g is the soil heat flux w m 2 ρ is the air density kg m 3 c p is the specific heat of the air j kg 1 c 1 vpd is the vapor pressure deficit kpa γ is the psychrometric constant kpa c 1 r c is the canopy resistance and r a is the aerodynamic resistance the equation for r a is as follows maidment 1996 8 r a ln z d z o m k 2 ln z h d z o h u z where z is the height m at wind measurement z h is the height m at air temperature and humidity measurements d is the zero plane displacement height m z om is the roughness height m for momentum transfer z oh is the roughness height m for vapor and heat transfer k is the von karman s constant 0 41 and u z is the wind speed at height z m s 1 the d and z om are derived based on the average canopy height h c m using the following widely used empirical equations brutsaert 1979 9 d 0 7 h c 10 z o m 0 1 h c 11 z o h 0 5 z o m the canopy resistance r c in eq 6 is derived from the canopy conductance g c the stomatal conductance g s in the triplex flux model does not consider the effects of drought on stomata and is estimated by using a n relative humidity rh and c a as input variables ball et al 1988 to consider the effects of vpd and soil water deficit on canopy resistance buckley 2019 stomatal conductance and canopy resistance are modified by the following functions 12 g s g 0 m a n r h c a max 0 min θ i θ w θ s θ w 1 max 0 min v p d c l o s e v p d v p d c l o s e v p d o p e n 1 13 r c 1 g c 1 g s l a i e where g 0 is the initial stomatal conductance at night mmol m 2 s 1 m is an empirical coefficient θ i is the volumetric soil moisture at a 30 cm depth and θ w and θ s are the wilting point and saturated soil moisture respectively the vpd open and vpd close are the vpd at stomatal opening and closure respectively the lai e is the effective leaf area index and can be set to the actually measured lai when lai 2 to 2 when 2 lai 4 and to lai 2 when lai 4 zhou et al 2008b 2 2 input data and parameters meteorological data used in the triplex cw flux model include atmospheric temperature t a c rh vpd kpa c a μmol mol 1 photosynthetic photon flux density ppfd μmol m 2 s 1 net radiation r n w m 2 soil heat flux g w m 2 wind speed v m s 1 and soil volumetric water content at a 30 cm depth swc table s1 the nep and le at a 30 min scale observed by flux tower are used to calibrate and validate the model the input parameters are presented in table s2 according to vegetation type climate and the geographical conditions of the flux tower site stomatal parameters m and g 0 are manually optimized 3 rtriplexcwflux package the triplex cw flux model is encoded in the rtriplexcwflux package users can install and library this package and then input the variables and parameters of the model to run the package in r software https github com shulansun rtriplex cw flux image 1 the overyear true means that the input data consists of full year data and the outputs of the triplex cw flux function include a table of long format data frame five graphs of simulated nep and et at 30 min scale during the entire studied period and in four seasons and one graph of monthly variations in input environmental factors when overyear false the outputs include a table of simulated nep and et and a graph of simulated nep and et at 30 min scale during the entire studied period before running the rtriplexcwflux package users is required to prepare the input variables and parameters and to check in advance whether the column names are consistent with those in internal data after running data inputpara and data inputvariable users can use view inputpara and view inputvariable to see the information and format of input variables and parameters tables respectively the description of column names definitions and units for the input variables was presented in table s1 the parameters of the model were listed in table s2 and the model output data frame was presented in table s3 the package contains a vignette file to provide an overview of its functions and use 4 case study 4 1 site description and data source the study site is located at the huitong national forest ecosystem research station 26 50 n 109 45 e huitong county hunan province china detailed information about the site and flux tower is provided in wen et al 2014 data used in the study were collected from january 2016 to december 2020 in 2016 chinese fir stand age was 20 years and stand density was 3394 stems ha 1 with an average stem diameter at breast height of 14 2 cm and an average tree height of 13 05 m xiang et al 2022 4 2 calibration and validation input parameters affect model performance and prediction accuracy in simulating nep and et input referenced values of major parameters including general parameters and photosynthesis and respiration parameters were summarized in table s4 three years of data from january 2016 to december 2018 were selected for model calibration the coefficient of determination r 2 root mean square error rmse were use to calibrate the model as shown in fig s1 r 2 and rmse indicated simulated nep and et were highly consistent with those of observations in addition the index of agreement ia was used to examine the model performance and was calculated using the following formula 14 i a 1 i 1 n s i o i 2 i 1 n s i o o i o 2 where s i is the i th simulated value o i is the i th observed value o is the average observed value ia ranges from 0 to 1 with high value indicating good performance of the model willmott 1981 4 3 results and discussion 4 3 1 diurnal variations in observed and simulated net ecosystem production and evapotranspiration the 30 min nep and et of model simulations and flux observations from january to december in 2019 and 2020 are shown in figs s2 s5 nep and et reached their peak at noon and valley at night during the day in winter from december to february nep and et values were low and their daily fluctuation was relatively small nep and et gradually increased in spring from march to may and reached the maximums in summer from june to august but started to decrease in autumn from september to november figs s2 s5 the diurnal and seasonal variations of nep and et showed consistent change patterns of r n and t a fig 2 suggesting that solar radiation and temperature are the key drivers of carbon and water fluxes tong et al 2014 yu et al 2008 nep and et simulated by triplex cw flux model could largely capture the diurnal variations in observed values but have some bias to peaks and valleys figs s2 s5 this result is consistent with other studies amthor et al 2001 sun et al 2008 zhang et al 2020 the deviation of peak and valley simulation might be due to 1 systematic and random errors in respiratory flux observation caused by lacking energy balance closure at night falge et al 2001 goulden et al 1996 grant et al 2005 2 the sensitivity of input parameters of the model in response to the changes of radiation and temperature at noon and night for example temperature sensitivity factor of soil respiration q10 could affect respiration within the ecosystem and the distribution between root respiration and heterotrophic respiration zhou et al 2008a therefore the data of soil respiration and its components root respiration and soil microbial respiration should be observed as a supplementary for tower flux data to optimize the input parameters and reduce the errors of simulated results 4 3 2 model validation to test the performance of the triplex cw flux model and display the basic functions of the rtriplexcwflux package the nep and et were simulated at a 30 min scale in a chinese fir plantation simulated nep and et were in good agreement with observed values in four seasons figs 3 and 4 and during entire period fig 5 the r 2 values ranged from 0 70 to 0 84 for nep and from 0 63 to 0 73 for et in four seasons and during the entire period 2019 2020 the rmse ranged from 0 03 to 0 07 g c m 2 for nep and from 0 02 to 0 04 mm for et in four seasons and during the entire period 2019 2020 the ia ranged from 0 90 to 0 95 g c m 2 for nep and from 0 87 to 0 92 mm for et in four seasons and during the entire period 2019 2020 the r 2 values in this study are higher than that in a study carried out in an old black spruce in canada r 2 0 62 sun et al 2008 and within the range of a bamboo forest r 2 0 78 0 91 zhang et al 2020 this indicates that the triplex cw flux model could be used to predict carbon flux in subtropical forests the simulation performance between triplex cw flux model and triplex flux model was compared using the flux data observed in the chinese fir plantation the results showed that nep simulated by the two models were in good agreement with observations in four seasons fig s6 however because et could not be estimated directly by triplex flux model we used gs calculated by triplex flux model and penman monteith equation to estimate et the prediction accuracy of triplex cw flux model was improved in terms of higher rmse and lower ia compared with the estimations by triplex flux model fig s7 this implies that the optimized stomatal conductance submodule by introducing vpd and soil moisture could improve predictive accuracy of the model for water flux nep simulated by triplex cw flux model was 278 87 gc m 2 yr 1 in 2019 and 340 30 gc m 2 yr 1 in 2020 which were within the range of global forests 246 875 gc m 2 yr 1 and were higher than the average value 101 200 gc m 2 yr 1 of subtropical forests in china zhao et al 2019 this indicates that chinese fir plantations have great capacity of carbon sequestration but nep in this study was lower than the value 362 39 g c m 2 yr 1 reported by yu et al 2014 in east asian monsoon subtropical forests stand age 100 years between 20 n and 40 n based on the eddy covariance technique this could be explained by the fact that old natural forests have higher carbon sequestration capacity than plantation forests liao et al 2010 the simulated et by triplex cw flux model was 581 31 mm 39 of annual precipitation in 2019 and 560 05 mm 33 of annual precipitation in 2020 which were lower than other study 63 88 sun et al 2018 this is mainly because penman monteith model estimates forest et by combining various meteorological data and the meteorological data observed by eddy covariance method is easy to be affected by meteorological conditions such as rainfall berbigier et al 2001 furthermore the eddy covariance has energy closure and the observed energy output including sensible heat and latent heat is 10 30 less than the energy input net radiation wilson et al 2002 4 3 3 model limitations and future directions our results showed that the triplex cw flux model had good performance to predict nep and et and could capture the diurnal and seasonal variations of forest carbon and forest fluxes however this model still has some limitations firstly some input parameters in this model are fixed leading to errors in simulation zhou et al 2008a more observed data about input parameters is required to examine their changes with environmental factors secondly precipitation is key source of water in forest ecosystems which affects the rates of photosynthesis and evapotranspiration tong et al 2014 thirdly the effects of soil nutrients on tree growth and carbon allocation are not considered in triplex cw flux model schönbeck et al 2021 these factors should be considered in future model development 5 conclusions to integrate carbon sequestration and et in forest ecosystems a new carbon water coupling model triplex cw flux was developed based on leaf stomata and incorporation of vpd and swc in a stomatal submodule the model effectively simulated nep and et dynamics under changing environmental conditions demonstrating the central role of drought stress in carbon water coupling process thus the triplex cw flux model can predict carbon sequestration and et in forest ecosystems under drought stress due to future global climate change the rtriplexcwflux package is successfully tested in a case study of chinese fir plantations and provides a convenient tool for model simulation although approximately 4 min may be required to run one year of data at 30 min scale 17 520 entries in the package parameters of studied tree species can be adjusted quickly and accurately moreover advanced statistical tools are easily used to further analyze simulated results in the r environment the approaches presented in this study can help to sustainably manage forests and mitigate climate change author contributions w h x c h p and x l z conceived the idea s l s w h x c h p and x l z designed the software and s l s and w h x wrote the manuscript all other authors contributed critically and approved the manuscript for publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the key research and development program of hunan province 2021nk2031 the national key research and development program of china 2021yfd22004 the science and technology innovation program of hunan province 2021rc3104 and the huitong forest ecological station funded by the state forestry and grassland administration of china 2021132078 appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105661 
25439,most existing hydrologic models and machine learning models failed to perform well on runoff prediction in data scarce regions as an alternative to this the long short term memory lstm prototypical network fusion model based on few shot learning is proposed where the strong learning ability of lstm and the low data dependence of prototypical network are combined the proposed model was calibrated and implemented on monthly runoff prediction in the lancang river basin lrb and the source region of the yellow river basin sryrb compared with eight state of the art data driven models lstm svr ann arma random forest simplernn gru and bilstm the proposed model outperformed especially when less training data were used results in the lrb indicate nse of the proposed model achieved 0 802 and 0 832 when the proportion of training data k was 20 and 45 improved by 0 527 and 0 222 relative to the mean nse of other models respectively in the sryrb nse reached 0 830 and improved by 0 354 when k was 40 the findings imply that the new few shot learning model provides a promising tool for runoff prediction in the two investigated basins and possibly other data scarce basins where precipitation dominates runoff change which will benefit regional water resources management and water security keywords runoff prediction few shot learning lstm prototypical network sparsely gauged basins data availability the authors do not have permission to share data 1 introduction runoff prediction is one of the most basic and core tasks in hydrological studies playing important roles in flood warning and prevention water resources allocation and sustainable basin management mosavi et al 2018 currently most of the widely used runoff predictions are data driven models which means that runoff is estimated based on the observed hydro meteorological data bloschl et al 2013 however numerous river basins in the world are either ungauged or lack sufficient gauges which causes great obstacles and difficulties to build hydrological models for runoff simulation and prediction especially for the basins located in areas with complex and hard geographical conditions sivapalan et al 2003 in fact these areas are the key areas where water security issues flood landslide etc frequently occur xu et al 2018 and thus hydrological information is urgently demanded moreover runoff information is needed for water resources use and management purposes in almost all people inhabited places bloschl et al 2013 therefore runoff prediction in data scarce regions is of great importance for regional water security and water resources management in this study data scarce means the available observed data used for runoff prediction are limited which may be because the ground based hydro meteorological stations are limited and sparse or because the time series of the available data are short to promote the development of hydrologic studies in data scarce basins international communities have made great efforts especially the international association for hydrology iahs officially launched the decade on predictions in ungauged basins pub in 2003 sivapalan et al 2003 the pub aims at achieving major advances in predictive capacity in ungauged basins and enabling hydrology to meet both scientific theoretical and societal practical obligations bloschl et al 2013 through a decade of comprehensive research huge progress has been made by the pub community in improving both runoff prediction methods and a fundamental understanding of how runoff variability changes across processes places and scales bloschl et al 2013 parajka et al 2013 salinas et al 2013 viglione et al 2013 however there is still plenty of room to improve runoff prediction in data scarce regions and requires further research and new approaches guo et al 2021 regarding the methods for runoff prediction in data scarce basins there are two different types process based methods and statistics based methods the process based methods mainly refer to hydrological models including conceptual models and semi distributed or distributed models however due to the lack of sufficient observation data the models related parameters in the data scarce basins need to be inferred from those of calibrated models in gauged basins nawaz et al 2016 to better transfer the information from gauged to ungauged basins many parameter regionalization approaches have been proposed which can be assembled into two groups one group is distance based methods which transfer parameters based on hydrological similarity in terms of spatial proximity zhang and chiew 2009 climate similarity parajka et al 2013 basin attribute similarity oudin et al 2008 or and runoff similarity pool et al 2021 rojas serna et al 2016 however identifying a suitable similarity metric is a non trivial work another group is regression based methods which attempt to identify the relations between model parameters and basin attributes seibert 1999 skaugen et al 2015 however the equifinality issue exists pool et al 2021 overall different parameter regionalization methods exhibit their own advantages and weaknesses whereas their performance vary largely among different basins with climatic and geographic heterogeneity pool et al 2021 zhang and chiew 2009 comparative assessment of parameter regionalization methods needs further investigation and disagreements exist on how to select the most appropriate regionalization method parajka et al 2013 pool et al 2021 in other words using process based methods to achieve reliable runoff prediction results in data scarce basins still remains a challenge guo et al 2021 statistics based methods instead of using physical equations of water energy and mass to represent hydrological processes normally use black box models to relate inputs i e impact elements and output i e runoff with the development of computer science statistics based methods have developed from simple linear regression to diverse machine learning methods such as support vector regression svr buyukyildiz et al 2014 autoregressive model ar banihabib et al 2019 gene expression programming gep das et al 2019 shamshirband et al 2020 autoregressive conditional heteroscedasticity arch bollerslev et al 1994 engle 1982 and its variants wang et al 2020 ann papacharalampous et al 2019 aghelpour and varshavian 2020 taormina and chau 2015 wu and chau 2013 and recently emerged deep learning methods such as lstm yuan et al 2018 zhu et al 2020 fu et al 2020 many previous studies have revealed that machine learning methods own great potentials in runoff prediction especially for lstm which has a strong learning ability of time series oyebode et al 2014 remesan and mathew 2016 solomatine et al 2009 xu et al 2018 however machine learning methods are data hungry which means they require a large amount of labelled data for training when they encounter few labelled data their performance may degrade or even fail therefore how to reduce the dependence of the model on a large amount of data still needs to be investigated deep learning models have achieved success in data intensive applications such as image classification text classification shen 2018 lecun et al 2015 silver et al 2016 however they are often limited when the available data set is small to tackle this problem few shot learning a new machine learning paradigm that utilizes a small amount of supervised information for learning is proposed yang and liu 2014 it can rapidly generalize prior knowledge to new tasks containing only a few samples with supervised information wang et al 2020 few shot learning has received widespread attention in recent years snell et al 2017 zhang et al 2018 finn et al 2017 for instance few shot learning methods have been widely applied in image recognition huang and jiao 2020 zhuo et al 2019 quan and lin 2018 natural language processing liu et al 2019 upadhyay et al 2018 biology and medicine zhao 2018 economic field he et al 2019 industrial and military fields liu et al 2019 and environmental studies huang and jiao 2020 however its application in the hydrological study is relatively little kimura et al 2020 based on how prior knowledge can be used to handle the small samples the existing few shot learning methods can be broadly divided into three categories model based fine tuning data augmentation and transfer learning wang et al 2020 specifically 1 model based fine tuning refers to pre trained model on a large amount of source data in gauged basins which needs to be fine tuned on the small amount of target data in data scarce basins howard and ruder 2018 this method may lead to overfitting due to the data imbalance between the source and target data nakamura and harada 2019 2 the data augmentation method is to use auxiliary data referring to data after flips translations or rotations to enhance the characteristics of the samples in the target data or expand the target data in data scarce basins so that the model can work better royle et al 2007 liu et al 2015 goodfellow et al 2014 dixit et al 2017 3 the transfer learning method is currently a relatively cutting edge method which refers to the transfer of learned knowledge in the gauged basins to a new domain i e data scarce basins liu et al 2018 transfer learning methods can be either based on metric learning snell et al 2017 meta learning sun et al 2019 or graph neural networks zhu et al 2021 under the framework of metric learning the prototypical network shows outstanding performance especially on few shot classification tasks snell et al 2017 its basic idea is mapping each sample into an embedded metric space and calculating the center for each class which refers to as prototype banerjee et al 2005 regarding a problem of few shot classification the prototypical network can learn a metric space by computing and minimizing distances to prototype representations of each class then for the untrained sample the prototype network calculates its distance to all prototype representations separately and selects the class corresponding to the nearest prototype representation as its predicted class banerjee et al 2005 the prototypical network combining artificial neural networks with few shot learning exerts powerful representation ability of the network itself and reduces its dependence on data scale snell et al 2017 therefore it provides a promising way for runoff prediction in data scarce regions most existing hydrologic models and machine learning models failed to perform well on runoff prediction in data scarce regions to improve the accuracy of runoff prediction in data scarce regions in this study motivated by the strong learning ability of lstm and the low data dependence of the prototypical network we propose a new model named lstm prototypical network fusion model for runoff prediction based on few shot learning the objectives are 1 to evaluate the performance of the proposed model on runoff prediction in two data scarce basins 2 to demonstrate the advantage of the proposed model via comparisons with the other representative data driven models 2 study area and data acquisition 2 1 study area the lancang mekong river is the largest transboundary river in southeast asia it originates from the eastern tibetan plateau in china and discharges into the south china sea flowing through china myanmar laos thailand cambodia and vietnam the upper reach of the lancang mekong river in china is called the lancang river the lancang river has a total length of 2161 km and covers a drainage area of 1 67 105 km2 the lancang river basin 21 14 24 88 n 99 16 101 83 e as shown in fig 1 is controlled by the yunjinghong hydrometric station which is the last national station on the mainstream of the lancang river before it flows out of china the lancang river has significant impact on water resources security in china influenced by the south asia monsoon systems the climate in the lancang river basin lrb shows clear seasonality namely the rainy season may october accounting for about 85 of annual precipitation and the dry season november april prone to drought zhang et al 2020 the spatial distribution of annual precipitation is also uneven ranging from over 1600 mm in the southeast to less than 250 mm in the northwest shi et al 2013 the annual average temperature increases from north to south and ranges between 12 3 and 14 3 c the lancang river basin has a complex topography with elevation ranging from 6619 m to 468 m above sea level limited by high altitude harsh environment and other special geographical conditions the study area is poorly gauged with few sparse ground stations established considering the high heterogeneity of the temporal and spatial distributions of water and heat along with lack of sufficient observation data in the lancang river basin we select it as a case study for runoff prediction based on few shot learning to further demonstrate the application capacity of the proposed model we take another data scarce basin named the source region of the yellow river basin sryrb as the second case study the sryrb 32 16 36 13 n 95 90 103 42 e has a drainage area of 121 972 km2 accounting for about 15 of the entire yellow river basin but it is sparsely gauged by 8 national rain gauges and 1 hydrometric station tangnaihai station the basin has different climatic and geographical conditions with the lancang river basin specifically the sryrb belongs to continental plateau climate zone with average annual precipitation of 400 700 mm and mean annual temperature of 5 to 3 c guan et al 2020 more than 70 of the precipitation occurs in summer june september the topography varies greatly with altitude ranging from 2663 m to 6253 m decreasing from west to east the land cover types are complex with the upper reaches dominated by sparse plateau grassland the middle reaches dominated by grasslands and marsh and downstream area dominated by alpine valley 2 2 data acquisition daily precipitation and temperature at six rain gauges in the lancang river basin during 1981 and 2010 and corresponding data at eight rain gauges in the sryrb during 1970 and 1995 are provided by the national meteorological information centre of china meteorological administration http data cma cn monthly runoff data at the yunjinghong hydrometric station recorded from 1981 to 2010 and monthly runoff at the tangnaihai station during 1970 and 1995 are acquired and collated from the hydrological yearbook of the people s republic of china 3 methodology 3 1 basic neural network models the multi layer perceptron mlp also known as artificial neural network mainly contains an input layer at least one hidden layer and an output layer lecun et al 2015 mlp is a fully connected network which means that each one neuron must connect with all the other neurons located in its adjacent layers for each neuron there is a weight and a bias when a vector is input into a layer it will be weighted and added a bias but the above operation is a linear mapping without non linear transformability to gain nonlinear mapping ability the output of the above operation should be input into an activation function which is a non linear function in general a recurrent neural network rnn medsker and jain 2001 is a type of artificial neural network ann used to process sequential data different from mlp rnn has a special feedback structure where each neuron receives current inputs and past state parameters from previous time steps for further calculation the structure allows the neural network to have memory and handle time dependent problems of time series data however simple rnns suffer gradient vanishing and gradient explosion problems when handling a long term data hochreiter and schmidhuber 1997 to address this problem lstm was proposed by hochreiter and schmidhuber 1997 as an improvement to simple rnns lstms add the special gate mechanism which includes four gates cell state forget gate input gate and output gate among them cell state in lstm can memorize long term information and update step by step forget gate controls how to update the previous cell states and how much previous information can be reserved input gate filters which information of the input can be updated into the current cell state output gate determines how much information of cell states can be output lstms have a high learning ability on extracting the temporal features of data series automatically formally the equations are shown as follows 1 f t σ w f h t 1 x t b f 2 i t σ w i h t 1 x t b i 3 o t σ w 0 h t 1 x t b o 4 h t o t tanh c t 5 σ x 1 1 e x 6 tanh x e x e x e x e x 7 σ x 1 1 e x 8 tanh x e x e x e x e x where c t f t i t and o t refer to cell states forget gates input gates and output gates at time step t respectively h t is a hidden state at time step t vector 1 vector 2 means a concatenate operation that connect two vectors into a longer vector represents the element wise multiplication w f w i w o and w c are lstm s weighted metrics and b f b i b o and b c are the bias parameters respectively σ and tanh as equations 7 and 8 shown are the activation functions that enable the model for non linear mapping prototypical network snell et al 2017 was proposed where a few training samples of each class can be acquired it will map each sample into an embedded metric space and calculating the center for each class based on metric learning we aim to make the representation distances of samples in the same class closer and the representation distances of samples in different classes further a loss function as shown in equation 11 is defined to maximize the probability of correct classification during testing unlabeled inputs are embedded into the same metric space and marked as the label of the closest class in metric space via calculating and comparing the distances with all the prototypes briefly the prototypical network method makes the classification task to significantly reduce the dependence on the amount of data 9 d x 1 y 1 x n y n 10 c k 1 s k x i y i s k 11 p φ y k x exp d f φ x c k k exp d f φ x c k 12 j φ log p φ y k x formally labelled dataset refers to d which is divided into support set and query set s k and q k represent the k th class of the support set and the query set respectively c k means the k th prototype of s k which is the average value of s k f φ x refers to the representation of x in which parameters of the model is φ d a b represents the distance in metric space where we often use euclidean distance 3 2 the proposed lstm prototypical network fusion model based on few shot learning motivated by the advantages of lstm and prototypical network we propose a lstm prototypical network fusion model based on few shot learning aiming to improve the accuracy of runoff prediction in data scarce regions fig 2 illustrates the overview of the proposed model the model contains three parts 1 feature extractor 2 prototypes reclassifying module and 3 metric learning module the feature extractor is a lstm network five hidden layers and each layer has 64 neurons in this study for embedding inputs into a specific feature space here we use windows sliding method to reconstruct the data suppose the size of the sliding window is t and the sliding step is one month the data 7 dimensions precipitation at six rain gauges and the runoff at the yunjinghong station in a sliding window t t t 1 and the runoff at time step t constitute a batch the batches are input into the lstm network the output of the feature extractor module is a feature representation for data in the sliding time window which includes a certain pattern and features for the predicted runoff prototypical network was originally designed for classification tasks here for runoff prediction a regression task we designed the prototypes reclassifying module following the graded forecast principle specifically according to different runoff percentiles we divide runoff into different grades 6 grades in total for the lancang river which can be used as expert knowledge and prior information to constrain feature space learning in metric learning module prototypes are calculated for every grade and the feature extractor is optimized by prototypes loss and regression loss so as to gain a better feature space specifically each time series of samples in a sliding time window has a particular feature representation while each prototype can be calculated as an average value of all the feature representations of the same grade as shown in fig 2 each red six pointed star refers to a prototype of a grade class in feature space the prototypes loss is used to optimize the similarity of feature representations between the sample and its prototype in the study the euclidean distance is used for similarity measurement the regression loss is used to measure the performance of runoff prediction by lstm in addition four key parameters of the proposed fusion model include the number of the hidden layer the number of neurons in each layer the size of sliding window and the thresholds of the classifier the first three hyper parameters are selected by using the grid search xia et al 2022 based on a validation set which accounts for 10 of the data after the training data set the grid search is a method of optimizing the performance of a model by traversing a given combination of parameters the last hyper parameter classifier thresholds is determined based on the top 5 20 50 80 and 95 of the runoff percentiles 3 3 framework of runoff prediction based on the proposed model fig 3 illustrates the framework of the proposed runoff prediction model based on few shot learning similar to the other data driven models the proposed model consists of three main steps data preprocessing training and testing however the details of each step designed according to the idea of the proposed model are different from those of other models generally the normal proportion of training data in machine learning methods is 70 80 gholamy et al 2018 in contrast in this study considering different data scarce situations and to highlight the advance of few shot learning we roughly set the ratio of training data k to a range of 20 45 with a changing interval of 5 the simple rationale is that if k becomes larger the advance of the proposed model may become not obvious if k becomes smaller the training data may be too less to train the model well however since the patterns of rainfall runoff may differ among different basins and over different periods or due to the different sizes lengths of available data series among different basins k values may also differ in different cases accordingly we set k as 0 20 0 30 0 35 0 40 for the case study of the lancang river basin lrb and set k as 0 30 0 35 and 0 40 for the case study in the source region of the yellow river basin sryrb respectively 3 3 1 data pre processing to keep consistent time scale of input data and output data daily precipitation at six rain gauges are converted to monthly precipitation to reduce the sensitivity of neural networks on data scale all the data are normalized into a range of 0 1 by using the discrete standardization method as shown in equation 13 13 y i j n o r m a l y i j y min y max y min where y max and y min are the maximum and minimal values of the whole data respectively y i j means the value of the j t h time step of the i t h variables to predict runoff input features selection and construction are important following the sliding window scheme the normalized data with 7 dimensions in a sliding window t t t 1 and the runoff at time step t form a batch 3 3 2 training phase the step by step training process of the proposed fusion model is presented in table 1 the training phase can be divided into four parts namely labeling embedding loss calculation and optimization firstly as the window slides every sample y i j t r a i n in the same window where y i j t r a i n refers to the i th training time series data at the j th time step is marked a grade label by matching the sample s value with the classifier c which is a series intervals t defined in advance secondly according to the grade label the feature extractor receives every sample f t n in every window where f t n means the t th representation of the n th sliding window and embeds certain feature representations into a feature space thirdly acquire the feature prototype p l n for different grades at the same window by calculating the average of all the feature representations with the same grade label at the same window p l n refers to the feature prototype of the l th grade label at the n th window correspondingly p l n means the number of the samples of the l th grade label at the n th window for the n th sliding windows calculate its prototype loss p l o s s n and regression loss r l o s s n specifically the metric distances for prototype loss and regression loss are the euclidean distance and the mean square error respectively afterwards the sum of all the prototype loss and regression loss of the whole sliding windows is calculated finally update the feature extractor parameters using the above two losses by the adam optimizer noticeably the global loss s u m l o s s is the weighted sum of the regression loss and the prototype loss where α β are their weight coefficients respectively 3 4 evaluation metrics to evaluate the performance of the proposed few shot regression model and the comparisons among different models certain evaluation indicators are necessary according to the literature published by bennett et al 2013 in this study the nash sutcliffe model efficiency coefficient nse mean absolute error mae root mean square error rmse relative volume error re akaike information criterion aic and bayesian information criterion bic are selected as the evaluation criteria shown as equations 14 19 14 n s e 1 i 1 n y i p r e y i 2 i 1 n y i y i 2 15 m a e i 1 n y i p r e y i n 16 r m s e i 1 n y i p r e y i 2 n 17 r e i 1 n y i p r e y i i 1 n y i 18 a i c 2 k 2 ln l 19 b i c k l n n 2 ln l where y i p r e and y i denote the predicted value and the observed value at time i respectively y i refers to the mean of the whole observed value n indicates the number of observations k represents the number of parameters of the model and l is the maximum of the likelihood function where we defined it as a normal distribution based on central limit theorem 4 results and discussion 4 1 trend analysis of the hydro meteorological variables in the lancang river basin the seasonal trends in the monthly precipitation and runoff series are firstly removed by using the difference method afterwards trend analysis of the hydro meteorological variables is conducted by using the mann kendall trend test method mann 1945 kendall 1955 fig 4 shows the difference series of monthly precipitation at six rain gauges and runoff at the yunjinghong hydrometric station during 1981 2010 in the lancang river basin fig 4 and trend test results both indicate that there are no obvious significant trends existed in the precipitation and runoff series the pearson correlation coefficients pccs between runoff at the yunjinghong station and precipitation at the six rain gauges over the period of 1981 2010 range from 0 67 to 0 71 and all of these correlation values are significant based on the p values 0 05 the pcc values imply that precipitation is a primary impact factor for runoff generation and that the six rain gauges have similar contributions to runoff at the yunjinghong station 4 2 runoff prediction results by using the proposed model in the lancang river basin based on runoff percentiles and expert knowledge we divide runoff in the lancang river basin into six grades with thresholds of 98 mm 121 mm 236 mm 466 mm and 674 mm respectively taking different proportions 20 45 of input data for training the corresponding performances of the proposed model on runoff prediction are summarized in table 2 briefly the proposed model performed well on runoff prediction even with limited data available with k increases from 0 2 to 0 45 nse of the proposed model improves from 0 802 to 0 832 and the error indicators mae rmse show steady downward tendencies comparing the results of different models under the same k values we can find that the proposed lstm prototypical network fusion model based on few shot learning has a lower data dependence and performs better on runoff prediction in data scarce region in addition aic and bic increase as the value of k decreases it means that when less data is available for model training the model will automatically build more complex rules to learn the patterns hidden in the data taking the best runoff prediction result i e k 0 45 nse 0 832 mae 0 04 rmse 0 07 re 0 01 aic 416 38 bic 383 91 as an example we display the predicted and the observed monthly runoff results at yunjinghong station during 1994 and 2010 as shown in fig 5 intuitively the predicted runoff by the proposed model fits the observed runoff well 4 3 comparison with the selected data driven models in the lancang river basin to further validate the advantage of the proposed lstm prototypical network fusion model we selected eight widely used data driven models lstm support vector regression svr ann autoregressive moving average arma random forest rf simple recurrent neural network simplernn gated recurrent neural network gru and bi directional long short term memory bilstm for comparison we exclude process based models because they often need substantial data that are unavailable in the data scarce regions table 3 shows the best performances of different models for runoff prediction in the lancang river basin based on nse mae rmse re aic and bic criteria with k ranging from 0 20 to 0 45 in general with the same k value the few shot learning based model is superior to all the other comparative models regarding most of the selected evaluation metrics especially for nse specifically the proposed model improves nse by 0 527 0 254 0 294 0 212 and 0 222 relative to the mean nse of the eight comparative data driven models when k is equal to 0 20 0 30 0 35 0 40 and 0 45 respectively comparing the few shot learning model with the lstm model under the same proportion of training data in terms of aic and bic we can find that the proposed few shot learning model balances the complexity and performance of lstm effectively as mentioned in section 3 2 we use the grid search xia et al 2022 method to optimize models parameters based on 10 of the data after the training data set it is worth noting that the lstm simplernn gru and bilstm have the same structure and hyper parameters with the feature extractor of the few shot learning based model comparison between the lstm and the proposed lstm prototypical network fusion model demonstrates that the few shot learning strategy can improve the performance of the lstm in a data scarce situation although svr ann arma and rf have different structures with the proposed model similarly we use the grid search method based on 10 of the data after the training data set to calibrate the models parameters 4 4 model validation in the source region of the yellow river basin sryrb in this case study the pearson correlation coefficients pccs between the precipitation at the eight rain gauges and runoff at the tanghaihai station over the period of 1970 1995 range from 0 69 to 0 75 all of which are significant with p value 0 05 based on runoff percentile runoff is divided into five grades in the sryrb considering the length of data series in the sryrb 1970 1995 26 years is shorter than that of the lrb 1981 2010 30 years we set the proportion of training data k relatively larger ranging from 30 35 40 10 of the data after the training data set are used for hyper parameter verification and the rest for validation table 4 lists the performance of the few shot learning model on runoff prediction in the sryrb during the validation period 1983 1995 as well as the comparison results with the other 8 data driven models the results indicate that the proposed model worked well and outperformed the rest models in terms of nse mae rmse aic and bic compared with the mean nse of the eight comparison models nse of the proposed model improved by 0 479 0 403 and 0 354 when k values are 0 30 0 35 and 0 40 respectively fig 6 displays the time series of the observed runoff along with the predicted runoff by using the proposed fusion model when k is 40 results in fig 6 indicate that the proposed model can predict monthly runoff well in most cases while peak runoff prediction still needs improvement 4 5 advantages limitations suggestions and future work in this study to improve the accuracy of runoff prediction in data scarce regions we proposed the lstm prototypical network fusion model from a new viewpoint of few shot learning one advantage of the proposed model is that it combines the advances of both lstm i e strong learning ability on time series data and prototypical network i e low dependence on data this advantage allows the proposed model to achieve good performance on runoff prediction with nse over 0 80 even with very limited training data accounting for 20 72 samples and outperform the eight widely used comparative data driven methods which often require for relatively large amount of data the second advantage is that following the graded forecast idea the proposed model relates classification task runoff grades and regression task runoff prediction to some extent via the prototype loss the combination of the two loss functions prototype loss and regression loss are designed and combined for objective optimization which can achieve both classification purpose in terms of feature space and regression purpose in terms of output space thirdly the proposed model requires few variables as input and is flexible to add more variables if necessary for instance only monthly runoff and precipitation are need as input data in the two case studies in comparison with process based hydrologic models it is flexible and easy to add more impact variables e g vegetation index into the proposed model if the related data are available however the proposed model also has its limitations for instance the proposed lstm prototypical network fusion model is based on data driven instead of physical mechanism so its interpretability to hydrologic processes is poor for example it is hard to explain the physical relationships among precipitation evaporation soil moisture and runoff by using the proposed model besides the hyper parameters such as the size of the sliding window the number of grades for runoff the thresholds for different grades of runoff the proportion of training data k of the proposed model are determined based on prior knowledge in this study however these parameters may vary among different basins and it is not easy to select appropriate values at once furthermore the structure of lstm network such as the numbers of layers and neurons is not fixed but depends on the size of the feature space and the complexity of the specific data patterns in total we tested the proposed model in five basins including two data scarce basins and three well gauged basins the results indicate that the proposed model performed well in the two data scarce basins the lancang river basin and the source region of the yellow river basin while failed in the other three basins lan river basin qingliu river basin xixian basin which are located in the middle or lower branches of the yellow river yangtze river and huai river respectively through the investigations we notice that the proposed model performed well in the cases where the input data rainfall and the output data runoff have high correlation coefficients e g above 0 6 if the correlation between input and output is poor the proposed model may probably fail actually we can understand the results in the following way for the lancang river basin and the source region of the yellow river basin rainfall is the major impact factor on runoff the proposed model can capture the rainfall runoff relationship even based on limited rain gauge data therefore the proposed model performed well in the two case studies however in the other three basins which are located at the middle or lower reaches of rivers intensive human activities exist and contribute to runoff change therefore the proposed model could not predict runoff well by only using the rainfall data in the other three basin so the lesson we learnt is that the proposed model is suitable for data scarce regions where precipitation dominates runoff change if not the performance of the proposed model is likely to be affected since the other important influencing factor e g land use change water withdraw of runoff are not explored by the model when the proposed model is applied to other suitable basins the following suggestions can be referenced by the users regarding the input data requirement such as the number of rain gauges the length of data series for model applications it is hard to give a specific value threshold since it depends on the rainfall runoff pattern of a specific basin besides although the proposed model reduces dependence on data it is still a data driven model and requires for the most basic information as input and it will still fail when the amount of data is too small hence what can be suggested to the users is using all the currently available data in the data scarce basin as much as possible at the very beginning of model application certain parameters of the model can be adjusted and optimized later after users learn more about the data moreover if the input data are redundant the model would selectively use the useful data but if we reduce the input data artificially we may reduce some data important for the model which may lead to poor results in addition we suggest divide the grades of runoff in a specific basin based on its runoff percentile considering the more grades classes may result in larger classification error we suggest set the number of grades in a range of 3 10 the proportion of training data k needs to be tuned and justified when the proposed model is applied in a potential basin and we recommend starting try from 20 to 45 most recent work in the field of runoff prediction in data scarce basins are more about regionalization methods althoff et al 2022 nogueira filho et al 2022 instead of data driven methods runoff prediction in ungauged basins of the brazilian cerrado biome althoff et al 2022 indicate that parameters donation by spatial proximity led to nse of 0 56 while by attributes proximity led to nse of 0 45 respectively the contrast between the regionalization models and the proposed model is not equal since they have different perspectives of solving the problem as mentioned above prior knowledge supporting to determine hyper parameters of a target region is very important to guide the model learning however prior knowledge is unavailable to enormous data scarce basins therefore in the future we attempt to improve the model to adaptively learn and determine the hyper parameters from the inputs furthermore daily or hourly runoff prediction model in data scarce basins will be explored in the future in addition integrating more factors affecting runoff into the model will help to expand the application scope of the model in data scarce regions the few shot learning based model can not only be used for runoff prediction in data scarce regions but also has the potential in wider environment related applications such as water quantity assessment in data scarce regions 5 conclusions in this study we proposed a new few shot learning model lstm prototypical network fusion model for runoff prediction in data scarce regions following the idea of graded forecast the proposed model achieved classification task in the feature space and regression task in the output space even with limited data to illustrate the performance of the proposed model we take monthly runoff prediction in the lancang river basin and the source region of the yellow river basin as case studies results in the lancang river basin indicate that the proposed model performed well with nse ranging from 0 802 to 0 832 when only 20 72 samples 45 162 samples of the data are used for training relative to the mean nse of the eight comparative data driven models the proposed model improves nse by 0 527 0 254 0 294 0 212 and 0 222 when k equals 0 20 0 30 0 35 0 40 and 0 45 respectively results in the sryrb show that nse of the proposed model reached 0 830 when training ratio was 40 0 354 higher than the mean nse of the comparison models compared with the eight state of the art comparative data driven models lstm svr ann arma rf simplernn gru and bilstm the proposed model demonstrated its superiority in terms of nse mae and rmse in both case studies the findings indicate that the proposed model provides a new promising tool for runoff prediction in the two investigated basins and possibly other data scarce basins where precipitation dominates runoff change which will benefit water resources management and sustainable utilization in these basins the proposed model needs evaluation and justification before applying in other data scarce basins author contributions q y and j s designed the research g w provided data support m y implemented the model and analysed the data q y and m y wrote the first manuscript draft j s g w and w z made valuable suggestions on discussion w z contributed to model comparisons all authors commented and revised the manuscript software and data availability name of software few shot learning based model for runoff prediction developers minghong yang qinli yang contact address no 2006 xiyuan avenue chengdu 611731 china email qinli yang uestc edu cn year first available 2021 required hardware and software the software works in python and cuda on windows based computers cost free software and data availability https github com ray yang521 few shot git runoff data are available upon request to the corresponding author declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been financially supported by the national key research and development program of china grant number 2021yfc3201100 the national natural science foundation of china grant numbers 52079026 41830863 and 61976044 sichuan science and technology program grant number 2020yfh0037 2022yfg0260 the belt and road fund on water and sustainability of the state key laboratory of hydrology water resources and hydraulic engineering grant number 2019nkzd02 the open research fund of state key laboratory of simulation and regulation of water cycle in river basin grant number iwhr skl 201911 and the fundamental research funds for the central universities grant number zygx2019z014 many thanks go to cobbinah m bernard who significantly contributed to the manuscript revision 
25439,most existing hydrologic models and machine learning models failed to perform well on runoff prediction in data scarce regions as an alternative to this the long short term memory lstm prototypical network fusion model based on few shot learning is proposed where the strong learning ability of lstm and the low data dependence of prototypical network are combined the proposed model was calibrated and implemented on monthly runoff prediction in the lancang river basin lrb and the source region of the yellow river basin sryrb compared with eight state of the art data driven models lstm svr ann arma random forest simplernn gru and bilstm the proposed model outperformed especially when less training data were used results in the lrb indicate nse of the proposed model achieved 0 802 and 0 832 when the proportion of training data k was 20 and 45 improved by 0 527 and 0 222 relative to the mean nse of other models respectively in the sryrb nse reached 0 830 and improved by 0 354 when k was 40 the findings imply that the new few shot learning model provides a promising tool for runoff prediction in the two investigated basins and possibly other data scarce basins where precipitation dominates runoff change which will benefit regional water resources management and water security keywords runoff prediction few shot learning lstm prototypical network sparsely gauged basins data availability the authors do not have permission to share data 1 introduction runoff prediction is one of the most basic and core tasks in hydrological studies playing important roles in flood warning and prevention water resources allocation and sustainable basin management mosavi et al 2018 currently most of the widely used runoff predictions are data driven models which means that runoff is estimated based on the observed hydro meteorological data bloschl et al 2013 however numerous river basins in the world are either ungauged or lack sufficient gauges which causes great obstacles and difficulties to build hydrological models for runoff simulation and prediction especially for the basins located in areas with complex and hard geographical conditions sivapalan et al 2003 in fact these areas are the key areas where water security issues flood landslide etc frequently occur xu et al 2018 and thus hydrological information is urgently demanded moreover runoff information is needed for water resources use and management purposes in almost all people inhabited places bloschl et al 2013 therefore runoff prediction in data scarce regions is of great importance for regional water security and water resources management in this study data scarce means the available observed data used for runoff prediction are limited which may be because the ground based hydro meteorological stations are limited and sparse or because the time series of the available data are short to promote the development of hydrologic studies in data scarce basins international communities have made great efforts especially the international association for hydrology iahs officially launched the decade on predictions in ungauged basins pub in 2003 sivapalan et al 2003 the pub aims at achieving major advances in predictive capacity in ungauged basins and enabling hydrology to meet both scientific theoretical and societal practical obligations bloschl et al 2013 through a decade of comprehensive research huge progress has been made by the pub community in improving both runoff prediction methods and a fundamental understanding of how runoff variability changes across processes places and scales bloschl et al 2013 parajka et al 2013 salinas et al 2013 viglione et al 2013 however there is still plenty of room to improve runoff prediction in data scarce regions and requires further research and new approaches guo et al 2021 regarding the methods for runoff prediction in data scarce basins there are two different types process based methods and statistics based methods the process based methods mainly refer to hydrological models including conceptual models and semi distributed or distributed models however due to the lack of sufficient observation data the models related parameters in the data scarce basins need to be inferred from those of calibrated models in gauged basins nawaz et al 2016 to better transfer the information from gauged to ungauged basins many parameter regionalization approaches have been proposed which can be assembled into two groups one group is distance based methods which transfer parameters based on hydrological similarity in terms of spatial proximity zhang and chiew 2009 climate similarity parajka et al 2013 basin attribute similarity oudin et al 2008 or and runoff similarity pool et al 2021 rojas serna et al 2016 however identifying a suitable similarity metric is a non trivial work another group is regression based methods which attempt to identify the relations between model parameters and basin attributes seibert 1999 skaugen et al 2015 however the equifinality issue exists pool et al 2021 overall different parameter regionalization methods exhibit their own advantages and weaknesses whereas their performance vary largely among different basins with climatic and geographic heterogeneity pool et al 2021 zhang and chiew 2009 comparative assessment of parameter regionalization methods needs further investigation and disagreements exist on how to select the most appropriate regionalization method parajka et al 2013 pool et al 2021 in other words using process based methods to achieve reliable runoff prediction results in data scarce basins still remains a challenge guo et al 2021 statistics based methods instead of using physical equations of water energy and mass to represent hydrological processes normally use black box models to relate inputs i e impact elements and output i e runoff with the development of computer science statistics based methods have developed from simple linear regression to diverse machine learning methods such as support vector regression svr buyukyildiz et al 2014 autoregressive model ar banihabib et al 2019 gene expression programming gep das et al 2019 shamshirband et al 2020 autoregressive conditional heteroscedasticity arch bollerslev et al 1994 engle 1982 and its variants wang et al 2020 ann papacharalampous et al 2019 aghelpour and varshavian 2020 taormina and chau 2015 wu and chau 2013 and recently emerged deep learning methods such as lstm yuan et al 2018 zhu et al 2020 fu et al 2020 many previous studies have revealed that machine learning methods own great potentials in runoff prediction especially for lstm which has a strong learning ability of time series oyebode et al 2014 remesan and mathew 2016 solomatine et al 2009 xu et al 2018 however machine learning methods are data hungry which means they require a large amount of labelled data for training when they encounter few labelled data their performance may degrade or even fail therefore how to reduce the dependence of the model on a large amount of data still needs to be investigated deep learning models have achieved success in data intensive applications such as image classification text classification shen 2018 lecun et al 2015 silver et al 2016 however they are often limited when the available data set is small to tackle this problem few shot learning a new machine learning paradigm that utilizes a small amount of supervised information for learning is proposed yang and liu 2014 it can rapidly generalize prior knowledge to new tasks containing only a few samples with supervised information wang et al 2020 few shot learning has received widespread attention in recent years snell et al 2017 zhang et al 2018 finn et al 2017 for instance few shot learning methods have been widely applied in image recognition huang and jiao 2020 zhuo et al 2019 quan and lin 2018 natural language processing liu et al 2019 upadhyay et al 2018 biology and medicine zhao 2018 economic field he et al 2019 industrial and military fields liu et al 2019 and environmental studies huang and jiao 2020 however its application in the hydrological study is relatively little kimura et al 2020 based on how prior knowledge can be used to handle the small samples the existing few shot learning methods can be broadly divided into three categories model based fine tuning data augmentation and transfer learning wang et al 2020 specifically 1 model based fine tuning refers to pre trained model on a large amount of source data in gauged basins which needs to be fine tuned on the small amount of target data in data scarce basins howard and ruder 2018 this method may lead to overfitting due to the data imbalance between the source and target data nakamura and harada 2019 2 the data augmentation method is to use auxiliary data referring to data after flips translations or rotations to enhance the characteristics of the samples in the target data or expand the target data in data scarce basins so that the model can work better royle et al 2007 liu et al 2015 goodfellow et al 2014 dixit et al 2017 3 the transfer learning method is currently a relatively cutting edge method which refers to the transfer of learned knowledge in the gauged basins to a new domain i e data scarce basins liu et al 2018 transfer learning methods can be either based on metric learning snell et al 2017 meta learning sun et al 2019 or graph neural networks zhu et al 2021 under the framework of metric learning the prototypical network shows outstanding performance especially on few shot classification tasks snell et al 2017 its basic idea is mapping each sample into an embedded metric space and calculating the center for each class which refers to as prototype banerjee et al 2005 regarding a problem of few shot classification the prototypical network can learn a metric space by computing and minimizing distances to prototype representations of each class then for the untrained sample the prototype network calculates its distance to all prototype representations separately and selects the class corresponding to the nearest prototype representation as its predicted class banerjee et al 2005 the prototypical network combining artificial neural networks with few shot learning exerts powerful representation ability of the network itself and reduces its dependence on data scale snell et al 2017 therefore it provides a promising way for runoff prediction in data scarce regions most existing hydrologic models and machine learning models failed to perform well on runoff prediction in data scarce regions to improve the accuracy of runoff prediction in data scarce regions in this study motivated by the strong learning ability of lstm and the low data dependence of the prototypical network we propose a new model named lstm prototypical network fusion model for runoff prediction based on few shot learning the objectives are 1 to evaluate the performance of the proposed model on runoff prediction in two data scarce basins 2 to demonstrate the advantage of the proposed model via comparisons with the other representative data driven models 2 study area and data acquisition 2 1 study area the lancang mekong river is the largest transboundary river in southeast asia it originates from the eastern tibetan plateau in china and discharges into the south china sea flowing through china myanmar laos thailand cambodia and vietnam the upper reach of the lancang mekong river in china is called the lancang river the lancang river has a total length of 2161 km and covers a drainage area of 1 67 105 km2 the lancang river basin 21 14 24 88 n 99 16 101 83 e as shown in fig 1 is controlled by the yunjinghong hydrometric station which is the last national station on the mainstream of the lancang river before it flows out of china the lancang river has significant impact on water resources security in china influenced by the south asia monsoon systems the climate in the lancang river basin lrb shows clear seasonality namely the rainy season may october accounting for about 85 of annual precipitation and the dry season november april prone to drought zhang et al 2020 the spatial distribution of annual precipitation is also uneven ranging from over 1600 mm in the southeast to less than 250 mm in the northwest shi et al 2013 the annual average temperature increases from north to south and ranges between 12 3 and 14 3 c the lancang river basin has a complex topography with elevation ranging from 6619 m to 468 m above sea level limited by high altitude harsh environment and other special geographical conditions the study area is poorly gauged with few sparse ground stations established considering the high heterogeneity of the temporal and spatial distributions of water and heat along with lack of sufficient observation data in the lancang river basin we select it as a case study for runoff prediction based on few shot learning to further demonstrate the application capacity of the proposed model we take another data scarce basin named the source region of the yellow river basin sryrb as the second case study the sryrb 32 16 36 13 n 95 90 103 42 e has a drainage area of 121 972 km2 accounting for about 15 of the entire yellow river basin but it is sparsely gauged by 8 national rain gauges and 1 hydrometric station tangnaihai station the basin has different climatic and geographical conditions with the lancang river basin specifically the sryrb belongs to continental plateau climate zone with average annual precipitation of 400 700 mm and mean annual temperature of 5 to 3 c guan et al 2020 more than 70 of the precipitation occurs in summer june september the topography varies greatly with altitude ranging from 2663 m to 6253 m decreasing from west to east the land cover types are complex with the upper reaches dominated by sparse plateau grassland the middle reaches dominated by grasslands and marsh and downstream area dominated by alpine valley 2 2 data acquisition daily precipitation and temperature at six rain gauges in the lancang river basin during 1981 and 2010 and corresponding data at eight rain gauges in the sryrb during 1970 and 1995 are provided by the national meteorological information centre of china meteorological administration http data cma cn monthly runoff data at the yunjinghong hydrometric station recorded from 1981 to 2010 and monthly runoff at the tangnaihai station during 1970 and 1995 are acquired and collated from the hydrological yearbook of the people s republic of china 3 methodology 3 1 basic neural network models the multi layer perceptron mlp also known as artificial neural network mainly contains an input layer at least one hidden layer and an output layer lecun et al 2015 mlp is a fully connected network which means that each one neuron must connect with all the other neurons located in its adjacent layers for each neuron there is a weight and a bias when a vector is input into a layer it will be weighted and added a bias but the above operation is a linear mapping without non linear transformability to gain nonlinear mapping ability the output of the above operation should be input into an activation function which is a non linear function in general a recurrent neural network rnn medsker and jain 2001 is a type of artificial neural network ann used to process sequential data different from mlp rnn has a special feedback structure where each neuron receives current inputs and past state parameters from previous time steps for further calculation the structure allows the neural network to have memory and handle time dependent problems of time series data however simple rnns suffer gradient vanishing and gradient explosion problems when handling a long term data hochreiter and schmidhuber 1997 to address this problem lstm was proposed by hochreiter and schmidhuber 1997 as an improvement to simple rnns lstms add the special gate mechanism which includes four gates cell state forget gate input gate and output gate among them cell state in lstm can memorize long term information and update step by step forget gate controls how to update the previous cell states and how much previous information can be reserved input gate filters which information of the input can be updated into the current cell state output gate determines how much information of cell states can be output lstms have a high learning ability on extracting the temporal features of data series automatically formally the equations are shown as follows 1 f t σ w f h t 1 x t b f 2 i t σ w i h t 1 x t b i 3 o t σ w 0 h t 1 x t b o 4 h t o t tanh c t 5 σ x 1 1 e x 6 tanh x e x e x e x e x 7 σ x 1 1 e x 8 tanh x e x e x e x e x where c t f t i t and o t refer to cell states forget gates input gates and output gates at time step t respectively h t is a hidden state at time step t vector 1 vector 2 means a concatenate operation that connect two vectors into a longer vector represents the element wise multiplication w f w i w o and w c are lstm s weighted metrics and b f b i b o and b c are the bias parameters respectively σ and tanh as equations 7 and 8 shown are the activation functions that enable the model for non linear mapping prototypical network snell et al 2017 was proposed where a few training samples of each class can be acquired it will map each sample into an embedded metric space and calculating the center for each class based on metric learning we aim to make the representation distances of samples in the same class closer and the representation distances of samples in different classes further a loss function as shown in equation 11 is defined to maximize the probability of correct classification during testing unlabeled inputs are embedded into the same metric space and marked as the label of the closest class in metric space via calculating and comparing the distances with all the prototypes briefly the prototypical network method makes the classification task to significantly reduce the dependence on the amount of data 9 d x 1 y 1 x n y n 10 c k 1 s k x i y i s k 11 p φ y k x exp d f φ x c k k exp d f φ x c k 12 j φ log p φ y k x formally labelled dataset refers to d which is divided into support set and query set s k and q k represent the k th class of the support set and the query set respectively c k means the k th prototype of s k which is the average value of s k f φ x refers to the representation of x in which parameters of the model is φ d a b represents the distance in metric space where we often use euclidean distance 3 2 the proposed lstm prototypical network fusion model based on few shot learning motivated by the advantages of lstm and prototypical network we propose a lstm prototypical network fusion model based on few shot learning aiming to improve the accuracy of runoff prediction in data scarce regions fig 2 illustrates the overview of the proposed model the model contains three parts 1 feature extractor 2 prototypes reclassifying module and 3 metric learning module the feature extractor is a lstm network five hidden layers and each layer has 64 neurons in this study for embedding inputs into a specific feature space here we use windows sliding method to reconstruct the data suppose the size of the sliding window is t and the sliding step is one month the data 7 dimensions precipitation at six rain gauges and the runoff at the yunjinghong station in a sliding window t t t 1 and the runoff at time step t constitute a batch the batches are input into the lstm network the output of the feature extractor module is a feature representation for data in the sliding time window which includes a certain pattern and features for the predicted runoff prototypical network was originally designed for classification tasks here for runoff prediction a regression task we designed the prototypes reclassifying module following the graded forecast principle specifically according to different runoff percentiles we divide runoff into different grades 6 grades in total for the lancang river which can be used as expert knowledge and prior information to constrain feature space learning in metric learning module prototypes are calculated for every grade and the feature extractor is optimized by prototypes loss and regression loss so as to gain a better feature space specifically each time series of samples in a sliding time window has a particular feature representation while each prototype can be calculated as an average value of all the feature representations of the same grade as shown in fig 2 each red six pointed star refers to a prototype of a grade class in feature space the prototypes loss is used to optimize the similarity of feature representations between the sample and its prototype in the study the euclidean distance is used for similarity measurement the regression loss is used to measure the performance of runoff prediction by lstm in addition four key parameters of the proposed fusion model include the number of the hidden layer the number of neurons in each layer the size of sliding window and the thresholds of the classifier the first three hyper parameters are selected by using the grid search xia et al 2022 based on a validation set which accounts for 10 of the data after the training data set the grid search is a method of optimizing the performance of a model by traversing a given combination of parameters the last hyper parameter classifier thresholds is determined based on the top 5 20 50 80 and 95 of the runoff percentiles 3 3 framework of runoff prediction based on the proposed model fig 3 illustrates the framework of the proposed runoff prediction model based on few shot learning similar to the other data driven models the proposed model consists of three main steps data preprocessing training and testing however the details of each step designed according to the idea of the proposed model are different from those of other models generally the normal proportion of training data in machine learning methods is 70 80 gholamy et al 2018 in contrast in this study considering different data scarce situations and to highlight the advance of few shot learning we roughly set the ratio of training data k to a range of 20 45 with a changing interval of 5 the simple rationale is that if k becomes larger the advance of the proposed model may become not obvious if k becomes smaller the training data may be too less to train the model well however since the patterns of rainfall runoff may differ among different basins and over different periods or due to the different sizes lengths of available data series among different basins k values may also differ in different cases accordingly we set k as 0 20 0 30 0 35 0 40 for the case study of the lancang river basin lrb and set k as 0 30 0 35 and 0 40 for the case study in the source region of the yellow river basin sryrb respectively 3 3 1 data pre processing to keep consistent time scale of input data and output data daily precipitation at six rain gauges are converted to monthly precipitation to reduce the sensitivity of neural networks on data scale all the data are normalized into a range of 0 1 by using the discrete standardization method as shown in equation 13 13 y i j n o r m a l y i j y min y max y min where y max and y min are the maximum and minimal values of the whole data respectively y i j means the value of the j t h time step of the i t h variables to predict runoff input features selection and construction are important following the sliding window scheme the normalized data with 7 dimensions in a sliding window t t t 1 and the runoff at time step t form a batch 3 3 2 training phase the step by step training process of the proposed fusion model is presented in table 1 the training phase can be divided into four parts namely labeling embedding loss calculation and optimization firstly as the window slides every sample y i j t r a i n in the same window where y i j t r a i n refers to the i th training time series data at the j th time step is marked a grade label by matching the sample s value with the classifier c which is a series intervals t defined in advance secondly according to the grade label the feature extractor receives every sample f t n in every window where f t n means the t th representation of the n th sliding window and embeds certain feature representations into a feature space thirdly acquire the feature prototype p l n for different grades at the same window by calculating the average of all the feature representations with the same grade label at the same window p l n refers to the feature prototype of the l th grade label at the n th window correspondingly p l n means the number of the samples of the l th grade label at the n th window for the n th sliding windows calculate its prototype loss p l o s s n and regression loss r l o s s n specifically the metric distances for prototype loss and regression loss are the euclidean distance and the mean square error respectively afterwards the sum of all the prototype loss and regression loss of the whole sliding windows is calculated finally update the feature extractor parameters using the above two losses by the adam optimizer noticeably the global loss s u m l o s s is the weighted sum of the regression loss and the prototype loss where α β are their weight coefficients respectively 3 4 evaluation metrics to evaluate the performance of the proposed few shot regression model and the comparisons among different models certain evaluation indicators are necessary according to the literature published by bennett et al 2013 in this study the nash sutcliffe model efficiency coefficient nse mean absolute error mae root mean square error rmse relative volume error re akaike information criterion aic and bayesian information criterion bic are selected as the evaluation criteria shown as equations 14 19 14 n s e 1 i 1 n y i p r e y i 2 i 1 n y i y i 2 15 m a e i 1 n y i p r e y i n 16 r m s e i 1 n y i p r e y i 2 n 17 r e i 1 n y i p r e y i i 1 n y i 18 a i c 2 k 2 ln l 19 b i c k l n n 2 ln l where y i p r e and y i denote the predicted value and the observed value at time i respectively y i refers to the mean of the whole observed value n indicates the number of observations k represents the number of parameters of the model and l is the maximum of the likelihood function where we defined it as a normal distribution based on central limit theorem 4 results and discussion 4 1 trend analysis of the hydro meteorological variables in the lancang river basin the seasonal trends in the monthly precipitation and runoff series are firstly removed by using the difference method afterwards trend analysis of the hydro meteorological variables is conducted by using the mann kendall trend test method mann 1945 kendall 1955 fig 4 shows the difference series of monthly precipitation at six rain gauges and runoff at the yunjinghong hydrometric station during 1981 2010 in the lancang river basin fig 4 and trend test results both indicate that there are no obvious significant trends existed in the precipitation and runoff series the pearson correlation coefficients pccs between runoff at the yunjinghong station and precipitation at the six rain gauges over the period of 1981 2010 range from 0 67 to 0 71 and all of these correlation values are significant based on the p values 0 05 the pcc values imply that precipitation is a primary impact factor for runoff generation and that the six rain gauges have similar contributions to runoff at the yunjinghong station 4 2 runoff prediction results by using the proposed model in the lancang river basin based on runoff percentiles and expert knowledge we divide runoff in the lancang river basin into six grades with thresholds of 98 mm 121 mm 236 mm 466 mm and 674 mm respectively taking different proportions 20 45 of input data for training the corresponding performances of the proposed model on runoff prediction are summarized in table 2 briefly the proposed model performed well on runoff prediction even with limited data available with k increases from 0 2 to 0 45 nse of the proposed model improves from 0 802 to 0 832 and the error indicators mae rmse show steady downward tendencies comparing the results of different models under the same k values we can find that the proposed lstm prototypical network fusion model based on few shot learning has a lower data dependence and performs better on runoff prediction in data scarce region in addition aic and bic increase as the value of k decreases it means that when less data is available for model training the model will automatically build more complex rules to learn the patterns hidden in the data taking the best runoff prediction result i e k 0 45 nse 0 832 mae 0 04 rmse 0 07 re 0 01 aic 416 38 bic 383 91 as an example we display the predicted and the observed monthly runoff results at yunjinghong station during 1994 and 2010 as shown in fig 5 intuitively the predicted runoff by the proposed model fits the observed runoff well 4 3 comparison with the selected data driven models in the lancang river basin to further validate the advantage of the proposed lstm prototypical network fusion model we selected eight widely used data driven models lstm support vector regression svr ann autoregressive moving average arma random forest rf simple recurrent neural network simplernn gated recurrent neural network gru and bi directional long short term memory bilstm for comparison we exclude process based models because they often need substantial data that are unavailable in the data scarce regions table 3 shows the best performances of different models for runoff prediction in the lancang river basin based on nse mae rmse re aic and bic criteria with k ranging from 0 20 to 0 45 in general with the same k value the few shot learning based model is superior to all the other comparative models regarding most of the selected evaluation metrics especially for nse specifically the proposed model improves nse by 0 527 0 254 0 294 0 212 and 0 222 relative to the mean nse of the eight comparative data driven models when k is equal to 0 20 0 30 0 35 0 40 and 0 45 respectively comparing the few shot learning model with the lstm model under the same proportion of training data in terms of aic and bic we can find that the proposed few shot learning model balances the complexity and performance of lstm effectively as mentioned in section 3 2 we use the grid search xia et al 2022 method to optimize models parameters based on 10 of the data after the training data set it is worth noting that the lstm simplernn gru and bilstm have the same structure and hyper parameters with the feature extractor of the few shot learning based model comparison between the lstm and the proposed lstm prototypical network fusion model demonstrates that the few shot learning strategy can improve the performance of the lstm in a data scarce situation although svr ann arma and rf have different structures with the proposed model similarly we use the grid search method based on 10 of the data after the training data set to calibrate the models parameters 4 4 model validation in the source region of the yellow river basin sryrb in this case study the pearson correlation coefficients pccs between the precipitation at the eight rain gauges and runoff at the tanghaihai station over the period of 1970 1995 range from 0 69 to 0 75 all of which are significant with p value 0 05 based on runoff percentile runoff is divided into five grades in the sryrb considering the length of data series in the sryrb 1970 1995 26 years is shorter than that of the lrb 1981 2010 30 years we set the proportion of training data k relatively larger ranging from 30 35 40 10 of the data after the training data set are used for hyper parameter verification and the rest for validation table 4 lists the performance of the few shot learning model on runoff prediction in the sryrb during the validation period 1983 1995 as well as the comparison results with the other 8 data driven models the results indicate that the proposed model worked well and outperformed the rest models in terms of nse mae rmse aic and bic compared with the mean nse of the eight comparison models nse of the proposed model improved by 0 479 0 403 and 0 354 when k values are 0 30 0 35 and 0 40 respectively fig 6 displays the time series of the observed runoff along with the predicted runoff by using the proposed fusion model when k is 40 results in fig 6 indicate that the proposed model can predict monthly runoff well in most cases while peak runoff prediction still needs improvement 4 5 advantages limitations suggestions and future work in this study to improve the accuracy of runoff prediction in data scarce regions we proposed the lstm prototypical network fusion model from a new viewpoint of few shot learning one advantage of the proposed model is that it combines the advances of both lstm i e strong learning ability on time series data and prototypical network i e low dependence on data this advantage allows the proposed model to achieve good performance on runoff prediction with nse over 0 80 even with very limited training data accounting for 20 72 samples and outperform the eight widely used comparative data driven methods which often require for relatively large amount of data the second advantage is that following the graded forecast idea the proposed model relates classification task runoff grades and regression task runoff prediction to some extent via the prototype loss the combination of the two loss functions prototype loss and regression loss are designed and combined for objective optimization which can achieve both classification purpose in terms of feature space and regression purpose in terms of output space thirdly the proposed model requires few variables as input and is flexible to add more variables if necessary for instance only monthly runoff and precipitation are need as input data in the two case studies in comparison with process based hydrologic models it is flexible and easy to add more impact variables e g vegetation index into the proposed model if the related data are available however the proposed model also has its limitations for instance the proposed lstm prototypical network fusion model is based on data driven instead of physical mechanism so its interpretability to hydrologic processes is poor for example it is hard to explain the physical relationships among precipitation evaporation soil moisture and runoff by using the proposed model besides the hyper parameters such as the size of the sliding window the number of grades for runoff the thresholds for different grades of runoff the proportion of training data k of the proposed model are determined based on prior knowledge in this study however these parameters may vary among different basins and it is not easy to select appropriate values at once furthermore the structure of lstm network such as the numbers of layers and neurons is not fixed but depends on the size of the feature space and the complexity of the specific data patterns in total we tested the proposed model in five basins including two data scarce basins and three well gauged basins the results indicate that the proposed model performed well in the two data scarce basins the lancang river basin and the source region of the yellow river basin while failed in the other three basins lan river basin qingliu river basin xixian basin which are located in the middle or lower branches of the yellow river yangtze river and huai river respectively through the investigations we notice that the proposed model performed well in the cases where the input data rainfall and the output data runoff have high correlation coefficients e g above 0 6 if the correlation between input and output is poor the proposed model may probably fail actually we can understand the results in the following way for the lancang river basin and the source region of the yellow river basin rainfall is the major impact factor on runoff the proposed model can capture the rainfall runoff relationship even based on limited rain gauge data therefore the proposed model performed well in the two case studies however in the other three basins which are located at the middle or lower reaches of rivers intensive human activities exist and contribute to runoff change therefore the proposed model could not predict runoff well by only using the rainfall data in the other three basin so the lesson we learnt is that the proposed model is suitable for data scarce regions where precipitation dominates runoff change if not the performance of the proposed model is likely to be affected since the other important influencing factor e g land use change water withdraw of runoff are not explored by the model when the proposed model is applied to other suitable basins the following suggestions can be referenced by the users regarding the input data requirement such as the number of rain gauges the length of data series for model applications it is hard to give a specific value threshold since it depends on the rainfall runoff pattern of a specific basin besides although the proposed model reduces dependence on data it is still a data driven model and requires for the most basic information as input and it will still fail when the amount of data is too small hence what can be suggested to the users is using all the currently available data in the data scarce basin as much as possible at the very beginning of model application certain parameters of the model can be adjusted and optimized later after users learn more about the data moreover if the input data are redundant the model would selectively use the useful data but if we reduce the input data artificially we may reduce some data important for the model which may lead to poor results in addition we suggest divide the grades of runoff in a specific basin based on its runoff percentile considering the more grades classes may result in larger classification error we suggest set the number of grades in a range of 3 10 the proportion of training data k needs to be tuned and justified when the proposed model is applied in a potential basin and we recommend starting try from 20 to 45 most recent work in the field of runoff prediction in data scarce basins are more about regionalization methods althoff et al 2022 nogueira filho et al 2022 instead of data driven methods runoff prediction in ungauged basins of the brazilian cerrado biome althoff et al 2022 indicate that parameters donation by spatial proximity led to nse of 0 56 while by attributes proximity led to nse of 0 45 respectively the contrast between the regionalization models and the proposed model is not equal since they have different perspectives of solving the problem as mentioned above prior knowledge supporting to determine hyper parameters of a target region is very important to guide the model learning however prior knowledge is unavailable to enormous data scarce basins therefore in the future we attempt to improve the model to adaptively learn and determine the hyper parameters from the inputs furthermore daily or hourly runoff prediction model in data scarce basins will be explored in the future in addition integrating more factors affecting runoff into the model will help to expand the application scope of the model in data scarce regions the few shot learning based model can not only be used for runoff prediction in data scarce regions but also has the potential in wider environment related applications such as water quantity assessment in data scarce regions 5 conclusions in this study we proposed a new few shot learning model lstm prototypical network fusion model for runoff prediction in data scarce regions following the idea of graded forecast the proposed model achieved classification task in the feature space and regression task in the output space even with limited data to illustrate the performance of the proposed model we take monthly runoff prediction in the lancang river basin and the source region of the yellow river basin as case studies results in the lancang river basin indicate that the proposed model performed well with nse ranging from 0 802 to 0 832 when only 20 72 samples 45 162 samples of the data are used for training relative to the mean nse of the eight comparative data driven models the proposed model improves nse by 0 527 0 254 0 294 0 212 and 0 222 when k equals 0 20 0 30 0 35 0 40 and 0 45 respectively results in the sryrb show that nse of the proposed model reached 0 830 when training ratio was 40 0 354 higher than the mean nse of the comparison models compared with the eight state of the art comparative data driven models lstm svr ann arma rf simplernn gru and bilstm the proposed model demonstrated its superiority in terms of nse mae and rmse in both case studies the findings indicate that the proposed model provides a new promising tool for runoff prediction in the two investigated basins and possibly other data scarce basins where precipitation dominates runoff change which will benefit water resources management and sustainable utilization in these basins the proposed model needs evaluation and justification before applying in other data scarce basins author contributions q y and j s designed the research g w provided data support m y implemented the model and analysed the data q y and m y wrote the first manuscript draft j s g w and w z made valuable suggestions on discussion w z contributed to model comparisons all authors commented and revised the manuscript software and data availability name of software few shot learning based model for runoff prediction developers minghong yang qinli yang contact address no 2006 xiyuan avenue chengdu 611731 china email qinli yang uestc edu cn year first available 2021 required hardware and software the software works in python and cuda on windows based computers cost free software and data availability https github com ray yang521 few shot git runoff data are available upon request to the corresponding author declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been financially supported by the national key research and development program of china grant number 2021yfc3201100 the national natural science foundation of china grant numbers 52079026 41830863 and 61976044 sichuan science and technology program grant number 2020yfh0037 2022yfg0260 the belt and road fund on water and sustainability of the state key laboratory of hydrology water resources and hydraulic engineering grant number 2019nkzd02 the open research fund of state key laboratory of simulation and regulation of water cycle in river basin grant number iwhr skl 201911 and the fundamental research funds for the central universities grant number zygx2019z014 many thanks go to cobbinah m bernard who significantly contributed to the manuscript revision 
