index,text
7300,newly available more detailed and accurate elevation data sets such as digital elevation models dems generated on the basis of imagery from terrestrial lidar light detection and ranging systems or unmanned aerial vehicles uavs can be used to improve flood model input data and consequently increase the accuracy of the flood modelling results this paper presents the first application of the mblend merging method and assesses the impact of combining different dems on flood modelling results it was demonstrated that different raster merging methods can have different and substantial impacts on these results in addition to the influence associated with the method used to merge the original dems the magnitude of the impact also depends on i the systematic horizontal and vertical differences of the dems and ii the orientation between the dem boundary and the terrain slope the greater water depth and flow velocity differences between the flood modelling results obtained using the reference dem and the merged dems ranged from 9 845 to 0 002 m and from 0 003 to 0 024 m s 1 respectively these differences can have a significant impact on flood hazard estimates in most of the cases investigated in this study the differences from the reference dem results were smaller for the mblend method than for the results of the two conventional methods this study highlighted the importance of dem merging when conducting flood modelling and provided hints on the best dem merging methods to use keywords dem merging flood modelling uav lidar mblend 1 urban flood modelling and the need to combine different terrain elevation data sets floods constitute one of the greatest natural risks adikari and yoshitani 2009 note that they account for approximately 30 of the total losses caused by natural disasters flood models are an invaluable resource for better understanding these phenomena and reducing their frequency and impact these models can be used to understand the hydraulic behaviour of a catchment and support the design of solutions to mitigate floods feed flood forecasting systems so they can issue flood alerts as well as to provide the required information to generate the flood risk maps that are key tools for planning the territory and managing emergency responses the accuracy of the model results is strongly dependent on the quality e g accuracy and resolution of the input data for the specific case of flood models terrain elevation plays an important role as overland flow is driven by gravity in recent decades digital elevation models 1 in this study the generic term dem digital elevation model is used to refer to the representation of terrain surfaces with no man made features also realised by the digital terrain model or of the terrain and man made features also realised by the digital surface model 1 dems have become the preferential source of elevation data wilson and gallant 2000 baghdadi et al 2005 terrain surface features in urban areas such as roads curbs buildings and other man made features can significantly influence the pattern of overland flow and flooding prodanović et al 1994 mark et al 2004 for this reason these features need to be represented in dems to model such phenomena fewtrell et al 2008 and leitão et al 2009 showed the effect of dem sources resolution and accuracy on the delineation of overland flow paths in urban catchments and on urban flood modelling applications these authors concluded that for flood modelling in urban areas the spatial resolution of the terrain representation needs to be relatively high i e with a maximum raster cell size of 5 m and preferably around 1 m the need for high resolution dems for urban flood modelling explains the interest in exploring new technologies and methodologies to generate terrain elevation data sources that i produce cost effective and high resolution data for specific areas such as those more prone to flooding and ii are easy and flexible enough to allow frequent surveys to be conducted in order to capture the changes in the catchment unmanned aerial vehicles uavs and ground based lidar solutions are two examples of such technologies leitão et al 2016a uavs are becoming increasingly common and their application is broadening thanks to their low cost and simple operation which leads to cost effective and flexible surveys during different times of the day and calendar seasons however uav surveys also suffer some drawbacks such as their limited battery capacity that restricts the duration and maximum altitude of flights terrain elevation data sets generated from uav imagery may consequently cover only specific parts of the catchment in urban areas ground based lidar survey solutions face a similar problem they can provide high resolution and high accuracy elevation information along streets but cannot provide data from areas located behind buildings or walls in any study of urban flooding as well as in most applications for modelling earth surface phenomena data voids within the study area are not desirable or even possible as they would significantly affect the modelling results furthermore the use of low resolution data because high resolution data sets do not cover the whole area is also undesirable leitão et al 2016b the above considerations show that the available data sets must be combined 2 in the literature methods of combining different raster data sets also refer to fusion merging or mosaicking in this study the authors adopt the term merging for methods that combine multiple raster data sets e g dems 2 in order to produce one that covers the whole area of interest with the highest possible accuracy bourgine et al 2004 leitão et al 2016b nevertheless the process of merging elevation data sets from different sources with different characteristics is challenging and may be a source of elevation artefacts katzil and doytsher 2003 luedeling et al 2007 according to various authors constantini et al 2006 ravanbakhsh and fraser 2013 leitão et al 2016a b dems should be merged in a way that retains their highest possible accuracy while ensuring smooth transitions between the merged data sets however most of the conventional dem merging methods available in gis software e g arcgis 3 arcgis is a widely used commercial gis software http www arcgis com3 3 or qgis 4 qgis is a widely used free and open source gis software http www qgis org4 4 ignore these recommendations and i change the most accurate data set in an attempt to achieve a smooth transition or ii do not merge the dems seamlessly creating artefacts in the representation of terrain which in turn may cause errors in the simulations of the relevant phenomena the most common options for merging dems in gis software are i the cover method which consists of a simple overlapping of the raster data sets and ii the averaging methods that consist in calculating the elevation mean simple or weighted from all dems on a cell by cell basis a few dem merging methods have been proposed in order to resolve the problems mentioned above when merging dems and also as a result of the newly available elevation data sources papasaika et al 2008 2009 schindler et al 2011 huafei et al 2012 and fuss et al 2016 have all presented methods that aim to take advantage of multiple data sets covering exactly the same area and thus increase the accuracy of the elevation representation in the merged dem however they do not resolve cases in which different dems overlap only partially without the same coverage in order to address this latter case other authors e g constantini et al 2006 warriner and mandlburger 2005 leitão et al 2016b and petrasova et al 2017 have proposed different dem merging methods that minimise the effects of the horizontal and vertical differences between the original dems hence removing the abrupt transitions between them while also taking advantage of the best and most accurate dem an example of this latter type of dem merging method the mblend method was proposed by leitão et al 2016a b it combines dems retaining the elevation values of the most accurate and higher resolution dem while achieving a smooth transition between the two dems along with two conventional dem merging methods commonly available in gis software the cover and the average methods available in the arcgis mosaicking tool the mblend method 5 the r mblend tool is freely available within the grass gis software https grass osgeo org grass72 manuals addons r mblend html5 5 is used in this study to evaluate the potential impact of dem merging on urban flood modelling results in addition the present study aims to demonstrate the problems created by conventional dem merging methods e g cover and average methods when used to conduct urban surface flow and flooding simulations and evaluate their performance and show the advantages of the mblend method for merging dems in urban surface flow and flooding simulations as its first application 2 methodology 2 1 study area and dems used in this study the current study involves three dem merging situations and three different merging methods which are used to generate dems subsequently used for flood modelling in order to analyse the impact of dem merging on flood hazard assessment a total of ten flood simulations were conducted nine of them using merged dems plus one using a reference dem the demuav the study is performed in part of a semi urban catchment located in switzerland 0 9 km2 in area the downstream part of the study area is relatively flat the dems used were selected from two original ones obtained using two different technologies airborne lidar and uav photogrammetry both these technologies produce high resolution dems and are according to leitão et al 2016b suitable for overland flow and flood modelling in urban areas 2 1 1 lidar dem the lidar dem used in this study was provided by the official cadastral service of the canton of lucerne switzerland it is presented in fig 1 a it has a spatial resolution of 0 5 0 5 m and a vertical accuracy of approximately 0 5 m it was last updated in july 2012 doe 2014 the minimum maximum average and standard deviation elevation values from the lidar dem are 434 3 602 1 485 9 and 46 0 m respectively 2 1 2 original uav dem the original uav dem demuav presented in fig 1b and used as the reference dem in the analysis was generated from aerial photos obtained in march 2014 using a fully autonomous fixed wing uav 6 ebee sensefly sa 6 this uav is electrically powered has a wingspan of 0 96 m and weighs approximately 0 7 kg including a payload of 0 15 kg it can cover around 0 1 km2 in approximately two hours which is important for the economic viability of uav remote sensing the photos were taken using a 16 mp compact digital canon ixus 127 hs camera and then processed to generate an orthophoto using the pix4d software package strecha et al 2011 the uav flight was conducted at 114 m above ground which enables a dem with approximately 0 035 m spatial resolution to be generated despite this maximum resolution the demuav was downsampled to match the horizontal resolution of the lidar dem 0 5 0 5 m the vertical accuracy of the demuav was estimated to be approximately 0 02 m the minimum maximum average and standard deviation elevation values from the demuav are 434 3 602 1 485 9 and 46 0 m respectively the elevation differences between the two original dems are presented in fig 1c as can be seen there is a small overall bias between the two dems as the elevation differences are not zero meters in most of the area this bias is an interesting feature for the current study as it allows different dem merging cases to be investigated as presented in the following two sections despite this bias elevation differences occur randomly over the study area 2 1 3 cropped uav dems based on the demuav three smaller uav dems were created by extracting parts of the original dem to investigate the effect of the relation between the contour lines and the boundary directions three situations were thus created fig 2 uava dem the boundary between the two dems is located along the contour lines and the high resolution dem is located upstream of the low resolution dem this dem covers 0 5 km2 55 of the study area uavb dem the boundary between the two dems is located along the contour lines and the high resolution dem is located downstream of the low resolution dem this dem covers 0 4 km2 45 of the study area and uavc dem the boundary between the two dems is perpendicular to the contour lines i e perpendicular to the direction of the maximum slope this dem covers 0 5 km2 55 of the study area 2 2 merged dems by considering the three possible situations represented by uava uavb and uavc dems and three dem merging methods cover average and mblend nine merged dems were created the three dem merging methods used were 1 the cover method available from the arcgis mosaic tool 2 the average method available from the arcgis mosaic tool and 3 the mblend method available from the grass raster toolbox the nine merged dems covering the whole study area are labelled as follows mdem a1 b1 and c1 dems were obtained using the cover merging method to merge the lidar dem with uava uavb and uavc dems respectively mdem a2 b2 and c2 dems were obtained using the average merging method to merge the lidar dem with uava uavb and uavc dems respectively and mdem a3 b3 and c3 dems were obtained using the mblend merging method to merge the lidar dem with uava uavb and uavc dems respectively one of the merged dems mdem a3 is presented in fig 3 a the elevation differences between mdem a3 and the reference dem demuav are presented in fig 3b an analysis of the elevation differences between the merged dems and demuav was conducted and is presented in table 1 the mean error me of the merged dems was considered to indicate the relative global bias whilst the standard deviation sd of the error indicated the relative global precision to assess the relative accuracy of the dems the mean absolute error mae and the root mean square error rmse were also calculated the values presented in table 1 show the better performance of the mblend method when comparing the elevation values of the merged dems and the demuav in all three cases mdem a mdem b and mdem c the merged dems obtained using the mblend method showed a smaller bias me and were more precise sd and more accurate mae rmse than those obtained by the other two methods these results are in agreement with those presented in previous studies using this method leitão et al 2016b de sousa and leitão in review they may indicate that the impact of dem merging is smaller when the dems are merged with the mblend method and that conventional dem merging methods may contribute to larger errors in the flood modelling results in order to determine this various simulations using the different merged dems were conducted and their results analysed in this study 2 3 performance assessment of the dem merging methods in terms of urban flood modelling and flood hazard estimation flood models are common tools used to assess flood hazards and risks their main outputs are time dependent water depths and velocities that can be combined to estimate the flood hazard using empirical functions such as those proposed by ramsbottom et al 2003 and smith and petley 2009 in this study the caddies flood model guidolin et al 2016 which is based on cellular automata was used to simulate urban flooding conditions and provide water depth and flow velocity results the rainfall event used to simulate the flooding conditions is a design rainfall event of one hour duration with a 10 year return period based on the empirical relation proposed by hörler and rhein 1962 and on the alternate blocks method txdot 1997 the maximum intensity of the event is around 120 mm h 1 fig 4 a in this study the flood hazard was divided into four classes as proposed by ramsbottom et al 2003 for flood hazard estimation along evacuation routes low medium high and extreme flood hazard fig 4b for each simulation time step in this case 120 s the hazard level on each raster cell was computed using the local simulated water depth and velocity flood hazard maps covering the entire study region were produced in this way for each simulation time step these time dependent hazard maps were produced for each of the merged dems and the original demuav 3 effects of dem merging on urban flood modelling results since dem merging methods promote terrain elevation changes to achieve smooth transitions between the original dems it is important to assess what impact these methods can have on the flood modelling results in order to do so this study looks into two relevant flood simulation results water depth and flow velocity and into the results of flood hazard estimation these results are presented and discussed in the following sections 3 1 simulated water depth to assess the differences in water depth between the results obtained using demuav and the merged dems the water depth at the end of the simulation and the peak water depth were analysed the me sd mae and rmse goodness of fit measures were calculated in a similar way to the comparison of the dems presented in section 3 2 they are shown in tables 2 and 3 furthermore maps of water depth differences were created for the water depth at the end of the simulation and for the peak water depth figs 5 and 6 as can be seen in most of the cases the water depth results obtained using the dem merged with the mblend method present smaller differences than the results obtained using the other dem merging methods this is clear for cases mdem a3 and mdem b3 whereas for case mdem c3 the differences are very similar to those obtained for case mdem c1 this is valid for both water depth comparisons see fig 7 analysis of the maps of water depth differences presented in figs 5 and 6 clearly shows the advantages of the mblend method for merging dems for all cases the water depth differences along the border of the two original dems are significantly smaller for the mblend cases mdem a3 mdem b3 and mdem c3 also noteworthy are the water depth differences caused by the average merging method which affect a larger area than those produced by the other two methods it can also be seen that merging case a is most problematic for the two conventional methods as it presents large water depth differences along the boundary between the two original dems this case is also where the mblend method shows a distinct and better performance when compared with the other methods the figures in table 2 also support this finding see table 4 it is also worth noting that the cover method performed better than the average method which is rather unexpected as the latter method aims to smooth the terrain elevation by removing the sharp elevation differences along the dem boundary in addition to the water depth analysis the calculated me and mae for the elevations of the dem merged with the average method were also greater supporting the weaker flood modelling results achieved using the dem merged with the average method the average dem merging method most likely creates artefacts in the merged dem which generate errors in the flow pattern these artefacts are not present in the dem merged with the cover method and are present to a lesser extent in the dem merged with the mblend method 3 2 simulated flow velocity the flow velocity is another result of flood modelling involved in flood hazard estimation a similar analysis to that performed for the water depth was conducted but in this case only the peak flow velocity was considered because at the end of the simulation the water is no longer moving so that the velocity is zero m s 1 everywhere the conclusions that can be drawn from this analysis are similar to those obtained from the water depth analysis the flow velocities observed with the dems merged using the mblend method are in most of the cases closer to those obtained using the demuav the results observed using the cover method especially in the mdemc case are slightly better than those obtained using the dem merged with the mblend method surprisingly the performance of the cover method was consistently better than that of the average method when analysing the velocity difference maps the differences are less pronounced than for the water depth differences map nevertheless for all three cases the differences between the results obtained using the dems merged with the cover and the mblend methods and those obtained using the demuav are very similar to each other and smaller than the results obtained with the average dem merging method it can also be seen that the area on which the differences occur is larger for this latter dem merging method even where a more accurate high resolution method is available 3 3 estimated flood hazard to further understand the impact of dem merging on flood hazard estimation the water depth and flow velocity simulation results were combined using the flood hazard function proposed by ramsbottom et al 2003 as described in section 2 3 this function comprises four classes however because the number of cells in the lower flood hazard class is much larger than that in the other three classes the results presented here focus only on the three highest flood hazard classes medium high and extreme flood hazard in order to avoid problems relating to the visualisation of the results the curves presented in fig 8 show how the number of cells in each flood hazard class evolve over time again the results obtained using the dems merged with the mblend method mdem a3 mdem b3 and mdem c3 dems are equal or better than those obtained using the mdems merged with the other two merging methods in the case of mdem a3 the better performance of mblend is clear when compared to the other dem merging methods mdem a1 and mdem a2 dems for the mdemb case the results are similar for the three dem merging methods it is noteworthy that in the medium flood hazard class plot the mblend curve follows the reference demuav curve more closely until approximately 2500 s mdem b3 case after which it gets closer to the two curves representing the other two dem merging methods the results for the mdemc case are also similar to each other and to the reference dem results like the results for the water depth and flow velocity presented in sections 4 1 and 4 2 the results obtained using the mdems obtained with the average method show the worst results this is especially visible for the medium and high flood hazard classes it suggests that when merging dems it may be better only to superimpose the dems rather than to simply average the elevation values of the two original dems fig 9 presents the number of cells in the different flood hazard classes for each of the three cases the negative and positive differences are presented in aggregated form as can be seen again in general the flood hazard estimations obtained with the dem merged using the mblend method show smaller differences from the reference demuav than those obtained using the other two merged dems the smaller differences achieved when using the mblend method are clearly visible in the mdema case where the boundary of the two original dems follows the contour lines in this case the downstream dem is slightly higher than the upstream dem creating an artificial step along the boundary where water accumulates therefore changing the flood hazard results particularly in this region this seems to be the most critical dem merging case and the one compared to which mblend presents an increased advantage again it can be seen that the number of cells in the different flood hazard classes is smaller for the cover method simulations than for the average method ones 4 conclusions the main objective of this study was to investigate the importance of dem merging when two or more dems need to be merged for flood modelling and flood hazard estimation to achieve this objective the water depth and flow velocity modelling results plus flood hazard estimates obtained using dems generated with i three dem merging methods and ii representing three different cases subject to the dem merging requirements were compared with those obtained using a reference dem the main conclusions of this study were dem merging methods generated elevation artefacts which propagate further to the flood modelling results and can visibly influence flood hazard estimations more advanced dem merging methods than those commonly available in gis software should consequently be considered to combine raster data sets the results obtained using the dems merged with the mblend method were similar to those obtained using the reference dem this shows the good performance of the mblend method in combining two dems based on this finding methods used to combine different elevation data sets should take the systematic horizontal and vertical inaccuracies between the original dems into account like the mblend method apart from this method and as described in the introduction a few other merging methods also consider the differences between the original dems and should be able to produce equivalent good results according to their authors the cover method performed better than the average method this unexpected result is most probably due to artefacts created by the averaging operation during the dem merging the calculated me and mae for the dem merged with the average method were larger which confirms the weaker flood modelling results achieved using the dem merged with this method these results indicate that urban flood modellers should avoid the average method when preparing inputs to their models it was demonstrated that in addition to the characteristics of the dems the relation between the dem slope and the original dems boundary can have a significant impact on the modelling results this was investigated in this study by creating three different dem merging cases the impacts on the modelling results are higher when the boundary between the merged dems runs parallel to the contour lines creating artefacts perpendicular to the main water flow paths in these cases mblend offers improvements compared with traditional methods in cases where the merging boundary runs perpendicular to the contour lines the impacts on the model results are considerably smaller and therefore the benefits of mblend less visible the findings presented in this paper suggest that with more elevation data sets available from newly available platforms e g uavs and high resolution satellites urban flood modellers will be more often faced with dem merging problems when attempting to increase the quality of the dems and consequently the quality of the flood modelling results the results presented in this study show that the operation of merging dems can affect the quality of the modelling results so that it needs to be conducted with particular care the present study may also be relevant to other fields studying other types of flow phenomena on the earth s surface such as volcanic lava flows in addition to the problem of combining dems for earth surface flow simulation other applications based on raster data that frequently benefit from combining data from different sources e g precipitation radar imagery may also face similar challenges it would be interesting to assess how raster merging methods would perform with different data sets and how these combined rasters can impact the quality of the final raster and the subsequent modelling results 
7300,newly available more detailed and accurate elevation data sets such as digital elevation models dems generated on the basis of imagery from terrestrial lidar light detection and ranging systems or unmanned aerial vehicles uavs can be used to improve flood model input data and consequently increase the accuracy of the flood modelling results this paper presents the first application of the mblend merging method and assesses the impact of combining different dems on flood modelling results it was demonstrated that different raster merging methods can have different and substantial impacts on these results in addition to the influence associated with the method used to merge the original dems the magnitude of the impact also depends on i the systematic horizontal and vertical differences of the dems and ii the orientation between the dem boundary and the terrain slope the greater water depth and flow velocity differences between the flood modelling results obtained using the reference dem and the merged dems ranged from 9 845 to 0 002 m and from 0 003 to 0 024 m s 1 respectively these differences can have a significant impact on flood hazard estimates in most of the cases investigated in this study the differences from the reference dem results were smaller for the mblend method than for the results of the two conventional methods this study highlighted the importance of dem merging when conducting flood modelling and provided hints on the best dem merging methods to use keywords dem merging flood modelling uav lidar mblend 1 urban flood modelling and the need to combine different terrain elevation data sets floods constitute one of the greatest natural risks adikari and yoshitani 2009 note that they account for approximately 30 of the total losses caused by natural disasters flood models are an invaluable resource for better understanding these phenomena and reducing their frequency and impact these models can be used to understand the hydraulic behaviour of a catchment and support the design of solutions to mitigate floods feed flood forecasting systems so they can issue flood alerts as well as to provide the required information to generate the flood risk maps that are key tools for planning the territory and managing emergency responses the accuracy of the model results is strongly dependent on the quality e g accuracy and resolution of the input data for the specific case of flood models terrain elevation plays an important role as overland flow is driven by gravity in recent decades digital elevation models 1 in this study the generic term dem digital elevation model is used to refer to the representation of terrain surfaces with no man made features also realised by the digital terrain model or of the terrain and man made features also realised by the digital surface model 1 dems have become the preferential source of elevation data wilson and gallant 2000 baghdadi et al 2005 terrain surface features in urban areas such as roads curbs buildings and other man made features can significantly influence the pattern of overland flow and flooding prodanović et al 1994 mark et al 2004 for this reason these features need to be represented in dems to model such phenomena fewtrell et al 2008 and leitão et al 2009 showed the effect of dem sources resolution and accuracy on the delineation of overland flow paths in urban catchments and on urban flood modelling applications these authors concluded that for flood modelling in urban areas the spatial resolution of the terrain representation needs to be relatively high i e with a maximum raster cell size of 5 m and preferably around 1 m the need for high resolution dems for urban flood modelling explains the interest in exploring new technologies and methodologies to generate terrain elevation data sources that i produce cost effective and high resolution data for specific areas such as those more prone to flooding and ii are easy and flexible enough to allow frequent surveys to be conducted in order to capture the changes in the catchment unmanned aerial vehicles uavs and ground based lidar solutions are two examples of such technologies leitão et al 2016a uavs are becoming increasingly common and their application is broadening thanks to their low cost and simple operation which leads to cost effective and flexible surveys during different times of the day and calendar seasons however uav surveys also suffer some drawbacks such as their limited battery capacity that restricts the duration and maximum altitude of flights terrain elevation data sets generated from uav imagery may consequently cover only specific parts of the catchment in urban areas ground based lidar survey solutions face a similar problem they can provide high resolution and high accuracy elevation information along streets but cannot provide data from areas located behind buildings or walls in any study of urban flooding as well as in most applications for modelling earth surface phenomena data voids within the study area are not desirable or even possible as they would significantly affect the modelling results furthermore the use of low resolution data because high resolution data sets do not cover the whole area is also undesirable leitão et al 2016b the above considerations show that the available data sets must be combined 2 in the literature methods of combining different raster data sets also refer to fusion merging or mosaicking in this study the authors adopt the term merging for methods that combine multiple raster data sets e g dems 2 in order to produce one that covers the whole area of interest with the highest possible accuracy bourgine et al 2004 leitão et al 2016b nevertheless the process of merging elevation data sets from different sources with different characteristics is challenging and may be a source of elevation artefacts katzil and doytsher 2003 luedeling et al 2007 according to various authors constantini et al 2006 ravanbakhsh and fraser 2013 leitão et al 2016a b dems should be merged in a way that retains their highest possible accuracy while ensuring smooth transitions between the merged data sets however most of the conventional dem merging methods available in gis software e g arcgis 3 arcgis is a widely used commercial gis software http www arcgis com3 3 or qgis 4 qgis is a widely used free and open source gis software http www qgis org4 4 ignore these recommendations and i change the most accurate data set in an attempt to achieve a smooth transition or ii do not merge the dems seamlessly creating artefacts in the representation of terrain which in turn may cause errors in the simulations of the relevant phenomena the most common options for merging dems in gis software are i the cover method which consists of a simple overlapping of the raster data sets and ii the averaging methods that consist in calculating the elevation mean simple or weighted from all dems on a cell by cell basis a few dem merging methods have been proposed in order to resolve the problems mentioned above when merging dems and also as a result of the newly available elevation data sources papasaika et al 2008 2009 schindler et al 2011 huafei et al 2012 and fuss et al 2016 have all presented methods that aim to take advantage of multiple data sets covering exactly the same area and thus increase the accuracy of the elevation representation in the merged dem however they do not resolve cases in which different dems overlap only partially without the same coverage in order to address this latter case other authors e g constantini et al 2006 warriner and mandlburger 2005 leitão et al 2016b and petrasova et al 2017 have proposed different dem merging methods that minimise the effects of the horizontal and vertical differences between the original dems hence removing the abrupt transitions between them while also taking advantage of the best and most accurate dem an example of this latter type of dem merging method the mblend method was proposed by leitão et al 2016a b it combines dems retaining the elevation values of the most accurate and higher resolution dem while achieving a smooth transition between the two dems along with two conventional dem merging methods commonly available in gis software the cover and the average methods available in the arcgis mosaicking tool the mblend method 5 the r mblend tool is freely available within the grass gis software https grass osgeo org grass72 manuals addons r mblend html5 5 is used in this study to evaluate the potential impact of dem merging on urban flood modelling results in addition the present study aims to demonstrate the problems created by conventional dem merging methods e g cover and average methods when used to conduct urban surface flow and flooding simulations and evaluate their performance and show the advantages of the mblend method for merging dems in urban surface flow and flooding simulations as its first application 2 methodology 2 1 study area and dems used in this study the current study involves three dem merging situations and three different merging methods which are used to generate dems subsequently used for flood modelling in order to analyse the impact of dem merging on flood hazard assessment a total of ten flood simulations were conducted nine of them using merged dems plus one using a reference dem the demuav the study is performed in part of a semi urban catchment located in switzerland 0 9 km2 in area the downstream part of the study area is relatively flat the dems used were selected from two original ones obtained using two different technologies airborne lidar and uav photogrammetry both these technologies produce high resolution dems and are according to leitão et al 2016b suitable for overland flow and flood modelling in urban areas 2 1 1 lidar dem the lidar dem used in this study was provided by the official cadastral service of the canton of lucerne switzerland it is presented in fig 1 a it has a spatial resolution of 0 5 0 5 m and a vertical accuracy of approximately 0 5 m it was last updated in july 2012 doe 2014 the minimum maximum average and standard deviation elevation values from the lidar dem are 434 3 602 1 485 9 and 46 0 m respectively 2 1 2 original uav dem the original uav dem demuav presented in fig 1b and used as the reference dem in the analysis was generated from aerial photos obtained in march 2014 using a fully autonomous fixed wing uav 6 ebee sensefly sa 6 this uav is electrically powered has a wingspan of 0 96 m and weighs approximately 0 7 kg including a payload of 0 15 kg it can cover around 0 1 km2 in approximately two hours which is important for the economic viability of uav remote sensing the photos were taken using a 16 mp compact digital canon ixus 127 hs camera and then processed to generate an orthophoto using the pix4d software package strecha et al 2011 the uav flight was conducted at 114 m above ground which enables a dem with approximately 0 035 m spatial resolution to be generated despite this maximum resolution the demuav was downsampled to match the horizontal resolution of the lidar dem 0 5 0 5 m the vertical accuracy of the demuav was estimated to be approximately 0 02 m the minimum maximum average and standard deviation elevation values from the demuav are 434 3 602 1 485 9 and 46 0 m respectively the elevation differences between the two original dems are presented in fig 1c as can be seen there is a small overall bias between the two dems as the elevation differences are not zero meters in most of the area this bias is an interesting feature for the current study as it allows different dem merging cases to be investigated as presented in the following two sections despite this bias elevation differences occur randomly over the study area 2 1 3 cropped uav dems based on the demuav three smaller uav dems were created by extracting parts of the original dem to investigate the effect of the relation between the contour lines and the boundary directions three situations were thus created fig 2 uava dem the boundary between the two dems is located along the contour lines and the high resolution dem is located upstream of the low resolution dem this dem covers 0 5 km2 55 of the study area uavb dem the boundary between the two dems is located along the contour lines and the high resolution dem is located downstream of the low resolution dem this dem covers 0 4 km2 45 of the study area and uavc dem the boundary between the two dems is perpendicular to the contour lines i e perpendicular to the direction of the maximum slope this dem covers 0 5 km2 55 of the study area 2 2 merged dems by considering the three possible situations represented by uava uavb and uavc dems and three dem merging methods cover average and mblend nine merged dems were created the three dem merging methods used were 1 the cover method available from the arcgis mosaic tool 2 the average method available from the arcgis mosaic tool and 3 the mblend method available from the grass raster toolbox the nine merged dems covering the whole study area are labelled as follows mdem a1 b1 and c1 dems were obtained using the cover merging method to merge the lidar dem with uava uavb and uavc dems respectively mdem a2 b2 and c2 dems were obtained using the average merging method to merge the lidar dem with uava uavb and uavc dems respectively and mdem a3 b3 and c3 dems were obtained using the mblend merging method to merge the lidar dem with uava uavb and uavc dems respectively one of the merged dems mdem a3 is presented in fig 3 a the elevation differences between mdem a3 and the reference dem demuav are presented in fig 3b an analysis of the elevation differences between the merged dems and demuav was conducted and is presented in table 1 the mean error me of the merged dems was considered to indicate the relative global bias whilst the standard deviation sd of the error indicated the relative global precision to assess the relative accuracy of the dems the mean absolute error mae and the root mean square error rmse were also calculated the values presented in table 1 show the better performance of the mblend method when comparing the elevation values of the merged dems and the demuav in all three cases mdem a mdem b and mdem c the merged dems obtained using the mblend method showed a smaller bias me and were more precise sd and more accurate mae rmse than those obtained by the other two methods these results are in agreement with those presented in previous studies using this method leitão et al 2016b de sousa and leitão in review they may indicate that the impact of dem merging is smaller when the dems are merged with the mblend method and that conventional dem merging methods may contribute to larger errors in the flood modelling results in order to determine this various simulations using the different merged dems were conducted and their results analysed in this study 2 3 performance assessment of the dem merging methods in terms of urban flood modelling and flood hazard estimation flood models are common tools used to assess flood hazards and risks their main outputs are time dependent water depths and velocities that can be combined to estimate the flood hazard using empirical functions such as those proposed by ramsbottom et al 2003 and smith and petley 2009 in this study the caddies flood model guidolin et al 2016 which is based on cellular automata was used to simulate urban flooding conditions and provide water depth and flow velocity results the rainfall event used to simulate the flooding conditions is a design rainfall event of one hour duration with a 10 year return period based on the empirical relation proposed by hörler and rhein 1962 and on the alternate blocks method txdot 1997 the maximum intensity of the event is around 120 mm h 1 fig 4 a in this study the flood hazard was divided into four classes as proposed by ramsbottom et al 2003 for flood hazard estimation along evacuation routes low medium high and extreme flood hazard fig 4b for each simulation time step in this case 120 s the hazard level on each raster cell was computed using the local simulated water depth and velocity flood hazard maps covering the entire study region were produced in this way for each simulation time step these time dependent hazard maps were produced for each of the merged dems and the original demuav 3 effects of dem merging on urban flood modelling results since dem merging methods promote terrain elevation changes to achieve smooth transitions between the original dems it is important to assess what impact these methods can have on the flood modelling results in order to do so this study looks into two relevant flood simulation results water depth and flow velocity and into the results of flood hazard estimation these results are presented and discussed in the following sections 3 1 simulated water depth to assess the differences in water depth between the results obtained using demuav and the merged dems the water depth at the end of the simulation and the peak water depth were analysed the me sd mae and rmse goodness of fit measures were calculated in a similar way to the comparison of the dems presented in section 3 2 they are shown in tables 2 and 3 furthermore maps of water depth differences were created for the water depth at the end of the simulation and for the peak water depth figs 5 and 6 as can be seen in most of the cases the water depth results obtained using the dem merged with the mblend method present smaller differences than the results obtained using the other dem merging methods this is clear for cases mdem a3 and mdem b3 whereas for case mdem c3 the differences are very similar to those obtained for case mdem c1 this is valid for both water depth comparisons see fig 7 analysis of the maps of water depth differences presented in figs 5 and 6 clearly shows the advantages of the mblend method for merging dems for all cases the water depth differences along the border of the two original dems are significantly smaller for the mblend cases mdem a3 mdem b3 and mdem c3 also noteworthy are the water depth differences caused by the average merging method which affect a larger area than those produced by the other two methods it can also be seen that merging case a is most problematic for the two conventional methods as it presents large water depth differences along the boundary between the two original dems this case is also where the mblend method shows a distinct and better performance when compared with the other methods the figures in table 2 also support this finding see table 4 it is also worth noting that the cover method performed better than the average method which is rather unexpected as the latter method aims to smooth the terrain elevation by removing the sharp elevation differences along the dem boundary in addition to the water depth analysis the calculated me and mae for the elevations of the dem merged with the average method were also greater supporting the weaker flood modelling results achieved using the dem merged with the average method the average dem merging method most likely creates artefacts in the merged dem which generate errors in the flow pattern these artefacts are not present in the dem merged with the cover method and are present to a lesser extent in the dem merged with the mblend method 3 2 simulated flow velocity the flow velocity is another result of flood modelling involved in flood hazard estimation a similar analysis to that performed for the water depth was conducted but in this case only the peak flow velocity was considered because at the end of the simulation the water is no longer moving so that the velocity is zero m s 1 everywhere the conclusions that can be drawn from this analysis are similar to those obtained from the water depth analysis the flow velocities observed with the dems merged using the mblend method are in most of the cases closer to those obtained using the demuav the results observed using the cover method especially in the mdemc case are slightly better than those obtained using the dem merged with the mblend method surprisingly the performance of the cover method was consistently better than that of the average method when analysing the velocity difference maps the differences are less pronounced than for the water depth differences map nevertheless for all three cases the differences between the results obtained using the dems merged with the cover and the mblend methods and those obtained using the demuav are very similar to each other and smaller than the results obtained with the average dem merging method it can also be seen that the area on which the differences occur is larger for this latter dem merging method even where a more accurate high resolution method is available 3 3 estimated flood hazard to further understand the impact of dem merging on flood hazard estimation the water depth and flow velocity simulation results were combined using the flood hazard function proposed by ramsbottom et al 2003 as described in section 2 3 this function comprises four classes however because the number of cells in the lower flood hazard class is much larger than that in the other three classes the results presented here focus only on the three highest flood hazard classes medium high and extreme flood hazard in order to avoid problems relating to the visualisation of the results the curves presented in fig 8 show how the number of cells in each flood hazard class evolve over time again the results obtained using the dems merged with the mblend method mdem a3 mdem b3 and mdem c3 dems are equal or better than those obtained using the mdems merged with the other two merging methods in the case of mdem a3 the better performance of mblend is clear when compared to the other dem merging methods mdem a1 and mdem a2 dems for the mdemb case the results are similar for the three dem merging methods it is noteworthy that in the medium flood hazard class plot the mblend curve follows the reference demuav curve more closely until approximately 2500 s mdem b3 case after which it gets closer to the two curves representing the other two dem merging methods the results for the mdemc case are also similar to each other and to the reference dem results like the results for the water depth and flow velocity presented in sections 4 1 and 4 2 the results obtained using the mdems obtained with the average method show the worst results this is especially visible for the medium and high flood hazard classes it suggests that when merging dems it may be better only to superimpose the dems rather than to simply average the elevation values of the two original dems fig 9 presents the number of cells in the different flood hazard classes for each of the three cases the negative and positive differences are presented in aggregated form as can be seen again in general the flood hazard estimations obtained with the dem merged using the mblend method show smaller differences from the reference demuav than those obtained using the other two merged dems the smaller differences achieved when using the mblend method are clearly visible in the mdema case where the boundary of the two original dems follows the contour lines in this case the downstream dem is slightly higher than the upstream dem creating an artificial step along the boundary where water accumulates therefore changing the flood hazard results particularly in this region this seems to be the most critical dem merging case and the one compared to which mblend presents an increased advantage again it can be seen that the number of cells in the different flood hazard classes is smaller for the cover method simulations than for the average method ones 4 conclusions the main objective of this study was to investigate the importance of dem merging when two or more dems need to be merged for flood modelling and flood hazard estimation to achieve this objective the water depth and flow velocity modelling results plus flood hazard estimates obtained using dems generated with i three dem merging methods and ii representing three different cases subject to the dem merging requirements were compared with those obtained using a reference dem the main conclusions of this study were dem merging methods generated elevation artefacts which propagate further to the flood modelling results and can visibly influence flood hazard estimations more advanced dem merging methods than those commonly available in gis software should consequently be considered to combine raster data sets the results obtained using the dems merged with the mblend method were similar to those obtained using the reference dem this shows the good performance of the mblend method in combining two dems based on this finding methods used to combine different elevation data sets should take the systematic horizontal and vertical inaccuracies between the original dems into account like the mblend method apart from this method and as described in the introduction a few other merging methods also consider the differences between the original dems and should be able to produce equivalent good results according to their authors the cover method performed better than the average method this unexpected result is most probably due to artefacts created by the averaging operation during the dem merging the calculated me and mae for the dem merged with the average method were larger which confirms the weaker flood modelling results achieved using the dem merged with this method these results indicate that urban flood modellers should avoid the average method when preparing inputs to their models it was demonstrated that in addition to the characteristics of the dems the relation between the dem slope and the original dems boundary can have a significant impact on the modelling results this was investigated in this study by creating three different dem merging cases the impacts on the modelling results are higher when the boundary between the merged dems runs parallel to the contour lines creating artefacts perpendicular to the main water flow paths in these cases mblend offers improvements compared with traditional methods in cases where the merging boundary runs perpendicular to the contour lines the impacts on the model results are considerably smaller and therefore the benefits of mblend less visible the findings presented in this paper suggest that with more elevation data sets available from newly available platforms e g uavs and high resolution satellites urban flood modellers will be more often faced with dem merging problems when attempting to increase the quality of the dems and consequently the quality of the flood modelling results the results presented in this study show that the operation of merging dems can affect the quality of the modelling results so that it needs to be conducted with particular care the present study may also be relevant to other fields studying other types of flow phenomena on the earth s surface such as volcanic lava flows in addition to the problem of combining dems for earth surface flow simulation other applications based on raster data that frequently benefit from combining data from different sources e g precipitation radar imagery may also face similar challenges it would be interesting to assess how raster merging methods would perform with different data sets and how these combined rasters can impact the quality of the final raster and the subsequent modelling results 
7301,accurate models of soil moisture are vital for solving core problems in meteorology hydrology agriculture and ecology the capacity for soil moisture modelling is growing rapidly with the development of high resolution continent scale gridded weather and soil data together with advances in modelling methods in particular the globalsoilmap net initiative represents next generation depth specific gridded soil products that may substantially increase soil moisture modelling capacity here we present an implementation of campbell s infiltration and redistribution model within the nichemapr microclimate modelling package for the r environment and use it to assess the predictive power provided by the globalsoilmap net product soil and landscape grid of australia slga 100 m as well as the coarser resolution global product soilgrids sg 250 m predictions were tested in detail against 3 years of root zone 3 75 cm soil moisture observation data from 35 monitoring sites within the oznet project in australia with additional tests of the finalised modelling approach against cosmic ray neutron cosmoz 0 50 cm 9 sites from 2011 to 2017 and satellite ascat 0 2 cm continent wide from 2007 to 2009 observations the model was forced by daily 0 05 5 km gridded meteorological data the nichemapr system predicted soil moisture to within experimental error for all data sets using the slga or the sg soil database the oznet soil moisture could be predicted with a root mean square error rmse of 0 075 m3 m 3 and a correlation coefficient r of 0 65 consistently through the soil profile without any parameter tuning soil moisture predictions based on the slga and sg datasets were 17 closer to the observations than when using a chloropleth derived soil data set digital atlas of australian soils with the greatest improvements occurring for deeper layers the cosmoz observations were predicted with similar accuracy r 0 76 and rmse of 0 085 m3 m 3 comparisons at the continental scale to 0 2 cm satellite data ascat showed that the slga sg datasets increased model fit over simulations using the daas soil properties r 0 63 rmse 15 vs r 0 48 rmse 18 respectively overall our results demonstrate the advantages of using globalsoilmap net products in combination with gridded weather data for modelling soil moisture at fine spatial and temporal resolution at the continental scale keywords microclimate model soil moisture land surface model soil hydraulic properties ascat 1 introduction soil moisture is a fundamental environmental variable for physical and biological processes in terrestrial systems water exchange between the soil and the atmosphere couples energy and mass fluxes and has a strong influence on weather forecasts and catchment hydrodynamics especially through memory effects e g jung et al 2010 koster et al 2000 it also drives biological productivity and ecosystem structure by constraining plant fungal and microbial growth rodriguez iturbe et al 1999 and can be a potent constraint on the distribution and abundance of animals with one or more life stages in the soil e g bodensteiner et al 2015 bradford and seymour 1988 packard et al 1987 for all of these reasons the capacity to accurately predict root zone 1 m deep soil moisture is a coveted goal in environmental science a variety of land surface models have been developed to compute soil moisture as a function of meteorological input variables and soil and vegetation parameters e g albergel et al 2012 best et al 2011 kowalczyk et al 2006 raupach et al 2009 viterbo and beljaars 1995 yang et al 2014 however their predictive capacity depends strongly on the availability of spatially explicit soil properties which are limited in spatial resolution and accuracy bandara et al 2011 holgate et al 2016 richter et al 2004 reviewed in table 1 for example holgate et al 2016 assessed the performance of four modelling approaches against sub hourly soil moisture observations in australia and found limited capacity to capture short term dynamics of the sort required in many biological applications richter et al 2004 found that a chloropleth derived continental soil database provide no improvement to soil moisture predictions over the use of spatially fixed soil hydraulic parameters and emphasised the need for depth varying soil properties remote sensing technologies for measuring soil moisture from space are creating new opportunities to characterise and model soil moisture holgate et al 2016 panciera et al 2014 renzullo et al 2014 wang and qu 2009 although they can be limited in coverage both temporally snapshots from aircraft or regular satellite passes every few days and spatially 5 cm depth 1 km2 for passive microwave satellite observations work is underway to estimate soil properties from remotely sensed soil moisture bandara et al 2013 bandara et al 2014 bandara et al 2015 this is a promising avenue but is inherently limited when soil properties vary strongly with depth the globalsoilmap net project hartemink et al 2010 aims to develop fine scale grids of soil properties at six different depths for the globe and such data sets have the capacity to substantially improve the predictive ability of soil moisture models the first continental output from this project is the soil and landscape grid of australia grundy et al 2015 viscarra rossel et al 2015 the slga data set includes estimates of the percentage of clay sand and silt as well as bulk density at six depths 0 5 cm 5 15 cm 15 30 cm 30 60 cm 60 100 cm and 100 200 cm at a resolution of 100 m 3 arc seconds for the entire continent a related new product is the soilgrids sg global soil dataset hengl et al 2017 hengl et al 2014 which includes the same data at 250 m resolution at the same depths unlike sg and other modern gridded data sets of soil properties that rely only on remote sensing based soil covariates the slga integrates historical soil information new measurement with spectroscopic sensors and novel spatial modelling grundy et al 2015 here we assess the capacity of these next generation soil data sets for modelling soil moisture in conjunction with a recently released microclimate model kearney and porter 2017 for the r programming environment r development core team 2012 this model is part of the nichemapr package for modelling energy and mass budgets of organisms there is currently no soil moisture model available within the increasingly popular r computing environment and thus our model fills a niche for an accessible and general microclimate modelling framework that includes soil moisture calculations the nichemapr microclimate model was originally developed to predict desert microclimates for estimating animal heat budgets beckman et al 1973 mitchell et al 1975 porter et al 1973 with a capacity to model soil temperature but not soil moisture kearney et al 2014b recently tested the same model for its capacity to predict hourly soil temperature across the usa when driven by a monthly global climatology kearney et al 2014a and in australia when driven by 0 05 5 km resolution daily gridded meteorological data jones et al 2009 and pre calculated soil moisture from the awap raupach et al 2009 here we modify the model to incorporate campbell s 1985 infiltration and redistribution algorithm and assess its performance at modelling both in situ and satellite derived soil moisture observations our study has the following parallel objectives i to test the general ability of the nichemapr microclimate model when integrated with campbell s infiltration and redistribution algorithm to predict soil moisture from continent scale forcing data and ii to assess whether the use of next generation soil properties databases slga sg can improve soil moisture predictions over early generation chloropleth maps daas 2 theoretical background a variety of approaches have been taken to modelling soil moisture that range from purely descriptive and phenomenological indices such as the keetch byram drought index kbdi keetch and byram 1968 to more mechanistic approaches such as that of campbell 1985 see mendoza et al 2015 for an overview we chose to implement program 11 1 henceforth c11 1 of campbell 1985 for simulating soil moisture water potential and humidity gradients in the presence of vegetation we chose this algorithm because it is capable of depth specific predictions and because the code was straight forward to integrate into the existing nichemapr microclimate modelling framework for computing soil temperature the c11 1 model takes the approach of linearising the differential equation for flow in space depth and using a newton raphson procedure to solve the non linear equations through time the linearising through space involves representing the soil as a series of capacitors and resistors see campbell 1985 fig 8 5 it uses matric potential as the dependent variable rather than matric flux potential with the consequence that it is most computationally efficient when soil is relatively dry the model is well described in campbell 1985 and in supplementary material s1 we provide a detailed summary of the model as implemented in the present study briefly the nichemapr microclimate model uses 10 user specified depth nodes for computing the heat budget we added code to c11 1 to insert extra nodes half way between the original 10 nodes such that a total of 19 nodes were used for soil moisture calculations the user input variables specific to c11 1 include for each of these 19 nodes the campbell aka clapp and hornberger exponent b the air entry potential ψ e j kg the saturated hydraulic conductivity k s kg s m 3 the bulk density ρ b kg m 3 and the root density m m 3 in addition to the leaf area index lai already mentioned we obtained estimates of these user input variables from the soil databases or by inverse fitting to the data as explained further in 3 3 and 3 4 all other model parameters for c11 1 were fixed at those suggested by campbell 1985 as described in s1 however the package allows user control over the resistance per unit length of root and leaf m3 kg 1 s 1 the critical leaf water potential for stomatal closure j kg 1 the stability parameter for the stomatal closure equation the maximum allowable mass balance error kg and the maximum number of iterations for mass balance 3 study sites and materials 3 1 observational data we used 35 of the oznet soil moisture monitoring sites smith et al 2012 http www oznet org au as our main empirical data with which to test our model predictions fig 1 for pictures of each site see supplementary file s2 the sites are within the murrumbidgee catchment of the state of new south wales and are monitored sub hourly the 100 000 km2 region ranges in elevation as defined in mcvicar and körner 2013 from 50 to 1900 m and experiences annual rainfall ranging from 330 mm in the arid west to 1900 mm in the more humid higher elevation east following bandara et al 2014 we focused on 2008 2010 which include two dry years and a wet year 2010 we also assessed our predictions against the cosmoz cosmoz csiro au hawdon et al 2014 data set for the 9 locations across australia considered by hawdon et al 2014 fig 1 which comprises hourly cosmic ray neutron data at between 5 cm 30 cm depth for various time periods between 2011 and 2016 finally we compared our predictions to the ascat 0 2 cm soil moisture product wagner et al 1999 for 2007 from the 0 05 resampled product produced by the ecosystem modelling and scaling infrastructure emast http www emast org au of the terrestrial ecosystem research network tern http www tern org au accessed 8th june 2017 3 2 forcing meteorological variables the model was driven by the awap 0 05 5 km resolution daily gridded meteorological data jones et al 2009 of minimum and maximum air temperature mean vapour pressure from the 9 am and 3 pm grids total daily solar used to derive cloud cover and total rainfall as described in detail by kearney et al 2014b in addition we used a daily gridded mean wind speed product mcvicar et al 2008 assuming maximum wind speed was double and minimum wind speed was half the mean value as described in more detail in kearney and porter 2017 hourly interpolations of air temperature wind speed relative humidity and cloud cover were made by specifying the timing of maxima and minima relative to dawn minimum air temperature and wind speed maximum humidity and maximum cloud cover all occurring at dawn in our simulations or solar noon maximum air temperature and wind speed minimum humidity and cloud cover all occurring one hour after solar noon see also ephrath et al 1996 for humidity wind speed and cloud cover hourly interpolations were based on linear functions between the solar noon value a minimum or maximum depending on the variable a midnight value computed as the mean of the maximum and minimum value and the dawn value as for the noon value for air temperature a more sophisticated algorithm was used where a linear change is assumed from midnight to dawn a sine wave from dawn to sunset and an exponential decay from sunset to midnight supplementary material appendix 1 section 4 in kearney and porter 2017 see also mcvicar 1999 parton and logan 1981 for the oznet comparisons we explored the performance of the model when run using three different rainfall measures we first considered the in situ sub hourly aggregated to hourly rainfall measurements provided with the soil moisture observations we then considered the awap gridded rainfall estimates these latter interpolated estimates are available at the continental scale for australia but are only provided as daily totals and we assumed that this all fell in the first hour of the day in our standard simulations finally we applied the rainfall disaggregation function disagsimul of the hyetosminute r package http www itia ntua gr en softinfo 3 which uses the bartlett lewis rectangular pulse model to disaggregate rainfall from daily to hourly rodriguez iturbe et al 1987 we used all the default settings of the disaggregation function 3 3 terrain and vegetation variables flat ground was assumed and site elevation was derived from a 0 0025 250 m digital elevation model air temperatures were corrected to this elevation from the original 0 05 values with an adiabatic lapse rate of 0 0039 c m 1 and 0 0077 c m 1 for minimum and maximum air temperature respectively ruddell et al 1990 surface albedo was derived from a 0 25 satellite product avhrr average over 1994 1999 resampled to 0 05 briggs 2011 we obtained satellite derived lai observations from the 8 day composite 1 km resolution modis lpdaac mod15a2 mosaic data set paget and king 2008 values were extracted for each location and splined to a daily time step using the native spline function in the statistical package r r development core team 2012 monthly emissivity for longwave radiation exchange was extracted from a 0 05 global emissivity database seemann et al 2008 and splined to daily values 3 4 soil properties previous tests of soil moisture models against the oznet data set used the digital atlas of australian soils daas richter et al 2004 or derived properties from remote sensed or actual measurements bandara et al 2014 bandara et al 2015 here we explore the potential for deriving soil moisture parameters from the new slga and sg datasets and compare it to simulations using the earlier daas we extracted the bulk density and the proportion of sand silt and clay for each site from the sgla and sg for all depths 0 5 cm 5 15 cm 15 30 cm 30 60 cm 60 100 cm and 100 200 cm note the maximum depth simulated was always 200 cm and we assumed that the potential influence of bedrock was captured by the bulk density measures we used r s spline function method natural to interpolate the extracted soil properties to the specific depth arrays chosen by the user for the model we then used the empirical functions of cosby et al 1984 their table 5 to compute the air entry water potential ψ e the campbell exponent b and the saturated hydraulic conductivity k s as an alternative means of obtaining soil hydraulic properties we looked up the values in campbell and norman s 1998 table 9 1 by converting the slga soil sand silt and clay quantities to the soil texture classes of the same table to assess the advantage of using the slga and sg databases over the daas we tested predictions based on bulk density estimates for the two profiles considered by the daas and used the texture type for each daas layer categories of sand sandy loam loam clay loam light clay and clay table 2 of mckenzie et al 2000 to look up soil properties from table 9 1 of campbell and norman 1998 for computing thermal soil properties we assumed that mineral fraction had a thermal conductivity of 2 5 w m 1 c 1 a density of 2560 kg m 3 and a specific heat of 870 j kg 1 k 1 table 8 2 in campbell and norman 1998 4 methods 4 1 in situ point measurement oznet comparisons we contrasted the accuracy of model predictions relative to the oznet observation data under the different treatments of rainfall described in section 3 2 observed disaggregated daily and daily and the different pedotransfer functions soil data sets slga and the digital atlas of australian soils described in section 3 4 to see how closely the model could fit the data when tuned to specific sites we used a particle swarm optimiser pso to fit model parameters to the observed data using just the first year of the 2008 2010 period we use the r package hydropso zambrano bigiarini and rojas 2013 to fit six adjustment factors a multiplier of the leaf area index lai range 0 2 a divisor for the root density profile rdiv range 0 75 10 a factor by which the bulk density of the top 8 cm of the soil profile was multiplied to simulate soil compaction scomp range 1 2 and factors by which the proportion of clay to sand of the top layer 2 5 cm δclay a middle layer 7 5 22 5 cm δclay b and the bottom layer 45 200 cm δclay c of the soil profile were adjusted range 1 to 1 the latter three parameters altered the prediction of soil composition from the slga while the former three parameters reflect more idiosyncratic local processes due to land management cropping clearing and soil compaction through livestock or machinery a goodness of fit function was supplied to the pso which computed the rmse for only the top three of the four soil layers as a compromise to achieve close fits to near surface conditions while maintaining consistency with deeper soil conditions all default settings were used for the hydropsos function and following bandara et al 2013 we used 30 particles but limited the maximum number of iterations to 50 because of the computationally intensive nature of the procedure 2 h per site with one 2 year simulation taking approximately 15 s on a single core of an intel 2 8 ghz i7 4900mq cpu we contrasted different modelling strategies using paired t tests between the values for r and rmse for all 35 sites at each of the four depths the summary statistics and paired t tests for soil moisture excluded the following site depth observations due to apparent measurement malfunction or aberrantly high moisture e g due to local irrigation rodger young pers comm k12 75 cm k12 45 cm a4 45 cm y3 4 cm similarly for soil temperature we excluded a1 4 cm a4 45 cm k12 3 cm k13 3 cm k1 4 cm k1 75 cm k2 4 cm k5 4 cm k7 3 cm m6 4 cm for the latter set all but one involved implausibly low temperature fluctuations in the top 5 cm likely due to shading from vegetation see discussion as well as two cases including one of the near surface measurements of clear sensor malfunction the summary statistics and plots for these excluded layers and all other layers can be found in supplementary documents s4 s10 we simulated 2007 as a spin up year one year of simulations on an hourly time step was always sufficient for soil moisture profiles to stabilise the oznet soil moisture observations are integrated measurements across different depths thus we chose a depth profile for our simulations that included the mid point of the observations for a given site and computed the root mean square error rmse and correlation coefficient r as measures of fit we similarly compared the model s soil temperature predictions with the oznet soil temperature observations where they were available 4 2 in situ cosmic ray neutron cosmoz comparisons for the cosmoz data we ran the microclimate model using the daily awap rainfall only but varied the soil database between the slga sg and daas the cosmoz sites vary in the temporal data extent and we simulated the maximum duration available including one year prior as a spin up year we compared our predictions to the 7 h average of current hour plus three hours before and after including rainfall events values as the hourly data are inherently noisy ca 0 01 m3 m 3 we made comparisons between our predictions at 2 5 5 10 and 20 cm as well as the time varying closest depth of all those simulated i e also including 30 60 100 200 cm to the estimated depth provided in the cosmoz data set 4 3 satellite ers scatterometer ascat comparisons for the satellite data comparison we ran the microclimate model based on daily awap rainfall across a regular sample grid of 1559 sites across australia querying the nearest available spatial data for the years 1990 to 2009 we obtained the average volumetric soil moisture across 0 1 and 2 cm and converted it to relative soil moisture based on the topmost bulk density layer for the relevant soil properties database slga sg or daas these values were compared to the nearest available ascat grid points across the continent for each day of the year for the year 2007 5 results 5 1 in situ point measurement oznet comparisons overall our model evaluation strategy for the oznet data resulted in four different kinds of comparison summarised in table 2 three of these comparisons assessed the performance of the nichemapr model in predicting soil moisture and soil temperature under different parameter and variable settings an additional set of simulations compared the nichemapr soil moisture predictions to those of the australian water availability project under the different parameter and variable settings we summarise the outcome of each test here and in table 2 and provide representative example time series plots fig 2 with more detailed summaries provided in the supplementary documents s3 all tests and s4 s10 all site and depth specific time series plots 5 1 1 pedotransfer functions the pedotransfer function of cosby et al 1984 produced significantly better fits for soil moisture than did the look up table approach based on campbell and norman s table 9 1 campbell and norman 1998 in terms of rmse at the deeper three levels but there were no significant differences for r tables 1 and 2 in s2 overall the rmse improved by 0 012 m3 m 3 when using the cosby pedotransfer function compared to the campbell and norman values 0 072 vs 0 084 m3 m 3 with r in both cases being 0 65 at the shallowest level the campbell and norman parameters gave slightly better but statistically non significant rmse for soil temperature there were no significant differences between the pedotransfer functions though the cosby et al function gave a slightly lower rmse tables 3 and 4 in s3 thus the cosby pedotransfer function was used for subsequent tests 5 1 2 treatment of rainfall there were no statistically significant differences in either r or rmse for soil moisture when using the empirical oznet hourly rainfall or the awap daily rainfall disaggregated to hourly tables 5 and 6 in s3 for soil temperature the oznet data significantly improved rmse at 45 and 75 cm though the differences were very minor 0 1 c rmse tables 7 and 8 in s3 when comparing predictions using disaggregated awap data compared to the daily aggregated awap data disaggregation improved r at 4 cm and 15 cm and improved rmse at 45 cm and 75 cm by 0 007 m3 m 3 tables 9 and 10 in s3 however aggregated awap gave better rmse at 4 cm an improvement of 0 009 m3 m 3 for soil temperature disaggregation improved rmse at 4 cm but decreased it at 15 and 45 cm and improved r at 4 and 45 cm but again the differences were only slight 0 15 c rmse tables 11 and 12 in s3 due to the minor nature of the advantages in disaggregating rainfall the unaltered awap daily rainfall was used for subsequent comparisons because it is less computationally intensive 5 1 3 soil properties use of the slga soil data set compared with the daas significantly improved r at 15 cm and rmse at all but the shallowest depth tables 13 and 14 in s3 the sg data set produced similarly strong correlations to the slga but the slga still improved the rmse at all depths but the shallowest table 17 and 18 in s3 the overall improvement in rmse when using the slga data set was 0 016 and 0 005 m3 m 3 over the daas and soilgrids respectively soil temperature predictions were also improved under slga when compared to daas tables 15 and 16 in s3 and sg tables 19 and 20 in s3 at 4 cm while being significantly worse at 15 cm but the differences were very slight 0 1 c 5 1 4 fitted parameters the pso for the slga scenario did not significantly improve r but did improve rmse for the top three depths with an overall improvement of 0 017 m3 m 3 and the greatest improvement 0 02 m3 m 3 at 4 and 45 cm tables 21 and 22 in s3 soil temperature predictions were only improved at 15 cm where r was slightly higher tables 23 and 24 in s3 there were no clear patterns in the way the parameters were adjusted by the pso except that the leaf area index and root density values were always increased from the initial estimates fig 1 in s3 also clay content was decreased in the top layer in 26 of the 35 sites and was increased in the bottom layer in 28 of the sites 5 1 5 nichemapr vs waterdyn contrasts the waterdyn model of the australian water availability project consistently showed a slightly stronger correlation coefficient than the nichemapr model 0 69 waterdyn vs 0 61 0 66 nichemapr tables 21 and 32 in s3 the rmse of the nichemapr model was consistently lower than the waterdyn predictions for all but the shallowest depth with strongly statistically significant differences in the order of 0 05 0 10 m3 m 3 under most scenarios tables 25 and 36 in s3 however for all comparisons except the fitted parameter nichemapr simulations the waterdyn predictions had lower rmse than the nichemapr predictions for the top 5 cm by an amount in the order of 0 01 0 02 m3 m 3 tables 25 and 36 in s3 5 2 in situ cosmic ray neutron cosmoz comparisons the nichemapr model predicted the cosmoz soil moisture at the stated time varying mean measurement depth with a mean r of 0 757 and an rmse of 0 082 m3 m 3 when using the slga table 38 in s3 this stated depth was estimated to be around 20 cm on average but the fixed depth predictions most closely matching the observations were shallower than this 5 or 10 cm for all but one site table 37 in s3 with r at these depths 0 77 and rmse 0 078 m3 m 3 table s38 overall the slga based predictions when compared to the daas based predictions gave higher r 0 76 vs 0 74 and lower rmse 0 075 vs 0 084 m3 m 3 table 2 in the main text and table 38 in s3 though these differences were only marginal from a statistical point of view table 35 in s9 there were no statistically significant differences between the model predictions when using the sg vs the slga soil data tables 40 and 41 in s3 5 3 satellite ers scatterometer ascat comparisons comparison of the 2007 ascat satellite data 0 2 cm soil moisture with that predicted by nichemapr using the slga soil dataset produced an r of 0 63 and an rmse of 15 relative soil moisture table 2 in the main text table 42 in s3 this was very similar to the results when using the sg soil dataset and consistently better than the equivalent comparison using the daas soil dataset where the r was 0 49 and rmse was 18 there was no systematic seasonal change in the predictive skill of the model under either soil dataset fig 2 in s3 although the predictions were biased downward 2 6 5 relative soil moisture for daas slga and sg respectively table 42 in s3 fig 2 in s3 the spatial pattern of the nichemapr predictions across the year strongly resembled the ascat data fig 4 6 discussion 6 1 how well can the nichemapr model capture continent scale soil moisture dynamics the first objective of this study was to test the nichemapr microclimate model s ability to jointly simulate soil moisture and temperature dynamics when driven by environmental data available for australia at the continental scale in this the model can be deemed successful with r in the order of 0 65 0 75 and rmse in the order of 0 07 0 15 m3 m 3 across root zone from the continent scale environmental inputs and without any renormalisation or parameter adjustment table 2 the model could also reproduce daily scale spatial patterns of 0 2 cm soil moisture across the year as detected via satellite fig 4 nichemapr s prediction skill compares favourably with previous satellite based and land surface model based predictions table 1 for example su et al 2013 compared satellite derived estimates of near surface soil moisture with oznet sites with a mean r of 0 68 range 0 63 0 71 and an rmse of 0 095 m3 m 3 range 0 088 0 103 prior to renormalisation after renormalisation rmse improved to 0 051 a study by holgate et al 2016 for sites across australia found a mean r of 0 70 range 0 61 0 73 for five satellite derived estimates and a mean r of 0 71 range 0 61 0 79 for five model based predictions in addition the nichemapr modelling system was also able to predict soil temperature with an r of 0 92 and rmse of 4 1 c similar to previous tests of this model against soil temperature data kearney et al 2014b the closeness of the model predictions to the observation data depends on three broad factors 1 the realism of the model physics 2 the veracity of the observational data and 3 the accuracy of the environmental input data from a model physics point of view it was found that inverse fitting via the pso procedure resulted in r of around 0 8 in the top soil layer and rmse of around 0 05 to 0 06 m3 m 3 in general when compared to the oznet data these quantifications and the qualitative match between observations and predictions evident in figs 2 4 and supplementary files s8 s9 and s10 show that the integration of campbell s c11 1 infiltration and redistribution model provides an adequate representation of the underlying physics when integrated into the nichemapr microclimate model of soil temperature the c11 1 program has the sophistications of incorporating a detailed transpiration module and depth varying soil properties and rooting densities through integration with the nichemapr microclimate model it can additionally include the effect of slope aspect and hillshade and the general computation of the heat budget for the soil however it is limited in not accounting for lateral flow i e it is spatially implicit and in assuming a saturated lowest depth a potential issue in very dry sandy conditions and it will not capture the effects of soil structure like cracks and the presence of boulders regarding the veracity of the observational data we excluded some apparent cases of probe error or in the case of very high soil moisture local irrigation in the oznet data set when generating our summary statistics see methods but it is still possible that some of the departure of our predictions from the observations relates to measurement precision or within site variability rather than model inaccuracy there can be high within site variability in soil moisture measures simply due to probe placement and the method of probe calibration famiglietti et al 2008 which will contribute to the variance in the observed soil moisture cracks in the soil can render sensors exposed to air and the use of temperature data from other sites for probe calibration in the case of local temperature probe failure are known issues in the oznet data set that may cause sudden jumps in the observations e g fig 2d 45 cm rodger young pers comm shading from tall grass due to the presence of cages to protect sensors from livestock would explain the lower than predicted shallow soil temperatures in some cases e g november december in fig 2a rodger young pers comm the calibration accuracy of the oznet soil moisture measurements is in the order of 0 03 m3 m 3 rmse smith et al 2012 thus our fitted predictions at an rmse of 0 05 to 0 06 m3 m 3 come close to the maximum potential fit to the observations similarly our model fit to the ascat satellite data involving r of 0 63 and rmse of 0 14 m3 m 3 is very close to the experimental error of these observations su et al 2013 as discussed above our rmse for the cosmoz data of 0 08 m3 m 3 is in fact lower than the reported experimental error for this technique of 0 16 m3 m 3 over the same sites hawdon et al 2014 the driving environmental inputs for the predictions involve continent scale gridded data sets of weather conditions and soil thermal and hydraulic properties the meteorological grids we used from awap rainfall air temperature vapour pressure and radiation inferred cloud cover jones et al 2009 and the mcvicar wind speed data mcvicar et al 2008 have a resolution of 5 km2 this is of course at a larger scale than local weather processes driving ground based soil moisture observations famiglietti et al 2008 we lapse rate corrected the air temperatures for elevation at 250 m2 resolution but rainfall in particular can vary on a fine spatial scale moreover the intra daily pattern of rainfall can be important in driving soil moisture dynamics whereas the awap rainfall is in the form of a daily total to assess the importance of the spatial and temporal coarseness of the awap rainfall inputs on the fit of our predictions we assessed how they compared when the model was run with site specific rainfall available through the oznet data set we also considered the potential value of disaggregating the daily rainfall to hourly using a formal rainfall disaggregation routine surprisingly we found no difference in model fit when we used the disaggregated awap rainfall in contrast to the observed oznet rainfall and only minor improvements on the whole when considering disaggregated awap rainfall to the raw daily awap rainfall this may in part reflect the incompleteness of oznet rainfall data set for some sites which we filled with the disaggregated awap predictions the disaggregation routine we used was not trained on local rainfall observations but rather was run with default parameter settings locally trained disaggregation may be of value when making detailed site specific predictions however our analyses show that this computationally expensive procedure did not improve predictions enough to warrant its use in continent scale calculations the nichemapr predictions are comparable to and in some respects better than the current state of the art model for modelling soil moisture in australia the waterdyn model of the australian water availability project it is difficult to dissect the underlying reasons for the differences between the model predictions the soil properties used were not the same due to different model structures and thus it may be that performance differences reflect the way the physical processes are characterised or issues relating to the soil data that they use the waterdyn model uses the depths of the a and b horizons as given by the daas as well as the saturated volumetric water content and the saturated hydraulic conductivity for each layer it does not use bulk density or air entry potential moreover raupach et al 2009 found that the waterdyn model performance was significantly reduced when using daas estimates of saturated hydraulic conductivity values had to be reduced by factors of 20 50 to obtain reasonable predictions and so used constant values per depth nonetheless the nichemapr model had a slightly higher overall accuracy considering all depths and rmse than the waterdyn model when based only on texture type and bulk density values available from the daas tables 33 and 34 in s2 6 2 does the globalsoilmap net product for australia improve soil moisture estimates the availability of accurate soil hydraulic properties is at least as important as meteorological data for soil moisture calculations table 1 the globalsoilmap net initiative has the objectives of 1 compiling digital soil properties map and 2 providing this information to the global scientific community for the purpose of modelling and evaluation studies hartemink et al 2010 however there have not yet been any assessments of the value of such data sets for soil moisture modelling the second major objective of this study was to assess the value of the new globalsoilmap net product for australia the soil and landscape grid of australia slga viscarra rossel et al 2015 for computing soil moisture we also considered the new 250 m resolution soilgrids product hengl et al 2017 hengl et al 2014 this is a particularly interesting comparison because australia already had a digitised soil properties data set at the continental scale the digital atlas of australia soils daas bureau of rural sciences 1991 mckenzie et al 2000 the daas is based on a large 7000 database of soil physical profiles across australia it is a two layer model of the soil and includes estimates of horizon thickness texture and bulk density for each horizon interpolated to a 5 km grid the slga is based on a larger historical soil data set 280 000 profiles as well as estimates derived from visible and infrared soil spectra and is at a finer spatial resolution 0 1 km grid as well as quantifying soil composition sand silt and clay percentage at 5 depths through the profile we found that the slga substantially improved soil moisture estimates when compared to both the in situ measures and the satellite data the magnitude of this improvement varied with the observational data set being around 0 015 m3 m 3 for the oznet data 0 04 m3 m 3 for the cosmoz data and 0 25 for the ascat data the sg dataset also produced similarly better predictions in the oznet comparisons where observations at different depths could be contrasted the improvement was greatest for the deeper soil layers however we also found that the slga and sg estimates of soil properties for the top 5 cm produced upward biased soil moisture estimates due to the predicted clay content especially for the yanco sites supplementary files s8 s9 in contrast the daas based estimates often provided much better estimates for this depth supplementary file s7 our parameter fitting procedure corrected this by reducing the clay content of the top layer fig 1 in supplementary file s3 as did the simple correction of assuming a loam for the top 5 cm data not shown that near surface soil properties were most weakly predicted by the continent scale soil data sets while the deeper soil moisture estimates were significantly improved by the slga and sg datasets is highly significant this is because satellite derived estimates of soil moisture are most relevant to the top 5 cm thus the satellite derived products as they come online should complement the slga and sg datasets to create a powerful combined data set on full profile soil hydraulic properties these findings illustrate the potential for the globalsoilmap net project and related endeavours to improve soil moisture modelling capacity in other parts of the world 7 conclusion we have shown that by incorporating campbell s 1985 infiltration and redistribution algorithm into the nichemapr microclimate model we were able to develop an effective general soil moisture modelling tool that could predict soil moisture at the continental scale with performance close to experimental error we also found that the new soil and landscape grid of australia slga and soilgrids sg data sets provided better estimates of soil moisture when compared with earlier the chloropleth derived gridded soil properties especially for deeper layers this illustrates the value of the next generation soil data products being developed under the globalsoilmap net project and related initiatives of which the slga is the first continent wide application the future integration of satellite derived estimates of soil hydraulic properties in the top 2 cm with globalsoilmap net estimates for deeper layers should greatly improve capacity for modelling this critically important environmental variable acknowledgements michael kearney was supported by an australian research council arc fellowship dp110102813 and james maino was supported by a david hay award computational support was provided by melbourne bioinformatics we thank gaylon campbell jeff walker dongreol ryu and rodger young for discussion and advice and four anonymous joh reviewers and the joh editorial team for comments that substantially improved the manuscript we acknowledge the csiro funded cosmoz network cosmoz csiro au for provision and processing of data appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 04 040 appendix a supplementary data supplementary data 1 supplementary data 10 supplementary data 2 supplementary data 3 supplementary data 4 supplementary data 5 supplementary data 6 supplementary data 7 supplementary data 8 supplementary data 9 
7301,accurate models of soil moisture are vital for solving core problems in meteorology hydrology agriculture and ecology the capacity for soil moisture modelling is growing rapidly with the development of high resolution continent scale gridded weather and soil data together with advances in modelling methods in particular the globalsoilmap net initiative represents next generation depth specific gridded soil products that may substantially increase soil moisture modelling capacity here we present an implementation of campbell s infiltration and redistribution model within the nichemapr microclimate modelling package for the r environment and use it to assess the predictive power provided by the globalsoilmap net product soil and landscape grid of australia slga 100 m as well as the coarser resolution global product soilgrids sg 250 m predictions were tested in detail against 3 years of root zone 3 75 cm soil moisture observation data from 35 monitoring sites within the oznet project in australia with additional tests of the finalised modelling approach against cosmic ray neutron cosmoz 0 50 cm 9 sites from 2011 to 2017 and satellite ascat 0 2 cm continent wide from 2007 to 2009 observations the model was forced by daily 0 05 5 km gridded meteorological data the nichemapr system predicted soil moisture to within experimental error for all data sets using the slga or the sg soil database the oznet soil moisture could be predicted with a root mean square error rmse of 0 075 m3 m 3 and a correlation coefficient r of 0 65 consistently through the soil profile without any parameter tuning soil moisture predictions based on the slga and sg datasets were 17 closer to the observations than when using a chloropleth derived soil data set digital atlas of australian soils with the greatest improvements occurring for deeper layers the cosmoz observations were predicted with similar accuracy r 0 76 and rmse of 0 085 m3 m 3 comparisons at the continental scale to 0 2 cm satellite data ascat showed that the slga sg datasets increased model fit over simulations using the daas soil properties r 0 63 rmse 15 vs r 0 48 rmse 18 respectively overall our results demonstrate the advantages of using globalsoilmap net products in combination with gridded weather data for modelling soil moisture at fine spatial and temporal resolution at the continental scale keywords microclimate model soil moisture land surface model soil hydraulic properties ascat 1 introduction soil moisture is a fundamental environmental variable for physical and biological processes in terrestrial systems water exchange between the soil and the atmosphere couples energy and mass fluxes and has a strong influence on weather forecasts and catchment hydrodynamics especially through memory effects e g jung et al 2010 koster et al 2000 it also drives biological productivity and ecosystem structure by constraining plant fungal and microbial growth rodriguez iturbe et al 1999 and can be a potent constraint on the distribution and abundance of animals with one or more life stages in the soil e g bodensteiner et al 2015 bradford and seymour 1988 packard et al 1987 for all of these reasons the capacity to accurately predict root zone 1 m deep soil moisture is a coveted goal in environmental science a variety of land surface models have been developed to compute soil moisture as a function of meteorological input variables and soil and vegetation parameters e g albergel et al 2012 best et al 2011 kowalczyk et al 2006 raupach et al 2009 viterbo and beljaars 1995 yang et al 2014 however their predictive capacity depends strongly on the availability of spatially explicit soil properties which are limited in spatial resolution and accuracy bandara et al 2011 holgate et al 2016 richter et al 2004 reviewed in table 1 for example holgate et al 2016 assessed the performance of four modelling approaches against sub hourly soil moisture observations in australia and found limited capacity to capture short term dynamics of the sort required in many biological applications richter et al 2004 found that a chloropleth derived continental soil database provide no improvement to soil moisture predictions over the use of spatially fixed soil hydraulic parameters and emphasised the need for depth varying soil properties remote sensing technologies for measuring soil moisture from space are creating new opportunities to characterise and model soil moisture holgate et al 2016 panciera et al 2014 renzullo et al 2014 wang and qu 2009 although they can be limited in coverage both temporally snapshots from aircraft or regular satellite passes every few days and spatially 5 cm depth 1 km2 for passive microwave satellite observations work is underway to estimate soil properties from remotely sensed soil moisture bandara et al 2013 bandara et al 2014 bandara et al 2015 this is a promising avenue but is inherently limited when soil properties vary strongly with depth the globalsoilmap net project hartemink et al 2010 aims to develop fine scale grids of soil properties at six different depths for the globe and such data sets have the capacity to substantially improve the predictive ability of soil moisture models the first continental output from this project is the soil and landscape grid of australia grundy et al 2015 viscarra rossel et al 2015 the slga data set includes estimates of the percentage of clay sand and silt as well as bulk density at six depths 0 5 cm 5 15 cm 15 30 cm 30 60 cm 60 100 cm and 100 200 cm at a resolution of 100 m 3 arc seconds for the entire continent a related new product is the soilgrids sg global soil dataset hengl et al 2017 hengl et al 2014 which includes the same data at 250 m resolution at the same depths unlike sg and other modern gridded data sets of soil properties that rely only on remote sensing based soil covariates the slga integrates historical soil information new measurement with spectroscopic sensors and novel spatial modelling grundy et al 2015 here we assess the capacity of these next generation soil data sets for modelling soil moisture in conjunction with a recently released microclimate model kearney and porter 2017 for the r programming environment r development core team 2012 this model is part of the nichemapr package for modelling energy and mass budgets of organisms there is currently no soil moisture model available within the increasingly popular r computing environment and thus our model fills a niche for an accessible and general microclimate modelling framework that includes soil moisture calculations the nichemapr microclimate model was originally developed to predict desert microclimates for estimating animal heat budgets beckman et al 1973 mitchell et al 1975 porter et al 1973 with a capacity to model soil temperature but not soil moisture kearney et al 2014b recently tested the same model for its capacity to predict hourly soil temperature across the usa when driven by a monthly global climatology kearney et al 2014a and in australia when driven by 0 05 5 km resolution daily gridded meteorological data jones et al 2009 and pre calculated soil moisture from the awap raupach et al 2009 here we modify the model to incorporate campbell s 1985 infiltration and redistribution algorithm and assess its performance at modelling both in situ and satellite derived soil moisture observations our study has the following parallel objectives i to test the general ability of the nichemapr microclimate model when integrated with campbell s infiltration and redistribution algorithm to predict soil moisture from continent scale forcing data and ii to assess whether the use of next generation soil properties databases slga sg can improve soil moisture predictions over early generation chloropleth maps daas 2 theoretical background a variety of approaches have been taken to modelling soil moisture that range from purely descriptive and phenomenological indices such as the keetch byram drought index kbdi keetch and byram 1968 to more mechanistic approaches such as that of campbell 1985 see mendoza et al 2015 for an overview we chose to implement program 11 1 henceforth c11 1 of campbell 1985 for simulating soil moisture water potential and humidity gradients in the presence of vegetation we chose this algorithm because it is capable of depth specific predictions and because the code was straight forward to integrate into the existing nichemapr microclimate modelling framework for computing soil temperature the c11 1 model takes the approach of linearising the differential equation for flow in space depth and using a newton raphson procedure to solve the non linear equations through time the linearising through space involves representing the soil as a series of capacitors and resistors see campbell 1985 fig 8 5 it uses matric potential as the dependent variable rather than matric flux potential with the consequence that it is most computationally efficient when soil is relatively dry the model is well described in campbell 1985 and in supplementary material s1 we provide a detailed summary of the model as implemented in the present study briefly the nichemapr microclimate model uses 10 user specified depth nodes for computing the heat budget we added code to c11 1 to insert extra nodes half way between the original 10 nodes such that a total of 19 nodes were used for soil moisture calculations the user input variables specific to c11 1 include for each of these 19 nodes the campbell aka clapp and hornberger exponent b the air entry potential ψ e j kg the saturated hydraulic conductivity k s kg s m 3 the bulk density ρ b kg m 3 and the root density m m 3 in addition to the leaf area index lai already mentioned we obtained estimates of these user input variables from the soil databases or by inverse fitting to the data as explained further in 3 3 and 3 4 all other model parameters for c11 1 were fixed at those suggested by campbell 1985 as described in s1 however the package allows user control over the resistance per unit length of root and leaf m3 kg 1 s 1 the critical leaf water potential for stomatal closure j kg 1 the stability parameter for the stomatal closure equation the maximum allowable mass balance error kg and the maximum number of iterations for mass balance 3 study sites and materials 3 1 observational data we used 35 of the oznet soil moisture monitoring sites smith et al 2012 http www oznet org au as our main empirical data with which to test our model predictions fig 1 for pictures of each site see supplementary file s2 the sites are within the murrumbidgee catchment of the state of new south wales and are monitored sub hourly the 100 000 km2 region ranges in elevation as defined in mcvicar and körner 2013 from 50 to 1900 m and experiences annual rainfall ranging from 330 mm in the arid west to 1900 mm in the more humid higher elevation east following bandara et al 2014 we focused on 2008 2010 which include two dry years and a wet year 2010 we also assessed our predictions against the cosmoz cosmoz csiro au hawdon et al 2014 data set for the 9 locations across australia considered by hawdon et al 2014 fig 1 which comprises hourly cosmic ray neutron data at between 5 cm 30 cm depth for various time periods between 2011 and 2016 finally we compared our predictions to the ascat 0 2 cm soil moisture product wagner et al 1999 for 2007 from the 0 05 resampled product produced by the ecosystem modelling and scaling infrastructure emast http www emast org au of the terrestrial ecosystem research network tern http www tern org au accessed 8th june 2017 3 2 forcing meteorological variables the model was driven by the awap 0 05 5 km resolution daily gridded meteorological data jones et al 2009 of minimum and maximum air temperature mean vapour pressure from the 9 am and 3 pm grids total daily solar used to derive cloud cover and total rainfall as described in detail by kearney et al 2014b in addition we used a daily gridded mean wind speed product mcvicar et al 2008 assuming maximum wind speed was double and minimum wind speed was half the mean value as described in more detail in kearney and porter 2017 hourly interpolations of air temperature wind speed relative humidity and cloud cover were made by specifying the timing of maxima and minima relative to dawn minimum air temperature and wind speed maximum humidity and maximum cloud cover all occurring at dawn in our simulations or solar noon maximum air temperature and wind speed minimum humidity and cloud cover all occurring one hour after solar noon see also ephrath et al 1996 for humidity wind speed and cloud cover hourly interpolations were based on linear functions between the solar noon value a minimum or maximum depending on the variable a midnight value computed as the mean of the maximum and minimum value and the dawn value as for the noon value for air temperature a more sophisticated algorithm was used where a linear change is assumed from midnight to dawn a sine wave from dawn to sunset and an exponential decay from sunset to midnight supplementary material appendix 1 section 4 in kearney and porter 2017 see also mcvicar 1999 parton and logan 1981 for the oznet comparisons we explored the performance of the model when run using three different rainfall measures we first considered the in situ sub hourly aggregated to hourly rainfall measurements provided with the soil moisture observations we then considered the awap gridded rainfall estimates these latter interpolated estimates are available at the continental scale for australia but are only provided as daily totals and we assumed that this all fell in the first hour of the day in our standard simulations finally we applied the rainfall disaggregation function disagsimul of the hyetosminute r package http www itia ntua gr en softinfo 3 which uses the bartlett lewis rectangular pulse model to disaggregate rainfall from daily to hourly rodriguez iturbe et al 1987 we used all the default settings of the disaggregation function 3 3 terrain and vegetation variables flat ground was assumed and site elevation was derived from a 0 0025 250 m digital elevation model air temperatures were corrected to this elevation from the original 0 05 values with an adiabatic lapse rate of 0 0039 c m 1 and 0 0077 c m 1 for minimum and maximum air temperature respectively ruddell et al 1990 surface albedo was derived from a 0 25 satellite product avhrr average over 1994 1999 resampled to 0 05 briggs 2011 we obtained satellite derived lai observations from the 8 day composite 1 km resolution modis lpdaac mod15a2 mosaic data set paget and king 2008 values were extracted for each location and splined to a daily time step using the native spline function in the statistical package r r development core team 2012 monthly emissivity for longwave radiation exchange was extracted from a 0 05 global emissivity database seemann et al 2008 and splined to daily values 3 4 soil properties previous tests of soil moisture models against the oznet data set used the digital atlas of australian soils daas richter et al 2004 or derived properties from remote sensed or actual measurements bandara et al 2014 bandara et al 2015 here we explore the potential for deriving soil moisture parameters from the new slga and sg datasets and compare it to simulations using the earlier daas we extracted the bulk density and the proportion of sand silt and clay for each site from the sgla and sg for all depths 0 5 cm 5 15 cm 15 30 cm 30 60 cm 60 100 cm and 100 200 cm note the maximum depth simulated was always 200 cm and we assumed that the potential influence of bedrock was captured by the bulk density measures we used r s spline function method natural to interpolate the extracted soil properties to the specific depth arrays chosen by the user for the model we then used the empirical functions of cosby et al 1984 their table 5 to compute the air entry water potential ψ e the campbell exponent b and the saturated hydraulic conductivity k s as an alternative means of obtaining soil hydraulic properties we looked up the values in campbell and norman s 1998 table 9 1 by converting the slga soil sand silt and clay quantities to the soil texture classes of the same table to assess the advantage of using the slga and sg databases over the daas we tested predictions based on bulk density estimates for the two profiles considered by the daas and used the texture type for each daas layer categories of sand sandy loam loam clay loam light clay and clay table 2 of mckenzie et al 2000 to look up soil properties from table 9 1 of campbell and norman 1998 for computing thermal soil properties we assumed that mineral fraction had a thermal conductivity of 2 5 w m 1 c 1 a density of 2560 kg m 3 and a specific heat of 870 j kg 1 k 1 table 8 2 in campbell and norman 1998 4 methods 4 1 in situ point measurement oznet comparisons we contrasted the accuracy of model predictions relative to the oznet observation data under the different treatments of rainfall described in section 3 2 observed disaggregated daily and daily and the different pedotransfer functions soil data sets slga and the digital atlas of australian soils described in section 3 4 to see how closely the model could fit the data when tuned to specific sites we used a particle swarm optimiser pso to fit model parameters to the observed data using just the first year of the 2008 2010 period we use the r package hydropso zambrano bigiarini and rojas 2013 to fit six adjustment factors a multiplier of the leaf area index lai range 0 2 a divisor for the root density profile rdiv range 0 75 10 a factor by which the bulk density of the top 8 cm of the soil profile was multiplied to simulate soil compaction scomp range 1 2 and factors by which the proportion of clay to sand of the top layer 2 5 cm δclay a middle layer 7 5 22 5 cm δclay b and the bottom layer 45 200 cm δclay c of the soil profile were adjusted range 1 to 1 the latter three parameters altered the prediction of soil composition from the slga while the former three parameters reflect more idiosyncratic local processes due to land management cropping clearing and soil compaction through livestock or machinery a goodness of fit function was supplied to the pso which computed the rmse for only the top three of the four soil layers as a compromise to achieve close fits to near surface conditions while maintaining consistency with deeper soil conditions all default settings were used for the hydropsos function and following bandara et al 2013 we used 30 particles but limited the maximum number of iterations to 50 because of the computationally intensive nature of the procedure 2 h per site with one 2 year simulation taking approximately 15 s on a single core of an intel 2 8 ghz i7 4900mq cpu we contrasted different modelling strategies using paired t tests between the values for r and rmse for all 35 sites at each of the four depths the summary statistics and paired t tests for soil moisture excluded the following site depth observations due to apparent measurement malfunction or aberrantly high moisture e g due to local irrigation rodger young pers comm k12 75 cm k12 45 cm a4 45 cm y3 4 cm similarly for soil temperature we excluded a1 4 cm a4 45 cm k12 3 cm k13 3 cm k1 4 cm k1 75 cm k2 4 cm k5 4 cm k7 3 cm m6 4 cm for the latter set all but one involved implausibly low temperature fluctuations in the top 5 cm likely due to shading from vegetation see discussion as well as two cases including one of the near surface measurements of clear sensor malfunction the summary statistics and plots for these excluded layers and all other layers can be found in supplementary documents s4 s10 we simulated 2007 as a spin up year one year of simulations on an hourly time step was always sufficient for soil moisture profiles to stabilise the oznet soil moisture observations are integrated measurements across different depths thus we chose a depth profile for our simulations that included the mid point of the observations for a given site and computed the root mean square error rmse and correlation coefficient r as measures of fit we similarly compared the model s soil temperature predictions with the oznet soil temperature observations where they were available 4 2 in situ cosmic ray neutron cosmoz comparisons for the cosmoz data we ran the microclimate model using the daily awap rainfall only but varied the soil database between the slga sg and daas the cosmoz sites vary in the temporal data extent and we simulated the maximum duration available including one year prior as a spin up year we compared our predictions to the 7 h average of current hour plus three hours before and after including rainfall events values as the hourly data are inherently noisy ca 0 01 m3 m 3 we made comparisons between our predictions at 2 5 5 10 and 20 cm as well as the time varying closest depth of all those simulated i e also including 30 60 100 200 cm to the estimated depth provided in the cosmoz data set 4 3 satellite ers scatterometer ascat comparisons for the satellite data comparison we ran the microclimate model based on daily awap rainfall across a regular sample grid of 1559 sites across australia querying the nearest available spatial data for the years 1990 to 2009 we obtained the average volumetric soil moisture across 0 1 and 2 cm and converted it to relative soil moisture based on the topmost bulk density layer for the relevant soil properties database slga sg or daas these values were compared to the nearest available ascat grid points across the continent for each day of the year for the year 2007 5 results 5 1 in situ point measurement oznet comparisons overall our model evaluation strategy for the oznet data resulted in four different kinds of comparison summarised in table 2 three of these comparisons assessed the performance of the nichemapr model in predicting soil moisture and soil temperature under different parameter and variable settings an additional set of simulations compared the nichemapr soil moisture predictions to those of the australian water availability project under the different parameter and variable settings we summarise the outcome of each test here and in table 2 and provide representative example time series plots fig 2 with more detailed summaries provided in the supplementary documents s3 all tests and s4 s10 all site and depth specific time series plots 5 1 1 pedotransfer functions the pedotransfer function of cosby et al 1984 produced significantly better fits for soil moisture than did the look up table approach based on campbell and norman s table 9 1 campbell and norman 1998 in terms of rmse at the deeper three levels but there were no significant differences for r tables 1 and 2 in s2 overall the rmse improved by 0 012 m3 m 3 when using the cosby pedotransfer function compared to the campbell and norman values 0 072 vs 0 084 m3 m 3 with r in both cases being 0 65 at the shallowest level the campbell and norman parameters gave slightly better but statistically non significant rmse for soil temperature there were no significant differences between the pedotransfer functions though the cosby et al function gave a slightly lower rmse tables 3 and 4 in s3 thus the cosby pedotransfer function was used for subsequent tests 5 1 2 treatment of rainfall there were no statistically significant differences in either r or rmse for soil moisture when using the empirical oznet hourly rainfall or the awap daily rainfall disaggregated to hourly tables 5 and 6 in s3 for soil temperature the oznet data significantly improved rmse at 45 and 75 cm though the differences were very minor 0 1 c rmse tables 7 and 8 in s3 when comparing predictions using disaggregated awap data compared to the daily aggregated awap data disaggregation improved r at 4 cm and 15 cm and improved rmse at 45 cm and 75 cm by 0 007 m3 m 3 tables 9 and 10 in s3 however aggregated awap gave better rmse at 4 cm an improvement of 0 009 m3 m 3 for soil temperature disaggregation improved rmse at 4 cm but decreased it at 15 and 45 cm and improved r at 4 and 45 cm but again the differences were only slight 0 15 c rmse tables 11 and 12 in s3 due to the minor nature of the advantages in disaggregating rainfall the unaltered awap daily rainfall was used for subsequent comparisons because it is less computationally intensive 5 1 3 soil properties use of the slga soil data set compared with the daas significantly improved r at 15 cm and rmse at all but the shallowest depth tables 13 and 14 in s3 the sg data set produced similarly strong correlations to the slga but the slga still improved the rmse at all depths but the shallowest table 17 and 18 in s3 the overall improvement in rmse when using the slga data set was 0 016 and 0 005 m3 m 3 over the daas and soilgrids respectively soil temperature predictions were also improved under slga when compared to daas tables 15 and 16 in s3 and sg tables 19 and 20 in s3 at 4 cm while being significantly worse at 15 cm but the differences were very slight 0 1 c 5 1 4 fitted parameters the pso for the slga scenario did not significantly improve r but did improve rmse for the top three depths with an overall improvement of 0 017 m3 m 3 and the greatest improvement 0 02 m3 m 3 at 4 and 45 cm tables 21 and 22 in s3 soil temperature predictions were only improved at 15 cm where r was slightly higher tables 23 and 24 in s3 there were no clear patterns in the way the parameters were adjusted by the pso except that the leaf area index and root density values were always increased from the initial estimates fig 1 in s3 also clay content was decreased in the top layer in 26 of the 35 sites and was increased in the bottom layer in 28 of the sites 5 1 5 nichemapr vs waterdyn contrasts the waterdyn model of the australian water availability project consistently showed a slightly stronger correlation coefficient than the nichemapr model 0 69 waterdyn vs 0 61 0 66 nichemapr tables 21 and 32 in s3 the rmse of the nichemapr model was consistently lower than the waterdyn predictions for all but the shallowest depth with strongly statistically significant differences in the order of 0 05 0 10 m3 m 3 under most scenarios tables 25 and 36 in s3 however for all comparisons except the fitted parameter nichemapr simulations the waterdyn predictions had lower rmse than the nichemapr predictions for the top 5 cm by an amount in the order of 0 01 0 02 m3 m 3 tables 25 and 36 in s3 5 2 in situ cosmic ray neutron cosmoz comparisons the nichemapr model predicted the cosmoz soil moisture at the stated time varying mean measurement depth with a mean r of 0 757 and an rmse of 0 082 m3 m 3 when using the slga table 38 in s3 this stated depth was estimated to be around 20 cm on average but the fixed depth predictions most closely matching the observations were shallower than this 5 or 10 cm for all but one site table 37 in s3 with r at these depths 0 77 and rmse 0 078 m3 m 3 table s38 overall the slga based predictions when compared to the daas based predictions gave higher r 0 76 vs 0 74 and lower rmse 0 075 vs 0 084 m3 m 3 table 2 in the main text and table 38 in s3 though these differences were only marginal from a statistical point of view table 35 in s9 there were no statistically significant differences between the model predictions when using the sg vs the slga soil data tables 40 and 41 in s3 5 3 satellite ers scatterometer ascat comparisons comparison of the 2007 ascat satellite data 0 2 cm soil moisture with that predicted by nichemapr using the slga soil dataset produced an r of 0 63 and an rmse of 15 relative soil moisture table 2 in the main text table 42 in s3 this was very similar to the results when using the sg soil dataset and consistently better than the equivalent comparison using the daas soil dataset where the r was 0 49 and rmse was 18 there was no systematic seasonal change in the predictive skill of the model under either soil dataset fig 2 in s3 although the predictions were biased downward 2 6 5 relative soil moisture for daas slga and sg respectively table 42 in s3 fig 2 in s3 the spatial pattern of the nichemapr predictions across the year strongly resembled the ascat data fig 4 6 discussion 6 1 how well can the nichemapr model capture continent scale soil moisture dynamics the first objective of this study was to test the nichemapr microclimate model s ability to jointly simulate soil moisture and temperature dynamics when driven by environmental data available for australia at the continental scale in this the model can be deemed successful with r in the order of 0 65 0 75 and rmse in the order of 0 07 0 15 m3 m 3 across root zone from the continent scale environmental inputs and without any renormalisation or parameter adjustment table 2 the model could also reproduce daily scale spatial patterns of 0 2 cm soil moisture across the year as detected via satellite fig 4 nichemapr s prediction skill compares favourably with previous satellite based and land surface model based predictions table 1 for example su et al 2013 compared satellite derived estimates of near surface soil moisture with oznet sites with a mean r of 0 68 range 0 63 0 71 and an rmse of 0 095 m3 m 3 range 0 088 0 103 prior to renormalisation after renormalisation rmse improved to 0 051 a study by holgate et al 2016 for sites across australia found a mean r of 0 70 range 0 61 0 73 for five satellite derived estimates and a mean r of 0 71 range 0 61 0 79 for five model based predictions in addition the nichemapr modelling system was also able to predict soil temperature with an r of 0 92 and rmse of 4 1 c similar to previous tests of this model against soil temperature data kearney et al 2014b the closeness of the model predictions to the observation data depends on three broad factors 1 the realism of the model physics 2 the veracity of the observational data and 3 the accuracy of the environmental input data from a model physics point of view it was found that inverse fitting via the pso procedure resulted in r of around 0 8 in the top soil layer and rmse of around 0 05 to 0 06 m3 m 3 in general when compared to the oznet data these quantifications and the qualitative match between observations and predictions evident in figs 2 4 and supplementary files s8 s9 and s10 show that the integration of campbell s c11 1 infiltration and redistribution model provides an adequate representation of the underlying physics when integrated into the nichemapr microclimate model of soil temperature the c11 1 program has the sophistications of incorporating a detailed transpiration module and depth varying soil properties and rooting densities through integration with the nichemapr microclimate model it can additionally include the effect of slope aspect and hillshade and the general computation of the heat budget for the soil however it is limited in not accounting for lateral flow i e it is spatially implicit and in assuming a saturated lowest depth a potential issue in very dry sandy conditions and it will not capture the effects of soil structure like cracks and the presence of boulders regarding the veracity of the observational data we excluded some apparent cases of probe error or in the case of very high soil moisture local irrigation in the oznet data set when generating our summary statistics see methods but it is still possible that some of the departure of our predictions from the observations relates to measurement precision or within site variability rather than model inaccuracy there can be high within site variability in soil moisture measures simply due to probe placement and the method of probe calibration famiglietti et al 2008 which will contribute to the variance in the observed soil moisture cracks in the soil can render sensors exposed to air and the use of temperature data from other sites for probe calibration in the case of local temperature probe failure are known issues in the oznet data set that may cause sudden jumps in the observations e g fig 2d 45 cm rodger young pers comm shading from tall grass due to the presence of cages to protect sensors from livestock would explain the lower than predicted shallow soil temperatures in some cases e g november december in fig 2a rodger young pers comm the calibration accuracy of the oznet soil moisture measurements is in the order of 0 03 m3 m 3 rmse smith et al 2012 thus our fitted predictions at an rmse of 0 05 to 0 06 m3 m 3 come close to the maximum potential fit to the observations similarly our model fit to the ascat satellite data involving r of 0 63 and rmse of 0 14 m3 m 3 is very close to the experimental error of these observations su et al 2013 as discussed above our rmse for the cosmoz data of 0 08 m3 m 3 is in fact lower than the reported experimental error for this technique of 0 16 m3 m 3 over the same sites hawdon et al 2014 the driving environmental inputs for the predictions involve continent scale gridded data sets of weather conditions and soil thermal and hydraulic properties the meteorological grids we used from awap rainfall air temperature vapour pressure and radiation inferred cloud cover jones et al 2009 and the mcvicar wind speed data mcvicar et al 2008 have a resolution of 5 km2 this is of course at a larger scale than local weather processes driving ground based soil moisture observations famiglietti et al 2008 we lapse rate corrected the air temperatures for elevation at 250 m2 resolution but rainfall in particular can vary on a fine spatial scale moreover the intra daily pattern of rainfall can be important in driving soil moisture dynamics whereas the awap rainfall is in the form of a daily total to assess the importance of the spatial and temporal coarseness of the awap rainfall inputs on the fit of our predictions we assessed how they compared when the model was run with site specific rainfall available through the oznet data set we also considered the potential value of disaggregating the daily rainfall to hourly using a formal rainfall disaggregation routine surprisingly we found no difference in model fit when we used the disaggregated awap rainfall in contrast to the observed oznet rainfall and only minor improvements on the whole when considering disaggregated awap rainfall to the raw daily awap rainfall this may in part reflect the incompleteness of oznet rainfall data set for some sites which we filled with the disaggregated awap predictions the disaggregation routine we used was not trained on local rainfall observations but rather was run with default parameter settings locally trained disaggregation may be of value when making detailed site specific predictions however our analyses show that this computationally expensive procedure did not improve predictions enough to warrant its use in continent scale calculations the nichemapr predictions are comparable to and in some respects better than the current state of the art model for modelling soil moisture in australia the waterdyn model of the australian water availability project it is difficult to dissect the underlying reasons for the differences between the model predictions the soil properties used were not the same due to different model structures and thus it may be that performance differences reflect the way the physical processes are characterised or issues relating to the soil data that they use the waterdyn model uses the depths of the a and b horizons as given by the daas as well as the saturated volumetric water content and the saturated hydraulic conductivity for each layer it does not use bulk density or air entry potential moreover raupach et al 2009 found that the waterdyn model performance was significantly reduced when using daas estimates of saturated hydraulic conductivity values had to be reduced by factors of 20 50 to obtain reasonable predictions and so used constant values per depth nonetheless the nichemapr model had a slightly higher overall accuracy considering all depths and rmse than the waterdyn model when based only on texture type and bulk density values available from the daas tables 33 and 34 in s2 6 2 does the globalsoilmap net product for australia improve soil moisture estimates the availability of accurate soil hydraulic properties is at least as important as meteorological data for soil moisture calculations table 1 the globalsoilmap net initiative has the objectives of 1 compiling digital soil properties map and 2 providing this information to the global scientific community for the purpose of modelling and evaluation studies hartemink et al 2010 however there have not yet been any assessments of the value of such data sets for soil moisture modelling the second major objective of this study was to assess the value of the new globalsoilmap net product for australia the soil and landscape grid of australia slga viscarra rossel et al 2015 for computing soil moisture we also considered the new 250 m resolution soilgrids product hengl et al 2017 hengl et al 2014 this is a particularly interesting comparison because australia already had a digitised soil properties data set at the continental scale the digital atlas of australia soils daas bureau of rural sciences 1991 mckenzie et al 2000 the daas is based on a large 7000 database of soil physical profiles across australia it is a two layer model of the soil and includes estimates of horizon thickness texture and bulk density for each horizon interpolated to a 5 km grid the slga is based on a larger historical soil data set 280 000 profiles as well as estimates derived from visible and infrared soil spectra and is at a finer spatial resolution 0 1 km grid as well as quantifying soil composition sand silt and clay percentage at 5 depths through the profile we found that the slga substantially improved soil moisture estimates when compared to both the in situ measures and the satellite data the magnitude of this improvement varied with the observational data set being around 0 015 m3 m 3 for the oznet data 0 04 m3 m 3 for the cosmoz data and 0 25 for the ascat data the sg dataset also produced similarly better predictions in the oznet comparisons where observations at different depths could be contrasted the improvement was greatest for the deeper soil layers however we also found that the slga and sg estimates of soil properties for the top 5 cm produced upward biased soil moisture estimates due to the predicted clay content especially for the yanco sites supplementary files s8 s9 in contrast the daas based estimates often provided much better estimates for this depth supplementary file s7 our parameter fitting procedure corrected this by reducing the clay content of the top layer fig 1 in supplementary file s3 as did the simple correction of assuming a loam for the top 5 cm data not shown that near surface soil properties were most weakly predicted by the continent scale soil data sets while the deeper soil moisture estimates were significantly improved by the slga and sg datasets is highly significant this is because satellite derived estimates of soil moisture are most relevant to the top 5 cm thus the satellite derived products as they come online should complement the slga and sg datasets to create a powerful combined data set on full profile soil hydraulic properties these findings illustrate the potential for the globalsoilmap net project and related endeavours to improve soil moisture modelling capacity in other parts of the world 7 conclusion we have shown that by incorporating campbell s 1985 infiltration and redistribution algorithm into the nichemapr microclimate model we were able to develop an effective general soil moisture modelling tool that could predict soil moisture at the continental scale with performance close to experimental error we also found that the new soil and landscape grid of australia slga and soilgrids sg data sets provided better estimates of soil moisture when compared with earlier the chloropleth derived gridded soil properties especially for deeper layers this illustrates the value of the next generation soil data products being developed under the globalsoilmap net project and related initiatives of which the slga is the first continent wide application the future integration of satellite derived estimates of soil hydraulic properties in the top 2 cm with globalsoilmap net estimates for deeper layers should greatly improve capacity for modelling this critically important environmental variable acknowledgements michael kearney was supported by an australian research council arc fellowship dp110102813 and james maino was supported by a david hay award computational support was provided by melbourne bioinformatics we thank gaylon campbell jeff walker dongreol ryu and rodger young for discussion and advice and four anonymous joh reviewers and the joh editorial team for comments that substantially improved the manuscript we acknowledge the csiro funded cosmoz network cosmoz csiro au for provision and processing of data appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 04 040 appendix a supplementary data supplementary data 1 supplementary data 10 supplementary data 2 supplementary data 3 supplementary data 4 supplementary data 5 supplementary data 6 supplementary data 7 supplementary data 8 supplementary data 9 
7302,streamflow is an essential component of the hydrologic cycle in the regional and global scale and the main source of fresh water supply it is highly associated with natural disasters such as droughts and floods therefore accurate streamflow forecasting is essential forecasting streamflow in general and monthly streamflow in particular is a complex process that cannot be handled by data driven models ddms only and requires pre processing wavelet transformation is a pre processing technique however application of continuous wavelet transformation cwt produces many scales that cause deterioration in the performance of any ddm because of the high number of redundant variables this study proposes multigene genetic programming mggp as a selection tool after the cwt analysis it selects important scales to be imposed into the artificial neural network ann a basin located in the southeast of turkey is selected as case study to prove the forecasting ability of the proposed model one month ahead downstream flow is used as output and downstream flow upstream rainfall temperature and potential evapotranspiration with associated lags are used as inputs before modeling wavelet coherence transformation wct analysis was conducted to analyze the relationship between variables in the time frequency domain several combinations were developed to investigate the effect of the variables on streamflow forecasting the results indicated a high localized correlation between the streamflow and other variables especially the upstream in the models of the standalone layout where the data were entered to ann and mggp without cwt the performance is found poor in the best scale layout where the best scale of the cwt identified as the highest correlated scale is chosen and enters to ann and mggp the performance increased slightly using the proposed model the performance improved dramatically particularly in forecasting the peak values because of the inclusion of several scales in which seasonality and irregularity can be captured using hydrological and meteorological variables also improved the ability to forecast the streamflow keywords wavelet coherence transformation continuous wavelet transformation artificial neural network data driven models 1 introduction streamflow is an essential component of the hydrologic cycle in the regional and global scale makkeasorn et al 2008 liu et al 2014 it is the main source of fresh water supply and is associated highly with natural disasters such as droughts and floods therefore accurate short term and long term forecasting of streamflow is essential for efficient water resources management particularly in regions vulnerable to floods and droughts kisi and cimen 2011 forecasting streamflow is a complicated task ravansalar et al 2017 because of the complexity of the process yaseen et al 2015 mehr and kahya 2017 this complexity is due mostly to the non stationarity and non linearity in the relationship between the streamflow and basin characteristics nourani et al 2011a mehr and kahya 2017 data driven models ddms can be used for modeling streamflow because they have the ability to deal with non linearity and map the relationship between the streamflow and factors that drive it ddms also obtain the optimum solution between inputs and outputs without involving the physical process solomatine et al 2008 many applications of forecasting streamflow use ddms including artificial neural network ann support vector machines adaptive neuro fuzzy inference system and genetic programming gp tokar and johnson 1999 kişi 2007 kisi et al 2012 nourani et al 2013 mehr et al 2015 abdollahi et al 2017 mehr and kahya 2017 mehr and nourani 2017 ddms can deal with the nonlinearity and non stationarity in the mean and variance elements but the main shortcoming of ddms is that they may not be able to cope with non stationarity fluctuation nourani et al 2011b if no preprocessing is applied to the data cannas et al 2006 one of the solutions is to apply the wavelet with the ability to cope with the seasonal cycling nonstationary component of the time series daubechies 1990 torrence and compo 1998 anctil and tape 2004 nourani et al 2011b a wavelet has good ability in representing the signal locally in time and frequency domains the decomposition of the nonstationary time series using wavelet into several scales lee and yamamoto 1994 extracts historical hidden information in time and frequency domains rajaee et al 2010 shoaib et al 2014 a wavelet with various ddm types has been applied widely in streamflow forecasting shiri and kisi 2010 kisi and cimen 2011 adamowski and prasher 2012 badrzadeh et al 2013 komasi and sharghi 2016 ravansalar et al 2017 and especially with the ann approach kişi and kerem cigizoglu 2007 carcano et al 2008 kişi 2009 adamowski and sun 2010 araghinejad et al 2011 pramanik et al 2011 adamowski et al 2012 isik et al 2013 nourani and parhizkar 2013 nourani et al 2014a gp is another ddm identified as robust explicit method that can be used for modeling dorado et al 2003 mehr et al 2013 mehr et al 2014 de falco et al 2005 reported several advantages of gp over ann 1 gp generates an explicit model understandable by humans 2 it automatically discovers the structure of the model by utilizing the given data 3 it has an adaptive evolutionary search ability that does not trap suboptimal unsatisfactory local solutions and 4 it has no specific knowledge thus gp has been applied widely in hydrological modeling problems rabunal et al 2007 aytek and kişi 2008 ghorbani et al 2010 kisi and guven 2010 mehr et al 2013 especially with the wavelet for streamflow forecasting mehr and kahya 2017 mehr and nourani 2017 ravansalar et al 2017 whether the inputs are preprocessed or not most studies applied gp as the final modeling method by giving the input and output and training the model gp is used as standalone modeling method gp can also be used as a sensitivity analysis tool because it has the ability to formulate and structure the equation used for forecasting which helps in selecting inputs after that imposed to another ddm this is similar to the model proposed by nourani et al 2011a b in which the authors proposed the wavelet genetic programming ann model where after the discrete wavelet transformation is applied gp applied as a sensitivity analysis to be then imposed into ann the wavelet transformation is performed by two forms continuous wavelet transform cwt and discrete wavelet transform dwt nourani et al 2014b reported that more than 80 of hydrological studies applied dwt as preprocessing this percentage is higher in streamflow forecasting because of the nature of the hydrologic time series which gathered discretely not continuously therefore dwt which is dyadic is appropriate for decomposing the time series allowing for the reconstruction of the original time series from the sub signals cwt represents the signal in many different and exact scales periods in that every scale has a number of coefficients equal to the same number of time steps therefore it is considered as redundant information especially when used for preprocessing to be imposed into ddm because increasing the number of redundant inputs deteriorates the model s ability to forecast galelli et al 2014 taormina et al 2016 dwt is dyadic and the coefficients are produced in a scale of 2m modes to capture the annual cycling in the daily mode a 28 day mode is used where the decomposition level is 8 which produced a nine sub time series the approximation and eight details for monthly data 24 mode approximately represents the annual time scale which produces a five sub time series nourani et al 2011a in cwt the daily data produces 365 and the monthly data produces 12 sub time series to capture the annual scale which can be a larger number if several variables are transformed using cwt several studies used cwt for forecasting streamflow by taking one scale coefficient or several scales based on data used for example adamowski 2008 used a standalone cwt method for river flow forecasting by applying the numerical method on coefficients of several scales chosen based on the cross wavelet analysis and compared the result with multivariate linear regression arima and ann adamowski 2008 reported good accuracy in one and two day leading time models but ann outperformed the proposed method with six days lead time cannas et al 2006 studied the use of cwt dwt and data partitioning as three different preprocessing techniques for predicting the river flow they found the first scale coefficients in the application of cwt provide the best accuracy but concluded that they do not result in a significant increase compared with the model that uses raw data shoaib et al 2014 investigated the effect of the selection of the mother wavelet function on the ability to forecast the runoff using only rainfall as input they used the best scale coefficients as identified by the highest correlation with the runoff they reported an increase in accuracy but the linear correlation coefficients are not sufficient to detect the rainfall runoff relation which is identified as nonlinear therefore a method with the ability to identify the most important scales contributing to the streamflow is necessary the most probable candidate is genetic programming gp the main objective of this study is to use the wct in order to identify the important variables and their scales in forecasting the downstream flow one month ahead accordingly a new hybrid model proposed in which only the important scales produced by cwt are chosen by mggp and imposed into ann 2 wavelet transformation wt a wavelet is defined as a small wave i e has a finite length with a zero average value it breaks the time series into its wavelets which are scaled and translated shifted version of the mother wavelet nason and von sachs 1999 it has the ability of providing time frequency representation of the signal as it provides time and frequency information simultaneously cannas et al 2006 it has the advantage of overcoming the issue of the non stationarity faced by the ddms daubechies 1990 anctil and tape 2004 shoaib et al 2014 alizadeh et al 2017 generally wt divided into two forms continuous wavelet transform cwt and discrete wavelet transform dwt only cwt is applied in this study the cwt is used for revealing the characteristics of the series in multi temporal scales the continuous wavelet coefficient w x a b is defined as 1 w x a b f t ψ a b t dt where 2 ψ a b t a 1 2 ψ t b a a b r a 0 where represents the complex conjugate of the function a and b are the parameters known as the scale and translation parameters respectively ψ t b a is the transformation function known as the mother wavelet 2 1 wavelet coherence transformation wct cwt has the ability to analyze the localized frequencies in the time series but is done only for one time series examining the relation between two time series in the time frequency domain is desirable investigating the regions with high common power in the time frequency domain is important if they have consistent phase relationship leading to a casualty conclusion therefore cross wavelet transformation xwt can be useful in this situation which is constructed using two cwts grinsted et al 2004 the xwt of two time series x and y is defined by jury et al 2002 as 3 w xy a b w x a b w y a b where represents the complex conjugate of the function the cross wavelet power is defined further by grinsted et al 2004 as w xy a b cross wavelet spectrum also provides a local phase difference estimation ℏ ϕ a b between the two time series for each point a b in the time frequency domain the estimation of the instantaneous phase difference between the two time series is possible by taking advantage of the phase difference being independent from the amplitude the phase difference is defined by jury et al 2002 as 4 ϕ b tan 1 a 1 a 2 im w xy a b da a 1 a 2 re w xy a b da where b corresponds to time a 1 a 2 represents the lower and upper scale limits respectively im and re represents the imaginary and real parts of w xy a b respectively xwt identifies only the regions with a high common power in the two time series wct has the advantage of measuring how coherent xwt is in the time frequency domain in that it measures how the two time series co varies and does not necessarily have high power grinsted et al 2004 wct is defined by torrence and webster 1999 as the squared absolute value of the smoothed cross wavelet spectrum normalized by the smoothed wavelet power spectra of the two time series 5 r b 2 a s a 1 w xy a b 2 s a 1 w x a b 2 s a 1 w y a b 2 where s is the smoothing operator eq 5 shows this equation is similar to the traditional correlation expression and wct as a localized correlation in the time frequency domain is useful grinsted et al 2004 the smoothing parameter is written as 6 s w s scale s time w a b where s scale refers to smoothing along the wavelet scale axis and s time refers to smoothing along the wavelet time axis the details of xwt and wct are presented by torrence and compo 1998 torrence and webster 1999 jury et al 2002 grinsted et al 2004 one of the most important issues in the wt in general is the cone of the influence coi as it affects the result in the edges coi is defined as the region in the spectrum of the wavelet in which the edge effect becomes important and that is because of the dealing with finite time series for the details of the coi refer to torrence and compo 1998 3 artificial neural network ann ann has several types classified according to the network structure and the learning algorithm feed forward back propagation or commonly known as feed forward neural network ffnn is the most commonly used structure fig 1 especially in hydrologic problems hsu et al 1995 govindaraju 2000a maier and dandy 2000 the expression of a three layered ffnn for obtaining an output postulated by alizadeh et al 2017 is 7 y k f 0 j 1 m n w kj f h i 1 n n w ij x i w j 0 w k 0 where i j and k are the neurons in the input hidden and output layers w ij is the weight of the connection between the neuron i th i e in the input layer and the neuron j th i e in the hidden layer w j 0 is the bias of the j th neuron in the hidden layer w kj is the weight of the connection between the j th and k th neurons in the output layer w k 0 is the bias of the k th neuron f h is the transfer function of the hidden layer f 0 is the activation function of the output layer x i is the i th input of the input layer y k is the computed value and n n and m n are the number of neurons in the input and hidden layers respectively in the back propagation algorithm the computed value y k is compared with the observed value y i e target in ann terminology and the differences i e error between y k and y are back propagated to the network to adjust the weights this process is known as the training process ann theory and applications in hydrology are reviewed by govindaraju 2000a b 4 genetic programming genetic programming gp which was first proposed by koza 1992 is a type of evolutionary algorithm it is one of the machine learning categories inspired by the biological evolution operations the operations include reproduction crossover mutation architecture changing operations patterned after gene duplication and gene deletion in nature the gp algorithm evaluates a user defined function known as fitness function that uses these operations and generates a population of computer programs randomly represented by a tree structure the best performing trees are bred together to generate a new population this process which mimics the darwinian evolution is repeated until the population solving the fitness function well is obtained searson 2015 gp can solve problems by extracting functional relationships between features called symbolic regression and grouping data into categories called classification gp has an advantage over traditional regression and other data driven models in that it does not require identification of the structure because it is a self structuring method mehr and kahya 2017 this characteristic important for reducing the complexity of the model by including only the variable contributes in the output a possible program is represented by a tree structure containing a root node an inner node s and leaves fig 2 shows an example of the representation of the tree structure of gp and the function of that example is written as 8 f x 1 x 2 3 5 x 1 cos x 2 exp cos x 1 gp method has major inputs including the following 1 pattern of training validation 2 fitness function such as root mean square error rmse 3 functional and terminal nodes or inner nodes and leaves for structure identification and 4 gp parameters for syntax tree formation mehr and nourani 2017 the functional set is chosen according to the complexity of the problem to be chosen the basic arithmetic parameters such as and can be used for a simple problem whereas complicated operators such as sin cos tan and exp are used for more complex problems gandomi et al 2015 several advancements have been suggested for the classical gp such as linear gp multi expression programming gene expression programming and multi gene gp mggp mehr and nourani 2017 mggp combines low depth gp trees linearly to improve classical gp fitness therefore mggp is expected to provide models simpler than those of the classical gp because it uses small trees suppose we have variable y to predict using several variables x 1 x 2 x n in mggp y is predicted by the summation of the weighted outputs of each tree gene and the bias as the additive model is considered searson 2015 based on fig 3 the mathematical expression can be written as 9 y b 0 b 1 t 1 b 2 t 2 b g t g where y is the predicted value t i is the vector of the output of the ith gene tree b 0 is the bias and b 1 b 2 b g are the regression coefficients and considered as weights of the gene tree the coefficients of the regression are determined using ordinary least square method for every individual mggp the advantage of depth restricted trees and the use of several other strategies i e expressional complexity and pareto tournament in mggp provides the possibility of having compact models which can be separable linearly hence they are automated post run simplified using symbolic math software the full details of mggp and full step description are presented by searson et al 2010 searson 2015 5 proposed hybrid model wmggpnn cwt has the advantage in the time frequency domain in that it transforms the time series in all scales and translations unlike the dwt dwt does the transformation in a dyadic form that scales are in 2m mode for example the annual cycling is captured from the daily data by 28 days mode in this case the decomposition level is 8 and that produced 9 sub time series one is the approximation and 8 details 24 mode is needed for capturing the annual cycling from the monthly data and that produce 5 sub time series nourani et al 2011a however the daily data produces 365 and the monthly data produces 12 sub time series to capture the annual scale in cwt using a several numbers of variable and transform them using cwt produces huge number of transformed sub time series for example in this study one of the developed models combinations see section 6 2 has 7 variables every variable is transformed with cwt using 128 scales which leads to 896 scales that every scale to be used as input vector in the case of this number of input vectors imposed into a ddm the models performance deteriorated as most of the inputs are redundant inputs galelli et al 2014 taormina et al 2016 mggp can produce an expression that contains only the input and contributes to the forecasting problem mehr and kahya 2017 therefore it can be utilized as a selection tool that selects the best scales that contribute to forecasting although it is a standalone forecasting method ann is a robust modeling method with a strong ability to capture non linearity the proposed method is as follows seasonality cycling and irregularity are captured by cwt which is applied to all variables with the highest possible scale in this study 128 the best scales are selected using mgpp by obtaining the expression from the model results the non linearity copes by imposing the best scales into ann models to forecast the one month ahead streamflow the schema of the proposed method is shown in fig 4 the details of the proposed model can be described in the following steps the lags of the used meteorological and hydrological variables v v 1 v 2 v n are transformed using cwt with the highest possible scale 128 scales for every variable 128 time series s v i s vi 1 s vi 2 s vi 128 i 1 2 n are obtained all the scales i e ts s v 1 s v 2 s v n are then entered into the mggp model as inputs and the current downstream flow ds t as output the evaluation criteria are obtained for the mggp model for evaluating its performance the important scales are obtained from mggp expression and gathered m s vij m where i represent the variable j represent the scale of that variable and m the importance of the scale only the important scales are then imposed to ann ds t f s vij 1 s vij 2 s vij m the performance of the models are then obtained to identify the best performed model 6 application 6 1 study area and data a basin named 1805 or goksu gokdere used as the study area fig 5 it is situated in a large basin namely seyhan basin and located between 35 34 44 e 36 06 45 e and 37 36 07 n 38 17 20 n in the southwest of turkey the physical characteristics of the basin are as follow area covers a surface of about 1790 km2 highly varying elevation 319 2967 m the longest water path 192 km and average slope equal to 23 the monthly streamflow measured at the goksu gokdere station situated on the outlet of the basin for from feb 1973 to sep 1994 was obtained from the general directory of water affairs which is part of the ministry of forests and water affairs to examine the effect of the contribution of several meteorological variables on streamflow forecasting upstream temperature rainfall and potential evapotranspiration were also used for the same period upstream data are collected from the same department and are measured in stations named goksu himmetli the monthly rainfall and temperature data of 17 stations were collected from the general directory of meteorology for the same period no meteorological station is located inside the basin under the study thus monthly temperature and rainfall were interpolated over a point in the middle of the basin and over the mean stream fig 5 the interpolation was implemented using inverse distance weighted idw with a power of three which was chosen after an intensive study of the best interpolation method using the 17 stations data around the interpolated point the examined methods are idw thiessen polygons trend surface analysis local polynomial interpolation thin plate spline and three kriging methods ordinary universal and simple ok uk and sk respectively each with several parameters and models evapotranspiration data were collected from the same department above but the missing data were more than the recorded data thus the evapotranspiration data were not used in this study the data obtained from the global data cruts3 23 jones and harris 2013 which is locally evaluated by hadi and tombul 2018 were used in this study for the same period the time series of the used variables is shown in fig 6 in this study the dataset was divided into two subsets in which 75 used for training from the and 25 used for validation characteristics of the datasets used in the study are summarized in table 1 6 2 methodology the first step in this study was to investigate the localized frequencies of the downstream in the time frequency domain which is implemented using the scalogram obtained by cwt the local correlation between the downstream and the meteorological and hydrological variables in the time frequency domain was examined by wct scalogram the phase difference was investigated for having an indication of the lag for every variable the scalograms of cwt and wct are shown in fig 7 to obtain the optimum lags of every variable which are used as input vector autocorrelation function acf and cross correlation function ccf can be applied because they have been applied widely sudheer et al 2002 shoaib et al 2015 however these functions are argued to be linear and do not capture the full relationship which is mostly in streamflow non linear nayak et al 2004 senthil kumar et al 2005 mehr and kahya 2017 another approach used for this purpose is the sequential addition of the lags the approach started the model with one lag and one lag was added in the second iteration up to a specific number of lags the number of lags is chosen according to the best performing model furundzic 1998 moosavi et al 2013 the present study applies both approaches first according to the autocorrelation function of the down streamflow ds and the cross correlation function between the ds and the used variable shown in fig 6 indicates that 2 2 and 1 lag for ds up streamflow up rainfall r respectively have a correlation with ds temperature t and pet have a significant correlation in the no lag time but the first lag in the correlation is small and not significant therefore the sequential addition method was also used that every variable was used alone for forecasting one month ahead of ds for ds up and rainfall the same lags of acf and ccf add a significant increase in the model forecasting for temperature and pet the first lag only contributes to the model several model combinations were developed after obtaining the optimum lag of every variable table 2 starting with a model that contains only the downstream all combinations include the lagged downstream because this component provides the auto regressivity and model performance worsened dramatically without this component the variables with their optimum lags were added to the lagged ds gradually to investigate their individual effect models that contain different combinations of variables are also developed to examine the combined effect of these variables before the data were used in the different schemas normalization was applied to all datasets to ensure they are within the same range increased accuracy and increased pace of calculations cannas et al 2006 nourani et al 2009 the general formula of normalization is written as 10 y b a x x min x max x min a where y is the normalized value of x x min and x max are the minimum and maximum values of the x variable respectively and a and b are the lowest and highest values to which the data is normalized respectively the normalization in this study used between 0 1 and 1 and these are the values of a and b respectively the standalone and best scale schemas were applied to evaluate the performance of the proposed model wtmggpann in the standalone schema the variables of the developed model combinations were imposed directly to ann and mggp the main advantage of this schema is that it enables assessment of the performance of the individual methods in which the data were imposed without pre processing except for normalization although mggp has the ability to capture the contributing inputs in the output all model combinations were entered one by one to address the influence of every variable on the forecasting performance in the second best scale schema the data were transformed using cwt for 128 scales and the scale with the highest correlation with the one month ahead downstream flow is identified and imposed as inputs to ann and mggp the data in the proposed model are transformed using cwt for 128 scales all scales are then entered as inputs to mggp to select the best scales finally the selected scales are imposed into ann for forecasting the one month ahead downstream flow some parameters in ann and mggp should be considered to obtain models with high performance for example the number of hidden layers and the number of neurons in the hidden layer are important in ann in this study the three layered network i e input layer hidden layer and output layer is used because it was approved as sufficient for such problems hornik et al 1989 kişi 2007 the number of the neurons in the hidden layer was obtained by trial and error for every layout the structure of ann in every layout is listed in table 3 several parameters in mggp are also important such as the number of population number of maximum generation and mutation rate mehr and kahya 2017 mehr and nourani 2017 the parameters of the mggp used in this study are tabulated in table 4 6 3 results and discussion the main advantage of wt on the time series and in hydrological data is to overcome the issue of non stationarity confronted by ddms another important advantage is the representation of the time series in time frequency domain that provides a robust analysis of the localized periodicities fig 7 a shows the cwt scalogram of the downstream flow the highest power is approximately scale 12 which is the most powerful waveband the data used are the monthly data and this is the annual periodicity this line around the 12 period indicates that the annual signal is stationary the wct scalograms are shown in fig 7 b e to examine the coherence i e the covariance in the time frequency domain between the downstream flow and each of the used variables the downstream and upstream flows have a high localized correlation coherence in all considered scales fig 7 b which agrees with the high correlation obtained from the cross correlation fig 6 grinsted et al 2004 the phase difference shows in phase that no phase difference is found between the two variables fig 7 b which is expected because most of the downstream is a result of the upstream particularly the base flow that mostly flows from the upstream although the area of the basin is large and other affecting variables could have more contributions the upstream gauge location is not in the longest water path but is located in the middle regarding the phase similarity the monthly data used should have no difference as the difference expected between the upstream and downstream in the smallest time scales daily or smaller in the wct of the downstream and rainfall fig 7 c scattered significant localized correlation in the scales is less than 7 months however a harmonic coherence is observed for the scales between 11 months and 30 months i e approximately one year to 2 5 years the highest coherence is seen in scale 12 which is the annual scale after the scale of 60 months 5 years the significant coherence is also present although most of it is under the cone of influence the phase difference shows a lag approximately 450 of the downstream from the rainfall due to the contribution of the rainfall this lag is small because of the monthly time scale in which the rainfall contribution should be seen higher in smaller time scales the temperature shows similar coherence in the scales less than 7 months fig 7 d a harmonic coherence is detected between 9 and 15 month scales with the highest being 12 months i e annual the phase difference indicates that the approximately 220 2700 lag between the downstream and the temperature may be because the effect of the temperature needs more time to be reflected on the streamflow in other words the effect of the temperature takes longer time to influence the streamflow not like the rainfall and upstream which needs less time pet shows a pattern similar to the temperature and this is because pet is a function of the temperature and is calculated using it most scales are important in analyzing and modeling the relationship between the downstream and the other variables therefore including all contributing scales would capture more seasonality cycling and irregularity before assessing the proposed model which proposed all scales a standalone layout was implemented the variables used for the forecasting of the downstream one month ahead are entered to ann and mggp directly without wavelet transformation to evaluate the performance of the ddms on the bare variables in ann and mggp the result indicates the model containing only the downstream has poor performance compared with the other combinations fig 8 in contrast of adding the rainfall which does not improve the model adding temperature and pet increased the model performance dramatically but the performance remained not higher than the performance of the model containing all variables which is the best performing model lagging the rainfall for one month is excessive because the effect of the rainfall does not require a long time to be reflected on the streamflow the effect of the temperature and pet needs more time to see its influence on the streamflow which is reflected in the monthly observations ann and mggp can deal with the non linearity of the relationship between the inputs and outputs and are both autoregressive approaches therefore the similar results of both approaches for all the models are obvious fig 8 ann and mggp standalone may not capture the seasonality of the data if no preprocessing is applied to the data cannas et al 2006 therefore wt as a solution to overcome the stationarity problem is applied as a pre processing approach cwt can be applied in the streamflow forecasting problems by choosing the first scale cannas et al 2006 or the best scale which is chosen as the scales have the highest correlation shoaib et al 2014 the best scale method is approved as the performance of the models increases shoaib et al 2014 it is adopted in this study for comparison the wavelet function and the scale are two important issues that must be considered in wt analysis in the best scale layout the best scale must be considered as input to ann and mggp the best scale was imposed on ann after the cwt transformation using several wavelet mother functions to choose the best wavelet function the considered functions are haar dmey db2 db3 db4 db5 db6 db7 db8 sym2 sym3 sym4 sym5 sym6 sym7 sym8 coif1 coif2 coif3 and coif4 the best performing wavelet function is found to be haar fig 9 this finding agrees with nourani et al 2009 nourani et al 2011b the haar wavelet function was used for the all the cwt analysis after this point which starts with the best scale layout of mggp table 5 shows that the results of the two approaches i e ann and mggp are close to each other with the slight outperformance to the favor of ann no large difference is observed between the performance of the model combinations in comparison with the first combination which contains only downstream lags the exception is that adding pet to the models noticeably improves the performance the best scale layout increased the performance of almost all the model combinations compared with the standalone layout the fact of having the highest correlated scale could improve the performance but with not much seasonality information extracted because one scale is not enough to obtain the hidden information in the time series the wct analysis fig 7b e shows that several scales have a correlation with the streamflow the correlation used here is a linear correlation in which the non linear relationship between the streamflow and the other variables cannot be captured using only one scale from the cwt analysis is not preferable because of the aforementioned reasons however the use of all scales leads to redundant information which causes the cost of calculation to dramatically increase and the performance of the model to decrease galelli et al 2014 taormina et al 2016 therefore the wmggpnn model is proposed this model enters all the scales to the mggp models to select the contributing scales and impose only these scales to ann the selected scales by the mggp model are listed in table 6 table 7 which shows the results of the proposed model indicates high performance with high r2 and low rmse i e the highest is 0 960 and 6 602 m3 s for validation subset the percentage of change in r2 and rmse of the proposed model from the best scale mggp and ann and full scale mggp are shown in fig 10 to compare the performance of the proposed model according to the figure the relative change percentage shows an increase in r2 and a decrease in the rmse in all the models the fitted versus the observed streamflow values plot of the best performing model combination fig 11 shows some of the scattered values around the best fit line in all models except the proposed model in which most of the points lay over the line therefore the proposed model improves the performance of streamflow forecasting the proposed model outperformed the best scale of mggp and ann because several scales are included as inputs to ann for example in the first model combination in the best scale layout the number of scales is two i e one for every variable but the number of scales obtained by mggp after entering 265 scales i e all the scales for two variables is eighteen table 3 the increase in performance can be attributed to the increase in the number of scales included in which more seasonality cycling and irregularity information are extracted one of the most important aspects of streamflow forecasting is the ability of the model to capture the extreme peak values therefore the peak flow forecasting ability comparison was conducted using 2 5 of the highest flows given that the standalone layout models are outperformed by the best scale layout models they were excluded from the comparison in the peak flow forecasting the proposed model demonstrates high accuracy in the forecasting of the peak flows being the highest in the model combination which includes downstream and pet with r2 that equals 0 998 and rmse that equals 1 231 m3 s table 8 the highest peak flow forecasting performance is obtained in two models the model includes downstream flow and temperature and the model includes downstream upstream and pet in the time of the dominating underestimation of the peak values of the streamflow using wannbs wmggpbs and wmggpfs the proposed model wmggpnn overlays the observed data almost perfectly fig 12 therefore including several scales can help in extracting the seasonality information and using ann which has a robust ability in dealing with the non linearity and auto regression improved the models to a level that can capture most of the fluctuations well cwt can be applied as a pre processing tool for forecasting the streamflow but using ddm alone may not be good cannas et al 2006 adamowski 2008 it can also improve the runoff modeling in the case of using rainfall only as inputs by choosing the best scale shoaib et al 2014 in this study the best scale layout improved the performance of the standalone layout but not significantly this increase is expected because the high linear correlation is normal but much of the information is not extracted and the advantage of applying cwt is not well utilized including more scales by applying mggp to the output of the cwt as a selection tool leads the selection of the most contributing scales and throws the redundant scales using these scales can help in extracting most of the hidden information with ann and in improving model performance dramatically especially in forecasting the peak values baratti et al 2003 cannas et al 2006 reported that the monthly rainfall and temperature at the gauge stations are not correlated with the behavior of the monthly runoff this study reveals that all the layouts which include meteorological and hydrological variables recorded in the catchment have a correlation with the streamflow because they improve the performance of the models 7 summary and conclusion a hydrological process such as a streamflow is characterized by high non linearity seasonality and non stationarity data driven models such as ann and mggp have the ability to cope with non linearity but not stationarity and seasonality wt is the pre processing techniques applied to deal with non stationarity and seasonality by transforming data into a time frequency domain applying cwt differs from applying dwt because a large number of scales coefficients are produced and most are redundant and deteriorate the performance of any ddm therefore this study used mggp as a selection tool that selects only the contributing scales to be imposed on ann an application is implemented by choosing a basin located in the southeast of turkey to improve the ability of the proposed model to increase the performance of the forecasting of monthly streamflow the monthly streamflow measured in the outlet of the basin is used upstream flow rainfall temperature and pet are added to the models to investigate their contribution to the downstream flow before the development of the model wct was conducted between the downstream flow and other variables to investigate the relationship in the time frequency domain given that the output of the forecasting is one month ahead downstream flow three layouts were considered in this study standalone which refers to data imposed to ann and mggp without pre processing best scale which refers to the cwt scale with the highest correlation with the downstream flow used as input to the ann and mggp and full scale which refers to the mggp used as a tool to select the contributing scales imposed on ann the results showed that upstream is correlated highly with downstream in almost all scales whereas the other variables are mostly locally correlated with the 12 month scales i e annual the temperature correlation pattern is similar to one of the pet because the latter is derived from the former the standalone forecasting layout exhibited poor performance for all model combinations the best scale layout improved the performance of the models slightly the proposed model wmggpnn improved the forecasting ability dramatically leading to a model that can capture the peak values excellently according to the wct analysis the streamflow is correlated with upstream rainfall temperature and pet variables in several scales although it shows no high linear correlation the standalone layout performed poorly because the relationship between the inputs and output is characterized by non linearity which can be handled by ddms seasonality irregularity and non stationarity the need for wt can be observed from this point using the best scale layout improved the performance because of the high linear correlation which is higher than the original variable but using linear correlation is insufficient in the existence of non linearity the main advantage of applying wt is to uncover the seasonality non stationarity and cycling information hidden in the original variable however one scale is not sufficient to carry this information therefore the proposed model wmggp which after the cwt analysis selects several scales that contribute to the output improved the performance dramatically the most important point is that the fluctuation in the streamflow as represented by peak values is well forecasted because the chosen scales include sufficient seasonality and irregularity information according to this study the restrictions of using cwt as a pre processing tool because of the high number of scales and the redundant information for predicting the monthly streamflow are avoidable this study applies only to monthly streamflow which is less predictable than the daily streamflow applying the same methodology to the daily streamflow is recommended acknowledgments the authors wish to thank devlet su i̇şleri general department of water affairs ministry of forest and water affairs and devlet meteroloji i̇şleri general department of meteorological affairs for providing the data necessary to complete this work the research is part of the project bap 1604f165 funded by anadolu university 
7302,streamflow is an essential component of the hydrologic cycle in the regional and global scale and the main source of fresh water supply it is highly associated with natural disasters such as droughts and floods therefore accurate streamflow forecasting is essential forecasting streamflow in general and monthly streamflow in particular is a complex process that cannot be handled by data driven models ddms only and requires pre processing wavelet transformation is a pre processing technique however application of continuous wavelet transformation cwt produces many scales that cause deterioration in the performance of any ddm because of the high number of redundant variables this study proposes multigene genetic programming mggp as a selection tool after the cwt analysis it selects important scales to be imposed into the artificial neural network ann a basin located in the southeast of turkey is selected as case study to prove the forecasting ability of the proposed model one month ahead downstream flow is used as output and downstream flow upstream rainfall temperature and potential evapotranspiration with associated lags are used as inputs before modeling wavelet coherence transformation wct analysis was conducted to analyze the relationship between variables in the time frequency domain several combinations were developed to investigate the effect of the variables on streamflow forecasting the results indicated a high localized correlation between the streamflow and other variables especially the upstream in the models of the standalone layout where the data were entered to ann and mggp without cwt the performance is found poor in the best scale layout where the best scale of the cwt identified as the highest correlated scale is chosen and enters to ann and mggp the performance increased slightly using the proposed model the performance improved dramatically particularly in forecasting the peak values because of the inclusion of several scales in which seasonality and irregularity can be captured using hydrological and meteorological variables also improved the ability to forecast the streamflow keywords wavelet coherence transformation continuous wavelet transformation artificial neural network data driven models 1 introduction streamflow is an essential component of the hydrologic cycle in the regional and global scale makkeasorn et al 2008 liu et al 2014 it is the main source of fresh water supply and is associated highly with natural disasters such as droughts and floods therefore accurate short term and long term forecasting of streamflow is essential for efficient water resources management particularly in regions vulnerable to floods and droughts kisi and cimen 2011 forecasting streamflow is a complicated task ravansalar et al 2017 because of the complexity of the process yaseen et al 2015 mehr and kahya 2017 this complexity is due mostly to the non stationarity and non linearity in the relationship between the streamflow and basin characteristics nourani et al 2011a mehr and kahya 2017 data driven models ddms can be used for modeling streamflow because they have the ability to deal with non linearity and map the relationship between the streamflow and factors that drive it ddms also obtain the optimum solution between inputs and outputs without involving the physical process solomatine et al 2008 many applications of forecasting streamflow use ddms including artificial neural network ann support vector machines adaptive neuro fuzzy inference system and genetic programming gp tokar and johnson 1999 kişi 2007 kisi et al 2012 nourani et al 2013 mehr et al 2015 abdollahi et al 2017 mehr and kahya 2017 mehr and nourani 2017 ddms can deal with the nonlinearity and non stationarity in the mean and variance elements but the main shortcoming of ddms is that they may not be able to cope with non stationarity fluctuation nourani et al 2011b if no preprocessing is applied to the data cannas et al 2006 one of the solutions is to apply the wavelet with the ability to cope with the seasonal cycling nonstationary component of the time series daubechies 1990 torrence and compo 1998 anctil and tape 2004 nourani et al 2011b a wavelet has good ability in representing the signal locally in time and frequency domains the decomposition of the nonstationary time series using wavelet into several scales lee and yamamoto 1994 extracts historical hidden information in time and frequency domains rajaee et al 2010 shoaib et al 2014 a wavelet with various ddm types has been applied widely in streamflow forecasting shiri and kisi 2010 kisi and cimen 2011 adamowski and prasher 2012 badrzadeh et al 2013 komasi and sharghi 2016 ravansalar et al 2017 and especially with the ann approach kişi and kerem cigizoglu 2007 carcano et al 2008 kişi 2009 adamowski and sun 2010 araghinejad et al 2011 pramanik et al 2011 adamowski et al 2012 isik et al 2013 nourani and parhizkar 2013 nourani et al 2014a gp is another ddm identified as robust explicit method that can be used for modeling dorado et al 2003 mehr et al 2013 mehr et al 2014 de falco et al 2005 reported several advantages of gp over ann 1 gp generates an explicit model understandable by humans 2 it automatically discovers the structure of the model by utilizing the given data 3 it has an adaptive evolutionary search ability that does not trap suboptimal unsatisfactory local solutions and 4 it has no specific knowledge thus gp has been applied widely in hydrological modeling problems rabunal et al 2007 aytek and kişi 2008 ghorbani et al 2010 kisi and guven 2010 mehr et al 2013 especially with the wavelet for streamflow forecasting mehr and kahya 2017 mehr and nourani 2017 ravansalar et al 2017 whether the inputs are preprocessed or not most studies applied gp as the final modeling method by giving the input and output and training the model gp is used as standalone modeling method gp can also be used as a sensitivity analysis tool because it has the ability to formulate and structure the equation used for forecasting which helps in selecting inputs after that imposed to another ddm this is similar to the model proposed by nourani et al 2011a b in which the authors proposed the wavelet genetic programming ann model where after the discrete wavelet transformation is applied gp applied as a sensitivity analysis to be then imposed into ann the wavelet transformation is performed by two forms continuous wavelet transform cwt and discrete wavelet transform dwt nourani et al 2014b reported that more than 80 of hydrological studies applied dwt as preprocessing this percentage is higher in streamflow forecasting because of the nature of the hydrologic time series which gathered discretely not continuously therefore dwt which is dyadic is appropriate for decomposing the time series allowing for the reconstruction of the original time series from the sub signals cwt represents the signal in many different and exact scales periods in that every scale has a number of coefficients equal to the same number of time steps therefore it is considered as redundant information especially when used for preprocessing to be imposed into ddm because increasing the number of redundant inputs deteriorates the model s ability to forecast galelli et al 2014 taormina et al 2016 dwt is dyadic and the coefficients are produced in a scale of 2m modes to capture the annual cycling in the daily mode a 28 day mode is used where the decomposition level is 8 which produced a nine sub time series the approximation and eight details for monthly data 24 mode approximately represents the annual time scale which produces a five sub time series nourani et al 2011a in cwt the daily data produces 365 and the monthly data produces 12 sub time series to capture the annual scale which can be a larger number if several variables are transformed using cwt several studies used cwt for forecasting streamflow by taking one scale coefficient or several scales based on data used for example adamowski 2008 used a standalone cwt method for river flow forecasting by applying the numerical method on coefficients of several scales chosen based on the cross wavelet analysis and compared the result with multivariate linear regression arima and ann adamowski 2008 reported good accuracy in one and two day leading time models but ann outperformed the proposed method with six days lead time cannas et al 2006 studied the use of cwt dwt and data partitioning as three different preprocessing techniques for predicting the river flow they found the first scale coefficients in the application of cwt provide the best accuracy but concluded that they do not result in a significant increase compared with the model that uses raw data shoaib et al 2014 investigated the effect of the selection of the mother wavelet function on the ability to forecast the runoff using only rainfall as input they used the best scale coefficients as identified by the highest correlation with the runoff they reported an increase in accuracy but the linear correlation coefficients are not sufficient to detect the rainfall runoff relation which is identified as nonlinear therefore a method with the ability to identify the most important scales contributing to the streamflow is necessary the most probable candidate is genetic programming gp the main objective of this study is to use the wct in order to identify the important variables and their scales in forecasting the downstream flow one month ahead accordingly a new hybrid model proposed in which only the important scales produced by cwt are chosen by mggp and imposed into ann 2 wavelet transformation wt a wavelet is defined as a small wave i e has a finite length with a zero average value it breaks the time series into its wavelets which are scaled and translated shifted version of the mother wavelet nason and von sachs 1999 it has the ability of providing time frequency representation of the signal as it provides time and frequency information simultaneously cannas et al 2006 it has the advantage of overcoming the issue of the non stationarity faced by the ddms daubechies 1990 anctil and tape 2004 shoaib et al 2014 alizadeh et al 2017 generally wt divided into two forms continuous wavelet transform cwt and discrete wavelet transform dwt only cwt is applied in this study the cwt is used for revealing the characteristics of the series in multi temporal scales the continuous wavelet coefficient w x a b is defined as 1 w x a b f t ψ a b t dt where 2 ψ a b t a 1 2 ψ t b a a b r a 0 where represents the complex conjugate of the function a and b are the parameters known as the scale and translation parameters respectively ψ t b a is the transformation function known as the mother wavelet 2 1 wavelet coherence transformation wct cwt has the ability to analyze the localized frequencies in the time series but is done only for one time series examining the relation between two time series in the time frequency domain is desirable investigating the regions with high common power in the time frequency domain is important if they have consistent phase relationship leading to a casualty conclusion therefore cross wavelet transformation xwt can be useful in this situation which is constructed using two cwts grinsted et al 2004 the xwt of two time series x and y is defined by jury et al 2002 as 3 w xy a b w x a b w y a b where represents the complex conjugate of the function the cross wavelet power is defined further by grinsted et al 2004 as w xy a b cross wavelet spectrum also provides a local phase difference estimation ℏ ϕ a b between the two time series for each point a b in the time frequency domain the estimation of the instantaneous phase difference between the two time series is possible by taking advantage of the phase difference being independent from the amplitude the phase difference is defined by jury et al 2002 as 4 ϕ b tan 1 a 1 a 2 im w xy a b da a 1 a 2 re w xy a b da where b corresponds to time a 1 a 2 represents the lower and upper scale limits respectively im and re represents the imaginary and real parts of w xy a b respectively xwt identifies only the regions with a high common power in the two time series wct has the advantage of measuring how coherent xwt is in the time frequency domain in that it measures how the two time series co varies and does not necessarily have high power grinsted et al 2004 wct is defined by torrence and webster 1999 as the squared absolute value of the smoothed cross wavelet spectrum normalized by the smoothed wavelet power spectra of the two time series 5 r b 2 a s a 1 w xy a b 2 s a 1 w x a b 2 s a 1 w y a b 2 where s is the smoothing operator eq 5 shows this equation is similar to the traditional correlation expression and wct as a localized correlation in the time frequency domain is useful grinsted et al 2004 the smoothing parameter is written as 6 s w s scale s time w a b where s scale refers to smoothing along the wavelet scale axis and s time refers to smoothing along the wavelet time axis the details of xwt and wct are presented by torrence and compo 1998 torrence and webster 1999 jury et al 2002 grinsted et al 2004 one of the most important issues in the wt in general is the cone of the influence coi as it affects the result in the edges coi is defined as the region in the spectrum of the wavelet in which the edge effect becomes important and that is because of the dealing with finite time series for the details of the coi refer to torrence and compo 1998 3 artificial neural network ann ann has several types classified according to the network structure and the learning algorithm feed forward back propagation or commonly known as feed forward neural network ffnn is the most commonly used structure fig 1 especially in hydrologic problems hsu et al 1995 govindaraju 2000a maier and dandy 2000 the expression of a three layered ffnn for obtaining an output postulated by alizadeh et al 2017 is 7 y k f 0 j 1 m n w kj f h i 1 n n w ij x i w j 0 w k 0 where i j and k are the neurons in the input hidden and output layers w ij is the weight of the connection between the neuron i th i e in the input layer and the neuron j th i e in the hidden layer w j 0 is the bias of the j th neuron in the hidden layer w kj is the weight of the connection between the j th and k th neurons in the output layer w k 0 is the bias of the k th neuron f h is the transfer function of the hidden layer f 0 is the activation function of the output layer x i is the i th input of the input layer y k is the computed value and n n and m n are the number of neurons in the input and hidden layers respectively in the back propagation algorithm the computed value y k is compared with the observed value y i e target in ann terminology and the differences i e error between y k and y are back propagated to the network to adjust the weights this process is known as the training process ann theory and applications in hydrology are reviewed by govindaraju 2000a b 4 genetic programming genetic programming gp which was first proposed by koza 1992 is a type of evolutionary algorithm it is one of the machine learning categories inspired by the biological evolution operations the operations include reproduction crossover mutation architecture changing operations patterned after gene duplication and gene deletion in nature the gp algorithm evaluates a user defined function known as fitness function that uses these operations and generates a population of computer programs randomly represented by a tree structure the best performing trees are bred together to generate a new population this process which mimics the darwinian evolution is repeated until the population solving the fitness function well is obtained searson 2015 gp can solve problems by extracting functional relationships between features called symbolic regression and grouping data into categories called classification gp has an advantage over traditional regression and other data driven models in that it does not require identification of the structure because it is a self structuring method mehr and kahya 2017 this characteristic important for reducing the complexity of the model by including only the variable contributes in the output a possible program is represented by a tree structure containing a root node an inner node s and leaves fig 2 shows an example of the representation of the tree structure of gp and the function of that example is written as 8 f x 1 x 2 3 5 x 1 cos x 2 exp cos x 1 gp method has major inputs including the following 1 pattern of training validation 2 fitness function such as root mean square error rmse 3 functional and terminal nodes or inner nodes and leaves for structure identification and 4 gp parameters for syntax tree formation mehr and nourani 2017 the functional set is chosen according to the complexity of the problem to be chosen the basic arithmetic parameters such as and can be used for a simple problem whereas complicated operators such as sin cos tan and exp are used for more complex problems gandomi et al 2015 several advancements have been suggested for the classical gp such as linear gp multi expression programming gene expression programming and multi gene gp mggp mehr and nourani 2017 mggp combines low depth gp trees linearly to improve classical gp fitness therefore mggp is expected to provide models simpler than those of the classical gp because it uses small trees suppose we have variable y to predict using several variables x 1 x 2 x n in mggp y is predicted by the summation of the weighted outputs of each tree gene and the bias as the additive model is considered searson 2015 based on fig 3 the mathematical expression can be written as 9 y b 0 b 1 t 1 b 2 t 2 b g t g where y is the predicted value t i is the vector of the output of the ith gene tree b 0 is the bias and b 1 b 2 b g are the regression coefficients and considered as weights of the gene tree the coefficients of the regression are determined using ordinary least square method for every individual mggp the advantage of depth restricted trees and the use of several other strategies i e expressional complexity and pareto tournament in mggp provides the possibility of having compact models which can be separable linearly hence they are automated post run simplified using symbolic math software the full details of mggp and full step description are presented by searson et al 2010 searson 2015 5 proposed hybrid model wmggpnn cwt has the advantage in the time frequency domain in that it transforms the time series in all scales and translations unlike the dwt dwt does the transformation in a dyadic form that scales are in 2m mode for example the annual cycling is captured from the daily data by 28 days mode in this case the decomposition level is 8 and that produced 9 sub time series one is the approximation and 8 details 24 mode is needed for capturing the annual cycling from the monthly data and that produce 5 sub time series nourani et al 2011a however the daily data produces 365 and the monthly data produces 12 sub time series to capture the annual scale in cwt using a several numbers of variable and transform them using cwt produces huge number of transformed sub time series for example in this study one of the developed models combinations see section 6 2 has 7 variables every variable is transformed with cwt using 128 scales which leads to 896 scales that every scale to be used as input vector in the case of this number of input vectors imposed into a ddm the models performance deteriorated as most of the inputs are redundant inputs galelli et al 2014 taormina et al 2016 mggp can produce an expression that contains only the input and contributes to the forecasting problem mehr and kahya 2017 therefore it can be utilized as a selection tool that selects the best scales that contribute to forecasting although it is a standalone forecasting method ann is a robust modeling method with a strong ability to capture non linearity the proposed method is as follows seasonality cycling and irregularity are captured by cwt which is applied to all variables with the highest possible scale in this study 128 the best scales are selected using mgpp by obtaining the expression from the model results the non linearity copes by imposing the best scales into ann models to forecast the one month ahead streamflow the schema of the proposed method is shown in fig 4 the details of the proposed model can be described in the following steps the lags of the used meteorological and hydrological variables v v 1 v 2 v n are transformed using cwt with the highest possible scale 128 scales for every variable 128 time series s v i s vi 1 s vi 2 s vi 128 i 1 2 n are obtained all the scales i e ts s v 1 s v 2 s v n are then entered into the mggp model as inputs and the current downstream flow ds t as output the evaluation criteria are obtained for the mggp model for evaluating its performance the important scales are obtained from mggp expression and gathered m s vij m where i represent the variable j represent the scale of that variable and m the importance of the scale only the important scales are then imposed to ann ds t f s vij 1 s vij 2 s vij m the performance of the models are then obtained to identify the best performed model 6 application 6 1 study area and data a basin named 1805 or goksu gokdere used as the study area fig 5 it is situated in a large basin namely seyhan basin and located between 35 34 44 e 36 06 45 e and 37 36 07 n 38 17 20 n in the southwest of turkey the physical characteristics of the basin are as follow area covers a surface of about 1790 km2 highly varying elevation 319 2967 m the longest water path 192 km and average slope equal to 23 the monthly streamflow measured at the goksu gokdere station situated on the outlet of the basin for from feb 1973 to sep 1994 was obtained from the general directory of water affairs which is part of the ministry of forests and water affairs to examine the effect of the contribution of several meteorological variables on streamflow forecasting upstream temperature rainfall and potential evapotranspiration were also used for the same period upstream data are collected from the same department and are measured in stations named goksu himmetli the monthly rainfall and temperature data of 17 stations were collected from the general directory of meteorology for the same period no meteorological station is located inside the basin under the study thus monthly temperature and rainfall were interpolated over a point in the middle of the basin and over the mean stream fig 5 the interpolation was implemented using inverse distance weighted idw with a power of three which was chosen after an intensive study of the best interpolation method using the 17 stations data around the interpolated point the examined methods are idw thiessen polygons trend surface analysis local polynomial interpolation thin plate spline and three kriging methods ordinary universal and simple ok uk and sk respectively each with several parameters and models evapotranspiration data were collected from the same department above but the missing data were more than the recorded data thus the evapotranspiration data were not used in this study the data obtained from the global data cruts3 23 jones and harris 2013 which is locally evaluated by hadi and tombul 2018 were used in this study for the same period the time series of the used variables is shown in fig 6 in this study the dataset was divided into two subsets in which 75 used for training from the and 25 used for validation characteristics of the datasets used in the study are summarized in table 1 6 2 methodology the first step in this study was to investigate the localized frequencies of the downstream in the time frequency domain which is implemented using the scalogram obtained by cwt the local correlation between the downstream and the meteorological and hydrological variables in the time frequency domain was examined by wct scalogram the phase difference was investigated for having an indication of the lag for every variable the scalograms of cwt and wct are shown in fig 7 to obtain the optimum lags of every variable which are used as input vector autocorrelation function acf and cross correlation function ccf can be applied because they have been applied widely sudheer et al 2002 shoaib et al 2015 however these functions are argued to be linear and do not capture the full relationship which is mostly in streamflow non linear nayak et al 2004 senthil kumar et al 2005 mehr and kahya 2017 another approach used for this purpose is the sequential addition of the lags the approach started the model with one lag and one lag was added in the second iteration up to a specific number of lags the number of lags is chosen according to the best performing model furundzic 1998 moosavi et al 2013 the present study applies both approaches first according to the autocorrelation function of the down streamflow ds and the cross correlation function between the ds and the used variable shown in fig 6 indicates that 2 2 and 1 lag for ds up streamflow up rainfall r respectively have a correlation with ds temperature t and pet have a significant correlation in the no lag time but the first lag in the correlation is small and not significant therefore the sequential addition method was also used that every variable was used alone for forecasting one month ahead of ds for ds up and rainfall the same lags of acf and ccf add a significant increase in the model forecasting for temperature and pet the first lag only contributes to the model several model combinations were developed after obtaining the optimum lag of every variable table 2 starting with a model that contains only the downstream all combinations include the lagged downstream because this component provides the auto regressivity and model performance worsened dramatically without this component the variables with their optimum lags were added to the lagged ds gradually to investigate their individual effect models that contain different combinations of variables are also developed to examine the combined effect of these variables before the data were used in the different schemas normalization was applied to all datasets to ensure they are within the same range increased accuracy and increased pace of calculations cannas et al 2006 nourani et al 2009 the general formula of normalization is written as 10 y b a x x min x max x min a where y is the normalized value of x x min and x max are the minimum and maximum values of the x variable respectively and a and b are the lowest and highest values to which the data is normalized respectively the normalization in this study used between 0 1 and 1 and these are the values of a and b respectively the standalone and best scale schemas were applied to evaluate the performance of the proposed model wtmggpann in the standalone schema the variables of the developed model combinations were imposed directly to ann and mggp the main advantage of this schema is that it enables assessment of the performance of the individual methods in which the data were imposed without pre processing except for normalization although mggp has the ability to capture the contributing inputs in the output all model combinations were entered one by one to address the influence of every variable on the forecasting performance in the second best scale schema the data were transformed using cwt for 128 scales and the scale with the highest correlation with the one month ahead downstream flow is identified and imposed as inputs to ann and mggp the data in the proposed model are transformed using cwt for 128 scales all scales are then entered as inputs to mggp to select the best scales finally the selected scales are imposed into ann for forecasting the one month ahead downstream flow some parameters in ann and mggp should be considered to obtain models with high performance for example the number of hidden layers and the number of neurons in the hidden layer are important in ann in this study the three layered network i e input layer hidden layer and output layer is used because it was approved as sufficient for such problems hornik et al 1989 kişi 2007 the number of the neurons in the hidden layer was obtained by trial and error for every layout the structure of ann in every layout is listed in table 3 several parameters in mggp are also important such as the number of population number of maximum generation and mutation rate mehr and kahya 2017 mehr and nourani 2017 the parameters of the mggp used in this study are tabulated in table 4 6 3 results and discussion the main advantage of wt on the time series and in hydrological data is to overcome the issue of non stationarity confronted by ddms another important advantage is the representation of the time series in time frequency domain that provides a robust analysis of the localized periodicities fig 7 a shows the cwt scalogram of the downstream flow the highest power is approximately scale 12 which is the most powerful waveband the data used are the monthly data and this is the annual periodicity this line around the 12 period indicates that the annual signal is stationary the wct scalograms are shown in fig 7 b e to examine the coherence i e the covariance in the time frequency domain between the downstream flow and each of the used variables the downstream and upstream flows have a high localized correlation coherence in all considered scales fig 7 b which agrees with the high correlation obtained from the cross correlation fig 6 grinsted et al 2004 the phase difference shows in phase that no phase difference is found between the two variables fig 7 b which is expected because most of the downstream is a result of the upstream particularly the base flow that mostly flows from the upstream although the area of the basin is large and other affecting variables could have more contributions the upstream gauge location is not in the longest water path but is located in the middle regarding the phase similarity the monthly data used should have no difference as the difference expected between the upstream and downstream in the smallest time scales daily or smaller in the wct of the downstream and rainfall fig 7 c scattered significant localized correlation in the scales is less than 7 months however a harmonic coherence is observed for the scales between 11 months and 30 months i e approximately one year to 2 5 years the highest coherence is seen in scale 12 which is the annual scale after the scale of 60 months 5 years the significant coherence is also present although most of it is under the cone of influence the phase difference shows a lag approximately 450 of the downstream from the rainfall due to the contribution of the rainfall this lag is small because of the monthly time scale in which the rainfall contribution should be seen higher in smaller time scales the temperature shows similar coherence in the scales less than 7 months fig 7 d a harmonic coherence is detected between 9 and 15 month scales with the highest being 12 months i e annual the phase difference indicates that the approximately 220 2700 lag between the downstream and the temperature may be because the effect of the temperature needs more time to be reflected on the streamflow in other words the effect of the temperature takes longer time to influence the streamflow not like the rainfall and upstream which needs less time pet shows a pattern similar to the temperature and this is because pet is a function of the temperature and is calculated using it most scales are important in analyzing and modeling the relationship between the downstream and the other variables therefore including all contributing scales would capture more seasonality cycling and irregularity before assessing the proposed model which proposed all scales a standalone layout was implemented the variables used for the forecasting of the downstream one month ahead are entered to ann and mggp directly without wavelet transformation to evaluate the performance of the ddms on the bare variables in ann and mggp the result indicates the model containing only the downstream has poor performance compared with the other combinations fig 8 in contrast of adding the rainfall which does not improve the model adding temperature and pet increased the model performance dramatically but the performance remained not higher than the performance of the model containing all variables which is the best performing model lagging the rainfall for one month is excessive because the effect of the rainfall does not require a long time to be reflected on the streamflow the effect of the temperature and pet needs more time to see its influence on the streamflow which is reflected in the monthly observations ann and mggp can deal with the non linearity of the relationship between the inputs and outputs and are both autoregressive approaches therefore the similar results of both approaches for all the models are obvious fig 8 ann and mggp standalone may not capture the seasonality of the data if no preprocessing is applied to the data cannas et al 2006 therefore wt as a solution to overcome the stationarity problem is applied as a pre processing approach cwt can be applied in the streamflow forecasting problems by choosing the first scale cannas et al 2006 or the best scale which is chosen as the scales have the highest correlation shoaib et al 2014 the best scale method is approved as the performance of the models increases shoaib et al 2014 it is adopted in this study for comparison the wavelet function and the scale are two important issues that must be considered in wt analysis in the best scale layout the best scale must be considered as input to ann and mggp the best scale was imposed on ann after the cwt transformation using several wavelet mother functions to choose the best wavelet function the considered functions are haar dmey db2 db3 db4 db5 db6 db7 db8 sym2 sym3 sym4 sym5 sym6 sym7 sym8 coif1 coif2 coif3 and coif4 the best performing wavelet function is found to be haar fig 9 this finding agrees with nourani et al 2009 nourani et al 2011b the haar wavelet function was used for the all the cwt analysis after this point which starts with the best scale layout of mggp table 5 shows that the results of the two approaches i e ann and mggp are close to each other with the slight outperformance to the favor of ann no large difference is observed between the performance of the model combinations in comparison with the first combination which contains only downstream lags the exception is that adding pet to the models noticeably improves the performance the best scale layout increased the performance of almost all the model combinations compared with the standalone layout the fact of having the highest correlated scale could improve the performance but with not much seasonality information extracted because one scale is not enough to obtain the hidden information in the time series the wct analysis fig 7b e shows that several scales have a correlation with the streamflow the correlation used here is a linear correlation in which the non linear relationship between the streamflow and the other variables cannot be captured using only one scale from the cwt analysis is not preferable because of the aforementioned reasons however the use of all scales leads to redundant information which causes the cost of calculation to dramatically increase and the performance of the model to decrease galelli et al 2014 taormina et al 2016 therefore the wmggpnn model is proposed this model enters all the scales to the mggp models to select the contributing scales and impose only these scales to ann the selected scales by the mggp model are listed in table 6 table 7 which shows the results of the proposed model indicates high performance with high r2 and low rmse i e the highest is 0 960 and 6 602 m3 s for validation subset the percentage of change in r2 and rmse of the proposed model from the best scale mggp and ann and full scale mggp are shown in fig 10 to compare the performance of the proposed model according to the figure the relative change percentage shows an increase in r2 and a decrease in the rmse in all the models the fitted versus the observed streamflow values plot of the best performing model combination fig 11 shows some of the scattered values around the best fit line in all models except the proposed model in which most of the points lay over the line therefore the proposed model improves the performance of streamflow forecasting the proposed model outperformed the best scale of mggp and ann because several scales are included as inputs to ann for example in the first model combination in the best scale layout the number of scales is two i e one for every variable but the number of scales obtained by mggp after entering 265 scales i e all the scales for two variables is eighteen table 3 the increase in performance can be attributed to the increase in the number of scales included in which more seasonality cycling and irregularity information are extracted one of the most important aspects of streamflow forecasting is the ability of the model to capture the extreme peak values therefore the peak flow forecasting ability comparison was conducted using 2 5 of the highest flows given that the standalone layout models are outperformed by the best scale layout models they were excluded from the comparison in the peak flow forecasting the proposed model demonstrates high accuracy in the forecasting of the peak flows being the highest in the model combination which includes downstream and pet with r2 that equals 0 998 and rmse that equals 1 231 m3 s table 8 the highest peak flow forecasting performance is obtained in two models the model includes downstream flow and temperature and the model includes downstream upstream and pet in the time of the dominating underestimation of the peak values of the streamflow using wannbs wmggpbs and wmggpfs the proposed model wmggpnn overlays the observed data almost perfectly fig 12 therefore including several scales can help in extracting the seasonality information and using ann which has a robust ability in dealing with the non linearity and auto regression improved the models to a level that can capture most of the fluctuations well cwt can be applied as a pre processing tool for forecasting the streamflow but using ddm alone may not be good cannas et al 2006 adamowski 2008 it can also improve the runoff modeling in the case of using rainfall only as inputs by choosing the best scale shoaib et al 2014 in this study the best scale layout improved the performance of the standalone layout but not significantly this increase is expected because the high linear correlation is normal but much of the information is not extracted and the advantage of applying cwt is not well utilized including more scales by applying mggp to the output of the cwt as a selection tool leads the selection of the most contributing scales and throws the redundant scales using these scales can help in extracting most of the hidden information with ann and in improving model performance dramatically especially in forecasting the peak values baratti et al 2003 cannas et al 2006 reported that the monthly rainfall and temperature at the gauge stations are not correlated with the behavior of the monthly runoff this study reveals that all the layouts which include meteorological and hydrological variables recorded in the catchment have a correlation with the streamflow because they improve the performance of the models 7 summary and conclusion a hydrological process such as a streamflow is characterized by high non linearity seasonality and non stationarity data driven models such as ann and mggp have the ability to cope with non linearity but not stationarity and seasonality wt is the pre processing techniques applied to deal with non stationarity and seasonality by transforming data into a time frequency domain applying cwt differs from applying dwt because a large number of scales coefficients are produced and most are redundant and deteriorate the performance of any ddm therefore this study used mggp as a selection tool that selects only the contributing scales to be imposed on ann an application is implemented by choosing a basin located in the southeast of turkey to improve the ability of the proposed model to increase the performance of the forecasting of monthly streamflow the monthly streamflow measured in the outlet of the basin is used upstream flow rainfall temperature and pet are added to the models to investigate their contribution to the downstream flow before the development of the model wct was conducted between the downstream flow and other variables to investigate the relationship in the time frequency domain given that the output of the forecasting is one month ahead downstream flow three layouts were considered in this study standalone which refers to data imposed to ann and mggp without pre processing best scale which refers to the cwt scale with the highest correlation with the downstream flow used as input to the ann and mggp and full scale which refers to the mggp used as a tool to select the contributing scales imposed on ann the results showed that upstream is correlated highly with downstream in almost all scales whereas the other variables are mostly locally correlated with the 12 month scales i e annual the temperature correlation pattern is similar to one of the pet because the latter is derived from the former the standalone forecasting layout exhibited poor performance for all model combinations the best scale layout improved the performance of the models slightly the proposed model wmggpnn improved the forecasting ability dramatically leading to a model that can capture the peak values excellently according to the wct analysis the streamflow is correlated with upstream rainfall temperature and pet variables in several scales although it shows no high linear correlation the standalone layout performed poorly because the relationship between the inputs and output is characterized by non linearity which can be handled by ddms seasonality irregularity and non stationarity the need for wt can be observed from this point using the best scale layout improved the performance because of the high linear correlation which is higher than the original variable but using linear correlation is insufficient in the existence of non linearity the main advantage of applying wt is to uncover the seasonality non stationarity and cycling information hidden in the original variable however one scale is not sufficient to carry this information therefore the proposed model wmggp which after the cwt analysis selects several scales that contribute to the output improved the performance dramatically the most important point is that the fluctuation in the streamflow as represented by peak values is well forecasted because the chosen scales include sufficient seasonality and irregularity information according to this study the restrictions of using cwt as a pre processing tool because of the high number of scales and the redundant information for predicting the monthly streamflow are avoidable this study applies only to monthly streamflow which is less predictable than the daily streamflow applying the same methodology to the daily streamflow is recommended acknowledgments the authors wish to thank devlet su i̇şleri general department of water affairs ministry of forest and water affairs and devlet meteroloji i̇şleri general department of meteorological affairs for providing the data necessary to complete this work the research is part of the project bap 1604f165 funded by anadolu university 
7303,snow has a unique characteristic in the water cycle that is snow falls during the entire winter season but the discharge from snowmelt is typically delayed until the melting period and occurs in a relatively short period therefore reliable observations from an optimal snow monitoring network are necessary for an efficient management of snowmelt water for flood prevention and hydropower generation the dual entropy and multiobjective optimization is applied to design snow monitoring networks in la grande river basin in québec and columbia river basin in british columbia while the networks are optimized to have the maximum amount of information with minimum redundancy based on entropy concepts this study extends the traditional entropy applications to the hydrometric network design by introducing several improvements first several data quantization cases and their effects on the snow network design problems were explored second the applicability the snow data assimilation system snodas products as synthetic datasets of potential stations was demonstrated in the design of the snow monitoring network of the columbia river basin third beyond finding the pareto optimal networks from the entropy with multi objective optimization the networks obtained for la grande river basin were further evaluated by applying three hydrologic models the calibrated hydrologic models simulated discharges using the updated snow water equivalent data from the pareto optimal networks then the model performances for high flows were compared to determine the best optimal network for enhanced spring runoff forecasting keywords hydrometric network snow monitoring network network design information theory hot spot map data quantization 1 introduction freshwater is one of the most valuable natural resources and spring freshet is a major source of freshwater in subarctic regions snow falls during the entire winter season but water from snowmelt is delayed until the melting period in spring and may occur in a relatively short time this snowmelt can cause severe flood events but also can be utilized to replenish hydropower reservoirs if well forecasted the importance of water monitoring is well documented by researchers water resources managers and decision makers mishra and coulibaly 2009 yang and burn 1994 the purpose of snow monitoring networks is to provide a comprehensive picture of snow cover in the monitored areas including extents depths and the variability of snow cover such information is essential for flood forecasting streamflow forecasting for reservoir operation planning and other activities despite the importance of hydrologic information declining trends in the spatial density of hydrometric networks were identified in many regions such as the united states mishra and coulibaly 2009 u s geological survey 1999 canada brown et al 2000 mishra and coulibaly 2009 pilon et al 1996 pyrce 2004 and africa rodda et al 1993 specifically the limitation of the canadian national hydrometric networks cnhn in terms of network density has been well documented coulibaly et al 2013 mishra and coulibaly 2010 in general given the large size of the country the cnhn has been below the wmo guidelines world meteorological organization 2008 for the minimum density of monitoring stations this implies either that there are not an adequate number of stations and or that their locations are not ideal or both most of the stations are located in the southern parts of the country and at lower elevations brown et al 2000 coulibaly et al 2013 hence hydrometric networks should be designed with careful consideration of their adequacy and effectiveness while simple wmo guidelines exist many have studied more scientific and practical network design methods which are well reviewed in the literature behmel et al 2016 chacon hurtado et al 2017 keum et al 2017 mishra and coulibaly 2009 mishra and coulibaly 2009 grouped the most widely used methods into eight categories statistically based methods spatial interpolation techniques entropy based methods optimization methods methods based on basin physiographic characteristics methods driven by sampling strategies hybrid methods and user survey approach among the diverse network design methods the entropy based methods have emerged as promising alternatives to traditional statistical methods mishra and coulibaly 2009 this study resorts to entropy based method combined with multi objective optimization entropy in information theory was developed to measure the amount of uncertainty in a dataset shannon 1948 and has often been applied to hydrometric network design by providing a measure of information content early studies e g husain 1989 yang and burn 1994 yoo et al 2008 utilized the entropy theory to evaluate the existing rain gauges or streamflow stations the entropy applications have been extended to determine proper locations of new additional monitoring stations for example the entropy terms have been used as the objective functions of multi objective optimization technique to search the optimal hydrometric networks e g alfonso et al 2013 2010a b kornelsen and coulibaly 2015 ridolfi et al 2014 samuel et al 2013 these studies specified the multi objective optimization problem as maximizing joint entropy i e total information content from a network and minimizing total correlation i e redundant shared information within the network which are the basic principles of the dual entropy and multiobjective optimization demo also leach et al 2016 2015 assessed the applicability of physical parameters such as hydrologic signatures indicators of hydrologic alteration and groundwater recharge as objectives in addition to the traditional entropy objectives however research regarding snow monitoring networks is limited while many entropy based hydrometric network studies have focused on streamflow and rainfall as reviewed above the objectives of this research are 1 to determine optimal snow monitoring networks that add more stations to the existing network 2 to evaluate the pareto optimal networks from the performance of hydrologic models and 3 to investigate the use of the snow data assimilation system snodas products for the time series data at potential stations while this research identifies the optimal snow networks in la grande river basin lgrb in québec and columbia river basin crb in british columbia the second objective focuses on the lgrb and the third objective explores the crb 2 study area and data 2 1 la grande river basin québec the first study area is a combined basin including la grande river basin and some of its surrounding watersheds that have been diverted into it the combined basin which includes 12 watersheds will be referred to herein as la grande river basin lgrb lgrb is located in north central québec as shown in the index map of fig 1 the total drainage area is approximately 209 000 km2 while the drainage area excluding diverted basins i e caniapiscau eastmain 1 rupert bief aval rupert bief amont lac mesgouez au lac mistassini and lac mistassini is approximately 99 000 km2 la grande river rises in central québec and flows west to james bay from the 30 year of climate observations at la grande riviere a station wmo id 71827 which is located near the outlet of la grande river daily mean temperatures are below zero from november to april and the lowest in january 22 c mean daily snowfall is the highest in november 1 9 cm day while the total precipitation is the highest in september see fig 2 based on the advanced very high resolution radiometer avhrr land cover dataset for québec 97 percent of the basin area is covered by forest while water bodies occupy 3 percent and nearly the entire basin is an undeveloped area due to the icy snowy fields and the numerous lakes covering most areas in northern territories including lgrb hydropower generation in canada is the second largest in the world bp 2015 and covers more than half of the total national electricity consumption specifically in québec hydropower supplies account for approximately 95 percent of the total electricity consumption hydro québec 2015 major hydropower plants in québec are mostly located in lgrb and the primary source of reservoir inflows for those plants is the snowmelt during spring to early summer lgrb currently has 47 snow monitoring sites hereafter existing stations of which 30 stations are hydro québec operating snow course sites and the other 17 stations are from environment and climate change canada each station is supposed to have monthly visits in winter months i e january february march and april however due to the availability and accessibility maintaining regular measurements e g 15th day in each winter month is often limited and cause snow data deficiency based on the physiographic units that wmo categorized to guide the minimum network density the lgrb falls under the interior plains unit and its minimum network density should be 575 km2 per non recording station or 5750 km2 per recording station world meteorological organization 2008 recalling that the total drainage area of lgrb is approximately 209 000 km2 there should be 37 automatic stations or 364 manual stations to meet the wmo guideline for a minimum snow network therefore it can be concluded that there is a large deficit in snow monitoring to overcome sparse density of snow monitoring networks and difficulties in data samplings there is a need to install automatic snowpack sensor stations in lgrb which can measure calculate and transmit the snow water equivalent swe with comparable accuracy to other ground based techniques such as snow courses snow pillows and weighing precipitation gauges the snow data for the existing stations were obtained from the sample observations during winter months for the years of 1970 2005 36 years so that the number of total data points for each station is 144 a gridded swe dataset for the entire study area is also available from hydro québec the hydropower company of québec province tapsoba et al 2005 specifically the 14 km resolution of the grid cells provides 990 grid points in lgrb in this study the weather stations currently installed in lgrb are chosen as the potential locations of new swe observations hereafter potential stations following personal communications with hydro québec about the snow monitoring needs and plans and the number of potential stations is 47 see fig 1 the snow data of the potential stations are obtained from the swe of the closest grid point it should be noted that the current monitoring sites called existing stations are mainly for the ground based snow observations such as snow courses snow pillows or weighing precipitation gauges while automatic snowpack sensor stations are being planned at the optimal locations of additional stations therefore the potential station locations which are very close to the existing monitoring sites are also considered given that automatic stations could be placed at selected manual monitoring sites to ensure continuity of historical snow observations 2 2 columbia river basin british columbia the entire columbia river basin covers parts of british columbia canada and the northwestern united states however this study will consider the upper parts of the columbia river basin fig 3 where the british columbia hydropower company namely bc hydro is responsible for the water resources management it should be noted that the southern part of the basin is extended to the states of idaho and montana and the american side of the basin is not included in the network design in this study the drainage area of the columbia river above arrow lakes dam and the kootenay river above kootenay canal excluding the u s portion of the kootenay lake watershed which will be referred to as crb hereafter is approximately 70 700 km2 the crb is made up of mostly mountainous terrain with only a few hilly areas as shown in fig 3 considering the geographic location and the elevation range of the crb it is critical that an adequate snow monitoring network exists however many stations have been installed at lower elevations even though there is typically more precipitation at higher elevation kite 1997 two long term climate stations ft steele dandy crk climate id 1153034 and sparwood climate id 1157630 are close to each other in the southeastern crb but located on different elevations which are 856 m and 1138 m respectively fig 4 presents the mean daily temperature and precipitation of 30 year observations at the two stations showing significant differences in precipitation patterns specifically there has been more snow at the higher location while the amount of rainfall is similar as the water resources in the crb have been variously used in hydropower generation irrigation recreation as well as ecosystem health the design of an efficient snow monitoring network is essential data for this study was collected from the british columbia river forecast centre bcrfc and environment and climate change canada eccc snow data stored on the bcrfc portal are from manual and automatic stations operated by bc ministry of environment bc hydro rio tonto alcan and metro vancouver the eccc stations are measuring snow on ground and swe from automatic stations manual snow surveying process includes extracting a snow core and weighting the amount of frozen and liquid water extracted the surveys are typically conducted at the beginning of the month from january to june and additionally mid month in may and june at selected stations the automatic system of bcrfc collects snow and climate data at 15 min or hourly interval and transmits on an hourly basis the numbers of snow monitoring stations that are currently in operation are 9 automatic and 35 manual stations from bcrfc and 4 automatic stations from eccc see fig 3 the physiographic unit of the crb is mostly matched with mountains based on the wmo guidelines of which non recording station minimum density should be 250 km2 per station and recording station minimum density should be about 2500 km2 world meteorological organization 2008 recalling the total study area of crb the required number of automatic stations to meet the wmo minimum network density is 29 to determine the required number of new stations for meeting the wmo guidelines the number of the manual stations were converted to the equivalent number of the automatic stations this conversion was based on the information from the wmo guidelines world meteorological organization 2008 in specific manual stations were modeled as 1 10th of an automatic station for the interpolation 35 manual bcrfc stations can be regarded as 3 5 automatic stations this is rounded to 3 stations so that the total number of existing snow monitoring stations is converted to 16 automatic stations including 13 automatic stations from both bcrfc and eccc therefore 13 new stations are required for the crb to meet the wmo guidelines the potential locations where new additional stations may be located are from the gridded binary code edition 2 grib2 which is an international grid standard for data transmission by the wmo glahn and lawrence 2002 world meteorological organization 2003 the grib2 format defines 935 824 polar stereographic grids in north america for the regional deterministic prediction system government of canada 2015 accordingly the number of potential stations becomes 344 as shown in fig 3 this study used the snodas database by national snow and ice data center national operational hydrologic remote sensing center 2004 in which data from satellite and snow stations are assimilated as the snow values for the potential stations after comparing the recent observations from the ground based stations and the snodas there were significant differences before oct 2008 however the two datasets match each other quite well afterward therefore the data period was chosen as the period of oct 2008 sep 2011 to ensure consistency and reduce uncertainty to further assess the consistency between the two datasets at the potential locations four geospatial analysis methods were used in specific 1 no adjustment i e raw data 2 linear regression and inverse distance weighting lridw 3 linear regression and kriging lrk matheron 1963 and 4 cokriging methods deutsch and journel 1992 were used to correct the snodas data based on the selected ground based gauge data the no adjustment just used the original swe values that were retrieved from snodas database the lridw firstly created a linear regression function between the historical snow observations and snodas then the residuals between the two datasets were interpolated using the idw method lastly the interpolated residuals were added to the estimated swe from the linear regression the lrk is similar to the lridw but in the second step the residuals were interpolated using the kriging method the cokriging used kriging estimates which are based on the distance to nearby observations and its variogram the distance to nearby snodas samples and its variogram and the cross variogram for the observations and snodas leave one out cross validation was then applied to evaluate the accuracy of each geospatial analysis technique in specific each of the nine bcrfc automatic stations was treated as if it was ungauged on at a time the root mean square error rmse value of each station and the median and the mean values for each geospatial analysis methods are shown in table 1 the results indicated that there were only minor differences between the four methods specifically 1 mm in median and 4 9 mm in mean considering that the performances of the four geospatial analysis methods were similar and that the cokriging method produced slightly lower rmse in median the cokriging was selected to generate the swe at each potential station in this study however it should be noted that other methods could also be used due to their competitive performances the results table 1 suggest that any of the four geospatial analysis techniques could be used in this case 3 background 3 1 entropy measures in the information theory shannon entropy provides a way to measure the amount of information content that can be given from a variable shannon 1948 the basic measure is marginal entropy which represents the amount of information content in a variable this concept can be extended to joint entropy and total correlation representing the multivariate information content and the duplicated information between variables respectively venn diagram provides one of the best conceptual descriptions of entropy fig 5 shows the schematic overview of entropy measures when there are three random variables e g blue orange and green in fig 5a the size of each color i e variable refers the marginal entropy of each variable fig 5a in a similar manner the joint entropy is given by the total area fig 5b and the duplicated information is shown as the overlapping area fig 5c therefore the efficient network may have the greater total area and the smaller overlapping area marginal entropy in hydrometric network design problems represents the inherent information contents from observations at station x singh 1997 1 h x i 1 n p x i log 2 p x i where h x is the marginal entropy of a station x in bits x i is the frequency of the ith bin from the time series histogram of station x p x i is the occurrence probability of x i and n is the total number of bins the marginal entropy can be extended to the joint entropy when the number of random variables is greater than one representing the total amount of information from a network with multiple stations 2 h x 1 x 2 x n i 1 1 n 1 i 2 1 n 2 i n 1 n n p x 1 i 1 x 2 i 2 x n i n log 2 p x 1 i 1 x 2 i 2 x n i n where h x 1 x 2 x n is the joint entropy of n stations n 1 n 2 n n are the number of bins of the according stations of the network containing n stations krstanovic and singh 1992 to calculate the joint probability p x 1 i 1 x 2 i 2 x n i n multivariate time series data agglomerate to form a new single variable time series the process of agglomeration is similar to the numeral system with various radixes that is the data of each station become the values of each digit alfonso et al 2010b due to the possible duplications of information contents between stations the joint entropy of a network is less than the sum of the marginal entropy of each station except for that every station is independent to each other the total correlation is a measure for evaluating inefficiency in a network and defined as the difference between the joint entropy and the sum of marginal entropies mcgill 1954 watanabe 1960 3 c x 1 x 2 x n i 1 n h x i h x 1 x 2 x n where c x 1 x 2 x n is the total correlation of n variables since eqs 1 3 are applicable to discrete random variables the number of events and the bin width is determined by quantization methods which will be provided in 4 2 quantization methods 3 2 data quantization there are two options of the entropy calculation from a time series dataset discrete entropy and continuous entropy singh 1997 entropy terms were initially presented for discrete variables shannon 1948 and extended to continuous variables considering most hydrological processes in the water cycle are continuous the use of continuous entropy may provide more reliable outcomes because there is no need to convert continuous time series to a discrete dataset however calculations of multivariate joint entropy from continuous distribution functions have essential limitations at this time specifically the proper distribution should be assumed and even more importantly the mathematical forms of continuous joint entropy other than gaussian distribution are still limited ozkul et al 2000 singh 2013 yang and burn 1994 therefore the discrete entropy is more applicable to hydrometric network designs to estimate discrete entropy from a hydrologic variable the continuous dataset should be discretized by any quantization method even if the actual value of the entropy may change due to data quantization while discrete entropy has largely been applied to the network design during the recent decades only a few have compared different quantization method as summarized by fahle et al 2015 most previous studies concluded that the final spatial distribution of optimal networks or rankings of candidate stations are not significantly affected by the quantization method alfonso et al 2013 li et al 2012 markus et al 2003 mishra and coulibaly 2010 alfonso et al 2014 brought the concept of ensemble entropy to take uncertainty from quantization into account specifically they introduced a numeric factor which is used in the quantization process as well as the multiplication factor however the conclusions of previous studies have not been made by comparing different quantization methods but brought by changing the parameters of factors in the same method on the other hand fahle et al 2015 found a pronounced influence of the multiplication factor and also compared the rounding and scott s methods scott 1979 they ultimately chose the rounding method since scott s method tends to smoothen the differences of the marginal entropies recently keum and coulibaly 2017 introduced a relative ranking concept to determine how the station rankings are affected by different quantization methods they concluded that the differences were evident when the different quantization methods were used 3 3 optimization tool the typical objectives of the entropy based optimization problems for hydrometric network design are maximizing joint entropy and minimizing total correlation keum et al 2017 one optimization approach is the maximum information minimum redundancy mimr which reduces problem complexity by combining the design objectives into a single comprehensive objective fahle et al 2015 li et al 2012 however the trade off weight of each objective should be assumed prior to the optimization to avoid any preassumption the dual entropy and multiobjective optimization demo samuel et al 2013 was chosen as the optimization tool in this study the demo finds optimal solutions by employing the entropy concept in information theory as a measure of information contents from a monitoring station or a hydrometric network and using the epsilon dominance hierarchical bayesian optimization algorithm ε hboa kollat et al 2008 samuel et al 2013 the ε hboa is an enhanced multi objective evolutionary algorithm which was developed to overcome mathematical difficulties due to the number of design objectives and the interdependencies between a large number of decision variables on hydrometric network design problems kollat et al 2008 in ε hboa three major additive features were added to its precursor the non dominated sorted genetic algorithm nsga deb 2000 1999 deb et al 2002 such as 1 the epsilon dominance for allowing a tolerance of the selection of pareto sets 2 the hierarchy for reducing problem complexity by hierarchical decomposition to sub problems and 3 the bayesian network models for exploiting dependencies between decision variables instead of mutation and crossover of traditional genetic algorithms kollat et al 2008 osman et al 2006 pelikan 2002 4 methods an overview of the steps in the methodology and the data processing is presented in fig 6 briefly the snow network design for la grande river basin integrates hydrologic model performances with the demo applications using the interpolated snow data for potential stations while the network design in columbia river basin utilizes snodas datasets for swe at potential stations the description of each step is as follows 1 1 gather swe data at ground based existing stations 1 2 obtain swe data at potential stations from the spatially interpolated grid data provided by hydro québec 1 3 search the pareto optimal networks by applying demo and datasets from 1 1 and 1 2 1 4 calculate watershed average swe from observations for hydrologic model calibration 1 5 calculate watershed average swe including the potential stations selected by demo 1 6 gather weather data to run hydrologic models 1 7 calibrate hydrologic models using observed swe and the weather data 1 8 simulate watershed outflows these are actually reservoir inflows in the lgrb using the calibrated models 1 9 gather the observed reservoir inflows 1 10 calculate model performance criteria 1 11 suggest the optimal network in the lgrb based on the model performance 2 1 gather swe data at ground based existing stations 2 2 locate snodas datasets 2 3 correct snodas product with station data using geospatial analysis and retrieve swe for potential station locations 2 4 apply demo for two cases case 2 no constraint about the number of additional stations and case 3 set the number of additional stations based on the wmo guideline 2 5 find the optimal networks from demo results 2 6 draw the hot spot maps using the selected stations in the optimal networks to determine the area where the additional monitoring effort is mostly required 4 1 determination of optimal networks based on the definitions of entropy measures an optimal hydrometric network should provide a maximum amount of effective information i e maximize fig 5b with a minimum amount of redundant information i e minimize fig 5c accordingly the design objectives become 4 max h s n m h e 1 e 2 e n x 1 x 2 x m min c s n m c e 1 e 2 e n x 1 x 2 x m subject to n and m are integers case 1 lgrb no constraint m 1 2 m max lgrb case 2 crb no constraint m 1 2 m max crb case 3 crb wmo guideline m m wmo where s n m is a network consisting of n existing stations e and m additional stations x m max is the total number of potential stations for each study basin and m wmo is the required number of new stations to meet the wmo guidelines here we propose three study cases in cases 1 and 2 there is no specific constraint about the number of new additional station but the maximum numbers are limited to the number of all potential stations in each basin in these cases the multi objective optimization will determine the number of new stations based on the objective function values on the other hand case 3 is additionally applied to the crb as there is a deficit in snow monitoring network and there is a large number of potential locations from the grib2 grid points in specific the number of additional stations is fixed as 13 in case 3 but the locations will be determined by the multiobjective optimization processes the details of each case are summarized in table 2 4 2 quantization methods as stated earlier entropy values using discrete entropy measures are affected by data quantization this study compares three types of quantization methods rounding eq 5 scott s eq 6 and sturges eq 7 5 rounding x r i floor 2 x i 2 i 6 scott h sc 3 49 s n 1 3 7 sturges h st r x 1 log 2 n where x r is the nearest integer value which is multiples of the multiplication factor i with given x floor is the floor function which removes all decimal places h sc and h st are the bin widths from scott s and sturges methods respectively s is the standard deviation of a series of x n is the total number of data points and r x is the range of a series of x rounding method is a simple uniform quantization method that results in products of the multiplication factor i scott s method was developed to determine the estimated bin widths underlying the gaussian density assumption but its application to other densities is still valid scott 1979 sturges method also gives the estimated class interval based on the range and the data points of a series sturges 1926 to calculate parameters of the bin width estimating methods i e the standard deviation of scott s method and the range of sturges method time series of all stations are aggregated so that the same estimated bin width for all stations is used this allows to avoid any possible overestimation or underestimation of information contents among stations 4 3 hydrologic models for the case 1 i e lgrb further analysis to determine better snow monitoring networks in terms of the hydrologic model performance is conducted for an enhanced spring runoff forecast first hydrologic models are calibrated using the swe of the existing station then the swe data is updated for each pareto optimal network with additional stations which are given by the results from demo next the hydrologic models simulate discharge i e reservoir inflows using each pareto optimal network finally the optimal network for spring peak flow is determined by evaluating the hydrologic model performance criteria in this study three hydrologic models are selected to obtain a general outcome of identifying the best performing optimal snow monitoring network the selected models are the service hydrométéorologique apports modulés intermédiaires hsami model the mcmaster university hydrologiska byråns vattenbalansavdelning mac hbv model and the sacramento soil moisture accounting sac sma model hsami is a lumped hydrologic model which uses basin averaged data as inputs this model was developed by hydro québec bisson and roberge 1983 fortin 2000 and has been mostly used to simulate streamflow especially in québec minville et al 2008 there are 23 parameters in hsami two for evapotranspiration six for snowmelt three for surface runoff estimation seven for vertical flow in soil and five for horizontal flow the hsami parameters were already calibrated to each watershed in the lgrb by hydro québec using the shuffled complex evolution optimization method and the nash sutcliffe efficiency criterion minville et al 2014 2008 nash and sutcliffe 1970 a 36 year data period is selected from 1970 to 2005 and separated into even and odd years for calibration and validation respectively to maintain consistency in the calibration of model parameters the selected calibration and validation periods are also used for the other hydrologic models mac hbv model is also a lumped rainfall runoff model and was modified from the original hbv model bergström 1976 by including nonlinear storage discharge formulation to provide better performance especially in baseflow estimation at ungauged sites samuel et al 2012 the mac hbv includes 14 parameters in four hydrological routines snow soil moisture response and routing scheme the snow routine represents changes in the snowpack using a simple degree day concept the soil moisture routine represents the soil moisture accounting or changes in soil moisture storage in the top soil layer the response function estimates the amount of runoff from the upper zone and lower zone based on the current water storage and the maximum storage for channel routing an equilateral triangular weighting function is used to obtain the final streamflow samuel et al 2011 for a detailed description of the mac hbv model the interested readers may refer to the journal articles by samuel et al 2012 2011 the third hydrologic model used is the sac sma model which has been widely used since its development burnash et al 1973 the sac sma conceptual model consists of eight input parameters and six state variables in this model moisture in a soil box is divided into two upper zones tension and free water storages and three lower zones tension water storage and primary and supplementary free water storages the two types of lower free water storages allow groundwater storages with two different drainage factors for better estimation of baseflow the routing approach used in this model is the nash cascade method and the same snow component and evapotranspiration calculation methods used in the mac hbv model are added to this model for more details on the adapted model please see razavi and coulibaly 2017 as stated previously the same time series data is used to calibrate and validate the mac hbv and the sac sma models 4 4 model performance criteria model performance statistics or criteria are often used to evaluate the accordance of predicted values to the observed values of hydrologic variables based on the mathematical formulation of targeting higher or lower part of the hydrograph the proper performance criteria will vary in this study three criteria which perform better especially in high flows or peak flows were selected pearson correlation coefficient pr nash sutcliffe efficiency nse and index of agreement ia krause et al 2005 for all three criteria the closer values are to 1 the more accurate the simulated flows are the model performance criteria used in this work are as follows 8 pr i 1 n q i q q i q i 1 n q i q 2 i 1 n q i q 2 9 nse 1 i 1 n q i q i 2 i 1 n q i q 2 10 ia 1 i 1 n q i q i 2 i 1 n q i q q i q 2 where q i is the observed flow at ith data point q is the mean observed flow q i is the simulated flow at ith data point q is the mean simulated flow and n is the number of data points given that the focus herein is on spring peak flow which is driven by snowmelt the selected criteria are therefore used to assess peak flow and are denoted as ppr pnse and pia respectively peak flows that are greater than one third of the mean peak flow are used to calculate the performance measures ribeiro et al 1998 5 results and discussions 5 1 selection of data quantization even if the same data were used entropy values can change due to the data quantization from zero for the one bin histogram to the saturated entropy log 2 n if each bin does not contain more than one value in it therefore it is reasonable to compare the effects of quantization on monitoring network design problems recall that the numbers of the data points are 144 for lgrb and 1095 for crb the maximum entropy values from eq 1 or 2 are log2 144 7 17 bits and log2 1095 10 01 bits respectively when the number of data points is relatively small compared to the variability of time series dataset i e many unique values the joint entropy can be easily saturated having the maximum value and then a network with the lowest total correlation among them will be chosen as the optimal network having an optimal network which has saturated entropy is less reliable since the network may be selected due to hitting the boundary i e the saturated entropy and not because of its optimality fig 7 shows the ranges of joint entropy values of the optimal networks with regards to different data quantization and their numbers of optimal networks from cases 1 and 2 while 1 100 mm swe were used as the multiplication factors of the rounding method in lgrb the highest joint entropies from the optimal networks that used a multiplication factor of 60 mm swe or less reached saturation the estimated bin widths from scott s method and sturges method are 43 7 and 81 6 mm swe respectively and the highest joint entropy of optimal networks using scott s method reached the saturated entropy while sturges method did not interestingly while the numbers of the optimal networks generally ascend when the multiplication factors of the rounding method increased the numbers using the estimated bin widths by scott s and sturges methods are smaller than the rounding methods with the closest multiplication factors in the case of crb the tested multiplication factors of rounding method are 1 5 10 15 and 20 mm swe and the estimated bin widths are calculated as 9 3 mm swe for scott s method and 18 0 mm swe for sturges methods respectively there is no quantization case which has an optimal network with the saturated entropy in crb compared to case 1 case 2 produced more optimal networks regardless of the quantization method it can be assumed that the smaller number of data points in lgrb caused the saturated entropy issue and the smaller number of optimal networks as the joint entropy values and the numbers of optimal networks are related to bin widths or multiplication factors as fixed bin widths and the number of data points a proper quantization may be able to lead to a reliable optimal network even though the effects of data quantization have been explored by several studies alfonso et al 2014 2010a keum and coulibaly 2017a li et al 2012 markus et al 2003 mishra and coulibaly 2010 to the best of our knowledge there is no universal guideline for the selection of quantization method therefore considering the joint entropy ranges of the optimal networks in both study areas we chose the sturges method as the primary quantization method for further analysis and others were used for comparison herein 5 2 optimal networks 5 2 1 la grande river basin after the demo application using sturges quantization method an ensemble of 31 pareto optimal networks in which one objective value cannot be improved without worsening the other objective value were determined the selected numbers of additional stations range from 1 to 5 among 47 potential locations and the joint entropy and the total correlation vary from 6 65 to 7 14 bits and 35 26 to 41 71 bits respectively recall that multiobjective optimization tools such as demo yield a number of pareto optimal networks the monitoring hot spot maps which enable to visualize the locations where monitoring efforts are highly required or not were drawn keum and coulibaly 2017a b kornelsen and coulibaly 2015 leach et al 2016 2015 in specific the number of occurrence in the optimal networks was counted and converted to a ratio ranging 0 100 for each station the strength of hot spot was then indicated by the size of the circle fig 8 shows the monitoring hot spot map using sturges quantization method in the case 1 it should be noted that a combination of the high percentage locations does not always indicate an optimal network but does inform about priority areas in network design extension the hot spots are highly noticeable at the stations 17 25 35 and 36 which were the members of the optimal networks 5 times or more while 30 stations out of 47 potential locations were not selected at all 5 2 2 columbia river basin considering all the existing stations remain active in the optimized networks and the number of potential stations is 344 the possible combinations of networks from cases 2 and 3 are 3 58 10 103 and 1 21 10 23 and the numbers of pareto optimal networks is 244 and 74 respectively it should be noted again that all the optimal solutions are considered equally good because none of the objective functions dominates one to another these pareto optimal solutions which are shown by blue circles in fig 9 are commonly called the non dominated solutions recalling that the case 2 does not have a constraint about the number of additional stations the demo determined numbers of additional stations vary from 44 to 134 when compared this range in case 2 to the additional number of station in case 3 which was fixed to 13 it can be confirmed that the wmo guideline only provides the minimum network density to avoid any data deficiency in water resources management world meteorological organization 2008 fig 10 shows the monitoring hot spot maps of each study case using sturges method for the both cases in crb comparing those maps the hot spots where the occurrence percentages are high of case 3 are fairly included in those of case 2 as well for example the common hot spots are shown along the valleys and some mountainous regions near the southeastern area of station 2a21p and the east of station 2d07a the results show that the difference in problem setting i e flexible vs fixed number of additional stations has evidently changed the spatial distributions of hot spots by expanding them considering that there are a number of the pareto optimal networks from each case and it is difficult to define the best design approach it is reasonable to preferentially install new snow monitoring stations where the hot spots are commonly appeared in case 3 to meet the minimum density by wmo and then expand to other hot spots in case 2 the hot spots shown in fig 10 were obtained by optimizing entropy values only and most of them appeared in valleys from a practical perspective water resources managers who support water supply and forecasting may tend to prioritize station installations at high snowpack areas while snow data users for ecological applications may be interested in the presence of permanent or sporadic winter snow cover others such as municipal city engineers may want to consider preferentially adding more stations in areas of high population density or industrial activity while this case study focused on the application of entropy theory based network design with snodas dataset the practical issues discussed here above have not been considered user specific needs could be included as additional cost function in demo however this is beyond the scope of this study 5 3 hydrologic model applications the demo application results in the case 1 were further analyzed by a combination of hydrologic models as previously stated the calibration results of the hsami model were obtained from minville et al 2008 in a similar manner the mac hbv and sac sma models were calibrated using the same data period by maximizing the nse from observed and simulated flows the calibration and validation results for the three models are presented in table 3 for the calibration all the nse values were greater than 0 6 and their averages are 0 76 0 79 and 0 84 for hsami mac hbv and sac sma respectively as for the validation many of the nse are greater than 0 6 except for three watersheds rupert bief amont rupert bief aval and lac mistassini these three watersheds do not have enough observed data points compared to the other watersheds as the observations were missing during approximately 80 of the total data period the average values of the validation nse are around 0 5 but the values are improved to 0 7 if the three watersheds with very limited data are excluded therefore it can be concluded that the model calibrations in general performed successfully to evaluate the enhanced performance gained by the hydrologic models when using the newly designed optimal networks swe data of each watershed was updated for each optimal network as the hydrologic models used in this study are semi distributed models set up for 12 watersheds the inversed distance weighting method idw was used to convert station data to watershed data specifically the station data was interpolated to the hydro québec grid points 990 grids in lgrb and the grid data in each watershed was averaged to estimate the watershed swe then three model performance criteria pnse ppr and pia were calculated for each watershed using the observed reservoir inflow and the simulated inflow from the three hydrologic models respectively the model performance statistics of the twelve watersheds were used to draw boxplots for each optimal network from all boxplots in fig 11 it appears that the optimal networks generally provided better performance than the existing network network 0 as the values of model performance increased even though all pareto optimal networks yield better performance than the existing network the best or recommended optimal network can be recognized when the values of performance criteria are sensitive to optimal networks comparing all the combinations of hydrologic models and performance criteria mac hbv model results evaluated by pnse were the most sensitive in specific the percent increase of pnse by optimal network 5 using mac hbv was estimated at 41 4 in la grande 3 watershed and 18 1 when averaged for all watersheds overall whatever the hydrologic models used moderate to large improvements were obtained when swe of optimal networks are used if compared to the existing network and the absolute criteria values are similar to each other however mac hbv shows larger changes than the other two models and the peak flow criteria were able to better differentiate the optimal networks therefore this case pnse using mac hbv is recommended for selecting the best network due to its higher sensitivity it should be noted that this conclusion is valid for this study and requires that a similar analysis be applied to other areas or other hydrologic variables such as rain or streamflow before drawing a more general conclusion in general whatever the hydrologic model the optimal network 5 provides better performance than other pareto optimal networks and the locations of its selected stations which are added to the existing stations are shown in fig 12 in this case network 5 is recommended as the best performing optimal network and it simply requires three additional stations in the locations identified fig 12 the three additional stations are well distributed spatially across the basin however considering that station 40 is not located on the strong hot spots the monitoring hot spot map does not always indicate the optimal network but should be used to inform the general locations where the monitoring efforts are mostly needed 6 conclusions a better estimation of swe is of particular importance for water resources planning and management in canadian watersheds due to the technological advances many automated snow monitoring sensors are able to measure the swe and transmit data to end users server without a frequent human access in this work a snow monitoring network design strategy for enhanced spring peak flow forecasting was proposed by investigating demo with snodas and hydrologic models three case studies were carried out to determine the optimal locations of new snow monitoring stations in case 1 hydrologic model simulations were included to further evaluate pareto optimal networks to identify the overall best performing optimal snow monitoring network in the lgrb in cases 2 and 3 a remote sensing database snodas was used to derive snow values for the potential snow monitoring locations in crb the difference between case 2 and 3 was the consideration of the wmo guideline for the minimum density of monitoring networks the number of additional stations in case 2 was not pre defined but determined during the demo process while that in case 3 it was fixed as 13 to meet the wmo minimum network guideline combining the demo with the hydrologic models was shown to be an effective strategy for the design of optimal snow networks the advantage of the entropy optimization is on determining optimal networks that have the maximum amount of information with minimum redundancy however as the optimization problem has multiple objectives it is unavoidable to have many optimal solutions or networks in the pareto fronts the use of hydrologic models as part of the network design process was effective in identifying the best performing network among the pareto optimal solutions especially for spring peak flow forecast it was found that in general all three hydrologic models used in case 1 yielded forecast improvement when an optimal snow network swe data was used however mac hbv appeared most sensitive to the changes of swe from the different optimal networks this allowed for the identification of the best performing optimal network most suitable for spring peak flow forecasting the analysis results indicated that network 5 offered the best performance as compared to other pareto optimal networks although the number of selected stations in the network 5 is lower compared to other networks it should be noted that other factors such as cost accessibility and other variables e g topography land cover could be added as additional objective functions in the snow network optimization process the applicability of snodas data in network design was assessed in cases 2 and 3 the optimization results and the derived monitoring hot spot maps were able to clearly locate regions where the monitoring efforts are mostly needed considering that the most challenging part of the demo application is to obtain hydrologic time series for the potential stations it was shown that the snodas products can be one of the solutions this study also analyzed the effects of the quantization method for the conversion to discrete time series in the entropy calculations the results from the case studies showed that the data quantization may affect the optimal networks as clearly shown in case 1 therefore careful consideration and an application of various quantization methods are required in hydrometric network design to obtain a reliable optimal network acknowledgments this research was jointly supported by the natural science and engineering research council nserc of canada environment canada hydro québec and bc hydro we would like to thank dr joshua kollat at pennsylvania state university for providing the source code of the ε hboa catherine guay marc durocher claude gignac from hydro québec for providing some data and rené roy for his assistance in the administrative approval process by hydro québec karina seto for processing some of the initial data and jos samuel at yukon college for the preliminary work and initial data processing for the crb the authors acknowledge marcus fahle and two anonymous reviewers for their comments that helped to improve the manuscript 
7303,snow has a unique characteristic in the water cycle that is snow falls during the entire winter season but the discharge from snowmelt is typically delayed until the melting period and occurs in a relatively short period therefore reliable observations from an optimal snow monitoring network are necessary for an efficient management of snowmelt water for flood prevention and hydropower generation the dual entropy and multiobjective optimization is applied to design snow monitoring networks in la grande river basin in québec and columbia river basin in british columbia while the networks are optimized to have the maximum amount of information with minimum redundancy based on entropy concepts this study extends the traditional entropy applications to the hydrometric network design by introducing several improvements first several data quantization cases and their effects on the snow network design problems were explored second the applicability the snow data assimilation system snodas products as synthetic datasets of potential stations was demonstrated in the design of the snow monitoring network of the columbia river basin third beyond finding the pareto optimal networks from the entropy with multi objective optimization the networks obtained for la grande river basin were further evaluated by applying three hydrologic models the calibrated hydrologic models simulated discharges using the updated snow water equivalent data from the pareto optimal networks then the model performances for high flows were compared to determine the best optimal network for enhanced spring runoff forecasting keywords hydrometric network snow monitoring network network design information theory hot spot map data quantization 1 introduction freshwater is one of the most valuable natural resources and spring freshet is a major source of freshwater in subarctic regions snow falls during the entire winter season but water from snowmelt is delayed until the melting period in spring and may occur in a relatively short time this snowmelt can cause severe flood events but also can be utilized to replenish hydropower reservoirs if well forecasted the importance of water monitoring is well documented by researchers water resources managers and decision makers mishra and coulibaly 2009 yang and burn 1994 the purpose of snow monitoring networks is to provide a comprehensive picture of snow cover in the monitored areas including extents depths and the variability of snow cover such information is essential for flood forecasting streamflow forecasting for reservoir operation planning and other activities despite the importance of hydrologic information declining trends in the spatial density of hydrometric networks were identified in many regions such as the united states mishra and coulibaly 2009 u s geological survey 1999 canada brown et al 2000 mishra and coulibaly 2009 pilon et al 1996 pyrce 2004 and africa rodda et al 1993 specifically the limitation of the canadian national hydrometric networks cnhn in terms of network density has been well documented coulibaly et al 2013 mishra and coulibaly 2010 in general given the large size of the country the cnhn has been below the wmo guidelines world meteorological organization 2008 for the minimum density of monitoring stations this implies either that there are not an adequate number of stations and or that their locations are not ideal or both most of the stations are located in the southern parts of the country and at lower elevations brown et al 2000 coulibaly et al 2013 hence hydrometric networks should be designed with careful consideration of their adequacy and effectiveness while simple wmo guidelines exist many have studied more scientific and practical network design methods which are well reviewed in the literature behmel et al 2016 chacon hurtado et al 2017 keum et al 2017 mishra and coulibaly 2009 mishra and coulibaly 2009 grouped the most widely used methods into eight categories statistically based methods spatial interpolation techniques entropy based methods optimization methods methods based on basin physiographic characteristics methods driven by sampling strategies hybrid methods and user survey approach among the diverse network design methods the entropy based methods have emerged as promising alternatives to traditional statistical methods mishra and coulibaly 2009 this study resorts to entropy based method combined with multi objective optimization entropy in information theory was developed to measure the amount of uncertainty in a dataset shannon 1948 and has often been applied to hydrometric network design by providing a measure of information content early studies e g husain 1989 yang and burn 1994 yoo et al 2008 utilized the entropy theory to evaluate the existing rain gauges or streamflow stations the entropy applications have been extended to determine proper locations of new additional monitoring stations for example the entropy terms have been used as the objective functions of multi objective optimization technique to search the optimal hydrometric networks e g alfonso et al 2013 2010a b kornelsen and coulibaly 2015 ridolfi et al 2014 samuel et al 2013 these studies specified the multi objective optimization problem as maximizing joint entropy i e total information content from a network and minimizing total correlation i e redundant shared information within the network which are the basic principles of the dual entropy and multiobjective optimization demo also leach et al 2016 2015 assessed the applicability of physical parameters such as hydrologic signatures indicators of hydrologic alteration and groundwater recharge as objectives in addition to the traditional entropy objectives however research regarding snow monitoring networks is limited while many entropy based hydrometric network studies have focused on streamflow and rainfall as reviewed above the objectives of this research are 1 to determine optimal snow monitoring networks that add more stations to the existing network 2 to evaluate the pareto optimal networks from the performance of hydrologic models and 3 to investigate the use of the snow data assimilation system snodas products for the time series data at potential stations while this research identifies the optimal snow networks in la grande river basin lgrb in québec and columbia river basin crb in british columbia the second objective focuses on the lgrb and the third objective explores the crb 2 study area and data 2 1 la grande river basin québec the first study area is a combined basin including la grande river basin and some of its surrounding watersheds that have been diverted into it the combined basin which includes 12 watersheds will be referred to herein as la grande river basin lgrb lgrb is located in north central québec as shown in the index map of fig 1 the total drainage area is approximately 209 000 km2 while the drainage area excluding diverted basins i e caniapiscau eastmain 1 rupert bief aval rupert bief amont lac mesgouez au lac mistassini and lac mistassini is approximately 99 000 km2 la grande river rises in central québec and flows west to james bay from the 30 year of climate observations at la grande riviere a station wmo id 71827 which is located near the outlet of la grande river daily mean temperatures are below zero from november to april and the lowest in january 22 c mean daily snowfall is the highest in november 1 9 cm day while the total precipitation is the highest in september see fig 2 based on the advanced very high resolution radiometer avhrr land cover dataset for québec 97 percent of the basin area is covered by forest while water bodies occupy 3 percent and nearly the entire basin is an undeveloped area due to the icy snowy fields and the numerous lakes covering most areas in northern territories including lgrb hydropower generation in canada is the second largest in the world bp 2015 and covers more than half of the total national electricity consumption specifically in québec hydropower supplies account for approximately 95 percent of the total electricity consumption hydro québec 2015 major hydropower plants in québec are mostly located in lgrb and the primary source of reservoir inflows for those plants is the snowmelt during spring to early summer lgrb currently has 47 snow monitoring sites hereafter existing stations of which 30 stations are hydro québec operating snow course sites and the other 17 stations are from environment and climate change canada each station is supposed to have monthly visits in winter months i e january february march and april however due to the availability and accessibility maintaining regular measurements e g 15th day in each winter month is often limited and cause snow data deficiency based on the physiographic units that wmo categorized to guide the minimum network density the lgrb falls under the interior plains unit and its minimum network density should be 575 km2 per non recording station or 5750 km2 per recording station world meteorological organization 2008 recalling that the total drainage area of lgrb is approximately 209 000 km2 there should be 37 automatic stations or 364 manual stations to meet the wmo guideline for a minimum snow network therefore it can be concluded that there is a large deficit in snow monitoring to overcome sparse density of snow monitoring networks and difficulties in data samplings there is a need to install automatic snowpack sensor stations in lgrb which can measure calculate and transmit the snow water equivalent swe with comparable accuracy to other ground based techniques such as snow courses snow pillows and weighing precipitation gauges the snow data for the existing stations were obtained from the sample observations during winter months for the years of 1970 2005 36 years so that the number of total data points for each station is 144 a gridded swe dataset for the entire study area is also available from hydro québec the hydropower company of québec province tapsoba et al 2005 specifically the 14 km resolution of the grid cells provides 990 grid points in lgrb in this study the weather stations currently installed in lgrb are chosen as the potential locations of new swe observations hereafter potential stations following personal communications with hydro québec about the snow monitoring needs and plans and the number of potential stations is 47 see fig 1 the snow data of the potential stations are obtained from the swe of the closest grid point it should be noted that the current monitoring sites called existing stations are mainly for the ground based snow observations such as snow courses snow pillows or weighing precipitation gauges while automatic snowpack sensor stations are being planned at the optimal locations of additional stations therefore the potential station locations which are very close to the existing monitoring sites are also considered given that automatic stations could be placed at selected manual monitoring sites to ensure continuity of historical snow observations 2 2 columbia river basin british columbia the entire columbia river basin covers parts of british columbia canada and the northwestern united states however this study will consider the upper parts of the columbia river basin fig 3 where the british columbia hydropower company namely bc hydro is responsible for the water resources management it should be noted that the southern part of the basin is extended to the states of idaho and montana and the american side of the basin is not included in the network design in this study the drainage area of the columbia river above arrow lakes dam and the kootenay river above kootenay canal excluding the u s portion of the kootenay lake watershed which will be referred to as crb hereafter is approximately 70 700 km2 the crb is made up of mostly mountainous terrain with only a few hilly areas as shown in fig 3 considering the geographic location and the elevation range of the crb it is critical that an adequate snow monitoring network exists however many stations have been installed at lower elevations even though there is typically more precipitation at higher elevation kite 1997 two long term climate stations ft steele dandy crk climate id 1153034 and sparwood climate id 1157630 are close to each other in the southeastern crb but located on different elevations which are 856 m and 1138 m respectively fig 4 presents the mean daily temperature and precipitation of 30 year observations at the two stations showing significant differences in precipitation patterns specifically there has been more snow at the higher location while the amount of rainfall is similar as the water resources in the crb have been variously used in hydropower generation irrigation recreation as well as ecosystem health the design of an efficient snow monitoring network is essential data for this study was collected from the british columbia river forecast centre bcrfc and environment and climate change canada eccc snow data stored on the bcrfc portal are from manual and automatic stations operated by bc ministry of environment bc hydro rio tonto alcan and metro vancouver the eccc stations are measuring snow on ground and swe from automatic stations manual snow surveying process includes extracting a snow core and weighting the amount of frozen and liquid water extracted the surveys are typically conducted at the beginning of the month from january to june and additionally mid month in may and june at selected stations the automatic system of bcrfc collects snow and climate data at 15 min or hourly interval and transmits on an hourly basis the numbers of snow monitoring stations that are currently in operation are 9 automatic and 35 manual stations from bcrfc and 4 automatic stations from eccc see fig 3 the physiographic unit of the crb is mostly matched with mountains based on the wmo guidelines of which non recording station minimum density should be 250 km2 per station and recording station minimum density should be about 2500 km2 world meteorological organization 2008 recalling the total study area of crb the required number of automatic stations to meet the wmo minimum network density is 29 to determine the required number of new stations for meeting the wmo guidelines the number of the manual stations were converted to the equivalent number of the automatic stations this conversion was based on the information from the wmo guidelines world meteorological organization 2008 in specific manual stations were modeled as 1 10th of an automatic station for the interpolation 35 manual bcrfc stations can be regarded as 3 5 automatic stations this is rounded to 3 stations so that the total number of existing snow monitoring stations is converted to 16 automatic stations including 13 automatic stations from both bcrfc and eccc therefore 13 new stations are required for the crb to meet the wmo guidelines the potential locations where new additional stations may be located are from the gridded binary code edition 2 grib2 which is an international grid standard for data transmission by the wmo glahn and lawrence 2002 world meteorological organization 2003 the grib2 format defines 935 824 polar stereographic grids in north america for the regional deterministic prediction system government of canada 2015 accordingly the number of potential stations becomes 344 as shown in fig 3 this study used the snodas database by national snow and ice data center national operational hydrologic remote sensing center 2004 in which data from satellite and snow stations are assimilated as the snow values for the potential stations after comparing the recent observations from the ground based stations and the snodas there were significant differences before oct 2008 however the two datasets match each other quite well afterward therefore the data period was chosen as the period of oct 2008 sep 2011 to ensure consistency and reduce uncertainty to further assess the consistency between the two datasets at the potential locations four geospatial analysis methods were used in specific 1 no adjustment i e raw data 2 linear regression and inverse distance weighting lridw 3 linear regression and kriging lrk matheron 1963 and 4 cokriging methods deutsch and journel 1992 were used to correct the snodas data based on the selected ground based gauge data the no adjustment just used the original swe values that were retrieved from snodas database the lridw firstly created a linear regression function between the historical snow observations and snodas then the residuals between the two datasets were interpolated using the idw method lastly the interpolated residuals were added to the estimated swe from the linear regression the lrk is similar to the lridw but in the second step the residuals were interpolated using the kriging method the cokriging used kriging estimates which are based on the distance to nearby observations and its variogram the distance to nearby snodas samples and its variogram and the cross variogram for the observations and snodas leave one out cross validation was then applied to evaluate the accuracy of each geospatial analysis technique in specific each of the nine bcrfc automatic stations was treated as if it was ungauged on at a time the root mean square error rmse value of each station and the median and the mean values for each geospatial analysis methods are shown in table 1 the results indicated that there were only minor differences between the four methods specifically 1 mm in median and 4 9 mm in mean considering that the performances of the four geospatial analysis methods were similar and that the cokriging method produced slightly lower rmse in median the cokriging was selected to generate the swe at each potential station in this study however it should be noted that other methods could also be used due to their competitive performances the results table 1 suggest that any of the four geospatial analysis techniques could be used in this case 3 background 3 1 entropy measures in the information theory shannon entropy provides a way to measure the amount of information content that can be given from a variable shannon 1948 the basic measure is marginal entropy which represents the amount of information content in a variable this concept can be extended to joint entropy and total correlation representing the multivariate information content and the duplicated information between variables respectively venn diagram provides one of the best conceptual descriptions of entropy fig 5 shows the schematic overview of entropy measures when there are three random variables e g blue orange and green in fig 5a the size of each color i e variable refers the marginal entropy of each variable fig 5a in a similar manner the joint entropy is given by the total area fig 5b and the duplicated information is shown as the overlapping area fig 5c therefore the efficient network may have the greater total area and the smaller overlapping area marginal entropy in hydrometric network design problems represents the inherent information contents from observations at station x singh 1997 1 h x i 1 n p x i log 2 p x i where h x is the marginal entropy of a station x in bits x i is the frequency of the ith bin from the time series histogram of station x p x i is the occurrence probability of x i and n is the total number of bins the marginal entropy can be extended to the joint entropy when the number of random variables is greater than one representing the total amount of information from a network with multiple stations 2 h x 1 x 2 x n i 1 1 n 1 i 2 1 n 2 i n 1 n n p x 1 i 1 x 2 i 2 x n i n log 2 p x 1 i 1 x 2 i 2 x n i n where h x 1 x 2 x n is the joint entropy of n stations n 1 n 2 n n are the number of bins of the according stations of the network containing n stations krstanovic and singh 1992 to calculate the joint probability p x 1 i 1 x 2 i 2 x n i n multivariate time series data agglomerate to form a new single variable time series the process of agglomeration is similar to the numeral system with various radixes that is the data of each station become the values of each digit alfonso et al 2010b due to the possible duplications of information contents between stations the joint entropy of a network is less than the sum of the marginal entropy of each station except for that every station is independent to each other the total correlation is a measure for evaluating inefficiency in a network and defined as the difference between the joint entropy and the sum of marginal entropies mcgill 1954 watanabe 1960 3 c x 1 x 2 x n i 1 n h x i h x 1 x 2 x n where c x 1 x 2 x n is the total correlation of n variables since eqs 1 3 are applicable to discrete random variables the number of events and the bin width is determined by quantization methods which will be provided in 4 2 quantization methods 3 2 data quantization there are two options of the entropy calculation from a time series dataset discrete entropy and continuous entropy singh 1997 entropy terms were initially presented for discrete variables shannon 1948 and extended to continuous variables considering most hydrological processes in the water cycle are continuous the use of continuous entropy may provide more reliable outcomes because there is no need to convert continuous time series to a discrete dataset however calculations of multivariate joint entropy from continuous distribution functions have essential limitations at this time specifically the proper distribution should be assumed and even more importantly the mathematical forms of continuous joint entropy other than gaussian distribution are still limited ozkul et al 2000 singh 2013 yang and burn 1994 therefore the discrete entropy is more applicable to hydrometric network designs to estimate discrete entropy from a hydrologic variable the continuous dataset should be discretized by any quantization method even if the actual value of the entropy may change due to data quantization while discrete entropy has largely been applied to the network design during the recent decades only a few have compared different quantization method as summarized by fahle et al 2015 most previous studies concluded that the final spatial distribution of optimal networks or rankings of candidate stations are not significantly affected by the quantization method alfonso et al 2013 li et al 2012 markus et al 2003 mishra and coulibaly 2010 alfonso et al 2014 brought the concept of ensemble entropy to take uncertainty from quantization into account specifically they introduced a numeric factor which is used in the quantization process as well as the multiplication factor however the conclusions of previous studies have not been made by comparing different quantization methods but brought by changing the parameters of factors in the same method on the other hand fahle et al 2015 found a pronounced influence of the multiplication factor and also compared the rounding and scott s methods scott 1979 they ultimately chose the rounding method since scott s method tends to smoothen the differences of the marginal entropies recently keum and coulibaly 2017 introduced a relative ranking concept to determine how the station rankings are affected by different quantization methods they concluded that the differences were evident when the different quantization methods were used 3 3 optimization tool the typical objectives of the entropy based optimization problems for hydrometric network design are maximizing joint entropy and minimizing total correlation keum et al 2017 one optimization approach is the maximum information minimum redundancy mimr which reduces problem complexity by combining the design objectives into a single comprehensive objective fahle et al 2015 li et al 2012 however the trade off weight of each objective should be assumed prior to the optimization to avoid any preassumption the dual entropy and multiobjective optimization demo samuel et al 2013 was chosen as the optimization tool in this study the demo finds optimal solutions by employing the entropy concept in information theory as a measure of information contents from a monitoring station or a hydrometric network and using the epsilon dominance hierarchical bayesian optimization algorithm ε hboa kollat et al 2008 samuel et al 2013 the ε hboa is an enhanced multi objective evolutionary algorithm which was developed to overcome mathematical difficulties due to the number of design objectives and the interdependencies between a large number of decision variables on hydrometric network design problems kollat et al 2008 in ε hboa three major additive features were added to its precursor the non dominated sorted genetic algorithm nsga deb 2000 1999 deb et al 2002 such as 1 the epsilon dominance for allowing a tolerance of the selection of pareto sets 2 the hierarchy for reducing problem complexity by hierarchical decomposition to sub problems and 3 the bayesian network models for exploiting dependencies between decision variables instead of mutation and crossover of traditional genetic algorithms kollat et al 2008 osman et al 2006 pelikan 2002 4 methods an overview of the steps in the methodology and the data processing is presented in fig 6 briefly the snow network design for la grande river basin integrates hydrologic model performances with the demo applications using the interpolated snow data for potential stations while the network design in columbia river basin utilizes snodas datasets for swe at potential stations the description of each step is as follows 1 1 gather swe data at ground based existing stations 1 2 obtain swe data at potential stations from the spatially interpolated grid data provided by hydro québec 1 3 search the pareto optimal networks by applying demo and datasets from 1 1 and 1 2 1 4 calculate watershed average swe from observations for hydrologic model calibration 1 5 calculate watershed average swe including the potential stations selected by demo 1 6 gather weather data to run hydrologic models 1 7 calibrate hydrologic models using observed swe and the weather data 1 8 simulate watershed outflows these are actually reservoir inflows in the lgrb using the calibrated models 1 9 gather the observed reservoir inflows 1 10 calculate model performance criteria 1 11 suggest the optimal network in the lgrb based on the model performance 2 1 gather swe data at ground based existing stations 2 2 locate snodas datasets 2 3 correct snodas product with station data using geospatial analysis and retrieve swe for potential station locations 2 4 apply demo for two cases case 2 no constraint about the number of additional stations and case 3 set the number of additional stations based on the wmo guideline 2 5 find the optimal networks from demo results 2 6 draw the hot spot maps using the selected stations in the optimal networks to determine the area where the additional monitoring effort is mostly required 4 1 determination of optimal networks based on the definitions of entropy measures an optimal hydrometric network should provide a maximum amount of effective information i e maximize fig 5b with a minimum amount of redundant information i e minimize fig 5c accordingly the design objectives become 4 max h s n m h e 1 e 2 e n x 1 x 2 x m min c s n m c e 1 e 2 e n x 1 x 2 x m subject to n and m are integers case 1 lgrb no constraint m 1 2 m max lgrb case 2 crb no constraint m 1 2 m max crb case 3 crb wmo guideline m m wmo where s n m is a network consisting of n existing stations e and m additional stations x m max is the total number of potential stations for each study basin and m wmo is the required number of new stations to meet the wmo guidelines here we propose three study cases in cases 1 and 2 there is no specific constraint about the number of new additional station but the maximum numbers are limited to the number of all potential stations in each basin in these cases the multi objective optimization will determine the number of new stations based on the objective function values on the other hand case 3 is additionally applied to the crb as there is a deficit in snow monitoring network and there is a large number of potential locations from the grib2 grid points in specific the number of additional stations is fixed as 13 in case 3 but the locations will be determined by the multiobjective optimization processes the details of each case are summarized in table 2 4 2 quantization methods as stated earlier entropy values using discrete entropy measures are affected by data quantization this study compares three types of quantization methods rounding eq 5 scott s eq 6 and sturges eq 7 5 rounding x r i floor 2 x i 2 i 6 scott h sc 3 49 s n 1 3 7 sturges h st r x 1 log 2 n where x r is the nearest integer value which is multiples of the multiplication factor i with given x floor is the floor function which removes all decimal places h sc and h st are the bin widths from scott s and sturges methods respectively s is the standard deviation of a series of x n is the total number of data points and r x is the range of a series of x rounding method is a simple uniform quantization method that results in products of the multiplication factor i scott s method was developed to determine the estimated bin widths underlying the gaussian density assumption but its application to other densities is still valid scott 1979 sturges method also gives the estimated class interval based on the range and the data points of a series sturges 1926 to calculate parameters of the bin width estimating methods i e the standard deviation of scott s method and the range of sturges method time series of all stations are aggregated so that the same estimated bin width for all stations is used this allows to avoid any possible overestimation or underestimation of information contents among stations 4 3 hydrologic models for the case 1 i e lgrb further analysis to determine better snow monitoring networks in terms of the hydrologic model performance is conducted for an enhanced spring runoff forecast first hydrologic models are calibrated using the swe of the existing station then the swe data is updated for each pareto optimal network with additional stations which are given by the results from demo next the hydrologic models simulate discharge i e reservoir inflows using each pareto optimal network finally the optimal network for spring peak flow is determined by evaluating the hydrologic model performance criteria in this study three hydrologic models are selected to obtain a general outcome of identifying the best performing optimal snow monitoring network the selected models are the service hydrométéorologique apports modulés intermédiaires hsami model the mcmaster university hydrologiska byråns vattenbalansavdelning mac hbv model and the sacramento soil moisture accounting sac sma model hsami is a lumped hydrologic model which uses basin averaged data as inputs this model was developed by hydro québec bisson and roberge 1983 fortin 2000 and has been mostly used to simulate streamflow especially in québec minville et al 2008 there are 23 parameters in hsami two for evapotranspiration six for snowmelt three for surface runoff estimation seven for vertical flow in soil and five for horizontal flow the hsami parameters were already calibrated to each watershed in the lgrb by hydro québec using the shuffled complex evolution optimization method and the nash sutcliffe efficiency criterion minville et al 2014 2008 nash and sutcliffe 1970 a 36 year data period is selected from 1970 to 2005 and separated into even and odd years for calibration and validation respectively to maintain consistency in the calibration of model parameters the selected calibration and validation periods are also used for the other hydrologic models mac hbv model is also a lumped rainfall runoff model and was modified from the original hbv model bergström 1976 by including nonlinear storage discharge formulation to provide better performance especially in baseflow estimation at ungauged sites samuel et al 2012 the mac hbv includes 14 parameters in four hydrological routines snow soil moisture response and routing scheme the snow routine represents changes in the snowpack using a simple degree day concept the soil moisture routine represents the soil moisture accounting or changes in soil moisture storage in the top soil layer the response function estimates the amount of runoff from the upper zone and lower zone based on the current water storage and the maximum storage for channel routing an equilateral triangular weighting function is used to obtain the final streamflow samuel et al 2011 for a detailed description of the mac hbv model the interested readers may refer to the journal articles by samuel et al 2012 2011 the third hydrologic model used is the sac sma model which has been widely used since its development burnash et al 1973 the sac sma conceptual model consists of eight input parameters and six state variables in this model moisture in a soil box is divided into two upper zones tension and free water storages and three lower zones tension water storage and primary and supplementary free water storages the two types of lower free water storages allow groundwater storages with two different drainage factors for better estimation of baseflow the routing approach used in this model is the nash cascade method and the same snow component and evapotranspiration calculation methods used in the mac hbv model are added to this model for more details on the adapted model please see razavi and coulibaly 2017 as stated previously the same time series data is used to calibrate and validate the mac hbv and the sac sma models 4 4 model performance criteria model performance statistics or criteria are often used to evaluate the accordance of predicted values to the observed values of hydrologic variables based on the mathematical formulation of targeting higher or lower part of the hydrograph the proper performance criteria will vary in this study three criteria which perform better especially in high flows or peak flows were selected pearson correlation coefficient pr nash sutcliffe efficiency nse and index of agreement ia krause et al 2005 for all three criteria the closer values are to 1 the more accurate the simulated flows are the model performance criteria used in this work are as follows 8 pr i 1 n q i q q i q i 1 n q i q 2 i 1 n q i q 2 9 nse 1 i 1 n q i q i 2 i 1 n q i q 2 10 ia 1 i 1 n q i q i 2 i 1 n q i q q i q 2 where q i is the observed flow at ith data point q is the mean observed flow q i is the simulated flow at ith data point q is the mean simulated flow and n is the number of data points given that the focus herein is on spring peak flow which is driven by snowmelt the selected criteria are therefore used to assess peak flow and are denoted as ppr pnse and pia respectively peak flows that are greater than one third of the mean peak flow are used to calculate the performance measures ribeiro et al 1998 5 results and discussions 5 1 selection of data quantization even if the same data were used entropy values can change due to the data quantization from zero for the one bin histogram to the saturated entropy log 2 n if each bin does not contain more than one value in it therefore it is reasonable to compare the effects of quantization on monitoring network design problems recall that the numbers of the data points are 144 for lgrb and 1095 for crb the maximum entropy values from eq 1 or 2 are log2 144 7 17 bits and log2 1095 10 01 bits respectively when the number of data points is relatively small compared to the variability of time series dataset i e many unique values the joint entropy can be easily saturated having the maximum value and then a network with the lowest total correlation among them will be chosen as the optimal network having an optimal network which has saturated entropy is less reliable since the network may be selected due to hitting the boundary i e the saturated entropy and not because of its optimality fig 7 shows the ranges of joint entropy values of the optimal networks with regards to different data quantization and their numbers of optimal networks from cases 1 and 2 while 1 100 mm swe were used as the multiplication factors of the rounding method in lgrb the highest joint entropies from the optimal networks that used a multiplication factor of 60 mm swe or less reached saturation the estimated bin widths from scott s method and sturges method are 43 7 and 81 6 mm swe respectively and the highest joint entropy of optimal networks using scott s method reached the saturated entropy while sturges method did not interestingly while the numbers of the optimal networks generally ascend when the multiplication factors of the rounding method increased the numbers using the estimated bin widths by scott s and sturges methods are smaller than the rounding methods with the closest multiplication factors in the case of crb the tested multiplication factors of rounding method are 1 5 10 15 and 20 mm swe and the estimated bin widths are calculated as 9 3 mm swe for scott s method and 18 0 mm swe for sturges methods respectively there is no quantization case which has an optimal network with the saturated entropy in crb compared to case 1 case 2 produced more optimal networks regardless of the quantization method it can be assumed that the smaller number of data points in lgrb caused the saturated entropy issue and the smaller number of optimal networks as the joint entropy values and the numbers of optimal networks are related to bin widths or multiplication factors as fixed bin widths and the number of data points a proper quantization may be able to lead to a reliable optimal network even though the effects of data quantization have been explored by several studies alfonso et al 2014 2010a keum and coulibaly 2017a li et al 2012 markus et al 2003 mishra and coulibaly 2010 to the best of our knowledge there is no universal guideline for the selection of quantization method therefore considering the joint entropy ranges of the optimal networks in both study areas we chose the sturges method as the primary quantization method for further analysis and others were used for comparison herein 5 2 optimal networks 5 2 1 la grande river basin after the demo application using sturges quantization method an ensemble of 31 pareto optimal networks in which one objective value cannot be improved without worsening the other objective value were determined the selected numbers of additional stations range from 1 to 5 among 47 potential locations and the joint entropy and the total correlation vary from 6 65 to 7 14 bits and 35 26 to 41 71 bits respectively recall that multiobjective optimization tools such as demo yield a number of pareto optimal networks the monitoring hot spot maps which enable to visualize the locations where monitoring efforts are highly required or not were drawn keum and coulibaly 2017a b kornelsen and coulibaly 2015 leach et al 2016 2015 in specific the number of occurrence in the optimal networks was counted and converted to a ratio ranging 0 100 for each station the strength of hot spot was then indicated by the size of the circle fig 8 shows the monitoring hot spot map using sturges quantization method in the case 1 it should be noted that a combination of the high percentage locations does not always indicate an optimal network but does inform about priority areas in network design extension the hot spots are highly noticeable at the stations 17 25 35 and 36 which were the members of the optimal networks 5 times or more while 30 stations out of 47 potential locations were not selected at all 5 2 2 columbia river basin considering all the existing stations remain active in the optimized networks and the number of potential stations is 344 the possible combinations of networks from cases 2 and 3 are 3 58 10 103 and 1 21 10 23 and the numbers of pareto optimal networks is 244 and 74 respectively it should be noted again that all the optimal solutions are considered equally good because none of the objective functions dominates one to another these pareto optimal solutions which are shown by blue circles in fig 9 are commonly called the non dominated solutions recalling that the case 2 does not have a constraint about the number of additional stations the demo determined numbers of additional stations vary from 44 to 134 when compared this range in case 2 to the additional number of station in case 3 which was fixed to 13 it can be confirmed that the wmo guideline only provides the minimum network density to avoid any data deficiency in water resources management world meteorological organization 2008 fig 10 shows the monitoring hot spot maps of each study case using sturges method for the both cases in crb comparing those maps the hot spots where the occurrence percentages are high of case 3 are fairly included in those of case 2 as well for example the common hot spots are shown along the valleys and some mountainous regions near the southeastern area of station 2a21p and the east of station 2d07a the results show that the difference in problem setting i e flexible vs fixed number of additional stations has evidently changed the spatial distributions of hot spots by expanding them considering that there are a number of the pareto optimal networks from each case and it is difficult to define the best design approach it is reasonable to preferentially install new snow monitoring stations where the hot spots are commonly appeared in case 3 to meet the minimum density by wmo and then expand to other hot spots in case 2 the hot spots shown in fig 10 were obtained by optimizing entropy values only and most of them appeared in valleys from a practical perspective water resources managers who support water supply and forecasting may tend to prioritize station installations at high snowpack areas while snow data users for ecological applications may be interested in the presence of permanent or sporadic winter snow cover others such as municipal city engineers may want to consider preferentially adding more stations in areas of high population density or industrial activity while this case study focused on the application of entropy theory based network design with snodas dataset the practical issues discussed here above have not been considered user specific needs could be included as additional cost function in demo however this is beyond the scope of this study 5 3 hydrologic model applications the demo application results in the case 1 were further analyzed by a combination of hydrologic models as previously stated the calibration results of the hsami model were obtained from minville et al 2008 in a similar manner the mac hbv and sac sma models were calibrated using the same data period by maximizing the nse from observed and simulated flows the calibration and validation results for the three models are presented in table 3 for the calibration all the nse values were greater than 0 6 and their averages are 0 76 0 79 and 0 84 for hsami mac hbv and sac sma respectively as for the validation many of the nse are greater than 0 6 except for three watersheds rupert bief amont rupert bief aval and lac mistassini these three watersheds do not have enough observed data points compared to the other watersheds as the observations were missing during approximately 80 of the total data period the average values of the validation nse are around 0 5 but the values are improved to 0 7 if the three watersheds with very limited data are excluded therefore it can be concluded that the model calibrations in general performed successfully to evaluate the enhanced performance gained by the hydrologic models when using the newly designed optimal networks swe data of each watershed was updated for each optimal network as the hydrologic models used in this study are semi distributed models set up for 12 watersheds the inversed distance weighting method idw was used to convert station data to watershed data specifically the station data was interpolated to the hydro québec grid points 990 grids in lgrb and the grid data in each watershed was averaged to estimate the watershed swe then three model performance criteria pnse ppr and pia were calculated for each watershed using the observed reservoir inflow and the simulated inflow from the three hydrologic models respectively the model performance statistics of the twelve watersheds were used to draw boxplots for each optimal network from all boxplots in fig 11 it appears that the optimal networks generally provided better performance than the existing network network 0 as the values of model performance increased even though all pareto optimal networks yield better performance than the existing network the best or recommended optimal network can be recognized when the values of performance criteria are sensitive to optimal networks comparing all the combinations of hydrologic models and performance criteria mac hbv model results evaluated by pnse were the most sensitive in specific the percent increase of pnse by optimal network 5 using mac hbv was estimated at 41 4 in la grande 3 watershed and 18 1 when averaged for all watersheds overall whatever the hydrologic models used moderate to large improvements were obtained when swe of optimal networks are used if compared to the existing network and the absolute criteria values are similar to each other however mac hbv shows larger changes than the other two models and the peak flow criteria were able to better differentiate the optimal networks therefore this case pnse using mac hbv is recommended for selecting the best network due to its higher sensitivity it should be noted that this conclusion is valid for this study and requires that a similar analysis be applied to other areas or other hydrologic variables such as rain or streamflow before drawing a more general conclusion in general whatever the hydrologic model the optimal network 5 provides better performance than other pareto optimal networks and the locations of its selected stations which are added to the existing stations are shown in fig 12 in this case network 5 is recommended as the best performing optimal network and it simply requires three additional stations in the locations identified fig 12 the three additional stations are well distributed spatially across the basin however considering that station 40 is not located on the strong hot spots the monitoring hot spot map does not always indicate the optimal network but should be used to inform the general locations where the monitoring efforts are mostly needed 6 conclusions a better estimation of swe is of particular importance for water resources planning and management in canadian watersheds due to the technological advances many automated snow monitoring sensors are able to measure the swe and transmit data to end users server without a frequent human access in this work a snow monitoring network design strategy for enhanced spring peak flow forecasting was proposed by investigating demo with snodas and hydrologic models three case studies were carried out to determine the optimal locations of new snow monitoring stations in case 1 hydrologic model simulations were included to further evaluate pareto optimal networks to identify the overall best performing optimal snow monitoring network in the lgrb in cases 2 and 3 a remote sensing database snodas was used to derive snow values for the potential snow monitoring locations in crb the difference between case 2 and 3 was the consideration of the wmo guideline for the minimum density of monitoring networks the number of additional stations in case 2 was not pre defined but determined during the demo process while that in case 3 it was fixed as 13 to meet the wmo minimum network guideline combining the demo with the hydrologic models was shown to be an effective strategy for the design of optimal snow networks the advantage of the entropy optimization is on determining optimal networks that have the maximum amount of information with minimum redundancy however as the optimization problem has multiple objectives it is unavoidable to have many optimal solutions or networks in the pareto fronts the use of hydrologic models as part of the network design process was effective in identifying the best performing network among the pareto optimal solutions especially for spring peak flow forecast it was found that in general all three hydrologic models used in case 1 yielded forecast improvement when an optimal snow network swe data was used however mac hbv appeared most sensitive to the changes of swe from the different optimal networks this allowed for the identification of the best performing optimal network most suitable for spring peak flow forecasting the analysis results indicated that network 5 offered the best performance as compared to other pareto optimal networks although the number of selected stations in the network 5 is lower compared to other networks it should be noted that other factors such as cost accessibility and other variables e g topography land cover could be added as additional objective functions in the snow network optimization process the applicability of snodas data in network design was assessed in cases 2 and 3 the optimization results and the derived monitoring hot spot maps were able to clearly locate regions where the monitoring efforts are mostly needed considering that the most challenging part of the demo application is to obtain hydrologic time series for the potential stations it was shown that the snodas products can be one of the solutions this study also analyzed the effects of the quantization method for the conversion to discrete time series in the entropy calculations the results from the case studies showed that the data quantization may affect the optimal networks as clearly shown in case 1 therefore careful consideration and an application of various quantization methods are required in hydrometric network design to obtain a reliable optimal network acknowledgments this research was jointly supported by the natural science and engineering research council nserc of canada environment canada hydro québec and bc hydro we would like to thank dr joshua kollat at pennsylvania state university for providing the source code of the ε hboa catherine guay marc durocher claude gignac from hydro québec for providing some data and rené roy for his assistance in the administrative approval process by hydro québec karina seto for processing some of the initial data and jos samuel at yukon college for the preliminary work and initial data processing for the crb the authors acknowledge marcus fahle and two anonymous reviewers for their comments that helped to improve the manuscript 
7304,lake taihu is a typical shallow lake which frequently happens sediment resuspension induced by wind induced waves the experiments are carried on to simulate the wave disturbance processes in wave flume by setting a series of wave periods 1 2 s 1 5 s 1 8 s and wave heights 2 cm 10 cm it aims to analyze the characteristics of sediment resuspension and the mechanisms of nutrients release and to evaluate the effects of sediment dredging on sediment resuspension and nutrients release in lake taihu the results show that wave shear stress during 2 cm and 10 cm wave height processes ranges 0 018 0 023 n m2 and 0 221 0 307 n m2 respectively wave shear stress has no significant differences between wave periods wave height has much more effects on sediment resuspension wave height of 2 cm could induce total suspended solids tss reaching up to 5 21 g m2 and resuspension flux of sediment m up to 1 74 g m2 tss sharply increases to 30 33 52 41 g m2 and m reached up to 48 94 g m2 when wave height reaches to 10 cm the disturbance depth under different sediment bulk weights ranges from 0 089 to 0 161 mm variation of suspended solids in 3 layers 1 cm 5 cm 20 cm above sediment interface has no significant differences organic matter tn and tp have positive relationship with ss organic matter is only accounted for 5 7 7 3 of ss the experiments under different sediment bulk densities 1 34 g cm3 1 47 g cm3 and 1 59 g cm3 find that tss and m fall by 44 2 and 39 8 with sediment bulk density increasing respectively total tn dtn tp and dtp decrease by 24 3 33 6 it indicates that sediment dredging could effectively reduce ss concentration and nutrient levels in water column the researches provide a theoretical basis for sediment dredging to control the shore zone of lake taihu for lake management keywords wave flume shear stress sediment resuspension sediment dredge nitrogen phosphorus 1 introduction shallow lakes are characterized by episodic resuspension of sediments resuspension occurs when the bottom shear stress exceeds the critical shear stress which depends on water content and sediment grain size håkanson and jansson 1983 several hydrodynamic processes can trigger resuspension including wind induced waves hamilton and mitchell 1997 wu et al 2013a b current and turbulence fluctuations you et al 2007 luo et al 2006 although wind induced waves are commonly the primary driving force in shallow lakes håkanson 2005 sheng and lick 1979 found that wave generated shear stress contributed to more than 70 of sediment resuspension happened in shallow lakes frequent wind induced resuspension generally occurs over temporal scales from minutes to hours villard et al 1999 liu and huang 2009 and may affect the function of lacustrine ecosystems the dynamic processes at water sediment interface are critical for transporting solid particles and nutrients wang et al 2014 wu et al 2013b internal nutrient release occurs when sediments are re suspended by wind driven currents and waves acting on the water sediment interface excessive nutrients loading especially nitrogen n and phosphorus p has led to the appearance of eutrophication and massive harmful algal blooms smith 1983 abell et al 2010 xu et al 2010 zhang et al 2014 lake taihu is a typical shallow lake with a surface area of 2338 km2 and a mean depth of 1 9 m which commonly happens wind inducedsediment resuspension due to the lake s high dynamic ratio håkanson 1982 approximately 1100 km2 of lakebed are covered by sediment which accounts for about 47 5 of the lake area luo et al 2004 due to the southeast monsoon sediments are primarily deposited in the western and northern portions of lake taihu with thickness greater than 4 m luo et al 2004 lakebed sediments are commonly rich in n and p especially top surface sediments nitrogen content in the surface sediment is 0 05 0 2 while total phosphorus is 0 01 0 05 fan et al 2000 the hydrology and nutrient input result in a trophic gradient characterized by hypertrophic conditions in the northern part of lake taihu and mesotrophic conditions in the southeastern part of lake taihu chen et al 2003 satellite imaging shows that algal blooms are prominent in large parts of the northwestern and central lake after typhoon events zhu et al 2014 in 2007 lake taihu experienced a toxic algae bloom that made headlines around the world qin et al 2010 over the subsequent decade considerable attention has been paid to eutrophication control in lake taihu nutrient release induced by sediment suspension is an important nutrient source in shallow eutrophic lakes wu et al 2013a wang et al 2015 current field monitoring of wind induced sediment resuspension mainly focuses on water quality fan et al 2003 zhu et al 2007 wu et al 2009 it is difficult to understand the relationship between wave action and nutrients release due to complex hydrodynamics numerical models are applied to reproduce water currents and waves to study the effects of hydrodynamics on sediment resuspension and water quality hu et al 2006 numerous experimental methods have been applied to analyzing the hydrodynamic effects of waves on sediment suspension including y shape apparatus you et al 2007 annular current flume experiments widdows et al 2002 and wave flume experiments sun et al 2006 wave flume experiment is ideal for defining wave stress by regulating wave height and period in this study we design wave flume experiments to explore the characteristics of sediment resuspension and the mechanism of sediment nutrient release in lake taihu the aims of these serial experiments are to 1 analyze the hydrodynamic conditions under different wave periods and wave heights in the wave flume 2 analyze the variations of suspended solids ss and nutrients in different hydrodynamic conditions 3 establish the relationships among wave characteristics resuspension and nutrients release 4 evaluate the effect of sediment dredging from the perspective of sediment bulk density 2 materials and methods 2 1 study area lake taihu is the third largest freshwater lake in the lower yangtze river delta in china between 30 56 31 33 n and 119 53 120 36 e water temperature ranges from 4 8 to 29 2 c with an average annual water temperature of 17 3 c zhao et al 2011 wind speed varies between 0 and 10 m s with an average wind speed of 4 3 m s southeaster in spring and summer and 0 9 m s northwester in the autumn and winter qian 2012 water quality has seriously degraded over the last several decades due to severe eutrophication huang et al 2014 outbreaks of cyanobacteria blooms are commonly correlated with sustained winds which confirms that wind induced wave disturbance is an essential driving force for internal nutrient release induced by sediment resuspension in lake taihu kristensen et al 1992 ding et al 2012 algal blooms are most intense in zhushan and meiliang bays xie et al 2003 fig 1 meiliang bay is a semi enclosed bay in the northern part of lake taihu with an area of 130 km2 and a mean depth of 1 8 m fifty percent of the meiliang bay lakebed is covered by sediment rich in organic matter fan et al 2000 since 1998 meiliang bay has experienced severe algal blooms in summer and autumn chen et al 2003 guo 2007 the liangxi and zhiwu rivers bring municipal contaminants from wuxi city and changzhou city into meiliang bay which contributes to increasing eutrophication 2 2 experiment sediments collected from meiliang bay mainly consist of silty sand with an average particle size of 13 μm the top 15 cm sediments rich in organic matter were collected by the environmental cutter suction dredger and then transferred to the laboratory at the hydraulic academy in nanjing chinese ministry of water resources the top 15 cm sediments are the active sediment layer in taihu lake qin et al 2004 hu et al 2006 then sediments were stirred well and paved on the middle of wave flume with sediment length 7 m and thickness 7 10 cm fig 2 after sediment in the flume naturally settled for one month water was slowly poured into the wave flume until the water depth reached at 40 cm depth then the system was kept still for several days the wave flume length 25 m width 0 5 m height 0 8 m is equipped with wave maker at the inlet and wave alleviator at the opposite end wave height and wave period were collected by capacitive wave instrument mts usa which are installed at both ends of the sediment section the whole system is automatically controlled via a computer the wave intensity is controlled by setting six groups of hydrodynamic conditions for different wave periods 1 2 s 1 5 s and 1 8 s and wave heights 2cm 10 cm to simulate wave disturbances commonly occurring in lake taihu the bulk density of top 2 cm sediments in wave flume is measured before each group test 1 34 g cm3 1 47 g cm3 1 59 g cm3 respectively water samples are collected at 3 water column layers 1 cm 5 cm and 20 cm above the water sediment interface respectively fig 1 three parallel water samples are taken for each layer after background sampling the wave maker is switched on to produce regular sinusoidal waves 2 cm height wh2 for 60 min and then 10 cm height wh10 for 120 min water samples are collected at 30 and 60 min during the wh2 wave period and at 70 90 120 and150 min during the wh10 wave period 2 3 sample analysis water samples were divided into two subsamples with one set of subsamples immediately filtered for suspended solids and the other set analyzed for nutrients to determine ss the remaining particulate matter on the filters after 100 250 ml sampling water was dried at 105 c for 4 h until its weight kept constant organic matter om was measured using a 4 h ignition of the dried particulate matter on a membrane at 550 c the loss on ignition loi is the ratio of om to ss total nitrogen tn total phosphorus tp dissolved total phosphorus dtp and dissolved total nitrogen dtn were determined by spectrophotometry after digestion with alkaline potassium persulfate jin and tu 1990 the detection limit is 0 01 mg l for tn and dtn and 0 01 mg l for tp and dtp 2 4 data processing preliminary treatment of the experimental data was performed in matlab 7 0 one way analysis of variance anova was carried out using spss 19 0 software to test for significant differences between the various hydrodynamic conditions a significance level of p 0 05 is used for all tests 2 4 1 wave shear stress grant and madsen 1979 first proposed the basic theory of bottom shear stress at the water sediment interface in shallow lakes the shear stress caused by wave at the water sediment interface can be calculated by the following equation 1 τ w b 0 5 ρ f w u m 2 where τ w b is the wave shear stress n m 2 ρ is the water density kg m 3 f w is the wave friction coefficient u m is the maximum wave orbital velocity near the bed m s the f w is calculated as follows jiang et al 2000 2 f w exp 5 2 a δ k s 0 19 6 0 where a δ is the wave particle amplitude m which is determined by linear wave theory a δ h s 2 sinh 2 π h l s where h s is effective wave height m l s is wave length m k s is physical roughness of the lake bottom this study uses 0 2 mm as the lake bottom roughness based on previous studies hawley 2000 luo 2004 nielsen et al 2001 qin et al 2004 when a δ k δ 1 59 f w 0 3 u m is calculated as follows 3 u m π h s t s sinh 2 π h l s where t s is wave period s and l s is effective wave length which can be expressed as follows 4 l s gt s 2 tanh 2 π h l s 2 π since the total shear stress is primarily caused by wind induced waves in lake taihu li et al 2017 the shear stress is approximated as the wave shear stress 2 4 2 resuspension flux samples were collected at 1 5 and 20 cm above the water sediment interface so the corresponding layer thicknesses were 3 cm bottom layer 7 cm middle layer and 30 cm upper layer respectively total amount of suspended solids in the water column per unit area is the total of suspended solids tss g m2 which is calculated as follows 5 tss i 1 n ss i δ z i 100 where ssi is suspended solids concentration in the i th layer mg l δzi is the thickness of the i th water layer cm n is total vertical layers 6 m i tss i tss i 1 the resuspension flux mi is the resuspension flux at i th sampling time the sediment disturbance depth can be calculated using the following equation gouleau et al 2000 gao and jia 2004 7 h m γ wet 1 w where h is the mean disturbance depth mm m is the total amount of resuspended solids g m2 γwet is the wet bulk density of sediments g cm3 w is the moisture content of sediments in order to analyze the variations of nutrient content we introduce the total amount of water column material per unit area t g m2 the specific calculation method is similar to tss 3 results 3 1 hydrodynamic characteristics the dynamic process has wh2 wave process and wh10 wave process during wh2 wave processes the maximum wave orbital velocity near the lakebed ranges from 0 031 m s to 0 041 m s table 1 bottom wave shear stress ranges from 0 018 to 0 023 n m2 table 1 when wave height reaches up to 10 cm the maximum wave orbital velocity varies from 0 154 m s to 0 256 m s and wave shear stress is from 0 221 n m2 to 0 307 n m2 the wave shear stresses during wh2 wave processes are remarkably small the wave shear stress during wh10 wave processes is significantly larger than that under 2 cm wave height p 0 01 the relationship between the wave period and bottom shear stress shows the positive correlation p 0 01 which indicates that shear stress magnitude increases with wave period increasing while wave shear stresses have no significant difference between 1 2 s 1 5 s and 1 8 s 3 2 sediment resuspension and hydrodynamic conditions wave shear stress acting on the sediment water interface exceeds the critical stress to induce sediment resuspension and to increase tss concentration according to the hydraulic judging criteria the critical stress for sediment resuspension means that the surface particles began to gradually float upward during the wh2 wave processes sediment particulates just floats near the sediment water interface and only the smaller sediment particles are suspended into the water column tss varies from 1 32 to 5 21 g m2 and resuspension flux is 0 38 1 74 g m2 under different wave periods during the wh2 wave period table 2 tss and resuspension flux m under different wave periods have no significant increase based on the initial values wave shear stresses under 10 cm wave height are much larger than the critical stress during wh10 wave processes sediment particulates are largely suspended into water column tss increases sharply to 30 33 g m2 52 41 g m2 which is almost 50 times of the initial value resuspension flux reached up to 48 94 g m2 tss and resuspension flux m under different wave periods has no significant difference p 0 01 sediment disturbance depth depends on wave shear and sediment bulk weight during 1 5 s wave period processes the calculated disturbance depth under different bulk weigh sediments ranges from 0 089 mm to 0 161 mm table 3 the disturbance depth is proportional to tss in the water column p 0 01 and inversely proportional to the wet bulk density of the sediment p 0 01 in other words the disturbance depth increases with the suspended solids increasing and decreased with sediment bulk density 3 3 variations of ss and nutrients during suspension processes before beginning to make waves the background water samples in three layers above sediment 1 cm 5 cm 20 cm are collected from bottom to top the layers ss concentration in water column is 3 4 mg l 3 4 mg l and 2 0 mg l fig 3 respectively the initial tss is 0 94 g m2 fig 4 after 60 min of wh2 wave processes there is no significant increase of ss concentration in the overlying water the ss of the bottom and middle layers rises slightly to 7 8 mg l and 8 1 mg l respectively and the upper layer is unexpectedly reduced to 1 75 mg l at wh2 stage the bottom shear stress is only 0 022 n m2 which is less than the critical shear stress so sediment particles remain near to the water sediment interface and oscillate with the wave action subsequently as the wave height increases to 10 cm ss in each layer increases rapidly and transparency of the overlying water is reduced ss concentration at 1 cm 5 cm and 20 cm above the water sediment interface increases to 27 times 17 times and 12 times of the initial value respectively tss increases rapidly to 15 times of background levels fig 4 ss concentration gradually increases and reaches to the peak after 120 min by now ss concentration in the bottom middle and upper layers is 160 9 mg l 127 8 mg l and 116 9 mg l respectively tss also increases to 48 8 g m2 which is higher than 50 times of initial value ss concentration of the bottom layer was significantly higher than that of the upper p 0 01 and middle p 0 01 layers and variation trends of ss concentration in three layers keep consistent tss has the positive correlation with om tn and tp table 4 p 0 01 which further indicates that the variation trends of om tn and tp are consistent with ss in the overlying water throughout the trial after wh2 wave process the total amount of om has a slight increase while loi decreased from 62 to 38 fig 4 while the total amount of tn and tp experience a slight increase fig 5 after 60 min of wh10 wave process total amount of om increases from 0 58 g m2 to 3 46 g m2 and loi decreases from 38 to 8 which indicates that the majority of re suspended solids are inorganic substances the background values of tn and tp is 0 704 g m2 and 0 015 g m2 respectively wh2 wave process has little effect on tn and tp concentration in water column after wh10 wave process begin tn and tp rapidly increase in the initial 5 10 min tn and tp steadily increases 1 16 to 1 86 times in the later 120 min dtn and dtp r show no obvious variation throughout the experiment specifically dtn and dtp significantly decrease after 30 min of wh10 wave which may be due to the increase of particles concentration which could adsorb the soluble nutrients 3 4 sediment resuspension under different sediment bulk densities sediment dredging remove the surface high organic and floppy sediment and induces to increase the sediment bulk density before the wave processes the bulk densities of the top 2 cm sediments is measured 1 34 g cm3 1 47 g cm3 1 59 g cm3 respectively after wave processes tss and resuspension flux in water column under three sediment bulk densities is shown in table 5 results show that tss and resuspension flux of suspended solids m under three different wave periods are significant different p 0 01 tss and m increased with wave period increasing at the same wave period tss and m has an extremely significant decreasing with the sediment bulk density gradually increasing p 0 001 table 5 fig 6 a after twice sediment dredging tss and m in the overlying water have fallen by about 44 2 and 39 8 respectively similarly om p 0 01 fig 6b tn p 0 01 fig 6c dtn p 0 01 fig 6d tp p 0 05 fig 6e and dtp p 0 05 fig 6f are significantly reduced with increasing bulk density after dredging twice final values for total tn dtn tp and dtp decreased by 24 3 33 6 4 discussion 4 1 dynamic analysis and sediment resuspension in shallow lake ecosystem wind induced waves are the primary force driving sediment resuspension qin et al 2006 stone 2011 sediment resuspension is commonly estimated as a function of the shear stress on the lakebed above a critical value wang et al 2014 the critical shear stress inducing sediment suspension range from 0 01 to 0 1 n m2 fan et al 2004 james et al 1997 sheng and lick 1979 in the series of wave flume experiments the wave shear stress during the wh2 processes wave period is between 0 018 n m2 and 0 023 n m2 this value is equivalent with the critical value of 0 019 n m2 calculated by luo et al 2006 and 0 02 n m2 by li et al 2017 the wave shear stress during the wh2 processes is significantly smaller than the critical value 0 037 n m2 which derives from the medium size sediment with 0 017 mm grain size and 1 3 g cm3 sediment bulk density by qin et al 2004 according to researches on the suspended solids and wave shear stress wh2 and wh10 wave processes are equivalent to the field wind speed of 3 0 m s and 8 0 m s in lake taihu based on shear stress in the field qin et al 2004 luo et al 2006 sun et al 2006 respectively the critical wave stress inducing sediment resuspension depends on sediment size viscosity water content shape density and other chemical properties therefore it is complicated to quantitatively describe the relationship between the critical stress and sediment properties luo et al 2006 thus sediments properties vary widely due to significant temporal and spatial variation in lake taihu the critical stress will change according to these variations previous studies qiao et al 2011 li et al 2017 indicate there is a relationship between wave induced shear stress and ss according to the researches about the quantitative relationship between ss and wind speed v ss αvβ ss0 qiao et al 2011 we analyze the relationship between bottom shear stress and total ss om tn tp respectively fig 7 bottom shear stress adequately correlates with total ss r2 0 990 p 0 01 om r2 0 956 p 0 01 tn r2 0 967 p 0 05 and tp r2 0 986 p 0 05 the ss of each layer is different in various suspended conditions furthermore ss and bottom shear stress show similar correlations in 3 different water layers that shows that the best fit formula for ss and wave shear stress is ss αvβ ssc0 with r2 0 990 li et al 2017 divided the sediment suspension process into four stages using three shear stress thresholds during wh2 wave process 3 0 m s wind speed only a small amount solids were floated above the lake bed however a large number of sediments solids are rapidly suspended after wh10 wave action 8 0 m s wind speed which is consistent with sediment resuspension theory li et al 2017 vertically results show that after 10 min wh10 wave disturbance ss in the bottom middle and upper layers increased by 27 times 17 times and 12 times respectively however after 60 min wh2 wave disturbance ss in the bottom layer increases by only 4 4 mg l and ss in the upper layer dramatically decreases from 2 0 mg l to 1 75 mg l that means that the resuspension of sediments is accompanied by concurrent sedimentation of the suspended matter this phenomenon is particularly evident in the wh2 wave process following wh10 wave process ss in all layer increases significantly in the initial 30 min and then gradually stabilizes qin et al 2004 and ding et al 2011 found that mass exchange near the water sediment interface occurs primarily within the top 5 10 cm of sediment during this process ss in the bottom layer 1cm above the water sediment interface is significantly higher than that of the upper 20 cm above the water sediment interface and middle layer 5cm above the water sediment interface p 0 01 and the variation trend of ss in these three layers were consistent 4 2 effects ofsediment resuspension on nutrients release the dynamic processes of near bed sediments are critical for transporting solid particles and nutrients wang et al 2014 wu et al 2013a internal nutrient release occurs when sediments are re suspended into water column by currents or waves wave disturbance rapidly increases ss in the overlying water with om accounting for only 5 7 7 3 the stronger the wave disturbance is the larger the proportion of inorganic particles in the suspended matter which is consistent with field observations ding et al 2012 li et al 2017 om tn and tp have positive relationship with ss the increase in om tn and tp concentrations in the overlying water are caused by wave induced sediment suspension which is consistent with other field studies in shallow lakes søndergaard et al 2013 qin et al 2006 ding et al 2012 in large shallow lakes the release of dissolved nutrients from sediment commonly occurs in conjunction with molecular diffusion adsorption and desorption and is affected by changes in redox conditions and hydrodynamic disturbances therefore it is difficult to quantitatively identify the effects of hydrodynamic disturbance on the internal release of dissolved nutrients based on field observations qin et al 2004 sun et al 2006 during wh2 wave process dtn and dtp in the bottom layer slightly increases with sediment resuspension after 30 min wh10 wave disturbance dtn and dtp concentrations in the overlying water rapidly decreased fig 5 one possible explanation is that soluble substances are easily adsorbed by particulate matter and subaqueous sediments sun et al 2006 zhang et al 2013 under wave disturbance sediments resuspension can largely increase ss concentration in the overlying water and decrease of median ss particle size therefore sediment resuspension induced by wavedistinctly increase fine particles to further increases nutrients adsorption another possible explanation is that the dissolved oxygen do concentration increases in the overlying water and the interstitial water of sediments due to the disturbance of oxygen enrichment ding et al 2011 it forms a thin oxide layer is formed on the surface of the sediments to increase the adsorption capacity of the sediments for metal elements such as iron fe and manganese mn zhu et al 2005 the critical shear stress in lake taihu ranges 0 01 0 1 n m2 qin et al 2004 luo et al 2006 it s roughly equivalent to 5 6 m s wind speed in the field nutrients release induced by hydrodynamics happens every day in lake taihu qin et al 2006 resuspension flux of nutrients also is very high it is evaluated that the medium wave disturbance intensity 5 6 m s wind speed could induce 166 mg m2 d of tn and 49 mg m2 d of tp release luo et al 2006 although most of tn and tp are particle nitrogen and particle phosphorus which could not be directly taken in by algae and bacteria 58 of the total phosphorus could be hydrolyzed into inorganic phosphate in lake taihu gao et al 2006 therefore sediment resuspension could compensate for nutrients deficiency of algae growth ding et al 2016 2017 and also lead to the algae bloom paerl 2008 wu et al 2013a ding et al observe the effect of typhoon morakot on lake taihu and found that the biomass of algae rapidly increased after typhoon passed away 2012 lake taihu is affected by monsoon and is blowed from south and southeast wind satellite imaging also showed that algal blooms developed in most parts of the northwestern and central lake after both typhoon events zhu et al 2014 wherefore algae blooms easily happen in these regions due to their location which are at divergence area of wind and have the long wind length strong sediment resuspensions frequently happen to promot nutrients cycling sediment dredging to control eutrophication in lake taihu has been controversial experimental results reflect that bulk density and tss show a highly negative correlation p 0 01 which is consistent with the results of the indoor simulation test zhong et al 2009a b similarly after sediment is dredged twice the sediment bulk density increase and tn dtn tp and dtp significantly decrease by 24 3 33 6 p 0 01 therefore it indicates that nutrients in sediments after dredging have a lower release potential while it is hard to carry on sediment dredge in open water area it is better to carry on sediment dredge on shore zone especially on the wind divergence area it is suggested that sediment dredging may be a useful measure for controlling eutrophication of shore zone in lake taihu 5 conclusion sediment resuspension is the typical characteristic of shallow lakes wave flume experiments could help to understand the characteristics of sediment resuspension in lake taihu bottom wave shear stresses have more close relationship with wave height than wave period and had the positive correlation with wave height under the 10 cm wave height wave shear stresses reached up to 0 307 n m2 which made the tss concentration rapidly increase to 82 98 g m2 and resuspension flux of sediment reach up to 72 38 g m2 the calculation showed the sediment disturbance depth reached up to 0 161 mm then sediment resuspension induced sediment nutrients releasing into water column especially particle n and p therefore hydrodynamics accelerated the nutrients cycling furthermore the experiments under different sediment bulk densities found sediment dredging could effectively reduce ss concentration and nutrient levels our research results provides a theoretical basis of sediment dredging to control eutrophication of shore zone of lake taihu for lake management acknowledgements this work was funded through the national natural science foundation of china grant no 41501518 41621002 41230744 41471021 the natural science foundation of jiangsu province bk20140184 and it also was supported by the fundamental research funds for the central universities 2014qna86 state key laboratory of lake science and environment no 2016skl006 the national key r d program of china no 2017yfc04052 and key research program of frontier sciences chinese academy of sciences no qyzdjsswdqc008 it also was funded by priority academic program development of jiangsu higher education institutions papd 
7304,lake taihu is a typical shallow lake which frequently happens sediment resuspension induced by wind induced waves the experiments are carried on to simulate the wave disturbance processes in wave flume by setting a series of wave periods 1 2 s 1 5 s 1 8 s and wave heights 2 cm 10 cm it aims to analyze the characteristics of sediment resuspension and the mechanisms of nutrients release and to evaluate the effects of sediment dredging on sediment resuspension and nutrients release in lake taihu the results show that wave shear stress during 2 cm and 10 cm wave height processes ranges 0 018 0 023 n m2 and 0 221 0 307 n m2 respectively wave shear stress has no significant differences between wave periods wave height has much more effects on sediment resuspension wave height of 2 cm could induce total suspended solids tss reaching up to 5 21 g m2 and resuspension flux of sediment m up to 1 74 g m2 tss sharply increases to 30 33 52 41 g m2 and m reached up to 48 94 g m2 when wave height reaches to 10 cm the disturbance depth under different sediment bulk weights ranges from 0 089 to 0 161 mm variation of suspended solids in 3 layers 1 cm 5 cm 20 cm above sediment interface has no significant differences organic matter tn and tp have positive relationship with ss organic matter is only accounted for 5 7 7 3 of ss the experiments under different sediment bulk densities 1 34 g cm3 1 47 g cm3 and 1 59 g cm3 find that tss and m fall by 44 2 and 39 8 with sediment bulk density increasing respectively total tn dtn tp and dtp decrease by 24 3 33 6 it indicates that sediment dredging could effectively reduce ss concentration and nutrient levels in water column the researches provide a theoretical basis for sediment dredging to control the shore zone of lake taihu for lake management keywords wave flume shear stress sediment resuspension sediment dredge nitrogen phosphorus 1 introduction shallow lakes are characterized by episodic resuspension of sediments resuspension occurs when the bottom shear stress exceeds the critical shear stress which depends on water content and sediment grain size håkanson and jansson 1983 several hydrodynamic processes can trigger resuspension including wind induced waves hamilton and mitchell 1997 wu et al 2013a b current and turbulence fluctuations you et al 2007 luo et al 2006 although wind induced waves are commonly the primary driving force in shallow lakes håkanson 2005 sheng and lick 1979 found that wave generated shear stress contributed to more than 70 of sediment resuspension happened in shallow lakes frequent wind induced resuspension generally occurs over temporal scales from minutes to hours villard et al 1999 liu and huang 2009 and may affect the function of lacustrine ecosystems the dynamic processes at water sediment interface are critical for transporting solid particles and nutrients wang et al 2014 wu et al 2013b internal nutrient release occurs when sediments are re suspended by wind driven currents and waves acting on the water sediment interface excessive nutrients loading especially nitrogen n and phosphorus p has led to the appearance of eutrophication and massive harmful algal blooms smith 1983 abell et al 2010 xu et al 2010 zhang et al 2014 lake taihu is a typical shallow lake with a surface area of 2338 km2 and a mean depth of 1 9 m which commonly happens wind inducedsediment resuspension due to the lake s high dynamic ratio håkanson 1982 approximately 1100 km2 of lakebed are covered by sediment which accounts for about 47 5 of the lake area luo et al 2004 due to the southeast monsoon sediments are primarily deposited in the western and northern portions of lake taihu with thickness greater than 4 m luo et al 2004 lakebed sediments are commonly rich in n and p especially top surface sediments nitrogen content in the surface sediment is 0 05 0 2 while total phosphorus is 0 01 0 05 fan et al 2000 the hydrology and nutrient input result in a trophic gradient characterized by hypertrophic conditions in the northern part of lake taihu and mesotrophic conditions in the southeastern part of lake taihu chen et al 2003 satellite imaging shows that algal blooms are prominent in large parts of the northwestern and central lake after typhoon events zhu et al 2014 in 2007 lake taihu experienced a toxic algae bloom that made headlines around the world qin et al 2010 over the subsequent decade considerable attention has been paid to eutrophication control in lake taihu nutrient release induced by sediment suspension is an important nutrient source in shallow eutrophic lakes wu et al 2013a wang et al 2015 current field monitoring of wind induced sediment resuspension mainly focuses on water quality fan et al 2003 zhu et al 2007 wu et al 2009 it is difficult to understand the relationship between wave action and nutrients release due to complex hydrodynamics numerical models are applied to reproduce water currents and waves to study the effects of hydrodynamics on sediment resuspension and water quality hu et al 2006 numerous experimental methods have been applied to analyzing the hydrodynamic effects of waves on sediment suspension including y shape apparatus you et al 2007 annular current flume experiments widdows et al 2002 and wave flume experiments sun et al 2006 wave flume experiment is ideal for defining wave stress by regulating wave height and period in this study we design wave flume experiments to explore the characteristics of sediment resuspension and the mechanism of sediment nutrient release in lake taihu the aims of these serial experiments are to 1 analyze the hydrodynamic conditions under different wave periods and wave heights in the wave flume 2 analyze the variations of suspended solids ss and nutrients in different hydrodynamic conditions 3 establish the relationships among wave characteristics resuspension and nutrients release 4 evaluate the effect of sediment dredging from the perspective of sediment bulk density 2 materials and methods 2 1 study area lake taihu is the third largest freshwater lake in the lower yangtze river delta in china between 30 56 31 33 n and 119 53 120 36 e water temperature ranges from 4 8 to 29 2 c with an average annual water temperature of 17 3 c zhao et al 2011 wind speed varies between 0 and 10 m s with an average wind speed of 4 3 m s southeaster in spring and summer and 0 9 m s northwester in the autumn and winter qian 2012 water quality has seriously degraded over the last several decades due to severe eutrophication huang et al 2014 outbreaks of cyanobacteria blooms are commonly correlated with sustained winds which confirms that wind induced wave disturbance is an essential driving force for internal nutrient release induced by sediment resuspension in lake taihu kristensen et al 1992 ding et al 2012 algal blooms are most intense in zhushan and meiliang bays xie et al 2003 fig 1 meiliang bay is a semi enclosed bay in the northern part of lake taihu with an area of 130 km2 and a mean depth of 1 8 m fifty percent of the meiliang bay lakebed is covered by sediment rich in organic matter fan et al 2000 since 1998 meiliang bay has experienced severe algal blooms in summer and autumn chen et al 2003 guo 2007 the liangxi and zhiwu rivers bring municipal contaminants from wuxi city and changzhou city into meiliang bay which contributes to increasing eutrophication 2 2 experiment sediments collected from meiliang bay mainly consist of silty sand with an average particle size of 13 μm the top 15 cm sediments rich in organic matter were collected by the environmental cutter suction dredger and then transferred to the laboratory at the hydraulic academy in nanjing chinese ministry of water resources the top 15 cm sediments are the active sediment layer in taihu lake qin et al 2004 hu et al 2006 then sediments were stirred well and paved on the middle of wave flume with sediment length 7 m and thickness 7 10 cm fig 2 after sediment in the flume naturally settled for one month water was slowly poured into the wave flume until the water depth reached at 40 cm depth then the system was kept still for several days the wave flume length 25 m width 0 5 m height 0 8 m is equipped with wave maker at the inlet and wave alleviator at the opposite end wave height and wave period were collected by capacitive wave instrument mts usa which are installed at both ends of the sediment section the whole system is automatically controlled via a computer the wave intensity is controlled by setting six groups of hydrodynamic conditions for different wave periods 1 2 s 1 5 s and 1 8 s and wave heights 2cm 10 cm to simulate wave disturbances commonly occurring in lake taihu the bulk density of top 2 cm sediments in wave flume is measured before each group test 1 34 g cm3 1 47 g cm3 1 59 g cm3 respectively water samples are collected at 3 water column layers 1 cm 5 cm and 20 cm above the water sediment interface respectively fig 1 three parallel water samples are taken for each layer after background sampling the wave maker is switched on to produce regular sinusoidal waves 2 cm height wh2 for 60 min and then 10 cm height wh10 for 120 min water samples are collected at 30 and 60 min during the wh2 wave period and at 70 90 120 and150 min during the wh10 wave period 2 3 sample analysis water samples were divided into two subsamples with one set of subsamples immediately filtered for suspended solids and the other set analyzed for nutrients to determine ss the remaining particulate matter on the filters after 100 250 ml sampling water was dried at 105 c for 4 h until its weight kept constant organic matter om was measured using a 4 h ignition of the dried particulate matter on a membrane at 550 c the loss on ignition loi is the ratio of om to ss total nitrogen tn total phosphorus tp dissolved total phosphorus dtp and dissolved total nitrogen dtn were determined by spectrophotometry after digestion with alkaline potassium persulfate jin and tu 1990 the detection limit is 0 01 mg l for tn and dtn and 0 01 mg l for tp and dtp 2 4 data processing preliminary treatment of the experimental data was performed in matlab 7 0 one way analysis of variance anova was carried out using spss 19 0 software to test for significant differences between the various hydrodynamic conditions a significance level of p 0 05 is used for all tests 2 4 1 wave shear stress grant and madsen 1979 first proposed the basic theory of bottom shear stress at the water sediment interface in shallow lakes the shear stress caused by wave at the water sediment interface can be calculated by the following equation 1 τ w b 0 5 ρ f w u m 2 where τ w b is the wave shear stress n m 2 ρ is the water density kg m 3 f w is the wave friction coefficient u m is the maximum wave orbital velocity near the bed m s the f w is calculated as follows jiang et al 2000 2 f w exp 5 2 a δ k s 0 19 6 0 where a δ is the wave particle amplitude m which is determined by linear wave theory a δ h s 2 sinh 2 π h l s where h s is effective wave height m l s is wave length m k s is physical roughness of the lake bottom this study uses 0 2 mm as the lake bottom roughness based on previous studies hawley 2000 luo 2004 nielsen et al 2001 qin et al 2004 when a δ k δ 1 59 f w 0 3 u m is calculated as follows 3 u m π h s t s sinh 2 π h l s where t s is wave period s and l s is effective wave length which can be expressed as follows 4 l s gt s 2 tanh 2 π h l s 2 π since the total shear stress is primarily caused by wind induced waves in lake taihu li et al 2017 the shear stress is approximated as the wave shear stress 2 4 2 resuspension flux samples were collected at 1 5 and 20 cm above the water sediment interface so the corresponding layer thicknesses were 3 cm bottom layer 7 cm middle layer and 30 cm upper layer respectively total amount of suspended solids in the water column per unit area is the total of suspended solids tss g m2 which is calculated as follows 5 tss i 1 n ss i δ z i 100 where ssi is suspended solids concentration in the i th layer mg l δzi is the thickness of the i th water layer cm n is total vertical layers 6 m i tss i tss i 1 the resuspension flux mi is the resuspension flux at i th sampling time the sediment disturbance depth can be calculated using the following equation gouleau et al 2000 gao and jia 2004 7 h m γ wet 1 w where h is the mean disturbance depth mm m is the total amount of resuspended solids g m2 γwet is the wet bulk density of sediments g cm3 w is the moisture content of sediments in order to analyze the variations of nutrient content we introduce the total amount of water column material per unit area t g m2 the specific calculation method is similar to tss 3 results 3 1 hydrodynamic characteristics the dynamic process has wh2 wave process and wh10 wave process during wh2 wave processes the maximum wave orbital velocity near the lakebed ranges from 0 031 m s to 0 041 m s table 1 bottom wave shear stress ranges from 0 018 to 0 023 n m2 table 1 when wave height reaches up to 10 cm the maximum wave orbital velocity varies from 0 154 m s to 0 256 m s and wave shear stress is from 0 221 n m2 to 0 307 n m2 the wave shear stresses during wh2 wave processes are remarkably small the wave shear stress during wh10 wave processes is significantly larger than that under 2 cm wave height p 0 01 the relationship between the wave period and bottom shear stress shows the positive correlation p 0 01 which indicates that shear stress magnitude increases with wave period increasing while wave shear stresses have no significant difference between 1 2 s 1 5 s and 1 8 s 3 2 sediment resuspension and hydrodynamic conditions wave shear stress acting on the sediment water interface exceeds the critical stress to induce sediment resuspension and to increase tss concentration according to the hydraulic judging criteria the critical stress for sediment resuspension means that the surface particles began to gradually float upward during the wh2 wave processes sediment particulates just floats near the sediment water interface and only the smaller sediment particles are suspended into the water column tss varies from 1 32 to 5 21 g m2 and resuspension flux is 0 38 1 74 g m2 under different wave periods during the wh2 wave period table 2 tss and resuspension flux m under different wave periods have no significant increase based on the initial values wave shear stresses under 10 cm wave height are much larger than the critical stress during wh10 wave processes sediment particulates are largely suspended into water column tss increases sharply to 30 33 g m2 52 41 g m2 which is almost 50 times of the initial value resuspension flux reached up to 48 94 g m2 tss and resuspension flux m under different wave periods has no significant difference p 0 01 sediment disturbance depth depends on wave shear and sediment bulk weight during 1 5 s wave period processes the calculated disturbance depth under different bulk weigh sediments ranges from 0 089 mm to 0 161 mm table 3 the disturbance depth is proportional to tss in the water column p 0 01 and inversely proportional to the wet bulk density of the sediment p 0 01 in other words the disturbance depth increases with the suspended solids increasing and decreased with sediment bulk density 3 3 variations of ss and nutrients during suspension processes before beginning to make waves the background water samples in three layers above sediment 1 cm 5 cm 20 cm are collected from bottom to top the layers ss concentration in water column is 3 4 mg l 3 4 mg l and 2 0 mg l fig 3 respectively the initial tss is 0 94 g m2 fig 4 after 60 min of wh2 wave processes there is no significant increase of ss concentration in the overlying water the ss of the bottom and middle layers rises slightly to 7 8 mg l and 8 1 mg l respectively and the upper layer is unexpectedly reduced to 1 75 mg l at wh2 stage the bottom shear stress is only 0 022 n m2 which is less than the critical shear stress so sediment particles remain near to the water sediment interface and oscillate with the wave action subsequently as the wave height increases to 10 cm ss in each layer increases rapidly and transparency of the overlying water is reduced ss concentration at 1 cm 5 cm and 20 cm above the water sediment interface increases to 27 times 17 times and 12 times of the initial value respectively tss increases rapidly to 15 times of background levels fig 4 ss concentration gradually increases and reaches to the peak after 120 min by now ss concentration in the bottom middle and upper layers is 160 9 mg l 127 8 mg l and 116 9 mg l respectively tss also increases to 48 8 g m2 which is higher than 50 times of initial value ss concentration of the bottom layer was significantly higher than that of the upper p 0 01 and middle p 0 01 layers and variation trends of ss concentration in three layers keep consistent tss has the positive correlation with om tn and tp table 4 p 0 01 which further indicates that the variation trends of om tn and tp are consistent with ss in the overlying water throughout the trial after wh2 wave process the total amount of om has a slight increase while loi decreased from 62 to 38 fig 4 while the total amount of tn and tp experience a slight increase fig 5 after 60 min of wh10 wave process total amount of om increases from 0 58 g m2 to 3 46 g m2 and loi decreases from 38 to 8 which indicates that the majority of re suspended solids are inorganic substances the background values of tn and tp is 0 704 g m2 and 0 015 g m2 respectively wh2 wave process has little effect on tn and tp concentration in water column after wh10 wave process begin tn and tp rapidly increase in the initial 5 10 min tn and tp steadily increases 1 16 to 1 86 times in the later 120 min dtn and dtp r show no obvious variation throughout the experiment specifically dtn and dtp significantly decrease after 30 min of wh10 wave which may be due to the increase of particles concentration which could adsorb the soluble nutrients 3 4 sediment resuspension under different sediment bulk densities sediment dredging remove the surface high organic and floppy sediment and induces to increase the sediment bulk density before the wave processes the bulk densities of the top 2 cm sediments is measured 1 34 g cm3 1 47 g cm3 1 59 g cm3 respectively after wave processes tss and resuspension flux in water column under three sediment bulk densities is shown in table 5 results show that tss and resuspension flux of suspended solids m under three different wave periods are significant different p 0 01 tss and m increased with wave period increasing at the same wave period tss and m has an extremely significant decreasing with the sediment bulk density gradually increasing p 0 001 table 5 fig 6 a after twice sediment dredging tss and m in the overlying water have fallen by about 44 2 and 39 8 respectively similarly om p 0 01 fig 6b tn p 0 01 fig 6c dtn p 0 01 fig 6d tp p 0 05 fig 6e and dtp p 0 05 fig 6f are significantly reduced with increasing bulk density after dredging twice final values for total tn dtn tp and dtp decreased by 24 3 33 6 4 discussion 4 1 dynamic analysis and sediment resuspension in shallow lake ecosystem wind induced waves are the primary force driving sediment resuspension qin et al 2006 stone 2011 sediment resuspension is commonly estimated as a function of the shear stress on the lakebed above a critical value wang et al 2014 the critical shear stress inducing sediment suspension range from 0 01 to 0 1 n m2 fan et al 2004 james et al 1997 sheng and lick 1979 in the series of wave flume experiments the wave shear stress during the wh2 processes wave period is between 0 018 n m2 and 0 023 n m2 this value is equivalent with the critical value of 0 019 n m2 calculated by luo et al 2006 and 0 02 n m2 by li et al 2017 the wave shear stress during the wh2 processes is significantly smaller than the critical value 0 037 n m2 which derives from the medium size sediment with 0 017 mm grain size and 1 3 g cm3 sediment bulk density by qin et al 2004 according to researches on the suspended solids and wave shear stress wh2 and wh10 wave processes are equivalent to the field wind speed of 3 0 m s and 8 0 m s in lake taihu based on shear stress in the field qin et al 2004 luo et al 2006 sun et al 2006 respectively the critical wave stress inducing sediment resuspension depends on sediment size viscosity water content shape density and other chemical properties therefore it is complicated to quantitatively describe the relationship between the critical stress and sediment properties luo et al 2006 thus sediments properties vary widely due to significant temporal and spatial variation in lake taihu the critical stress will change according to these variations previous studies qiao et al 2011 li et al 2017 indicate there is a relationship between wave induced shear stress and ss according to the researches about the quantitative relationship between ss and wind speed v ss αvβ ss0 qiao et al 2011 we analyze the relationship between bottom shear stress and total ss om tn tp respectively fig 7 bottom shear stress adequately correlates with total ss r2 0 990 p 0 01 om r2 0 956 p 0 01 tn r2 0 967 p 0 05 and tp r2 0 986 p 0 05 the ss of each layer is different in various suspended conditions furthermore ss and bottom shear stress show similar correlations in 3 different water layers that shows that the best fit formula for ss and wave shear stress is ss αvβ ssc0 with r2 0 990 li et al 2017 divided the sediment suspension process into four stages using three shear stress thresholds during wh2 wave process 3 0 m s wind speed only a small amount solids were floated above the lake bed however a large number of sediments solids are rapidly suspended after wh10 wave action 8 0 m s wind speed which is consistent with sediment resuspension theory li et al 2017 vertically results show that after 10 min wh10 wave disturbance ss in the bottom middle and upper layers increased by 27 times 17 times and 12 times respectively however after 60 min wh2 wave disturbance ss in the bottom layer increases by only 4 4 mg l and ss in the upper layer dramatically decreases from 2 0 mg l to 1 75 mg l that means that the resuspension of sediments is accompanied by concurrent sedimentation of the suspended matter this phenomenon is particularly evident in the wh2 wave process following wh10 wave process ss in all layer increases significantly in the initial 30 min and then gradually stabilizes qin et al 2004 and ding et al 2011 found that mass exchange near the water sediment interface occurs primarily within the top 5 10 cm of sediment during this process ss in the bottom layer 1cm above the water sediment interface is significantly higher than that of the upper 20 cm above the water sediment interface and middle layer 5cm above the water sediment interface p 0 01 and the variation trend of ss in these three layers were consistent 4 2 effects ofsediment resuspension on nutrients release the dynamic processes of near bed sediments are critical for transporting solid particles and nutrients wang et al 2014 wu et al 2013a internal nutrient release occurs when sediments are re suspended into water column by currents or waves wave disturbance rapidly increases ss in the overlying water with om accounting for only 5 7 7 3 the stronger the wave disturbance is the larger the proportion of inorganic particles in the suspended matter which is consistent with field observations ding et al 2012 li et al 2017 om tn and tp have positive relationship with ss the increase in om tn and tp concentrations in the overlying water are caused by wave induced sediment suspension which is consistent with other field studies in shallow lakes søndergaard et al 2013 qin et al 2006 ding et al 2012 in large shallow lakes the release of dissolved nutrients from sediment commonly occurs in conjunction with molecular diffusion adsorption and desorption and is affected by changes in redox conditions and hydrodynamic disturbances therefore it is difficult to quantitatively identify the effects of hydrodynamic disturbance on the internal release of dissolved nutrients based on field observations qin et al 2004 sun et al 2006 during wh2 wave process dtn and dtp in the bottom layer slightly increases with sediment resuspension after 30 min wh10 wave disturbance dtn and dtp concentrations in the overlying water rapidly decreased fig 5 one possible explanation is that soluble substances are easily adsorbed by particulate matter and subaqueous sediments sun et al 2006 zhang et al 2013 under wave disturbance sediments resuspension can largely increase ss concentration in the overlying water and decrease of median ss particle size therefore sediment resuspension induced by wavedistinctly increase fine particles to further increases nutrients adsorption another possible explanation is that the dissolved oxygen do concentration increases in the overlying water and the interstitial water of sediments due to the disturbance of oxygen enrichment ding et al 2011 it forms a thin oxide layer is formed on the surface of the sediments to increase the adsorption capacity of the sediments for metal elements such as iron fe and manganese mn zhu et al 2005 the critical shear stress in lake taihu ranges 0 01 0 1 n m2 qin et al 2004 luo et al 2006 it s roughly equivalent to 5 6 m s wind speed in the field nutrients release induced by hydrodynamics happens every day in lake taihu qin et al 2006 resuspension flux of nutrients also is very high it is evaluated that the medium wave disturbance intensity 5 6 m s wind speed could induce 166 mg m2 d of tn and 49 mg m2 d of tp release luo et al 2006 although most of tn and tp are particle nitrogen and particle phosphorus which could not be directly taken in by algae and bacteria 58 of the total phosphorus could be hydrolyzed into inorganic phosphate in lake taihu gao et al 2006 therefore sediment resuspension could compensate for nutrients deficiency of algae growth ding et al 2016 2017 and also lead to the algae bloom paerl 2008 wu et al 2013a ding et al observe the effect of typhoon morakot on lake taihu and found that the biomass of algae rapidly increased after typhoon passed away 2012 lake taihu is affected by monsoon and is blowed from south and southeast wind satellite imaging also showed that algal blooms developed in most parts of the northwestern and central lake after both typhoon events zhu et al 2014 wherefore algae blooms easily happen in these regions due to their location which are at divergence area of wind and have the long wind length strong sediment resuspensions frequently happen to promot nutrients cycling sediment dredging to control eutrophication in lake taihu has been controversial experimental results reflect that bulk density and tss show a highly negative correlation p 0 01 which is consistent with the results of the indoor simulation test zhong et al 2009a b similarly after sediment is dredged twice the sediment bulk density increase and tn dtn tp and dtp significantly decrease by 24 3 33 6 p 0 01 therefore it indicates that nutrients in sediments after dredging have a lower release potential while it is hard to carry on sediment dredge in open water area it is better to carry on sediment dredge on shore zone especially on the wind divergence area it is suggested that sediment dredging may be a useful measure for controlling eutrophication of shore zone in lake taihu 5 conclusion sediment resuspension is the typical characteristic of shallow lakes wave flume experiments could help to understand the characteristics of sediment resuspension in lake taihu bottom wave shear stresses have more close relationship with wave height than wave period and had the positive correlation with wave height under the 10 cm wave height wave shear stresses reached up to 0 307 n m2 which made the tss concentration rapidly increase to 82 98 g m2 and resuspension flux of sediment reach up to 72 38 g m2 the calculation showed the sediment disturbance depth reached up to 0 161 mm then sediment resuspension induced sediment nutrients releasing into water column especially particle n and p therefore hydrodynamics accelerated the nutrients cycling furthermore the experiments under different sediment bulk densities found sediment dredging could effectively reduce ss concentration and nutrient levels our research results provides a theoretical basis of sediment dredging to control eutrophication of shore zone of lake taihu for lake management acknowledgements this work was funded through the national natural science foundation of china grant no 41501518 41621002 41230744 41471021 the natural science foundation of jiangsu province bk20140184 and it also was supported by the fundamental research funds for the central universities 2014qna86 state key laboratory of lake science and environment no 2016skl006 the national key r d program of china no 2017yfc04052 and key research program of frontier sciences chinese academy of sciences no qyzdjsswdqc008 it also was funded by priority academic program development of jiangsu higher education institutions papd 
