index,text
585,an improved information transfer mechanism is developed to couple fluid and granular modules for incompressible smoothed particle hydrodynamics modelling of sediment transport the interaction force pair necessitates effective projection of information among parallel continuum based modules the particle position in either module is susceptible to considerable variation due to their lagrangian nature an attempt has been made to minimize the effective thickness of the diffused interface between the fluid and granular particles the proposed model reduces artificial drag force resulting from diffused computation of effective porosity the revised framework outperforms in coarse resolution compared to its existing counterpart keywords isph sediment transport information coupling 1 introduction lagrangian particle methods are generally well equipped to capture the interaction between rapid flow and fluidized granular bed e g landslide generated waves sediment transport coupled flow models utilize various algorithms for interchanging information between sub modules i e fluid and granular media however sharp discontinuity is often flattened out by these interpolation algorithms this work aims to reduce the adverse impact of information loss by improving interpolation algorithms between different components of the coupled flow framework the continuum conceptualization of effective porosity does not remain valid beyond a certain minimal representative elemental volume rev the volume averaged flow equations lose their validity beyond this threshold length scale corresponding to minimum rev the distribution of effective porosity across interface should not depend on the resolution adopted for a particular simulation several researchers have formulated various elaborate algorithms related to computational fluid dynamics discrete element method cfd dem to achieve an actual distribution of effective porosity wu et al 2009 have developed an efficient analytical method based on particle volume shared by various unstructured grids for coupled discrete particle methods generally cfd dem directly calculates voidage function through the number of particles present in a particular fluid cell zhao and shan 2013 shan and zhao 2014 recent studies have suggested creating a time dependent mesh analytical solution boyce et al 2014 based on current co ordinates of granular particles at every time step to calculate the fraction of the volume associated with each fluid cell traditionally cfd dem assumes the full volume of the solid particle stays within the corresponding grid thus all solid particles present within a fluid cell have equal contribution through their representative volume irrespective of their positions peng et al 2016 have introduced a particle meshing method to take care of solid particle volume present within an eulerian grid in a lagrangian method like sph the fluid cell volume can be mapped to the idea of influence domain the contribution of particles has been generally taken through an uncorrected kernel value employed on them sun et al 2013 the contribution of the particle decreases monotonically from the concerned location leading to an efficient representation typically cfd dem is an ensemble of coupled eulerian cfd and discrete dem methods the effective porosity and other coupling parameters are directly or crudely interpolated at the center of the fluid cell sun and xiao 2016 zhao et al 2017 thus the resolution of fluid cells inevitably becomes coarser compared to the discrete granular module in coupled sph dem robinson et al 2014 markauskas et al 2017 tan and chen 2017 systems particles in both parallel modules are moving separately thus the information needs to be suitably interchanged between modules the drag force computation for any particle requires relative velocity at a common location which is computed generally through a shepard corrected kernel robinson et al 2014 the pressure gradient and viscous term acting on the solid particle can be evaluated directly for a coupled cfd dem method from the corresponding fluid cell coupled sph dem markauskas et al 2017 tan and chen 2017 generally avoids the pressure coupling by directly incorporating buoyancy force into account it also removes the possibility of enforcing the erroneous pressure gradient resulting from spurious numerical fluctuations in weakly compressible sph generally locally averaged navier stokes equation requires a comparatively coarser resolution compared to the granular module of fluid phase to obtain sufficiently smooth porosity field sun et al 2013 tan and chen 2017 there are also instances of coupled sph dem frameworks komoroczi et al 2013 ren et al 2014 canelas et al 2016 where dem resolution is coarser compared to sph however they are not suitable for simulation of sediment transport mechanism a hybrid continuum based isph isph framework has been developed in a previous study pahar and dhar 2017 for incompressible modelling of sediment transport the framework shares similarities with existing sph dem models though the granular media is modelled using an incompressible continuum conceptualization fig 1 through rheological approximation the modules interact with each other through an interaction force pair kafui et al 2002 the force pair consists of fluid pressure gradient the divergence of viscous stress tensor and drag force each of these components would require extensive information from the respective parallel module the fluid pressure is inherently smooth due to divergence free nature of truly incompressible sph lee et al 2008 thus fluid pressure gradient is incorporated into the interaction force pair without the additional assumption of hydrostatic pressure distribution buoyancy different shades of blue in the continuum based fluid phase in fig 1 represent flow through porous media lighter and outside it darker the interface between two phases should be clearly delineated to distinguish the effect of resistive force vector originating from solid skeletal structures the modules exist in parallel domains although their coordinate systems coexist interaction forces are calculated based on the projection of one module to another one prior work utilized a zeroth order smoothing algorithm to transmit information between modules though it ensures a smoothed distribution of different properties the rapid discontinuity is flattened out leading to loss of information for example drag force varies non linearly with corresponding effective porosity it vanishes for zones having unit effective porosity a large diffused interface between porous and non porous zone would exert an extra artificial drag force on the fluid module thus it may adversely affect the computation of erosive processes in the current study an improved information transmission mechanism between coupled modules has been proposed the projection of information becomes tricky for these separated yet coupled modules as both sets of particles are highly susceptible to irregular distribution gradient divergence s of properties are directly considered for projection in the higher order interpolation sph requires information regarding volume associated with the center particle to calculate the gradient divergence at the concerned location however these criteria are unlikely to satisfy as particles tend to travel separately in their respective domains unlike the existing sph dem formulations the present framework utilizes initial identical and coarser resolution for the granular module the improved technique utilizes a first order consistent method to take care of rapid yet effective interpolation of information between parallel modules effect of artificial drag forces resulting in the previous model has been thoroughly minimized by the proposed variant the modified framework is compared with existing experimental data along with numerical result simulated by the erstwhile formulation 2 information transfer between modules anderson and jackson 1967 had developed the averaged equations for fluid flow over deformable granular bed considering continuum approximation for the granular phase the governing equations of continuity and momentum for granular and fluid phases can be written as pahar and dhar 2018 1a u s 0 1b η u f 0 1c d u s d t 1 ϕ ρ s p s g 1 ϕ ρ s τ s f f s ϕ ρ s 1d d η u f d t η ρ f p f η g ν 2 η u f f d ρ f 1 ρ f τ where u s granular velocity u f fluid velocity ϕ solid fraction p s granular pressure g acceleration due to gravity ρ s granular density f f s interaction force pair per unit volume η effective porosity p f fluid pressure f d drag force per unit volume ν fluid viscosity τ ρ f sub particle scale tensor for modelling small scale flow information the interaction force can be represented as kafui et al 2002 2 f f s 1 η p f 1 η τ f d drag force f d is determined through the following expression following di felice 1994 3 f d 1 8 n p c d i ρ f π d 2 η 2 ξ u f u s u f u s where n p number of granular particles present in a unit volume ξ corrective porosity function the drag force component is a non linear function of effective porosity in a continuum approach np can be calculated as the volume of solid 1 η divided by the volume of a single particle π 6 d 3 generally bed roughness is treated through drag based formulation kazemi et al 2017 near the bed zone the bed is considered as a rigid boundary for these models the proposed model does not consider granular media as a solid boundary rather fluid outside and inside porous media is solved through a volume averaged filtered navier stokes equation the momentum equation contains drag force which incorporates the information regarding particle diameter and relative velocity between fluid and granular particles a diffused interface will ensure a smudged distribution of np which in turn introduces artificial drag force even when the granular particles are not immediate neighbours of fluid particles the information transfer mechanism between modules is described in detail in the following sections 2 1 information transfer granular to fluid module the terms containing fluid pressure gradient and divergence of viscous stress are implicitly embedded into the fluid module thus the informations required from the granular module for calculating drag force are the effective porosity and granular velocity at each fluid particle according to taylor series any quantity ψ can be written as 4 ψ b ψ a r a b ψ a where r a b relative position vector between particles a and b r a position vector of particle a in discrete form the aforementioned equation can be written as 5 ψ b ψ a r a b c 1 n t ψ c ψ a v c w a c v c volume associated with particle c present in the support domain of particle a w a c kernel value at any particle c with respect to center particle a the total number of particles present in a compact support domain of a single module is denoted by nt the center particle position is represented by a however the information regarding center particle position and volume associated with it remains unknown all the particles present in the support domain except the concerned center particle remain in an identical module the kernel gradient possesses zero value at the center location thus eq 5 can be written as 6 ψ a 1 r a b c 1 n t v c w a c ψ b r a b c 1 n t ψ c v c w a c eq 6 is valid for only a pair of particles inside the influence region similar sets of equations can be produced by sweeping over all the particles present inside support domain applying a kernel throughout the particles inside the support domain the equation can be modified as 7 ψ a b 1 n t 1 r a b c 1 n t v c w a c v b w a b b 1 n t ψ b r a b c 1 n t ψ c v c w a c v b w a b 2 1 1 effective porosity calculation in cfd dem effective porosity of a fluid cell is generally determined based on the total volume of solid particles present in the respective cell to obtain the same in the coupled sph sph model a concept of ghost particles is introduced fig 2 fig 2a depicts the representative particle positions for both fluid blue and granular yellow modules the presence of granular particles in the support domain has been shown in fig 2b a discontinuity of solid fraction exists exactly at the interface the completeness of the corrupted support domain is maintained using a few virtual grey particles the value of solid fraction associated with these particles is zero the gradient and higher order derivatives of solid fraction is also considered to be zero the location and associated volume of these ghost particles remain unknown the representative volume associated with virtual granular particles need not be identical to that of actual granular particle outside the porous media for the real particles present inside the domain the solid fraction at central location can be written as 8 ϕ b ϕ a r a b ϕ a ϕ a r a b c 1 n t ϕ c ϕ a v c w a c ϕ a r a b c 1 n t ϕ c v c w a c nt represents the total number of particles present in the compact support domain thus nt becomes equal to n a c t n v i r where nact and nvir are respective numbers of actual and virtual particles inside the support domain ϕ a can be discarded as the influence domain has compact support zeroth order consistency for the virtual particles assumed inside the domain the solid fraction at central location can be written as 9 ϕ a ϕ b r a b ϕ b eq 9 utilizes the information regarding the solid fraction and gradient of solid fraction which are zero as discontinuity prevails outside granular domain at the virtual particles accordingly these particles have zero contribution at central location the ghost particles also ensures b n t w a b v b to become unity for compact support domain thus solid fraction at any location can be written as 10 ϕ a b 1 n a c t ϕ b r a b c 1 n t ϕ c v c w a c v b w a b b 1 n v i r ϕ b r a b ϕ b v b w a b b 1 n a c t ϕ b r a b c 1 n a c t ϕ c v c w a c v b w a b for the virtual granular particles solid fraction and gradient of solid fraction are zero it may appear that zero solid fraction and its gradient for virtual granular particles is rather conflicting however the calculation of gradient depends upon choice of smoothing length and representative volume associated with virtual particles considering very small smoothing length along with numerous virtual granular particles represent physically consistent result their number location and associated volumes do not feature in the equation thus effective porosity at any point can be written as 11 η a 1 ϕ a 2 1 2 velocity interpolation granular velocity also needs to be interpolated on the fluid particle position for relative velocity calculation including the virtual particles in the simulation would require knowledge of their respective velocity which is likely to be non zero unlike the previous case thus the virtual particles cannot be directly included the kernel gradient has to be corrected for corrupted support domain the interpolated property ψ can be written as eq 7 12 ψ a b 1 n a c t ψ b r a b c 1 n a c t ψ c v c w a c v b w a b b 1 n a c t 1 r a b c 1 n a c t v c w a c v b w a b w and w represent shepard corrected kernel and corrected not shepard corrected kernel gradient bonet and lok 1999 the kernel gradients do not need to be zeroth order consistent as the difference ψ c ψ a is incorporated for computation of gradients zeroth and first order consistent kernel gradients will contain volume and location of the unknown central particle utilizing first order consistent kernels will implicitly interpolate the information of central particle representative distributions of effective porosity for different methods have been shown in fig 3 fig 3a depicts actual distribution of solid particles and corresponding effective porosity associated with fluid particles the porosity can be calculated directly by averaging the void space in a unit volume fig 3b however it leads to a diffused interface having width ς where ς denotes the diameter of the circular support domain the partition of volume associated with fluid particles inside the support domain affects the calculation of effective porosity the inverse distance weighting property of the kernel reduces the diffused nature of the interface zeroth order including the kernel gradients into the eq 7 would lead to a more rapid transition of effective porosity ideally an abrupt change of effective porosity should exist between these zones however in a continuum approach it is difficult to calculate the actual distribution of discontinuous effective porosity effective porosity cannot be calculated by utilizing the nearest neighbour approach with minimal average interparticle distance due to its inherent continuity and differentiability constraints to validate the proposed algorithm a representative coupled model has been shown distributions of fluid and granular particles have been depicted in fig 4 a regular particle distribution has been adopted for both modules with identical inter particle distance the topmost granular particle layer superimposes with the fluid particle layer the distributions of effective porosity obtained from direct averaging with and without volume trimming kernel and kernel gradients have been shown in fig 5 a d the exact distribution of effective porosity for the fluid particles has been depicted in fig 5e the effective porosity of fluid particle layer coinciding with topmost granular layer is considered to be 1 ϕ 2 direct averaging considers identical contributions of particles present in a support domain each particle is considered to be associated with an identical circular volume fig 5a the effective porosity is calculated from the ratio of the total volume represented by the solid particles and the volume of the support domain following traditional cfd dem methods it underestimates the effective porosity for the interior portion of the granular domain total volume carried by every particle is included irrespective of its position total volume associated with the particles present in the vicinity of the boundary of the support domain may not remain within the support domain edges are trimmed for the second case fig 5b however the width of the interface remains just as large as the previous case the circular volume representation becomes essentially invalid for the current particle distribution each particle represents a square volume with the particle placed at the center of the square a voronoi diagram should be drawn at each time instance around the particles to get accurate volume representation as particle position is susceptible to large temporal variation a straightforward kernel approximation is adopted in the third case fig 5c the corruption in the support domain is calculated through completeness of kernel however particle layers present above granular profile get heavily influenced the current method provides suitable distribution of effective porosity leading to sharp interface fig 5d the error δη in effective porosity calculation for each of these methods have been shown in fig 6 where δ η η exact η calculated the embedded kernel gradient influences the effective porosity through corresponding particle distance concerning the center particle the kernel gradient peaks in the vicinity of the center particle for both compact corrupted support domain in sph a continuous kernel is transformed into discrete quantities for weight calculation thus a larger support domain would be a better choice regarding unity representation however the width of the diffused interface also increases with larger support domain in the current study 1 60δx has been taken as smoothing length for a c2 kernel wendland 1995 maximum 6 layers of particles will be influenced by the proposed method for identical resolution of either module the effect of kernel gradient vanishes for a full influence domain the interior portion of granular media the error in the calculation of effective porosity for a compact support domain is in order of 0 47 adopting higher order kernels c4 c6 may further decrease the error in computation however being too concentrated around the central particle they would require a larger support domain effectively increasing the width of the diffused interface it should be also noted that the mechanism for relative velocity calculation is developed on the basis of zeroth order inconsistency c 1 n w a c x 0 both shepard corrected kernel and kernel gradient must include the center point and volume associated with it in the current coupled modular framework fluid and granular particles are present in the parallel domain while projecting information to another counterpart thus the volume of the projected center particle remains unknown the volume of the center particle does not feature in the calculation of the proposed method 2 2 fluid to granular module unlike the last case the full interaction pair needs to be incorporated into the granular module thus the pressure gradient and divergence of stress have to be accurately calculated at every granular particle position the procedure has been shown in the fig 7 the transfer process has been highlighted for a few isolated granular particles present inside the high velocity fluid regime generally the granular domain remains fully submerged leading to a compact support domain in parallel modules for erosion resulting from the high velocity fluid stream granular particles may become separated from the bed the fluid property is calculated on the granular particles with zeroth order averaging through shepard corrected kernel in the previous framework ultimately the gradient divergence of any property can be computed based on the support domain of the granular media in the final step this procedure will be able to produce proper result inside the porous media i e where each granular particle has a sufficient number of granular particles present in its support domain however for isolated granular particles this may result in erroneous interpolation of information the case shown in fig 7 contains 3 4 particles present in the influence zone each granular particle sparse irregular distribution of these particles around the edge of the support domain will result in a badly scaled corrective matrix for kernel gradient modification bonet and lok 1999 thus the kernel gradient may have to be taken as zero for these secluded particles however inside a high velocity fluid stream the pressure gradient is bound to be sufficiently high to affect the particle movement thus a different procedure has been proposed in the current study in the present work the pressure is not directly interpolated into the granular particles instead the fluid pressure gradient is calculated on each fluid particle the pressure gradient is directly interpolated on the granular particles the procedure requires additional computational overhead for searching and calculating pressure gradient stress divergence on each fluid particles however the procedure is completed within the first two steps of the aforementioned mechanism 3 results and discussions the proposed mechanism is applied to three cases of dam break induced erosion enclosed in a box the initial condition is depicted in the fig 8 all three cases have sand as the bed material spinewine 2005 the lock gate is removed swiftly to study the temporal variation of bed these cases are also simulated using the previous method and compared with the proposed variant the resolution is deliberately kept coarse initial interparticle spacing l 0 0 0125 m to assess the effectiveness of the model all the simulation parameters e g support domain radius free surface detection criteria etc have been kept identical for both variants the third case is also replicated with separate resolutions for different modules l 0 s a n d 0 02 m l 0 w a t e r 0 0125 m to study the effect of the developed algorithm the smoothing length is taken as 1 60l 0 calculation of effective porosity utilizes the smoothing length of the granular domain whereas the transfer of fluid information to granular module considers smoothing length of the fluid module in case of identical resolutions the smoothing length for either of the cases remains unchanged simulated fluid profiles with the proposed framework along with experimental data have been shown in fig 9 the effective porosity serves as an indicator of the granular profile at different time instances the experimental data of spinewine 2005 has been shown by using circles for water and triangles for granular profile the progression of fluid fronts for the third scenario along with identical coarse resolutions for granular media has been depicted in fig 10 the coupled isph framework with the proposed mechanism can reproduce accurate fluid and granular profiles there are slight discrepancies in initial time instances which may be attributed to instantaneous gate movement however upward downward gate removal fu and jin 2016 produces different results a comprehensive evaluation of these issues is beyond the scope of the current study it should also be noted that effective porosity is not applicable for numerical models having lengthscales smaller than the proper rev in the present study the minimum average interparticle spacing is assumed to be approximately 6d d being the granular particle diameter finer resolutions for continuum models may produce lesser numerical error though they may not necessarily provide a physically accurate result compared to their coarser counterparts the initial effective porosity distribution for the existing proposed numerical model with coarse identical resolutions is shown in fig 11 zero on the z axis denotes the position of the granular media the diffused effect is prominently visible for the coarse resolution for the erstwhile model the effect of granular media quickly disappears outside the porous domain for either of the resolutions the overestimation of effective porosity for fluid particles inside the granular media is reduced substantially for the proposed variant for a superimposed fluid particle layer at zeroth level effective porosity does not vary considerably however zero distance is quite unlikely to happen as both modules are susceptible to rapid movement according to their corresponding governing equations the distribution in both present previous model is heavily dependent on smoothing length and kernel choice the larger smoothing length for coarse particle distribution produces a diffused porosity distribution a different approximation of kernel gradient e g derivative approximation through mps may help for such cases however comparing various approximations of kernel gradients is beyond the scope of the current study the drag force distribution at 0 25 s has been shown in fig 12 for different information mechanisms fluid velocity decreases suddenly at the granular surface for the current improved framework unlike the previous case thus exposed granular particles are more prone to dislodgement due to the imposition of concentrated drag force instead of diffused distribution the drag force attains its maximum value just outside the granular domain for the erstwhile framework this defect is amplified by the combined errors in the calculation of effective porosity and particle number np calculation of relative velocity also influences the spread of drag forces far outside the granular media current modification produces a comparatively concentrated distribution of effective porosity and relative velocity thus the drag force reduces rapidly outside the granular domain the peak value of drag force just coincides with the granular interface the proposed model is essentially applicable for bed load transport some particles may get dislodged and move higher into fluid stream with large velocity however they do not undergo phase change or get diffused into fluid particles the temporal variation of bed level is shown in fig 13 the bed level variation for third scenario are compared for existing and improved models with both identical and coarse resolutions the patterns over 2 s have been depicted in fig 14 even though the overall pattern for either of the cases remains similar a few particles can be seen moving separately in the improved framework it can be attributed to sharp velocity distribution of the fluid nearby granular interface for the zeroth order consistent cases the granular slope becomes stable within a short period for scenario i ii for the third scenario similar trend can be observed for the improved previous models having both identical different resolutions it may be noted that initially 5 horizontal layers of particle signify the granular domain for scenario iiib fig 14 highlights the capability of the proposed model across different resolutions the dislodgement of particles at the downstream of the gate can be related to dune migration the proposed model is more suitable for such scenarios whereas the previous model forms an apparent protective overlay of artificial drag force just above the granular interface figs 9 and 10 show that the current variant can reproduce fluid and bed profiles satisfactorily the current mechanism may slightly increase the computational time as an extra step involving gradient divergence calculation has to be incorporated thus the corrective matrices need to be inverted at every location however searching of particles is required irrespective of the mechanisms overall the proposed modification consumes 4 of its predecessor 4 conclusion the current study demonstrates a significant improvement over existing coupled lagrangian models sph dem sph sph regarding applicability in lower identical resolutions inclusion of gradients reduces the diffused distribution of effective porosity the information transfer mechanism has been tested for three cases of rapid sediment transport induced by instantaneous dam break flows the smeared effect of resistive forces arising out of granular media has been reduced substantially consequently granular particles tend to travel longer distance while interacting with the rapidly moving fluid front effective porosity is calculated efficiently for the loosely disintegrated fluid granular interface the effect of artificial drag force has been severely curtailed though it cannot be eliminated completely 
585,an improved information transfer mechanism is developed to couple fluid and granular modules for incompressible smoothed particle hydrodynamics modelling of sediment transport the interaction force pair necessitates effective projection of information among parallel continuum based modules the particle position in either module is susceptible to considerable variation due to their lagrangian nature an attempt has been made to minimize the effective thickness of the diffused interface between the fluid and granular particles the proposed model reduces artificial drag force resulting from diffused computation of effective porosity the revised framework outperforms in coarse resolution compared to its existing counterpart keywords isph sediment transport information coupling 1 introduction lagrangian particle methods are generally well equipped to capture the interaction between rapid flow and fluidized granular bed e g landslide generated waves sediment transport coupled flow models utilize various algorithms for interchanging information between sub modules i e fluid and granular media however sharp discontinuity is often flattened out by these interpolation algorithms this work aims to reduce the adverse impact of information loss by improving interpolation algorithms between different components of the coupled flow framework the continuum conceptualization of effective porosity does not remain valid beyond a certain minimal representative elemental volume rev the volume averaged flow equations lose their validity beyond this threshold length scale corresponding to minimum rev the distribution of effective porosity across interface should not depend on the resolution adopted for a particular simulation several researchers have formulated various elaborate algorithms related to computational fluid dynamics discrete element method cfd dem to achieve an actual distribution of effective porosity wu et al 2009 have developed an efficient analytical method based on particle volume shared by various unstructured grids for coupled discrete particle methods generally cfd dem directly calculates voidage function through the number of particles present in a particular fluid cell zhao and shan 2013 shan and zhao 2014 recent studies have suggested creating a time dependent mesh analytical solution boyce et al 2014 based on current co ordinates of granular particles at every time step to calculate the fraction of the volume associated with each fluid cell traditionally cfd dem assumes the full volume of the solid particle stays within the corresponding grid thus all solid particles present within a fluid cell have equal contribution through their representative volume irrespective of their positions peng et al 2016 have introduced a particle meshing method to take care of solid particle volume present within an eulerian grid in a lagrangian method like sph the fluid cell volume can be mapped to the idea of influence domain the contribution of particles has been generally taken through an uncorrected kernel value employed on them sun et al 2013 the contribution of the particle decreases monotonically from the concerned location leading to an efficient representation typically cfd dem is an ensemble of coupled eulerian cfd and discrete dem methods the effective porosity and other coupling parameters are directly or crudely interpolated at the center of the fluid cell sun and xiao 2016 zhao et al 2017 thus the resolution of fluid cells inevitably becomes coarser compared to the discrete granular module in coupled sph dem robinson et al 2014 markauskas et al 2017 tan and chen 2017 systems particles in both parallel modules are moving separately thus the information needs to be suitably interchanged between modules the drag force computation for any particle requires relative velocity at a common location which is computed generally through a shepard corrected kernel robinson et al 2014 the pressure gradient and viscous term acting on the solid particle can be evaluated directly for a coupled cfd dem method from the corresponding fluid cell coupled sph dem markauskas et al 2017 tan and chen 2017 generally avoids the pressure coupling by directly incorporating buoyancy force into account it also removes the possibility of enforcing the erroneous pressure gradient resulting from spurious numerical fluctuations in weakly compressible sph generally locally averaged navier stokes equation requires a comparatively coarser resolution compared to the granular module of fluid phase to obtain sufficiently smooth porosity field sun et al 2013 tan and chen 2017 there are also instances of coupled sph dem frameworks komoroczi et al 2013 ren et al 2014 canelas et al 2016 where dem resolution is coarser compared to sph however they are not suitable for simulation of sediment transport mechanism a hybrid continuum based isph isph framework has been developed in a previous study pahar and dhar 2017 for incompressible modelling of sediment transport the framework shares similarities with existing sph dem models though the granular media is modelled using an incompressible continuum conceptualization fig 1 through rheological approximation the modules interact with each other through an interaction force pair kafui et al 2002 the force pair consists of fluid pressure gradient the divergence of viscous stress tensor and drag force each of these components would require extensive information from the respective parallel module the fluid pressure is inherently smooth due to divergence free nature of truly incompressible sph lee et al 2008 thus fluid pressure gradient is incorporated into the interaction force pair without the additional assumption of hydrostatic pressure distribution buoyancy different shades of blue in the continuum based fluid phase in fig 1 represent flow through porous media lighter and outside it darker the interface between two phases should be clearly delineated to distinguish the effect of resistive force vector originating from solid skeletal structures the modules exist in parallel domains although their coordinate systems coexist interaction forces are calculated based on the projection of one module to another one prior work utilized a zeroth order smoothing algorithm to transmit information between modules though it ensures a smoothed distribution of different properties the rapid discontinuity is flattened out leading to loss of information for example drag force varies non linearly with corresponding effective porosity it vanishes for zones having unit effective porosity a large diffused interface between porous and non porous zone would exert an extra artificial drag force on the fluid module thus it may adversely affect the computation of erosive processes in the current study an improved information transmission mechanism between coupled modules has been proposed the projection of information becomes tricky for these separated yet coupled modules as both sets of particles are highly susceptible to irregular distribution gradient divergence s of properties are directly considered for projection in the higher order interpolation sph requires information regarding volume associated with the center particle to calculate the gradient divergence at the concerned location however these criteria are unlikely to satisfy as particles tend to travel separately in their respective domains unlike the existing sph dem formulations the present framework utilizes initial identical and coarser resolution for the granular module the improved technique utilizes a first order consistent method to take care of rapid yet effective interpolation of information between parallel modules effect of artificial drag forces resulting in the previous model has been thoroughly minimized by the proposed variant the modified framework is compared with existing experimental data along with numerical result simulated by the erstwhile formulation 2 information transfer between modules anderson and jackson 1967 had developed the averaged equations for fluid flow over deformable granular bed considering continuum approximation for the granular phase the governing equations of continuity and momentum for granular and fluid phases can be written as pahar and dhar 2018 1a u s 0 1b η u f 0 1c d u s d t 1 ϕ ρ s p s g 1 ϕ ρ s τ s f f s ϕ ρ s 1d d η u f d t η ρ f p f η g ν 2 η u f f d ρ f 1 ρ f τ where u s granular velocity u f fluid velocity ϕ solid fraction p s granular pressure g acceleration due to gravity ρ s granular density f f s interaction force pair per unit volume η effective porosity p f fluid pressure f d drag force per unit volume ν fluid viscosity τ ρ f sub particle scale tensor for modelling small scale flow information the interaction force can be represented as kafui et al 2002 2 f f s 1 η p f 1 η τ f d drag force f d is determined through the following expression following di felice 1994 3 f d 1 8 n p c d i ρ f π d 2 η 2 ξ u f u s u f u s where n p number of granular particles present in a unit volume ξ corrective porosity function the drag force component is a non linear function of effective porosity in a continuum approach np can be calculated as the volume of solid 1 η divided by the volume of a single particle π 6 d 3 generally bed roughness is treated through drag based formulation kazemi et al 2017 near the bed zone the bed is considered as a rigid boundary for these models the proposed model does not consider granular media as a solid boundary rather fluid outside and inside porous media is solved through a volume averaged filtered navier stokes equation the momentum equation contains drag force which incorporates the information regarding particle diameter and relative velocity between fluid and granular particles a diffused interface will ensure a smudged distribution of np which in turn introduces artificial drag force even when the granular particles are not immediate neighbours of fluid particles the information transfer mechanism between modules is described in detail in the following sections 2 1 information transfer granular to fluid module the terms containing fluid pressure gradient and divergence of viscous stress are implicitly embedded into the fluid module thus the informations required from the granular module for calculating drag force are the effective porosity and granular velocity at each fluid particle according to taylor series any quantity ψ can be written as 4 ψ b ψ a r a b ψ a where r a b relative position vector between particles a and b r a position vector of particle a in discrete form the aforementioned equation can be written as 5 ψ b ψ a r a b c 1 n t ψ c ψ a v c w a c v c volume associated with particle c present in the support domain of particle a w a c kernel value at any particle c with respect to center particle a the total number of particles present in a compact support domain of a single module is denoted by nt the center particle position is represented by a however the information regarding center particle position and volume associated with it remains unknown all the particles present in the support domain except the concerned center particle remain in an identical module the kernel gradient possesses zero value at the center location thus eq 5 can be written as 6 ψ a 1 r a b c 1 n t v c w a c ψ b r a b c 1 n t ψ c v c w a c eq 6 is valid for only a pair of particles inside the influence region similar sets of equations can be produced by sweeping over all the particles present inside support domain applying a kernel throughout the particles inside the support domain the equation can be modified as 7 ψ a b 1 n t 1 r a b c 1 n t v c w a c v b w a b b 1 n t ψ b r a b c 1 n t ψ c v c w a c v b w a b 2 1 1 effective porosity calculation in cfd dem effective porosity of a fluid cell is generally determined based on the total volume of solid particles present in the respective cell to obtain the same in the coupled sph sph model a concept of ghost particles is introduced fig 2 fig 2a depicts the representative particle positions for both fluid blue and granular yellow modules the presence of granular particles in the support domain has been shown in fig 2b a discontinuity of solid fraction exists exactly at the interface the completeness of the corrupted support domain is maintained using a few virtual grey particles the value of solid fraction associated with these particles is zero the gradient and higher order derivatives of solid fraction is also considered to be zero the location and associated volume of these ghost particles remain unknown the representative volume associated with virtual granular particles need not be identical to that of actual granular particle outside the porous media for the real particles present inside the domain the solid fraction at central location can be written as 8 ϕ b ϕ a r a b ϕ a ϕ a r a b c 1 n t ϕ c ϕ a v c w a c ϕ a r a b c 1 n t ϕ c v c w a c nt represents the total number of particles present in the compact support domain thus nt becomes equal to n a c t n v i r where nact and nvir are respective numbers of actual and virtual particles inside the support domain ϕ a can be discarded as the influence domain has compact support zeroth order consistency for the virtual particles assumed inside the domain the solid fraction at central location can be written as 9 ϕ a ϕ b r a b ϕ b eq 9 utilizes the information regarding the solid fraction and gradient of solid fraction which are zero as discontinuity prevails outside granular domain at the virtual particles accordingly these particles have zero contribution at central location the ghost particles also ensures b n t w a b v b to become unity for compact support domain thus solid fraction at any location can be written as 10 ϕ a b 1 n a c t ϕ b r a b c 1 n t ϕ c v c w a c v b w a b b 1 n v i r ϕ b r a b ϕ b v b w a b b 1 n a c t ϕ b r a b c 1 n a c t ϕ c v c w a c v b w a b for the virtual granular particles solid fraction and gradient of solid fraction are zero it may appear that zero solid fraction and its gradient for virtual granular particles is rather conflicting however the calculation of gradient depends upon choice of smoothing length and representative volume associated with virtual particles considering very small smoothing length along with numerous virtual granular particles represent physically consistent result their number location and associated volumes do not feature in the equation thus effective porosity at any point can be written as 11 η a 1 ϕ a 2 1 2 velocity interpolation granular velocity also needs to be interpolated on the fluid particle position for relative velocity calculation including the virtual particles in the simulation would require knowledge of their respective velocity which is likely to be non zero unlike the previous case thus the virtual particles cannot be directly included the kernel gradient has to be corrected for corrupted support domain the interpolated property ψ can be written as eq 7 12 ψ a b 1 n a c t ψ b r a b c 1 n a c t ψ c v c w a c v b w a b b 1 n a c t 1 r a b c 1 n a c t v c w a c v b w a b w and w represent shepard corrected kernel and corrected not shepard corrected kernel gradient bonet and lok 1999 the kernel gradients do not need to be zeroth order consistent as the difference ψ c ψ a is incorporated for computation of gradients zeroth and first order consistent kernel gradients will contain volume and location of the unknown central particle utilizing first order consistent kernels will implicitly interpolate the information of central particle representative distributions of effective porosity for different methods have been shown in fig 3 fig 3a depicts actual distribution of solid particles and corresponding effective porosity associated with fluid particles the porosity can be calculated directly by averaging the void space in a unit volume fig 3b however it leads to a diffused interface having width ς where ς denotes the diameter of the circular support domain the partition of volume associated with fluid particles inside the support domain affects the calculation of effective porosity the inverse distance weighting property of the kernel reduces the diffused nature of the interface zeroth order including the kernel gradients into the eq 7 would lead to a more rapid transition of effective porosity ideally an abrupt change of effective porosity should exist between these zones however in a continuum approach it is difficult to calculate the actual distribution of discontinuous effective porosity effective porosity cannot be calculated by utilizing the nearest neighbour approach with minimal average interparticle distance due to its inherent continuity and differentiability constraints to validate the proposed algorithm a representative coupled model has been shown distributions of fluid and granular particles have been depicted in fig 4 a regular particle distribution has been adopted for both modules with identical inter particle distance the topmost granular particle layer superimposes with the fluid particle layer the distributions of effective porosity obtained from direct averaging with and without volume trimming kernel and kernel gradients have been shown in fig 5 a d the exact distribution of effective porosity for the fluid particles has been depicted in fig 5e the effective porosity of fluid particle layer coinciding with topmost granular layer is considered to be 1 ϕ 2 direct averaging considers identical contributions of particles present in a support domain each particle is considered to be associated with an identical circular volume fig 5a the effective porosity is calculated from the ratio of the total volume represented by the solid particles and the volume of the support domain following traditional cfd dem methods it underestimates the effective porosity for the interior portion of the granular domain total volume carried by every particle is included irrespective of its position total volume associated with the particles present in the vicinity of the boundary of the support domain may not remain within the support domain edges are trimmed for the second case fig 5b however the width of the interface remains just as large as the previous case the circular volume representation becomes essentially invalid for the current particle distribution each particle represents a square volume with the particle placed at the center of the square a voronoi diagram should be drawn at each time instance around the particles to get accurate volume representation as particle position is susceptible to large temporal variation a straightforward kernel approximation is adopted in the third case fig 5c the corruption in the support domain is calculated through completeness of kernel however particle layers present above granular profile get heavily influenced the current method provides suitable distribution of effective porosity leading to sharp interface fig 5d the error δη in effective porosity calculation for each of these methods have been shown in fig 6 where δ η η exact η calculated the embedded kernel gradient influences the effective porosity through corresponding particle distance concerning the center particle the kernel gradient peaks in the vicinity of the center particle for both compact corrupted support domain in sph a continuous kernel is transformed into discrete quantities for weight calculation thus a larger support domain would be a better choice regarding unity representation however the width of the diffused interface also increases with larger support domain in the current study 1 60δx has been taken as smoothing length for a c2 kernel wendland 1995 maximum 6 layers of particles will be influenced by the proposed method for identical resolution of either module the effect of kernel gradient vanishes for a full influence domain the interior portion of granular media the error in the calculation of effective porosity for a compact support domain is in order of 0 47 adopting higher order kernels c4 c6 may further decrease the error in computation however being too concentrated around the central particle they would require a larger support domain effectively increasing the width of the diffused interface it should be also noted that the mechanism for relative velocity calculation is developed on the basis of zeroth order inconsistency c 1 n w a c x 0 both shepard corrected kernel and kernel gradient must include the center point and volume associated with it in the current coupled modular framework fluid and granular particles are present in the parallel domain while projecting information to another counterpart thus the volume of the projected center particle remains unknown the volume of the center particle does not feature in the calculation of the proposed method 2 2 fluid to granular module unlike the last case the full interaction pair needs to be incorporated into the granular module thus the pressure gradient and divergence of stress have to be accurately calculated at every granular particle position the procedure has been shown in the fig 7 the transfer process has been highlighted for a few isolated granular particles present inside the high velocity fluid regime generally the granular domain remains fully submerged leading to a compact support domain in parallel modules for erosion resulting from the high velocity fluid stream granular particles may become separated from the bed the fluid property is calculated on the granular particles with zeroth order averaging through shepard corrected kernel in the previous framework ultimately the gradient divergence of any property can be computed based on the support domain of the granular media in the final step this procedure will be able to produce proper result inside the porous media i e where each granular particle has a sufficient number of granular particles present in its support domain however for isolated granular particles this may result in erroneous interpolation of information the case shown in fig 7 contains 3 4 particles present in the influence zone each granular particle sparse irregular distribution of these particles around the edge of the support domain will result in a badly scaled corrective matrix for kernel gradient modification bonet and lok 1999 thus the kernel gradient may have to be taken as zero for these secluded particles however inside a high velocity fluid stream the pressure gradient is bound to be sufficiently high to affect the particle movement thus a different procedure has been proposed in the current study in the present work the pressure is not directly interpolated into the granular particles instead the fluid pressure gradient is calculated on each fluid particle the pressure gradient is directly interpolated on the granular particles the procedure requires additional computational overhead for searching and calculating pressure gradient stress divergence on each fluid particles however the procedure is completed within the first two steps of the aforementioned mechanism 3 results and discussions the proposed mechanism is applied to three cases of dam break induced erosion enclosed in a box the initial condition is depicted in the fig 8 all three cases have sand as the bed material spinewine 2005 the lock gate is removed swiftly to study the temporal variation of bed these cases are also simulated using the previous method and compared with the proposed variant the resolution is deliberately kept coarse initial interparticle spacing l 0 0 0125 m to assess the effectiveness of the model all the simulation parameters e g support domain radius free surface detection criteria etc have been kept identical for both variants the third case is also replicated with separate resolutions for different modules l 0 s a n d 0 02 m l 0 w a t e r 0 0125 m to study the effect of the developed algorithm the smoothing length is taken as 1 60l 0 calculation of effective porosity utilizes the smoothing length of the granular domain whereas the transfer of fluid information to granular module considers smoothing length of the fluid module in case of identical resolutions the smoothing length for either of the cases remains unchanged simulated fluid profiles with the proposed framework along with experimental data have been shown in fig 9 the effective porosity serves as an indicator of the granular profile at different time instances the experimental data of spinewine 2005 has been shown by using circles for water and triangles for granular profile the progression of fluid fronts for the third scenario along with identical coarse resolutions for granular media has been depicted in fig 10 the coupled isph framework with the proposed mechanism can reproduce accurate fluid and granular profiles there are slight discrepancies in initial time instances which may be attributed to instantaneous gate movement however upward downward gate removal fu and jin 2016 produces different results a comprehensive evaluation of these issues is beyond the scope of the current study it should also be noted that effective porosity is not applicable for numerical models having lengthscales smaller than the proper rev in the present study the minimum average interparticle spacing is assumed to be approximately 6d d being the granular particle diameter finer resolutions for continuum models may produce lesser numerical error though they may not necessarily provide a physically accurate result compared to their coarser counterparts the initial effective porosity distribution for the existing proposed numerical model with coarse identical resolutions is shown in fig 11 zero on the z axis denotes the position of the granular media the diffused effect is prominently visible for the coarse resolution for the erstwhile model the effect of granular media quickly disappears outside the porous domain for either of the resolutions the overestimation of effective porosity for fluid particles inside the granular media is reduced substantially for the proposed variant for a superimposed fluid particle layer at zeroth level effective porosity does not vary considerably however zero distance is quite unlikely to happen as both modules are susceptible to rapid movement according to their corresponding governing equations the distribution in both present previous model is heavily dependent on smoothing length and kernel choice the larger smoothing length for coarse particle distribution produces a diffused porosity distribution a different approximation of kernel gradient e g derivative approximation through mps may help for such cases however comparing various approximations of kernel gradients is beyond the scope of the current study the drag force distribution at 0 25 s has been shown in fig 12 for different information mechanisms fluid velocity decreases suddenly at the granular surface for the current improved framework unlike the previous case thus exposed granular particles are more prone to dislodgement due to the imposition of concentrated drag force instead of diffused distribution the drag force attains its maximum value just outside the granular domain for the erstwhile framework this defect is amplified by the combined errors in the calculation of effective porosity and particle number np calculation of relative velocity also influences the spread of drag forces far outside the granular media current modification produces a comparatively concentrated distribution of effective porosity and relative velocity thus the drag force reduces rapidly outside the granular domain the peak value of drag force just coincides with the granular interface the proposed model is essentially applicable for bed load transport some particles may get dislodged and move higher into fluid stream with large velocity however they do not undergo phase change or get diffused into fluid particles the temporal variation of bed level is shown in fig 13 the bed level variation for third scenario are compared for existing and improved models with both identical and coarse resolutions the patterns over 2 s have been depicted in fig 14 even though the overall pattern for either of the cases remains similar a few particles can be seen moving separately in the improved framework it can be attributed to sharp velocity distribution of the fluid nearby granular interface for the zeroth order consistent cases the granular slope becomes stable within a short period for scenario i ii for the third scenario similar trend can be observed for the improved previous models having both identical different resolutions it may be noted that initially 5 horizontal layers of particle signify the granular domain for scenario iiib fig 14 highlights the capability of the proposed model across different resolutions the dislodgement of particles at the downstream of the gate can be related to dune migration the proposed model is more suitable for such scenarios whereas the previous model forms an apparent protective overlay of artificial drag force just above the granular interface figs 9 and 10 show that the current variant can reproduce fluid and bed profiles satisfactorily the current mechanism may slightly increase the computational time as an extra step involving gradient divergence calculation has to be incorporated thus the corrective matrices need to be inverted at every location however searching of particles is required irrespective of the mechanisms overall the proposed modification consumes 4 of its predecessor 4 conclusion the current study demonstrates a significant improvement over existing coupled lagrangian models sph dem sph sph regarding applicability in lower identical resolutions inclusion of gradients reduces the diffused distribution of effective porosity the information transfer mechanism has been tested for three cases of rapid sediment transport induced by instantaneous dam break flows the smeared effect of resistive forces arising out of granular media has been reduced substantially consequently granular particles tend to travel longer distance while interacting with the rapidly moving fluid front effective porosity is calculated efficiently for the loosely disintegrated fluid granular interface the effect of artificial drag force has been severely curtailed though it cannot be eliminated completely 
586,local characteristics of extreme rainfall quantiles manifested through intensity duration frequency idf curves are key to infrastructure design due to climate change rainfall extremes are subject to changes it is therefore crucial to explore the potential impacts these changes will have on design storms a new strain of methodologies quantile based downscaling approaches have recently been proposed to exclusively downscale extreme rainfall quantiles obtained from global climate models gcms these approaches however have not been systematically intercompared and the uncertainties related to assigning future design storms are poorly understood this study evaluates the functionality of three quantile based downscaling methods during the historical and future periods in montreal canada results show that the performance of quantile based downscaling approaches in reproducing observed extreme quantiles can be divergent at lower return periods however differences between the three schemes are not significant similar performances for reproducing historical rainfall extremes however does not necessarily imply similar future projections due to the different functionalities of the three approaches in mapping gcm projections into finer scales despite these uncertainties the total projection range of future rainfall extremes are in many cases comparable to the confidence interval of the parametric probability distribution when fitted to the observed annual maximum rainfall series a risk based approach to accommodate this uncertainty in vulnerability assessments through evaluating potential alterations in historical rainfall extremes using an ensemble projection coming from multiple downscaling approaches is suggested this allows for the selection of design storms based on the acceptable level of risk and given budgetary and operational restrictions keywords extreme rainfall quantiles design storms intensity duration frequency idf curves climate change quantile based downscaling benchmarking 1 introduction extreme rainfall quantiles are the basis for the design and operation of various urban infrastructures including storm water management systems and are also necessary in terms of upgrading urban infrastructure in an engineering context information on rainfall extremes is commonly communicated through intensity duration frequency idf curves cheng and aghakouckak 2014 veneziano and yoon 2013 sarhadi and soulis 2017 although they are often considered stationary in time see mailhot et al 2007 agilan and umamahesh 2016 sufficient evidence exists to indicate significant climate change induced alterations in extreme rainfall characteristics reflected in the idf curves kim et al 2006 seneviratne et al 2012 berg et al 2013 burn and taleghani 2013 colmet daage et al 2018 these changes are expected to intensify in future fowler et al 2005 westra et al 2014 posing an engineering challenge as future flood defense systems can no longer be designed based solely on historical extremes evaluating the impact of climate change on rainfall extremes often involves using projections from global climate models gcms see ipcc 2013 gcms however are limited in their ability to represent precipitation in particular gcms often underestimate point scale extreme rainfall events especially those with large return periods maraun and widmann 2018 moreover gcm projections are highly divergent between models even under a unique scenario of change smith 2002 wilby 2010 to overcome this a large ensemble of gcms is often used to quantify future normal and extreme rainfall willems 2013 wilby 2017 san martín et al 2017 with hopes that the range captured by the ensemble includes potential future extremes nevertheless current gcm projections are mostly available at a daily scale with a spatial resolution of one hundred kilometers or more ipcc 2013 which is far coarser than the spatiotemporal resolution required for local design and assessment purposes to overcome the scale discrepancies between available projections and the need for impact assessment various downscaling approaches have been proposed despite the availability of dynamic downscaling approaches e g bergström et al 2001 wood et al 2004 the cascade of statistical downscaling models is the most commonly used approach particularly for impact assessment statistical downscaling is based on finding a set of empirical links between the gcms outputs and the fine scale climate fields during a historical baseline period by assuming that the empirical links between climate variables at large and local scales remain unchanged in the future fine scale future climate variables can be estimated using future projections of gcms along with the empirical mapping established during the baseline period wilby et al 2004 diaz nieto and wilby 2005 jacobeit et al 2014 in this setup statistical models are first used to spatially downscale the daily gcm projections to daily rainfall series at the local scale wilby et al 1998 2002 regression based frameworks weather typing approaches and stochastic weather generators are widely used statistical techniques that can translate daily gcm outputs to daily local rainfall series e g wilby et al 2000 semenov et al 2002 kang et al 2007 hundecha and bárdossy 2008 solaiman et al 2011 tareghian and rasmussen 2013 spatially downscaled daily rainfall time series can be further disaggregated or temporally downscaled into finer time resolutions in fact various parametric and non parametric disaggregation methods have been used to transfer daily rainfall values to sub daily e g solaiman and simonovic 2011 liew et al 2014 rodríguez et al 2014 gunawardhana et al 2018 and sub hourly values peck et al 2012 accordingly through the cascade use of spatial and temporal downscaling models one or more continuous realizations of fine scale rainfall can be obtained from this the annual maximum precipitation amp can be extracted across daily and sub daily durations the amp series can then be analyzed using a parametric probability distribution to quantify extreme rainfall quantiles corresponding to a range of return periods e g nazemi et al 2014 the resultant projected idf curves can then be further bias corrected based on historical idf curves the performance of cascade downscaling for the estimation of fine scale rainfall extremes has been widely studied in the literature e g mohymont et al 2004 haylock et al 2006 maraun et al 2011 tryhorn and degaetano 2011 sunyer et al 2012 camici et al 2013 bi et al 2017 these studies among others revealed that continuously downscaled projections remain limited until very recently the main focus in continuous downscaling methods was to represent the first few statistical moments of an observed rainfall series these first statistical moments however may poorly represent the empirical i e observed frequency of rainfall extremes schoof 2013 wilby 2017 papalexiou 2018 furthermore both spatial and temporal downscaling are uncertain due to inappropriate choices of predictors model structure and or parameters e g cavazos and hewitson 2005 najafi et al 2010 this results in large cascade uncertainty when estimating extreme rainfall quantiles e g chandler et al 2011 bi et al 2017 to avoid some of the aforementioned limitations in cascade downscaling a focus on downscaling extreme rainfall quantiles instead of downscaling the whole rainfall series has been explored in the recent literature the term quantile based downscaling is used to refer to these methods this must be distinguished from quantile regression friederichs and hense 2007 cannon 2011 or quantile mapping li et al 2010 gudmundsson et al 2012 which have been used as tools for the spatial downscaling of rainfall series e g mirhosseini et al 2013 although quantile based downscaling approaches may include other forms of uncertainty also discussed in this paper their advantage can allow for a reduction in the propagation of error in the cascade downscaling approach as they focus on certain rainfall quantiles rather than the entire time series as a result quantile based downscaling approaches are particularly suitable for projecting idf curves under future conditions moreover they include simpler methodologies and are less computationally expensive thereby facilitating their application in practice schardong et al 2015 srivastav et al 2015 approaches to quantile based downscaling differ based on their modeling assumptions and the way in which they build the mapping relationships between model based and observed rainfall extremes technically a quantile based downscaling approach can be seen as either i the parametric transformation of quantiles or ii as the parametric transformation of several moments of rainfall extremes across a range of across a range of spatiotemporal scales from the first category of models hassanzadeh et al 2014 developed the quantile quantile downscaling qqd method to spatiotemporally downscale extreme rainfall quantiles in one parametric platform similarly equidistance quantile matching eqm developed by srivastav et al 2014 spatiotemporally downscales extreme rainfall quantiles using a two step parametric procedure from the second category of quantile based approaches the scale invariance method sim developed by nguyen et al 2007 derives the distributions of short duration local extreme rainfalls based on those of longer duration using the scaling relationships between non central moments over different rainfall durations although these methodologies present alternative hypotheses to tackle the downscaling problem little is known about their relative performance as they have not yet been formally benchmarked and intercompared this study aims to perform a critical evaluation of qqd eqm and sim in montreal canada to the best of the authors knowledge these three techniques are the only generic parametric quantile based downscaling approaches that are used to project idf curves in canada and elsewhere e g singh et al 2016 elshorbagy et al 2015 hadipour et al 2013 willems et al 2012 the following section explains these quantile based downscaling methodologies general information about the observed and gcm based data are provided in section 3 the benchmarking procedure for diagnosing model functionality and quantifying the total uncertainty is explained in section 4 and the results are presented in section 5 section 6 provides a discussion related to the handling of uncertainties in extreme rainfall quantiles by promoting a risk based framework to vulnerability assessment and design under uncertain future conditions finally section 7 summarizes the key findings and provides some concluding remarks 2 quantile based downscaling of rainfall extremes 2 1 characterizing extreme rainfall quantiles as noted above in quantile based downscaling approaches the focus of downscaling is shifted from continuous time series to amps or their corresponding quantiles obtained through a parametric probability distribution the generalized extreme value gev distribution has been frequently used for characterizing amps nguyen et al 2017 gev can be seen as a combination of gumbel fréchet and weibull probability distributions hosking and wallis 2005 katz 2012 building on the assumption that amps are independent and identically distributed iid random variables it has been widely shown that the gev distribution can sufficiently describe the empirical distribution of amps across various scales and regions e g bougadis and adamowski 2006 nazemi et al 2011 yilmaz and perera 2013 blanchet et al 2016 and can better describe the upper tail behavior of the extremal data by introducing the shape parameter khaliq et al 2006 overeem et al 2008 the gev distribution can be formulated as hosking et al 1985 1 f x μ σ ε e x p 1 ε x μ σ 1 ε where f 01 is the non exceedance probability i e the percentile of the random variable x i e quantile μεr is the location parameter σ 0 is the scale parameter and εεr is the shape parameter the maximum likelihood method katz and brown 1992 was used to estimate gev parameter values and their corresponding confidence intervals 2 2 quantile quantile downscaling qqd the qqd is inspired by the existence of a strong quantile quantile relationship between daily rainfall extremes at a large scale and the corresponding daily and sub daily rainfall extremes at a local scale see hassanzadeh et al 2014 for details of the methodology fig 1 shows the procedure taken in the qqd for the estimation of future extreme rainfall quantiles at fine spatiotemporal scales in brief a probability distribution in this case gev is first fitted to i daily and sub daily amps observed at the local scale during a baseline historical period ii daily model based amps during the same baseline and iii daily model based amps in future periods secondly a set of functional relationships are found between model based extreme rainfall quantiles and corresponding local rainfall quantiles during the baseline period third by assuming that the extracted quantile quantile relationships during the baseline period remain unchanged in time the relationships are fed by daily model based extreme rainfall quantiles in future to estimate the corresponding extreme rainfall quantiles at daily and sub daily scales locally furthermore linear regression is used to form quantile quantile relationships between model based and local extremes due to the existence of strongly linear relationships between daily gcm based and observed amp quantiles at daily and sub daily scales manifested through extracted functional relationships as a result of an extensive evolutionary search see hassanzadeh et al 2014 2 3 equidistance quantile matching eqm as the qqd method solely considers the quantile quantile relationships between large scale and local rainfall extremes during the baseline period the temporal correspondence between the model based projections and local rainfall extremes are completely ignored in addition the quantile quantile relationships between local and model based extreme rainfalls are assumed to be unchanged over time to avoid these limitations eqm uses a two step approach to find the linkage between the model based amps and the pseudo local amps during the baseline period and to find the linkage between model based daily amps and the pseudo model based projections of future daily amps see srivastav et al 2014 for methodological details pseudo amps are amp series that are reordered to correspond with the order of model based amps during a typical baseline period using these two groups of mappings the future model based daily amps can be translated into future local amps across a range of temporal scales fig 2 shows the workflow of the eqm downscaling procedure in brief the downscaling procedure includes 1 fitting a probability distribution i e gev to daily and sub daily observed amps during the baseline as well as to model based daily amps during the baseline and future periods 2 finding a set of linear relationships between the daily and sub daily amps which are reordered based on the model based amps i e pseudo local amps and the model based amps steps i ii iv v 3 finding a similar set of linear relationships between the model based amps during baseline and pseudo future amp conditions that are reordered to replicate the order of baseline model based amps steps ii iii vi and vii 4 estimating the future amps at the local scale by feeding the relationships found in 2 and 3 with future model based daily amp quantiles step viii and finally 5 fitting a probability distribution gev to estimate future local amps to quantify future extreme rainfall quantiles at the local scale step ix for more details see simonovic et al 2016 eqm is used to develop the idf cc tool software http www idf cc uwo ca that constructs future idf curves for both gauged and ungauged locations across canada sandink et al 2016 2 4 scale invariance method sim in contrast to qqd and eqm which aim at finding a set of direct mappings between quantiles of daily and sub daily amps the focus of sim is to derive the statistical properties of sub daily amps based on those of daily amps this is achieved by finding scaling relationships between non central moments ncms of extreme rainfall events of different durations fig 3 shows the workflow of the method the downscaling procedure includes two main stages 1 performing the necessary bias adjustment of daily model based amps with corresponding local values and accordingly estimating the ncms of the adjusted daily model based amps steps i to iii and 2 deriving the distributions and quantiles for sub daily model based amps based on those of the adjusted daily model based amps steps iv to vi given the simplicity of the workflow the sim method significantly reduces parametric complexity for more details on sim see nguyen et al 2007 2008 3 case study and data the greater montreal area is the second most populous region in canada with a 30 year annual average rainfall of 785 mm from 1981 to 2010 recorded at montreal s pierre trudeau international airport previous studies showed that the range of variability in precipitation is expected to increase in montreal e g nalley et al 2012 ville de montreal 2017 most importantly climate change will likely alter the magnitude and frequency of daily e g jaramilo and nazemi 2017 and sub daily e g mailhot et al 2007 desramaut 2008 extreme rainfall in the study area sub hourly to daily 5 min to 24 h duration amps of montreal during the period of 1961 to 1990 were obtained from data observed at montreal s pierre trudeau international airport available at http climate weather gc ca historical data search historic data e html only precipitation events during the no snow season from may to october were considered as a result the data only included extreme rainfall events for model based data we used the national aeronautics and space administration s nasa downscaled daily precipitation product nex gddp available at https cds nccs nasa gov nex gddp nex gddp includes projections of 21 gcms that are bias corrected and spatially downscaled to a 25 km 25 km resolution the bias corrected daily precipitation projections extend from 1950 through 2005 retrospective runs as well as from 2006 to 2100 prospective runs prospective projections are available for two representative concentration pathways rcps of 4 5 and 8 5 w m2 which respectively represent stabilized and high radiative forcing van vuuren et al 2011 fix et al 2018 model based daily amps at the grid that included montreal s pierre trudeau airport were extracted from 20 models that had complete data for both the retrospective and prospective periods see table s1 in the supplementary materials for the list of gcms model based amps were divided into four 30 year periods including the historical baseline 1961 1990 as well as future projections representing short term 2011 2040 mid term 2041 2070 and long term 2071 2100 futures 4 implementation and benchmarking procedure 4 1 validation of gev shape parameters the gev distribution was used to estimate the observed and model based amp quantiles before implementing the fitted distribution for quantifying extreme quantiles however it is important to verify that reasonable estimates are obtained for all gev parameters in particular the values of the shape parameter play a critical role in presenting the tails of the distribution fig 4 shows the estimated shape parameters of the gev distribution fitted to observed amps at daily sub daily and sub hourly scales during the baseline period left panel as well as model based daily amps during baseline and future periods right panel the range of expected shape parameters for observed and model based amps is between 0 5 and 0 5 this is in line with earlier findings e g hosking et al 1985 martins and stedinger 2000 papalexiou and koutsoyiannis 2013 in particular tan and gan 2016 reported a range of 0 5 to 0 5 for the shape parameter of the gev distribution fitted to observed amps of 463 stations across canada including this study area 4 2 benchmarking the functionality of quantile based downscaling approaches in order to systematically benchmark the functionality of quantile based downscaling schemes the first step was to assess the accuracy of different methods in reproducing baseline extreme rainfall quantiles across a range of durations and return periods the expected relative errors in percentage were then calculated for the three downscaling methods and used the 20 gcms we then tested if similar estimations during the historical period necessitates similar projections in the future time horizons to benchmark the difference in future estimates of local extreme rainfall quantiles the expected relative differences in percentage between the future extremes and the corresponding baseline values was calculated similar to the comparison made during the baseline period the expected values were calculated by averaging the relative change over the 20 gcms considered the observed and model based extreme rainfall quantiles at the daily scale are unique this means that the predictor predictand are similar for the three downscaling approaches therefore any discrepancy between projections of local daily extreme rainfall quantiles at a particular return period can be linked to the different functionality of downscaling approaches in converting the model based daily extremes to corresponding local values this was used as a basis to quantify the effect i e the footprint of downscaling approaches on altering model based rainfall extremes and to address downscaling uncertainty for this purpose the difference between corresponding downscaled extreme quantiles under baseline and future conditions were calculated accordingly the pair differences obtained from the 20 gcms were averaged to provide the expected difference between local estimations of extreme rainfall quantiles under baseline and future conditions for each percentile rcp future horizon and downscaling methodology in parallel future horizon and return period the difference between the baseline and future gcm based daily extreme rainfall quantiles was calculated for each model at given rcp conditions the differences obtained for the 20 gcms were then averaged to estimate the expected model based difference between future and baseline rainfall extremes using these measures the footprint of each downscaling approach in altering the daily model based extreme rainfall quantiles was quantified by calculating the relative discrepancy between expected differences at the local and grid scale obtained for a particular percentile future period and rcp scenario it should be noted that this is a scaled measure values above zero indicate that the downscaling method increases the difference between the corresponding daily quantiles at the grid scale when mapping them to the local scale values below zero indicate that the downscaling approach decreases the existing discrepancies at the grid scale values equal to zero indicate the difference between estimated local daily extreme rainfall quantiles is equal to the corresponding difference at the grid scale a similar approach was taken to quantify how a quantile based downscaling approach perturbs the difference between future model based daily extreme rainfall quantiles at a particular duration and future horizon under two different rcps first for each percentile gcm future period and downscaling methodology the difference between estimations of local daily extreme quantiles under rcp4 5 and rcp8 5 was calculated for each gcm model values obtained for the 20 gcms were then averaged to provide the expected difference between local estimations of extreme rainfall quantiles under two possible climate futures in parallel the difference between future model based daily extreme rainfall quantiles under rcp 8 5 and rcp 4 5 was calculated similarly for each future horizon and return period the differences obtained by the 20 gcm models were averaged to estimate the expected value for the differences between gridded estimations of extreme rainfall quantiles under the two rcp scenarios by calculating the percentage of relative discrepancy between the expected differences at the grid and local scales the effect of the downscaling approach on altering the model based extreme rainfall quantiles can be estimated at each return period and future horizon this is again a scaled measure values above zero indicate that the downscaling method increases the difference obtained at the grid scale between two rcps at a particular percentile and future horizon values below zero indicate that the downscaling approach decreases the gridded difference between two rcps values equal to zero resemble conditions in which the difference between estimated local daily extreme rainfall quantiles are equal to the corresponding difference at the grid scale 4 3 benchmarking the total uncertainty in future projections theoretically the projections of local extreme rainfall quantiles at a specific return period duration and rcp can be divergent due to the different combinations of gcm models and quantile based downscaling methods to gauge the total uncertainty in estimating the future extreme quantiles a collective ensemble for future projections at each return period duration and rcp scenario was built this was done by mixing the results obtained from applying the three downscaling methods to the 20 gcm outputs and two rcps hereafter collective ensemble projection cep at each duration and return period the variability of cep was compared with the 95 confidence interval of the gev distribution fitted to the observed amps during the baseline period these confidence intervals represent the uncertainty in the estimation of historical extreme rainfall quantiles without any contributions from gcms rcps and or downscaling methods if the range of cep was larger than the range of observed extreme rainfall quantiles during the baseline period then the future projections included more uncertainty compared to the observed extreme rainfall quantiles 5 results for simplicity s sake the results are presented for only three durations i e 5 min 1 h and 24 h and three return periods i e 2 10 and 100 year return periods corresponding to 0 5 0 9 and 0 99 percentiles respectively 5 1 effectiveness in reproducing observed extreme quantiles ideally quantile based downscaling approaches should be able to match the observed extreme rainfall quantiles during the baseline period without any error across all gcms however this is rarely the case fig 5 shows the ensembles of relative errors in reproducing the observed extreme rainfall quantiles during the baseline period these ensembles were obtained by downscaling the historical simulations of the 20 bias corrected gcms using the three quantile based algorithms qqd eqm and sim among the three downscaling methods qqd and eqm demonstrated almost identical ensembles in terms of range and median of relative errors under all durations and or return periods nonetheless they were unable to downscale all bias corrected gcms with equal effectiveness the discrepancy between downscaling performances across gcms increases at the largest return period our further analysis showed that the gcm based extreme rainfall quantiles are considerably smaller than corresponding observed values during the baseline period in particular the maximum expected relative difference between gcm based and observed extreme rainfall quantiles is 40 which occurs during the 100 year return period this indicates that using downscaling approaches the difference between estimated and observed extreme rainfall quantiles has been reduced see fig 5 right bottom panel however downscaling approaches eqm and qqd were not able to completely reduce the difference between gcm based and observed extreme rainfall quantiles in 100 year return period the sim method in contrast presented the smallest range of relative error revealing that this method downscaled all bias corrected gcms with similar skill and hence was more robust compared to qqd and eqm when reproducing observed baseline extremes the expected percentage of relative error in reproducing the observed extremes for selected durations and return periods was also estimated the results are shown in table s2 in the supplementary materials the lowest error values for each case are bolded this analysis clearly shows that the performance of the three downscaling approaches varies across rainfall durations and return periods highlighting the fact that none of the considered approaches consistently outperformed the others in addition in certain cases two downscaling approaches provide very similar results therefore choosing only one downscaling method with a slightly better mean or range of relative error during the baseline period may in fact lead to failure in capturing all modes of potential change in extreme rainfalls finally it is apparent that the magnitude of expected error increases considerably under the 100 year return period across all durations this means that the quantile based downscaling approaches are not able to fully reproduce extreme quantiles when the return period goes beyond the length of data record and that extrapolation at the upper tail of the gev distribution is required 5 2 discrepancies in projections of future extreme quantiles as a first step to benchmark the functionality of quantile based downscaling methods future extreme rainfall quantiles across a range of durations and return periods were estimated and compared with the corresponding observed extreme rainfall quantiles during the baseline period fig 6 shows the results of this analysis in terms of the ensembles of relative differences between future and observed local extreme rainfall quantiles projected quantiles were estimated using the three quantile based downscaling methods applied to the 20 bias corrected gcms in the three future time horizons under rcp 4 5 in each panel and for each return period the boxplots for qqd eqm and sim are shown in black green and blue respectively given a particular duration and return period the sign and magnitude of change in future extreme rainfall quantiles varies significantly based on the applied downscaling method as a general observation similar downscaling skills during the historical period do not necessarily result in similar projections during future periods for instance despite the fact that qqd and eqm map the observed extreme rainfall quantiles with similar skill during the baseline period see fig 5 they do not provide similar projections under future conditions in fact the discrepancy between the results of qqd and eqm are significant and the estimated signs of change are inconsistent across various durations and or return periods in contrast while qqd and sim presented different performances in matching the observed extreme rainfall quantiles during the baseline period they provide relatively similar projections under future conditions fig 7 shows similar results for rcp 8 5 when comparing figs 6 and 7 it is observed that the three downscaling methods project the same signs of change in extreme rainfall quantiles under both rcps in almost all durations and return periods nevertheless the average magnitudes of change over the gcms are consistently larger under rcp 8 5 this translates into a greater difference between the three downscaling methods particularly for larger return periods and over the long term future horizon for instance while eqm estimates a 20 decrease for long term future daily extremes at the 100 year return period sim estimates a 20 increase see bottom right panel in fig 7 this results into a 40 expected difference between projections obtained using these two downscaling methods handling the uncertainty related to estimating the sign and magnitude of change is extremely important and will be discussed in more detail in section 6 5 3 divergent functionality of quantile based downscaling methods given the significant discrepancies in the future projections of local rainfall extremes obtained using the three different quantile based downscaling approaches further efforts toward a detailed understanding of the differences in the functionality of quantile based downscaling approaches across a range of durations return periods and projection horizons are required fig 8 summarizes the footprint of downscaling approaches in altering the gridded extremes under the rcp 4 5 and rcp 8 5 scenarios in top and bottom rows respectively in general the three quantile based methods demonstrate their largest effect on altering the model based quantiles at the 100 year return period this requires extrapolation beyond the record length using the gev distribution this extrapolation results in substantial differences in the future daily projections across the three downscaling approaches in addition the figure shows the divergent functionality of downscaling methods in converting the model based daily extreme rainfall quantiles to corresponding local estimates for instance while qqd increases the difference between baseline and future model based extreme rainfall quantiles the eqm dampens this difference this divergent functionality justifies the difference between projections of extreme rainfall quantiles obtained from these two algorithms among the three quantile based schemes sim has the largest impact on altering the model based extreme rainfall quantiles during the downscaling process similarly the percentage of relative change in projections of daily extreme rainfall quantiles were calculated under both rcps 4 5 and 8 5 at the gcm and local scales results are summarized in fig 9 for each downscaling scheme and future horizon in general these results confirm the earlier results presented in fig 8 in which the sign and magnitude of footprints depend on the downscaling method with the largest impact occurring at the 100 year return period in addition stronger footprints are evident in most cases during the 2071 2100 period 5 4 gauging the total uncertainty the results presented in section 5 2 revealed substantial differences in future projections of extreme rainfall quantiles particularly under return periods beyond the observational length in this case 30 years where extrapolation based on gev distribution was required it was also shown in section 5 3 that the uncertainty in future projections corresponded to the divergent functionality of quantile based downscaling approaches that can have significantly different footprint when converting gridded projections of extreme quantiles into corresponding local values one question however remains unanswered how large is this uncertainty compared to the existing uncertainty in assigning historical extreme rainfall quantiles to answer this the ceps of downscaled extreme rainfall quantiles across the nine combinations of durations and return periods were considered and implemented in the benchmarking procedure outlined in section 4 3 fig 10 summarizes the results of this analysis in all panels solid and dashed lines demonstrate expected values and 95 confidence intervals for observed extreme rainfall quantiles respectively boxplots show the range of ceps for different extreme rainfall quantiles obtained under rcps 4 5 and 8 5 green dots display the expected values of ceps obtained by averaging the projections across three downscaling methods and twenty gcms some interesting observations can be seen in this figure most importantly for 1 h and 24 h durations in the 100 year return period the cep has a smaller range compared to the corresponding baseline extreme rainfall quantiles this means that under these conditions the uncertainty in assigning the extreme rainfall quantiles during the baseline period is larger than the uncertainty in future projections based on the ensemble of 120 members i e 3 downscaling approaches 20 climate models 2 rcps these analyses indicate that the uncertainty in projecting the future extreme rainfall quantiles due to combinations of gcms rcps and downscaling approaches was comparable to the uncertainty in assigning the extreme rainfall quantiles during the baseline period manifested through 95 confidence interval 6 discussion results presented in section 5 highlighted the uncertainties in projecting extreme rainfall quantiles using quantile based downscaling approaches due to the divergent functionality of the downscaling procedures in converting the gridded extreme quantiles to corresponding local values in addition it was noted that as a result of the extrapolation made by the gev distribution the uncertainty increases in the 100 year return period although this uncertainty can be substantial the total uncertainty in assigning future extreme rainfall quantiles is comparable to the uncertainty in assigning the extreme rainfall quantiles during the baseline period here we argue that similar to the confidence interval of the gev distribution the uncertainty in the estimation of future extreme rainfall quantiles can be handled and described using the empirical characteristics of ensemble projections below we discuss this in more details and highlight its practical implications for vulnerability assessment of existing systems and for picking a design value for new infrastructures 6 1 implications on vulnerability assessment of existing infrastructure to ensure the reliable operation of existing urban infrastructures under future conditions it is critical to assess the vulnerability of the system designed under historical conditions it should be noted that the values of extreme rainfall quantiles on the idf curves represents the most likely parametric sets of the gev distribution associated with the best fit within the 95 confidence interval as a result they do not include information on the range of potential rainfall intensities associated with a given return period theoretically this range can be described by a risk profile which quantifies the empirical probability distribution functions epdfs of the ensemble members within the historical 95 confidence interval of the gev distribution a similar approach could be taken to quantify the risk profiles associated with future projections of extreme rainfall quantiles based on a set of gcms and downscaling approaches these risk profiles characterize the uncertainty of future extreme rainfall quantiles in light of the climate models rcps and downscaling methods considered when compared with the corresponding risk profile during the baseline period it is possible to estimate the change in the range of extreme rainfall quantiles and their associated probability as an example fig 11 shows the risk profiles for the baseline period base and future extreme rainfall quantiles obtained using different downscaling methods applied to the 20 bias corrected gcm projections for 2041 2070 under rcps 4 5 and 8 5 the results for this time horizon are shown here due to their importance for assessment and development of current urban infrastructure in montreal it is apparent from the figure that the range shape and statistics of the future risk profiles are subject to change compared to the baseline period depending on the downscaling approaches used there could however be substantial uncertainty in the form and magnitude of change in risk profiles in the absence of any further information on the suitability of climate models and or downscaling approaches we suggest that the cep be used to provide a basis for representing the uncertainty in extreme rainfall quantiles and accordingly the associated risk in existing infrastructures 6 2 implications on selecting future design storms the selection of a design storm is based on the type and importance of infrastructure under consideration choosing a smaller value for a design storm can increase the chance of infrastructure failure however selecting a larger design storm can lead to over designing the infrastructure which can ultimately increase the cost of construction operation and maintenance one way to incorporate the ensemble of climate change projections into design practice is to define an acceptable risk level and select a design value from a range of possibilities accordingly if the accepted level of risk is known the empirical characteristics of future extreme rainfall quantiles can provide a basis for selecting the design storm in light of the considered gcms and downscaling approaches used empirical cumulative distribution functions of future extreme rainfall quantiles can be used to estimate extreme rainfall intensities across various exceedance probabilities fig 12 shows this analysis for future extreme rainfall quantiles obtained under rcps 4 5 and 8 5 20 gcms and downscaling approaches for the period of 2041 2070 and compares the future risks with associated historical values based on fitting gev distribution to baseline amps climate change alters the current design storms under certain risk levels however the magnitude and sign of change is strongly dependent on the climate models and or downscaling methodology used given the lack of other information on the suitability of a particular gcm and or downscaling methodology the use of all three downscaling approaches in the form of a cep is suggested to assign future design storms rather than relying on results based on individual downscaling approaches for the case shown it can be argued that rainfall extremes are likely to increase in lower return periods i e 2 and 10 years for the 100 year return period the cep profile stays inside the identified profile for the base furthermore the estimated future design storms with a 100 year return period at the 10 level of risk acceptance are smaller than corresponding historical values in all durations see orange line in the bottom row of fig 12 this highlights the need to reconsider the role of existing uncertainty in estimating baseline extreme rainfall particularly in the context of future design 7 conclusions the aim of this study was to benchmark the functionality and associated uncertainty of three quantile based downscaling methods with a greater goal of providing useful insights for practical applications three downscaling methods namely qqd eqm and sim were used to downscale gridded extreme rainfall quantiles obtained from an ensemble of 40 bias corrected gcm rcp pairs to corresponding local values across a range of durations in montreal quebec results showed that none of the downscaling methods could perfectly represent observed extreme rainfall quantiles under the considered durations and return periods moreover while qqd and eqm reproduced the observed extreme rainfall quantiles with similar performance their estimated changes in future extreme rainfall quantiles differ significantly in contrast qqd and sim methods despite different representations of the observed extreme rainfall quantiles during the baseline period presented similar extreme rainfall projections under future conditions to trace the source of the differences between the three downscaling approaches the impact of each downscaling approach in transferring gridded daily extreme rainfall quantiles to corresponding daily extremes at the local scale was quantified it was found that while downscaling by eqm decreases the difference between baseline and future model based extreme rainfall quantiles qqd and sim slightly and greatly increase this difference respectively this observation explains the divergence between future projections of extreme rainfall quantiles obtained from the three methods results also showed that the estimation of future design storms is sensitive to the applied downscaling method in particular the eqm and sim presented the smallest and largest estimations respectively and their differences could be large relying on only one of these downscaling approaches may result in the under or overestimation of design storms which in practice may have large socio economic and environmental consequences we propose then that the projected extreme rainfall quantiles obtained from an ensemble of gcms and quantile based downscaling methods should be considered rather than relying solely on only one methodology it was shown however that the total uncertainty was comparable to the existing uncertainty in assigning the extreme rainfall quantiles during the baseline period particularly at larger durations and return periods to handle this we propose analyzing the corresponding likelihood information in the form of risk profiles and selecting appropriate design storms from a large ensemble based on an acceptable level of risk using empirical characteristics of ensemble projections it was shown how the magnitude and the probability of occurrence in extreme rainfall quantiles will change for montreal under future conditions based on our analyses the future extreme rainfall quantiles will likely increase at smaller return periods but may decrease under the 100 year storm it should be noted that we do not promote quantile based downscaling approaches over continuous downscaling schemes in fact idf curves and therefore quantile based downscaling procedures can only be suitable for design under a single extreme event flooding however may be due to multiple non extreme events that take place sequentially for consideration of such events continuous downscaling techniques are indeed required there is therefore a need to systematically compare the quantile based and continuous based downscaling approaches to identify the pros and cons of each methodology inevitably resulting in the development of more reliable algorithms that are relevant for the estimation of extreme rainfall quantiles moreover whether quantile based or continuous downscaling is used there is a need to investigate the uncertainty in extreme rainfall quantiles due to fitting a probability distribution function and its estimated parameters this issue is important in particular for the estimation of extreme quantiles at large return periods e g 100 year which require extrapolation using parametric probability distributions as a final remark it is important to note that we focused on extreme rainfall events during may to october this means that our analysis did not capture snowmelt and rain on snow events these events play a key role in the formation of urban flooding and infrastructure failure in cold regions particularly under climate change conditions that may increase the frequency of such events hatami et al 2018 we suggest that consideration of extreme precipitation and melt events during winter periods e g yan et al 2018 also be considered in the construction of idf curves in canadian cities acknowledgement this study is supported by an nserc file no 505755 16 engage grant held by jan adamowski we would like to thank mr alain charron ville de montreal for his continuous support of this project from provision of local scale data to constructive feedbacks on our research findings the authors also thank pablo jaramillo for providing model based daily amps of montreal under baseline and future conditions supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 07 001 appendix supplementary materials image application 1 image application 2 image application 3 
586,local characteristics of extreme rainfall quantiles manifested through intensity duration frequency idf curves are key to infrastructure design due to climate change rainfall extremes are subject to changes it is therefore crucial to explore the potential impacts these changes will have on design storms a new strain of methodologies quantile based downscaling approaches have recently been proposed to exclusively downscale extreme rainfall quantiles obtained from global climate models gcms these approaches however have not been systematically intercompared and the uncertainties related to assigning future design storms are poorly understood this study evaluates the functionality of three quantile based downscaling methods during the historical and future periods in montreal canada results show that the performance of quantile based downscaling approaches in reproducing observed extreme quantiles can be divergent at lower return periods however differences between the three schemes are not significant similar performances for reproducing historical rainfall extremes however does not necessarily imply similar future projections due to the different functionalities of the three approaches in mapping gcm projections into finer scales despite these uncertainties the total projection range of future rainfall extremes are in many cases comparable to the confidence interval of the parametric probability distribution when fitted to the observed annual maximum rainfall series a risk based approach to accommodate this uncertainty in vulnerability assessments through evaluating potential alterations in historical rainfall extremes using an ensemble projection coming from multiple downscaling approaches is suggested this allows for the selection of design storms based on the acceptable level of risk and given budgetary and operational restrictions keywords extreme rainfall quantiles design storms intensity duration frequency idf curves climate change quantile based downscaling benchmarking 1 introduction extreme rainfall quantiles are the basis for the design and operation of various urban infrastructures including storm water management systems and are also necessary in terms of upgrading urban infrastructure in an engineering context information on rainfall extremes is commonly communicated through intensity duration frequency idf curves cheng and aghakouckak 2014 veneziano and yoon 2013 sarhadi and soulis 2017 although they are often considered stationary in time see mailhot et al 2007 agilan and umamahesh 2016 sufficient evidence exists to indicate significant climate change induced alterations in extreme rainfall characteristics reflected in the idf curves kim et al 2006 seneviratne et al 2012 berg et al 2013 burn and taleghani 2013 colmet daage et al 2018 these changes are expected to intensify in future fowler et al 2005 westra et al 2014 posing an engineering challenge as future flood defense systems can no longer be designed based solely on historical extremes evaluating the impact of climate change on rainfall extremes often involves using projections from global climate models gcms see ipcc 2013 gcms however are limited in their ability to represent precipitation in particular gcms often underestimate point scale extreme rainfall events especially those with large return periods maraun and widmann 2018 moreover gcm projections are highly divergent between models even under a unique scenario of change smith 2002 wilby 2010 to overcome this a large ensemble of gcms is often used to quantify future normal and extreme rainfall willems 2013 wilby 2017 san martín et al 2017 with hopes that the range captured by the ensemble includes potential future extremes nevertheless current gcm projections are mostly available at a daily scale with a spatial resolution of one hundred kilometers or more ipcc 2013 which is far coarser than the spatiotemporal resolution required for local design and assessment purposes to overcome the scale discrepancies between available projections and the need for impact assessment various downscaling approaches have been proposed despite the availability of dynamic downscaling approaches e g bergström et al 2001 wood et al 2004 the cascade of statistical downscaling models is the most commonly used approach particularly for impact assessment statistical downscaling is based on finding a set of empirical links between the gcms outputs and the fine scale climate fields during a historical baseline period by assuming that the empirical links between climate variables at large and local scales remain unchanged in the future fine scale future climate variables can be estimated using future projections of gcms along with the empirical mapping established during the baseline period wilby et al 2004 diaz nieto and wilby 2005 jacobeit et al 2014 in this setup statistical models are first used to spatially downscale the daily gcm projections to daily rainfall series at the local scale wilby et al 1998 2002 regression based frameworks weather typing approaches and stochastic weather generators are widely used statistical techniques that can translate daily gcm outputs to daily local rainfall series e g wilby et al 2000 semenov et al 2002 kang et al 2007 hundecha and bárdossy 2008 solaiman et al 2011 tareghian and rasmussen 2013 spatially downscaled daily rainfall time series can be further disaggregated or temporally downscaled into finer time resolutions in fact various parametric and non parametric disaggregation methods have been used to transfer daily rainfall values to sub daily e g solaiman and simonovic 2011 liew et al 2014 rodríguez et al 2014 gunawardhana et al 2018 and sub hourly values peck et al 2012 accordingly through the cascade use of spatial and temporal downscaling models one or more continuous realizations of fine scale rainfall can be obtained from this the annual maximum precipitation amp can be extracted across daily and sub daily durations the amp series can then be analyzed using a parametric probability distribution to quantify extreme rainfall quantiles corresponding to a range of return periods e g nazemi et al 2014 the resultant projected idf curves can then be further bias corrected based on historical idf curves the performance of cascade downscaling for the estimation of fine scale rainfall extremes has been widely studied in the literature e g mohymont et al 2004 haylock et al 2006 maraun et al 2011 tryhorn and degaetano 2011 sunyer et al 2012 camici et al 2013 bi et al 2017 these studies among others revealed that continuously downscaled projections remain limited until very recently the main focus in continuous downscaling methods was to represent the first few statistical moments of an observed rainfall series these first statistical moments however may poorly represent the empirical i e observed frequency of rainfall extremes schoof 2013 wilby 2017 papalexiou 2018 furthermore both spatial and temporal downscaling are uncertain due to inappropriate choices of predictors model structure and or parameters e g cavazos and hewitson 2005 najafi et al 2010 this results in large cascade uncertainty when estimating extreme rainfall quantiles e g chandler et al 2011 bi et al 2017 to avoid some of the aforementioned limitations in cascade downscaling a focus on downscaling extreme rainfall quantiles instead of downscaling the whole rainfall series has been explored in the recent literature the term quantile based downscaling is used to refer to these methods this must be distinguished from quantile regression friederichs and hense 2007 cannon 2011 or quantile mapping li et al 2010 gudmundsson et al 2012 which have been used as tools for the spatial downscaling of rainfall series e g mirhosseini et al 2013 although quantile based downscaling approaches may include other forms of uncertainty also discussed in this paper their advantage can allow for a reduction in the propagation of error in the cascade downscaling approach as they focus on certain rainfall quantiles rather than the entire time series as a result quantile based downscaling approaches are particularly suitable for projecting idf curves under future conditions moreover they include simpler methodologies and are less computationally expensive thereby facilitating their application in practice schardong et al 2015 srivastav et al 2015 approaches to quantile based downscaling differ based on their modeling assumptions and the way in which they build the mapping relationships between model based and observed rainfall extremes technically a quantile based downscaling approach can be seen as either i the parametric transformation of quantiles or ii as the parametric transformation of several moments of rainfall extremes across a range of across a range of spatiotemporal scales from the first category of models hassanzadeh et al 2014 developed the quantile quantile downscaling qqd method to spatiotemporally downscale extreme rainfall quantiles in one parametric platform similarly equidistance quantile matching eqm developed by srivastav et al 2014 spatiotemporally downscales extreme rainfall quantiles using a two step parametric procedure from the second category of quantile based approaches the scale invariance method sim developed by nguyen et al 2007 derives the distributions of short duration local extreme rainfalls based on those of longer duration using the scaling relationships between non central moments over different rainfall durations although these methodologies present alternative hypotheses to tackle the downscaling problem little is known about their relative performance as they have not yet been formally benchmarked and intercompared this study aims to perform a critical evaluation of qqd eqm and sim in montreal canada to the best of the authors knowledge these three techniques are the only generic parametric quantile based downscaling approaches that are used to project idf curves in canada and elsewhere e g singh et al 2016 elshorbagy et al 2015 hadipour et al 2013 willems et al 2012 the following section explains these quantile based downscaling methodologies general information about the observed and gcm based data are provided in section 3 the benchmarking procedure for diagnosing model functionality and quantifying the total uncertainty is explained in section 4 and the results are presented in section 5 section 6 provides a discussion related to the handling of uncertainties in extreme rainfall quantiles by promoting a risk based framework to vulnerability assessment and design under uncertain future conditions finally section 7 summarizes the key findings and provides some concluding remarks 2 quantile based downscaling of rainfall extremes 2 1 characterizing extreme rainfall quantiles as noted above in quantile based downscaling approaches the focus of downscaling is shifted from continuous time series to amps or their corresponding quantiles obtained through a parametric probability distribution the generalized extreme value gev distribution has been frequently used for characterizing amps nguyen et al 2017 gev can be seen as a combination of gumbel fréchet and weibull probability distributions hosking and wallis 2005 katz 2012 building on the assumption that amps are independent and identically distributed iid random variables it has been widely shown that the gev distribution can sufficiently describe the empirical distribution of amps across various scales and regions e g bougadis and adamowski 2006 nazemi et al 2011 yilmaz and perera 2013 blanchet et al 2016 and can better describe the upper tail behavior of the extremal data by introducing the shape parameter khaliq et al 2006 overeem et al 2008 the gev distribution can be formulated as hosking et al 1985 1 f x μ σ ε e x p 1 ε x μ σ 1 ε where f 01 is the non exceedance probability i e the percentile of the random variable x i e quantile μεr is the location parameter σ 0 is the scale parameter and εεr is the shape parameter the maximum likelihood method katz and brown 1992 was used to estimate gev parameter values and their corresponding confidence intervals 2 2 quantile quantile downscaling qqd the qqd is inspired by the existence of a strong quantile quantile relationship between daily rainfall extremes at a large scale and the corresponding daily and sub daily rainfall extremes at a local scale see hassanzadeh et al 2014 for details of the methodology fig 1 shows the procedure taken in the qqd for the estimation of future extreme rainfall quantiles at fine spatiotemporal scales in brief a probability distribution in this case gev is first fitted to i daily and sub daily amps observed at the local scale during a baseline historical period ii daily model based amps during the same baseline and iii daily model based amps in future periods secondly a set of functional relationships are found between model based extreme rainfall quantiles and corresponding local rainfall quantiles during the baseline period third by assuming that the extracted quantile quantile relationships during the baseline period remain unchanged in time the relationships are fed by daily model based extreme rainfall quantiles in future to estimate the corresponding extreme rainfall quantiles at daily and sub daily scales locally furthermore linear regression is used to form quantile quantile relationships between model based and local extremes due to the existence of strongly linear relationships between daily gcm based and observed amp quantiles at daily and sub daily scales manifested through extracted functional relationships as a result of an extensive evolutionary search see hassanzadeh et al 2014 2 3 equidistance quantile matching eqm as the qqd method solely considers the quantile quantile relationships between large scale and local rainfall extremes during the baseline period the temporal correspondence between the model based projections and local rainfall extremes are completely ignored in addition the quantile quantile relationships between local and model based extreme rainfalls are assumed to be unchanged over time to avoid these limitations eqm uses a two step approach to find the linkage between the model based amps and the pseudo local amps during the baseline period and to find the linkage between model based daily amps and the pseudo model based projections of future daily amps see srivastav et al 2014 for methodological details pseudo amps are amp series that are reordered to correspond with the order of model based amps during a typical baseline period using these two groups of mappings the future model based daily amps can be translated into future local amps across a range of temporal scales fig 2 shows the workflow of the eqm downscaling procedure in brief the downscaling procedure includes 1 fitting a probability distribution i e gev to daily and sub daily observed amps during the baseline as well as to model based daily amps during the baseline and future periods 2 finding a set of linear relationships between the daily and sub daily amps which are reordered based on the model based amps i e pseudo local amps and the model based amps steps i ii iv v 3 finding a similar set of linear relationships between the model based amps during baseline and pseudo future amp conditions that are reordered to replicate the order of baseline model based amps steps ii iii vi and vii 4 estimating the future amps at the local scale by feeding the relationships found in 2 and 3 with future model based daily amp quantiles step viii and finally 5 fitting a probability distribution gev to estimate future local amps to quantify future extreme rainfall quantiles at the local scale step ix for more details see simonovic et al 2016 eqm is used to develop the idf cc tool software http www idf cc uwo ca that constructs future idf curves for both gauged and ungauged locations across canada sandink et al 2016 2 4 scale invariance method sim in contrast to qqd and eqm which aim at finding a set of direct mappings between quantiles of daily and sub daily amps the focus of sim is to derive the statistical properties of sub daily amps based on those of daily amps this is achieved by finding scaling relationships between non central moments ncms of extreme rainfall events of different durations fig 3 shows the workflow of the method the downscaling procedure includes two main stages 1 performing the necessary bias adjustment of daily model based amps with corresponding local values and accordingly estimating the ncms of the adjusted daily model based amps steps i to iii and 2 deriving the distributions and quantiles for sub daily model based amps based on those of the adjusted daily model based amps steps iv to vi given the simplicity of the workflow the sim method significantly reduces parametric complexity for more details on sim see nguyen et al 2007 2008 3 case study and data the greater montreal area is the second most populous region in canada with a 30 year annual average rainfall of 785 mm from 1981 to 2010 recorded at montreal s pierre trudeau international airport previous studies showed that the range of variability in precipitation is expected to increase in montreal e g nalley et al 2012 ville de montreal 2017 most importantly climate change will likely alter the magnitude and frequency of daily e g jaramilo and nazemi 2017 and sub daily e g mailhot et al 2007 desramaut 2008 extreme rainfall in the study area sub hourly to daily 5 min to 24 h duration amps of montreal during the period of 1961 to 1990 were obtained from data observed at montreal s pierre trudeau international airport available at http climate weather gc ca historical data search historic data e html only precipitation events during the no snow season from may to october were considered as a result the data only included extreme rainfall events for model based data we used the national aeronautics and space administration s nasa downscaled daily precipitation product nex gddp available at https cds nccs nasa gov nex gddp nex gddp includes projections of 21 gcms that are bias corrected and spatially downscaled to a 25 km 25 km resolution the bias corrected daily precipitation projections extend from 1950 through 2005 retrospective runs as well as from 2006 to 2100 prospective runs prospective projections are available for two representative concentration pathways rcps of 4 5 and 8 5 w m2 which respectively represent stabilized and high radiative forcing van vuuren et al 2011 fix et al 2018 model based daily amps at the grid that included montreal s pierre trudeau airport were extracted from 20 models that had complete data for both the retrospective and prospective periods see table s1 in the supplementary materials for the list of gcms model based amps were divided into four 30 year periods including the historical baseline 1961 1990 as well as future projections representing short term 2011 2040 mid term 2041 2070 and long term 2071 2100 futures 4 implementation and benchmarking procedure 4 1 validation of gev shape parameters the gev distribution was used to estimate the observed and model based amp quantiles before implementing the fitted distribution for quantifying extreme quantiles however it is important to verify that reasonable estimates are obtained for all gev parameters in particular the values of the shape parameter play a critical role in presenting the tails of the distribution fig 4 shows the estimated shape parameters of the gev distribution fitted to observed amps at daily sub daily and sub hourly scales during the baseline period left panel as well as model based daily amps during baseline and future periods right panel the range of expected shape parameters for observed and model based amps is between 0 5 and 0 5 this is in line with earlier findings e g hosking et al 1985 martins and stedinger 2000 papalexiou and koutsoyiannis 2013 in particular tan and gan 2016 reported a range of 0 5 to 0 5 for the shape parameter of the gev distribution fitted to observed amps of 463 stations across canada including this study area 4 2 benchmarking the functionality of quantile based downscaling approaches in order to systematically benchmark the functionality of quantile based downscaling schemes the first step was to assess the accuracy of different methods in reproducing baseline extreme rainfall quantiles across a range of durations and return periods the expected relative errors in percentage were then calculated for the three downscaling methods and used the 20 gcms we then tested if similar estimations during the historical period necessitates similar projections in the future time horizons to benchmark the difference in future estimates of local extreme rainfall quantiles the expected relative differences in percentage between the future extremes and the corresponding baseline values was calculated similar to the comparison made during the baseline period the expected values were calculated by averaging the relative change over the 20 gcms considered the observed and model based extreme rainfall quantiles at the daily scale are unique this means that the predictor predictand are similar for the three downscaling approaches therefore any discrepancy between projections of local daily extreme rainfall quantiles at a particular return period can be linked to the different functionality of downscaling approaches in converting the model based daily extremes to corresponding local values this was used as a basis to quantify the effect i e the footprint of downscaling approaches on altering model based rainfall extremes and to address downscaling uncertainty for this purpose the difference between corresponding downscaled extreme quantiles under baseline and future conditions were calculated accordingly the pair differences obtained from the 20 gcms were averaged to provide the expected difference between local estimations of extreme rainfall quantiles under baseline and future conditions for each percentile rcp future horizon and downscaling methodology in parallel future horizon and return period the difference between the baseline and future gcm based daily extreme rainfall quantiles was calculated for each model at given rcp conditions the differences obtained for the 20 gcms were then averaged to estimate the expected model based difference between future and baseline rainfall extremes using these measures the footprint of each downscaling approach in altering the daily model based extreme rainfall quantiles was quantified by calculating the relative discrepancy between expected differences at the local and grid scale obtained for a particular percentile future period and rcp scenario it should be noted that this is a scaled measure values above zero indicate that the downscaling method increases the difference between the corresponding daily quantiles at the grid scale when mapping them to the local scale values below zero indicate that the downscaling approach decreases the existing discrepancies at the grid scale values equal to zero indicate the difference between estimated local daily extreme rainfall quantiles is equal to the corresponding difference at the grid scale a similar approach was taken to quantify how a quantile based downscaling approach perturbs the difference between future model based daily extreme rainfall quantiles at a particular duration and future horizon under two different rcps first for each percentile gcm future period and downscaling methodology the difference between estimations of local daily extreme quantiles under rcp4 5 and rcp8 5 was calculated for each gcm model values obtained for the 20 gcms were then averaged to provide the expected difference between local estimations of extreme rainfall quantiles under two possible climate futures in parallel the difference between future model based daily extreme rainfall quantiles under rcp 8 5 and rcp 4 5 was calculated similarly for each future horizon and return period the differences obtained by the 20 gcm models were averaged to estimate the expected value for the differences between gridded estimations of extreme rainfall quantiles under the two rcp scenarios by calculating the percentage of relative discrepancy between the expected differences at the grid and local scales the effect of the downscaling approach on altering the model based extreme rainfall quantiles can be estimated at each return period and future horizon this is again a scaled measure values above zero indicate that the downscaling method increases the difference obtained at the grid scale between two rcps at a particular percentile and future horizon values below zero indicate that the downscaling approach decreases the gridded difference between two rcps values equal to zero resemble conditions in which the difference between estimated local daily extreme rainfall quantiles are equal to the corresponding difference at the grid scale 4 3 benchmarking the total uncertainty in future projections theoretically the projections of local extreme rainfall quantiles at a specific return period duration and rcp can be divergent due to the different combinations of gcm models and quantile based downscaling methods to gauge the total uncertainty in estimating the future extreme quantiles a collective ensemble for future projections at each return period duration and rcp scenario was built this was done by mixing the results obtained from applying the three downscaling methods to the 20 gcm outputs and two rcps hereafter collective ensemble projection cep at each duration and return period the variability of cep was compared with the 95 confidence interval of the gev distribution fitted to the observed amps during the baseline period these confidence intervals represent the uncertainty in the estimation of historical extreme rainfall quantiles without any contributions from gcms rcps and or downscaling methods if the range of cep was larger than the range of observed extreme rainfall quantiles during the baseline period then the future projections included more uncertainty compared to the observed extreme rainfall quantiles 5 results for simplicity s sake the results are presented for only three durations i e 5 min 1 h and 24 h and three return periods i e 2 10 and 100 year return periods corresponding to 0 5 0 9 and 0 99 percentiles respectively 5 1 effectiveness in reproducing observed extreme quantiles ideally quantile based downscaling approaches should be able to match the observed extreme rainfall quantiles during the baseline period without any error across all gcms however this is rarely the case fig 5 shows the ensembles of relative errors in reproducing the observed extreme rainfall quantiles during the baseline period these ensembles were obtained by downscaling the historical simulations of the 20 bias corrected gcms using the three quantile based algorithms qqd eqm and sim among the three downscaling methods qqd and eqm demonstrated almost identical ensembles in terms of range and median of relative errors under all durations and or return periods nonetheless they were unable to downscale all bias corrected gcms with equal effectiveness the discrepancy between downscaling performances across gcms increases at the largest return period our further analysis showed that the gcm based extreme rainfall quantiles are considerably smaller than corresponding observed values during the baseline period in particular the maximum expected relative difference between gcm based and observed extreme rainfall quantiles is 40 which occurs during the 100 year return period this indicates that using downscaling approaches the difference between estimated and observed extreme rainfall quantiles has been reduced see fig 5 right bottom panel however downscaling approaches eqm and qqd were not able to completely reduce the difference between gcm based and observed extreme rainfall quantiles in 100 year return period the sim method in contrast presented the smallest range of relative error revealing that this method downscaled all bias corrected gcms with similar skill and hence was more robust compared to qqd and eqm when reproducing observed baseline extremes the expected percentage of relative error in reproducing the observed extremes for selected durations and return periods was also estimated the results are shown in table s2 in the supplementary materials the lowest error values for each case are bolded this analysis clearly shows that the performance of the three downscaling approaches varies across rainfall durations and return periods highlighting the fact that none of the considered approaches consistently outperformed the others in addition in certain cases two downscaling approaches provide very similar results therefore choosing only one downscaling method with a slightly better mean or range of relative error during the baseline period may in fact lead to failure in capturing all modes of potential change in extreme rainfalls finally it is apparent that the magnitude of expected error increases considerably under the 100 year return period across all durations this means that the quantile based downscaling approaches are not able to fully reproduce extreme quantiles when the return period goes beyond the length of data record and that extrapolation at the upper tail of the gev distribution is required 5 2 discrepancies in projections of future extreme quantiles as a first step to benchmark the functionality of quantile based downscaling methods future extreme rainfall quantiles across a range of durations and return periods were estimated and compared with the corresponding observed extreme rainfall quantiles during the baseline period fig 6 shows the results of this analysis in terms of the ensembles of relative differences between future and observed local extreme rainfall quantiles projected quantiles were estimated using the three quantile based downscaling methods applied to the 20 bias corrected gcms in the three future time horizons under rcp 4 5 in each panel and for each return period the boxplots for qqd eqm and sim are shown in black green and blue respectively given a particular duration and return period the sign and magnitude of change in future extreme rainfall quantiles varies significantly based on the applied downscaling method as a general observation similar downscaling skills during the historical period do not necessarily result in similar projections during future periods for instance despite the fact that qqd and eqm map the observed extreme rainfall quantiles with similar skill during the baseline period see fig 5 they do not provide similar projections under future conditions in fact the discrepancy between the results of qqd and eqm are significant and the estimated signs of change are inconsistent across various durations and or return periods in contrast while qqd and sim presented different performances in matching the observed extreme rainfall quantiles during the baseline period they provide relatively similar projections under future conditions fig 7 shows similar results for rcp 8 5 when comparing figs 6 and 7 it is observed that the three downscaling methods project the same signs of change in extreme rainfall quantiles under both rcps in almost all durations and return periods nevertheless the average magnitudes of change over the gcms are consistently larger under rcp 8 5 this translates into a greater difference between the three downscaling methods particularly for larger return periods and over the long term future horizon for instance while eqm estimates a 20 decrease for long term future daily extremes at the 100 year return period sim estimates a 20 increase see bottom right panel in fig 7 this results into a 40 expected difference between projections obtained using these two downscaling methods handling the uncertainty related to estimating the sign and magnitude of change is extremely important and will be discussed in more detail in section 6 5 3 divergent functionality of quantile based downscaling methods given the significant discrepancies in the future projections of local rainfall extremes obtained using the three different quantile based downscaling approaches further efforts toward a detailed understanding of the differences in the functionality of quantile based downscaling approaches across a range of durations return periods and projection horizons are required fig 8 summarizes the footprint of downscaling approaches in altering the gridded extremes under the rcp 4 5 and rcp 8 5 scenarios in top and bottom rows respectively in general the three quantile based methods demonstrate their largest effect on altering the model based quantiles at the 100 year return period this requires extrapolation beyond the record length using the gev distribution this extrapolation results in substantial differences in the future daily projections across the three downscaling approaches in addition the figure shows the divergent functionality of downscaling methods in converting the model based daily extreme rainfall quantiles to corresponding local estimates for instance while qqd increases the difference between baseline and future model based extreme rainfall quantiles the eqm dampens this difference this divergent functionality justifies the difference between projections of extreme rainfall quantiles obtained from these two algorithms among the three quantile based schemes sim has the largest impact on altering the model based extreme rainfall quantiles during the downscaling process similarly the percentage of relative change in projections of daily extreme rainfall quantiles were calculated under both rcps 4 5 and 8 5 at the gcm and local scales results are summarized in fig 9 for each downscaling scheme and future horizon in general these results confirm the earlier results presented in fig 8 in which the sign and magnitude of footprints depend on the downscaling method with the largest impact occurring at the 100 year return period in addition stronger footprints are evident in most cases during the 2071 2100 period 5 4 gauging the total uncertainty the results presented in section 5 2 revealed substantial differences in future projections of extreme rainfall quantiles particularly under return periods beyond the observational length in this case 30 years where extrapolation based on gev distribution was required it was also shown in section 5 3 that the uncertainty in future projections corresponded to the divergent functionality of quantile based downscaling approaches that can have significantly different footprint when converting gridded projections of extreme quantiles into corresponding local values one question however remains unanswered how large is this uncertainty compared to the existing uncertainty in assigning historical extreme rainfall quantiles to answer this the ceps of downscaled extreme rainfall quantiles across the nine combinations of durations and return periods were considered and implemented in the benchmarking procedure outlined in section 4 3 fig 10 summarizes the results of this analysis in all panels solid and dashed lines demonstrate expected values and 95 confidence intervals for observed extreme rainfall quantiles respectively boxplots show the range of ceps for different extreme rainfall quantiles obtained under rcps 4 5 and 8 5 green dots display the expected values of ceps obtained by averaging the projections across three downscaling methods and twenty gcms some interesting observations can be seen in this figure most importantly for 1 h and 24 h durations in the 100 year return period the cep has a smaller range compared to the corresponding baseline extreme rainfall quantiles this means that under these conditions the uncertainty in assigning the extreme rainfall quantiles during the baseline period is larger than the uncertainty in future projections based on the ensemble of 120 members i e 3 downscaling approaches 20 climate models 2 rcps these analyses indicate that the uncertainty in projecting the future extreme rainfall quantiles due to combinations of gcms rcps and downscaling approaches was comparable to the uncertainty in assigning the extreme rainfall quantiles during the baseline period manifested through 95 confidence interval 6 discussion results presented in section 5 highlighted the uncertainties in projecting extreme rainfall quantiles using quantile based downscaling approaches due to the divergent functionality of the downscaling procedures in converting the gridded extreme quantiles to corresponding local values in addition it was noted that as a result of the extrapolation made by the gev distribution the uncertainty increases in the 100 year return period although this uncertainty can be substantial the total uncertainty in assigning future extreme rainfall quantiles is comparable to the uncertainty in assigning the extreme rainfall quantiles during the baseline period here we argue that similar to the confidence interval of the gev distribution the uncertainty in the estimation of future extreme rainfall quantiles can be handled and described using the empirical characteristics of ensemble projections below we discuss this in more details and highlight its practical implications for vulnerability assessment of existing systems and for picking a design value for new infrastructures 6 1 implications on vulnerability assessment of existing infrastructure to ensure the reliable operation of existing urban infrastructures under future conditions it is critical to assess the vulnerability of the system designed under historical conditions it should be noted that the values of extreme rainfall quantiles on the idf curves represents the most likely parametric sets of the gev distribution associated with the best fit within the 95 confidence interval as a result they do not include information on the range of potential rainfall intensities associated with a given return period theoretically this range can be described by a risk profile which quantifies the empirical probability distribution functions epdfs of the ensemble members within the historical 95 confidence interval of the gev distribution a similar approach could be taken to quantify the risk profiles associated with future projections of extreme rainfall quantiles based on a set of gcms and downscaling approaches these risk profiles characterize the uncertainty of future extreme rainfall quantiles in light of the climate models rcps and downscaling methods considered when compared with the corresponding risk profile during the baseline period it is possible to estimate the change in the range of extreme rainfall quantiles and their associated probability as an example fig 11 shows the risk profiles for the baseline period base and future extreme rainfall quantiles obtained using different downscaling methods applied to the 20 bias corrected gcm projections for 2041 2070 under rcps 4 5 and 8 5 the results for this time horizon are shown here due to their importance for assessment and development of current urban infrastructure in montreal it is apparent from the figure that the range shape and statistics of the future risk profiles are subject to change compared to the baseline period depending on the downscaling approaches used there could however be substantial uncertainty in the form and magnitude of change in risk profiles in the absence of any further information on the suitability of climate models and or downscaling approaches we suggest that the cep be used to provide a basis for representing the uncertainty in extreme rainfall quantiles and accordingly the associated risk in existing infrastructures 6 2 implications on selecting future design storms the selection of a design storm is based on the type and importance of infrastructure under consideration choosing a smaller value for a design storm can increase the chance of infrastructure failure however selecting a larger design storm can lead to over designing the infrastructure which can ultimately increase the cost of construction operation and maintenance one way to incorporate the ensemble of climate change projections into design practice is to define an acceptable risk level and select a design value from a range of possibilities accordingly if the accepted level of risk is known the empirical characteristics of future extreme rainfall quantiles can provide a basis for selecting the design storm in light of the considered gcms and downscaling approaches used empirical cumulative distribution functions of future extreme rainfall quantiles can be used to estimate extreme rainfall intensities across various exceedance probabilities fig 12 shows this analysis for future extreme rainfall quantiles obtained under rcps 4 5 and 8 5 20 gcms and downscaling approaches for the period of 2041 2070 and compares the future risks with associated historical values based on fitting gev distribution to baseline amps climate change alters the current design storms under certain risk levels however the magnitude and sign of change is strongly dependent on the climate models and or downscaling methodology used given the lack of other information on the suitability of a particular gcm and or downscaling methodology the use of all three downscaling approaches in the form of a cep is suggested to assign future design storms rather than relying on results based on individual downscaling approaches for the case shown it can be argued that rainfall extremes are likely to increase in lower return periods i e 2 and 10 years for the 100 year return period the cep profile stays inside the identified profile for the base furthermore the estimated future design storms with a 100 year return period at the 10 level of risk acceptance are smaller than corresponding historical values in all durations see orange line in the bottom row of fig 12 this highlights the need to reconsider the role of existing uncertainty in estimating baseline extreme rainfall particularly in the context of future design 7 conclusions the aim of this study was to benchmark the functionality and associated uncertainty of three quantile based downscaling methods with a greater goal of providing useful insights for practical applications three downscaling methods namely qqd eqm and sim were used to downscale gridded extreme rainfall quantiles obtained from an ensemble of 40 bias corrected gcm rcp pairs to corresponding local values across a range of durations in montreal quebec results showed that none of the downscaling methods could perfectly represent observed extreme rainfall quantiles under the considered durations and return periods moreover while qqd and eqm reproduced the observed extreme rainfall quantiles with similar performance their estimated changes in future extreme rainfall quantiles differ significantly in contrast qqd and sim methods despite different representations of the observed extreme rainfall quantiles during the baseline period presented similar extreme rainfall projections under future conditions to trace the source of the differences between the three downscaling approaches the impact of each downscaling approach in transferring gridded daily extreme rainfall quantiles to corresponding daily extremes at the local scale was quantified it was found that while downscaling by eqm decreases the difference between baseline and future model based extreme rainfall quantiles qqd and sim slightly and greatly increase this difference respectively this observation explains the divergence between future projections of extreme rainfall quantiles obtained from the three methods results also showed that the estimation of future design storms is sensitive to the applied downscaling method in particular the eqm and sim presented the smallest and largest estimations respectively and their differences could be large relying on only one of these downscaling approaches may result in the under or overestimation of design storms which in practice may have large socio economic and environmental consequences we propose then that the projected extreme rainfall quantiles obtained from an ensemble of gcms and quantile based downscaling methods should be considered rather than relying solely on only one methodology it was shown however that the total uncertainty was comparable to the existing uncertainty in assigning the extreme rainfall quantiles during the baseline period particularly at larger durations and return periods to handle this we propose analyzing the corresponding likelihood information in the form of risk profiles and selecting appropriate design storms from a large ensemble based on an acceptable level of risk using empirical characteristics of ensemble projections it was shown how the magnitude and the probability of occurrence in extreme rainfall quantiles will change for montreal under future conditions based on our analyses the future extreme rainfall quantiles will likely increase at smaller return periods but may decrease under the 100 year storm it should be noted that we do not promote quantile based downscaling approaches over continuous downscaling schemes in fact idf curves and therefore quantile based downscaling procedures can only be suitable for design under a single extreme event flooding however may be due to multiple non extreme events that take place sequentially for consideration of such events continuous downscaling techniques are indeed required there is therefore a need to systematically compare the quantile based and continuous based downscaling approaches to identify the pros and cons of each methodology inevitably resulting in the development of more reliable algorithms that are relevant for the estimation of extreme rainfall quantiles moreover whether quantile based or continuous downscaling is used there is a need to investigate the uncertainty in extreme rainfall quantiles due to fitting a probability distribution function and its estimated parameters this issue is important in particular for the estimation of extreme quantiles at large return periods e g 100 year which require extrapolation using parametric probability distributions as a final remark it is important to note that we focused on extreme rainfall events during may to october this means that our analysis did not capture snowmelt and rain on snow events these events play a key role in the formation of urban flooding and infrastructure failure in cold regions particularly under climate change conditions that may increase the frequency of such events hatami et al 2018 we suggest that consideration of extreme precipitation and melt events during winter periods e g yan et al 2018 also be considered in the construction of idf curves in canadian cities acknowledgement this study is supported by an nserc file no 505755 16 engage grant held by jan adamowski we would like to thank mr alain charron ville de montreal for his continuous support of this project from provision of local scale data to constructive feedbacks on our research findings the authors also thank pablo jaramillo for providing model based daily amps of montreal under baseline and future conditions supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 07 001 appendix supplementary materials image application 1 image application 2 image application 3 
587,a consistent smoothed particle hydrodynamics sph approach is used to simulate the anisotropic dispersion of a solute in porous media consistency demands using large numbers of neighbors with increasing resolution the method is tested against the anisotropic dispersion of a gaussian contaminant plume with irregularly distributed particles the solution for isotropic dispersion converges to second order accuracy when at sufficiently high resolution a large number of neighbors is used within the kernel support for low to moderate anisotropy the convergence rates are close to second order while for large anisotropic dispersion the solutions converge to better than first order for randomly distributed particles the solutions are also better than first order independently of the degree of anisotropy when negative concentrations arise they are several orders of magnitude smaller than those encountered with standard sph and comparable to those obtained with the mwsph scheme of avesani et al the method is also insensitive to particle disorder and achieves an overall accuracy comparable to the mwsph method using a much simpler approach keywords particle method transport advection diffusion anisotropic dispersion porous media 1 introduction solute transport through heterogeneous porous media is observed in a wide variety of natural processes and industrial applications which span the design of oil recovery strategies to the development of packed bed reactors in chemical plants to the establishment of fluid flow potential through construction materials solute transport is also observed in the spread of contaminants in subsurface water lewis and sjostrom 2010 and the transport of nutrients in soil water parkinson et al 2000 in general the migration of solutes in a geological medium is governed by three different mechanisms namely fluid advection molecular diffusion and mechanical or hydrodynamical dispersion advection causes the translation of the solute field with the flow velocity while diffusion is independent of the flow and describes the spread of the solute due to the presence of concentration gradients on the other hand mechanical dispersion is caused by the different flow paths that fluid particles take due to the heterogeneity of the porous medium it is therefore a property characteristic of the porous medium and can be considered to materialize the tortuosity of the particle trails due to the random arrangement and the interconnectivity of the channels constituting the pore space mechanical dispersion is in general assumed to obey fick s law bear 1988 and it is known to increase linearly with increasing fluid pore velocity except at low velocities where molecular diffusion dominates poulsen et al 2008 hunt and skinner 2010 sharma and poulsen 2010 dispersive transport occurs in three directions i e parallel to the main flow longitudinal dispersion and along the other two orthogonal directions perpendicular to the main flow transverse dispersion in general the spreading of the solute is anisotropic with the transverse dispersion being at least an order of magnitude smaller than the longitudinal dispersion bear 1988 solute transport by the simultaneous action of these three mechanisms usually rely on numerical solutions of the classical advection dispersion equation ade coupled to the mass and momentum conservation laws bear 1988 for instance in contaminant hydrology it is often assumed that the transport can be modeled as a flow in a homogeneous domain with some dispersion coefficient designed to accommodate the molecular diffusion and the dispersion of the contaminant due to small scale heterogeneity in mathematical form the dispersion coefficient in anisotropic media is represented by a tensor of second rank bear 1988 this together with the rapid changes in magnitude and direction of the flow velocity in geological formations make the numerical prediction of anisotropic dispersion with traditional finite difference and finite volume methods very difficult such numerical solutions were found to suffer from unphysical oscillations and negative concentrations when dispersion is strongly anisotropic and the principal direction of dispersion deviates significantly from the grid orientation potier 2005 nordbotten and aavatsmark 2005 mlacnik and durlofsky 2006 yuan and sheng 2008 lipnikov et al 2009 only a few studies have assessed the use of particle methods to simulate anisotropic dispersion zimmermann et al 2001 beaudoin et al 2003 herrera et al 2009 herrera and beckie 2013 avesani et al 2015 because of their lagrangian character particle methods are free of numerical dispersion and artificial mixing when simulating flow advection and so they appear to be better suited than traditional grid based methods to simulate solute dilution and mixing in particular zimmermann et al 2001 evaluated the performance of the particle strength exchange pse method degond and mas gallic 1989 to simulate anisotropic dispersion in a homogeneous porous medium under uniform and nonuniform flow conditions finding acceptably accurate results only when a lagrangian distortion re meshing step was employed to correct for the flow field topology leading to clustering or spreading an alternative particle method to the pse scheme was proposed by beaudoin et al 2003 based on the diffusion velocity concept degond and mustieles 1990 however compared to the pse scheme this method gains in robustness and simplicity at the price of losing accuracy on the other hand herrera et al 2009 performed simulations of conservative solute transport in heterogeneous porous media with the aid of standard smoothed particle hydrodynamics sph gingold and monaghan 1977 obtaining results comparable to those from grid based techniques they found that no re meshing procedure is necessary if the second order derivatives are approximated in terms of only first order derivatives of the kernel function as is currently done in most sph applications to reduce the sensitivity to particle disorder cleary and monaghan 1999 more recently herrera and beckie 2013 tested their sph scheme against a benchmark problem of anisotropic dispersive transport and performed direct comparison of their results with an implementation of the pse method and a standard finite volume scheme with all three methods the numerical solutions were seen to exhibit artificial oscillations and negative concentrations when the off diagonal terms of the tensorial dispersion coefficient are nonzero also the solutions exhibited larger errors and slower convergence rates compared to the isotropic case they concluded that the non linear amplification of these oscillations represents a severe drawback for flow simulations where the anisotropic nature of the dispersion tensor must be considered to cope with this difficulty avesani et al 2015 proposed a modified version of the standard sph scheme based on a moving least squares weighted essentially non oscillatory mls weno avesani et al 2014 reconstruction of concentrations referred to as the mwsph method which produced accurate results and a drastic reduction of unphysical oscillations compared to previous sph calculations a different sph scheme based on a new sph approximation for the anisotropic diffusion operator called anisotropic sph approximation for anisotropic diffusion asphad was proposed by tran duc et al 2016 in this case the diffusion operator is first approximated by an integral in a coordinate system in which it is isotropic an inverse transformation is then applied to the integral so that the diffusion operator and the smoothing length become anisotropic for isotropic dispersion asphad matches very well the analytical result while for anisotropic dispersion anisotropy is somehow reduced because of the smoothing effect in sph although asphad conserves very well the principal diffusing directions for anisotropic dispersion the method is rather sensitive to the particle distribution unfortunately no comments were made by the authors on the ability of asphad to suppress negative concentrations for large anisotropic dispersion it follows from the work of herrera and beckie 2013 that the accuracy of dispersive transport simulations with standard sph is highly sensitive to the spatial distribution of particles in general the accuracy declines as the degree of particle disorder increases even for isotropic diffusion dispersion models however the calculations of avesani et al 2015 have also shown that the mwsph method is rather insensitive to the spatial distribution of particles which solves the drawback of standard sph when applied to dispersive transport in the presence of heterogeneous velocity fields however the considerable gain of accuracy of the mwsph method is associated with an increase of the computational cost due to the intermediate steps involved in the mls weno reconstruction and stencils construction at comparable spatial resolution the mwsph scheme demands up to two orders of magnitude more cpu time than standard sph avesani et al 2015 here we propose a consistent sph approach to simulate the anisotropic dispersion of a solute in porous media furthermore the performance of the method is compared with standard sph herrera and beckie 2013 and mwsph avesani et al 2015 under varying degrees of anisotropic dispersion to analyze the occurrence of negative concentrations finally the performance of the method for randomly distributed particles is also investigated by comparing the convergence rate of the solutions with the case of irregularly quasi random particle distributions the method is based on consistency considerations in standard sph sigalotti et al 2016 where the number of neighbors n and the smoothing length h are initially set in terms of the total number of particles n by means of scaling relations that comply with the asymptotic limits n h 0 and n for complete sph consistency as the spatial resolution is increased rasio 2000 zhu et al 2015 the consistency and accuracy of sph improves as the number of neighbors is increased with resolution which is consistent with the expected dependence of the sph particle discretization errors on 1 n monaghan 1992 read et al 2010 sigalotti et al 2019 in particular the error analysis of the sph representations of the continuity and momentum equations performed by read et al 2010 demonstrated that this error contributes with zeroth order terms that would persist when working with a fixed low number of neighbors even though n and h 0 causing a complete loss of particle consistency these findings were corroborated analytically by sigalotti et al 2019 who derived the explicit functional dependence of the error bounds on the sph interpolation parameters namely the smoothing length h and the number of neighbors n within the kernel support for the particle approximation of a function using the poisson summation formula therefore it is of interest to investigate whether the modified sph method with varied number of neighbors will improve the accuracy and convergence rate of diffusion dispersion transport simulations when particle consistency is restored the numerical solutions become insensitive to particle disorder and the estimates of the function and its first derivatives converge essentially at the same rate sigalotti et al 2016 however nothing has been concluded in previous analyses on the consistency of the sph approximations for second order derivatives which are key in the approximation of the diffusion dispersion operator except for their well known reduced sensitivity to particle disorder when they are expressed in terms of first order derivatives of the kernel function cleary and monaghan 1999 2 mathematical formulation non reactive solute transport in porous media is described by the ade which in lagrangian coordinates can be written as 1 d x d t v 2 d c d t d c c v where v v x t is the fluid velocity vector c c x t is the scalar tracer concentration and d is a symmetric tensor of second rank defining the diffusion and dispersion coefficient note that for a uniform velocity field eqs 1 and 2 are not coupled in component form the diffusion and dispersion coefficient is defined by the relation bear 1988 scheidegger 1961 3 d i j α t v d m δ i j α l α t v i v j v where α l is the longitudinal dispersivity α t is the transverse dispersivity d m is the molecular diffusion coefficient and δij is the kronecker delta in general α t α l by at least an order of magnitude so that hydrodynamical dispersion is anisotropic bear 1988 it is clear from relation 3 that the dispersion coefficient depends on the magnitude and direction of the fluid velocity as well as on the ratio α t α l if α t 0 relation 3 becomes 4 d i j d m δ i j α l v i v j v for dispersion along the longitudinal direction only moreover when α t α l α the off diagonal terms of tensor dij vanish and the dispersion is isotropic with 5 d i j α v d m δ i j in two space dimensions 2d the components of tensor dij as defined by relation 3 in cartesian coordinates are 6 d x x α t v d m α l α t v x 2 v d x y d y x α l α t v x v y v d y y α t v d m α l α t v y 2 v with v v x 2 v y 2 1 2 for incompressible flow v 0 and so eqs 1 and 2 are coupled through the dispersion coefficient thus for a time and space varying velocity field the anisotropic dispersion will also be time and space varying 3 sph formulation a sph approximation for eq 2 can be constructed by first looking at the discretization form of the term d c following herrera and beckie 2013 this term satisfies the identity 7 i 1 m j 1 m x i d i j c x j 1 2 i 1 m j 1 m 2 x i x j d i j c c 2 d i j x i x j d i j 2 c x i x j in index notation where m 2 in 2d and m 3 in three dimensions 3d according to this the sph discretization of the diffusion dispersion operator reduces to estimating second order derivatives of a scalar function a general sph approximation for the second order derivative of a continuous function f f x at the position of particle a say x a has the form 8 2 f x i x j a b 1 n m b ρ b f a f b γ x i a b x j a b x a b 2 δ i j 1 x a b 2 x a b a w a b where the summation is over n neighbors within the kernel support x a b x a x b x a b 2 x a b x a b w a b w x a b h is the kernel function γ 4 in 2d and γ 5 in 3d a formal derivation of the approximation 8 is given by yildiz et al 2009 using the sph identity 25 of español and revenga 2003 for i j we find that 9 i 1 m j 1 m γ x i a b x j a b x a b 2 δ i j i 1 m γ x i a b x i a b x a b 2 δ i i 2 and so the double sum of eq 8 over i and j reduces to the sph laplacian approximation which was found by morris et al 1997 to be less sensitive to particle disorder than the more direct laplacian form written in terms of second order derivatives of the kernel function substitution of eq 8 into eq 7 with f dijc f dij and f c leads after some algebraic steps to the sph representation of the dispersive term which at the position of particle a reads as follows 10 d c a 1 2 b 1 n m b ρ a b d a b c a c b x a b 2 x a b a w a b where ρ a b ρ a ρ b 2 and 11 d a b i 1 m j 1 m 4 d i j a d i j b d i j a d i j b γ x i a b x j a b x a b 2 δ i j in the final steps of the above derivation the term d i j a d i j b appearing in the definition of d a b has been replaced by the ratio 4 d i j a d i j b d i j a d i j b of the geometrical over the arithmetic means of tensor component dij between particles a and b in order to guarantee continuity of the dispersive term when dij is discontinuous cleary and monaghan 1999 finally the convective term c v on the right hand side of eq 2 is written in sph form using the standard approximation 12 c v a c a b 1 n m b ρ a b i 1 m v x i b v x i a w a b x i a for spreading of the solute in an external spatially uniform velocity field this term vanishes identically and eq 2 reduces to a pure diffusion dispersion equation 3 1 sph consistency the concept of consistency is related to how closely the discrete equations approximate the exact differential equations in particular if a sph approximation can reproduce an mth order polynomial exactly we say that the sph approximation has cm consistency or m 1 th order accuracy the standard sph method is known to have c 0 and c 1 kernel consistency for conventional kernel functions however satisfaction of these kernel consistency conditions does not imply that c 0 and c 1 particle consistency is also satisfied by the corresponding sph approximation sigalotti et al 2016 zhu et al 2015 liu and liu 2006 this discrepancy between the kernel and particle approximations is what we mean in sph by loss of the particle consistency it is well known that standard sph does not even have c 0 particle consistency due to violation of the discrete normalization condition of the kernel sigalotti et al 2016 rasio 2000 read et al 2010 sigalotti et al 2019 liu and liu 2006 the loss of consistency in the particle approximation procedure is usually associated with a the truncation of the kernel function at and near a physical domain boundary b the particle disorder and c the use of temporally and spatially varying smoothing lengths liu and liu 2006 it was not until recently that zhu et al 2015 identified another source of particle inconsistency associated with the fixed low number of neighbors within the compact support of the kernel function it has been common practice in sph simulations to use low numbers of neighbors 60 independently of the total number of particles they showed that c 0 particle consistency can be achieved only when n is sufficiently large so that the zeroth order error term carried by the sph discretization declines read et al 2010 sigalotti et al 2019 making the sph sum approximation to approach the continuous kernel approximation limit for n 1 zhu et al 2015 parameterized this error as 1 nγ where γ 0 5 for randomly distributed particles and γ 1 for low discrepancy sequences of particles i e quasi random distributions as is more appropriate for sph based on a balance between the leading error of the kernel approximation h 2 as is appropriate for most commonly used kernels and the 1 nγ error of the particle approximation zhu et al 2015 derived the scaling relations n n 1 3 β and h n 1 β for β 5 7 which satisfy the joint limit n h 0 and n with n n 0 rasio 2000 for full particle consistency when n is increased a recent analysis has demonstrated that with the use of the above scaling relations c 0 consistency is restored for both the particle estimates of a function and its first order derivatives with the numerical solution becoming insensitive to particle disorder and the effects of domain boundaries sigalotti et al 2016 in passing we note that if c 0 particle consistency is achieved c 1 particle consistency is also guaranteed because of the kernel symmetry an intermediate value of β 6 is appropriate when the smoothing is performed on pseudo randomly distributed particles for the simulations of this paper we then allow h to vary with n as h n 1 6 with this choice a family of curves can be obtained for the dependence of n on n out of the set of possible curves we have chosen the scaling relations n 2 81n 0 675 and h 1 29 n 0 247 to set the initial values of n and h in terms of n these exponents are slightly larger than the reference values of 0 5 and 1 3 when β 6 with this choice large numbers of neighbors can be accommodated for a given n while keeping reasonably large values of h however other choices with exponents closer to or even slightly smaller than the reference values produced convergence rates of the numerical solutions that remained essentially the same sigalotti et al 2016 3 2 the kernel function most commonly used kernels cannot support arbitrarily large number of neighbors because for finite sized smoothing lengths they suffer from the so called pairing instability which sets in when particles start to form pairs dehnen and aly 2012 in some extreme cases the paired particles become so close to each other that they emerge as effectively one particle leading to a loss of resolution while keeping the computational cost the same a promising family of kernel functions which are free from the pairing instability and therefore show much better convergence properties than traditional kernels are the wendland functions wendland 1995 dehnen and aly 2012 here we adopt the wendland c4 function defined as dehnen and aly 2012 13 w q h b 1 q 6 1 6 q 35 3 q 2 if q 1 and 0 otherwise where q x x h b 9 π h 2 in 2d and b 495 32 π h 3 in 3d a kernel function of this type has a strictly positive fourier transform and no inflection points these are desirable properties for convergence studies since the neighbor number can be increased without limit in addition wendland functions are reluctant to allow particle motion on sub resolution scales thereby maintaining very regular particle distributions even in highly dynamical tests rosswog 2015 this is a further important concern for accuracy and convergence of the sph method 3 3 time step control and time integration eqs 1 and 2 are solved explicitly by means of a predictor corrector leapfrog integration scheme the time step control for numerical stability is set by the condition herrera and beckie 2013 14 δ t c t h 2 i 1 m d i i so that the minimum δt is taken over all particles herrera and beckie 2013 found through numerical experiments that their sph solutions were stable if c t 0 1 in order to provide a comparison with their solutions and evaluate the effects of improving the consistency of sph when using arbitrarily large numbers of neighbors we use the same value for the constant c t in the predictor step particle positions and concentrations are evolved to an intermediate time t l 1 2 through the sequence 15 x a l 1 2 x a l 1 2 δ t v a l c a l 1 2 c a l 1 2 δ t d c d t a l with these updates the time rate of change of the concentration is calculated at the intermediate time level for use in the corrector step where quantities are advanced to the new time t l 1 according to 16 x a l 1 x a l δ t v a l 1 2 c a l 1 c a l δ t d c d t a l 1 2 for the simulation cases of this paper the flow velocity is given as an external constant input parameter and therefore v a l v a l 1 2 in the more general case of a temporally and spatially varying velocity field the continuity and momentum equations must be solved coupled to eqs 1 and 2 which can be easily accommodated in the above time integrator 4 numerical simulations in this section we analyze the accuracy of the method for 2d simulations of the anisotropic dispersion of a gaussian pollutant source the method is applied to a benchmark diffusion dispersion problem with analytical solution zimmermann et al 2001 beaudoin et al 2003 herrera and beckie 2013 in particular we test the performance of our sph scheme by comparing the numerical results with those obtained by avesani et al 2015 with their mwsph method 4 1 model setup and initial conditions the test consists of releasing instantaneously an initial gaussian concentration of solute mass δ m 10 4 kg given by 17 c x t 0 c 0 exp x x 0 2 y y 0 2 2 w 2 in a two dimensional unbounded domain with a temporally and spatially constant velocity field where c 0 0 32 kg m 3 is the maximum initial concentration w 44 m is the width of the gaussian plume at t 0 and x 0 y 0 are the coordinates of the plume center at t 0 the unbounded domain is modeled by a square of side length l 2000 m and by implementing periodic boundary conditions at its borders the contaminant is injected at the center of the square so that x 0 y 0 1000 m because of the constant flow velocity the advective term on the right hand side of eq 2 is identically zero and so the transport process will depend on the flow only through the relation between the dispersion coefficient and the flow velocity the analytical solution for the solute concentration at any time t 0 is given by 18 c x t c 0 w 2 c 4 exp x x 0 2 a 1 y y 0 2 a 2 4 x x 0 y y 0 a 3 8 t 2 c 2 4 w 2 t c 3 2 w 2 where 19 a 1 2 t d y y w 2 a 2 2 t d x x w 2 a 3 t d x y c 2 d x x d y y d x y 2 c 3 d x x d y y c 4 4 t 2 c 2 2 t w 2 c 3 w 4 1 2 the longitudinal dispersivity is α l 10 m and the constant flow velocity is taken to be v v x 2 v y 2 1 m day 1 1 16 10 5 m s 1 in addition the molecular diffusivity d m in eq 3 is set to zero and so the dispersion tensor will only depend on the longitudinal and transverse dispersivities and on the magnitude and direction of the flow velocity 4 2 simulation cases one set of calculations is performed with irregularly distributed particles following a low discrepancy i e quasi random 1 1 a quasi random sequence corresponds to a set of irregularly distributed points which is not random in a mathematical sense in contrast to a truly random distribution where the points clump together in a quasi random sequence the points are scattered around without a clear pattern and never clumped together that is they are better spread out filling the space more efficiently these sequences are defined as having low discrepancy that is the discrepancy between the number of points that actually fall in a volume and the number of points that we would expect to fall in the same volume is small sequence we will study the convergence rate of our modified sph scheme for four different dispersivity ratios α t α l and two different flow orientations with increasing number of neighbors and decreasing smoothing lengths as the resolution is increased according to the scaling relations defined in section 3 1 table 1 lists the spatial resolution parameters the first four columns list the mean particle separation distance normalized to l the total number of sph particles the corresponding number of neighbors and the smoothing lengths normalized to l respectively while the fifth column gives the cpu time in seconds of the code on an intel r xeon e5 2690 v3 cpu clockspeed 2 6 ghz processor with 12 cores in order to provide direct comparison with the cases of irregularly distributed particles a second set of calculations is carried out with randomly distributed particles examples of the initial irregular and random particle distributions are shown in figs 1 and 2 respectively for the lowest resolution case n 625 the same patterns are employed for all higher resolution runs of table 1 in figs 1 and 2 the coordinates of particle positions are normalized to the side length l of the square domain we consider values of α t α l 0 001 0 01 0 1 and 1 0 where α t α l 1 0 corresponds to isotropic dispersion and α t α l 0 001 corresponds to very large anisotropic dispersion in addition we choose two different orientations for the flow velocity i e θ 0 and θ 45 measured with respect to the x axis with these choices v x v and v y 0 for θ 0 and v x v y 2 v 2 for θ 45 in order to provide comparison with standard sph and mwsph simulations the range of values of these parameters are the same employed by herrera and beckie 2013 and avesani et al 2015 eqs 1 and 2 are solved in normalized form using the following prescriptions t t 300 days c x t c x t c 0 w w l α l α l l α t α t l x 0 x 0 l y 0 y 0 l v v v 0 dxx dxx v 0 l dxy dxy v 0 l and dyy dyy v 0 l where v 0 20 3 m days 1 this is the mean velocity that a particle should have to cross the side length l 2000 m of the computational domain in 300 days for each pair of values α t α l θ and the eleven resolutions of table 1 the convergence study for the case of irregularly distributed particles amounts to a total number of 88 independent runs calculations with randomly distributed particles were performed only for θ 0 and total number of particles varying from n 10000 to 1 000 000 corresponding to 28 additional runs for α t α l 0 001 0 01 0 1 and 1 0 in all cases the simulations were terminated after 300 days i e normalized time t 1 the models were run using a modified version of the fully parallel open source code dualsphysics for diffusive and dispersive transport problems which relies on sph theory for solving the fluid dynamics equations gómez gesteira et al 2012 crespo et al 2015 5 results 5 1 isotropic dispersion we first consider the case of isotropic dispersion when α t α l 1 0 fig 3 shows the difference of the maximum concentration values between the analytical and numerical solutions normalized to the maximum initial concentration c 0 as a function of time for the case of irregularly distributed particles the curves belong to different resolution runs for the same model and the symbols on each curve display the data at discrete intervals of about 30 days as the resolution is increased the difference in the maximum concentrations decreases at t 300 days the relative errors drop from 18 when n 22500 and n 2435 to 0 2 when n 562500 and n 21382 reaching values lower than 0 04 when the resolution is increased to n 1 000 000 and n 31529 the maximum relative errors are shifted to earlier times as the resolution is improved with values ranging from 6 to 2 2 when n increases from 21 382 to 31529 hence standard sph is very sensitive to the choice of n and n and therefore to the smoothing length since larger values of n correspond to smaller values of h the trends shown in fig 3 are similar to those reported by avesani et al 2015 in their fig 5 where the difference is seen to increase from the beginning reaching a maximum value much earlier than predicted here and then decreasing to an asymptotic value at later times as the contaminant concentration smooths out however almost identical curves to those shown in fig 3 for n 21382 and n 31529 are obtained when these models are recalculated with perfectly uniform particle distributions meaning that the numerical solution becomes insensitive to the degree of particle disorder when the zeroth order discretization errors are reduced for large values of n similar trends to fig 3 are also obtained for the isotropic dispersion using randomly distributed particles as the number of neighbors is increased the size of the smoothing length is decreased and the zeroth order errors carried by the particle discretization decay as 1 n making the sph approximation to become essentially insensitive to particle disorder sigalotti et al 2016 when working with finite h consistency demands that n while accuracy demands that n and h 0 in order to have convergent results in the limit n the choice of scaling relations slightly different to n 2 81n 0 675 and h 1 29 n 0 247 with exponents closer to 0 5 in the former case and to 0 33 in the latter case will result in correspondingly smaller values of n and h at given n thus improving the accuracy in terms of h and reducing the computational cost since less neighbors will fill the kernel support for the runs with irregularly distributed particles fig 4 compares the concentration profiles for the isotropic case at different resolutions with the analytical solution solid line at t 300 days when θ 0 left and θ 45 right in all cases the profiles are shown along a section forming an angle of 0 with respect to the x axis independently of the flow orientation the numerical solution matches the analytical one everywhere when n 562500 and n 21382 with root mean square errors rmses that are less than 2 7 10 4 see section 5 3 as the resolution decreases the matching deteriorates particularly around the peak of the distribution where the distance between the maximum analytical and numerical concentrations increases and the distribution becomes relatively broader very similar profiles as shown in the left plot of fig 4 are also obtained for θ 0 with randomly distributed particles the concentration field for α t α l 1 0 with n 1 000 000 n 31529 and θ 0 is shown in fig 5 as compared to the analytical solution at t 300 days the shape and structure of the contaminant plume are almost identical to the analytical solution and independent of the flow orientation for the isotropic runs including those with coarser resolutions the numerical solution is always free from spurious oscillations that cause negative values of the concentration 5 2 anisotropic dispersion we now explore the performance of the method when the dispersion is anisotropic i e when the dispersivity ratio α t α l 1 in particular figs 6 and 7 show the temporal variation of the difference of the maximum concentration values between the numerical and analytical solutions at various resolutions for the highly anisotropic cases α t α l 0 01 and 0 001 respectively in both figures the results are shown for the case of irregularly distributed particles while similar trends are obtained for the case of randomly distributed particles the differences are always positive independently of the resolution meaning that sph overestimates the maximum concentration for the highest resolution run i e for n 1 000 000 and n 31529 the maximum relative error at earlier times is smaller than 1 for both dispersivities which is comparable with those reported by avesani et al 2015 with their mwsph m3 and mwsph m4 models for irregularly distributed particles at t 300 days the relative errors have decayed to less than 0 12 regardless of the value of α t α l meaning that good convergence is achieved by sph at such resolution independently of the dispersivity ratio this behavior can be explained as follows the combined kernel and particle discretization errors in sph go as sigalotti et al 2019 20 e a 0 n a 1 h n a 2 n a 2 k h 2 where the term proportional to a 2 k is the contribution from the continuous kernel approximation while the remaining zeroth first and second order terms are the contributions from the particle discretization for large n the error is dominated by the term a 2 k h 2 while for small h the main source of error is given by the zeroth order term a 0 n this term contributes with an irreducible error even when n and h 0 thus for finite values of h consistency is achieved only for sufficiently large values of n while accuracy results from the additional requirement that h be sufficiently small therefore smaller concentration differences could be obtained by reducing further the size of the kernel support and increasing the number of neighbors hence as n the second order error is governed by the kernel approximation and the size of h fig 8 compares the concentration profiles at different spatial resolutions for the case of irregularly distributed particles with the analytical solution solid line at t 300 days for anisotropic dispersion with α t α l 0 01 and flow orientation θ 0 top left and θ 45 top right and with α t α l 0 001 for θ 0 bottom left and θ 45 bottom right as in fig 4 all profiles are shown along a section inclined of 0 with respect to the x axis for both flow orientations and dispersion ratios the numerical solution is seen to match the analytical profile for n 1 000 000 and n 31529 as the resolution is lowered the numerical profiles become broader and their separation from the analytical solution increases around the peak of the distribution as shown in section 5 3 the worst rmse between the numerical and analytical distributions at the highest resolution is 6 5 10 4 and occurs for α t α l 0 001 and θ 45 for comparison when θ 0 the deviation from the analytical profile is slightly smaller 5 0 10 4 fig 9 shows similar plots for the case of randomly distributed particles and flow orientation θ 0 when α t α l 0 01 left and α t α l 0 001 right at the highest resolution the rmse errors around the peak of the distribution are 4 86 10 3 for α t α l 0 01 and 4 91 10 3 for α t α l 0 001 which are about an order of magnitude higher than for the corresponding cases in fig 8 this is consistent with the findings of zhu et al 2015 who showed numerically that at comparable spatial resolution the sph discretization error goes as 1 n for low discrepancy quasi random sequences of particles as given by eq 20 while it decays more slowly as 1 n 0 5 for randomly distributed particles negative concentrations are recurrently found in anisotropic transport models not only in sph simulations but also with other particle methods and traditional mesh based techniques herrera and beckie 2013 when the off diagonal components of the dispersion tensor are nonzero the numerical solutions exhibit artificial oscillations which amplify nonlinearly such oscillations induce negative values of the concentration which represent serious limitations for the correct prediction of anisotropic solute dispersion according to previous sph simulations the situation worsens when the particles are irregularly distributed for example for moderately large dispersivities α t α l 0 01 herrera and beckie 2013 reported maximum negative values of the concentration of 1 7 10 2 kg m 3 5 3 10 2 c 0 for a quasi random distribution although our simulations always predict positive concentrations for the isotropic scenario independently on the flow orientation the same is not true for the anisotropic case fig 10 shows the magnitude of the maximum negative values as a function of the total number of particles for all three dispersion ratios when the particles are irregularly arranged see fig 1 the results exhibit a negligible dependence on the flow direction as it is observed by comparing the left θ 0 and right θ 45 plots as expected the smallest magnitudes at all resolutions occur for α t α l 0 1 in this case the magnitude of the negative concentrations range from 4 9 10 7 c 0 for n 10000 to 3 0 10 6 c 0 for n 1 000 000 when the dispersivity is increased to α t α l 0 01 and 0 001 the magnitude of the maximum negative concentrations increases by a factor of 2 ranging from 8 1 10 7 c 0 for n 10000 to 5 7 10 6 c 0 for n 1 000 000 when α t α l 0 001 however there is no much difference in the negative concentration values when increasing the dispersivity from 0 01 to 0 001 similar trends are shown in fig 11 for the case of randomly distributed particles when the flow orientation is θ 0 the magnitudes of the maximum negative concentrations at all resolutions are seen to slightly increase compared to the case of irregularly distributed particles left plot of fig 10 with values ranging from 6 6 10 7 c 0 for n 10000 to 3 2 10 6 c 0 for n 1 000 000 when α t α l 0 1 for α t α l 0 001 these values range from 1 04 10 6 c 0 for n 10000 to 6 0 10 6 c 0 for n 1 000 000 in all cases the magnitude of the negative concentrations increases with increasing resolution which is not surprising because at coarser resolution more numerical diffusion is added which causes more damping of the unphysical oscillations however at the highest resolutions the rate of increase of the magnitude of the negative concentrations slows down and so it appears that at even higher resolutions these magnitudes will stabilize toward a constant value compared to herrera and beckie 2013 the present sph scheme produces fewer negative concentrations than standard sph with maximum negative concentrations that are from 3 to 4 orders of magnitudes lower however avesani et al 2015 reported a limit to the absolute value of negative concentrations less than about 10 7 c 0 with their mwsph scheme in their case concentrations and concentration gradients are computed at the midpoint of the segment connecting particle pairs using local high order reconstruction at the particle positions given the concentrations of the surrounding particles such non oscillatory weno reconstruction technique on moving points has proved to be efficient in drastically reducing the amplitude of the unphysical oscillations the top panels of fig 12 display the concentration fields with irregularly distributed particles at t 300 days for the anisotropic dispersion runs with α t α l 0 1 and 0 01 when θ 45 using n 1 000 000 and n 31529 as compared to the analytical solution the bottom panels show the case when α t α l 0 001 for θ 0 left and 45 right fig 13 displays the same concentration fields with randomly distributed particles for α t α l 0 01 and 0 001 when θ 0 using n 1 000 000 and n 31529 at this resolution all solutions exhibit negative concentrations represented by the white bands along the transversal direction on both sides of the plume elongation the white bands are oriented parallel to the flow direction and always appear in the tail of the distributions where the concentration decays asymptotically to zero the elliptic deformation and structure of the contaminant plume due to the anisotropic dispersion is remarkably similar to the analytical solution in all cases compared to standard sph herrera and beckie 2013 the region occupied by the negative concentrations are now distributed over smaller portions of the computational domain and their widths are comparable to those from the accurate mwsph results reported by avesani et al 2015 the gain in terms of accuracy of the mwsph method is associated to an increase of the computational cost in their table 2 avesani et al 2015 give the cpu time of their serial code for particles distributed uniformly within the domain on an intel r core tm i7 2640m cpu 2 80 ghz single processor the cpu times in seconds for all resolutions with the present parallelized sph code are given in the last column of table 1 however a comparison of the computational cost between the mwsph code and the present scheme can only be done in an approximate sense for instance our parallelized code on an intel r xeon e5 2690 v3 cpu with clockspeed 2 6 ghz and 12 cores will run about 100 times faster than a serial version on a single intel r core tm i7 2640m processor therefore an inspection of table 1 and their table 2 shows that for similar resolutions the computational demand of both schemes is comparable however one advantage of the present sph scheme is that it gives comparable accuracy to the mwsph method with a much simpler approach for the isotropic case herrera and beckie 2013 found that the use of higher order kernels does not provide a significant improvement of the numerical solution when the particles are regularly distributed in contrast the use of higher order cutoff functions in the pse method results in smaller errors with maximum magnitudes of the negative concentration values that range from less than 1 for second order cutoff functions to less than 0 0003 of the maximum concentration for a fourth order cutoff function zimmermann et al 2001 however for the anisotropic case it was found that the use of higher order sph kernels has only a limited impact on removing the unphysical oscillations in the transverse direction herrera and beckie 2013 5 3 sph errors and accuracy we measure the accuracy of the numerical solutions by means of a root mean square error rmse 21 rmse c 1 n a 1 n c a anal c a sph 2 which is closely related to the l 2 norm error since the sph errors have a normal rather than a uniform distribution the rmse will provide a better representation of the error distribution than other statistical metrics chai and draxler 2014 in addition the rmse consists of squaring the magnitude of the errors before they are averaged and so it gives a relatively higher weight to errors with large absolute values for the case of irregularly distributed particles fig 14 shows the rmse of the numerical concentrations normalized to the initial maximum concentration as a function of l δx for θ 0 left and θ 45 right respectively for the isotropic models α t α l 1 0 the error declines as δx 2 02 for θ 0 and as δx 1 94 for θ 45 for anisotropic dispersion the convergence becomes slower than second order with δx 1 76 for α t α l 0 1 to δx 1 30 for the more extreme case when α t α l 0 001 and θ 0 similar convergence rates are obtained for θ 45 with the rmses varying from δx 1 71 for α t α l 0 1 to δx 1 29 for α t α l 0 001 thus at relatively small particle spacing i e large values of l δx the rate of convergence is close to second order for isotropic dispersion meaning that c 1 particle consistency is achieved when n 562500 and n 21382 for moderately low dispersivities α t α l 0 1 the convergence rate is also close to second order while for larger dispersivities α t α l 0 01 the convergence rates slow down though they are always better than first order even in the most extreme case when α t α l 0 001 the approximate quadratic convergence for α t α l 0 1 is reassuring since this dispersion ratio is in line with what is indeed observed experimentally bear 1988 for comparison fig 15 shows the corresponding rmse of the numerical concentrations for the case when the particles are randomly distributed and θ 0 in this case the error declines as δx 1 20 for both the isotropic and anisotropic dispersion while this error is always better than first order it is also independent of the dispersivity fig 16 shows the dependence of the relative difference between the maximum numerical and analytical concentrations for the case of irregularly distributed particles when θ 0 left and θ 45 right the results are shown for all resolutions and dispersion ratios in all cases the relative difference decreases as the resolution is increased good convergence to the analytical solution is always achieved for n 562500 and n 21382 independently of the flow orientation and dispersion ratio as the resolution is further increased to n 1 000 000 and n 31529 the relative differences tend to zero with c sph c anal 1 9 10 4 c 0 for α t α l 1 0 7 2 10 4 c 0 for α t α l 0 1 8 2 10 4 c 0 for α t α l 0 01 and 8 0 10 4 c 0 for α t α l 0 001 convergence is also achieved for all dispersivities when the particles are randomly distributed as shown in fig 17 for θ 0 and n 1 000 000 with n 31529 neighbors at this resolution c sph c anal 3 2 10 3 c 0 for α t α l 0 1 5 7 10 3 c 0 for α t α l 0 01 and 6 0 10 3 c 0 in good agreement with the result that sph converges more slowly for randomly distributed particles than for low discrepancy sequences zhu et al 2015 for large anisotropic dispersion the use of large numbers of neighbors results in lower errors and improved convergence rates as compared to standard sph this can be better seen from fig 18 where the rmses are plotted versus the dispersion ratio α t α l for all models when θ 0 an almost identical plot is obtained when the flow orientation is changed to θ 45 a result which is consistent with the findings of previous dispersion simulations using equispaced particles when passing from n 562500 and n 21382 to n 1 000 000 and n 31529 the rmse is seen to decay more rapidly for isotropic than for anisotropic dispersion while at low resolution the errors are almost constant for the range of dispersivity ratios considered in the light of these results a reduction of the zeroth order discretization errors that appear when passing from the continuous kernel to the particle approximation is enough to guarantee convergence for sph simulations of anisotropic dispersion preliminary tests have shown that using smaller time steps and different scalings leading to smaller values of h while maintaining large numbers of neighbors have little effect on improving the convergence rate for large anisotropic dispersion on the other hand restoring consistency makes sph to become insensitive to particle disorder sigalotti et al 2016 this is true because for large numbers of neighbors within the kernel support the sph interpolation errors decrease independently of how the particles are distributed within the kernel support moreover compared to more traditional kernels the wendland functions prevent the growth of particle disorder as they do not allow particle motion at sub resolution scales that is at scales smaller than the smoothing length rosswog 2015 eq 2 is the classical fick s second law describing the diffusion and dispersion of a substance and results from combining the continuity equation for the concentration and fick s first law for the flux it is well known that due to its parabolic character this equation issues an infinite velocity of propagation which is clearly unphysical compte and metzler 1997 that is according to its analytical solution 18 even for very small times there will always exist a finite amount of the diffusing and dispersing contaminant at large distances from the site where it was initially injected implying an infinitely fast propagation therefore any small numerical oscillation at large distances from the contaminant may give rise to negative concentrations in order to cope with this difficulty r cattaneo 1948 proposed a modification by adding an ad hoc small term of the form τd 2 c dt 2 to the classical fick s first law which converts eq 2 into the more general form 22 τ d 2 c d t 2 d c d t d c which is of a damped wave equation type known as the telegrapher or cattaneo s equation while fick s first law implies that the flux adjusts instantaneously to the gradient of the concentration giving rise to an unrealistically infinitely fast spreading of local disturbances the term τd 2 c dt 2 turns the ade into a hyperbolic equation where now the flux relaxes with some characteristic time constant τ as a consequence the propagation velocity along the longitudinal and transverse directions namely v p i d i i τ 1 2 remains finite this model has been successfully used to account for anomalous dispersion in amorphous materials scher and montroll 1975 among many other applications that have been reported in the literature until recent years a non local theory of transport proves necessary in materials with microscale inhomogeneities such as the porous media whenever the mean residence time of a tracer in the material is comparable to the correlation time which is the time required for the tracer to sample the microstructures koch and brady 1987 examples of such systems involves the initial dispersion of a pollutant in the atmosphere in rivers and in ground water flows brady and koch 1988 in the singular limit τ 0 v p i and so eq 22 reduces to eq 2 however in passing we remind that hyperbolic equations may not preserve strict positivity that is even though c x t 0 the numerical solution of eq 22 may in general suffer from negative values mendez et al 2010 in the framework of a consistent sph scheme the problem then reduces to investigate under which conditions either eq 2 or eq 22 can be used to model anomalous transport for large anisotropy ratios while keeping the solution free from the numerical instabilities that lead to unphysical negative concentrations 6 conclusions in this paper we have proposed a consistent smoothed particle hydrodynamics sph approach to simulate the anisotropic dispersion of a solute in porous media zeroth and first order consistency of the sph scheme is achieved by setting the number of neighbors within the kernel support n and the size of the smoothing length h in terms of the total number of particles n by means of power law relations of the form n n 1 3 β and h n 1 β for β 5 7 zhu et al 2015 which satisfy the asymptotic limits n h 0 and n with n n 0 for full particle consistency as n is increased rasio 2000 the consistency and accuracy of sph improves as the number of neighbors is increased and the size of the smoothing length is decreased with resolution which is consistent with the expected dependence of the sph discretization errors on 1 n monaghan 1992 read et al 2010 sigalotti et al 2019 these errors contribute with zeroth order terms that would persist when working with small numbers of neighbors even though n and h 0 leading to complete loss of consistency read et al 2010 a wendland c4 function is employed for the kernel interpolation which can support large numbers of particles without a pairing instability dehnen and aly 2012 we test the performance of the consistent sph scheme against the analytical solution of a two dimensional benchmark test for the isotropic and anisotropic dispersion of a gaussian plume for irregularly and randomly distributed particles and compare with previous sph simulations for the same test problem herrera and beckie 2013 avesani et al 2015 as the number of particles and neighbors within the kernel support are increased the distance between the numerical and analytical concentration solutions is reduced both solutions are seen to match independently of the flow orientation and dispersivity ratio when n 1 000 000 and n 31590 suggesting that these values may represent a lower limit for convergence of sph for anisotropic transport the numerical solutions exhibit convergence rates and magnitudes of the negative concentrations comparable to those reported by avesani et al 2015 with their mwsph method based on a moving least squares weighted essentially non oscillatory mls weno reconstruction of concentrations which is at best one of the most accurate particle methods for approximating anisotropic dispersion however an advantage of the present method compared to mwsph is its simplicity any user working with standard sph can easily modify his her code to improve convergence and accuracy on dispersion problems this aspect is of significance for most practical applications where prediction of local scale dispersion on plume movement mixing and dilution are important the same is true for applications of solute dispersion in multiphase flows and reactive transport up to the level of resolution tried here the solutions are not free from unphysical oscillations that induce negative values of the concentrations however the use of large numbers of neighbors coupled to small kernel supports results in negative concentration values that are several orders of magnitude lower than predicted by standard sph simulations herrera and beckie 2013 and comparable to those from accurate mwsph simulations avesani et al 2015 evidently restoring approximate first order consistency for the particle approximation is not enough to ensure a full positive solution for anisotropic dispersion the apparently irreducible problem associated with the unphysical oscillations for highly anisotropic dispersion must not be necessarily attributed to intrinsic deficiencies of the numerical method but rather to the parabolic character of the classical fick s second law which issues an infinite velocity of propagation giving rise to an unrealistically infinitely fast spreading of local disturbances at large distances from the site where they were initially injected acknowledgements we thank the anonymous reviewers for raising a number of suggestions that have improved the content of the manuscript the calculations of this paper were performed using the computational facilities of the abacus centro de matemática aplicada y cómputo de alto rendimiento of cinvestav we acknowledge funding from the european union s horizon 2020 programme under the enerxico project grant agreement no 828947 and under the mexican conacyt sener hidrocarburos grant agreement no b s 69926 one of us c e a r acknowledges finantial support from the conacyt sener energy sustainability strategic project no 212602 aztlan platform l di g s acknowledges support from the universidad autónoma metropolitana azcapotzalco uam a through internal funds the source code is available upon request by writing to the email address iqcarlosug gmail com 
587,a consistent smoothed particle hydrodynamics sph approach is used to simulate the anisotropic dispersion of a solute in porous media consistency demands using large numbers of neighbors with increasing resolution the method is tested against the anisotropic dispersion of a gaussian contaminant plume with irregularly distributed particles the solution for isotropic dispersion converges to second order accuracy when at sufficiently high resolution a large number of neighbors is used within the kernel support for low to moderate anisotropy the convergence rates are close to second order while for large anisotropic dispersion the solutions converge to better than first order for randomly distributed particles the solutions are also better than first order independently of the degree of anisotropy when negative concentrations arise they are several orders of magnitude smaller than those encountered with standard sph and comparable to those obtained with the mwsph scheme of avesani et al the method is also insensitive to particle disorder and achieves an overall accuracy comparable to the mwsph method using a much simpler approach keywords particle method transport advection diffusion anisotropic dispersion porous media 1 introduction solute transport through heterogeneous porous media is observed in a wide variety of natural processes and industrial applications which span the design of oil recovery strategies to the development of packed bed reactors in chemical plants to the establishment of fluid flow potential through construction materials solute transport is also observed in the spread of contaminants in subsurface water lewis and sjostrom 2010 and the transport of nutrients in soil water parkinson et al 2000 in general the migration of solutes in a geological medium is governed by three different mechanisms namely fluid advection molecular diffusion and mechanical or hydrodynamical dispersion advection causes the translation of the solute field with the flow velocity while diffusion is independent of the flow and describes the spread of the solute due to the presence of concentration gradients on the other hand mechanical dispersion is caused by the different flow paths that fluid particles take due to the heterogeneity of the porous medium it is therefore a property characteristic of the porous medium and can be considered to materialize the tortuosity of the particle trails due to the random arrangement and the interconnectivity of the channels constituting the pore space mechanical dispersion is in general assumed to obey fick s law bear 1988 and it is known to increase linearly with increasing fluid pore velocity except at low velocities where molecular diffusion dominates poulsen et al 2008 hunt and skinner 2010 sharma and poulsen 2010 dispersive transport occurs in three directions i e parallel to the main flow longitudinal dispersion and along the other two orthogonal directions perpendicular to the main flow transverse dispersion in general the spreading of the solute is anisotropic with the transverse dispersion being at least an order of magnitude smaller than the longitudinal dispersion bear 1988 solute transport by the simultaneous action of these three mechanisms usually rely on numerical solutions of the classical advection dispersion equation ade coupled to the mass and momentum conservation laws bear 1988 for instance in contaminant hydrology it is often assumed that the transport can be modeled as a flow in a homogeneous domain with some dispersion coefficient designed to accommodate the molecular diffusion and the dispersion of the contaminant due to small scale heterogeneity in mathematical form the dispersion coefficient in anisotropic media is represented by a tensor of second rank bear 1988 this together with the rapid changes in magnitude and direction of the flow velocity in geological formations make the numerical prediction of anisotropic dispersion with traditional finite difference and finite volume methods very difficult such numerical solutions were found to suffer from unphysical oscillations and negative concentrations when dispersion is strongly anisotropic and the principal direction of dispersion deviates significantly from the grid orientation potier 2005 nordbotten and aavatsmark 2005 mlacnik and durlofsky 2006 yuan and sheng 2008 lipnikov et al 2009 only a few studies have assessed the use of particle methods to simulate anisotropic dispersion zimmermann et al 2001 beaudoin et al 2003 herrera et al 2009 herrera and beckie 2013 avesani et al 2015 because of their lagrangian character particle methods are free of numerical dispersion and artificial mixing when simulating flow advection and so they appear to be better suited than traditional grid based methods to simulate solute dilution and mixing in particular zimmermann et al 2001 evaluated the performance of the particle strength exchange pse method degond and mas gallic 1989 to simulate anisotropic dispersion in a homogeneous porous medium under uniform and nonuniform flow conditions finding acceptably accurate results only when a lagrangian distortion re meshing step was employed to correct for the flow field topology leading to clustering or spreading an alternative particle method to the pse scheme was proposed by beaudoin et al 2003 based on the diffusion velocity concept degond and mustieles 1990 however compared to the pse scheme this method gains in robustness and simplicity at the price of losing accuracy on the other hand herrera et al 2009 performed simulations of conservative solute transport in heterogeneous porous media with the aid of standard smoothed particle hydrodynamics sph gingold and monaghan 1977 obtaining results comparable to those from grid based techniques they found that no re meshing procedure is necessary if the second order derivatives are approximated in terms of only first order derivatives of the kernel function as is currently done in most sph applications to reduce the sensitivity to particle disorder cleary and monaghan 1999 more recently herrera and beckie 2013 tested their sph scheme against a benchmark problem of anisotropic dispersive transport and performed direct comparison of their results with an implementation of the pse method and a standard finite volume scheme with all three methods the numerical solutions were seen to exhibit artificial oscillations and negative concentrations when the off diagonal terms of the tensorial dispersion coefficient are nonzero also the solutions exhibited larger errors and slower convergence rates compared to the isotropic case they concluded that the non linear amplification of these oscillations represents a severe drawback for flow simulations where the anisotropic nature of the dispersion tensor must be considered to cope with this difficulty avesani et al 2015 proposed a modified version of the standard sph scheme based on a moving least squares weighted essentially non oscillatory mls weno avesani et al 2014 reconstruction of concentrations referred to as the mwsph method which produced accurate results and a drastic reduction of unphysical oscillations compared to previous sph calculations a different sph scheme based on a new sph approximation for the anisotropic diffusion operator called anisotropic sph approximation for anisotropic diffusion asphad was proposed by tran duc et al 2016 in this case the diffusion operator is first approximated by an integral in a coordinate system in which it is isotropic an inverse transformation is then applied to the integral so that the diffusion operator and the smoothing length become anisotropic for isotropic dispersion asphad matches very well the analytical result while for anisotropic dispersion anisotropy is somehow reduced because of the smoothing effect in sph although asphad conserves very well the principal diffusing directions for anisotropic dispersion the method is rather sensitive to the particle distribution unfortunately no comments were made by the authors on the ability of asphad to suppress negative concentrations for large anisotropic dispersion it follows from the work of herrera and beckie 2013 that the accuracy of dispersive transport simulations with standard sph is highly sensitive to the spatial distribution of particles in general the accuracy declines as the degree of particle disorder increases even for isotropic diffusion dispersion models however the calculations of avesani et al 2015 have also shown that the mwsph method is rather insensitive to the spatial distribution of particles which solves the drawback of standard sph when applied to dispersive transport in the presence of heterogeneous velocity fields however the considerable gain of accuracy of the mwsph method is associated with an increase of the computational cost due to the intermediate steps involved in the mls weno reconstruction and stencils construction at comparable spatial resolution the mwsph scheme demands up to two orders of magnitude more cpu time than standard sph avesani et al 2015 here we propose a consistent sph approach to simulate the anisotropic dispersion of a solute in porous media furthermore the performance of the method is compared with standard sph herrera and beckie 2013 and mwsph avesani et al 2015 under varying degrees of anisotropic dispersion to analyze the occurrence of negative concentrations finally the performance of the method for randomly distributed particles is also investigated by comparing the convergence rate of the solutions with the case of irregularly quasi random particle distributions the method is based on consistency considerations in standard sph sigalotti et al 2016 where the number of neighbors n and the smoothing length h are initially set in terms of the total number of particles n by means of scaling relations that comply with the asymptotic limits n h 0 and n for complete sph consistency as the spatial resolution is increased rasio 2000 zhu et al 2015 the consistency and accuracy of sph improves as the number of neighbors is increased with resolution which is consistent with the expected dependence of the sph particle discretization errors on 1 n monaghan 1992 read et al 2010 sigalotti et al 2019 in particular the error analysis of the sph representations of the continuity and momentum equations performed by read et al 2010 demonstrated that this error contributes with zeroth order terms that would persist when working with a fixed low number of neighbors even though n and h 0 causing a complete loss of particle consistency these findings were corroborated analytically by sigalotti et al 2019 who derived the explicit functional dependence of the error bounds on the sph interpolation parameters namely the smoothing length h and the number of neighbors n within the kernel support for the particle approximation of a function using the poisson summation formula therefore it is of interest to investigate whether the modified sph method with varied number of neighbors will improve the accuracy and convergence rate of diffusion dispersion transport simulations when particle consistency is restored the numerical solutions become insensitive to particle disorder and the estimates of the function and its first derivatives converge essentially at the same rate sigalotti et al 2016 however nothing has been concluded in previous analyses on the consistency of the sph approximations for second order derivatives which are key in the approximation of the diffusion dispersion operator except for their well known reduced sensitivity to particle disorder when they are expressed in terms of first order derivatives of the kernel function cleary and monaghan 1999 2 mathematical formulation non reactive solute transport in porous media is described by the ade which in lagrangian coordinates can be written as 1 d x d t v 2 d c d t d c c v where v v x t is the fluid velocity vector c c x t is the scalar tracer concentration and d is a symmetric tensor of second rank defining the diffusion and dispersion coefficient note that for a uniform velocity field eqs 1 and 2 are not coupled in component form the diffusion and dispersion coefficient is defined by the relation bear 1988 scheidegger 1961 3 d i j α t v d m δ i j α l α t v i v j v where α l is the longitudinal dispersivity α t is the transverse dispersivity d m is the molecular diffusion coefficient and δij is the kronecker delta in general α t α l by at least an order of magnitude so that hydrodynamical dispersion is anisotropic bear 1988 it is clear from relation 3 that the dispersion coefficient depends on the magnitude and direction of the fluid velocity as well as on the ratio α t α l if α t 0 relation 3 becomes 4 d i j d m δ i j α l v i v j v for dispersion along the longitudinal direction only moreover when α t α l α the off diagonal terms of tensor dij vanish and the dispersion is isotropic with 5 d i j α v d m δ i j in two space dimensions 2d the components of tensor dij as defined by relation 3 in cartesian coordinates are 6 d x x α t v d m α l α t v x 2 v d x y d y x α l α t v x v y v d y y α t v d m α l α t v y 2 v with v v x 2 v y 2 1 2 for incompressible flow v 0 and so eqs 1 and 2 are coupled through the dispersion coefficient thus for a time and space varying velocity field the anisotropic dispersion will also be time and space varying 3 sph formulation a sph approximation for eq 2 can be constructed by first looking at the discretization form of the term d c following herrera and beckie 2013 this term satisfies the identity 7 i 1 m j 1 m x i d i j c x j 1 2 i 1 m j 1 m 2 x i x j d i j c c 2 d i j x i x j d i j 2 c x i x j in index notation where m 2 in 2d and m 3 in three dimensions 3d according to this the sph discretization of the diffusion dispersion operator reduces to estimating second order derivatives of a scalar function a general sph approximation for the second order derivative of a continuous function f f x at the position of particle a say x a has the form 8 2 f x i x j a b 1 n m b ρ b f a f b γ x i a b x j a b x a b 2 δ i j 1 x a b 2 x a b a w a b where the summation is over n neighbors within the kernel support x a b x a x b x a b 2 x a b x a b w a b w x a b h is the kernel function γ 4 in 2d and γ 5 in 3d a formal derivation of the approximation 8 is given by yildiz et al 2009 using the sph identity 25 of español and revenga 2003 for i j we find that 9 i 1 m j 1 m γ x i a b x j a b x a b 2 δ i j i 1 m γ x i a b x i a b x a b 2 δ i i 2 and so the double sum of eq 8 over i and j reduces to the sph laplacian approximation which was found by morris et al 1997 to be less sensitive to particle disorder than the more direct laplacian form written in terms of second order derivatives of the kernel function substitution of eq 8 into eq 7 with f dijc f dij and f c leads after some algebraic steps to the sph representation of the dispersive term which at the position of particle a reads as follows 10 d c a 1 2 b 1 n m b ρ a b d a b c a c b x a b 2 x a b a w a b where ρ a b ρ a ρ b 2 and 11 d a b i 1 m j 1 m 4 d i j a d i j b d i j a d i j b γ x i a b x j a b x a b 2 δ i j in the final steps of the above derivation the term d i j a d i j b appearing in the definition of d a b has been replaced by the ratio 4 d i j a d i j b d i j a d i j b of the geometrical over the arithmetic means of tensor component dij between particles a and b in order to guarantee continuity of the dispersive term when dij is discontinuous cleary and monaghan 1999 finally the convective term c v on the right hand side of eq 2 is written in sph form using the standard approximation 12 c v a c a b 1 n m b ρ a b i 1 m v x i b v x i a w a b x i a for spreading of the solute in an external spatially uniform velocity field this term vanishes identically and eq 2 reduces to a pure diffusion dispersion equation 3 1 sph consistency the concept of consistency is related to how closely the discrete equations approximate the exact differential equations in particular if a sph approximation can reproduce an mth order polynomial exactly we say that the sph approximation has cm consistency or m 1 th order accuracy the standard sph method is known to have c 0 and c 1 kernel consistency for conventional kernel functions however satisfaction of these kernel consistency conditions does not imply that c 0 and c 1 particle consistency is also satisfied by the corresponding sph approximation sigalotti et al 2016 zhu et al 2015 liu and liu 2006 this discrepancy between the kernel and particle approximations is what we mean in sph by loss of the particle consistency it is well known that standard sph does not even have c 0 particle consistency due to violation of the discrete normalization condition of the kernel sigalotti et al 2016 rasio 2000 read et al 2010 sigalotti et al 2019 liu and liu 2006 the loss of consistency in the particle approximation procedure is usually associated with a the truncation of the kernel function at and near a physical domain boundary b the particle disorder and c the use of temporally and spatially varying smoothing lengths liu and liu 2006 it was not until recently that zhu et al 2015 identified another source of particle inconsistency associated with the fixed low number of neighbors within the compact support of the kernel function it has been common practice in sph simulations to use low numbers of neighbors 60 independently of the total number of particles they showed that c 0 particle consistency can be achieved only when n is sufficiently large so that the zeroth order error term carried by the sph discretization declines read et al 2010 sigalotti et al 2019 making the sph sum approximation to approach the continuous kernel approximation limit for n 1 zhu et al 2015 parameterized this error as 1 nγ where γ 0 5 for randomly distributed particles and γ 1 for low discrepancy sequences of particles i e quasi random distributions as is more appropriate for sph based on a balance between the leading error of the kernel approximation h 2 as is appropriate for most commonly used kernels and the 1 nγ error of the particle approximation zhu et al 2015 derived the scaling relations n n 1 3 β and h n 1 β for β 5 7 which satisfy the joint limit n h 0 and n with n n 0 rasio 2000 for full particle consistency when n is increased a recent analysis has demonstrated that with the use of the above scaling relations c 0 consistency is restored for both the particle estimates of a function and its first order derivatives with the numerical solution becoming insensitive to particle disorder and the effects of domain boundaries sigalotti et al 2016 in passing we note that if c 0 particle consistency is achieved c 1 particle consistency is also guaranteed because of the kernel symmetry an intermediate value of β 6 is appropriate when the smoothing is performed on pseudo randomly distributed particles for the simulations of this paper we then allow h to vary with n as h n 1 6 with this choice a family of curves can be obtained for the dependence of n on n out of the set of possible curves we have chosen the scaling relations n 2 81n 0 675 and h 1 29 n 0 247 to set the initial values of n and h in terms of n these exponents are slightly larger than the reference values of 0 5 and 1 3 when β 6 with this choice large numbers of neighbors can be accommodated for a given n while keeping reasonably large values of h however other choices with exponents closer to or even slightly smaller than the reference values produced convergence rates of the numerical solutions that remained essentially the same sigalotti et al 2016 3 2 the kernel function most commonly used kernels cannot support arbitrarily large number of neighbors because for finite sized smoothing lengths they suffer from the so called pairing instability which sets in when particles start to form pairs dehnen and aly 2012 in some extreme cases the paired particles become so close to each other that they emerge as effectively one particle leading to a loss of resolution while keeping the computational cost the same a promising family of kernel functions which are free from the pairing instability and therefore show much better convergence properties than traditional kernels are the wendland functions wendland 1995 dehnen and aly 2012 here we adopt the wendland c4 function defined as dehnen and aly 2012 13 w q h b 1 q 6 1 6 q 35 3 q 2 if q 1 and 0 otherwise where q x x h b 9 π h 2 in 2d and b 495 32 π h 3 in 3d a kernel function of this type has a strictly positive fourier transform and no inflection points these are desirable properties for convergence studies since the neighbor number can be increased without limit in addition wendland functions are reluctant to allow particle motion on sub resolution scales thereby maintaining very regular particle distributions even in highly dynamical tests rosswog 2015 this is a further important concern for accuracy and convergence of the sph method 3 3 time step control and time integration eqs 1 and 2 are solved explicitly by means of a predictor corrector leapfrog integration scheme the time step control for numerical stability is set by the condition herrera and beckie 2013 14 δ t c t h 2 i 1 m d i i so that the minimum δt is taken over all particles herrera and beckie 2013 found through numerical experiments that their sph solutions were stable if c t 0 1 in order to provide a comparison with their solutions and evaluate the effects of improving the consistency of sph when using arbitrarily large numbers of neighbors we use the same value for the constant c t in the predictor step particle positions and concentrations are evolved to an intermediate time t l 1 2 through the sequence 15 x a l 1 2 x a l 1 2 δ t v a l c a l 1 2 c a l 1 2 δ t d c d t a l with these updates the time rate of change of the concentration is calculated at the intermediate time level for use in the corrector step where quantities are advanced to the new time t l 1 according to 16 x a l 1 x a l δ t v a l 1 2 c a l 1 c a l δ t d c d t a l 1 2 for the simulation cases of this paper the flow velocity is given as an external constant input parameter and therefore v a l v a l 1 2 in the more general case of a temporally and spatially varying velocity field the continuity and momentum equations must be solved coupled to eqs 1 and 2 which can be easily accommodated in the above time integrator 4 numerical simulations in this section we analyze the accuracy of the method for 2d simulations of the anisotropic dispersion of a gaussian pollutant source the method is applied to a benchmark diffusion dispersion problem with analytical solution zimmermann et al 2001 beaudoin et al 2003 herrera and beckie 2013 in particular we test the performance of our sph scheme by comparing the numerical results with those obtained by avesani et al 2015 with their mwsph method 4 1 model setup and initial conditions the test consists of releasing instantaneously an initial gaussian concentration of solute mass δ m 10 4 kg given by 17 c x t 0 c 0 exp x x 0 2 y y 0 2 2 w 2 in a two dimensional unbounded domain with a temporally and spatially constant velocity field where c 0 0 32 kg m 3 is the maximum initial concentration w 44 m is the width of the gaussian plume at t 0 and x 0 y 0 are the coordinates of the plume center at t 0 the unbounded domain is modeled by a square of side length l 2000 m and by implementing periodic boundary conditions at its borders the contaminant is injected at the center of the square so that x 0 y 0 1000 m because of the constant flow velocity the advective term on the right hand side of eq 2 is identically zero and so the transport process will depend on the flow only through the relation between the dispersion coefficient and the flow velocity the analytical solution for the solute concentration at any time t 0 is given by 18 c x t c 0 w 2 c 4 exp x x 0 2 a 1 y y 0 2 a 2 4 x x 0 y y 0 a 3 8 t 2 c 2 4 w 2 t c 3 2 w 2 where 19 a 1 2 t d y y w 2 a 2 2 t d x x w 2 a 3 t d x y c 2 d x x d y y d x y 2 c 3 d x x d y y c 4 4 t 2 c 2 2 t w 2 c 3 w 4 1 2 the longitudinal dispersivity is α l 10 m and the constant flow velocity is taken to be v v x 2 v y 2 1 m day 1 1 16 10 5 m s 1 in addition the molecular diffusivity d m in eq 3 is set to zero and so the dispersion tensor will only depend on the longitudinal and transverse dispersivities and on the magnitude and direction of the flow velocity 4 2 simulation cases one set of calculations is performed with irregularly distributed particles following a low discrepancy i e quasi random 1 1 a quasi random sequence corresponds to a set of irregularly distributed points which is not random in a mathematical sense in contrast to a truly random distribution where the points clump together in a quasi random sequence the points are scattered around without a clear pattern and never clumped together that is they are better spread out filling the space more efficiently these sequences are defined as having low discrepancy that is the discrepancy between the number of points that actually fall in a volume and the number of points that we would expect to fall in the same volume is small sequence we will study the convergence rate of our modified sph scheme for four different dispersivity ratios α t α l and two different flow orientations with increasing number of neighbors and decreasing smoothing lengths as the resolution is increased according to the scaling relations defined in section 3 1 table 1 lists the spatial resolution parameters the first four columns list the mean particle separation distance normalized to l the total number of sph particles the corresponding number of neighbors and the smoothing lengths normalized to l respectively while the fifth column gives the cpu time in seconds of the code on an intel r xeon e5 2690 v3 cpu clockspeed 2 6 ghz processor with 12 cores in order to provide direct comparison with the cases of irregularly distributed particles a second set of calculations is carried out with randomly distributed particles examples of the initial irregular and random particle distributions are shown in figs 1 and 2 respectively for the lowest resolution case n 625 the same patterns are employed for all higher resolution runs of table 1 in figs 1 and 2 the coordinates of particle positions are normalized to the side length l of the square domain we consider values of α t α l 0 001 0 01 0 1 and 1 0 where α t α l 1 0 corresponds to isotropic dispersion and α t α l 0 001 corresponds to very large anisotropic dispersion in addition we choose two different orientations for the flow velocity i e θ 0 and θ 45 measured with respect to the x axis with these choices v x v and v y 0 for θ 0 and v x v y 2 v 2 for θ 45 in order to provide comparison with standard sph and mwsph simulations the range of values of these parameters are the same employed by herrera and beckie 2013 and avesani et al 2015 eqs 1 and 2 are solved in normalized form using the following prescriptions t t 300 days c x t c x t c 0 w w l α l α l l α t α t l x 0 x 0 l y 0 y 0 l v v v 0 dxx dxx v 0 l dxy dxy v 0 l and dyy dyy v 0 l where v 0 20 3 m days 1 this is the mean velocity that a particle should have to cross the side length l 2000 m of the computational domain in 300 days for each pair of values α t α l θ and the eleven resolutions of table 1 the convergence study for the case of irregularly distributed particles amounts to a total number of 88 independent runs calculations with randomly distributed particles were performed only for θ 0 and total number of particles varying from n 10000 to 1 000 000 corresponding to 28 additional runs for α t α l 0 001 0 01 0 1 and 1 0 in all cases the simulations were terminated after 300 days i e normalized time t 1 the models were run using a modified version of the fully parallel open source code dualsphysics for diffusive and dispersive transport problems which relies on sph theory for solving the fluid dynamics equations gómez gesteira et al 2012 crespo et al 2015 5 results 5 1 isotropic dispersion we first consider the case of isotropic dispersion when α t α l 1 0 fig 3 shows the difference of the maximum concentration values between the analytical and numerical solutions normalized to the maximum initial concentration c 0 as a function of time for the case of irregularly distributed particles the curves belong to different resolution runs for the same model and the symbols on each curve display the data at discrete intervals of about 30 days as the resolution is increased the difference in the maximum concentrations decreases at t 300 days the relative errors drop from 18 when n 22500 and n 2435 to 0 2 when n 562500 and n 21382 reaching values lower than 0 04 when the resolution is increased to n 1 000 000 and n 31529 the maximum relative errors are shifted to earlier times as the resolution is improved with values ranging from 6 to 2 2 when n increases from 21 382 to 31529 hence standard sph is very sensitive to the choice of n and n and therefore to the smoothing length since larger values of n correspond to smaller values of h the trends shown in fig 3 are similar to those reported by avesani et al 2015 in their fig 5 where the difference is seen to increase from the beginning reaching a maximum value much earlier than predicted here and then decreasing to an asymptotic value at later times as the contaminant concentration smooths out however almost identical curves to those shown in fig 3 for n 21382 and n 31529 are obtained when these models are recalculated with perfectly uniform particle distributions meaning that the numerical solution becomes insensitive to the degree of particle disorder when the zeroth order discretization errors are reduced for large values of n similar trends to fig 3 are also obtained for the isotropic dispersion using randomly distributed particles as the number of neighbors is increased the size of the smoothing length is decreased and the zeroth order errors carried by the particle discretization decay as 1 n making the sph approximation to become essentially insensitive to particle disorder sigalotti et al 2016 when working with finite h consistency demands that n while accuracy demands that n and h 0 in order to have convergent results in the limit n the choice of scaling relations slightly different to n 2 81n 0 675 and h 1 29 n 0 247 with exponents closer to 0 5 in the former case and to 0 33 in the latter case will result in correspondingly smaller values of n and h at given n thus improving the accuracy in terms of h and reducing the computational cost since less neighbors will fill the kernel support for the runs with irregularly distributed particles fig 4 compares the concentration profiles for the isotropic case at different resolutions with the analytical solution solid line at t 300 days when θ 0 left and θ 45 right in all cases the profiles are shown along a section forming an angle of 0 with respect to the x axis independently of the flow orientation the numerical solution matches the analytical one everywhere when n 562500 and n 21382 with root mean square errors rmses that are less than 2 7 10 4 see section 5 3 as the resolution decreases the matching deteriorates particularly around the peak of the distribution where the distance between the maximum analytical and numerical concentrations increases and the distribution becomes relatively broader very similar profiles as shown in the left plot of fig 4 are also obtained for θ 0 with randomly distributed particles the concentration field for α t α l 1 0 with n 1 000 000 n 31529 and θ 0 is shown in fig 5 as compared to the analytical solution at t 300 days the shape and structure of the contaminant plume are almost identical to the analytical solution and independent of the flow orientation for the isotropic runs including those with coarser resolutions the numerical solution is always free from spurious oscillations that cause negative values of the concentration 5 2 anisotropic dispersion we now explore the performance of the method when the dispersion is anisotropic i e when the dispersivity ratio α t α l 1 in particular figs 6 and 7 show the temporal variation of the difference of the maximum concentration values between the numerical and analytical solutions at various resolutions for the highly anisotropic cases α t α l 0 01 and 0 001 respectively in both figures the results are shown for the case of irregularly distributed particles while similar trends are obtained for the case of randomly distributed particles the differences are always positive independently of the resolution meaning that sph overestimates the maximum concentration for the highest resolution run i e for n 1 000 000 and n 31529 the maximum relative error at earlier times is smaller than 1 for both dispersivities which is comparable with those reported by avesani et al 2015 with their mwsph m3 and mwsph m4 models for irregularly distributed particles at t 300 days the relative errors have decayed to less than 0 12 regardless of the value of α t α l meaning that good convergence is achieved by sph at such resolution independently of the dispersivity ratio this behavior can be explained as follows the combined kernel and particle discretization errors in sph go as sigalotti et al 2019 20 e a 0 n a 1 h n a 2 n a 2 k h 2 where the term proportional to a 2 k is the contribution from the continuous kernel approximation while the remaining zeroth first and second order terms are the contributions from the particle discretization for large n the error is dominated by the term a 2 k h 2 while for small h the main source of error is given by the zeroth order term a 0 n this term contributes with an irreducible error even when n and h 0 thus for finite values of h consistency is achieved only for sufficiently large values of n while accuracy results from the additional requirement that h be sufficiently small therefore smaller concentration differences could be obtained by reducing further the size of the kernel support and increasing the number of neighbors hence as n the second order error is governed by the kernel approximation and the size of h fig 8 compares the concentration profiles at different spatial resolutions for the case of irregularly distributed particles with the analytical solution solid line at t 300 days for anisotropic dispersion with α t α l 0 01 and flow orientation θ 0 top left and θ 45 top right and with α t α l 0 001 for θ 0 bottom left and θ 45 bottom right as in fig 4 all profiles are shown along a section inclined of 0 with respect to the x axis for both flow orientations and dispersion ratios the numerical solution is seen to match the analytical profile for n 1 000 000 and n 31529 as the resolution is lowered the numerical profiles become broader and their separation from the analytical solution increases around the peak of the distribution as shown in section 5 3 the worst rmse between the numerical and analytical distributions at the highest resolution is 6 5 10 4 and occurs for α t α l 0 001 and θ 45 for comparison when θ 0 the deviation from the analytical profile is slightly smaller 5 0 10 4 fig 9 shows similar plots for the case of randomly distributed particles and flow orientation θ 0 when α t α l 0 01 left and α t α l 0 001 right at the highest resolution the rmse errors around the peak of the distribution are 4 86 10 3 for α t α l 0 01 and 4 91 10 3 for α t α l 0 001 which are about an order of magnitude higher than for the corresponding cases in fig 8 this is consistent with the findings of zhu et al 2015 who showed numerically that at comparable spatial resolution the sph discretization error goes as 1 n for low discrepancy quasi random sequences of particles as given by eq 20 while it decays more slowly as 1 n 0 5 for randomly distributed particles negative concentrations are recurrently found in anisotropic transport models not only in sph simulations but also with other particle methods and traditional mesh based techniques herrera and beckie 2013 when the off diagonal components of the dispersion tensor are nonzero the numerical solutions exhibit artificial oscillations which amplify nonlinearly such oscillations induce negative values of the concentration which represent serious limitations for the correct prediction of anisotropic solute dispersion according to previous sph simulations the situation worsens when the particles are irregularly distributed for example for moderately large dispersivities α t α l 0 01 herrera and beckie 2013 reported maximum negative values of the concentration of 1 7 10 2 kg m 3 5 3 10 2 c 0 for a quasi random distribution although our simulations always predict positive concentrations for the isotropic scenario independently on the flow orientation the same is not true for the anisotropic case fig 10 shows the magnitude of the maximum negative values as a function of the total number of particles for all three dispersion ratios when the particles are irregularly arranged see fig 1 the results exhibit a negligible dependence on the flow direction as it is observed by comparing the left θ 0 and right θ 45 plots as expected the smallest magnitudes at all resolutions occur for α t α l 0 1 in this case the magnitude of the negative concentrations range from 4 9 10 7 c 0 for n 10000 to 3 0 10 6 c 0 for n 1 000 000 when the dispersivity is increased to α t α l 0 01 and 0 001 the magnitude of the maximum negative concentrations increases by a factor of 2 ranging from 8 1 10 7 c 0 for n 10000 to 5 7 10 6 c 0 for n 1 000 000 when α t α l 0 001 however there is no much difference in the negative concentration values when increasing the dispersivity from 0 01 to 0 001 similar trends are shown in fig 11 for the case of randomly distributed particles when the flow orientation is θ 0 the magnitudes of the maximum negative concentrations at all resolutions are seen to slightly increase compared to the case of irregularly distributed particles left plot of fig 10 with values ranging from 6 6 10 7 c 0 for n 10000 to 3 2 10 6 c 0 for n 1 000 000 when α t α l 0 1 for α t α l 0 001 these values range from 1 04 10 6 c 0 for n 10000 to 6 0 10 6 c 0 for n 1 000 000 in all cases the magnitude of the negative concentrations increases with increasing resolution which is not surprising because at coarser resolution more numerical diffusion is added which causes more damping of the unphysical oscillations however at the highest resolutions the rate of increase of the magnitude of the negative concentrations slows down and so it appears that at even higher resolutions these magnitudes will stabilize toward a constant value compared to herrera and beckie 2013 the present sph scheme produces fewer negative concentrations than standard sph with maximum negative concentrations that are from 3 to 4 orders of magnitudes lower however avesani et al 2015 reported a limit to the absolute value of negative concentrations less than about 10 7 c 0 with their mwsph scheme in their case concentrations and concentration gradients are computed at the midpoint of the segment connecting particle pairs using local high order reconstruction at the particle positions given the concentrations of the surrounding particles such non oscillatory weno reconstruction technique on moving points has proved to be efficient in drastically reducing the amplitude of the unphysical oscillations the top panels of fig 12 display the concentration fields with irregularly distributed particles at t 300 days for the anisotropic dispersion runs with α t α l 0 1 and 0 01 when θ 45 using n 1 000 000 and n 31529 as compared to the analytical solution the bottom panels show the case when α t α l 0 001 for θ 0 left and 45 right fig 13 displays the same concentration fields with randomly distributed particles for α t α l 0 01 and 0 001 when θ 0 using n 1 000 000 and n 31529 at this resolution all solutions exhibit negative concentrations represented by the white bands along the transversal direction on both sides of the plume elongation the white bands are oriented parallel to the flow direction and always appear in the tail of the distributions where the concentration decays asymptotically to zero the elliptic deformation and structure of the contaminant plume due to the anisotropic dispersion is remarkably similar to the analytical solution in all cases compared to standard sph herrera and beckie 2013 the region occupied by the negative concentrations are now distributed over smaller portions of the computational domain and their widths are comparable to those from the accurate mwsph results reported by avesani et al 2015 the gain in terms of accuracy of the mwsph method is associated to an increase of the computational cost in their table 2 avesani et al 2015 give the cpu time of their serial code for particles distributed uniformly within the domain on an intel r core tm i7 2640m cpu 2 80 ghz single processor the cpu times in seconds for all resolutions with the present parallelized sph code are given in the last column of table 1 however a comparison of the computational cost between the mwsph code and the present scheme can only be done in an approximate sense for instance our parallelized code on an intel r xeon e5 2690 v3 cpu with clockspeed 2 6 ghz and 12 cores will run about 100 times faster than a serial version on a single intel r core tm i7 2640m processor therefore an inspection of table 1 and their table 2 shows that for similar resolutions the computational demand of both schemes is comparable however one advantage of the present sph scheme is that it gives comparable accuracy to the mwsph method with a much simpler approach for the isotropic case herrera and beckie 2013 found that the use of higher order kernels does not provide a significant improvement of the numerical solution when the particles are regularly distributed in contrast the use of higher order cutoff functions in the pse method results in smaller errors with maximum magnitudes of the negative concentration values that range from less than 1 for second order cutoff functions to less than 0 0003 of the maximum concentration for a fourth order cutoff function zimmermann et al 2001 however for the anisotropic case it was found that the use of higher order sph kernels has only a limited impact on removing the unphysical oscillations in the transverse direction herrera and beckie 2013 5 3 sph errors and accuracy we measure the accuracy of the numerical solutions by means of a root mean square error rmse 21 rmse c 1 n a 1 n c a anal c a sph 2 which is closely related to the l 2 norm error since the sph errors have a normal rather than a uniform distribution the rmse will provide a better representation of the error distribution than other statistical metrics chai and draxler 2014 in addition the rmse consists of squaring the magnitude of the errors before they are averaged and so it gives a relatively higher weight to errors with large absolute values for the case of irregularly distributed particles fig 14 shows the rmse of the numerical concentrations normalized to the initial maximum concentration as a function of l δx for θ 0 left and θ 45 right respectively for the isotropic models α t α l 1 0 the error declines as δx 2 02 for θ 0 and as δx 1 94 for θ 45 for anisotropic dispersion the convergence becomes slower than second order with δx 1 76 for α t α l 0 1 to δx 1 30 for the more extreme case when α t α l 0 001 and θ 0 similar convergence rates are obtained for θ 45 with the rmses varying from δx 1 71 for α t α l 0 1 to δx 1 29 for α t α l 0 001 thus at relatively small particle spacing i e large values of l δx the rate of convergence is close to second order for isotropic dispersion meaning that c 1 particle consistency is achieved when n 562500 and n 21382 for moderately low dispersivities α t α l 0 1 the convergence rate is also close to second order while for larger dispersivities α t α l 0 01 the convergence rates slow down though they are always better than first order even in the most extreme case when α t α l 0 001 the approximate quadratic convergence for α t α l 0 1 is reassuring since this dispersion ratio is in line with what is indeed observed experimentally bear 1988 for comparison fig 15 shows the corresponding rmse of the numerical concentrations for the case when the particles are randomly distributed and θ 0 in this case the error declines as δx 1 20 for both the isotropic and anisotropic dispersion while this error is always better than first order it is also independent of the dispersivity fig 16 shows the dependence of the relative difference between the maximum numerical and analytical concentrations for the case of irregularly distributed particles when θ 0 left and θ 45 right the results are shown for all resolutions and dispersion ratios in all cases the relative difference decreases as the resolution is increased good convergence to the analytical solution is always achieved for n 562500 and n 21382 independently of the flow orientation and dispersion ratio as the resolution is further increased to n 1 000 000 and n 31529 the relative differences tend to zero with c sph c anal 1 9 10 4 c 0 for α t α l 1 0 7 2 10 4 c 0 for α t α l 0 1 8 2 10 4 c 0 for α t α l 0 01 and 8 0 10 4 c 0 for α t α l 0 001 convergence is also achieved for all dispersivities when the particles are randomly distributed as shown in fig 17 for θ 0 and n 1 000 000 with n 31529 neighbors at this resolution c sph c anal 3 2 10 3 c 0 for α t α l 0 1 5 7 10 3 c 0 for α t α l 0 01 and 6 0 10 3 c 0 in good agreement with the result that sph converges more slowly for randomly distributed particles than for low discrepancy sequences zhu et al 2015 for large anisotropic dispersion the use of large numbers of neighbors results in lower errors and improved convergence rates as compared to standard sph this can be better seen from fig 18 where the rmses are plotted versus the dispersion ratio α t α l for all models when θ 0 an almost identical plot is obtained when the flow orientation is changed to θ 45 a result which is consistent with the findings of previous dispersion simulations using equispaced particles when passing from n 562500 and n 21382 to n 1 000 000 and n 31529 the rmse is seen to decay more rapidly for isotropic than for anisotropic dispersion while at low resolution the errors are almost constant for the range of dispersivity ratios considered in the light of these results a reduction of the zeroth order discretization errors that appear when passing from the continuous kernel to the particle approximation is enough to guarantee convergence for sph simulations of anisotropic dispersion preliminary tests have shown that using smaller time steps and different scalings leading to smaller values of h while maintaining large numbers of neighbors have little effect on improving the convergence rate for large anisotropic dispersion on the other hand restoring consistency makes sph to become insensitive to particle disorder sigalotti et al 2016 this is true because for large numbers of neighbors within the kernel support the sph interpolation errors decrease independently of how the particles are distributed within the kernel support moreover compared to more traditional kernels the wendland functions prevent the growth of particle disorder as they do not allow particle motion at sub resolution scales that is at scales smaller than the smoothing length rosswog 2015 eq 2 is the classical fick s second law describing the diffusion and dispersion of a substance and results from combining the continuity equation for the concentration and fick s first law for the flux it is well known that due to its parabolic character this equation issues an infinite velocity of propagation which is clearly unphysical compte and metzler 1997 that is according to its analytical solution 18 even for very small times there will always exist a finite amount of the diffusing and dispersing contaminant at large distances from the site where it was initially injected implying an infinitely fast propagation therefore any small numerical oscillation at large distances from the contaminant may give rise to negative concentrations in order to cope with this difficulty r cattaneo 1948 proposed a modification by adding an ad hoc small term of the form τd 2 c dt 2 to the classical fick s first law which converts eq 2 into the more general form 22 τ d 2 c d t 2 d c d t d c which is of a damped wave equation type known as the telegrapher or cattaneo s equation while fick s first law implies that the flux adjusts instantaneously to the gradient of the concentration giving rise to an unrealistically infinitely fast spreading of local disturbances the term τd 2 c dt 2 turns the ade into a hyperbolic equation where now the flux relaxes with some characteristic time constant τ as a consequence the propagation velocity along the longitudinal and transverse directions namely v p i d i i τ 1 2 remains finite this model has been successfully used to account for anomalous dispersion in amorphous materials scher and montroll 1975 among many other applications that have been reported in the literature until recent years a non local theory of transport proves necessary in materials with microscale inhomogeneities such as the porous media whenever the mean residence time of a tracer in the material is comparable to the correlation time which is the time required for the tracer to sample the microstructures koch and brady 1987 examples of such systems involves the initial dispersion of a pollutant in the atmosphere in rivers and in ground water flows brady and koch 1988 in the singular limit τ 0 v p i and so eq 22 reduces to eq 2 however in passing we remind that hyperbolic equations may not preserve strict positivity that is even though c x t 0 the numerical solution of eq 22 may in general suffer from negative values mendez et al 2010 in the framework of a consistent sph scheme the problem then reduces to investigate under which conditions either eq 2 or eq 22 can be used to model anomalous transport for large anisotropy ratios while keeping the solution free from the numerical instabilities that lead to unphysical negative concentrations 6 conclusions in this paper we have proposed a consistent smoothed particle hydrodynamics sph approach to simulate the anisotropic dispersion of a solute in porous media zeroth and first order consistency of the sph scheme is achieved by setting the number of neighbors within the kernel support n and the size of the smoothing length h in terms of the total number of particles n by means of power law relations of the form n n 1 3 β and h n 1 β for β 5 7 zhu et al 2015 which satisfy the asymptotic limits n h 0 and n with n n 0 for full particle consistency as n is increased rasio 2000 the consistency and accuracy of sph improves as the number of neighbors is increased and the size of the smoothing length is decreased with resolution which is consistent with the expected dependence of the sph discretization errors on 1 n monaghan 1992 read et al 2010 sigalotti et al 2019 these errors contribute with zeroth order terms that would persist when working with small numbers of neighbors even though n and h 0 leading to complete loss of consistency read et al 2010 a wendland c4 function is employed for the kernel interpolation which can support large numbers of particles without a pairing instability dehnen and aly 2012 we test the performance of the consistent sph scheme against the analytical solution of a two dimensional benchmark test for the isotropic and anisotropic dispersion of a gaussian plume for irregularly and randomly distributed particles and compare with previous sph simulations for the same test problem herrera and beckie 2013 avesani et al 2015 as the number of particles and neighbors within the kernel support are increased the distance between the numerical and analytical concentration solutions is reduced both solutions are seen to match independently of the flow orientation and dispersivity ratio when n 1 000 000 and n 31590 suggesting that these values may represent a lower limit for convergence of sph for anisotropic transport the numerical solutions exhibit convergence rates and magnitudes of the negative concentrations comparable to those reported by avesani et al 2015 with their mwsph method based on a moving least squares weighted essentially non oscillatory mls weno reconstruction of concentrations which is at best one of the most accurate particle methods for approximating anisotropic dispersion however an advantage of the present method compared to mwsph is its simplicity any user working with standard sph can easily modify his her code to improve convergence and accuracy on dispersion problems this aspect is of significance for most practical applications where prediction of local scale dispersion on plume movement mixing and dilution are important the same is true for applications of solute dispersion in multiphase flows and reactive transport up to the level of resolution tried here the solutions are not free from unphysical oscillations that induce negative values of the concentrations however the use of large numbers of neighbors coupled to small kernel supports results in negative concentration values that are several orders of magnitude lower than predicted by standard sph simulations herrera and beckie 2013 and comparable to those from accurate mwsph simulations avesani et al 2015 evidently restoring approximate first order consistency for the particle approximation is not enough to ensure a full positive solution for anisotropic dispersion the apparently irreducible problem associated with the unphysical oscillations for highly anisotropic dispersion must not be necessarily attributed to intrinsic deficiencies of the numerical method but rather to the parabolic character of the classical fick s second law which issues an infinite velocity of propagation giving rise to an unrealistically infinitely fast spreading of local disturbances at large distances from the site where they were initially injected acknowledgements we thank the anonymous reviewers for raising a number of suggestions that have improved the content of the manuscript the calculations of this paper were performed using the computational facilities of the abacus centro de matemática aplicada y cómputo de alto rendimiento of cinvestav we acknowledge funding from the european union s horizon 2020 programme under the enerxico project grant agreement no 828947 and under the mexican conacyt sener hidrocarburos grant agreement no b s 69926 one of us c e a r acknowledges finantial support from the conacyt sener energy sustainability strategic project no 212602 aztlan platform l di g s acknowledges support from the universidad autónoma metropolitana azcapotzalco uam a through internal funds the source code is available upon request by writing to the email address iqcarlosug gmail com 
588,optimizing a multi reservoir system is challenging due to the problem of the curse of dimensionality in this paper rule based improved dynamic programming ridp and stochastic dynamic programming risdp algorithms for the optimal operation of a system with a number of parallel reservoirs are proposed to alleviate the dimensionality problem the improvement is based on a key property the monotonic dependence relationship between individual reservoir carryover storage and system water availability which is derived with the assumption of the non decreasing storage distribution characteristic of a parallel reservoir system furthermore a diagnosis procedure is employed to remove infeasible state transitions which enables the application of the monotonic relationship within the feasible solution space in general the computational complexity of n s n 2 from dp can be reduced to ns n from ridp ns is the number of storage discretization for individual reservoirs n is the number of reservoirs in a parallel system with controlled solution accuracy the improved algorithms are applied to a real world parallel reservoir system in northeastern china the results demonstrate the computational efficiency and effectiveness of ridp and risdp keywords improved dynamic programming monotonic dependence relationship state transitions parallel reservoir system 1 introduction reservoirs are basic elements of complex water resources systems and operations of multi reservoir system have been widely studied for spatial and temporal regulation of water flow to satisfy the various needs yeh 1985 oliveira and loucks 1997 singh et al 2017 however the optimal decision making process for the coordinated operation of a parallel reservoir system as well as other complex multi reservoir system is always challenging because of the nonlinearity of system dynamics dimensionality of state variables and stochasticity of future inflow to overcome these challenges many advanced optimization techniques are proposed as can be found from some comprehensive state of the art reviews by yeh 1985 labadie 2004 rani and moreira 2009 and ahmad et al 2014 as one of the most popular optimization techniques discrete dynamic programming dp has been widely adopted to solve reservoir operation problem since it can take advantage of the temporally sequential reservoirs decision making process and handle the nonlinear noncontinuous objective functions and constraints yeh 1985 labadie 2004 furthermore based on a markov decision process the formulation of stochastic dynamic programming sdp can explicitly take the stochasticity of future streamflow into account yeh 1985 labadie 2004 singh et al 2017 however the applicability of dp especially sdp is limited to reservoir systems with only a very limited number of individual reservoirs due to the so called curse of dimensionality i e the exponential increase in computational burden and memory requirements as the number of state variables increases based on the global optimality of dp proved by the bellman s principle of optimality bradley et al 1977 various modifications have been performed on the traditional dp formulation to target significant improvement of computational efficiency with controlled accuracy powell 2011 sutton and barto 2017 which can be further summarized as the following two active research directions one is to decompose a multi stage multi reservoir operation problem into multi stage multiple single reservoir operation problem e g via dynamic programming with successive approximation dpsa tilmant and kelman 2007 or multiple one stage multi reservoir operation problem e g via progressive optimality algorithm poa howson and sancho 1975 or multi stage aggregation disaggregation reservoir operation problem e g via aggregation disaggregation approaches turgeon 1980 turgeon and charbonnea 1998 although the dimensional complexity of dp and sdp is significantly reduced the severe loss of accuracy might be suffered through those approximation schemes yeh 1985 zeng et al 2015 the other direction is to narrow down the search space of dp and sdp based on some information e g the inflow forecast information stedinger et al 1984 and snow water equivalent information pina et al 2017 or practical constraints e g non negative release constraint mousavi et al 2003 minimum and maximum hydropower production constraints trezoz and yeh 1987 or special characteristics of a reservoir system e g the relationships between the decision variables and state variables zhao et al 2012 2014 2017 for example stedinger et al 1984 proposed a sdp model which employed the best inflow forecast instead of the preceding period s flow as a hydrological state variable for the derivation of the optimal operation policies by using the non negative release constraints to diagnose the feasibility of storage state transition mousavi et al 2003 devised an iterative method to eliminate the infeasible state transitions for cascade reservoir operation optimization following the derived monotonic relationship between reservoir initial storage and release decisions zhao et al 2012 2014 2017 proposed an algorithm to improve the computational efficiency of dp and sdp for single reservoir water supply hydropower generation and flood control operation however the application of the distributed characteristics of individual reservoir storage to overcome the dimensionality problem of multi reservoir operation optimization has received few attentions in this paper we will derive a monotonic relationship between individual reservoir carryover storage and the water availability of a parallel reservoir system by introducing the spatially distributed characteristics of individual reservoir storage we will further use this relationship to improve the most widely used formulation of dp i e discrete dp and markov based sdp yeh 1985 labadie 2004 vijay et al 2017 for parallel reservoir operation optimization under deterministic and stochastic conditions respectively since the nyc rule proposed by clark 1956 to equalize the probability of filling of each reservoir in a parallel system the spatial distribution of individual reservoir storage has been extensively studied and several heuristic operating rules are suggested for the coordinated operation of a parallel reservoir system with a joint demand such as the space rule clark 1956 maass et al 1962 parametric rule nalbantis and koutsoyiannis 1997 and storage target curves lund and ferreira 1996 perera and codner 1996 among these storage distribution rules one common procedure is to define the desired carryover storage target proportional to the total system carryover storage which implies the synchronized drawdown refill cycles for the reservoirs in parallel for example by the so called space rule the space remaining in individual reservoirs of a parallel reservoir system is positively related to that in the total system both the parametric rule and the target storage curves attempt to avoid the situation of having one individual reservoir carryover storage decreasing while the total carryover storage of the system increasing oliveira and loucks 1997 following the non decreasing storage distribution characteristic between the desired individual carryover storage and the total system carryover storage clark 1956 nalbantis and koutsoyiannis 1997 lund and ferreira 1996 oliveira and loucks 1997 this paper will derive a monotonic relationship between individual reservoir carryover storage and the parallel system water availability with a general concave utility function to enable the application of the monotonic property within the feasible region of the optimization problem a diagnosed procedure similar to what proposed by mousavi et al 2003 is employed for removing the state transitions resulting in negative individual reservoir releases therefore based on both the feasibility of state transition and the monotonic property the optimal solutions of the proposed method are searched within a feasible region by which only a small number of state variable combinations need to be tested this is different from the traditional dp optimization that is performed conditionally on all discrete combinations of storage vector in dp labadie 2004 according to our knowledge this study can be the first to apply these two properties to address the curse of dimensionality for multi reservoir operation optimization especially for parallel reservoir system operation the proposed method is also applicable for other parallel systems such as parallel drainage systems with the same non decreasing characteristic of reserved storage or space distribution the rest of this paper is organized as follows section 2 presents the formulation of a non constraint dynamic optimization model for the operation of reservoirs in parallel and derives the monotonic relationships between individual carryover storage and the total system water availability section 3 provides a diagnosing procedure for the feasibility of state transition with the consideration of physical constraints the improved dp and sdp methods based on the derived properties are described in section 4 a parallel reservoir system in northeastern china is illustrated as a case study in section 5 finally the conclusions of this paper are given in section 6 2 dynamic optimization model and the monotonic property this section describes a non constraint dynamic optimization model based on bellman equation and derives the monotonic relationship between the optimal carryover storage of individual reservoir and the total water availability of parallel system under both deterministic and stochastic conditions 2 1 dynamic optimization model for a parallel reservoir system the dynamic operation of a parallel reservoir system for water supply over multi stages can be described with an objective to maximize the cumulative total water use utilities over the operational horizon labadie 2004 1a max t 1 t u t r t t o t a l 1b r t t o t a l i 1 n r i t where r i t is the water release from individual reservoir i at time t r t t o t a l is the total water release of parallel reservoir system at time t which is equal to the sum of all the individual reservoir releases u t r t t o t a l is the total water supply utility function for parallel reservoir system at time t n is the number of reservoirs in a parallel system t is the length of operational time horizon the model expressed in eq 1 involves a sequence of decisions for a number of reservoirs and the direct solutions of such a large nonlinear optimization model are difficult to obtain yeh 1985 oliveira and loucks 1997 labadie 2004 for a set of problems that satisfy the bellman s principle of optimality the bellman equation has been adopted to decompose the multi stage problem into a series of two stage optimization problems birge and louveaux 2011 the backward recursion bellman equation for a system of parallel reservoirs with joint water demand can be expressed as 2 c u t s 1 t s n t max u t r t t o t a l c u t 1 s 1 t 1 s n t 1 t where cut s 1 t s n t is the value function representing the maximum return accumulated from the current period t to the final period t conditioned on the initial storage state vector s 1 t s n t in eq 2 the decision variables are selected to determine how much water should be released from each reservoir while the state variables are set as the amount of water stored in each reservoir at the end of this period since there is no direct hydraulic connection for reservoirs in parallel the state transitions are characterized by the mass balance equation of each reservoir 3a s i t 1 s i t i i t r i t t 3b w a i t s i t i i t t where s i t is the storage of i reservoir at the beginning of time t i i t is the inflow of i reservoir at time t wa i t is the water availability of i reservoir at time t which is defined as the sum of initial water storage plus current inflow during time period t for i reservoir for simplicity utility discount evaporation and other losses are assumed to be small enough and can be neglected in eqs 3a and 3b substituting eqs 3a and 3b into eq 2 the bellman equation can be written as 4 c u t s 1 t s n t max u t r t t o t a l c u t 1 w a 1 t r 1 t w a n t r n t t provided that the water delivery utilities in eq 1 have the desired concave properties for all the operational periods the value function cut s 1 t s n t is considered to inherit the concavity from the utility function fama 1970 carroll and kimball 1996 since it represents the cumulative water supply utilities from period t to t zhao et al 2012 2017 for the unconstrained optimization problem the first order condition is both the necessary and sufficient condition when the objective is continuously differentiable concave function bazaraa et al 2006 thus the optimal values of release and carryover storage of reservoir i i e r i t and s i t 1 can be specified from the first order condition of eq 4 as follows 5 u t r i t c u t 1 s i t 1 d s i t 1 d r i t c u t 1 s i t 1 because the marginal values of water release from different reservoirs are identical in eq 1 i e u t r i t u t r j t eq 5 becomes 6 u t r 1 t u t r n t c u t 1 s 1 t 1 c u t 1 s n t 1 where u t r i t reflects the marginal utility of water release from reservoir i at period t c u t 1 s i t 1 denotes the marginal cumulative utility from period t 1 to t with respect to the carryover storage at the end of period t i e s i t 1 and it measures the impacts of carryover storage on the cumulative utility of reservoir operation in the future periods eq 6 indicates the marginal utility principle for water allocation among reservoirs in parallel system i e marginal utilities of water delivery and carryover storage for different reservoirs are equal 2 2 monotonic property under deterministic condition for further mathematical derivation two realistic assumptions are used to characterize the spatial distribution of individual reservoir storage firstly the optimal carryover storage of individual reservoir at time t 1 is assumed to be a non decreasing function of the total system carryover storage i e the optimal level of storage would increase or at least maintain the same for each reservoir if the total system carryover storage increases this assumption is widely adopted for operation of parallel reservoir systems in both practice and in theory johnson et al 1991 sand 1984 zeng et al 2015 such as the commonly used space rule clark 1956 maass et al 1962 storage target curves lund and ferreira 1996 perera and codner 1996 and parametric rule nalbantis and koutsoyiannis 1997 secondly it is conceivable that the marginal value of carryover storage for each reservoir decreases or remains unchanged if the total storage of other reservoirs increases since more water is available in the system for not only current but also the future demand i e 2 c u t 1 s i t 1 i 1 n s i t 1 s i t 1 0 a special case of these two assumptions is when the value function cu t 1 s 1 t 1 s n t 1 is an additively separable function in terms of s i t 1 and s j t 1 i e the carryover storage of reservoir j i e s j t 1 does not affect the marginal value function of reservoir i then 2 c u t 1 s i t 1 s j t 1 0 this kind of value functions is often used to analytically identify the characteristics of storage distribution especially for real time operation of a multi reservoir system where the carryover storage target of each individual reservoir at the end of each time step can be predefined in practice johnson et al 1991 zeng et al 2015 subsequently the monotonic relationship between the optimal storage carried over to period t 1 and the total system water availability at period t can be derived as bellow for any two optimal values of carryover storage for reservoir i i e s i t 1 1 and s i t 1 2 corresponding to two water availability levels i e i 1 n w a i t 1 and i 1 n w a i t 2 if s i t 1 1 s i t 1 2 according to the first assumption we have i 1 n s i t 1 1 i 1 n s i t 1 2 and then i 1 n s i t 1 1 s i t 1 1 i 1 n s i t 1 2 s i t 1 2 otherwise if i 1 n s i t 1 1 s i t 1 1 i 1 n s i t 1 2 s i t 1 2 there must exist s j t 1 1 s j t 1 2 which leads to i 1 n s i t 1 1 i 1 n s i t 1 2 and s i t 1 1 s i t 1 2 subsequently applying the second assumption and the diminishing marginal utility property of cu t 1 with respect to s i t 1 the relationship between marginal values of carryover storage related to s i t 1 1 and s i t 1 2 can be specified as 7a c u t 1 s i t 1 1 s 1 t 1 1 s n t 1 1 c u t 1 s i t 1 1 s 1 t 1 2 s n t 1 2 since i 1 n s i t 1 2 s i t 1 2 i 1 n s i t 1 1 s i t 1 1 7b c u t 1 s i t 1 1 s 1 t 1 2 s n t 1 2 c u t 1 s i t 1 2 s 1 t 1 2 s n t 1 2 since s i t 1 2 s i t 1 1 7c then c u t 1 s i t 1 1 s 1 t 1 1 s n t 1 1 c u t 1 s i t 1 1 s 1 t 1 2 s n t 1 2 c u t 1 s i t 1 2 s 1 t 1 2 s n t 1 2 following the first order condition i e eq 6 and the diminishing marginal utility property of ut we can have u t r i t 1 c u t 1 s i t 1 1 c u t 1 s i t 1 2 u t r i t 2 and then r t 1 t o t a l r t 2 t o t a l therefore the total water availability with respect to s i t 1 1 i e i 1 n w a i t 1 r t 1 t o t a l i 1 n s i t 1 1 is less than that with respect to s i t 1 2 i e i 1 n w a i t 2 r t 2 t o t a l i 1 n s i t 1 2 and the increasing monotonicity between the optimal carryover storage and the total water availability of a parallel reservoir system is derived as illustrated in fig 1 particularly for any total water availability if i 1 n w a i t 1 i 1 n w a i t 2 there should have s i t 1 1 s i t 1 2 and no other factors would affect the carryover storage decision 2 3 monotonic property under stochastic condition the derivation of the monotonic property presented in section 2 2 assumes deterministic inflow of each reservoir at each period however in the real world the inflow of reservoir i at period t 1 i e ii t 1 is imperfectly known at the beginning of the operational period t therefore the monotonic relationship between the optimal reservoir carryover storage and the total water availability of parallel reservoir system needs to be further verified with the consideration of hydrological uncertainty assumed that the inflow into each reservoir can be specified as a markov process then the randomness of the inflow ii t 1 at time step t 1 is addressed through a prior inflow transition probability p i i t 1 i i t given inflow ii t at time step t yeh 1985 labadie 2004 vijay et al 2017 analogy to eq 2 the bellman equation of a typical discretized sdp is 8a c u t s 1 t s n t i 1 t i n t max u t r t t o t a l e x p c u t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 t 8b e x p c u t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 i 1 n i i t 1 p i i t 1 i i t c u t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 where p i i t 1 i i t is the conditional probability of the inflow ii t 1 at time step t 1 on ii t at time step t expcu t 1 is the expected value function representing the maximum expected return accumulated from period t to the final period t conditioned on the initial storage state vector s 1 t 1 s n t 1 and inflow vector i 1 t 1 i n t 1 comparing eq 2 with eq 8a it can be seen that reservoir inflow are also defined as state variables in the value function cut s 1 t s n t i 1 t i n t under stochastic condition which is different from the value function cut s 1 t s n t under deterministic condition however it should be noted that for any given combination of ii t 1 the partial dependence relationship between cu t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 and si t 1 under stochastic condition is considered to be concave as the one under deterministic condition i e 2 c u t 1 s i t 1 2 0 moreover the partially concave dependence relationship between cu t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 and si t 1 would lead to the concave relationship between expcu t 1 and si t 1 since transition probability p i i t 1 i i t is non negative and si t 1 and ii t 1 are independent with each other following the similar derivation presented in section 2 1 i e marginal utility principle and section 2 2 i e the two assumptions and diminishing marginal utility property of concave function the monotonic relationship between the optimal carryover storage and the total water availability of parallel reservoir system can also be identified under stochastic condition the monotonic property derived from deterministic and stochastic inflow conditions plays an important role in narrowing the search space of dp and sdp as described in section 4 3 the feasibility of state transition the monotonic property discussed above is applicable when the system is only constrained by flow mass balance taking account of other physical constraints i e storage constraints 0 s i t 1 k i where k i is the storage capacity of reservoir i non negative release r i t 0 and total release constraints 0 r t t o t a l d t where dt is the projected demand of the parallel reservoir system the feasibility of state transition should be enabled before the next iteration search of decision variables otherwise the search can be misled for example the program may end with a negative release if the transition starts a low initial storage and ends with a high final storage note that a traditional dp or sdp program will discrete the storage s between 0 or another specified value and the capacity k and perform all discrete combinations of storage vector labadie 2004 thus the storage constraints will be satisfied but the release constraints will not on the other hand if the non negative release constraint r i t 0 is satisfied then the lower bound constraint of the total release will also be satisfied 0 r t t o t a l thus this section illustrates how to diagnose the feasibility of state transition with the consideration of the constraints of non negative releases and the upper bound of the total release and remove the infeasible transitions from further computing procedures for any given combinations of initial storage and inflow of reservoir i at period t suppose a null release is resulted from the carryover storage s i t 1 then we have 9 s i t 1 s i t i i t r i t s i t i i t s i t 1 by eq 9 the carryover storage should not exceed s i t 1 and then s i t 1 can be used as a criterion for the carryover storage to detect the status of reservoir release for example when any carryover storage is higher than s i t 1 a negative release would be obtained from eq 9 otherwise a positive release is specified on the other hand the value of s i t 1 can be derived from the state transition in eq 9 once the release shifts from non negative status to negative status in a discretized dp and sdp to reduce computation burden all the carryover storage above s i t 1 are removed from further computation otherwise the program will end with a negative release similarly for the upper bound constraint of the total release a different value of s i t 1 will be specified from the equation r t t o t a l d t for each reservoir and all the carryover storage of reservoir i that below the specified s i t 1 will be removed from further computing to avoid the violation of the total release constraint following the description above the carryover storage of reservoir i will be tested with an ascending order from 0 to ki to obtain the value of s i t 1 and remove the cases with carryover storage above s i t 1 which can result in a negative release in contrast a descending order from ki to 0 of si t 1 is used for the consideration of the upper bound constraint of the total release based on these procedures a diagnosis of the status of the constraints of non negative release and the upper bound of the total release can be conducted to enable the application of the monotonic property described in section 2 within the feasible solution space 4 improved dp and sdp for a discretized dp enumeration over all discrete combinations of carryover storage would be exhaustively performed to derive the optimal decisions for a number of individual reservoirs therefore the computational complexity of dp exponentially increases with the number of state variables which is well known as the curse of dimensionality in this section the monotonic property derived in section 2 and the diagnosed procedure for the feasibility of state transition discussed in section 3 are employed to design an algorithm to alleviate the curse of dimensionality via improved dp and sdp denoted as ridp and risdp in the following the general principle for the algorithm design can be stated as searching within the feasible region for the first state combination of initial storage i e 0 0 the minimum storage for each reservoir in this paper and narrowing the search space for the subsequent state combinations i e s 1 t s n t this will take advantage of both the feasibility of state transition and the monotonic property first of all the infeasible state transitions that end with negative releases will be identified and excluded from the calculation process secondly according to the monotonic relationship between the optimal carryover storage or the mean carryover storage under stochastic condition and the total system water availability at period t if the optimal carryover storage at period t 1 s 1 t 1 s n t 1 corresponding to the initial storage at period t s 1 t s n t is determined then only n 1 number of finial storage states namely s 1 t 1 s n t 1 s 1 t 1 i l s n t 1 s 1 t 1 s n t 1 i l need to be tested to derive the optimal solution for the given initial state s 1 t s i t il s n t where il is the interval length used for variable increment in the optimal solution search of ridp and risdp the details of ridp are shown in fig 2 and described as follows 1 initialize state variables for simplicity discretizing storage volume with an equal interval length i e il for each reservoir at each period and number them in the ascending order from 1to nsi t where nsi t is the number of storage discretization for reservoir i at period t 2 search within the feasible region for the first state combination of initial storage i e the minimum storage is set as the state 1 for each reservoir the feasibility of state transition is diagnosed by the following procedure for reservoir i suppose the release i e r i t resulting from initial state 1 and finial state npi t 1 where npi t il corresponds to s i t 1 in section 3 and npi t 1 nsi t 1 would be just shifted from non negative status to negative status then any carryover storage level above npi t 1 will surely lead to infeasible transition as long as reservoir inflow i i t remains constant as a result npi t 1 determines the number of feasible state transitions for reservoir i under given initial storage s i t and inflow i i t to avoid unnecessary computation on infeasible state transitions the searching procedure stops at the state npi t 1 and then moves to test the feasibility of state transition for another reservoir as shown in fig 2a following this procedure the optimal releases and carryover storages are only selected from the feasible region and the computational complexity of this step is reduced from ns 1 t 1 ns i t 1 ns n t 1 of dp to np 1 t 1 np i t 1 np n t 1 of ridp 1 narrow the searching space on the basis of the optimal carryover storage specified from step 2 the optimal solutions for any other subsequent initial state s 1 t s i t il s n t i e nk 1 t nk i t 1 nk n t in fig 2b and nki t il corresponds to s i t are identified from an iterative search approach by evaluating only the optimal carryover storage s 1 t 1 s n t 1 i e nq 1 t 1 nq n t 1 in fig 2b and nqi t 1 il corresponds to s i t 1 corresponding to initial state s 1 t s n t i e nk 1 t nk n t in fig 2b and the state s 1 t 1 s n t 1 with only one interval increment namely s 1 t 1 i l s n t 1 s 1 t 1 s n t 1 i l i e nq 1 t 1 1 nq n t 1 nq 1 t 1 nq n t 1 1 in fig 2b 2 iterate computation repeat step 3 until all the state combinations of initial storage are tested via steps 1 4 a multi reservoir multi period optimization operation problem can be solved by recursive computation the computational complexity of ridp is np 1 t 1 np i t 1 np n t 1 n 1 ns 1 t 1 ns i t 1 ns n t 1 1 whereas that of dp is ns 1 t 1 ns i t 1 ns n t 1 2 specially if only the diagnosed procedure of feasible state transitions is used for algorithm design then the computational complexity of ridp is about np 1 t 1 np i t 1 np n t 1 ns 1 t 1 ns i t 1 ns n t 1 on the other hand if only the monotonic property is used then the computational complexity of ridp is ns 1 t 1 ns i t 1 ns n t 1 n 1 ns 1 t 1 ns i t 1 ns n t 1 1 different from ridp risdp searches the optimal decisions from both the discretized state of carryover storage and inflow scenarios suppose the inflow scenarios are discretized with the same number i e nlt for each reservoir at each period then the computational complexity of risdp can be expressed as np 1 t 1 np i t 1 np n t 1 n 1 ns 1 t 1 ns i t 1 ns n t 1 1 nlt n compared to that of ns 1 t 1 ns i t 1 ns n t 1 2 nlt n of sdp following the procedures listed above the computational load can be significantly reduced especially for a model with a large number of state variables and or a large number of storage discretization as demonstrated in the case study 5 case study the improved algorithms i e ridp and risdp are compared to the primary algorithms i e dp and sdp and the aggregation disaggregation dp i e ad dp proposed by turgeon 1980 through a real world water supply project i e biliuhe yingnahe zhuanjiaolou b y z parallel reservoir system in liaoning province of china 5 1 b y z parallel system description as the largest commercial city in the northeast part of china dalian city suffers severe water scarcity to meet water demand of the increasing population and rapid economic development three reservoirs located on different streams i e biliuhe reservoir b reservoir yingnahe reservoir y reservoir and zhuanjiaolou reservoir z reservoir are jointly operated to meet the municipal and industrial m i water demand of dalian city the layout of b y z parallel system is shown in fig 3 b reservoir is situated in the downstream area of the biliuhe river with an effective storage capacity of 644 59 million m3 and annual runoff of 550 41 million m3 y reservoir is situated in the upstream area of the yingnahe river with an effective storage capacity of 215 58 million m3 and annual runoff of 325 68 million m3 z reservoir is situated in the upstream area of the huli river with an effective storage capacity of 108 27 million m3 and annual runoff of 70 67 million m3 the objective function for the model applied to the b y z reservoir system is set to minimize the shortage index si which is widely used to assess water supply performance throughout the planning horizon tu et al 2008 zeng et al 2014 10 m i n s i 100 t d t r t t o t a l d t 2 where si is a decreasing convex function of the total release with a negative first order derivative and a positive second order derivative with respect to the release historical inflow data from 1958 to 2012 are collected and the cumulative probability function at each 10 day period the operation time step in this study is identified from this long term inflow data following that five annual inflow scenarios of each reservoir are defined on the basis of cumulative probabilities of 10 30 50 70 and 90 percentiles at each operation period which is used to represent very wet wet normal dry and very dry inflow scenarios respectively therefore 5 5 5 125 inflow scenarios are designed in table 1 for the further comparison study for simplicity the water demand of year 2030 is set as 16 04 million m3 for each operation period 10 day without considering the variation from period to period six storage discretization levels i e interval lengths ranging from 10 to 20 million m3 are employed for the algorithm implementation all the traditional and improved algorithms are executed with gun fortran http www codeblocks org on a lenovo thinkpad t470p laptop with intel r core tm i7 7700hq 2 80 ghz 8 00 gb of ram the water supply indicators are given in fig 4 and 5 while the average execution times are displayed in fig 8 and 9 5 2 water supply performances of dp and ridp as can be seen from fig 4a there is no significant difference between the operation results i e si derived from dp and ridp under the various combinations of inflow scenarios and storage discretization levels particularly as shown in fig 4b the optimal values of si corresponding to the three set of inflow scenarios no 66 to 75 81 to 100 and 101 to 125 are close to the values of si derived by dp with a storage discretization level of 10 million m3 in addition both dp and ridp generate the same level of si with different storage discretization levels under the same inflow scenario as shown in fig 4c to compare the results from dp and ridp the results si of dp are used as benchmark and the differences between ridp and dp are shown in fig 5 fig 5a plots the differences for the combinations of inflow scenarios and storage discretization levels as can be seen low differences are found for most scenarios and the average difference over all combinations of inflow scenarios and discretization levels is about 0 20 and the maximum relative error is 9 61 as further illustrated in fig 5b the average difference does not exceed 1 for most inflow scenarios except several inflow scenarios no 76 to 80 see the table 1 where the values of si are extremely low less than 0 05 the maximum difference is 5 71 in addition as illustrated in fig 5c the average differences ranging from 0 01 to 0 45 are below 0 5 for all storage discretization levels and the maximum value is 0 45 therefore the differences between ridp and dp are low under the various storage discretization levels and inflow scenarios although the performance of ridp is affected by the inflow scenarios that result in extremely low shortage index si values to explore the causes for the relative high differences between ridp and dp under inflow scenarios no 76 to 80 fig 6 displays the marginal values of water delivery derived from dp and ridp with the discretization level of 14 million m3 under inflow scenario 77 which results in the maximum difference fig 7 shows the corresponding storage trajectory for each of the three reservoirs as can be seen form fig 6 the marginal costs of water release i e negative marginal values between dp and ridp are equal for most of periods except periods 24 to 26 these differences can be explained from the corresponding storage trajectory of b and y reservoirs in fig 7 compared to the results of dp ridp results in lower carryover storage of y reservoir and higher storage of b reservoir at period 23 and 25 under the operation derived from dp the shortage levels at periods 25 and 26 are lowered down and the marginal cost of water delivery are almost equally allocated during periods 24 to 26 however with ridp the carryover storage of y reservoir is maintained at the lowest level while the storage level of b reservoir is lowered down during periods 24 to 26 thus the marginal cost cannot be equal during periods 24 to 26 since the minimum storage constraint of y reservoir is binding these results imply that the performance of ridp can be restricted when the ratio of the storage capacity of one individual reservoir to its average annual inflow is relatively much lower than that of others 5 3 computing time of dp and ridp the computing time of dp and ridp with different discretization levels are illustrated in fig 8 a and 8b a finer discretization level usually requires longer computation time tejada guibert et al 1993 for the various levels of storage discretization the computing time of ridp is always less than 0 30 of that used by dp for example the time of dp is 8 19 s while that of ridp is 0 02 s for the discretization level of 20 million m3 in addition it can be seen that the computing time of dp increases much faster than ridp with finer discretization levels as shown in fig 9 this observation reflects the description of the computational complexity of ridp vs dp i e dp is proportional to ns 6 and ridp is proportional to ns 3 these results demonstrate that the execution time can be significantly reduced through ridp especially for a high storage discretization levels or a large number of state variables the significant efficiency improvement of ridp is achieved by both the initial search i e the feasibility of state transition and subsequent search i e the monotonic property designed with ridp to evaluate to what extent these two strategies contribute to the efficiency improvement of ridp two diagnosis procedures are conducted one using the feasibility of state transition only named as ridp1 and the other with the monotonic property only i e named as ridp2 the computing time for these two procedures are shown in fig 8a and b the average computing time of dp under different discretization levels is about 2 times of that used by ridp1 while the average computing time with ridp2 is only 1 2 times that used by ridp thus the experiments with this case study reservoir systems indicate that the significant improvement of computation efficiency with ridp is attributed to the monotonic relationship between the carryover storage from period t to period t 1 and the total system water availability at period t 5 4 comparison of ridp with ad dp the effectiveness and efficiency of ridp is further verified through the comparison of ridp with the aggregation disaggregation dp i e ad dp proposed by turgeon 1980 both methods are applied to different inflow scenarios with the same storage discretization level of 10 million m3 first the computing time of ridp is 0 14 s compared to 51 11 s with ad dp second as can be seen in fig 10 the water shortage level resulting from ad dp is higher than that from ridp the result difference between the two methods can be explained from their different optimal solution search mechanisms for ad dp the total release and individual reservoir carryover storage decisions are all made according to a system perspective thus the nonnegative release constraint is considered only after the total release is specified for the equivalent reservoir and a negative individual reservoir release might be taken account into the total release decision of the parallel reservoir system johnson et al 1991 which lead to a relatively poor water supply performance to avoid this situation a relatively homogeneous distribution of reservoir water availability is often required zeng et al 2015 in contrast the heterogeneity of water availability resulting from different reservoir inflows and reservoir storage capacities are allowed with the non decreasing characteristic of storage distribution in ridp therefore the release and carryover storage decisions are made with consideration of the operation performance of individual reservoirs including a full set of constraints on both the system and individual reservoirs 5 5 operation results of sdp and risdp to test the algorithm with a stochastic dynamic optimization problem sdp and risdp are applied to the b y z parallel reservoir system with the storage discretization level of 10 million m3 and the inflow discretization number of 5 for each reservoir at each period the results of sdp and risdp are summarized in table 2 risdp also exhibits high computational efficiency and accuracy the computing time of risdp is only 8 91 s compared to 64 766 88 s of sdp for running the models over a one year time horizon and the difference between the si values from sdp and risdp is 0 39 in addition the computing time of sdp is nearly 125 times of that of dp with the same discretization level of 10 million m3 while the computing time of risdp is only 56 times of that of ridp this reflects the computational complexity expression described in section 4 where the computational complexity of sdp is nlt 3 times of dp nlt is the number of inflow discretization and nlt 5 in this case study but the computational complexity of risdp is less than nlt 3 times of ridp because of the search with the feasibility of state transition used in risdp 6 conclusions this paper presents improved dp and sdp algorithms based on a derived monotonic relationship between individual reservoir carryover storage and parallel reservoir system water availability a diagnosis procedure is further employed to enable the application of the monotonic property within the feasible solution space of the optimization problem taking advantage of these two properties the infeasible state transitions are removed from computation by using certain constraints in the diagnosis procedure non negative release and the upper bound of the total rerelease and the monotonic relationship allows only n 1 number of state combinations to be tested in general for the optimization of a parallel reservoir system with a joint water demand the computational complexity of the ridp is proportional to ns n compared to n s n 2 of a traditional dp ns is the number of storage discretization for individual reservoir n is the number of reservoirs in a parallel system to demonstrate the effectiveness and efficiency of the proposed algorithms the b y z reservoir system in northeastern china is employed as a case study numerical results with the case study show that 1 the ridp is an efficient algorithm under the various combinations of storage discretization levels and inflow scenarios especially for high accuracy of a large number of state variables i e with a high storage discretization level 2 ridp ends with both a better computational efficiency and a larger water shortage reduction than ad dp 3 compared to sdp risdp can also significantly reduce the computing time without much loss of solution accuracy 4 however the efficiency of the proposed algorithms may be limited when the ratio of the storage capacity of one individual reservoir to its average annual inflow is relatively much lower than that of others although the proposed algorithms are designed for the operation optimization of parallel reservoir systems the general principle of the proposed methods i e the monotonic property and the feasibility of state transition can be also used together with other techniques for the complex multi reservoir systems with reservoirs both in parallel and in series acknowledgments this research is supported by the national natural science foundation of china 51609174 91647204 11771058 china postdoctoral science foundation 2016m602359 and the major program of national science and technology support plan of china 2016yfc0402209 2016yfc0400203 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 07 003 appendix supplementary materials image application 1 
588,optimizing a multi reservoir system is challenging due to the problem of the curse of dimensionality in this paper rule based improved dynamic programming ridp and stochastic dynamic programming risdp algorithms for the optimal operation of a system with a number of parallel reservoirs are proposed to alleviate the dimensionality problem the improvement is based on a key property the monotonic dependence relationship between individual reservoir carryover storage and system water availability which is derived with the assumption of the non decreasing storage distribution characteristic of a parallel reservoir system furthermore a diagnosis procedure is employed to remove infeasible state transitions which enables the application of the monotonic relationship within the feasible solution space in general the computational complexity of n s n 2 from dp can be reduced to ns n from ridp ns is the number of storage discretization for individual reservoirs n is the number of reservoirs in a parallel system with controlled solution accuracy the improved algorithms are applied to a real world parallel reservoir system in northeastern china the results demonstrate the computational efficiency and effectiveness of ridp and risdp keywords improved dynamic programming monotonic dependence relationship state transitions parallel reservoir system 1 introduction reservoirs are basic elements of complex water resources systems and operations of multi reservoir system have been widely studied for spatial and temporal regulation of water flow to satisfy the various needs yeh 1985 oliveira and loucks 1997 singh et al 2017 however the optimal decision making process for the coordinated operation of a parallel reservoir system as well as other complex multi reservoir system is always challenging because of the nonlinearity of system dynamics dimensionality of state variables and stochasticity of future inflow to overcome these challenges many advanced optimization techniques are proposed as can be found from some comprehensive state of the art reviews by yeh 1985 labadie 2004 rani and moreira 2009 and ahmad et al 2014 as one of the most popular optimization techniques discrete dynamic programming dp has been widely adopted to solve reservoir operation problem since it can take advantage of the temporally sequential reservoirs decision making process and handle the nonlinear noncontinuous objective functions and constraints yeh 1985 labadie 2004 furthermore based on a markov decision process the formulation of stochastic dynamic programming sdp can explicitly take the stochasticity of future streamflow into account yeh 1985 labadie 2004 singh et al 2017 however the applicability of dp especially sdp is limited to reservoir systems with only a very limited number of individual reservoirs due to the so called curse of dimensionality i e the exponential increase in computational burden and memory requirements as the number of state variables increases based on the global optimality of dp proved by the bellman s principle of optimality bradley et al 1977 various modifications have been performed on the traditional dp formulation to target significant improvement of computational efficiency with controlled accuracy powell 2011 sutton and barto 2017 which can be further summarized as the following two active research directions one is to decompose a multi stage multi reservoir operation problem into multi stage multiple single reservoir operation problem e g via dynamic programming with successive approximation dpsa tilmant and kelman 2007 or multiple one stage multi reservoir operation problem e g via progressive optimality algorithm poa howson and sancho 1975 or multi stage aggregation disaggregation reservoir operation problem e g via aggregation disaggregation approaches turgeon 1980 turgeon and charbonnea 1998 although the dimensional complexity of dp and sdp is significantly reduced the severe loss of accuracy might be suffered through those approximation schemes yeh 1985 zeng et al 2015 the other direction is to narrow down the search space of dp and sdp based on some information e g the inflow forecast information stedinger et al 1984 and snow water equivalent information pina et al 2017 or practical constraints e g non negative release constraint mousavi et al 2003 minimum and maximum hydropower production constraints trezoz and yeh 1987 or special characteristics of a reservoir system e g the relationships between the decision variables and state variables zhao et al 2012 2014 2017 for example stedinger et al 1984 proposed a sdp model which employed the best inflow forecast instead of the preceding period s flow as a hydrological state variable for the derivation of the optimal operation policies by using the non negative release constraints to diagnose the feasibility of storage state transition mousavi et al 2003 devised an iterative method to eliminate the infeasible state transitions for cascade reservoir operation optimization following the derived monotonic relationship between reservoir initial storage and release decisions zhao et al 2012 2014 2017 proposed an algorithm to improve the computational efficiency of dp and sdp for single reservoir water supply hydropower generation and flood control operation however the application of the distributed characteristics of individual reservoir storage to overcome the dimensionality problem of multi reservoir operation optimization has received few attentions in this paper we will derive a monotonic relationship between individual reservoir carryover storage and the water availability of a parallel reservoir system by introducing the spatially distributed characteristics of individual reservoir storage we will further use this relationship to improve the most widely used formulation of dp i e discrete dp and markov based sdp yeh 1985 labadie 2004 vijay et al 2017 for parallel reservoir operation optimization under deterministic and stochastic conditions respectively since the nyc rule proposed by clark 1956 to equalize the probability of filling of each reservoir in a parallel system the spatial distribution of individual reservoir storage has been extensively studied and several heuristic operating rules are suggested for the coordinated operation of a parallel reservoir system with a joint demand such as the space rule clark 1956 maass et al 1962 parametric rule nalbantis and koutsoyiannis 1997 and storage target curves lund and ferreira 1996 perera and codner 1996 among these storage distribution rules one common procedure is to define the desired carryover storage target proportional to the total system carryover storage which implies the synchronized drawdown refill cycles for the reservoirs in parallel for example by the so called space rule the space remaining in individual reservoirs of a parallel reservoir system is positively related to that in the total system both the parametric rule and the target storage curves attempt to avoid the situation of having one individual reservoir carryover storage decreasing while the total carryover storage of the system increasing oliveira and loucks 1997 following the non decreasing storage distribution characteristic between the desired individual carryover storage and the total system carryover storage clark 1956 nalbantis and koutsoyiannis 1997 lund and ferreira 1996 oliveira and loucks 1997 this paper will derive a monotonic relationship between individual reservoir carryover storage and the parallel system water availability with a general concave utility function to enable the application of the monotonic property within the feasible region of the optimization problem a diagnosed procedure similar to what proposed by mousavi et al 2003 is employed for removing the state transitions resulting in negative individual reservoir releases therefore based on both the feasibility of state transition and the monotonic property the optimal solutions of the proposed method are searched within a feasible region by which only a small number of state variable combinations need to be tested this is different from the traditional dp optimization that is performed conditionally on all discrete combinations of storage vector in dp labadie 2004 according to our knowledge this study can be the first to apply these two properties to address the curse of dimensionality for multi reservoir operation optimization especially for parallel reservoir system operation the proposed method is also applicable for other parallel systems such as parallel drainage systems with the same non decreasing characteristic of reserved storage or space distribution the rest of this paper is organized as follows section 2 presents the formulation of a non constraint dynamic optimization model for the operation of reservoirs in parallel and derives the monotonic relationships between individual carryover storage and the total system water availability section 3 provides a diagnosing procedure for the feasibility of state transition with the consideration of physical constraints the improved dp and sdp methods based on the derived properties are described in section 4 a parallel reservoir system in northeastern china is illustrated as a case study in section 5 finally the conclusions of this paper are given in section 6 2 dynamic optimization model and the monotonic property this section describes a non constraint dynamic optimization model based on bellman equation and derives the monotonic relationship between the optimal carryover storage of individual reservoir and the total water availability of parallel system under both deterministic and stochastic conditions 2 1 dynamic optimization model for a parallel reservoir system the dynamic operation of a parallel reservoir system for water supply over multi stages can be described with an objective to maximize the cumulative total water use utilities over the operational horizon labadie 2004 1a max t 1 t u t r t t o t a l 1b r t t o t a l i 1 n r i t where r i t is the water release from individual reservoir i at time t r t t o t a l is the total water release of parallel reservoir system at time t which is equal to the sum of all the individual reservoir releases u t r t t o t a l is the total water supply utility function for parallel reservoir system at time t n is the number of reservoirs in a parallel system t is the length of operational time horizon the model expressed in eq 1 involves a sequence of decisions for a number of reservoirs and the direct solutions of such a large nonlinear optimization model are difficult to obtain yeh 1985 oliveira and loucks 1997 labadie 2004 for a set of problems that satisfy the bellman s principle of optimality the bellman equation has been adopted to decompose the multi stage problem into a series of two stage optimization problems birge and louveaux 2011 the backward recursion bellman equation for a system of parallel reservoirs with joint water demand can be expressed as 2 c u t s 1 t s n t max u t r t t o t a l c u t 1 s 1 t 1 s n t 1 t where cut s 1 t s n t is the value function representing the maximum return accumulated from the current period t to the final period t conditioned on the initial storage state vector s 1 t s n t in eq 2 the decision variables are selected to determine how much water should be released from each reservoir while the state variables are set as the amount of water stored in each reservoir at the end of this period since there is no direct hydraulic connection for reservoirs in parallel the state transitions are characterized by the mass balance equation of each reservoir 3a s i t 1 s i t i i t r i t t 3b w a i t s i t i i t t where s i t is the storage of i reservoir at the beginning of time t i i t is the inflow of i reservoir at time t wa i t is the water availability of i reservoir at time t which is defined as the sum of initial water storage plus current inflow during time period t for i reservoir for simplicity utility discount evaporation and other losses are assumed to be small enough and can be neglected in eqs 3a and 3b substituting eqs 3a and 3b into eq 2 the bellman equation can be written as 4 c u t s 1 t s n t max u t r t t o t a l c u t 1 w a 1 t r 1 t w a n t r n t t provided that the water delivery utilities in eq 1 have the desired concave properties for all the operational periods the value function cut s 1 t s n t is considered to inherit the concavity from the utility function fama 1970 carroll and kimball 1996 since it represents the cumulative water supply utilities from period t to t zhao et al 2012 2017 for the unconstrained optimization problem the first order condition is both the necessary and sufficient condition when the objective is continuously differentiable concave function bazaraa et al 2006 thus the optimal values of release and carryover storage of reservoir i i e r i t and s i t 1 can be specified from the first order condition of eq 4 as follows 5 u t r i t c u t 1 s i t 1 d s i t 1 d r i t c u t 1 s i t 1 because the marginal values of water release from different reservoirs are identical in eq 1 i e u t r i t u t r j t eq 5 becomes 6 u t r 1 t u t r n t c u t 1 s 1 t 1 c u t 1 s n t 1 where u t r i t reflects the marginal utility of water release from reservoir i at period t c u t 1 s i t 1 denotes the marginal cumulative utility from period t 1 to t with respect to the carryover storage at the end of period t i e s i t 1 and it measures the impacts of carryover storage on the cumulative utility of reservoir operation in the future periods eq 6 indicates the marginal utility principle for water allocation among reservoirs in parallel system i e marginal utilities of water delivery and carryover storage for different reservoirs are equal 2 2 monotonic property under deterministic condition for further mathematical derivation two realistic assumptions are used to characterize the spatial distribution of individual reservoir storage firstly the optimal carryover storage of individual reservoir at time t 1 is assumed to be a non decreasing function of the total system carryover storage i e the optimal level of storage would increase or at least maintain the same for each reservoir if the total system carryover storage increases this assumption is widely adopted for operation of parallel reservoir systems in both practice and in theory johnson et al 1991 sand 1984 zeng et al 2015 such as the commonly used space rule clark 1956 maass et al 1962 storage target curves lund and ferreira 1996 perera and codner 1996 and parametric rule nalbantis and koutsoyiannis 1997 secondly it is conceivable that the marginal value of carryover storage for each reservoir decreases or remains unchanged if the total storage of other reservoirs increases since more water is available in the system for not only current but also the future demand i e 2 c u t 1 s i t 1 i 1 n s i t 1 s i t 1 0 a special case of these two assumptions is when the value function cu t 1 s 1 t 1 s n t 1 is an additively separable function in terms of s i t 1 and s j t 1 i e the carryover storage of reservoir j i e s j t 1 does not affect the marginal value function of reservoir i then 2 c u t 1 s i t 1 s j t 1 0 this kind of value functions is often used to analytically identify the characteristics of storage distribution especially for real time operation of a multi reservoir system where the carryover storage target of each individual reservoir at the end of each time step can be predefined in practice johnson et al 1991 zeng et al 2015 subsequently the monotonic relationship between the optimal storage carried over to period t 1 and the total system water availability at period t can be derived as bellow for any two optimal values of carryover storage for reservoir i i e s i t 1 1 and s i t 1 2 corresponding to two water availability levels i e i 1 n w a i t 1 and i 1 n w a i t 2 if s i t 1 1 s i t 1 2 according to the first assumption we have i 1 n s i t 1 1 i 1 n s i t 1 2 and then i 1 n s i t 1 1 s i t 1 1 i 1 n s i t 1 2 s i t 1 2 otherwise if i 1 n s i t 1 1 s i t 1 1 i 1 n s i t 1 2 s i t 1 2 there must exist s j t 1 1 s j t 1 2 which leads to i 1 n s i t 1 1 i 1 n s i t 1 2 and s i t 1 1 s i t 1 2 subsequently applying the second assumption and the diminishing marginal utility property of cu t 1 with respect to s i t 1 the relationship between marginal values of carryover storage related to s i t 1 1 and s i t 1 2 can be specified as 7a c u t 1 s i t 1 1 s 1 t 1 1 s n t 1 1 c u t 1 s i t 1 1 s 1 t 1 2 s n t 1 2 since i 1 n s i t 1 2 s i t 1 2 i 1 n s i t 1 1 s i t 1 1 7b c u t 1 s i t 1 1 s 1 t 1 2 s n t 1 2 c u t 1 s i t 1 2 s 1 t 1 2 s n t 1 2 since s i t 1 2 s i t 1 1 7c then c u t 1 s i t 1 1 s 1 t 1 1 s n t 1 1 c u t 1 s i t 1 1 s 1 t 1 2 s n t 1 2 c u t 1 s i t 1 2 s 1 t 1 2 s n t 1 2 following the first order condition i e eq 6 and the diminishing marginal utility property of ut we can have u t r i t 1 c u t 1 s i t 1 1 c u t 1 s i t 1 2 u t r i t 2 and then r t 1 t o t a l r t 2 t o t a l therefore the total water availability with respect to s i t 1 1 i e i 1 n w a i t 1 r t 1 t o t a l i 1 n s i t 1 1 is less than that with respect to s i t 1 2 i e i 1 n w a i t 2 r t 2 t o t a l i 1 n s i t 1 2 and the increasing monotonicity between the optimal carryover storage and the total water availability of a parallel reservoir system is derived as illustrated in fig 1 particularly for any total water availability if i 1 n w a i t 1 i 1 n w a i t 2 there should have s i t 1 1 s i t 1 2 and no other factors would affect the carryover storage decision 2 3 monotonic property under stochastic condition the derivation of the monotonic property presented in section 2 2 assumes deterministic inflow of each reservoir at each period however in the real world the inflow of reservoir i at period t 1 i e ii t 1 is imperfectly known at the beginning of the operational period t therefore the monotonic relationship between the optimal reservoir carryover storage and the total water availability of parallel reservoir system needs to be further verified with the consideration of hydrological uncertainty assumed that the inflow into each reservoir can be specified as a markov process then the randomness of the inflow ii t 1 at time step t 1 is addressed through a prior inflow transition probability p i i t 1 i i t given inflow ii t at time step t yeh 1985 labadie 2004 vijay et al 2017 analogy to eq 2 the bellman equation of a typical discretized sdp is 8a c u t s 1 t s n t i 1 t i n t max u t r t t o t a l e x p c u t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 t 8b e x p c u t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 i 1 n i i t 1 p i i t 1 i i t c u t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 where p i i t 1 i i t is the conditional probability of the inflow ii t 1 at time step t 1 on ii t at time step t expcu t 1 is the expected value function representing the maximum expected return accumulated from period t to the final period t conditioned on the initial storage state vector s 1 t 1 s n t 1 and inflow vector i 1 t 1 i n t 1 comparing eq 2 with eq 8a it can be seen that reservoir inflow are also defined as state variables in the value function cut s 1 t s n t i 1 t i n t under stochastic condition which is different from the value function cut s 1 t s n t under deterministic condition however it should be noted that for any given combination of ii t 1 the partial dependence relationship between cu t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 and si t 1 under stochastic condition is considered to be concave as the one under deterministic condition i e 2 c u t 1 s i t 1 2 0 moreover the partially concave dependence relationship between cu t 1 s 1 t 1 s n t 1 i 1 t 1 i n t 1 and si t 1 would lead to the concave relationship between expcu t 1 and si t 1 since transition probability p i i t 1 i i t is non negative and si t 1 and ii t 1 are independent with each other following the similar derivation presented in section 2 1 i e marginal utility principle and section 2 2 i e the two assumptions and diminishing marginal utility property of concave function the monotonic relationship between the optimal carryover storage and the total water availability of parallel reservoir system can also be identified under stochastic condition the monotonic property derived from deterministic and stochastic inflow conditions plays an important role in narrowing the search space of dp and sdp as described in section 4 3 the feasibility of state transition the monotonic property discussed above is applicable when the system is only constrained by flow mass balance taking account of other physical constraints i e storage constraints 0 s i t 1 k i where k i is the storage capacity of reservoir i non negative release r i t 0 and total release constraints 0 r t t o t a l d t where dt is the projected demand of the parallel reservoir system the feasibility of state transition should be enabled before the next iteration search of decision variables otherwise the search can be misled for example the program may end with a negative release if the transition starts a low initial storage and ends with a high final storage note that a traditional dp or sdp program will discrete the storage s between 0 or another specified value and the capacity k and perform all discrete combinations of storage vector labadie 2004 thus the storage constraints will be satisfied but the release constraints will not on the other hand if the non negative release constraint r i t 0 is satisfied then the lower bound constraint of the total release will also be satisfied 0 r t t o t a l thus this section illustrates how to diagnose the feasibility of state transition with the consideration of the constraints of non negative releases and the upper bound of the total release and remove the infeasible transitions from further computing procedures for any given combinations of initial storage and inflow of reservoir i at period t suppose a null release is resulted from the carryover storage s i t 1 then we have 9 s i t 1 s i t i i t r i t s i t i i t s i t 1 by eq 9 the carryover storage should not exceed s i t 1 and then s i t 1 can be used as a criterion for the carryover storage to detect the status of reservoir release for example when any carryover storage is higher than s i t 1 a negative release would be obtained from eq 9 otherwise a positive release is specified on the other hand the value of s i t 1 can be derived from the state transition in eq 9 once the release shifts from non negative status to negative status in a discretized dp and sdp to reduce computation burden all the carryover storage above s i t 1 are removed from further computation otherwise the program will end with a negative release similarly for the upper bound constraint of the total release a different value of s i t 1 will be specified from the equation r t t o t a l d t for each reservoir and all the carryover storage of reservoir i that below the specified s i t 1 will be removed from further computing to avoid the violation of the total release constraint following the description above the carryover storage of reservoir i will be tested with an ascending order from 0 to ki to obtain the value of s i t 1 and remove the cases with carryover storage above s i t 1 which can result in a negative release in contrast a descending order from ki to 0 of si t 1 is used for the consideration of the upper bound constraint of the total release based on these procedures a diagnosis of the status of the constraints of non negative release and the upper bound of the total release can be conducted to enable the application of the monotonic property described in section 2 within the feasible solution space 4 improved dp and sdp for a discretized dp enumeration over all discrete combinations of carryover storage would be exhaustively performed to derive the optimal decisions for a number of individual reservoirs therefore the computational complexity of dp exponentially increases with the number of state variables which is well known as the curse of dimensionality in this section the monotonic property derived in section 2 and the diagnosed procedure for the feasibility of state transition discussed in section 3 are employed to design an algorithm to alleviate the curse of dimensionality via improved dp and sdp denoted as ridp and risdp in the following the general principle for the algorithm design can be stated as searching within the feasible region for the first state combination of initial storage i e 0 0 the minimum storage for each reservoir in this paper and narrowing the search space for the subsequent state combinations i e s 1 t s n t this will take advantage of both the feasibility of state transition and the monotonic property first of all the infeasible state transitions that end with negative releases will be identified and excluded from the calculation process secondly according to the monotonic relationship between the optimal carryover storage or the mean carryover storage under stochastic condition and the total system water availability at period t if the optimal carryover storage at period t 1 s 1 t 1 s n t 1 corresponding to the initial storage at period t s 1 t s n t is determined then only n 1 number of finial storage states namely s 1 t 1 s n t 1 s 1 t 1 i l s n t 1 s 1 t 1 s n t 1 i l need to be tested to derive the optimal solution for the given initial state s 1 t s i t il s n t where il is the interval length used for variable increment in the optimal solution search of ridp and risdp the details of ridp are shown in fig 2 and described as follows 1 initialize state variables for simplicity discretizing storage volume with an equal interval length i e il for each reservoir at each period and number them in the ascending order from 1to nsi t where nsi t is the number of storage discretization for reservoir i at period t 2 search within the feasible region for the first state combination of initial storage i e the minimum storage is set as the state 1 for each reservoir the feasibility of state transition is diagnosed by the following procedure for reservoir i suppose the release i e r i t resulting from initial state 1 and finial state npi t 1 where npi t il corresponds to s i t 1 in section 3 and npi t 1 nsi t 1 would be just shifted from non negative status to negative status then any carryover storage level above npi t 1 will surely lead to infeasible transition as long as reservoir inflow i i t remains constant as a result npi t 1 determines the number of feasible state transitions for reservoir i under given initial storage s i t and inflow i i t to avoid unnecessary computation on infeasible state transitions the searching procedure stops at the state npi t 1 and then moves to test the feasibility of state transition for another reservoir as shown in fig 2a following this procedure the optimal releases and carryover storages are only selected from the feasible region and the computational complexity of this step is reduced from ns 1 t 1 ns i t 1 ns n t 1 of dp to np 1 t 1 np i t 1 np n t 1 of ridp 1 narrow the searching space on the basis of the optimal carryover storage specified from step 2 the optimal solutions for any other subsequent initial state s 1 t s i t il s n t i e nk 1 t nk i t 1 nk n t in fig 2b and nki t il corresponds to s i t are identified from an iterative search approach by evaluating only the optimal carryover storage s 1 t 1 s n t 1 i e nq 1 t 1 nq n t 1 in fig 2b and nqi t 1 il corresponds to s i t 1 corresponding to initial state s 1 t s n t i e nk 1 t nk n t in fig 2b and the state s 1 t 1 s n t 1 with only one interval increment namely s 1 t 1 i l s n t 1 s 1 t 1 s n t 1 i l i e nq 1 t 1 1 nq n t 1 nq 1 t 1 nq n t 1 1 in fig 2b 2 iterate computation repeat step 3 until all the state combinations of initial storage are tested via steps 1 4 a multi reservoir multi period optimization operation problem can be solved by recursive computation the computational complexity of ridp is np 1 t 1 np i t 1 np n t 1 n 1 ns 1 t 1 ns i t 1 ns n t 1 1 whereas that of dp is ns 1 t 1 ns i t 1 ns n t 1 2 specially if only the diagnosed procedure of feasible state transitions is used for algorithm design then the computational complexity of ridp is about np 1 t 1 np i t 1 np n t 1 ns 1 t 1 ns i t 1 ns n t 1 on the other hand if only the monotonic property is used then the computational complexity of ridp is ns 1 t 1 ns i t 1 ns n t 1 n 1 ns 1 t 1 ns i t 1 ns n t 1 1 different from ridp risdp searches the optimal decisions from both the discretized state of carryover storage and inflow scenarios suppose the inflow scenarios are discretized with the same number i e nlt for each reservoir at each period then the computational complexity of risdp can be expressed as np 1 t 1 np i t 1 np n t 1 n 1 ns 1 t 1 ns i t 1 ns n t 1 1 nlt n compared to that of ns 1 t 1 ns i t 1 ns n t 1 2 nlt n of sdp following the procedures listed above the computational load can be significantly reduced especially for a model with a large number of state variables and or a large number of storage discretization as demonstrated in the case study 5 case study the improved algorithms i e ridp and risdp are compared to the primary algorithms i e dp and sdp and the aggregation disaggregation dp i e ad dp proposed by turgeon 1980 through a real world water supply project i e biliuhe yingnahe zhuanjiaolou b y z parallel reservoir system in liaoning province of china 5 1 b y z parallel system description as the largest commercial city in the northeast part of china dalian city suffers severe water scarcity to meet water demand of the increasing population and rapid economic development three reservoirs located on different streams i e biliuhe reservoir b reservoir yingnahe reservoir y reservoir and zhuanjiaolou reservoir z reservoir are jointly operated to meet the municipal and industrial m i water demand of dalian city the layout of b y z parallel system is shown in fig 3 b reservoir is situated in the downstream area of the biliuhe river with an effective storage capacity of 644 59 million m3 and annual runoff of 550 41 million m3 y reservoir is situated in the upstream area of the yingnahe river with an effective storage capacity of 215 58 million m3 and annual runoff of 325 68 million m3 z reservoir is situated in the upstream area of the huli river with an effective storage capacity of 108 27 million m3 and annual runoff of 70 67 million m3 the objective function for the model applied to the b y z reservoir system is set to minimize the shortage index si which is widely used to assess water supply performance throughout the planning horizon tu et al 2008 zeng et al 2014 10 m i n s i 100 t d t r t t o t a l d t 2 where si is a decreasing convex function of the total release with a negative first order derivative and a positive second order derivative with respect to the release historical inflow data from 1958 to 2012 are collected and the cumulative probability function at each 10 day period the operation time step in this study is identified from this long term inflow data following that five annual inflow scenarios of each reservoir are defined on the basis of cumulative probabilities of 10 30 50 70 and 90 percentiles at each operation period which is used to represent very wet wet normal dry and very dry inflow scenarios respectively therefore 5 5 5 125 inflow scenarios are designed in table 1 for the further comparison study for simplicity the water demand of year 2030 is set as 16 04 million m3 for each operation period 10 day without considering the variation from period to period six storage discretization levels i e interval lengths ranging from 10 to 20 million m3 are employed for the algorithm implementation all the traditional and improved algorithms are executed with gun fortran http www codeblocks org on a lenovo thinkpad t470p laptop with intel r core tm i7 7700hq 2 80 ghz 8 00 gb of ram the water supply indicators are given in fig 4 and 5 while the average execution times are displayed in fig 8 and 9 5 2 water supply performances of dp and ridp as can be seen from fig 4a there is no significant difference between the operation results i e si derived from dp and ridp under the various combinations of inflow scenarios and storage discretization levels particularly as shown in fig 4b the optimal values of si corresponding to the three set of inflow scenarios no 66 to 75 81 to 100 and 101 to 125 are close to the values of si derived by dp with a storage discretization level of 10 million m3 in addition both dp and ridp generate the same level of si with different storage discretization levels under the same inflow scenario as shown in fig 4c to compare the results from dp and ridp the results si of dp are used as benchmark and the differences between ridp and dp are shown in fig 5 fig 5a plots the differences for the combinations of inflow scenarios and storage discretization levels as can be seen low differences are found for most scenarios and the average difference over all combinations of inflow scenarios and discretization levels is about 0 20 and the maximum relative error is 9 61 as further illustrated in fig 5b the average difference does not exceed 1 for most inflow scenarios except several inflow scenarios no 76 to 80 see the table 1 where the values of si are extremely low less than 0 05 the maximum difference is 5 71 in addition as illustrated in fig 5c the average differences ranging from 0 01 to 0 45 are below 0 5 for all storage discretization levels and the maximum value is 0 45 therefore the differences between ridp and dp are low under the various storage discretization levels and inflow scenarios although the performance of ridp is affected by the inflow scenarios that result in extremely low shortage index si values to explore the causes for the relative high differences between ridp and dp under inflow scenarios no 76 to 80 fig 6 displays the marginal values of water delivery derived from dp and ridp with the discretization level of 14 million m3 under inflow scenario 77 which results in the maximum difference fig 7 shows the corresponding storage trajectory for each of the three reservoirs as can be seen form fig 6 the marginal costs of water release i e negative marginal values between dp and ridp are equal for most of periods except periods 24 to 26 these differences can be explained from the corresponding storage trajectory of b and y reservoirs in fig 7 compared to the results of dp ridp results in lower carryover storage of y reservoir and higher storage of b reservoir at period 23 and 25 under the operation derived from dp the shortage levels at periods 25 and 26 are lowered down and the marginal cost of water delivery are almost equally allocated during periods 24 to 26 however with ridp the carryover storage of y reservoir is maintained at the lowest level while the storage level of b reservoir is lowered down during periods 24 to 26 thus the marginal cost cannot be equal during periods 24 to 26 since the minimum storage constraint of y reservoir is binding these results imply that the performance of ridp can be restricted when the ratio of the storage capacity of one individual reservoir to its average annual inflow is relatively much lower than that of others 5 3 computing time of dp and ridp the computing time of dp and ridp with different discretization levels are illustrated in fig 8 a and 8b a finer discretization level usually requires longer computation time tejada guibert et al 1993 for the various levels of storage discretization the computing time of ridp is always less than 0 30 of that used by dp for example the time of dp is 8 19 s while that of ridp is 0 02 s for the discretization level of 20 million m3 in addition it can be seen that the computing time of dp increases much faster than ridp with finer discretization levels as shown in fig 9 this observation reflects the description of the computational complexity of ridp vs dp i e dp is proportional to ns 6 and ridp is proportional to ns 3 these results demonstrate that the execution time can be significantly reduced through ridp especially for a high storage discretization levels or a large number of state variables the significant efficiency improvement of ridp is achieved by both the initial search i e the feasibility of state transition and subsequent search i e the monotonic property designed with ridp to evaluate to what extent these two strategies contribute to the efficiency improvement of ridp two diagnosis procedures are conducted one using the feasibility of state transition only named as ridp1 and the other with the monotonic property only i e named as ridp2 the computing time for these two procedures are shown in fig 8a and b the average computing time of dp under different discretization levels is about 2 times of that used by ridp1 while the average computing time with ridp2 is only 1 2 times that used by ridp thus the experiments with this case study reservoir systems indicate that the significant improvement of computation efficiency with ridp is attributed to the monotonic relationship between the carryover storage from period t to period t 1 and the total system water availability at period t 5 4 comparison of ridp with ad dp the effectiveness and efficiency of ridp is further verified through the comparison of ridp with the aggregation disaggregation dp i e ad dp proposed by turgeon 1980 both methods are applied to different inflow scenarios with the same storage discretization level of 10 million m3 first the computing time of ridp is 0 14 s compared to 51 11 s with ad dp second as can be seen in fig 10 the water shortage level resulting from ad dp is higher than that from ridp the result difference between the two methods can be explained from their different optimal solution search mechanisms for ad dp the total release and individual reservoir carryover storage decisions are all made according to a system perspective thus the nonnegative release constraint is considered only after the total release is specified for the equivalent reservoir and a negative individual reservoir release might be taken account into the total release decision of the parallel reservoir system johnson et al 1991 which lead to a relatively poor water supply performance to avoid this situation a relatively homogeneous distribution of reservoir water availability is often required zeng et al 2015 in contrast the heterogeneity of water availability resulting from different reservoir inflows and reservoir storage capacities are allowed with the non decreasing characteristic of storage distribution in ridp therefore the release and carryover storage decisions are made with consideration of the operation performance of individual reservoirs including a full set of constraints on both the system and individual reservoirs 5 5 operation results of sdp and risdp to test the algorithm with a stochastic dynamic optimization problem sdp and risdp are applied to the b y z parallel reservoir system with the storage discretization level of 10 million m3 and the inflow discretization number of 5 for each reservoir at each period the results of sdp and risdp are summarized in table 2 risdp also exhibits high computational efficiency and accuracy the computing time of risdp is only 8 91 s compared to 64 766 88 s of sdp for running the models over a one year time horizon and the difference between the si values from sdp and risdp is 0 39 in addition the computing time of sdp is nearly 125 times of that of dp with the same discretization level of 10 million m3 while the computing time of risdp is only 56 times of that of ridp this reflects the computational complexity expression described in section 4 where the computational complexity of sdp is nlt 3 times of dp nlt is the number of inflow discretization and nlt 5 in this case study but the computational complexity of risdp is less than nlt 3 times of ridp because of the search with the feasibility of state transition used in risdp 6 conclusions this paper presents improved dp and sdp algorithms based on a derived monotonic relationship between individual reservoir carryover storage and parallel reservoir system water availability a diagnosis procedure is further employed to enable the application of the monotonic property within the feasible solution space of the optimization problem taking advantage of these two properties the infeasible state transitions are removed from computation by using certain constraints in the diagnosis procedure non negative release and the upper bound of the total rerelease and the monotonic relationship allows only n 1 number of state combinations to be tested in general for the optimization of a parallel reservoir system with a joint water demand the computational complexity of the ridp is proportional to ns n compared to n s n 2 of a traditional dp ns is the number of storage discretization for individual reservoir n is the number of reservoirs in a parallel system to demonstrate the effectiveness and efficiency of the proposed algorithms the b y z reservoir system in northeastern china is employed as a case study numerical results with the case study show that 1 the ridp is an efficient algorithm under the various combinations of storage discretization levels and inflow scenarios especially for high accuracy of a large number of state variables i e with a high storage discretization level 2 ridp ends with both a better computational efficiency and a larger water shortage reduction than ad dp 3 compared to sdp risdp can also significantly reduce the computing time without much loss of solution accuracy 4 however the efficiency of the proposed algorithms may be limited when the ratio of the storage capacity of one individual reservoir to its average annual inflow is relatively much lower than that of others although the proposed algorithms are designed for the operation optimization of parallel reservoir systems the general principle of the proposed methods i e the monotonic property and the feasibility of state transition can be also used together with other techniques for the complex multi reservoir systems with reservoirs both in parallel and in series acknowledgments this research is supported by the national natural science foundation of china 51609174 91647204 11771058 china postdoctoral science foundation 2016m602359 and the major program of national science and technology support plan of china 2016yfc0402209 2016yfc0400203 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 07 003 appendix supplementary materials image application 1 
589,high sea levels can be conducive to coastal flooding coastal erosion and inland salt water intrusion and thus pose a significant threat to coastal communities ecosystems and coastal assets increases in high water levels have been attributed largely to rising mean sea levels associated with intra seasonal to interannual climate modes of variability such as the el niño southern oscillation enso here we examine the predictability of the seasonal frequency of high sea levels using the niño3 4 index different high sea level quantities are considered at 23 tide gauges along the u s west coast including storm surge and nuisance minor floods at each site we develop a statistical probabilistic forecasting model for seasonal high sea level frequency during the cold period of october march as predictors we compare the use of 1 seasonal niño3 4 index observations over the warm antecedent period of july september and 2 seasonal niño3 4 index forecasts from the north american multi model ensemble nmme over the cold concurrent period of october march results indicate that the niño3 4 observations are a good predictor of seasonal high sea level frequency especially for predicting the storm surge frequency correlation coefficients between the observed and modelled seasonal storm surge frequency range from 0 6 to 0 95 at most of the 23 tide gauges in the predictive model when using nmme niño3 4 index correlation coefficients range between approximately 0 4 and 0 7 at the southern gauges for niño3 4 index forecasts initialized from october to june the skill decreases with lead time our results provide insights into the seasonal predictability of high sea levels using enso patterns which is important for planning and coastal management keywords high sea level niño3 4 nmme nuisance floods storm surges 1 introduction coastal communities ecosystems and infrastructure are under threat of extreme sea level events and flood risk in many parts of the world extreme sea levels are generated mainly by the interaction of mean sea level high tides and storm surges pugh and woodworth 2015 the term mean sea level msl refers to the average level of the sea over a given period relative to the land storm surges are high water levels caused by abrupt changes in atmospheric pressure and wind e g tropical and extratropical storms globally it is estimated that about 200 million people are currently living in areas vulnerable to extreme sea levels from storm surge flooding that number is expected to increase to 800 million by 2080 due to the increase in coastal population nicholls 2010 such a rise in sea level is likely to exacerbate the effects of major storms on coastal communities and infrastructure extreme sea levels have increased worldwide and their interannual fluctuations are driven by changes in regional climate patterns processes such as the el niño southern oscillation enso menéndez and woodworth 2010 along the u s east and west coasts el niño conditions are found to increase mean sea levels and frequency of high storm surges these conditions also cause minor nuisance flooding when they coincide with high tides sweet et al 2014 recent analysis of tide gauge data indicated that nuisance flood frequencies were amplified at 49 locations along the u s west and east coasts during the el niño phase of enso sweet et al 2018 overall it is now well documented that regional sea level variability is associated with climate modes such as enso e g bromirski et al 2003 2011 hamlington et al 2015 roberts et al 2016 sweet and zervas 2011 thompson et al 2013 wahl and chambers 2016 zhang and church 2012 seasonal high sea level hsl forecasting is an important tool for coastal planning and decision making the forecasts can be used for preparedness and awareness purposes as well as to inform annual budgeting and to help establish proactive responses strategies hence it is important to be able to predict the occurrence of hsls including nuisance minor floods that cause damage to low lying infrastructure such as commercial properties houses harbors and beaches existing research has principally focused on projecting long term changes in coastal flooding cayan et al 2008 church et al 2013 little et al 2015 slangen et al 2014 but less is known about the predictability of hsls at shorter i e seasonal time scales the predictability of sea levels has been examined using either dynamical or statistical approaches dynamical models have been used to investigate the predictability of sea level on seasonal timescales e g mcintosh et al 2015 miles et al 2014 roberts et al 2016 for example dynamical coupled ocean atmosphere models have been used to predict seasonal sea level anomalies globally with a lead time of up to seven months miles et al 2014 skillful forecasts of sea level anomalies have been obtained for coastal locations in the tropical indian and pacific oceans up to eight months ahead based on the dynamical predictive ocean atmosphere model mcintosh et al 2015 observations have even been combined with eddy permitting global circulation model experiments to evaluate the drivers and predictability of dynamical sea levels on seasonal to interannual timescales using 15 leading modes of climate variability in the atlantic indian pacific and southern oceans roberts et al 2016 statistical approaches have also proven useful in predicting sea levels chowdhury and chu 2015 chowdhury et al 2007 these methods depend on historical time series of training data to predict the sea level variability at a given tide gauge such approaches have shown that enso and sea surface temperature sst are primary factors in modulating sea level variability in the tropical pacific ocean on seasonal time scales chowdhury et al 2007 the authors used canonical correlation analysis to forecast sea levels over seasonal time scales 3 to 5 month lead times in the u s affiliated pacific islands the model was later improved over longer forecast horizons by adding zonal wind as a predictor chowdhury and chu 2015 in this study we focus on the usefulness of enso patterns for predicting hsl frequency at tide gauges along the u s west coast the aims of this paper are thus to 1 develop probabilistic models relating the occurrence of october march hsl events at tide gauges along the u s west coast to the concurrent enso conditions 2 quantify the skill of the observed october march and the lagged july september enso in forecasting seasonal hsl occurrences at locations where enso was an important predictor 2 data and methodology 2 1 data we use hourly tide gauge data obtained from the center for operational oceanographic products and services of the national oceanic and atmospheric administration coops noaa we selected 23 gauges along the u s west coast that have over 25 complete years of data between 1984 and 2018 a year is defined as starting in july and a complete year is one with over 90 of hourly measurements each year fig 1 we consider three sea level quantities for each tide gauge to evaluate where the enso signal is the strongest the raw sea level time series that include both the tidal and seasonal mean sea level cycles raw sea level the time series representing storm surges obtained after removal of the tidal oscillation component from the raw sea level observations i e non tidal residuals the time series obtained after removal of both the tidal oscillations and the mean sea level seasonality i e raw sea level minus msl minus tides smt mean sea level variability is accounted for by subtracting the monthly sea level moving median at each tide gauge record the removal of tidal oscillations from the observed sea level time series is accomplished by applying a tidal harmonic analysis at each tide gauge record using the 37 noaa tidal constituents we use the tideharmonics r package stephenson 2016 to run the harmonic analysis next we use the daily highest water levels exceeding a given threshold to define hsl frequency two threshold levels have been used at each tide gauge fig 2 the nuisance flood threshold level used only for the raw sea level time series i e the minor flood threshold established by the noaa national weather service at each gauge nuisance flood events are considered a growing problem at many locations along the u s coastline and are increasing in frequency e g sweet and park 2014 vitousek et al 2017 since there are only five gauges in the study area that have a defined official nuisance flood threshold fig 1 we derived the rest of the nuisance threshold proxies by using a linear relationship between the official noaa thresholds and the great diurnal tide range gt tidal datum which refers to the difference in height between the mean higher high water mhhw tidal datum and the mean lower low water mllw tidal datum following sweet et al 2018 i e estimated nuisance level 1 04 gt 0 5 m see sweet et al 2018 for detail on the threshold estimation approach table s1 in the supplemental materials indicates the official and derived thresholds for each of the considered tide gauges in this study the 99 5th percentile of the distribution of the three different sea level quantities i e raw non tidal and smt time series we compute the total annual number of days exceeding these thresholds over the cold season from october to march to produce four time series of seasonal sea level exceedance days denoted hereafter by nuisance sl995 res995 and smt995 nuisance stands for the raw sea level days exceeding the nuisance threshold sl995 res995 and smt995 denote raw sea level days non tidal residual storm surges days and smt days exceeding the 99 5th percentile threshold respectively we choose october march period because most of highest water levels along the west coast occur during that cold period menéndez and woodworth 2010 sweet et al 2018 the sst data are obtained from the met office hadley center hadisst rayner et al 2003 from these data we aggregated the sst values over two separate periods a the cold season concurrent october march sst average for every year year starts in july at each pixel globally b the warm season antecedent july september average sst for every year at each pixel globally we use the warm lagged sst to test whether there is greater predictability from using observed preceding ssts in comparison with the enso forecasts over the concurrent period see the following section recent studies have shown that the predictability of hydroclimatology over north america can be increased when considering antecedent pacific ssts from preceding seasons of the target season e g delsole and banerjee 2017 mamalakis et al 2018 the observed niño3 4 sst index are obtained from the global climate observing system working group on surface pressure rayner et al 2003 2 2 modeling the relationship between hsls and ssts niño3 4 index to examine the predictability of hsls from niño3 4 over the cold season october march we first model the four sea level quantities nuisance sl995 res995 and smt995 at every tide gauge using concurrent and preceding ssts as the predictor at each sst pixel globally at tide gauges where a significant relationship is found with sst anomalies in the central pacific niño3 4 region we then model the sea level quantities using the niño3 4 index in all cases we model the number of days exceeding the thresholds described in section 2 1 using negative binomial regression nbr hilbe 2011 in a probabilistic framework based on generalized additive models for location scale and shape gamlss rigby and stasinopoulos 2005 stasinopoulos and rigby 2015 we chose a nbr model instead of the more conventional poisson regression because of the discrete nature of the sea level data and the large variability in the data we found that the sea level variability is too large to be described by a poisson distribution where the mean and the variance are the same and the poisson distribution can be viewed as a special case of the negative binomial distribution we write ni as the number of days exceeding a threshold in the ith year ni has a conditional negative binomial distribution rigby and stasinopoulos 2005 1 p n i μ i σ i γ n i μ i σ i σ i n i γ μ i σ i γ n i 1 1 σ i n i μ i σ i where ni 0 1 2 μ 0 and σ 0 in this parameterization the mean is equal to the parameter μ and the standard deviation is equal to 1 σ μ the parameter σ is a dispersion parameter that allows us to describe the over dispersion in the data with poisson regression being a special case as σ 0 we consider linear dependencies between the parameters μi and σi and the covariate xsst niño the october march and july september sst anomalies i e niño3 4 index 2 μ i e x p β 0 β 1 x s s t ni n o i σ i e x p γ 0 γ 1 x s s t ni n o i 2 3 niño3 4 index forecasts october march niño3 4 index forecasts are computed using sst outputs from eight coupled global climate models gcms from the north american multi model ensemble nmme kirtman et al 2014 as described in kumar et al 2017 and zhang et al 2017 hindcasts and forecasts of sst data are obtained from eight models cancm3 cancm4 ccsm3 ccsm4 cfsv2 gfdl cm2 1 gfdl flor b01 and nasa geos5 acronyms defined in table 1 these gcms provide forecasts of different variables at monthly resolutions from the early 1980s to the present with lead times ranging from 0 5 to 11 5 months we compute the mean sst forecast over the niño3 4 region 5 s 5 n and 170 120 w using all 94 nmme members from the eight gcms we aggregate the niño3 4 index forecasts over the 6 month period so for example a forecast of the october march niño3 4 index initialized in october will be comprised of the average monthly forecast over the six months i e forecasts for october with 0 5 month lead november with 1 5 month lead december with 2 5 month lead january with 3 5 month lead february with 4 5 month lead and march with 5 5 month lead 3 results 3 1 model results high sea level frequencies for each sea level quantity nuisance sl995 res995 and smt995 were modelled at each tide gauge along the u s west coast using a negative binomial regression nbr model and ssts as the predictor eq 2 the model was fit globally at every sst pixel averaged over the october march cold season and july sept preceding warm season fig 3 illustrates the computed relationship between hsl days exceeding each threshold and the predictors cold season in the top row warm season in the bottom row at the charleston tide gauge oregon supplemental fig set 1a b indicates the model fit for each of the 23 tide gauges for both predictors note that for some tide gauge stations where there were not enough exceedances nuisance distribution we did not fit the nbr model fig 2 we find a positive and statistically significant relationship between the number of hsl days exceeding each threshold and the observed sst in the niño3 4 region for most of the other 22 tide gauge stations fig set 1a b this relationship is illustrated using the μ parameter at the charleston tide gauge fig 3 where we find a positive relationship between both the preceding july september and the concurrent october march ssts and october march sea level exceedances for nuisance sl995 and res995 however the relationship is less strong when modeling the sea level distribution component with msl removed smt995 fig 3 fourth column the positive relationship between high sea level exceedances and ssts suggests that warmer sea waters in the central and eastern tropical pacific ocean correspond to higher sea level frequency additionally these results imply that seasonal sea level frequency along the u s west coast is indeed tied to enso patterns which can be an important predictor in modeling the seasonal frequency of high water levels including nuisance floods and storm surges given the weak relationship between ssts and hsls after removing the msl it appears that enso is not an important predictor when modeling hsls unrelated to msl in other words enso relates to the frequency of hsls primarily via its influence on mean sea levels given the existence of a significant positive relationship between sst in the niño3 4 region and hsls i e nuisance sl995 and res995 at most of the tide gauges we evaluate how well this relationship describes the year to year variations in these quantities specifically we fit a negative binomial regression model using the observed niño3 4 sst index averaged over the concurrent october march and the antecedent july september periods as explanatory variable for the sea level exceedance data fig 4 fig set 2a b across the 23 gauges the probabilistic nbr models capture the interannual variations in sea level data well and are consistent with results obtained from the relationship between sst and sea level frequency of nuisance sl995 and res995 fig 3 we find differences in the model fit depending on whether we focus on the exceedances above the nuisance level or above the 99 5th percentile of the sea level distributions fig 4 moreover results indicate that both the concurrent and the antecedent niño3 4 can be used to forecast seasonal sea level exceedances at the charleston tide gauge correlations between the median modeled probability distribution and observed seasonal hsls at the charleston tide gauge using concurrent preceding niño3 4 are about 0 72 0 74 0 76 0 59 0 64 0 60 for nuisance sl995 and res995 respectively with correlations significant at the 5 level we summarize the model performance at all 23 tide gauges using the 50th percentile of the modelled probability distribution of seasonal sea levels in a taylor diagram in fig 5 taylor 2001 each panel indicates the correlation coefficients centered root mean square between observed and modelled seasonal sea level exceedances and their normalized standard deviations for each sea level quantity i e nuisance sl995 and res995 at most of the 22 tide gauges the correlation coefficients between modelled and observed sea level exceedances range between 0 4 and 0 95 the best scores in terms of correlation coefficients and rmse are obtained when modelling storm surge res995 exceedances the sites with the best model fits are the tide gauges located along the southern u s west coast i e san diego bay la jolla los angeles santa monica for nuisance floods the scores are generally lower than for the other sea level quantities however we find good model fits at several tide gauges e g 4 14 17 2 with the correlation coefficients varying between 0 5 and 0 72 it can also be seen that the standard deviations of the fitted distributions are generally lower than the standard deviations of the observed values the model that uses the concurrent observed cold season october march niño3 4 as predictor slightly outperforms the one that uses of the preceding warm july september niño3 4 in modelling the hsls in all metrics including the standard deviation ratio as one might expect overall these results indicate that both october march niño3 4 forecasts and observed lagged july september niño3 4 can describe seasonal hsls at many tide gauges along the u s west coast the advantage of using the observed july september niño3 4 is that it can be used directly to forecast sea levels at the tide gauges without having to rely on gcm forecasts to predict niño3 4 which might also add to the uncertainties of the model 3 2 forecast results the modelling results indicated that the model with the concurrent cold season october march niño3 4 performed slightly better than the model using the antecedent warm season july september niño3 4 here we investigate the use of seasonal forecasts of the cold season october march to assess whether it is possible to predict hsls several months ahead to do so we first compute the nmme forecasts of the october march niño3 4 index methods we find that the skill of the mean raw october march niño3 4 index forecast ranges from a correlation coefficient of 0 91 when the forecast is initialized in october just before the onset of niño3 4 and decreases as the lead time increases to 0 69 when the forecast is initialized in april 6 5 months before the onset of niño3 4 table 2 the mean nmme niño 3 4 index forecast mean of all the available gcm members is used as the predictor in the models described in section 2 1 the model is trained using the observed niño3 4 index over 70 of each historical gauge record note the length of the record varies depending on the gauge we store the parameters from the trained model and use them to compute the sea level exceedance forecast using the nmme niño3 4 index forecast value as the predictor on the remaining 30 of each gauge record we also trained the model using 80 of each gauge record and tested on the remaining 20 results are provided in the fig s3 in the supplemental material with correlation scores showed some differences but similar patterns across sites the overall forecasting performance is evaluated by comparing the median of the probabilistic forecast distribution for each lead time and the observations for every sea level quantity i e nuisance sl995 res995 and for all tide gauges fig 6 summarizes the forecast skill using the correlation coefficient and rmse between the observations and the median of the forecast distribution of sea level frequency at each tide gauge and for each sea level quantity overall the highest correlation scores are recorded at the southern tide gauges e g la jolla san diego los angeles santa monica san francisco we find that the forecasts are more skillful for the highest storm surge and for the highest raw sea level frequencies compared to the nuisance flood frequency correlation scores for storm surge frequency are relatively high and range between 0 5 and 0 8 at the southern gauges california gauges across all initialization times overall the forecast skill does not exhibit a perfect monotonic decrease as the initialization time increases fig 6 however there are some gauges such as the san francisco gauge where the forecast skill does decrease broadly with increasing initialization time from 0 7 in october 1 month ahead to 0 53 in april 6 5 months ahead of the period of interest if we consider the raw sea level frequencies sl995 the forecast skill is also higher in the southern gauges but the correlation scores are slightly lower than for res995 from 0 35 to 0 82 fig 6 2nd row nuisance forecasts are typically weaker than the high sea level sl995 and storm surge forecasts r ranges between 0 3 and 0 4 broadly except at the san francisco gauge where r 0 7 for the niño3 4 index forecast initialized in august 2 5 months ahead overall these results indicate that nmme nino 3 4 forecasts can be used to estimate seasonal sea level frequency with most accuracy in california and for the seasonal storm surge frequency compared to the other sea level quantities 4 summary and conclusions the primary goal of this study has been to examine the use of enso patterns and sea surface temperature fluctuations in the tropical pacific ocean in predicting high sea level frequency at the seasonal time scale in tide gauges along the u s west coast as outlined in the introduction several studies have shown substantial year to year variability in sea levels along the us west coast induced by the influence of enso on weather and ocean circulation patterns our proposed nbr model shows potential in capturing these variabilities in hsl frequency at seasonal scales we find statistically significant relationships between seasonal hsl frequency and variations of tropical pacific ssts and subsequently the niño 3 4 index at most of the 23 tide gauges using the niño3 4 index we find that the predictability of the seasonal hsl frequency varies at each tide gauge and across different sea level components the enso signal is better captured when we focus on seasonal high storm surge frequency compared to raw sea level sl995 or nuisance exceedances because the tidal component highly influence both sl995 and nuisance high waters sweet et al 2018 higher model skills are found in the southern tide gauges of california one possible explanation for this higher skill is the stronger signal of enso during el niño winters when the storm track and pacific jet displace southward over the eastern pacific seager et al 2010 affecting the southern southwestern united states and mexico the resulting storm surges can cause damage and induce nuisance floods especially if they occur during high tides sweet et al 2014 2018 as one would expect when removing msl variability the predictability of hsls is significantly reduced in most of the gauges since much of the enso related sea level variability that contributes to high water is removed by the subtraction of the monthly median sea level the model fit is slightly better across tide gauges using the concurrent october march niño3 4 index rather than the index during the antecedent warm july september period however in term of predictions using the observed antecedent niño3 4 index as a predictor of seasonal hsl frequency instead of using gcm cold season forecasts will have the benefit of reducing any uncertainty associated with the forecasts in conclusion the main results can be summarized as follows there is an association between enso and the seasonal frequency of hsls this relationship is well captured through negative binomial regression models developed for different hsl quantities our proposed nbr niño3 4 based model provides valuable skill for predicting seasonal hsl frequency along the u s west coast the seasonal high storm surge frequency is the component with the greatest predictability we find higher forecasting skill in the southernmost tide gauges the niño3 4 index can be used to predict hsls ahead of the october march cold season both 1 by using nmme niño3 4 index cold season forecasts and 2 by using the antecedent warm season niño3 4 index observation the nmme forecasts of the october march niño3 4 index can be used to predict exceedance of the seasonal hsls with lead times of up to 6 months at some tide gauges however the skill decreases as the niño3 4 forecast lead time increases as would be expected further improvements could be achieved by developing weighted average schemes that leverage the performance of different models at different lead times these results have important implications for coastal management and can be used for hazard mitigation response as well as to inform annual budgeting in flood prone locations and for emergency mobilizations and proactive response acknowledgments the authors thank two anonymous reviewers for comments and suggestions that have notably improved the manuscript the hourly water level data were downloaded from noaa s tides and currents website https tidesandcurrents noaa gov gabriele villarini acknowledges financial support from the national science foundation career grant ags 1349827 and the usace institute for water resources supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 07 007 appendix supplementary materials image application 1 image application 2 
589,high sea levels can be conducive to coastal flooding coastal erosion and inland salt water intrusion and thus pose a significant threat to coastal communities ecosystems and coastal assets increases in high water levels have been attributed largely to rising mean sea levels associated with intra seasonal to interannual climate modes of variability such as the el niño southern oscillation enso here we examine the predictability of the seasonal frequency of high sea levels using the niño3 4 index different high sea level quantities are considered at 23 tide gauges along the u s west coast including storm surge and nuisance minor floods at each site we develop a statistical probabilistic forecasting model for seasonal high sea level frequency during the cold period of october march as predictors we compare the use of 1 seasonal niño3 4 index observations over the warm antecedent period of july september and 2 seasonal niño3 4 index forecasts from the north american multi model ensemble nmme over the cold concurrent period of october march results indicate that the niño3 4 observations are a good predictor of seasonal high sea level frequency especially for predicting the storm surge frequency correlation coefficients between the observed and modelled seasonal storm surge frequency range from 0 6 to 0 95 at most of the 23 tide gauges in the predictive model when using nmme niño3 4 index correlation coefficients range between approximately 0 4 and 0 7 at the southern gauges for niño3 4 index forecasts initialized from october to june the skill decreases with lead time our results provide insights into the seasonal predictability of high sea levels using enso patterns which is important for planning and coastal management keywords high sea level niño3 4 nmme nuisance floods storm surges 1 introduction coastal communities ecosystems and infrastructure are under threat of extreme sea level events and flood risk in many parts of the world extreme sea levels are generated mainly by the interaction of mean sea level high tides and storm surges pugh and woodworth 2015 the term mean sea level msl refers to the average level of the sea over a given period relative to the land storm surges are high water levels caused by abrupt changes in atmospheric pressure and wind e g tropical and extratropical storms globally it is estimated that about 200 million people are currently living in areas vulnerable to extreme sea levels from storm surge flooding that number is expected to increase to 800 million by 2080 due to the increase in coastal population nicholls 2010 such a rise in sea level is likely to exacerbate the effects of major storms on coastal communities and infrastructure extreme sea levels have increased worldwide and their interannual fluctuations are driven by changes in regional climate patterns processes such as the el niño southern oscillation enso menéndez and woodworth 2010 along the u s east and west coasts el niño conditions are found to increase mean sea levels and frequency of high storm surges these conditions also cause minor nuisance flooding when they coincide with high tides sweet et al 2014 recent analysis of tide gauge data indicated that nuisance flood frequencies were amplified at 49 locations along the u s west and east coasts during the el niño phase of enso sweet et al 2018 overall it is now well documented that regional sea level variability is associated with climate modes such as enso e g bromirski et al 2003 2011 hamlington et al 2015 roberts et al 2016 sweet and zervas 2011 thompson et al 2013 wahl and chambers 2016 zhang and church 2012 seasonal high sea level hsl forecasting is an important tool for coastal planning and decision making the forecasts can be used for preparedness and awareness purposes as well as to inform annual budgeting and to help establish proactive responses strategies hence it is important to be able to predict the occurrence of hsls including nuisance minor floods that cause damage to low lying infrastructure such as commercial properties houses harbors and beaches existing research has principally focused on projecting long term changes in coastal flooding cayan et al 2008 church et al 2013 little et al 2015 slangen et al 2014 but less is known about the predictability of hsls at shorter i e seasonal time scales the predictability of sea levels has been examined using either dynamical or statistical approaches dynamical models have been used to investigate the predictability of sea level on seasonal timescales e g mcintosh et al 2015 miles et al 2014 roberts et al 2016 for example dynamical coupled ocean atmosphere models have been used to predict seasonal sea level anomalies globally with a lead time of up to seven months miles et al 2014 skillful forecasts of sea level anomalies have been obtained for coastal locations in the tropical indian and pacific oceans up to eight months ahead based on the dynamical predictive ocean atmosphere model mcintosh et al 2015 observations have even been combined with eddy permitting global circulation model experiments to evaluate the drivers and predictability of dynamical sea levels on seasonal to interannual timescales using 15 leading modes of climate variability in the atlantic indian pacific and southern oceans roberts et al 2016 statistical approaches have also proven useful in predicting sea levels chowdhury and chu 2015 chowdhury et al 2007 these methods depend on historical time series of training data to predict the sea level variability at a given tide gauge such approaches have shown that enso and sea surface temperature sst are primary factors in modulating sea level variability in the tropical pacific ocean on seasonal time scales chowdhury et al 2007 the authors used canonical correlation analysis to forecast sea levels over seasonal time scales 3 to 5 month lead times in the u s affiliated pacific islands the model was later improved over longer forecast horizons by adding zonal wind as a predictor chowdhury and chu 2015 in this study we focus on the usefulness of enso patterns for predicting hsl frequency at tide gauges along the u s west coast the aims of this paper are thus to 1 develop probabilistic models relating the occurrence of october march hsl events at tide gauges along the u s west coast to the concurrent enso conditions 2 quantify the skill of the observed october march and the lagged july september enso in forecasting seasonal hsl occurrences at locations where enso was an important predictor 2 data and methodology 2 1 data we use hourly tide gauge data obtained from the center for operational oceanographic products and services of the national oceanic and atmospheric administration coops noaa we selected 23 gauges along the u s west coast that have over 25 complete years of data between 1984 and 2018 a year is defined as starting in july and a complete year is one with over 90 of hourly measurements each year fig 1 we consider three sea level quantities for each tide gauge to evaluate where the enso signal is the strongest the raw sea level time series that include both the tidal and seasonal mean sea level cycles raw sea level the time series representing storm surges obtained after removal of the tidal oscillation component from the raw sea level observations i e non tidal residuals the time series obtained after removal of both the tidal oscillations and the mean sea level seasonality i e raw sea level minus msl minus tides smt mean sea level variability is accounted for by subtracting the monthly sea level moving median at each tide gauge record the removal of tidal oscillations from the observed sea level time series is accomplished by applying a tidal harmonic analysis at each tide gauge record using the 37 noaa tidal constituents we use the tideharmonics r package stephenson 2016 to run the harmonic analysis next we use the daily highest water levels exceeding a given threshold to define hsl frequency two threshold levels have been used at each tide gauge fig 2 the nuisance flood threshold level used only for the raw sea level time series i e the minor flood threshold established by the noaa national weather service at each gauge nuisance flood events are considered a growing problem at many locations along the u s coastline and are increasing in frequency e g sweet and park 2014 vitousek et al 2017 since there are only five gauges in the study area that have a defined official nuisance flood threshold fig 1 we derived the rest of the nuisance threshold proxies by using a linear relationship between the official noaa thresholds and the great diurnal tide range gt tidal datum which refers to the difference in height between the mean higher high water mhhw tidal datum and the mean lower low water mllw tidal datum following sweet et al 2018 i e estimated nuisance level 1 04 gt 0 5 m see sweet et al 2018 for detail on the threshold estimation approach table s1 in the supplemental materials indicates the official and derived thresholds for each of the considered tide gauges in this study the 99 5th percentile of the distribution of the three different sea level quantities i e raw non tidal and smt time series we compute the total annual number of days exceeding these thresholds over the cold season from october to march to produce four time series of seasonal sea level exceedance days denoted hereafter by nuisance sl995 res995 and smt995 nuisance stands for the raw sea level days exceeding the nuisance threshold sl995 res995 and smt995 denote raw sea level days non tidal residual storm surges days and smt days exceeding the 99 5th percentile threshold respectively we choose october march period because most of highest water levels along the west coast occur during that cold period menéndez and woodworth 2010 sweet et al 2018 the sst data are obtained from the met office hadley center hadisst rayner et al 2003 from these data we aggregated the sst values over two separate periods a the cold season concurrent october march sst average for every year year starts in july at each pixel globally b the warm season antecedent july september average sst for every year at each pixel globally we use the warm lagged sst to test whether there is greater predictability from using observed preceding ssts in comparison with the enso forecasts over the concurrent period see the following section recent studies have shown that the predictability of hydroclimatology over north america can be increased when considering antecedent pacific ssts from preceding seasons of the target season e g delsole and banerjee 2017 mamalakis et al 2018 the observed niño3 4 sst index are obtained from the global climate observing system working group on surface pressure rayner et al 2003 2 2 modeling the relationship between hsls and ssts niño3 4 index to examine the predictability of hsls from niño3 4 over the cold season october march we first model the four sea level quantities nuisance sl995 res995 and smt995 at every tide gauge using concurrent and preceding ssts as the predictor at each sst pixel globally at tide gauges where a significant relationship is found with sst anomalies in the central pacific niño3 4 region we then model the sea level quantities using the niño3 4 index in all cases we model the number of days exceeding the thresholds described in section 2 1 using negative binomial regression nbr hilbe 2011 in a probabilistic framework based on generalized additive models for location scale and shape gamlss rigby and stasinopoulos 2005 stasinopoulos and rigby 2015 we chose a nbr model instead of the more conventional poisson regression because of the discrete nature of the sea level data and the large variability in the data we found that the sea level variability is too large to be described by a poisson distribution where the mean and the variance are the same and the poisson distribution can be viewed as a special case of the negative binomial distribution we write ni as the number of days exceeding a threshold in the ith year ni has a conditional negative binomial distribution rigby and stasinopoulos 2005 1 p n i μ i σ i γ n i μ i σ i σ i n i γ μ i σ i γ n i 1 1 σ i n i μ i σ i where ni 0 1 2 μ 0 and σ 0 in this parameterization the mean is equal to the parameter μ and the standard deviation is equal to 1 σ μ the parameter σ is a dispersion parameter that allows us to describe the over dispersion in the data with poisson regression being a special case as σ 0 we consider linear dependencies between the parameters μi and σi and the covariate xsst niño the october march and july september sst anomalies i e niño3 4 index 2 μ i e x p β 0 β 1 x s s t ni n o i σ i e x p γ 0 γ 1 x s s t ni n o i 2 3 niño3 4 index forecasts october march niño3 4 index forecasts are computed using sst outputs from eight coupled global climate models gcms from the north american multi model ensemble nmme kirtman et al 2014 as described in kumar et al 2017 and zhang et al 2017 hindcasts and forecasts of sst data are obtained from eight models cancm3 cancm4 ccsm3 ccsm4 cfsv2 gfdl cm2 1 gfdl flor b01 and nasa geos5 acronyms defined in table 1 these gcms provide forecasts of different variables at monthly resolutions from the early 1980s to the present with lead times ranging from 0 5 to 11 5 months we compute the mean sst forecast over the niño3 4 region 5 s 5 n and 170 120 w using all 94 nmme members from the eight gcms we aggregate the niño3 4 index forecasts over the 6 month period so for example a forecast of the october march niño3 4 index initialized in october will be comprised of the average monthly forecast over the six months i e forecasts for october with 0 5 month lead november with 1 5 month lead december with 2 5 month lead january with 3 5 month lead february with 4 5 month lead and march with 5 5 month lead 3 results 3 1 model results high sea level frequencies for each sea level quantity nuisance sl995 res995 and smt995 were modelled at each tide gauge along the u s west coast using a negative binomial regression nbr model and ssts as the predictor eq 2 the model was fit globally at every sst pixel averaged over the october march cold season and july sept preceding warm season fig 3 illustrates the computed relationship between hsl days exceeding each threshold and the predictors cold season in the top row warm season in the bottom row at the charleston tide gauge oregon supplemental fig set 1a b indicates the model fit for each of the 23 tide gauges for both predictors note that for some tide gauge stations where there were not enough exceedances nuisance distribution we did not fit the nbr model fig 2 we find a positive and statistically significant relationship between the number of hsl days exceeding each threshold and the observed sst in the niño3 4 region for most of the other 22 tide gauge stations fig set 1a b this relationship is illustrated using the μ parameter at the charleston tide gauge fig 3 where we find a positive relationship between both the preceding july september and the concurrent october march ssts and october march sea level exceedances for nuisance sl995 and res995 however the relationship is less strong when modeling the sea level distribution component with msl removed smt995 fig 3 fourth column the positive relationship between high sea level exceedances and ssts suggests that warmer sea waters in the central and eastern tropical pacific ocean correspond to higher sea level frequency additionally these results imply that seasonal sea level frequency along the u s west coast is indeed tied to enso patterns which can be an important predictor in modeling the seasonal frequency of high water levels including nuisance floods and storm surges given the weak relationship between ssts and hsls after removing the msl it appears that enso is not an important predictor when modeling hsls unrelated to msl in other words enso relates to the frequency of hsls primarily via its influence on mean sea levels given the existence of a significant positive relationship between sst in the niño3 4 region and hsls i e nuisance sl995 and res995 at most of the tide gauges we evaluate how well this relationship describes the year to year variations in these quantities specifically we fit a negative binomial regression model using the observed niño3 4 sst index averaged over the concurrent october march and the antecedent july september periods as explanatory variable for the sea level exceedance data fig 4 fig set 2a b across the 23 gauges the probabilistic nbr models capture the interannual variations in sea level data well and are consistent with results obtained from the relationship between sst and sea level frequency of nuisance sl995 and res995 fig 3 we find differences in the model fit depending on whether we focus on the exceedances above the nuisance level or above the 99 5th percentile of the sea level distributions fig 4 moreover results indicate that both the concurrent and the antecedent niño3 4 can be used to forecast seasonal sea level exceedances at the charleston tide gauge correlations between the median modeled probability distribution and observed seasonal hsls at the charleston tide gauge using concurrent preceding niño3 4 are about 0 72 0 74 0 76 0 59 0 64 0 60 for nuisance sl995 and res995 respectively with correlations significant at the 5 level we summarize the model performance at all 23 tide gauges using the 50th percentile of the modelled probability distribution of seasonal sea levels in a taylor diagram in fig 5 taylor 2001 each panel indicates the correlation coefficients centered root mean square between observed and modelled seasonal sea level exceedances and their normalized standard deviations for each sea level quantity i e nuisance sl995 and res995 at most of the 22 tide gauges the correlation coefficients between modelled and observed sea level exceedances range between 0 4 and 0 95 the best scores in terms of correlation coefficients and rmse are obtained when modelling storm surge res995 exceedances the sites with the best model fits are the tide gauges located along the southern u s west coast i e san diego bay la jolla los angeles santa monica for nuisance floods the scores are generally lower than for the other sea level quantities however we find good model fits at several tide gauges e g 4 14 17 2 with the correlation coefficients varying between 0 5 and 0 72 it can also be seen that the standard deviations of the fitted distributions are generally lower than the standard deviations of the observed values the model that uses the concurrent observed cold season october march niño3 4 as predictor slightly outperforms the one that uses of the preceding warm july september niño3 4 in modelling the hsls in all metrics including the standard deviation ratio as one might expect overall these results indicate that both october march niño3 4 forecasts and observed lagged july september niño3 4 can describe seasonal hsls at many tide gauges along the u s west coast the advantage of using the observed july september niño3 4 is that it can be used directly to forecast sea levels at the tide gauges without having to rely on gcm forecasts to predict niño3 4 which might also add to the uncertainties of the model 3 2 forecast results the modelling results indicated that the model with the concurrent cold season october march niño3 4 performed slightly better than the model using the antecedent warm season july september niño3 4 here we investigate the use of seasonal forecasts of the cold season october march to assess whether it is possible to predict hsls several months ahead to do so we first compute the nmme forecasts of the october march niño3 4 index methods we find that the skill of the mean raw october march niño3 4 index forecast ranges from a correlation coefficient of 0 91 when the forecast is initialized in october just before the onset of niño3 4 and decreases as the lead time increases to 0 69 when the forecast is initialized in april 6 5 months before the onset of niño3 4 table 2 the mean nmme niño 3 4 index forecast mean of all the available gcm members is used as the predictor in the models described in section 2 1 the model is trained using the observed niño3 4 index over 70 of each historical gauge record note the length of the record varies depending on the gauge we store the parameters from the trained model and use them to compute the sea level exceedance forecast using the nmme niño3 4 index forecast value as the predictor on the remaining 30 of each gauge record we also trained the model using 80 of each gauge record and tested on the remaining 20 results are provided in the fig s3 in the supplemental material with correlation scores showed some differences but similar patterns across sites the overall forecasting performance is evaluated by comparing the median of the probabilistic forecast distribution for each lead time and the observations for every sea level quantity i e nuisance sl995 res995 and for all tide gauges fig 6 summarizes the forecast skill using the correlation coefficient and rmse between the observations and the median of the forecast distribution of sea level frequency at each tide gauge and for each sea level quantity overall the highest correlation scores are recorded at the southern tide gauges e g la jolla san diego los angeles santa monica san francisco we find that the forecasts are more skillful for the highest storm surge and for the highest raw sea level frequencies compared to the nuisance flood frequency correlation scores for storm surge frequency are relatively high and range between 0 5 and 0 8 at the southern gauges california gauges across all initialization times overall the forecast skill does not exhibit a perfect monotonic decrease as the initialization time increases fig 6 however there are some gauges such as the san francisco gauge where the forecast skill does decrease broadly with increasing initialization time from 0 7 in october 1 month ahead to 0 53 in april 6 5 months ahead of the period of interest if we consider the raw sea level frequencies sl995 the forecast skill is also higher in the southern gauges but the correlation scores are slightly lower than for res995 from 0 35 to 0 82 fig 6 2nd row nuisance forecasts are typically weaker than the high sea level sl995 and storm surge forecasts r ranges between 0 3 and 0 4 broadly except at the san francisco gauge where r 0 7 for the niño3 4 index forecast initialized in august 2 5 months ahead overall these results indicate that nmme nino 3 4 forecasts can be used to estimate seasonal sea level frequency with most accuracy in california and for the seasonal storm surge frequency compared to the other sea level quantities 4 summary and conclusions the primary goal of this study has been to examine the use of enso patterns and sea surface temperature fluctuations in the tropical pacific ocean in predicting high sea level frequency at the seasonal time scale in tide gauges along the u s west coast as outlined in the introduction several studies have shown substantial year to year variability in sea levels along the us west coast induced by the influence of enso on weather and ocean circulation patterns our proposed nbr model shows potential in capturing these variabilities in hsl frequency at seasonal scales we find statistically significant relationships between seasonal hsl frequency and variations of tropical pacific ssts and subsequently the niño 3 4 index at most of the 23 tide gauges using the niño3 4 index we find that the predictability of the seasonal hsl frequency varies at each tide gauge and across different sea level components the enso signal is better captured when we focus on seasonal high storm surge frequency compared to raw sea level sl995 or nuisance exceedances because the tidal component highly influence both sl995 and nuisance high waters sweet et al 2018 higher model skills are found in the southern tide gauges of california one possible explanation for this higher skill is the stronger signal of enso during el niño winters when the storm track and pacific jet displace southward over the eastern pacific seager et al 2010 affecting the southern southwestern united states and mexico the resulting storm surges can cause damage and induce nuisance floods especially if they occur during high tides sweet et al 2014 2018 as one would expect when removing msl variability the predictability of hsls is significantly reduced in most of the gauges since much of the enso related sea level variability that contributes to high water is removed by the subtraction of the monthly median sea level the model fit is slightly better across tide gauges using the concurrent october march niño3 4 index rather than the index during the antecedent warm july september period however in term of predictions using the observed antecedent niño3 4 index as a predictor of seasonal hsl frequency instead of using gcm cold season forecasts will have the benefit of reducing any uncertainty associated with the forecasts in conclusion the main results can be summarized as follows there is an association between enso and the seasonal frequency of hsls this relationship is well captured through negative binomial regression models developed for different hsl quantities our proposed nbr niño3 4 based model provides valuable skill for predicting seasonal hsl frequency along the u s west coast the seasonal high storm surge frequency is the component with the greatest predictability we find higher forecasting skill in the southernmost tide gauges the niño3 4 index can be used to predict hsls ahead of the october march cold season both 1 by using nmme niño3 4 index cold season forecasts and 2 by using the antecedent warm season niño3 4 index observation the nmme forecasts of the october march niño3 4 index can be used to predict exceedance of the seasonal hsls with lead times of up to 6 months at some tide gauges however the skill decreases as the niño3 4 forecast lead time increases as would be expected further improvements could be achieved by developing weighted average schemes that leverage the performance of different models at different lead times these results have important implications for coastal management and can be used for hazard mitigation response as well as to inform annual budgeting in flood prone locations and for emergency mobilizations and proactive response acknowledgments the authors thank two anonymous reviewers for comments and suggestions that have notably improved the manuscript the hourly water level data were downloaded from noaa s tides and currents website https tidesandcurrents noaa gov gabriele villarini acknowledges financial support from the national science foundation career grant ags 1349827 and the usace institute for water resources supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 07 007 appendix supplementary materials image application 1 image application 2 
