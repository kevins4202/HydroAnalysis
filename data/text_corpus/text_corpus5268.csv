index,text
26340,as global demand for food energy and water resources continues to increase decision makers in these sectors must find sustainable ways to produce and provide for the growing population while many models have been created to aid in decision making in these systems there is a lack of robust integrated models that enable an understanding of the interconnections of these systems this study develops a modeling framework that explores the connections of the corn and ethanol systems two major food and energy resources a crop modeling tool dssat and a biofuel life cycle assessment tool greet are connected using a service oriented architecture programming approach a python program is developed to connect these two models and run scenario analyses to assess environmental impacts of the integrated system this paper explores the impact of decisions such as fertilizer use and plant population on environmental effects of greenhouse gases energy use and water in the integrated system keywords food water energy nexus crop modeling biofuels decision support system life cycle assessment service oriented architecture 1 introduction with the global population nearing 9 billion people by the year 2050 there is a need to better utilize the food energy and water few resources that are essential to living in the 21st century godfray et al 2010 the population growth puts pressure on the existing resources which must be used more intelligently to support the projected population the food and energy systems of the world are highly interconnected and understanding these interlinkages is vital in sustainably solving the resource demand problem that will perpetuate in the future bazilian et al 2011 energy is essential for producing crops but with the progress of biofuels food is also used as a means to produce energy which can displace the currently used fossil fuels crop modelers have been successful in modeling crop systems such as corn which can be processed into corn ethanol biofuel experts have also developed models for the biofuel system but there is a gap in integrating these two systems using the existing tools this is true for coupling any interconnected system in the few nexus daher and mohtar 2015 explore few modeling using a macroscopic approach their model the wef nexus tool focuses on the environmental impacts of producing a certain food source of a whole region or country by estimating water land energy and carbon requirements the wef nexus tool calculates not only environmental impact but also financial cost of either producing or importing the food source hang et al 2016 on the other hand aims to integrate the few nexus on a local scale rather than regional it is also very general and can be applied to different food sources for a single production system it uses an exergy balance approach to make calculations which gives the total environmental impact of the system using food water and energy researchers have recently been attempting to couple systems to identify interlinkages in the few nexus some studies provide a general framework for systems of given scales and others aim specifically to connect certain agricultural systems the international food policy research institute ifpri developed the international model for policy analysis of agricultural commodities and trade impact as a tool to understand the effect that certain policies and decisions have on sustainability and food security impact integrates economic climate and crop models to simulate the national and international agricultural markets more meaningful assessments of the impacts on the environment food production and economy will result in including all system interconnections robinson et al 2015 the integrated farm system model ifsm is an example of an integrated model for specific agricultural systems as opposed to using general equations that can be applied broadly ifsm integrates the crop and livestock systems to measure the total environmental impact of the decisions made in these sectors the model requires inputs for eight different modules and modifies existing models to create a sustainability assessment rotz et al 2012 the biofuel energy systems simulator bess ties together the crop beef and ethanol systems bess calculates the production of each system and also provides a sustainability assessment over an annual production cycle liska et al 2009 the ceres maize cropping model was integrated with the root zone water quality model rzwqm to couple water quality with agricultural crop production this was done to have a more comprehensive understanding of the interactions between these two systems as a decision support tool ma et al 2006 the markal energy model and center for agricultural and rural development card market model were integrated to properly tie together the crop and biofuel systems it is mainly an economic integration of two models that simulate the market for the biofuels their methodology involved linking relevant inputs and outputs of both models and analyzing the effect that certain events have on the market for fuels and related commodities elobeid et al 2013 an environmental assessment of biofuel production in greece was performed by using the gemis software as an lca of biofuels from crop production to fuel and byproduct production four biofuel products were analyzed and compared based on environmental assessment results fontaras et al 2012 kim and dale 2005 integrated several models and datasets to assess crop biofuel system daycent for soil dynamics nass data for crop production and management greet for ethanol production and the epa traci assessment method for environmental impact validation is a concern for systems models because it is difficult to find empirical data to evaluate and improve the performance of the models using data measured in real world environments tanure et al 2015 developed a bioeconomic model focusing on the crop livestock system by integrating validated equations and creating four sub modules herd structure and animal characteristics animal nutrient requirements weather soil pasture animal integration and economics the individual equations used were pre validated and then the whole model will be validated in a dynamic way by comparing scenarios to case studies of brazilian systems of pasture based beef cattle production the ifsm has been slowly validating its simulations using data found in the midwestern u s making it more accurate for temperate climates but less accurate for tropical climates tanure et al 2015 spatially integrated cropping models can be validated to the usda nass dataset this is done by zhang et al 2010 as their spatially explicit integrated modeling framework seimf data can be compared to geographic crop production data previous attempts at integrated models were developed in the form of stand alone software or programs the individual systems are usually connected by writing the connecting formulas into a single program or running the models manually to develop assessments of the integrated system this is seen in both the ifsm and bess models as it integrates several systems using existing models by rewriting the equations as part of a new program since many individual models already exist in their own software it would be much more practical to integrate them using service oriented architecture soa soa is a way to integrate software components as applications using communication subroutines these models can be integrated as a web service and made available to users and developers as applications this technique has been used for many disciplines to combine several systems together the open geospatial consortium ogc developed a web processing service wps to expose hydrological models as web services castronova et al 2013 users can use an open modeling interface to predict runoff in certain areas by sending inputs to a hydrological model topmodel which is available in a model workflow evapotranspiration calculation is another model that is required in the workflow and an example of integrated models working together on a web service many researchers have used web services and soa to couple climatic and hydrological models to understand the integration of these two systems or for specific end user application such as flood emergency response goodall et al 2011 2013 tan et al 2016 in the field of bioinformatics the european bioinformatics institute embi developed a loosely coupled web service that allows users to access large databases search tools and analysis tools mcwilliam et al 2013 analysis tools such as genetic sequence similarity and biological sequence alignment are integrated via advanced workflows with available data in the data repository users can integrate additional functionality to web sites and programs on the web for social sciences several web services exist to analyze the sentiment and opinions of the public for various online services such as youtube twitter and facebook serrano guerrero et al 2015 these analysis tools are models that are exposed as a web service application and can analyze data for companies to incorporate into their software the models are integrated into a workflow and can be used by developers to build analytical tools agricultural models have not capitalized on soa to improve decision making and to couple systems including the crop and biofuel systems with a loose coupling framework these models can be available for anyone to use independently with their original functionality but can also be used together to run crucial impact assessments more specifically the tool developed through this study will help crop growers and biofuel producers intuitively utilize these models together to understand how different decisions they make can affect the environment on a larger system boundary this will make computation of the environmental impact more practical the soa can be created as a web service that any user is free to use either to write their own software or to call the functions directly the objectives of this study were to 1 develop a framework for integrating two well accepted validated models in the cropping and biofuel systems using a service oriented architecture 2 evaluate scenarios to demonstrate the utility of the framework as a decision support tool 3 conduct a sensitivity analysis to determine effects of varying key model parameters on system response 2 material and methods 2 1 model description 2 1 1 model identification desirable attributes for models are wide acceptance high validity and high functionality the cropping models need to take weather soil and management inputs and output potential yield for that season the biofuel model must take crop yield and key resources in production i e fertilizer irrigation to output not only fuel production but also environmental impacts of resource use and emissions there exist several models that simulate the crop and ethanol systems individually because the crop system is inherent to biofuel production they must be coupled in order to understand the interactions between the systems de carvalho lopes and steidle neto 2011 break down many of the cropping models that are widely used and validated in their respective fields some of these models include ceres cropgro environmental policy integrated climate model epic hybrid maize and apsim additionally the decision support system for agrotechnology transfer dssat is a program that acts as a wrapper function for many different cropping models including the popular ceres and cropgro hoogenboom et al 2015 all tools successfully model the cropping system of interest and are meant to be used as decision support systems for either private decision makers or policy makers the spatial and temporal scales and equations may differ from model to model but the functionalities are similar dssat is used in more than 100 countries and has been under constant development for more than 20 years dssat also has a high volume of data backing up calibrating its models so it can be used to predict crop yield in a variety of locations with high validity there are over 60 inputs that can be plugged into the dssat model which shows how functional it can be for researchers and if the model can be properly wrapped dssat can offer high functionality into the integration of the crop and biofuel systems dssat has also been integrated with other models in the past the impact model for example integrates dssat for their crop production module when determining the potential for food supply ifsm uses equations found in dssat to calculate their crop yields dssat is not only high in functionality but researchers have experience in integrating it with other systems as well making this the best tool to use for our application since biofuel production is closely associated with environmental impact most of these models are life cycle assessments lca in order to determine the environmental effect benefit of producing a certain type of fuel from raw materials to consumption some well known fuels lcas include the greenhouse gases regulated emissions and energy use in transportation greet model simapro the global emission model for integrated systems gemis and the market allocation model markal the greet model is widely used largely because it is sponsored by the us department of energy elgowainy et al 2012 and argonne national laboratory consistently updates the data that drives the model the latest major update to the corn ethanol pathway was in 2014 and the current greet version used is greet 2015 wang et al 2014 when performing lca with greet it includes the crop farming pathways which give some functionality to the user in defining cropping inputs for example the corn ethanol pathway includes the corn farming process which allows the user to specify irrigation fertilizer and chemicals per unit weight of corn produced the markal model behaves in a similar way to calculate environmental impact of biofuel production with a large database it also includes a linear programming algorithm that helps the user in determining the least cost solution on the energy resources to use kannan et al 2007 the purpose for this model however is not to find an optimal solution for the user but to calculate the system impact from decisions defined by the user markal also does not have near the user base or support that greet has given its numerous benefits greet is the chosen biofuels model dssat and greet are also capable of simulating a wide range of different crops and biofuels this study will focus on integrating the corn and ethanol systems but the same principles can be applied to a variety of biofuel systems such as biodiesel from soybean or ethanol from corn residue 2 1 2 computational structure the soa will follow the framework described in fig 1 the middle layer shows the model pipeline and at the very bottom are classes that create inputs from user actions these feed into the control and batch methods that create the crop experiment file and batch file for the dssat model under the dssatfile class dssatfile is inherited by the dssatmodel class which runs the file created from the user inputs dssatmodel returns yield and all other outputs associated with dssat this is inherited by the ds greet class which runs the greet model biofuel pathway based on the crop outputs simulated by dssat every module of this structure can be called by a client on a representation state transfer rest server any user that has access to the server can make requests to the server to utilize the integrated model in the middle layer the integrated model interacts with a data repository to store and access data that are used in the subroutines 2 1 3 dssat wrapper the cropping model in this integrated model should be able to simulate crop yield based on several environmental factors and management practices that the user controls creating a wrapper for the dssat model allows the program to run and be connected to other programs which makes it viable for integration pydssat is a python wrapper that runs the dssat model in its original fortran code he et al 2015 the program makes a large number of input assumptions and also wraps the model by compiling the original fortran source code the program is open source so it was used as a reference to write the python code used in our wrapper the method for writing the dssat wrapper follows a similar flow to pydssat and is shown in fig 1 it must create an experiment file based on farming inputs run the file through the model and process the outputs the experiment file used by dssat is a text file that compiles the inputs in a certain format for the simulation that the user wants to run in our implementation this is created by making a python class dssatfile the pydssat code allows the user to input the following variables crop type soil type weather station start year end year planting date and model mode the integrated model only simulates one year so end year is not needed it is also only being run in batch mode where a single experiment file is being used so the mode is defaulted as b for batch since dssat is robust and has many inputs it is difficult to fit them all as inputs into one class practically other classes were created to enter management data as comma separated files csv files irrigation fertilizer harvest tillage and chemical application all have their own class defined to take in scheduling inputs written to a csv file the dssat model can be run in the command line using dscsm046 exe which is available with installation of the software pydssat runs the model by compiling fortran code but running the batch file in the command line is simpler and provides the same results the pydsssat class dssatmodel runs the model by calling the executable in the terminal and taking in data from the experiment file and the management files the model controller finishes writing the experiment file that did not yet include the management inputs from the irrigation fertilizer harvest tillage and chemical classes after the model is run it creates output files that show results for yield crop growth soil water balance and more 2 1 4 greet wrapper the objective of the biofuel model is to not only calculate the total biofuel production from crop production but also the resource use and emissions associated with the biofuel in comparison to a reference fuel to understand the environmental impact of producing the clean burning fuel the biofuel pathway is compared to the reformulated gasoline pathway greet can compare lcas of different fuels such as ethanol and gasoline on a per mj basis the pathways in the greet model used for this tool are the e85 gasoline blending and transportation to refueling station pathway and the reformulated gasoline e10 blending and transportation to refueling station pathway the ethanol pathway is broken down into several processes and calculates all results associated with this pathway it begins with the corn production for biofuel refinery process which includes some inputs and outputs found in dssat this is where the dssat inputs are written into the greet model when appropriate the results feed into the ethanol production process which is chosen as dry mill ethanol production w corn oil extraction since this is a common method in many ethanol plants in the u s while the user can specify the percent of ethanol being produced from each process in the software for this study it was assumed that 100 of the ethanol produced is from dry mill with oil extraction greet calculates emissions from the beginning of the corn farming process to the end of the transportation process to show final resource results for the pathway while many inputs in greet overlap with dssat inputs others are more difficult to estimate the rest of the inputs require the user to know how energy and resources as a whole are used for the rest of the operations such as vehicle usage tillage and transportation greet also has numerous libraries and pathways for resources that can be edited the defaults for these inputs are us averages so for the purposes of this study the default energy uses were kept to make the tool more intuitive greet uses terminology called well to pump wtp and pump to wheel ptw to describe where the product is in its life cycle the wtp pathway describes resource use and emissions associated with the production of a product from its raw materials to transporting it to a fuel pump ready to be consumed the ptw pathway describes resource use and emissions associated with the consumption of the product this study will look at the full well to wheel wtw pathway of the corn ethanol lifecycle the corn ethanol pathway in greet uses a displacement method by default and distillers grains and solubles dgs displaces animal feeds which is a mix of corn soybean meal urea and soyoil for different types of livestock the displacement values per feed and livestock are averaged and used as the allocation amount to credit to dgs byproduct because of the displacement of soybean meal and soybean oil the water consumption associated with them is taken as credit which may result in a negative wtp water consumption value if irrigation amount is too low this must be considered when making conclusions from the greet results also while dssat calculates some environmental footprints such as water use for corn greet already takes into account the footprint produced from corn farming hence any environmental assessment done in the integrated model is done based on greet values not dssat greet operates based on calculating environmental results from ethanol amount but the dssat model only provides corn yield the program must convert kg ha of corn to total liters l of ethanol based on greet s current conversion rate of 10 598 l ethanol bushel corn since a liter of e85 does not provide the same amount of energy as a liter of e10 the volume calculated must be converted into energy mj further the crop farming inputs in greet are on a per unit weight of crop produced basis while the user inputs these values on a per hectare of field basis in dssat irrigation and fertilizer inputs from dssat are converted accordingly to be written into the greet file calculatorbatch is a program that was compiled using an api developed by the makers of greet the arguments for this program are the greet file name year s of simulation and the id for the fuel pathways or mixes the greet wrapper runs the written greet file into the calculatorbatch program using the values specified by the farming system there are two pathways being used corn ethanol production and e10 reformulated gasoline so the greet model is ran for both pathways using the amount of energy produced in ethanol production while this api is a useful tool and essential for the development of this wrapper it is only able to calculate resources for wtp and does not include the results from actual usage of the fuel much of the offset in emissions and resource use comes from the burning of the biofuel in comparison to fossil fuel so it is essential to include the ptw pathways as well fortunately only the wtp pathway results are variable due to the cropping inputs of irrigation fertilizer and chemicals the ptw results do not change on a per mj basis since those are completely separate from production and only dependent on the vehicle that is consuming the fuel the results for these ptw pathways on a per mj basis were stored in a separate class wherever there were ptw emissions present once the total amount of ethanol energy is calculated from the beginning of the program it is input into the ptw class to output total resources the vehicles chosen for both pathways were ones that are commonly found for their respective fuels flex fuel vehicle ffv was used for ethanol while standard ignition internal combustion engine vehicle si icev was used for e10 reformulated gasoline flex fuel vehicles have engines that are able to burn fuels that have higher blends of ethanol which is why they are commonly used for e85 alternative fuels data center 2017 e10 gasoline is similar enough to gasoline that a standard internal combustion engine found in most vehicles can run it anjikar et al 2017 after adding results from the api along with results from the ptw class the program will output total wtw results for ethanol and gasoline the total flow of the program from inputs to outputs is represented in fig 2 2 2 post processing the design for post processing the resource results is based on wu et al 2007 who evaluated the benefits of using ethanol relative to gasoline for fueling vehicles the main outputs that this study analyzes are greenhouse gas ghg emissions energy usage air pollutants and water consumption ghg emissions are calculated as co2 equivalent from co2 ch4 n2o and biogenic co2 based on their global warming potential values 1 25 296 and 1 respectively ipcc 2007 the criteria air pollutants identified when comparing the two fuel pathways are volatile organic compounds voc co nitrous oxide nox particulate matter pm10 and sulfur oxide sox these are all significant emissions produced in the corn ethanol pathway that affect human health the energy savings from nonrenewable fuel use are also important to analyze as total fossil fuel energy and petroleum fuel energy use are compared in this model fossil fuel energy consists of total use from coal natural gas and petroleum energy in the life cycle petroleum fuel use is singled out from the other fossil fuels because most of the benefit from producing ethanol comes from the reduction of petroleum fuel water consumption is highly sensitive to irrigation in the crop pathway and can be beneficial or detrimental based on management and weather after the yield is calculated in dssat in kg ha it is converted to ethanol in mj to compare the ethanol production to gasoline an equivalent amount of mj of gasoline is calculated in the greet life cycle and the two pathways are compared to each other in this study the functional unit of comparison is total mj of e85 possibly produced by 1 ha of corn production this effectively defines the system boundary to start with production of cropping inputs and end with the consumption of e85 fuel in a vehicle the lca tracks fertilizer and chemical inputs down to energy inputs of raw materials such as ammonia and phosphate environmental footprint of raw materials for ethanol production are tracked for enzymes yeast and chemicals blending gasoline for e85 is also tracked along with its raw inputs the system boundary ends once the fuel reaches the vehicle that uses it and is consumed since dgs displacement of livestock feeds is credited the boundary also includes consumption of dgs and its environmental benefits there are two ways to compare the footprint of e85 fuel to e10 fuel one is by total footprint and the other is percentage footprint of e10 simply subtracting the results from the e85 gasoline pathway from the e10 pathway provides the absolute savings percentage savings is the percent of the resource saved by using e85 as opposed to e10 this provides a normalized value that can be compared across different categories negative values for either method show that producing ethanol is beneficial to gasoline in that category wu et al 2007 used percentage savings to compare several different fuel pathways on a per unit of distance basis it is important to understand both methods when making the assessment since absolute savings encourage larger crop and biofuel production operations while percentage savings encourage more efficient operations on a per unit of energy basis this gives the user more options on how they can make operations decisions that may be more environmentally sound as an additional note water footprint is commonly used referred to as a way to track the use of water in different phases and stages of a process in this paper water footprint is defined as the difference in total water consumption between e85 and e10 life cycles a framework for water tracking was not included for this paper in the greet model 2 3 scenario analysis 2 3 1 base scenario for comparison the driving inputs of both models are nitrogen amount from fertilizer application and water use from irrigation this is because these management inputs have a high effect on yield depending on the growing conditions and have a high environmental footprint from life cycle production and use a sensitivity analysis of the effect of fertilizer on the integrated model was conducted to determine the impact on the system from changing one of the parameters a base scenario for corn production is developed from regional data and management recommendations of corn farming in the eastern nebraska region this scenario is based only on environmental and user inputs as greet default values for emissions and energy use are kept the same the sample field location and year used are mead ne and 2015 weather data for this season were obtained from the high plains regional council center hrpcc in the form of daily precipitation minimum temperature maximum temperature relative humidity solar radiation and location coordinates the plant population for irrigated corn that is typically used in the midwestern region is approximately 10 plants m2 barr et al 2013 anhydrous ammonia is a common fertilizer material used for nitrogen application so this method was used for the baseline scenario the corn hybrid chosen for this study called gdd2600 has a 113 day maturity medium season hybrid from planting to physiological maturity the soil type used should accurately reflect the profile found in eastern nebraska corn fields so the soil file in dssat chosen is for loamy soil this soil file gives descriptions by layer based on water holding capacity density and nutrient characteristics the water holding capacities were adjusted based on local data to more accurately reflect the nebraska soil profile unpublished research data values for field capacity fc wilting point and organic carbon matter omc are represented in table 1 planting and harvesting dates are based on data from the usda national agricultural statistics service nass in eastern nebraska recommended times to plant corn are between april 27th and may 18th while the recommended times to harvest corn are between october 4th and november 10th usda 2010 thus the dates chosen for planting and harvesting are may 1st and october 15th respectively fertilizer amounts used in the simulations were based on the amounts used in long term field research conducted by irmak 2015a 2015b in nebraska it is common to apply fertilizer using side dressing and especially since it is being applied on the same day as planting it is used for this scenario in dssat this application method is named banded on surface phosphorous and potassium are normally not included in fertilizer for corn so these were left out of the simulation dssat has an option to automatically irrigate the field if it senses water stress throughout the season water stress for yield should not be a constraint for this simulation so irrigation was simulated using automated irrigation option so that water is not a constraint for achieving potential grain yield the state of nebraska has a large aquifer and uses groundwater for irrigation nebraska commonly uses center pivots to irrigate their fields so the equivalent setting in dssat was set to sprinkler all baseline values are recorded in table 2 pesticide use is an option for this integrated model but was left out of this case study since dssat does not handle pest hazards well when predicting yield if pesticide use were to be included the integrated model would only calculate the negative environmental impacts from greet and leave out the positive yield effects it has on cropping in this scenario it is assumed that there are optimal conditions for pests and weeds so pesticide chemicals are not used tillage settings are also important to consider in this integrated system and in dssat it is defined as drill no till this is not defined in greet however due to the difficulty of adding vehicle fuel use in the tool 2 3 2 nitrogen fertilizer sensitivity the production of corn has several sources of emissions such as soil emissions fuel use and chemical applications the corn production process in greet has default values of 2 40 g bu 0 095 g kg and 2 73 g bu 0 107 g kg of nox and n2o emissions respectively due to soil and fuel use fertilizer applications have high added environmental impact and increase the lca for corn ethanol s emissions nitrogen amount was analyzed relative to the baseline value of 150 kg n ha which is normal for this field the following values for nitrogen application were run in the model assuming the baseline scenario for all other variables 0 50 100 150 200 250 and 300 kg n ha resource savings from producing e85 over e10 were calculated both as a total difference and as a percent reduction total footprint shows the physical results and changes with each scenario while percent footprint shows the sensitivity of output parameters to nitrogen fertilizer application the pathway is separated into wtp and ptw which both have different contributions to the overall results to quantify the difference the program was altered to calculate emissions from wtp and ptw separately the two results of ghg emissions and nonrenewable energy use were compared to each other based on these different pathways 2 3 3 optimal plant population a range of plant populations was used to gauge the sensitivity of this input on the dssat model there are significant breaks in the relationship between plant population and yield for values under 40k plants ha and over 120k plants ha so this sets the bounds for values of plant population table 3 therefore the model was run for plant populations in the range of 5 12 plants m2 over the span of 15 years from 2001 to 2015 this will show the relationship between varying weather and the optimal plant population as a possible decision point for farmers with higher plant population in general higher nutrient application is required each plant population was run with a range of nitrogen fertilizer amounts to determine the optimal value nitrogen fertilizer values from 10 to 600 kg n ha were used to show the full relationship of nitrogen to yield for this plant population and the maximum described by dssat the optimal nitrogen application amount was determined by picking the value where yield exceeds 90 of the max as shown in fig 3 this was done to ensure that a consistent fertilizer value was chosen for each plant population depending on the nutrient requirements once nitrogen amounts are picked for each plant population they were used as input into the model to calculate resource results for each scenario since nitrogen amount increases with plant population the results should reflect an increase in ghg footprint however if the yield gains offsets the nitrogen use then the ghg footprint can actually be reduced to show the complete ghg footprint matrix in relation to plant population and nitrogen fertilizer application combinations of both variables were run plant populations were inputted in the range of 5 12 plants m2 and nitrogen applications were inputted in the range of 100 250 kg n ha with increments of 1 plants m2 for plant population and 10 kg n ha for nitrogen applications the ghg footprints of each scenario are calculated to show the relationships with both of these variables simultaneously 3 results and discussion 3 1 model verification before performing analyses the python program must be verified with the models in the original software verification of the dssat wrapper is done by running the model twice using the same scenario both manually through the dssat software and with the inputcreator class in the wrapper both programs produce a summary file that shows generic outputs of the simulation run including yield seasonal weather data soil water balance and management summary three scenarios with three different nitrogen fertilizer amounts 0 100 and 200 kg ha were run in both programs and the summary files exactly matched which verifies the accuracy of the wrapper verification on the greet model is done in a similar way by running the software manually using a scenario and then running the same scenario with the greet wrapper written in python the wtp pathway is run with the wrapper so that part is validated with the software the results in the wrapper are based on total emissions from the amount of energy produced in dssat since it is inherited so the scenario that is run manually through greet must be done on the basis of total energy produced the same baseline scenario is run through the program to get results from the greet model only the wtp results are printed to compare to the results obtained from manual use of the model ptw calculations are linear and directly taken from the model so these do not need to be verified the comparisons are shown in table 4 where results are calculated on the basis of the amount of energy created from the ethanol process the error between the two results is minimal and is attributed to rounding error from the greet software 3 2 sensitivity and scenario analysis 3 2 1 sensitivity analysis on nitrogen fertilizer fig 4 a and b shows the results for nitrogen application on ghg emissions and nonrenewable energy use overlaid with the corresponding yield up to 100 kg n ha increasing nitrogen application increases the yield potential and lowers the ghg footprint of the system since more e85 energy can be produced to displace e10 however yield levels off after this value and can no longer offset the footprint caused by increasing nitrogen rate therefore the ghg footprint of the system will sharply increase beyond 100 kg ha because these emissions are highly sensitive to nitrogen fertilizer use as displayed in fig 4a the total footprint of both nonrenewable energy categories in fig 4b shows a steady increase that evens out past 150 kg n applied per hectare fossil fuel and petroleum fuel footprints appear to closely mirror the relationship between yield and nitrogen rate this means that these footprint categories are more sensitive to the amount of e85 produced than the amount of nitrogen applied the petroleum fuel footprint is the lowest as e10 requires much more petroleum in production than e85 on a per mj basis the total fossil fuel footprint is not as low because e85 is detrimental higher footprint in comparison to e10 in its coal and natural gas footprint fig 4b shows this as the petroleum fuel continues to steadily decrease with nitrogen application while the total fossil fuel footprint starts to increase as yield levels off the sensitivity is more clearly shown in fig 4c that normalized footprint for each category in comparison to the optimal fertilizer rate according to ghg footprint the trends look different in this subfigure because it is comparing all scenarios to a central value of 150 kg n ha the percent change in footprint is much higher for ghg across the range of nitrogen rates in comparison to the fossil fuel categories petroleum fuel also shows very little sensitivity to nitrogen which confirms the findings from fig 4b this integrated model can inform decision makers on the optimal fertilizer application not only based on yield but based on environmental impact in the model yield would continue to increase with increasing nitrogen but with ghg savings there is a clear optimum value for this scenario at 100 kg n ha while e85 is beneficial to e10 for all of the categories described above it is not as beneficial in emission of criteria pollutants as found in fig 5 in every scenario and for each category e10 gasoline emits fewer pollutants than e85 with increasing nitrogen application the emissions footprints become more positive this shows one of the tradeoffs of producing ethanol when using it to displace reformulated gasoline as a fuel each pollutant shows a similar increasing trend in fig 5b where emissions compared to the middle nitrogen scenario of 150 kg n ha nox emissions are impacted the greatest with increasing nitrogen which is reflected in greet s nitrogen and corn production pathway in the nitrogen mix pathway producing 1 kg of nitrogen emits 7 46g nox and in the corn pathway 1bu of corn emits 2 40g nox water consumption in the crop biofuel system is influenced by irrigation amount and the amount of water used in the ethanol plant for producing ethanol processing water consumption for ethanol is greater than gasoline marginally by about 0 076 l mj however irrigation has the most significant impact and the practices that a decision maker has can greatly influence the water footprint of corn ethanol in this scenario irrigation is automated when needed if the crops grow larger due to increased nitrogen then more irrigation may be needed to maintain the growth and should reflect larger consumptive use fig 6 a shows the total water consumption from the e85 pathway in relation to nitrogen application rate water consumption has a negative trend with nitrogen even though yields are increasing and consumption is expected to rise the results indicate that dssat may not be able to properly handle the relationship between nitrogen and water consumption climatic conditions influence how increased nitrogen levels impact crop physiology yield and evapotranspiration which is not a linear response in this scenario the annual precipitation is 731 mm which is high for this region the percent footprint though does show a clear downward trend of water footprint with higher fertilizer the increase in yield creates a lower water footprint and offsets the use from irrigation which does not change as much fig 7 shows the breakdown of the footprints between the wtp pathway and the ptw pathway for ghg and fossil fuel categories the production of ethanol emits more co2e than gasoline on a per mj basis and that is reflected in the wtp results where emissions increase as nitrogen use increases conversely the ghg emissions for the ptw pathway decrease due to consumption of ethanol as e85 use emits much less ghg s than e10 and offsets the ghg increase from the wtp pathway as seen in the full wtw pathway the footprint is negative through this range of nitrogen application but as it gets higher the ptw footprint levels off and wtp footprint continues to increase at a near constant rate the total wtw pathway shows a net negative footprint for ethanol which shows the offsetting effects of clean consumption in vehicles ptw footprint does not have an impact on any of the other categories the differences between e85 and e10 come only from the wtp pathway in all of these resources this is because in the wtw analysis the vehicle used for both e85 and e10 fuels are kept the same since the engine that is burning the fuel is standardized there is no difference in energy water or air pollutant savings per mj of e85 and e10 for example fig 7b shows that fossil fuel footprint only changes with the production of the fuel not the consumption ghg emissions is the only category where there is a difference between e85 or e10 consumption because of the composition of the fuel 3 2 2 optimal plant population fig 8 shows the relationship between total resource and emissions footprint and plant population the fossil fuel energy footprint steadily decreases with increase in plant population this is because based on the assumptions made within the dssat model the increasing nitrogen application does not have as much of an offsetting effect on the energy footprint from producing with a higher plant population ghg emissions footprint on the other hand is heavily influenced by nitrogen application and that shows in fig 8a where it steadily increases the ghg footprint levels off after 9 plants m2 because nitrogen levels are also leveling off as shown in table 3 much like in section 3 1 1 the criteria pollutant footprints increase with plant population and are a net positive compared to using e10 gasoline with this scenario analysis the decision maker may weigh the environmental trade offs when determining plant population as it has differing effects on these categories while fig 8 uses predetermined nitrogen values for each plant population fig 9 shows the relationships of ghg footprint to nitrogen fertilizer and plant population the ghg footprint seems to be optimal with lower nitrogen application and higher plant population density this seems to be the ideal decision for nitrogen application and plant density to provide the best ghg footprint within the system the ghg footprint is highest when the plant population is very low and the nitrogen application is very high this is because the low population density is not producing enough ethanol to offset ghg emissions created by the high fertilizer use 3 3 end user application this program for integrating the crop and biofuel systems is primarily purposed as an environmental decision support tool both dssat and greet have large user bases who use these tools to make informed decisions based on either production output or environmental impact de carvalho lopes and steidle neto 2011 kopanos et al 2017 connecting the two creates a powerful tool that allows the user to do both and understand the impact that their decisions have on the integrated system many scenario analyses on the impacts of nitrogen fertilizer and plant population on corn growth have been done in the past there are experimental field studies that test these variables against each other to determine their effect on grain yield for different years and different locations blumenthal et al 2003 found a linear relationship between plant population density and grain yield and a quadratic relationship between total soil nitrogen and grain yield in western nebraska counties ping et al 2008 studied the relationship between site specific nitrogen and population density management and economic performance which showed little impact though it did increase nitrogen use efficiency irmak and djaman 2016 concluded a positive relationship with plant population density and grain yield with mixed results for evapotranspiration all of these results contribute heavily to understanding the effect that nitrogen application and plant population have on growth and economics of the farmer but it is still in the closed system of the individual farming operation this integrated tool provides a way to do these scenario analyses to understand not only how these variables can affect growth but also the impact on emissions resource use and the environment when coupled with the ethanol system this is a way to use the valuable crop research that is done to produce cropping models for the understanding of a larger system likewise lca users will have a better understanding of the tradeoffs of using biofuels with a more functional corn farming pathway greet currently has a corn farming process included but is limited in inputs the default values are derived from usda nass data which averages management practices from regions of completely different climates and land integrating the dssat model allows the user to make greet location specific which can be a valuable tool for businesses that operate close to biofuel plants in various regions future work can include scenarios that run in different locations to analyze how different areas affect the environmental impact of producing biofuels this study is focused on corn ethanol production in the midwest but the corn ethanol system is also prevalent in further stretches of the great plains and the south a database could be created of these various locations and based on the weather and land differences the model can also run scenarios for various other biofuels it includes biodiesel from soybean oil and ethanol production from corn stover sorghum and forest residue feedstocks dssat includes models for soybean and corn stover production so additional scenarios can be run to better understand how these systems are interconnected in comparison to corn ethanol and reformulated gasoline corn stover is an important addition to the system as it is tightly connected to both the corn production and biofuel systems cellulosic ethanol plants are commonly found in the midwest united states and can result in further emission savings from corn production residue from crop yields can be simulated using dssat and distributed to have a more accurate understanding of the emissions within this integrated system there are limitations to this tool some of which are inherently attributed to the individual models dssat s large user base mostly consists of researchers and scientists who do not make field decisions in hopes of selling their product dssat is also limited as an irrigation scheduling tool the automated irrigation setting does not give the user many options to adjust for strategies such as implementing deficit irrigation greet also does not incorporate irrigation pumping costs when calculating ghg emissions and energy use for the corn ethanol pathway many producers consider irrigation pumping a large expense especially for drier regions where irrigation is required greet is limited to calculating direct emissions from each system and as a result indirect emissions were not taken into account for this study a user can add emissions from indirect sources but this study focused specifically on the capabilities of the two models used the framework allows for future work to explore the possibilities of adding indirect emissions from other models greet also does not consider methane emissions from irrigation and other agriculturally related emissions so those were left out of the scope of this paper this framework is also mainly a tool used for environmental assessment and does not include profitability decision makers are more likely to act based on their best financial interest even if there is an environmentally optimal solution a more applicable tool will include economics in addition to environmental impact so that a user is more likely to use it modelers and researchers have been developing and finding new ways to practically integrate agricultural production systems this paper offers the methodology and framework for a tool that can connect two well established models in their respective fields in the generation where apis and model wrappers are taking over the world of software and technology it is important that agricultural models and software can keep up with this trend zhong et al 2009 programmers have made many tools to facilitate the connection of various programs which were utilized in this paper the greet api the compiled dssat program and the various python packages used in the wrapper the method also allows for the models to be automatically updated as the developers continue to work on the individual models other researchers may do similar studies and run their own holistic scenario analyses like how nitrogen fertilizer application was connected to total ghg emissions this soa offers users the ability to utilize these tools practically to make their own environmental assessments of corn ethanol scenarios this framework makes the integrated model accessible for developers to create their own software for decision makers to run their own scenarios and for researchers to conduct analyses on this integrated system 4 conclusion the dssat and greet models were connected by running each program through apis developed using python several scenarios based on decisions that can benefit a user in studying what if scenarios were run through this integrated model to identify the total environmental assessment of the full system these scenario analyses can provide the user with a better understanding of the impact that certain key decisions such as fertilizer application can have on the environment and the rest of the system the dssat greet wrapper is not only a tool that can be used to evaluate and understand sustainability aspects of the system but it also offers a step towards helping modelers and programmers make more effective decision support tools stakeholders in the crop and biofuel systems can better understand their effect on the other technology companies large and small develop apis in the hopes of working together to make more powerful tools for society and a similar opportunity is available for environmental modelers future work will include integrating a model of a livestock system into the framework incorporating other cropping systems accounting for irrigation pumping costs evaluating economic impacts and developing an enhanced environmental impact assessment that includes other factors such as eutrophication and human health components this integration will also be tested against long term climate models and make system predictions for the future acknowledgements the authors thank the national science foundation nsf innovations in the food energy and water systems infews program for funding this work and other related projects under award number 1639478 the authors thank deepak kumar university of illinois at urbana champaign for his work in verification of greet model data appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 004 
26340,as global demand for food energy and water resources continues to increase decision makers in these sectors must find sustainable ways to produce and provide for the growing population while many models have been created to aid in decision making in these systems there is a lack of robust integrated models that enable an understanding of the interconnections of these systems this study develops a modeling framework that explores the connections of the corn and ethanol systems two major food and energy resources a crop modeling tool dssat and a biofuel life cycle assessment tool greet are connected using a service oriented architecture programming approach a python program is developed to connect these two models and run scenario analyses to assess environmental impacts of the integrated system this paper explores the impact of decisions such as fertilizer use and plant population on environmental effects of greenhouse gases energy use and water in the integrated system keywords food water energy nexus crop modeling biofuels decision support system life cycle assessment service oriented architecture 1 introduction with the global population nearing 9 billion people by the year 2050 there is a need to better utilize the food energy and water few resources that are essential to living in the 21st century godfray et al 2010 the population growth puts pressure on the existing resources which must be used more intelligently to support the projected population the food and energy systems of the world are highly interconnected and understanding these interlinkages is vital in sustainably solving the resource demand problem that will perpetuate in the future bazilian et al 2011 energy is essential for producing crops but with the progress of biofuels food is also used as a means to produce energy which can displace the currently used fossil fuels crop modelers have been successful in modeling crop systems such as corn which can be processed into corn ethanol biofuel experts have also developed models for the biofuel system but there is a gap in integrating these two systems using the existing tools this is true for coupling any interconnected system in the few nexus daher and mohtar 2015 explore few modeling using a macroscopic approach their model the wef nexus tool focuses on the environmental impacts of producing a certain food source of a whole region or country by estimating water land energy and carbon requirements the wef nexus tool calculates not only environmental impact but also financial cost of either producing or importing the food source hang et al 2016 on the other hand aims to integrate the few nexus on a local scale rather than regional it is also very general and can be applied to different food sources for a single production system it uses an exergy balance approach to make calculations which gives the total environmental impact of the system using food water and energy researchers have recently been attempting to couple systems to identify interlinkages in the few nexus some studies provide a general framework for systems of given scales and others aim specifically to connect certain agricultural systems the international food policy research institute ifpri developed the international model for policy analysis of agricultural commodities and trade impact as a tool to understand the effect that certain policies and decisions have on sustainability and food security impact integrates economic climate and crop models to simulate the national and international agricultural markets more meaningful assessments of the impacts on the environment food production and economy will result in including all system interconnections robinson et al 2015 the integrated farm system model ifsm is an example of an integrated model for specific agricultural systems as opposed to using general equations that can be applied broadly ifsm integrates the crop and livestock systems to measure the total environmental impact of the decisions made in these sectors the model requires inputs for eight different modules and modifies existing models to create a sustainability assessment rotz et al 2012 the biofuel energy systems simulator bess ties together the crop beef and ethanol systems bess calculates the production of each system and also provides a sustainability assessment over an annual production cycle liska et al 2009 the ceres maize cropping model was integrated with the root zone water quality model rzwqm to couple water quality with agricultural crop production this was done to have a more comprehensive understanding of the interactions between these two systems as a decision support tool ma et al 2006 the markal energy model and center for agricultural and rural development card market model were integrated to properly tie together the crop and biofuel systems it is mainly an economic integration of two models that simulate the market for the biofuels their methodology involved linking relevant inputs and outputs of both models and analyzing the effect that certain events have on the market for fuels and related commodities elobeid et al 2013 an environmental assessment of biofuel production in greece was performed by using the gemis software as an lca of biofuels from crop production to fuel and byproduct production four biofuel products were analyzed and compared based on environmental assessment results fontaras et al 2012 kim and dale 2005 integrated several models and datasets to assess crop biofuel system daycent for soil dynamics nass data for crop production and management greet for ethanol production and the epa traci assessment method for environmental impact validation is a concern for systems models because it is difficult to find empirical data to evaluate and improve the performance of the models using data measured in real world environments tanure et al 2015 developed a bioeconomic model focusing on the crop livestock system by integrating validated equations and creating four sub modules herd structure and animal characteristics animal nutrient requirements weather soil pasture animal integration and economics the individual equations used were pre validated and then the whole model will be validated in a dynamic way by comparing scenarios to case studies of brazilian systems of pasture based beef cattle production the ifsm has been slowly validating its simulations using data found in the midwestern u s making it more accurate for temperate climates but less accurate for tropical climates tanure et al 2015 spatially integrated cropping models can be validated to the usda nass dataset this is done by zhang et al 2010 as their spatially explicit integrated modeling framework seimf data can be compared to geographic crop production data previous attempts at integrated models were developed in the form of stand alone software or programs the individual systems are usually connected by writing the connecting formulas into a single program or running the models manually to develop assessments of the integrated system this is seen in both the ifsm and bess models as it integrates several systems using existing models by rewriting the equations as part of a new program since many individual models already exist in their own software it would be much more practical to integrate them using service oriented architecture soa soa is a way to integrate software components as applications using communication subroutines these models can be integrated as a web service and made available to users and developers as applications this technique has been used for many disciplines to combine several systems together the open geospatial consortium ogc developed a web processing service wps to expose hydrological models as web services castronova et al 2013 users can use an open modeling interface to predict runoff in certain areas by sending inputs to a hydrological model topmodel which is available in a model workflow evapotranspiration calculation is another model that is required in the workflow and an example of integrated models working together on a web service many researchers have used web services and soa to couple climatic and hydrological models to understand the integration of these two systems or for specific end user application such as flood emergency response goodall et al 2011 2013 tan et al 2016 in the field of bioinformatics the european bioinformatics institute embi developed a loosely coupled web service that allows users to access large databases search tools and analysis tools mcwilliam et al 2013 analysis tools such as genetic sequence similarity and biological sequence alignment are integrated via advanced workflows with available data in the data repository users can integrate additional functionality to web sites and programs on the web for social sciences several web services exist to analyze the sentiment and opinions of the public for various online services such as youtube twitter and facebook serrano guerrero et al 2015 these analysis tools are models that are exposed as a web service application and can analyze data for companies to incorporate into their software the models are integrated into a workflow and can be used by developers to build analytical tools agricultural models have not capitalized on soa to improve decision making and to couple systems including the crop and biofuel systems with a loose coupling framework these models can be available for anyone to use independently with their original functionality but can also be used together to run crucial impact assessments more specifically the tool developed through this study will help crop growers and biofuel producers intuitively utilize these models together to understand how different decisions they make can affect the environment on a larger system boundary this will make computation of the environmental impact more practical the soa can be created as a web service that any user is free to use either to write their own software or to call the functions directly the objectives of this study were to 1 develop a framework for integrating two well accepted validated models in the cropping and biofuel systems using a service oriented architecture 2 evaluate scenarios to demonstrate the utility of the framework as a decision support tool 3 conduct a sensitivity analysis to determine effects of varying key model parameters on system response 2 material and methods 2 1 model description 2 1 1 model identification desirable attributes for models are wide acceptance high validity and high functionality the cropping models need to take weather soil and management inputs and output potential yield for that season the biofuel model must take crop yield and key resources in production i e fertilizer irrigation to output not only fuel production but also environmental impacts of resource use and emissions there exist several models that simulate the crop and ethanol systems individually because the crop system is inherent to biofuel production they must be coupled in order to understand the interactions between the systems de carvalho lopes and steidle neto 2011 break down many of the cropping models that are widely used and validated in their respective fields some of these models include ceres cropgro environmental policy integrated climate model epic hybrid maize and apsim additionally the decision support system for agrotechnology transfer dssat is a program that acts as a wrapper function for many different cropping models including the popular ceres and cropgro hoogenboom et al 2015 all tools successfully model the cropping system of interest and are meant to be used as decision support systems for either private decision makers or policy makers the spatial and temporal scales and equations may differ from model to model but the functionalities are similar dssat is used in more than 100 countries and has been under constant development for more than 20 years dssat also has a high volume of data backing up calibrating its models so it can be used to predict crop yield in a variety of locations with high validity there are over 60 inputs that can be plugged into the dssat model which shows how functional it can be for researchers and if the model can be properly wrapped dssat can offer high functionality into the integration of the crop and biofuel systems dssat has also been integrated with other models in the past the impact model for example integrates dssat for their crop production module when determining the potential for food supply ifsm uses equations found in dssat to calculate their crop yields dssat is not only high in functionality but researchers have experience in integrating it with other systems as well making this the best tool to use for our application since biofuel production is closely associated with environmental impact most of these models are life cycle assessments lca in order to determine the environmental effect benefit of producing a certain type of fuel from raw materials to consumption some well known fuels lcas include the greenhouse gases regulated emissions and energy use in transportation greet model simapro the global emission model for integrated systems gemis and the market allocation model markal the greet model is widely used largely because it is sponsored by the us department of energy elgowainy et al 2012 and argonne national laboratory consistently updates the data that drives the model the latest major update to the corn ethanol pathway was in 2014 and the current greet version used is greet 2015 wang et al 2014 when performing lca with greet it includes the crop farming pathways which give some functionality to the user in defining cropping inputs for example the corn ethanol pathway includes the corn farming process which allows the user to specify irrigation fertilizer and chemicals per unit weight of corn produced the markal model behaves in a similar way to calculate environmental impact of biofuel production with a large database it also includes a linear programming algorithm that helps the user in determining the least cost solution on the energy resources to use kannan et al 2007 the purpose for this model however is not to find an optimal solution for the user but to calculate the system impact from decisions defined by the user markal also does not have near the user base or support that greet has given its numerous benefits greet is the chosen biofuels model dssat and greet are also capable of simulating a wide range of different crops and biofuels this study will focus on integrating the corn and ethanol systems but the same principles can be applied to a variety of biofuel systems such as biodiesel from soybean or ethanol from corn residue 2 1 2 computational structure the soa will follow the framework described in fig 1 the middle layer shows the model pipeline and at the very bottom are classes that create inputs from user actions these feed into the control and batch methods that create the crop experiment file and batch file for the dssat model under the dssatfile class dssatfile is inherited by the dssatmodel class which runs the file created from the user inputs dssatmodel returns yield and all other outputs associated with dssat this is inherited by the ds greet class which runs the greet model biofuel pathway based on the crop outputs simulated by dssat every module of this structure can be called by a client on a representation state transfer rest server any user that has access to the server can make requests to the server to utilize the integrated model in the middle layer the integrated model interacts with a data repository to store and access data that are used in the subroutines 2 1 3 dssat wrapper the cropping model in this integrated model should be able to simulate crop yield based on several environmental factors and management practices that the user controls creating a wrapper for the dssat model allows the program to run and be connected to other programs which makes it viable for integration pydssat is a python wrapper that runs the dssat model in its original fortran code he et al 2015 the program makes a large number of input assumptions and also wraps the model by compiling the original fortran source code the program is open source so it was used as a reference to write the python code used in our wrapper the method for writing the dssat wrapper follows a similar flow to pydssat and is shown in fig 1 it must create an experiment file based on farming inputs run the file through the model and process the outputs the experiment file used by dssat is a text file that compiles the inputs in a certain format for the simulation that the user wants to run in our implementation this is created by making a python class dssatfile the pydssat code allows the user to input the following variables crop type soil type weather station start year end year planting date and model mode the integrated model only simulates one year so end year is not needed it is also only being run in batch mode where a single experiment file is being used so the mode is defaulted as b for batch since dssat is robust and has many inputs it is difficult to fit them all as inputs into one class practically other classes were created to enter management data as comma separated files csv files irrigation fertilizer harvest tillage and chemical application all have their own class defined to take in scheduling inputs written to a csv file the dssat model can be run in the command line using dscsm046 exe which is available with installation of the software pydssat runs the model by compiling fortran code but running the batch file in the command line is simpler and provides the same results the pydsssat class dssatmodel runs the model by calling the executable in the terminal and taking in data from the experiment file and the management files the model controller finishes writing the experiment file that did not yet include the management inputs from the irrigation fertilizer harvest tillage and chemical classes after the model is run it creates output files that show results for yield crop growth soil water balance and more 2 1 4 greet wrapper the objective of the biofuel model is to not only calculate the total biofuel production from crop production but also the resource use and emissions associated with the biofuel in comparison to a reference fuel to understand the environmental impact of producing the clean burning fuel the biofuel pathway is compared to the reformulated gasoline pathway greet can compare lcas of different fuels such as ethanol and gasoline on a per mj basis the pathways in the greet model used for this tool are the e85 gasoline blending and transportation to refueling station pathway and the reformulated gasoline e10 blending and transportation to refueling station pathway the ethanol pathway is broken down into several processes and calculates all results associated with this pathway it begins with the corn production for biofuel refinery process which includes some inputs and outputs found in dssat this is where the dssat inputs are written into the greet model when appropriate the results feed into the ethanol production process which is chosen as dry mill ethanol production w corn oil extraction since this is a common method in many ethanol plants in the u s while the user can specify the percent of ethanol being produced from each process in the software for this study it was assumed that 100 of the ethanol produced is from dry mill with oil extraction greet calculates emissions from the beginning of the corn farming process to the end of the transportation process to show final resource results for the pathway while many inputs in greet overlap with dssat inputs others are more difficult to estimate the rest of the inputs require the user to know how energy and resources as a whole are used for the rest of the operations such as vehicle usage tillage and transportation greet also has numerous libraries and pathways for resources that can be edited the defaults for these inputs are us averages so for the purposes of this study the default energy uses were kept to make the tool more intuitive greet uses terminology called well to pump wtp and pump to wheel ptw to describe where the product is in its life cycle the wtp pathway describes resource use and emissions associated with the production of a product from its raw materials to transporting it to a fuel pump ready to be consumed the ptw pathway describes resource use and emissions associated with the consumption of the product this study will look at the full well to wheel wtw pathway of the corn ethanol lifecycle the corn ethanol pathway in greet uses a displacement method by default and distillers grains and solubles dgs displaces animal feeds which is a mix of corn soybean meal urea and soyoil for different types of livestock the displacement values per feed and livestock are averaged and used as the allocation amount to credit to dgs byproduct because of the displacement of soybean meal and soybean oil the water consumption associated with them is taken as credit which may result in a negative wtp water consumption value if irrigation amount is too low this must be considered when making conclusions from the greet results also while dssat calculates some environmental footprints such as water use for corn greet already takes into account the footprint produced from corn farming hence any environmental assessment done in the integrated model is done based on greet values not dssat greet operates based on calculating environmental results from ethanol amount but the dssat model only provides corn yield the program must convert kg ha of corn to total liters l of ethanol based on greet s current conversion rate of 10 598 l ethanol bushel corn since a liter of e85 does not provide the same amount of energy as a liter of e10 the volume calculated must be converted into energy mj further the crop farming inputs in greet are on a per unit weight of crop produced basis while the user inputs these values on a per hectare of field basis in dssat irrigation and fertilizer inputs from dssat are converted accordingly to be written into the greet file calculatorbatch is a program that was compiled using an api developed by the makers of greet the arguments for this program are the greet file name year s of simulation and the id for the fuel pathways or mixes the greet wrapper runs the written greet file into the calculatorbatch program using the values specified by the farming system there are two pathways being used corn ethanol production and e10 reformulated gasoline so the greet model is ran for both pathways using the amount of energy produced in ethanol production while this api is a useful tool and essential for the development of this wrapper it is only able to calculate resources for wtp and does not include the results from actual usage of the fuel much of the offset in emissions and resource use comes from the burning of the biofuel in comparison to fossil fuel so it is essential to include the ptw pathways as well fortunately only the wtp pathway results are variable due to the cropping inputs of irrigation fertilizer and chemicals the ptw results do not change on a per mj basis since those are completely separate from production and only dependent on the vehicle that is consuming the fuel the results for these ptw pathways on a per mj basis were stored in a separate class wherever there were ptw emissions present once the total amount of ethanol energy is calculated from the beginning of the program it is input into the ptw class to output total resources the vehicles chosen for both pathways were ones that are commonly found for their respective fuels flex fuel vehicle ffv was used for ethanol while standard ignition internal combustion engine vehicle si icev was used for e10 reformulated gasoline flex fuel vehicles have engines that are able to burn fuels that have higher blends of ethanol which is why they are commonly used for e85 alternative fuels data center 2017 e10 gasoline is similar enough to gasoline that a standard internal combustion engine found in most vehicles can run it anjikar et al 2017 after adding results from the api along with results from the ptw class the program will output total wtw results for ethanol and gasoline the total flow of the program from inputs to outputs is represented in fig 2 2 2 post processing the design for post processing the resource results is based on wu et al 2007 who evaluated the benefits of using ethanol relative to gasoline for fueling vehicles the main outputs that this study analyzes are greenhouse gas ghg emissions energy usage air pollutants and water consumption ghg emissions are calculated as co2 equivalent from co2 ch4 n2o and biogenic co2 based on their global warming potential values 1 25 296 and 1 respectively ipcc 2007 the criteria air pollutants identified when comparing the two fuel pathways are volatile organic compounds voc co nitrous oxide nox particulate matter pm10 and sulfur oxide sox these are all significant emissions produced in the corn ethanol pathway that affect human health the energy savings from nonrenewable fuel use are also important to analyze as total fossil fuel energy and petroleum fuel energy use are compared in this model fossil fuel energy consists of total use from coal natural gas and petroleum energy in the life cycle petroleum fuel use is singled out from the other fossil fuels because most of the benefit from producing ethanol comes from the reduction of petroleum fuel water consumption is highly sensitive to irrigation in the crop pathway and can be beneficial or detrimental based on management and weather after the yield is calculated in dssat in kg ha it is converted to ethanol in mj to compare the ethanol production to gasoline an equivalent amount of mj of gasoline is calculated in the greet life cycle and the two pathways are compared to each other in this study the functional unit of comparison is total mj of e85 possibly produced by 1 ha of corn production this effectively defines the system boundary to start with production of cropping inputs and end with the consumption of e85 fuel in a vehicle the lca tracks fertilizer and chemical inputs down to energy inputs of raw materials such as ammonia and phosphate environmental footprint of raw materials for ethanol production are tracked for enzymes yeast and chemicals blending gasoline for e85 is also tracked along with its raw inputs the system boundary ends once the fuel reaches the vehicle that uses it and is consumed since dgs displacement of livestock feeds is credited the boundary also includes consumption of dgs and its environmental benefits there are two ways to compare the footprint of e85 fuel to e10 fuel one is by total footprint and the other is percentage footprint of e10 simply subtracting the results from the e85 gasoline pathway from the e10 pathway provides the absolute savings percentage savings is the percent of the resource saved by using e85 as opposed to e10 this provides a normalized value that can be compared across different categories negative values for either method show that producing ethanol is beneficial to gasoline in that category wu et al 2007 used percentage savings to compare several different fuel pathways on a per unit of distance basis it is important to understand both methods when making the assessment since absolute savings encourage larger crop and biofuel production operations while percentage savings encourage more efficient operations on a per unit of energy basis this gives the user more options on how they can make operations decisions that may be more environmentally sound as an additional note water footprint is commonly used referred to as a way to track the use of water in different phases and stages of a process in this paper water footprint is defined as the difference in total water consumption between e85 and e10 life cycles a framework for water tracking was not included for this paper in the greet model 2 3 scenario analysis 2 3 1 base scenario for comparison the driving inputs of both models are nitrogen amount from fertilizer application and water use from irrigation this is because these management inputs have a high effect on yield depending on the growing conditions and have a high environmental footprint from life cycle production and use a sensitivity analysis of the effect of fertilizer on the integrated model was conducted to determine the impact on the system from changing one of the parameters a base scenario for corn production is developed from regional data and management recommendations of corn farming in the eastern nebraska region this scenario is based only on environmental and user inputs as greet default values for emissions and energy use are kept the same the sample field location and year used are mead ne and 2015 weather data for this season were obtained from the high plains regional council center hrpcc in the form of daily precipitation minimum temperature maximum temperature relative humidity solar radiation and location coordinates the plant population for irrigated corn that is typically used in the midwestern region is approximately 10 plants m2 barr et al 2013 anhydrous ammonia is a common fertilizer material used for nitrogen application so this method was used for the baseline scenario the corn hybrid chosen for this study called gdd2600 has a 113 day maturity medium season hybrid from planting to physiological maturity the soil type used should accurately reflect the profile found in eastern nebraska corn fields so the soil file in dssat chosen is for loamy soil this soil file gives descriptions by layer based on water holding capacity density and nutrient characteristics the water holding capacities were adjusted based on local data to more accurately reflect the nebraska soil profile unpublished research data values for field capacity fc wilting point and organic carbon matter omc are represented in table 1 planting and harvesting dates are based on data from the usda national agricultural statistics service nass in eastern nebraska recommended times to plant corn are between april 27th and may 18th while the recommended times to harvest corn are between october 4th and november 10th usda 2010 thus the dates chosen for planting and harvesting are may 1st and october 15th respectively fertilizer amounts used in the simulations were based on the amounts used in long term field research conducted by irmak 2015a 2015b in nebraska it is common to apply fertilizer using side dressing and especially since it is being applied on the same day as planting it is used for this scenario in dssat this application method is named banded on surface phosphorous and potassium are normally not included in fertilizer for corn so these were left out of the simulation dssat has an option to automatically irrigate the field if it senses water stress throughout the season water stress for yield should not be a constraint for this simulation so irrigation was simulated using automated irrigation option so that water is not a constraint for achieving potential grain yield the state of nebraska has a large aquifer and uses groundwater for irrigation nebraska commonly uses center pivots to irrigate their fields so the equivalent setting in dssat was set to sprinkler all baseline values are recorded in table 2 pesticide use is an option for this integrated model but was left out of this case study since dssat does not handle pest hazards well when predicting yield if pesticide use were to be included the integrated model would only calculate the negative environmental impacts from greet and leave out the positive yield effects it has on cropping in this scenario it is assumed that there are optimal conditions for pests and weeds so pesticide chemicals are not used tillage settings are also important to consider in this integrated system and in dssat it is defined as drill no till this is not defined in greet however due to the difficulty of adding vehicle fuel use in the tool 2 3 2 nitrogen fertilizer sensitivity the production of corn has several sources of emissions such as soil emissions fuel use and chemical applications the corn production process in greet has default values of 2 40 g bu 0 095 g kg and 2 73 g bu 0 107 g kg of nox and n2o emissions respectively due to soil and fuel use fertilizer applications have high added environmental impact and increase the lca for corn ethanol s emissions nitrogen amount was analyzed relative to the baseline value of 150 kg n ha which is normal for this field the following values for nitrogen application were run in the model assuming the baseline scenario for all other variables 0 50 100 150 200 250 and 300 kg n ha resource savings from producing e85 over e10 were calculated both as a total difference and as a percent reduction total footprint shows the physical results and changes with each scenario while percent footprint shows the sensitivity of output parameters to nitrogen fertilizer application the pathway is separated into wtp and ptw which both have different contributions to the overall results to quantify the difference the program was altered to calculate emissions from wtp and ptw separately the two results of ghg emissions and nonrenewable energy use were compared to each other based on these different pathways 2 3 3 optimal plant population a range of plant populations was used to gauge the sensitivity of this input on the dssat model there are significant breaks in the relationship between plant population and yield for values under 40k plants ha and over 120k plants ha so this sets the bounds for values of plant population table 3 therefore the model was run for plant populations in the range of 5 12 plants m2 over the span of 15 years from 2001 to 2015 this will show the relationship between varying weather and the optimal plant population as a possible decision point for farmers with higher plant population in general higher nutrient application is required each plant population was run with a range of nitrogen fertilizer amounts to determine the optimal value nitrogen fertilizer values from 10 to 600 kg n ha were used to show the full relationship of nitrogen to yield for this plant population and the maximum described by dssat the optimal nitrogen application amount was determined by picking the value where yield exceeds 90 of the max as shown in fig 3 this was done to ensure that a consistent fertilizer value was chosen for each plant population depending on the nutrient requirements once nitrogen amounts are picked for each plant population they were used as input into the model to calculate resource results for each scenario since nitrogen amount increases with plant population the results should reflect an increase in ghg footprint however if the yield gains offsets the nitrogen use then the ghg footprint can actually be reduced to show the complete ghg footprint matrix in relation to plant population and nitrogen fertilizer application combinations of both variables were run plant populations were inputted in the range of 5 12 plants m2 and nitrogen applications were inputted in the range of 100 250 kg n ha with increments of 1 plants m2 for plant population and 10 kg n ha for nitrogen applications the ghg footprints of each scenario are calculated to show the relationships with both of these variables simultaneously 3 results and discussion 3 1 model verification before performing analyses the python program must be verified with the models in the original software verification of the dssat wrapper is done by running the model twice using the same scenario both manually through the dssat software and with the inputcreator class in the wrapper both programs produce a summary file that shows generic outputs of the simulation run including yield seasonal weather data soil water balance and management summary three scenarios with three different nitrogen fertilizer amounts 0 100 and 200 kg ha were run in both programs and the summary files exactly matched which verifies the accuracy of the wrapper verification on the greet model is done in a similar way by running the software manually using a scenario and then running the same scenario with the greet wrapper written in python the wtp pathway is run with the wrapper so that part is validated with the software the results in the wrapper are based on total emissions from the amount of energy produced in dssat since it is inherited so the scenario that is run manually through greet must be done on the basis of total energy produced the same baseline scenario is run through the program to get results from the greet model only the wtp results are printed to compare to the results obtained from manual use of the model ptw calculations are linear and directly taken from the model so these do not need to be verified the comparisons are shown in table 4 where results are calculated on the basis of the amount of energy created from the ethanol process the error between the two results is minimal and is attributed to rounding error from the greet software 3 2 sensitivity and scenario analysis 3 2 1 sensitivity analysis on nitrogen fertilizer fig 4 a and b shows the results for nitrogen application on ghg emissions and nonrenewable energy use overlaid with the corresponding yield up to 100 kg n ha increasing nitrogen application increases the yield potential and lowers the ghg footprint of the system since more e85 energy can be produced to displace e10 however yield levels off after this value and can no longer offset the footprint caused by increasing nitrogen rate therefore the ghg footprint of the system will sharply increase beyond 100 kg ha because these emissions are highly sensitive to nitrogen fertilizer use as displayed in fig 4a the total footprint of both nonrenewable energy categories in fig 4b shows a steady increase that evens out past 150 kg n applied per hectare fossil fuel and petroleum fuel footprints appear to closely mirror the relationship between yield and nitrogen rate this means that these footprint categories are more sensitive to the amount of e85 produced than the amount of nitrogen applied the petroleum fuel footprint is the lowest as e10 requires much more petroleum in production than e85 on a per mj basis the total fossil fuel footprint is not as low because e85 is detrimental higher footprint in comparison to e10 in its coal and natural gas footprint fig 4b shows this as the petroleum fuel continues to steadily decrease with nitrogen application while the total fossil fuel footprint starts to increase as yield levels off the sensitivity is more clearly shown in fig 4c that normalized footprint for each category in comparison to the optimal fertilizer rate according to ghg footprint the trends look different in this subfigure because it is comparing all scenarios to a central value of 150 kg n ha the percent change in footprint is much higher for ghg across the range of nitrogen rates in comparison to the fossil fuel categories petroleum fuel also shows very little sensitivity to nitrogen which confirms the findings from fig 4b this integrated model can inform decision makers on the optimal fertilizer application not only based on yield but based on environmental impact in the model yield would continue to increase with increasing nitrogen but with ghg savings there is a clear optimum value for this scenario at 100 kg n ha while e85 is beneficial to e10 for all of the categories described above it is not as beneficial in emission of criteria pollutants as found in fig 5 in every scenario and for each category e10 gasoline emits fewer pollutants than e85 with increasing nitrogen application the emissions footprints become more positive this shows one of the tradeoffs of producing ethanol when using it to displace reformulated gasoline as a fuel each pollutant shows a similar increasing trend in fig 5b where emissions compared to the middle nitrogen scenario of 150 kg n ha nox emissions are impacted the greatest with increasing nitrogen which is reflected in greet s nitrogen and corn production pathway in the nitrogen mix pathway producing 1 kg of nitrogen emits 7 46g nox and in the corn pathway 1bu of corn emits 2 40g nox water consumption in the crop biofuel system is influenced by irrigation amount and the amount of water used in the ethanol plant for producing ethanol processing water consumption for ethanol is greater than gasoline marginally by about 0 076 l mj however irrigation has the most significant impact and the practices that a decision maker has can greatly influence the water footprint of corn ethanol in this scenario irrigation is automated when needed if the crops grow larger due to increased nitrogen then more irrigation may be needed to maintain the growth and should reflect larger consumptive use fig 6 a shows the total water consumption from the e85 pathway in relation to nitrogen application rate water consumption has a negative trend with nitrogen even though yields are increasing and consumption is expected to rise the results indicate that dssat may not be able to properly handle the relationship between nitrogen and water consumption climatic conditions influence how increased nitrogen levels impact crop physiology yield and evapotranspiration which is not a linear response in this scenario the annual precipitation is 731 mm which is high for this region the percent footprint though does show a clear downward trend of water footprint with higher fertilizer the increase in yield creates a lower water footprint and offsets the use from irrigation which does not change as much fig 7 shows the breakdown of the footprints between the wtp pathway and the ptw pathway for ghg and fossil fuel categories the production of ethanol emits more co2e than gasoline on a per mj basis and that is reflected in the wtp results where emissions increase as nitrogen use increases conversely the ghg emissions for the ptw pathway decrease due to consumption of ethanol as e85 use emits much less ghg s than e10 and offsets the ghg increase from the wtp pathway as seen in the full wtw pathway the footprint is negative through this range of nitrogen application but as it gets higher the ptw footprint levels off and wtp footprint continues to increase at a near constant rate the total wtw pathway shows a net negative footprint for ethanol which shows the offsetting effects of clean consumption in vehicles ptw footprint does not have an impact on any of the other categories the differences between e85 and e10 come only from the wtp pathway in all of these resources this is because in the wtw analysis the vehicle used for both e85 and e10 fuels are kept the same since the engine that is burning the fuel is standardized there is no difference in energy water or air pollutant savings per mj of e85 and e10 for example fig 7b shows that fossil fuel footprint only changes with the production of the fuel not the consumption ghg emissions is the only category where there is a difference between e85 or e10 consumption because of the composition of the fuel 3 2 2 optimal plant population fig 8 shows the relationship between total resource and emissions footprint and plant population the fossil fuel energy footprint steadily decreases with increase in plant population this is because based on the assumptions made within the dssat model the increasing nitrogen application does not have as much of an offsetting effect on the energy footprint from producing with a higher plant population ghg emissions footprint on the other hand is heavily influenced by nitrogen application and that shows in fig 8a where it steadily increases the ghg footprint levels off after 9 plants m2 because nitrogen levels are also leveling off as shown in table 3 much like in section 3 1 1 the criteria pollutant footprints increase with plant population and are a net positive compared to using e10 gasoline with this scenario analysis the decision maker may weigh the environmental trade offs when determining plant population as it has differing effects on these categories while fig 8 uses predetermined nitrogen values for each plant population fig 9 shows the relationships of ghg footprint to nitrogen fertilizer and plant population the ghg footprint seems to be optimal with lower nitrogen application and higher plant population density this seems to be the ideal decision for nitrogen application and plant density to provide the best ghg footprint within the system the ghg footprint is highest when the plant population is very low and the nitrogen application is very high this is because the low population density is not producing enough ethanol to offset ghg emissions created by the high fertilizer use 3 3 end user application this program for integrating the crop and biofuel systems is primarily purposed as an environmental decision support tool both dssat and greet have large user bases who use these tools to make informed decisions based on either production output or environmental impact de carvalho lopes and steidle neto 2011 kopanos et al 2017 connecting the two creates a powerful tool that allows the user to do both and understand the impact that their decisions have on the integrated system many scenario analyses on the impacts of nitrogen fertilizer and plant population on corn growth have been done in the past there are experimental field studies that test these variables against each other to determine their effect on grain yield for different years and different locations blumenthal et al 2003 found a linear relationship between plant population density and grain yield and a quadratic relationship between total soil nitrogen and grain yield in western nebraska counties ping et al 2008 studied the relationship between site specific nitrogen and population density management and economic performance which showed little impact though it did increase nitrogen use efficiency irmak and djaman 2016 concluded a positive relationship with plant population density and grain yield with mixed results for evapotranspiration all of these results contribute heavily to understanding the effect that nitrogen application and plant population have on growth and economics of the farmer but it is still in the closed system of the individual farming operation this integrated tool provides a way to do these scenario analyses to understand not only how these variables can affect growth but also the impact on emissions resource use and the environment when coupled with the ethanol system this is a way to use the valuable crop research that is done to produce cropping models for the understanding of a larger system likewise lca users will have a better understanding of the tradeoffs of using biofuels with a more functional corn farming pathway greet currently has a corn farming process included but is limited in inputs the default values are derived from usda nass data which averages management practices from regions of completely different climates and land integrating the dssat model allows the user to make greet location specific which can be a valuable tool for businesses that operate close to biofuel plants in various regions future work can include scenarios that run in different locations to analyze how different areas affect the environmental impact of producing biofuels this study is focused on corn ethanol production in the midwest but the corn ethanol system is also prevalent in further stretches of the great plains and the south a database could be created of these various locations and based on the weather and land differences the model can also run scenarios for various other biofuels it includes biodiesel from soybean oil and ethanol production from corn stover sorghum and forest residue feedstocks dssat includes models for soybean and corn stover production so additional scenarios can be run to better understand how these systems are interconnected in comparison to corn ethanol and reformulated gasoline corn stover is an important addition to the system as it is tightly connected to both the corn production and biofuel systems cellulosic ethanol plants are commonly found in the midwest united states and can result in further emission savings from corn production residue from crop yields can be simulated using dssat and distributed to have a more accurate understanding of the emissions within this integrated system there are limitations to this tool some of which are inherently attributed to the individual models dssat s large user base mostly consists of researchers and scientists who do not make field decisions in hopes of selling their product dssat is also limited as an irrigation scheduling tool the automated irrigation setting does not give the user many options to adjust for strategies such as implementing deficit irrigation greet also does not incorporate irrigation pumping costs when calculating ghg emissions and energy use for the corn ethanol pathway many producers consider irrigation pumping a large expense especially for drier regions where irrigation is required greet is limited to calculating direct emissions from each system and as a result indirect emissions were not taken into account for this study a user can add emissions from indirect sources but this study focused specifically on the capabilities of the two models used the framework allows for future work to explore the possibilities of adding indirect emissions from other models greet also does not consider methane emissions from irrigation and other agriculturally related emissions so those were left out of the scope of this paper this framework is also mainly a tool used for environmental assessment and does not include profitability decision makers are more likely to act based on their best financial interest even if there is an environmentally optimal solution a more applicable tool will include economics in addition to environmental impact so that a user is more likely to use it modelers and researchers have been developing and finding new ways to practically integrate agricultural production systems this paper offers the methodology and framework for a tool that can connect two well established models in their respective fields in the generation where apis and model wrappers are taking over the world of software and technology it is important that agricultural models and software can keep up with this trend zhong et al 2009 programmers have made many tools to facilitate the connection of various programs which were utilized in this paper the greet api the compiled dssat program and the various python packages used in the wrapper the method also allows for the models to be automatically updated as the developers continue to work on the individual models other researchers may do similar studies and run their own holistic scenario analyses like how nitrogen fertilizer application was connected to total ghg emissions this soa offers users the ability to utilize these tools practically to make their own environmental assessments of corn ethanol scenarios this framework makes the integrated model accessible for developers to create their own software for decision makers to run their own scenarios and for researchers to conduct analyses on this integrated system 4 conclusion the dssat and greet models were connected by running each program through apis developed using python several scenarios based on decisions that can benefit a user in studying what if scenarios were run through this integrated model to identify the total environmental assessment of the full system these scenario analyses can provide the user with a better understanding of the impact that certain key decisions such as fertilizer application can have on the environment and the rest of the system the dssat greet wrapper is not only a tool that can be used to evaluate and understand sustainability aspects of the system but it also offers a step towards helping modelers and programmers make more effective decision support tools stakeholders in the crop and biofuel systems can better understand their effect on the other technology companies large and small develop apis in the hopes of working together to make more powerful tools for society and a similar opportunity is available for environmental modelers future work will include integrating a model of a livestock system into the framework incorporating other cropping systems accounting for irrigation pumping costs evaluating economic impacts and developing an enhanced environmental impact assessment that includes other factors such as eutrophication and human health components this integration will also be tested against long term climate models and make system predictions for the future acknowledgements the authors thank the national science foundation nsf innovations in the food energy and water systems infews program for funding this work and other related projects under award number 1639478 the authors thank deepak kumar university of illinois at urbana champaign for his work in verification of greet model data appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 004 
26341,communities are at risk from extreme events and natural disasters that can lead to dangerous situations for residents improving resilience by helping people learn how to better prepare for recover from and adapt to disasters is critical to reduce the impacts of these extreme events this project presents an intelligent system flood ai designed to improve societal preparedness for flooding by providing a knowledge engine that uses voice recognition artificial intelligence and natural language processing based on a generalized ontology for disasters with a primary focus on flooding the knowledge engine uses flood ontology to connect user input to relevant knowledge discovery channels on flooding by developing a data acquisition and processing framework using environmental observations forecast models and knowledge bases the framework s communication channels include web based systems agent based chatbots smartphone applications automated web workflows and smart home devices opening the knowledge discovery for flooding to many unique use cases graphical abstract image keywords intelligent systems natural language processing knowledge generation ontology disaster preparedness information communication software availability name flood ai knowledge engine developersyusuf sermet and ibrahim demir yusuf sermet and ibrahim demiriowa flood center university of iowa contact information 100 stanley hydraulics lab iowa city ia 52242 usa software requiredinternetbrowser messaging applications program language php postgresql server side html javascript client side availability and costany user can access flood ai at prototype stage upon request at no cost any user can access flood ai at prototype stage upon request at no cost it will be free to the public at the operational stage 1 introduction an immense amount of data is constantly being generated from observations and simulations to monitor environmental conditions in real time and forecast potential events demir et al 2015 the national research council nrc 2012 report puts forth a vision of a nation that is resilient to extreme events by the year 2030 the report highlights the importance of data notes existing gaps in our information and acknowledges the need to address these challenges suggesting that every individual should have access to the risk and vulnerability information they need to make their communities more resilient recent breakthroughs in sensor networks and remote sensing technologies greatly facilitate this process and allow scientists to gather large scale high resolution datasets on the environment water quality and weather conditions most of these datasets are shared through custom interfaces and technical formats for limited stakeholders making it difficult for the public or other non targeted groups to effectively access and understand the data ames et al 2012 advancements and new information technology techniques are making it possible to manage analyze and present large scale environmental data and modeling results acquired from various sources on the web xiaodan et al 2010 accessing data from these systems often requires navigating between many interfaces in the information system or opening numerous web based resources using conventional search methods system managers should use workflows to organize the information in these systems presenting the data in a curated and structured format using ontologies to improve effective data access and sharing generalized workflows allow users to easily access and share the data information and modeling results gil et al 2016 ontologies capture concepts attributes and relationships to structure a phenomenon while also explicitly defining the necessary rules and constraints gruber 1993 there has been extensive work in many disciplines on information systems that aim to facilitate knowledge retrieval and discovery using ontologies e g cui et al 2017 kaufmann et al 2006 lopez et al 2007 moussa and abdel kader 2011 aqualog lopez et al 2007 presents a question answering system that accepts a natural language question uses an ontology to refine queries and measure similarities of the user query with the appropriate knowledge bases and returns the answer querix kaufmann et al 2006 provides a natural language interface that queries ontologies by parsing the textual input and executing a query to return the answer kbqa cui et al 2017 shows a template based evaluation of an input question with complex and chained relationships in environmental science ontowedss ceccaroni et al 2004 presents a decision support system in the wastewater treatment domain that uses an ontology to augment the capabilities of rule based and case based reasoning systems in 2007 wiegand 2007 discussed how geospatial e government portals can allow enhanced search and access to information using multiple ontologies gcp crop information system framework manansala et al 2007 is based on ontology development data annotation and data queries using ontology a web based geospatial problem solving environment jung et al 2013 for earthquakes provides formal definitions that web service providers present and integrate with geographic information services using domain expert approved ontologies wanner et al 2015 introduced an ontology based decision support system that acquires data from various sources on the web to provide personalized environmental information such as the pollen traffic in certain wind temperature and air conditions in the natural disasters domain ontofire kalabokidis et al 2011 provides an ontology based geo portal with a focus on wildfires the user interacts with the system through a browser by discovering the semantic relationships using lists without any natural language input qiu et al 2017 introduced a flood disaster management system fdms that expresses environmental model results and disaster related datasets in the form of ontologies on the web the ontology approach enables the autonomous behavior of fdms while accessing datasets and retrieving information to decrease human interaction thus saving time and expediting disaster response this paper presents an intelligent system flood ai for knowledge generation and communication on flood related data and information flood ai uses disaster ontologies natural language processing artificial intelligence methods and voice recognition to generate knowledge the developers organized the flood related data and modeling results to provide definitive answers to factual inquiries the knowledge engine uses the flood ontology and concepts to connect user input to relevant knowledge discovery outputs on flooding it includes a data acquisition and processing framework for existing environmental observations forecast models and social network data streams a communication framework supports user interaction and delivery of information to users the interaction and delivery channels include voice and text input via web based systems agent based chat bots e g microsoft skype facebook messenger smartphone and augmented reality applications e g smart assistant automated web workflows e g ifttt ms flow and smart home devices e g google home amazon echo to open the knowledge discovery for flooding to thousands of cases and applications one of the major contributions of this work is the development of knowledge generation engine which accept natural language queries to access and analyze vast amount of flood related data and models via extensive set of communication channels for ease of access in contrast to the previous works flood ai eliminates the need for traversing through complex information systems and datasets by allowing the user to access the knowledge as if by talking with an actual flood expert furthermore extensible and modular design of knowledge engine allows its adaptation by different domains as well as for different purposes the remainder of this article is organized as follows section 2 presents the system design and describes the disaster and flood ontology as well as the structure and details of the knowledge engine and its components section 3 focuses on the implementation of flood ai for various communication and interaction channels and devices section 4 discusses future work and provides conclusions 2 knowledge engine flood ai uses the data and resources of the iowa flood information system ifis a generalized cyberinfrastructure platform for flood preparedness and response to populate and test its capabilities for flood related knowledge generation the ifis is a web based platform developed by the iowa flood center ifc to provide access to flood inundation maps real time flood conditions flood forecasts and flood related data information and interactive visualizations for communities in iowa demir and krajewski 2013 krajewski et al 2017 the ifis is optimized with custom database structures and queries demir and szczepanek 2017 for hydrological datasets allowing seamless user access flood ai s knowledge engine is confined on both data and information domain i e spatial and temporal boundary and context and ontology level although the system is generalized the domain limits are set to demonstrate the system with a use case the system is limited to provide knowledge only on weather and flooding and data domain is limited to the state of iowa to cover up to 1 month temporal scale 14 days past and 10 days future flood ai consists of ontology management natural language processing nlp and query mapping and execution qme modules fig 1 the ontology management module accepts and processes the given ontology to provide domain specific knowledge to the engine while easily storing the ontology in a relational database this module is used only when the ontology is updated the nlp module uses ontology to extract useful information from the question including location date and time ontological entities and relationships and intent the query mapping and execution module maps the question to one of the models and generates the desired knowledge using information extracted from the question the nlp and qme modules are integrated to form the inference engine the knowledge engine can be integrated to various interfaces as described in the implementation section flood ai uses service oriented architecture soa in which each module has an api that interacts with other apis rather than a single api for the whole system soa is a software design approach used to create an architecture based on the use of services the advantage of this approach is that it offers each flood ai module as a service allowing applications to benefit from the system according to their needs for example if an ontology in another domain was given to the ontology management module it would perform the same process independently assuming the ontology is formed with the specifications described in the following sections the nlp module can use the output of the ontology module to analyze the natural language question in that domain this structure allows the expansion of the knowledge engine to other natural disasters as well as any science or engineering domain another benefit of this architecture is that separate modules can be used by applications with different objectives for instance an application that retrieves flood related tweets from twitter can analyze them using only the ontology management and nlp modules without need for the qme module 2 1 ontology management module we developed a comprehensive flood and information system is ontology that can be extended to other natural disasters as part of this project we determined the scope of the ontologies from the available data and resources on flooding and the possible questions that the flood ai system will answer domain knowledge and datasets are integrated from various sources including national oceanic and atmospheric administration noaa federal emergency management agency fema united states geological survey usgs iihr hydroscience engineering iowa flood center ifc online knowledge systems e g wikipedia and via consulting to domain experts ontology development requires involvement of domain experts who may lack an advanced level of experience in computer sciences to facilitate their involvement an intermediary for ontology development is needed which should allow illustration and visual editing to remove any technical complexity simultaneous access to allow experts to collaborate online and intuitive development to provide a convenient and desirable tool and machine readable representation of the ontology for integration with information systems due to their wide adaptation in the industry and academia using uml presents the unique opportunity to benefit from the vast number of mature tools that satisfies above objectives we implement ontologies by leveraging an online collaborative unified modelling language uml tool that allows visual editing of the ontology these features simplify the development process and assure the integrity of the ontology development through a collective process and involvement of domain experts uml models can be expressed in xml metadata interchange xmi standard to assure machine readable format we selected the xml format for its flexibility in formatting and representation compared to data models or formats we implemented the flood and is ontologies designed for the system using a uml intermediary that can extract the class diagrams into an xmi standard for use in the engine uml class diagrams can be expressed using xmi without any loss of information since both are standards of open management group omg representation of a uml class diagram in xmi standard results in the same tree structure independent of the intermediate used in the process the ontology used in the knowledge framework is converted to a relational database format for its effective use in web based systems ontology plays a crucial role for processing and mapping of each natural language question as well as determining the suitable data resources representing the ontology in the form of relational database tables instead of the original representation e g xmi rdf xml allows the knowledge engine to improve performance and scale as the demand grows we implemented the ontology module using php scripting language on the server side this allows us to create a separate component to parse the ontology which is in xmi format to create data structures and build relationships while also respecting the dependencies since xmi is written in xml any xml parsing libraries e g simplexml can be used for parsing tree based structure of xml data php comes with built in functions to output an iterable collection of arrays and objects consisting of the ontological elements the ontology management module is used only when the ontology is updated and does not affect the user experience 2 1 1 ontology data model the xml parser returns a collection of concepts with corresponding attributes definitions synonyms hierarchical connections is a relationship and ontological relationships definitions and synonyms for concepts are represented in the description field of an entity in uml which requires additional parsing to store all the information in the ontology and to facilitate access we created relational database tables for concepts attributes relationships synonyms and descriptions once the data structures are in place three steps must be completed to make the data rapidly and efficiently accessible id mapping inheritance implementation and effective database storage 2 1 2 id mapping each element in the ontology such as concepts attributes relationships and annotations has ids to uniquely identify them however these arbitrary ids are auto generated and assigned by the uml to xmi converter each id consists of 23 characters and requires string comparison for every operation which may not be practical to use as array indexes as this adds an unnecessary overhead and decreases the readability the ids are reassigned with integers starting from root of the tree with a breadth first approach this operation provides better readability with fast access to array elements while decreasing the size of the xmi file by approximately 24 packagedelement name flood id v wtsblneewh17zd5sb95q xsi type uml class packagedelement name flood id 1001 xsi type uml class 2 1 3 implementation of inheritance the ontology development process shows a strong resemblance to object oriented design although there are vital differences noy and mcguinness 2001 a fundamental feature of both is support for inheritance inheritance in ontologies means that when a child object inherits from a parent object the child will demonstrate the behavior of the parent rumbaugh et al 1991 inheriting the relationships attributes restrictions and annotations for instance the concept instrument has an attribute called manufacturer and sub classes such as radar and rain gauge the class radar should also have the attribute manufacturer implicitly by the formal definition instead of discovering such relationship at runtime ontology is processed to unfold the inherited characteristics of entities in result of a space time tradeoff the time complexity of this process is o t n where n is the number of concepts in the ontology and t is the number of elements in the inheritable property group e g attribute relationship constraint which has the most elements 2 1 4 database storage storage of the resulting data structures in a database allows for efficient access the ontology is designed as an extendible structure and it is crucial that it can be updated with limited effort the module employs a script whose execution will be triggered only if the parser is called with a predetermined flag passed to the script once the flag is triggered the script will be executed to convert the data structures into an intermediate format i e json and save it to the database 2 2 nlp module the processing of natural language queries is a critical component of flood ai extracting the true intent of the question with details to answer the question is a challenging process because the same knowledge can be requested by varying questions and similar questions might have different meanings or intent many nlp tools and systems e g stanford nlp wit ai nlptools were evaluated for web based systems most of these libraries and tools require more time than expected from a web based system to parse the question to extract different syntactic features they also provide limited capabilities to personalize the methods to connect ontologies a natural language processing library is developed for the knowledge engine and optimized for question parsing with a selected domain knowledge the nlp library which was developed for the knowledge engine is not restricted to the environmental field and can be used with any other domain and field parsing and extracting useful information i e features takes place at several stages in the workflow including extraction of ontological concepts and explicit or implicit relationships location time and date intent and output type mathematical and statistical operations query parameters miscellaneous terms and overcoming typos or issues in the question table 1 presents several sample questions and their components as parsed by the engine 2 2 1 extraction of ontological entities the ontologies are stored in a relational database preserving the ontological relationships and features these records will be accessed to process each question during processing the ontology is downloaded and stored in the client side until the session is closed using http cookies after ontology is retrieved from the database the system searches the question for any ontological concepts or attributes the system then discovers relationships between these concepts and attributes to use in the scoring module it informs the user if any information is missing needed to produce the desired results with high confidence during the processing step for ontological words the system takes plural versions into account as well as different verb tenses because some technical terms can be used interchangeably in questions e g drainage area and river basin lambert 2006 a comprehensive synonym dictionary is defined in the ontology if any synonym is found in the question it is then matched to the corresponding entity or concept within the program flow disambiguation caused by polysemy is overcome using the contextual relationships for example the attribute stage represents the level of flood e g action stage minor flood stage if its relationship to the concept flood is discovered whereas it means the height of water if it is related to the concept river lemmatization in ontological entities is achieved by representing their names and synonyms in their base form as well as using a lookup dictionary for lemmas such as good and better these mechanisms in ontology processing help the knowledge engine better understand the questions when compared to a conventional keyword based search 2 2 2 extraction of location information accurate deduction of location information is essential to answer questions with the correct geographical context and spatial query customization location information is categorized into several groups including state county city community river and implicit location terms the system finds explicit location terms by searching the relational database of locations with metadata which currently covers locations only within the state of iowa implicit location terms include word groups that suggest a location and but do not state it directly e g my location my community my watershed and upstream when the nlp module encounters such terms it has two strategies to estimate the approximate location of the user depending on the client device and the communication channel if the user s device and the current channel used to access the engine allow sharing geolocation fine or coarse geolocation is retrieved by using gps or network positioning however in cases of the inability in retrieving user s geolocation and the possibility of user requesting to define a home community different than user s current location knowledge engine asks back to user to explicitly state user s address this address is then parsed and converted into a structured geolocation using google geocoding api both strategies result in saving of the retrieved geolocation to allow query mapping and execution module to assess the closest community river or other locational entity depending on the expectation of the user s question if a user does not specify a location and user s question requires location information the knowledge engine assumes that the user implies user s current location and refers to the above strategies 2 2 3 extraction of time information accurate assessment of date and time is one of the most challenging issues for the nlp module because there are many ways of describing date and time information in the questions and because of limitations with the available spatiotemporal data and information if the user specified time doesn t satisfy any query requirement the engine has three options the engine either correlates the question with current or possible time information asks for missing specific time information as an input or answers the question with possible time variations for example the user might ask for stream level information for a stage sensor this way what was the stream level for squaw creek four days ago the data frequency is 15 min for stream sensors for this example since the user didn t specify a specific time information the system can use three different strategies to provide a solution a use the current time of the question as provided by user e g 4 pm on july 5 and subtracting four days e g 4 pm on july 1 b ask the user to provide a specific time or c return a result that shows all stream values in a graph for the whole day four days ago with maximum and minimum values pointed out this uncertainty emerges in many cases in different ways the decision on which strategy to choose has been left to the queries so that specific cases can be taken into account for explicitly specified dates and times the engine is capable of extracting variations of statements including 25th of june 2008 at 5 30 pm tomorrow night and five days ago 2 2 4 intent and output type analysis it is also important for the system to understand what result the user wants for his or her question possible output types from the knowledge engine are listed in the table 2 extracting the intent of the question is mainly based on keywords such as where when what what and which location how how long and how many these keywords are not enough to extract the true intent of the question additional concepts and components within the question must be extracted to get an accurate intent estimation we can separate definition and scientific questions using the parts of speech information and the type of ontological objects keywords in a question such as definition meaning and description can help connect the intent to a generic scientific output however for some keywords that have a numerical value or describe an attribute of a concept this can be a challenge for example questions such as what is the definition of rating curve are not challenging because they include the keyword definition however if the question is what is the return period for the current rainfall or flood map then the intent is a numeric value rather than a definition some keywords can be an attribute of a concept or event and require a numerical value the user needs to specify concepts or supporting information in the question to request a numeric value for the concept inclusion of an event or additional supporting keywords increase the likelihood that the question is an analysis question then a definition question for example both questions what is the river level and what is the weather lack location time and other necessary information and aren t necessarily asked in the best form of english these questions might look like definition questions to the knowledge engine although they expect sensor readings and analyses as answers the system considers the number and relationship of the ontological entities in the question as well as the likelihood that they are asked with the intent of receiving an analysis is assessed if the nlp module decides that the question was asked with analysis intention it assumes the closest location community or river to the user and the current time to process the data and returns the answer accordingly natural language questions will often not be in the correct structure furthermore the capability of the engine being used by many services including chat applications elicits a more complex question set the rise of real time communication technologies e g sms email chat multi player gaming has tailored a new text language for continuous and dense conversations leclair 2011 use of slang abbreviations and inaccurately structured sentences is very common to overcome the difficulties presented by the informality of the written natural language the nlp system uses a third party spell checker upon the transfer of an input question to the server the library responds with an object consisting of the potential spelling mistakes and suggestions to correct them this is not only useful to correct misspelled words but also to understand what the user intended to ask judging by the context in addition to using spell checker the engine also facilitates a dictionary of the common abbreviations especially popular among young users that are out of the range of the checker e g 2moro tomorrow 2nite tonight 2 3 query mapping and execution after extracting all useful information from the question the next step for engine is to decide on a strategy to answer the question for any given question the answering system requires information on which databases to use which analyses to apply and in which format to provide the answer however it wouldn t be practical to provide guidance for every possible question hence we introduce question models for the knowledge engine to understand the concept of the question model consider the example what was the coolest day in 2008 using historical weather data provided by noaa and other agencies we can solve this question with a query there are hundreds of different question variations that rely on querying the weather data instead of writing queries for all these different questions we design a model for questions that have similar intents and or answer expectations the process of mapping these questions to question models requires scoring of different features and metrics within the question the overall process for scoring is described by a flow chart fig 2 the question model consists of two parts a natural language knowledge model description for each question type and corresponding procedures to answer the question the nlp module parses and analyzes the descriptions for actual questions and then saves them in the database for fast access to be matched with the questions that have similar solutions or intents the benefit of parsing the descriptions by the nlp module which is also used to parse the actual questions rather than manually creating a structure for each question type is that it allows the system to grow dynamically as the scoring metrics and features discovered by the nlp module change it also assures greater flexibility for adding new question descriptions even by domain experts who don t have in depth computer science experience there are basic guidelines to follow when creating a new question description for instance if the question will require time and or location information the keywords space time and or spacetime should appear in the description judging by the information given in the question a single question model can generate all query combinations for a certain intent instead of writing hundreds of queries manually another advantage of the question model is that it allows the addition of more models easily as the scope of the engine grows without affecting the stability of the system number of question models are related with the amount of knowledge that engine can cover as new datasets map layers and information connect to the system new knowledge is covered by adding new questions models or modifying the existing ones question models are defined manually by developers based on user questions and available data and information a number of potential knowledge categories for questions are determined in the system however there are no strict lines between these categories and questions may expect an answer that will have components in multiple categories that is why the question models are assigned certain weights to accurately return the answer that the user intended the probabilistic value of each question model s affinity to the user s question are assessed by examining the common features and criteria for example if a parameter e g 28 feet in what is the flood damage for my community with 28 feet of stage height is found in the input question the models that expect or allow custom parameters for their queries will have a higher affinity by a factor of a pre assigned weight these weights e g c n c f u s e d determined manually upon review of the success rate of mapping sample input questions to question models in the future the weights can be automatically optimized using supervised learning methods once enough training data is gathered affinity of the input question to the question models can be formulated as a f f i n i t y  q c 1 x 1 c 2 x 2 c 3 x 3 c n x n where c 1 c 2 c n weights i e coefficient to describe the importance of the feature used for mapping the input question to question models x 1 x 2 x n number of matches in the corresponding feature i e p 1 p 2 p n a input question processed by the nlp module q question model the matching of features e g ontological elements intent output type predefined terms time and location between input question and a question model can be formulated as p n i 1 k j 1 l f s i r j s i  s r j  r where function f is defined as f x y x x y o t h e r w i s e and s and r are sets of extracted information for each feature for the input question and the question model respectively matches in certain features may require additional processing to fuse their scores with a distinct coefficient for example related ontological concepts attributes and relationships can be checked and scored as a  q a  q c f u s e d g p o n t o l o g i c a l f e a t u r e p o n t o l o g i c a l f e a t u r e where g is a function that takes a set of ontological elements as input and returns the respective parents for example consider two questions what is the flood condition and what is flood the first question clearly asks for the current flood condition of the user s location with implicit time and location values on the other hand the second user asks for the definition of flood though both questions look quite similar the question model that can answer the first question will be expecting time and location information since there is no explicit time and location specified in the first question it is not a reliable criterion by itself for mapping the question correctly once the questions are mapped onto to the ontology both of the questions will be connected to the flood concept however condition is an attribute of the flood concept which implies a more specific target since attributes define the characteristics of their associated concept as in this case having an ontological concept and a related attribute in a question will result in a higher affinity because of its higher weight once a question is mapped to a model it follows the procedure to generate knowledge within the given parameters datasets are acquired from external sources in a scheduled structure to reduce the data acquisition traffic and improve the performance of queries by keeping a small subset of data 30 days in the system database most of the queries include time and location components which allow queries to quickly reduce to smaller spatial and temporal domain as the usage and data coverage increase scaling challenges can be addressed via different approaches for example results of queries with the highest frequency of access can be cached to return the output faster the components and scope for question models are described in table 3 while mapping a given question is important deciding whether a question should or should not be mapped is also essential filtering a completely unrelated question is relatively easy compared to a question that has some similarities to a question model with a low matching score instead of choosing the model with highest affinity value for the question we specified a confidence threshold if the question remains under the confidence threshold the system presents potential supported questions that the user can select from even if there is only one model that demonstrates any resemblance to the original question this method presents an observable intelligence of the system to the user by showing a more natural interaction this method can be further enhanced with a conversational approach by asking the user a question e g did you mean current flood conditions for your community to clarify the user s question 3 implementation we implemented flood ai knowledge engine using a server side scripting language i e php hypertext preprocessor to benefit from the server s computational power and to accommodate frequent database access various built in functions and extensions of php were utilized throughout the software several agile software development processes were used during development including iterative continuous development and behavior driven design we have not completely adopted the agile process in the system because there is no notion of customer flood ai is created in modular architecture with an extendable structure to improve its scope and expand to other domains flood ai is powered by the iowa flood information system s ifis models and data and information resources the ifis provides real time flood data access analysis and visualization capabilities for flood ai in support of flood preparedness and response for iowa communities flood ai can be implemented as a service in many cases providing a flexible system that can be integrated with various devices and tools that support voice recognition control and interaction flood ai implementation options include web integration ifis automated workflow systems ifttt ms flow chat bots microsoft skype facebook messenger and home automation devices google home amazon echo 3 1 software as a service flood ai is designed and structured as a stand alone software as a service saas application that can be incorporated into different software systems through its application program interface api the saas model advises that applications can be offered in a centralized server to allow access to users via the internet rather than requiring the user to install the application on client machine fox et al 2013 by adopting saas model flood ai offers several benefits as follows a developers can update the knowledge engine and the server hardware at their convenience without updating the user application b users can operate the application without worrying about whether their hardware is sufficient c the user application does not need to comply with different computing environments thus minimizing compatibility issues and d frequently updated environmental big data is better offered online from a centralized server storage of user data in the engine s servers does not constitute a security or privacy concern because flood ai does not require authentication or ask for users personal information the system s response time is primarily dependent on the user s internet connection because most of the processing is handled on the server side and takes milliseconds saas applications are suitable for deployment and maintenance in platform as a service paas infrastructures that undertake the responsibility of management and scaling it allows horizontal scaling to keep up with the increasing number of users without modifying the core of the application this scaling is essential in flood ai s life cycle because of the public s and authorities changing demands for flood related knowledge in the case of disasters or issued risks people will tend to request more information to understand the effects of the recent event during a past flood event in iowa september 2016 the iowa flood information system went offline for a couple of minutes because of use by more than 80 000 people in a short period because of a scaling issue affecting some components although flood ai is currently hosted on a conventional server it will be deployed to a paas infrastructure in the future to benefit from the horizontal scaling and reduced maintenance for server administration 3 2 integration with web based systems flood ai is integrated on a web platform i e ifis as a use case on how web based systems can benefit from the knowledge engine web integration of flood ai is provided as a proof of concept for integration of the knowledge service to a platform and as an example for possible user input methods web integration will be the most common approach used by website to incorporate flood ai on their platforms flood ai can be enabled on any website by referencing the core library and adding a couple of lines of code to the website web system integration provides three input methods for user communication with flood ai including question selection from a list typing the question using a keyboard and asking the question by voice using a microphone question selection from a list has proven useful allowing users to explore the capabilities of the engine in addition it looks friendlier to conventional users users can select a question and provide an input e g location time for the question if necessary the interface sends the question to the engine and computes an answer in many formats fig 3 the second input method typing the question is the most common option because of the common use of keyword based search engines voice enabled communication the third input method is supported using the web speech api in html5 html5 speech recognition and synthesis apis are still considered experimental and have only moderate support globally in web browsers 3 3 integration with instant messaging systems internet bots or bots are software modules designed to perform an automated task over the internet dunham and melnick 2008 they are mostly associated with web crawling or ai characters in video games and movies lately the term bot or chatbots is also being used for interactive agents that conduct voice or text based conversations many instant messaging applications e g facebook messenger and microsoft skype have enabled developers to integrate their own chatbots to automate communication with users as if the bots are real people chatbots offer a practical method to access information rather than a new way of presenting information considering that several modern instant messaging applications having more than a billion active users chatbots have the potential to reach a vast number of people simultaneously as we state in this paper one of the goals of our project is to make it as easy as possible to access the knowledge engine and receive flood related information thus we have developed a universal bot application that can be integrated with various messaging platforms flood ai is implemented as a chatbot fig 4 on two of the largest instant messaging communities i e facebook messenger and microsoft skype these platforms serve as an intermediary service between flood ai bot application and the conversation channels flood ai bot is implemented on the server side using node js and deployed on cloud servers azure flood ai communicates with the server each time a user asks a question after receiving a user question the intermediate service analyzes and filters the question before communicating with flood ai filters analyze the question using a keyword based search for any conversational elements in order to separate them from flood queries examples could be what can i ask help do you know any jokes etc if the question is determined to be in the domain of flood ai the bot receives the question and makes a web service request to flood ai api the api is configured with the necessary parameters such as channel specific information and default location if provided since users will tend to have a certain permanent location on which their questions will be based it would be more practical to ask and save each user s base location base location can be assumed for each query unless explicitly specified otherwise in the question another challenge is to keep up with the words that have been frequently used in informal chat english which is processed by engine s nlp module both chat applications require the server to use secured connection ssl as the bot s backend to prevent sniffing eavesdropping or person in the middle a k a man in the middle attacks to discover the security token used to identify the web connection s authenticity security is important not only because bots have the advantage of accessing user profile information but also because the messaging platform might silently listen to the chatting traffic between user and the bot which would have detrimental results in the case of sensitive information transfer 3 4 integration with automated workflow systems many web systems provide data and services to developers or end users through web services and application programming interfaces apis this allows developers to integrate capabilities and data streams from these services to their applications and systems this often requires technical expertise and programming on both the server and client side integrating and communicating with outside services this option is not effective and technically limiting for millions of possible end users who might benefit from these integrated systems automated workflow systems e g microsoft flow ifttt allow end users to create automated workflows between applications and services without any programming flood ai has integrated its various services including flood warning and sensor levels into automated workflow services with which users can easily connect with other applications this brings thousands of opportunities for users to trigger an action when an event is observed on flood ai service users can create workflows using flood ai service to call a number send a social media post turn on lights turn off an irrigation system send sms turn off air conditioning and more based on flood ai information such as a sensor or flood level reaching a certain threshold one of the important use cases is creation of an sms based flood ai workflow that will enable the access of the knowledge system without internet access fieldwork is a critical component of environmental research that forces scientists professionals and decision makers to work in areas where there is no access or limited access to the internet while performing duties such as disaster management during emergencies or deploying and maintaining instruments in the field these professionals may need a critical analysis that relies on real time spatiotemporal data flood ai api is integrated with automated workflow systems to create an sms service allowing quick and easy access to the capabilities of the knowledge engine without using the internet users can send questions to flood ai system and receive the computed answer in accordance with short message service standards integration with automated workflow systems allows flood ai to connect with many applications used in daily life by non technical people because no coding experience is required to merge different solutions that can semantically be linked together 3 5 integration with home automation devices smart speakers are internet enabled wireless stand alone devices that usually incorporate a smart virtual personal assistant with the ability to access a variety of platforms and services they allow users to interact with intelligent personal assistants via voice commands to achieve certain tasks such as turning down the temperature in a home asking for a recipe or ordering a product from a selected supplier such automated devices are best suited for use in home environments with a reliable internet connection the flood ai system is integrated with two smart home automation devices i e amazon echo and google home to enable end users to access flood related information in a conversational manner while eliminating the need to initiate any kind of software or application it provides a more human interaction by allowing the user to ask a question as if calling out to a colleague for help this also reduces the steps needed to access information and generate knowledge which often requires either a smartphone or web application and manual interaction by the user to initialize the application 4 verification and validation during the development of the knowledge engine a group of stakeholders who work in the domain of environmental sciences as well as general public provided opinion and question suggestions these inputs are benefited to elicit the requirements of the software and define the user expectancy the knowledge engine utilizes various dynamic testing methods to verify that it conforms to the specifications and validate that the resulting product satisfies the user unit testing is employed to assure singular functions to perform their intended duty for example a function that is assigned to transfer data e g curl from an outside source using a certain type of protocol e g https may act unexpectedly for various reasons since it depends on an outside source furthermore accessing outside sources may require a proxy server depending on the host server s regulations thus performing individual tests in micro level allows the developers to identify and separate potential issues automatically in case of software update and server change integration testing has been employed in a bottom up manner to assure different modules e g ontology management module nlp module query mapping and execution interact with each other properly without causing any complications this is particularly important due to the knowledge engine s adaptation of soa as described in the introduction of section 2 finally acceptance i e system tests have been defined to automatically test the whole system from the user s perspective by defining pass fail conditions for user stories acceptance tests are created for each major variation of a question model to assure the user will get the expected answer after each update to the engine database or the server acceptance tests are run to find any complications this paper focuses on the architecture and development of the knowledge engine and its integration to different communication channels validation of the software currently depends on the automated methods to enable future analysis on the user satisfaction the knowledge engine saves the question text answer generated by the engine the platform it s asked from date and time and location if available for every question this data can further be examined manually by a human to find any discrepancy between the generated answer and the original question s intended answer in the future the knowledge engine will ask for user feedback in each session to determine the unsatisfactory questions extensive validation will be applied as system collects questions during operation from a large user base 5 conclusions and future work this paper provides an overview of flood ai knowledge engine its unique information interfaces and its functionality as an educational and operational tool as well as various implementations of the system to provide knowledge on flood related data and resources we have used a comprehensive flood ontology with an extendable structure to develop a web based knowledge system for emergency preparedness and response as opposed to the conventional keyword based or structural search approaches this system allows users to ask natural language questions and instantaneously receive computed answers flood ai is developed as a software as a service saas application which frees the user from the inconvenience of the installation process and adhering the hardware requirements it also enables the integration of the engine with third party software to allow access from various intermediates including web based systems e g ifis agent based bots e g microsoft skype facebook messenger smartphone applications e g smart assistant and automated web workflows e g ifttt ms flow all these implementation options will make it easy for users to access flood ai from any communication channel familiar and comfortable for them it also makes it possible for millions of users to access the system alongside flood ai s potential to assist the public decision makers and authorities as they respond to disasters the system also can raise public awareness to natural hazards the service oriented architecture of flood ai allows each module to be independent from each other and communicate via application interfaces domain knowledge of the system comes from the knowledge base created using the information systems and flood ontology ontology of flood ai although focused on floods is structured to allow extensions in the field of natural disasters e g wildfire earthquake meteorological disasters and space disasters experts in these areas can leverage the knowledge engine by extending the ontology to their domain and provide data and resources in their field acknowledgement the work reported here has been possible with the support and work of many members of the iowa flood center and iihr hydroscience engineering at the university of iowa authors specifically thank muhammed ali sit for his contributions on smart home automation systems 
26341,communities are at risk from extreme events and natural disasters that can lead to dangerous situations for residents improving resilience by helping people learn how to better prepare for recover from and adapt to disasters is critical to reduce the impacts of these extreme events this project presents an intelligent system flood ai designed to improve societal preparedness for flooding by providing a knowledge engine that uses voice recognition artificial intelligence and natural language processing based on a generalized ontology for disasters with a primary focus on flooding the knowledge engine uses flood ontology to connect user input to relevant knowledge discovery channels on flooding by developing a data acquisition and processing framework using environmental observations forecast models and knowledge bases the framework s communication channels include web based systems agent based chatbots smartphone applications automated web workflows and smart home devices opening the knowledge discovery for flooding to many unique use cases graphical abstract image keywords intelligent systems natural language processing knowledge generation ontology disaster preparedness information communication software availability name flood ai knowledge engine developersyusuf sermet and ibrahim demir yusuf sermet and ibrahim demiriowa flood center university of iowa contact information 100 stanley hydraulics lab iowa city ia 52242 usa software requiredinternetbrowser messaging applications program language php postgresql server side html javascript client side availability and costany user can access flood ai at prototype stage upon request at no cost any user can access flood ai at prototype stage upon request at no cost it will be free to the public at the operational stage 1 introduction an immense amount of data is constantly being generated from observations and simulations to monitor environmental conditions in real time and forecast potential events demir et al 2015 the national research council nrc 2012 report puts forth a vision of a nation that is resilient to extreme events by the year 2030 the report highlights the importance of data notes existing gaps in our information and acknowledges the need to address these challenges suggesting that every individual should have access to the risk and vulnerability information they need to make their communities more resilient recent breakthroughs in sensor networks and remote sensing technologies greatly facilitate this process and allow scientists to gather large scale high resolution datasets on the environment water quality and weather conditions most of these datasets are shared through custom interfaces and technical formats for limited stakeholders making it difficult for the public or other non targeted groups to effectively access and understand the data ames et al 2012 advancements and new information technology techniques are making it possible to manage analyze and present large scale environmental data and modeling results acquired from various sources on the web xiaodan et al 2010 accessing data from these systems often requires navigating between many interfaces in the information system or opening numerous web based resources using conventional search methods system managers should use workflows to organize the information in these systems presenting the data in a curated and structured format using ontologies to improve effective data access and sharing generalized workflows allow users to easily access and share the data information and modeling results gil et al 2016 ontologies capture concepts attributes and relationships to structure a phenomenon while also explicitly defining the necessary rules and constraints gruber 1993 there has been extensive work in many disciplines on information systems that aim to facilitate knowledge retrieval and discovery using ontologies e g cui et al 2017 kaufmann et al 2006 lopez et al 2007 moussa and abdel kader 2011 aqualog lopez et al 2007 presents a question answering system that accepts a natural language question uses an ontology to refine queries and measure similarities of the user query with the appropriate knowledge bases and returns the answer querix kaufmann et al 2006 provides a natural language interface that queries ontologies by parsing the textual input and executing a query to return the answer kbqa cui et al 2017 shows a template based evaluation of an input question with complex and chained relationships in environmental science ontowedss ceccaroni et al 2004 presents a decision support system in the wastewater treatment domain that uses an ontology to augment the capabilities of rule based and case based reasoning systems in 2007 wiegand 2007 discussed how geospatial e government portals can allow enhanced search and access to information using multiple ontologies gcp crop information system framework manansala et al 2007 is based on ontology development data annotation and data queries using ontology a web based geospatial problem solving environment jung et al 2013 for earthquakes provides formal definitions that web service providers present and integrate with geographic information services using domain expert approved ontologies wanner et al 2015 introduced an ontology based decision support system that acquires data from various sources on the web to provide personalized environmental information such as the pollen traffic in certain wind temperature and air conditions in the natural disasters domain ontofire kalabokidis et al 2011 provides an ontology based geo portal with a focus on wildfires the user interacts with the system through a browser by discovering the semantic relationships using lists without any natural language input qiu et al 2017 introduced a flood disaster management system fdms that expresses environmental model results and disaster related datasets in the form of ontologies on the web the ontology approach enables the autonomous behavior of fdms while accessing datasets and retrieving information to decrease human interaction thus saving time and expediting disaster response this paper presents an intelligent system flood ai for knowledge generation and communication on flood related data and information flood ai uses disaster ontologies natural language processing artificial intelligence methods and voice recognition to generate knowledge the developers organized the flood related data and modeling results to provide definitive answers to factual inquiries the knowledge engine uses the flood ontology and concepts to connect user input to relevant knowledge discovery outputs on flooding it includes a data acquisition and processing framework for existing environmental observations forecast models and social network data streams a communication framework supports user interaction and delivery of information to users the interaction and delivery channels include voice and text input via web based systems agent based chat bots e g microsoft skype facebook messenger smartphone and augmented reality applications e g smart assistant automated web workflows e g ifttt ms flow and smart home devices e g google home amazon echo to open the knowledge discovery for flooding to thousands of cases and applications one of the major contributions of this work is the development of knowledge generation engine which accept natural language queries to access and analyze vast amount of flood related data and models via extensive set of communication channels for ease of access in contrast to the previous works flood ai eliminates the need for traversing through complex information systems and datasets by allowing the user to access the knowledge as if by talking with an actual flood expert furthermore extensible and modular design of knowledge engine allows its adaptation by different domains as well as for different purposes the remainder of this article is organized as follows section 2 presents the system design and describes the disaster and flood ontology as well as the structure and details of the knowledge engine and its components section 3 focuses on the implementation of flood ai for various communication and interaction channels and devices section 4 discusses future work and provides conclusions 2 knowledge engine flood ai uses the data and resources of the iowa flood information system ifis a generalized cyberinfrastructure platform for flood preparedness and response to populate and test its capabilities for flood related knowledge generation the ifis is a web based platform developed by the iowa flood center ifc to provide access to flood inundation maps real time flood conditions flood forecasts and flood related data information and interactive visualizations for communities in iowa demir and krajewski 2013 krajewski et al 2017 the ifis is optimized with custom database structures and queries demir and szczepanek 2017 for hydrological datasets allowing seamless user access flood ai s knowledge engine is confined on both data and information domain i e spatial and temporal boundary and context and ontology level although the system is generalized the domain limits are set to demonstrate the system with a use case the system is limited to provide knowledge only on weather and flooding and data domain is limited to the state of iowa to cover up to 1 month temporal scale 14 days past and 10 days future flood ai consists of ontology management natural language processing nlp and query mapping and execution qme modules fig 1 the ontology management module accepts and processes the given ontology to provide domain specific knowledge to the engine while easily storing the ontology in a relational database this module is used only when the ontology is updated the nlp module uses ontology to extract useful information from the question including location date and time ontological entities and relationships and intent the query mapping and execution module maps the question to one of the models and generates the desired knowledge using information extracted from the question the nlp and qme modules are integrated to form the inference engine the knowledge engine can be integrated to various interfaces as described in the implementation section flood ai uses service oriented architecture soa in which each module has an api that interacts with other apis rather than a single api for the whole system soa is a software design approach used to create an architecture based on the use of services the advantage of this approach is that it offers each flood ai module as a service allowing applications to benefit from the system according to their needs for example if an ontology in another domain was given to the ontology management module it would perform the same process independently assuming the ontology is formed with the specifications described in the following sections the nlp module can use the output of the ontology module to analyze the natural language question in that domain this structure allows the expansion of the knowledge engine to other natural disasters as well as any science or engineering domain another benefit of this architecture is that separate modules can be used by applications with different objectives for instance an application that retrieves flood related tweets from twitter can analyze them using only the ontology management and nlp modules without need for the qme module 2 1 ontology management module we developed a comprehensive flood and information system is ontology that can be extended to other natural disasters as part of this project we determined the scope of the ontologies from the available data and resources on flooding and the possible questions that the flood ai system will answer domain knowledge and datasets are integrated from various sources including national oceanic and atmospheric administration noaa federal emergency management agency fema united states geological survey usgs iihr hydroscience engineering iowa flood center ifc online knowledge systems e g wikipedia and via consulting to domain experts ontology development requires involvement of domain experts who may lack an advanced level of experience in computer sciences to facilitate their involvement an intermediary for ontology development is needed which should allow illustration and visual editing to remove any technical complexity simultaneous access to allow experts to collaborate online and intuitive development to provide a convenient and desirable tool and machine readable representation of the ontology for integration with information systems due to their wide adaptation in the industry and academia using uml presents the unique opportunity to benefit from the vast number of mature tools that satisfies above objectives we implement ontologies by leveraging an online collaborative unified modelling language uml tool that allows visual editing of the ontology these features simplify the development process and assure the integrity of the ontology development through a collective process and involvement of domain experts uml models can be expressed in xml metadata interchange xmi standard to assure machine readable format we selected the xml format for its flexibility in formatting and representation compared to data models or formats we implemented the flood and is ontologies designed for the system using a uml intermediary that can extract the class diagrams into an xmi standard for use in the engine uml class diagrams can be expressed using xmi without any loss of information since both are standards of open management group omg representation of a uml class diagram in xmi standard results in the same tree structure independent of the intermediate used in the process the ontology used in the knowledge framework is converted to a relational database format for its effective use in web based systems ontology plays a crucial role for processing and mapping of each natural language question as well as determining the suitable data resources representing the ontology in the form of relational database tables instead of the original representation e g xmi rdf xml allows the knowledge engine to improve performance and scale as the demand grows we implemented the ontology module using php scripting language on the server side this allows us to create a separate component to parse the ontology which is in xmi format to create data structures and build relationships while also respecting the dependencies since xmi is written in xml any xml parsing libraries e g simplexml can be used for parsing tree based structure of xml data php comes with built in functions to output an iterable collection of arrays and objects consisting of the ontological elements the ontology management module is used only when the ontology is updated and does not affect the user experience 2 1 1 ontology data model the xml parser returns a collection of concepts with corresponding attributes definitions synonyms hierarchical connections is a relationship and ontological relationships definitions and synonyms for concepts are represented in the description field of an entity in uml which requires additional parsing to store all the information in the ontology and to facilitate access we created relational database tables for concepts attributes relationships synonyms and descriptions once the data structures are in place three steps must be completed to make the data rapidly and efficiently accessible id mapping inheritance implementation and effective database storage 2 1 2 id mapping each element in the ontology such as concepts attributes relationships and annotations has ids to uniquely identify them however these arbitrary ids are auto generated and assigned by the uml to xmi converter each id consists of 23 characters and requires string comparison for every operation which may not be practical to use as array indexes as this adds an unnecessary overhead and decreases the readability the ids are reassigned with integers starting from root of the tree with a breadth first approach this operation provides better readability with fast access to array elements while decreasing the size of the xmi file by approximately 24 packagedelement name flood id v wtsblneewh17zd5sb95q xsi type uml class packagedelement name flood id 1001 xsi type uml class 2 1 3 implementation of inheritance the ontology development process shows a strong resemblance to object oriented design although there are vital differences noy and mcguinness 2001 a fundamental feature of both is support for inheritance inheritance in ontologies means that when a child object inherits from a parent object the child will demonstrate the behavior of the parent rumbaugh et al 1991 inheriting the relationships attributes restrictions and annotations for instance the concept instrument has an attribute called manufacturer and sub classes such as radar and rain gauge the class radar should also have the attribute manufacturer implicitly by the formal definition instead of discovering such relationship at runtime ontology is processed to unfold the inherited characteristics of entities in result of a space time tradeoff the time complexity of this process is o t n where n is the number of concepts in the ontology and t is the number of elements in the inheritable property group e g attribute relationship constraint which has the most elements 2 1 4 database storage storage of the resulting data structures in a database allows for efficient access the ontology is designed as an extendible structure and it is crucial that it can be updated with limited effort the module employs a script whose execution will be triggered only if the parser is called with a predetermined flag passed to the script once the flag is triggered the script will be executed to convert the data structures into an intermediate format i e json and save it to the database 2 2 nlp module the processing of natural language queries is a critical component of flood ai extracting the true intent of the question with details to answer the question is a challenging process because the same knowledge can be requested by varying questions and similar questions might have different meanings or intent many nlp tools and systems e g stanford nlp wit ai nlptools were evaluated for web based systems most of these libraries and tools require more time than expected from a web based system to parse the question to extract different syntactic features they also provide limited capabilities to personalize the methods to connect ontologies a natural language processing library is developed for the knowledge engine and optimized for question parsing with a selected domain knowledge the nlp library which was developed for the knowledge engine is not restricted to the environmental field and can be used with any other domain and field parsing and extracting useful information i e features takes place at several stages in the workflow including extraction of ontological concepts and explicit or implicit relationships location time and date intent and output type mathematical and statistical operations query parameters miscellaneous terms and overcoming typos or issues in the question table 1 presents several sample questions and their components as parsed by the engine 2 2 1 extraction of ontological entities the ontologies are stored in a relational database preserving the ontological relationships and features these records will be accessed to process each question during processing the ontology is downloaded and stored in the client side until the session is closed using http cookies after ontology is retrieved from the database the system searches the question for any ontological concepts or attributes the system then discovers relationships between these concepts and attributes to use in the scoring module it informs the user if any information is missing needed to produce the desired results with high confidence during the processing step for ontological words the system takes plural versions into account as well as different verb tenses because some technical terms can be used interchangeably in questions e g drainage area and river basin lambert 2006 a comprehensive synonym dictionary is defined in the ontology if any synonym is found in the question it is then matched to the corresponding entity or concept within the program flow disambiguation caused by polysemy is overcome using the contextual relationships for example the attribute stage represents the level of flood e g action stage minor flood stage if its relationship to the concept flood is discovered whereas it means the height of water if it is related to the concept river lemmatization in ontological entities is achieved by representing their names and synonyms in their base form as well as using a lookup dictionary for lemmas such as good and better these mechanisms in ontology processing help the knowledge engine better understand the questions when compared to a conventional keyword based search 2 2 2 extraction of location information accurate deduction of location information is essential to answer questions with the correct geographical context and spatial query customization location information is categorized into several groups including state county city community river and implicit location terms the system finds explicit location terms by searching the relational database of locations with metadata which currently covers locations only within the state of iowa implicit location terms include word groups that suggest a location and but do not state it directly e g my location my community my watershed and upstream when the nlp module encounters such terms it has two strategies to estimate the approximate location of the user depending on the client device and the communication channel if the user s device and the current channel used to access the engine allow sharing geolocation fine or coarse geolocation is retrieved by using gps or network positioning however in cases of the inability in retrieving user s geolocation and the possibility of user requesting to define a home community different than user s current location knowledge engine asks back to user to explicitly state user s address this address is then parsed and converted into a structured geolocation using google geocoding api both strategies result in saving of the retrieved geolocation to allow query mapping and execution module to assess the closest community river or other locational entity depending on the expectation of the user s question if a user does not specify a location and user s question requires location information the knowledge engine assumes that the user implies user s current location and refers to the above strategies 2 2 3 extraction of time information accurate assessment of date and time is one of the most challenging issues for the nlp module because there are many ways of describing date and time information in the questions and because of limitations with the available spatiotemporal data and information if the user specified time doesn t satisfy any query requirement the engine has three options the engine either correlates the question with current or possible time information asks for missing specific time information as an input or answers the question with possible time variations for example the user might ask for stream level information for a stage sensor this way what was the stream level for squaw creek four days ago the data frequency is 15 min for stream sensors for this example since the user didn t specify a specific time information the system can use three different strategies to provide a solution a use the current time of the question as provided by user e g 4 pm on july 5 and subtracting four days e g 4 pm on july 1 b ask the user to provide a specific time or c return a result that shows all stream values in a graph for the whole day four days ago with maximum and minimum values pointed out this uncertainty emerges in many cases in different ways the decision on which strategy to choose has been left to the queries so that specific cases can be taken into account for explicitly specified dates and times the engine is capable of extracting variations of statements including 25th of june 2008 at 5 30 pm tomorrow night and five days ago 2 2 4 intent and output type analysis it is also important for the system to understand what result the user wants for his or her question possible output types from the knowledge engine are listed in the table 2 extracting the intent of the question is mainly based on keywords such as where when what what and which location how how long and how many these keywords are not enough to extract the true intent of the question additional concepts and components within the question must be extracted to get an accurate intent estimation we can separate definition and scientific questions using the parts of speech information and the type of ontological objects keywords in a question such as definition meaning and description can help connect the intent to a generic scientific output however for some keywords that have a numerical value or describe an attribute of a concept this can be a challenge for example questions such as what is the definition of rating curve are not challenging because they include the keyword definition however if the question is what is the return period for the current rainfall or flood map then the intent is a numeric value rather than a definition some keywords can be an attribute of a concept or event and require a numerical value the user needs to specify concepts or supporting information in the question to request a numeric value for the concept inclusion of an event or additional supporting keywords increase the likelihood that the question is an analysis question then a definition question for example both questions what is the river level and what is the weather lack location time and other necessary information and aren t necessarily asked in the best form of english these questions might look like definition questions to the knowledge engine although they expect sensor readings and analyses as answers the system considers the number and relationship of the ontological entities in the question as well as the likelihood that they are asked with the intent of receiving an analysis is assessed if the nlp module decides that the question was asked with analysis intention it assumes the closest location community or river to the user and the current time to process the data and returns the answer accordingly natural language questions will often not be in the correct structure furthermore the capability of the engine being used by many services including chat applications elicits a more complex question set the rise of real time communication technologies e g sms email chat multi player gaming has tailored a new text language for continuous and dense conversations leclair 2011 use of slang abbreviations and inaccurately structured sentences is very common to overcome the difficulties presented by the informality of the written natural language the nlp system uses a third party spell checker upon the transfer of an input question to the server the library responds with an object consisting of the potential spelling mistakes and suggestions to correct them this is not only useful to correct misspelled words but also to understand what the user intended to ask judging by the context in addition to using spell checker the engine also facilitates a dictionary of the common abbreviations especially popular among young users that are out of the range of the checker e g 2moro tomorrow 2nite tonight 2 3 query mapping and execution after extracting all useful information from the question the next step for engine is to decide on a strategy to answer the question for any given question the answering system requires information on which databases to use which analyses to apply and in which format to provide the answer however it wouldn t be practical to provide guidance for every possible question hence we introduce question models for the knowledge engine to understand the concept of the question model consider the example what was the coolest day in 2008 using historical weather data provided by noaa and other agencies we can solve this question with a query there are hundreds of different question variations that rely on querying the weather data instead of writing queries for all these different questions we design a model for questions that have similar intents and or answer expectations the process of mapping these questions to question models requires scoring of different features and metrics within the question the overall process for scoring is described by a flow chart fig 2 the question model consists of two parts a natural language knowledge model description for each question type and corresponding procedures to answer the question the nlp module parses and analyzes the descriptions for actual questions and then saves them in the database for fast access to be matched with the questions that have similar solutions or intents the benefit of parsing the descriptions by the nlp module which is also used to parse the actual questions rather than manually creating a structure for each question type is that it allows the system to grow dynamically as the scoring metrics and features discovered by the nlp module change it also assures greater flexibility for adding new question descriptions even by domain experts who don t have in depth computer science experience there are basic guidelines to follow when creating a new question description for instance if the question will require time and or location information the keywords space time and or spacetime should appear in the description judging by the information given in the question a single question model can generate all query combinations for a certain intent instead of writing hundreds of queries manually another advantage of the question model is that it allows the addition of more models easily as the scope of the engine grows without affecting the stability of the system number of question models are related with the amount of knowledge that engine can cover as new datasets map layers and information connect to the system new knowledge is covered by adding new questions models or modifying the existing ones question models are defined manually by developers based on user questions and available data and information a number of potential knowledge categories for questions are determined in the system however there are no strict lines between these categories and questions may expect an answer that will have components in multiple categories that is why the question models are assigned certain weights to accurately return the answer that the user intended the probabilistic value of each question model s affinity to the user s question are assessed by examining the common features and criteria for example if a parameter e g 28 feet in what is the flood damage for my community with 28 feet of stage height is found in the input question the models that expect or allow custom parameters for their queries will have a higher affinity by a factor of a pre assigned weight these weights e g c n c f u s e d determined manually upon review of the success rate of mapping sample input questions to question models in the future the weights can be automatically optimized using supervised learning methods once enough training data is gathered affinity of the input question to the question models can be formulated as a f f i n i t y  q c 1 x 1 c 2 x 2 c 3 x 3 c n x n where c 1 c 2 c n weights i e coefficient to describe the importance of the feature used for mapping the input question to question models x 1 x 2 x n number of matches in the corresponding feature i e p 1 p 2 p n a input question processed by the nlp module q question model the matching of features e g ontological elements intent output type predefined terms time and location between input question and a question model can be formulated as p n i 1 k j 1 l f s i r j s i  s r j  r where function f is defined as f x y x x y o t h e r w i s e and s and r are sets of extracted information for each feature for the input question and the question model respectively matches in certain features may require additional processing to fuse their scores with a distinct coefficient for example related ontological concepts attributes and relationships can be checked and scored as a  q a  q c f u s e d g p o n t o l o g i c a l f e a t u r e p o n t o l o g i c a l f e a t u r e where g is a function that takes a set of ontological elements as input and returns the respective parents for example consider two questions what is the flood condition and what is flood the first question clearly asks for the current flood condition of the user s location with implicit time and location values on the other hand the second user asks for the definition of flood though both questions look quite similar the question model that can answer the first question will be expecting time and location information since there is no explicit time and location specified in the first question it is not a reliable criterion by itself for mapping the question correctly once the questions are mapped onto to the ontology both of the questions will be connected to the flood concept however condition is an attribute of the flood concept which implies a more specific target since attributes define the characteristics of their associated concept as in this case having an ontological concept and a related attribute in a question will result in a higher affinity because of its higher weight once a question is mapped to a model it follows the procedure to generate knowledge within the given parameters datasets are acquired from external sources in a scheduled structure to reduce the data acquisition traffic and improve the performance of queries by keeping a small subset of data 30 days in the system database most of the queries include time and location components which allow queries to quickly reduce to smaller spatial and temporal domain as the usage and data coverage increase scaling challenges can be addressed via different approaches for example results of queries with the highest frequency of access can be cached to return the output faster the components and scope for question models are described in table 3 while mapping a given question is important deciding whether a question should or should not be mapped is also essential filtering a completely unrelated question is relatively easy compared to a question that has some similarities to a question model with a low matching score instead of choosing the model with highest affinity value for the question we specified a confidence threshold if the question remains under the confidence threshold the system presents potential supported questions that the user can select from even if there is only one model that demonstrates any resemblance to the original question this method presents an observable intelligence of the system to the user by showing a more natural interaction this method can be further enhanced with a conversational approach by asking the user a question e g did you mean current flood conditions for your community to clarify the user s question 3 implementation we implemented flood ai knowledge engine using a server side scripting language i e php hypertext preprocessor to benefit from the server s computational power and to accommodate frequent database access various built in functions and extensions of php were utilized throughout the software several agile software development processes were used during development including iterative continuous development and behavior driven design we have not completely adopted the agile process in the system because there is no notion of customer flood ai is created in modular architecture with an extendable structure to improve its scope and expand to other domains flood ai is powered by the iowa flood information system s ifis models and data and information resources the ifis provides real time flood data access analysis and visualization capabilities for flood ai in support of flood preparedness and response for iowa communities flood ai can be implemented as a service in many cases providing a flexible system that can be integrated with various devices and tools that support voice recognition control and interaction flood ai implementation options include web integration ifis automated workflow systems ifttt ms flow chat bots microsoft skype facebook messenger and home automation devices google home amazon echo 3 1 software as a service flood ai is designed and structured as a stand alone software as a service saas application that can be incorporated into different software systems through its application program interface api the saas model advises that applications can be offered in a centralized server to allow access to users via the internet rather than requiring the user to install the application on client machine fox et al 2013 by adopting saas model flood ai offers several benefits as follows a developers can update the knowledge engine and the server hardware at their convenience without updating the user application b users can operate the application without worrying about whether their hardware is sufficient c the user application does not need to comply with different computing environments thus minimizing compatibility issues and d frequently updated environmental big data is better offered online from a centralized server storage of user data in the engine s servers does not constitute a security or privacy concern because flood ai does not require authentication or ask for users personal information the system s response time is primarily dependent on the user s internet connection because most of the processing is handled on the server side and takes milliseconds saas applications are suitable for deployment and maintenance in platform as a service paas infrastructures that undertake the responsibility of management and scaling it allows horizontal scaling to keep up with the increasing number of users without modifying the core of the application this scaling is essential in flood ai s life cycle because of the public s and authorities changing demands for flood related knowledge in the case of disasters or issued risks people will tend to request more information to understand the effects of the recent event during a past flood event in iowa september 2016 the iowa flood information system went offline for a couple of minutes because of use by more than 80 000 people in a short period because of a scaling issue affecting some components although flood ai is currently hosted on a conventional server it will be deployed to a paas infrastructure in the future to benefit from the horizontal scaling and reduced maintenance for server administration 3 2 integration with web based systems flood ai is integrated on a web platform i e ifis as a use case on how web based systems can benefit from the knowledge engine web integration of flood ai is provided as a proof of concept for integration of the knowledge service to a platform and as an example for possible user input methods web integration will be the most common approach used by website to incorporate flood ai on their platforms flood ai can be enabled on any website by referencing the core library and adding a couple of lines of code to the website web system integration provides three input methods for user communication with flood ai including question selection from a list typing the question using a keyboard and asking the question by voice using a microphone question selection from a list has proven useful allowing users to explore the capabilities of the engine in addition it looks friendlier to conventional users users can select a question and provide an input e g location time for the question if necessary the interface sends the question to the engine and computes an answer in many formats fig 3 the second input method typing the question is the most common option because of the common use of keyword based search engines voice enabled communication the third input method is supported using the web speech api in html5 html5 speech recognition and synthesis apis are still considered experimental and have only moderate support globally in web browsers 3 3 integration with instant messaging systems internet bots or bots are software modules designed to perform an automated task over the internet dunham and melnick 2008 they are mostly associated with web crawling or ai characters in video games and movies lately the term bot or chatbots is also being used for interactive agents that conduct voice or text based conversations many instant messaging applications e g facebook messenger and microsoft skype have enabled developers to integrate their own chatbots to automate communication with users as if the bots are real people chatbots offer a practical method to access information rather than a new way of presenting information considering that several modern instant messaging applications having more than a billion active users chatbots have the potential to reach a vast number of people simultaneously as we state in this paper one of the goals of our project is to make it as easy as possible to access the knowledge engine and receive flood related information thus we have developed a universal bot application that can be integrated with various messaging platforms flood ai is implemented as a chatbot fig 4 on two of the largest instant messaging communities i e facebook messenger and microsoft skype these platforms serve as an intermediary service between flood ai bot application and the conversation channels flood ai bot is implemented on the server side using node js and deployed on cloud servers azure flood ai communicates with the server each time a user asks a question after receiving a user question the intermediate service analyzes and filters the question before communicating with flood ai filters analyze the question using a keyword based search for any conversational elements in order to separate them from flood queries examples could be what can i ask help do you know any jokes etc if the question is determined to be in the domain of flood ai the bot receives the question and makes a web service request to flood ai api the api is configured with the necessary parameters such as channel specific information and default location if provided since users will tend to have a certain permanent location on which their questions will be based it would be more practical to ask and save each user s base location base location can be assumed for each query unless explicitly specified otherwise in the question another challenge is to keep up with the words that have been frequently used in informal chat english which is processed by engine s nlp module both chat applications require the server to use secured connection ssl as the bot s backend to prevent sniffing eavesdropping or person in the middle a k a man in the middle attacks to discover the security token used to identify the web connection s authenticity security is important not only because bots have the advantage of accessing user profile information but also because the messaging platform might silently listen to the chatting traffic between user and the bot which would have detrimental results in the case of sensitive information transfer 3 4 integration with automated workflow systems many web systems provide data and services to developers or end users through web services and application programming interfaces apis this allows developers to integrate capabilities and data streams from these services to their applications and systems this often requires technical expertise and programming on both the server and client side integrating and communicating with outside services this option is not effective and technically limiting for millions of possible end users who might benefit from these integrated systems automated workflow systems e g microsoft flow ifttt allow end users to create automated workflows between applications and services without any programming flood ai has integrated its various services including flood warning and sensor levels into automated workflow services with which users can easily connect with other applications this brings thousands of opportunities for users to trigger an action when an event is observed on flood ai service users can create workflows using flood ai service to call a number send a social media post turn on lights turn off an irrigation system send sms turn off air conditioning and more based on flood ai information such as a sensor or flood level reaching a certain threshold one of the important use cases is creation of an sms based flood ai workflow that will enable the access of the knowledge system without internet access fieldwork is a critical component of environmental research that forces scientists professionals and decision makers to work in areas where there is no access or limited access to the internet while performing duties such as disaster management during emergencies or deploying and maintaining instruments in the field these professionals may need a critical analysis that relies on real time spatiotemporal data flood ai api is integrated with automated workflow systems to create an sms service allowing quick and easy access to the capabilities of the knowledge engine without using the internet users can send questions to flood ai system and receive the computed answer in accordance with short message service standards integration with automated workflow systems allows flood ai to connect with many applications used in daily life by non technical people because no coding experience is required to merge different solutions that can semantically be linked together 3 5 integration with home automation devices smart speakers are internet enabled wireless stand alone devices that usually incorporate a smart virtual personal assistant with the ability to access a variety of platforms and services they allow users to interact with intelligent personal assistants via voice commands to achieve certain tasks such as turning down the temperature in a home asking for a recipe or ordering a product from a selected supplier such automated devices are best suited for use in home environments with a reliable internet connection the flood ai system is integrated with two smart home automation devices i e amazon echo and google home to enable end users to access flood related information in a conversational manner while eliminating the need to initiate any kind of software or application it provides a more human interaction by allowing the user to ask a question as if calling out to a colleague for help this also reduces the steps needed to access information and generate knowledge which often requires either a smartphone or web application and manual interaction by the user to initialize the application 4 verification and validation during the development of the knowledge engine a group of stakeholders who work in the domain of environmental sciences as well as general public provided opinion and question suggestions these inputs are benefited to elicit the requirements of the software and define the user expectancy the knowledge engine utilizes various dynamic testing methods to verify that it conforms to the specifications and validate that the resulting product satisfies the user unit testing is employed to assure singular functions to perform their intended duty for example a function that is assigned to transfer data e g curl from an outside source using a certain type of protocol e g https may act unexpectedly for various reasons since it depends on an outside source furthermore accessing outside sources may require a proxy server depending on the host server s regulations thus performing individual tests in micro level allows the developers to identify and separate potential issues automatically in case of software update and server change integration testing has been employed in a bottom up manner to assure different modules e g ontology management module nlp module query mapping and execution interact with each other properly without causing any complications this is particularly important due to the knowledge engine s adaptation of soa as described in the introduction of section 2 finally acceptance i e system tests have been defined to automatically test the whole system from the user s perspective by defining pass fail conditions for user stories acceptance tests are created for each major variation of a question model to assure the user will get the expected answer after each update to the engine database or the server acceptance tests are run to find any complications this paper focuses on the architecture and development of the knowledge engine and its integration to different communication channels validation of the software currently depends on the automated methods to enable future analysis on the user satisfaction the knowledge engine saves the question text answer generated by the engine the platform it s asked from date and time and location if available for every question this data can further be examined manually by a human to find any discrepancy between the generated answer and the original question s intended answer in the future the knowledge engine will ask for user feedback in each session to determine the unsatisfactory questions extensive validation will be applied as system collects questions during operation from a large user base 5 conclusions and future work this paper provides an overview of flood ai knowledge engine its unique information interfaces and its functionality as an educational and operational tool as well as various implementations of the system to provide knowledge on flood related data and resources we have used a comprehensive flood ontology with an extendable structure to develop a web based knowledge system for emergency preparedness and response as opposed to the conventional keyword based or structural search approaches this system allows users to ask natural language questions and instantaneously receive computed answers flood ai is developed as a software as a service saas application which frees the user from the inconvenience of the installation process and adhering the hardware requirements it also enables the integration of the engine with third party software to allow access from various intermediates including web based systems e g ifis agent based bots e g microsoft skype facebook messenger smartphone applications e g smart assistant and automated web workflows e g ifttt ms flow all these implementation options will make it easy for users to access flood ai from any communication channel familiar and comfortable for them it also makes it possible for millions of users to access the system alongside flood ai s potential to assist the public decision makers and authorities as they respond to disasters the system also can raise public awareness to natural hazards the service oriented architecture of flood ai allows each module to be independent from each other and communicate via application interfaces domain knowledge of the system comes from the knowledge base created using the information systems and flood ontology ontology of flood ai although focused on floods is structured to allow extensions in the field of natural disasters e g wildfire earthquake meteorological disasters and space disasters experts in these areas can leverage the knowledge engine by extending the ontology to their domain and provide data and resources in their field acknowledgement the work reported here has been possible with the support and work of many members of the iowa flood center and iihr hydroscience engineering at the university of iowa authors specifically thank muhammed ali sit for his contributions on smart home automation systems 
26342,bayesian networks bns are an increasingly popular method for modelling environmental systems the discretization of continuous variables is often required to use bns there are three main methods of discretization manual unsupervised and supervised here we compare and demonstrate each approach with a bn that predicts coastal erosion results reveal that supervised discretization methods produced bns of the highest average predictive skill 73 8 followed by manual discretization 69 0 and unsupervised discretization 64 8 however each method has specific advantages that may make them more suitable for particular applications manual methods can produce physical meaningful bns which is favorable in environmental modelling supervised methods can autonomously and optimally discretize variables and may be preferred when predictive skill is a modelling priority unsupervised methods are computationally simple and versatile the optimal discretization scheme should consider both the performance and practicality of the scheme 1 introduction bayesian networks bns are probabilistic graphical models that can be used to represent causal systems pearl 1988 bns have several key features that make them useful for environmental modelling they can easily handle non linear systems have low computational cost can deal with missing data and data from different sources explicitly include uncertainties and have a simple and intuitive graphical structure that is easily understood by non technical users uusitalo 2007 chen and pollino 2012 as a result bns are an increasingly popular method of environmental modelling aguilera et al 2011 and have been used in a variety of applications to date for example modelling and supporting decision making in water resource management castelletti and soncini sessa 2007 conducting ecological risk assessments pollino et al 2007 and modelling wildlife habitat and population viability marcot et al 2001 modelling coastal vulnerability to sea level rise gutierrez et al 2011 and integrated modelling of socioeconomic and biophysical processes for natural resource management kragt et al 2011 despite their wide applicability to environmental modelling most commonly used bn software packages and algorithms require discrete data which is a limitation because environmental systems are often characterized by continuous attributes this means that discretization of continuous data is often necessary to effectively and efficiently use bns for environmental modelling there are three main methods of discretizing continuous data for use in bns 1 manual in which discretization is specified by an expert user 2 supervised in which the value of the output variable s is used to automatically optimize discretization of other variables in the system and 3 unsupervised in which information about the output variables is not available or not used and discretization is based on the distribution of each individual variable dougherty et al 1995 the process of discretizing continuous data for use in a bn can result in a loss of information from the system friedman and goldszmidt 1996 and can significantly influence bn model performance fienen and plant 2015 nojavan et al 2017 despite this the impacts of different methods of discretization on bn performance have not been well discussed in the literature death et al 2015 nojavan et al 2017 and discretization has been an overlooked and undocumented process in many environmental bn applications to date for example in a recent review of bn applications in environmental modelling aguilera et al 2011 noted that approximately 50 of studies that discretized continuous data for use in a bn did not discuss the discretization method used to begin to address this issue a recent study by nojavan et al 2017 compared different algorithms of unsupervised discretization and found that while no one algorithm consistently outperformed others the method of discretization could influence model performance the study by nojavan et al 2017 did not evaluate manual or supervised discretization supervised discretization algorithms are of particular interest here because unlike manual and unsupervised methods they remain largely unused in environmental bn applications this is surprising as supervised discretization algorithms are an efficient method of autonomously discretizing continuous data to maximize predictability of the output variable and have been shown to produce more predictive models than unsupervised discretization algorithms e g fayyad and irani 1993 dougherty et al 1995 the present study aims to extend the work of nojavan et al 2017 by comparing the effect of manual unsupervised and supervised discretization on the performance of an environmental bn used to predict coastal erosion from storms each method of discretization is evaluated and practical guidelines for their use in future bn studies are proposed 2 methodology 2 1 bayesian networks a bn is a graphical representation of the joint probability distribution of a system the structure of a bn is formally known as a directed acyclic graph and is composed of nodes representing variables in the system and arcs representing causality between nodes e g fig 1 conditional dependencies amongst variables in the system are quantified in conditional probability tables cpts to make predictions of the system these conditional dependencies and the prior distribution of variables are used with bayes theorem 1 p r i o j p o j r i p r i p o j where p r i o j is the predicted or posterior probability of a response r i conditioned on the observation s o j p o j r i is the likelihood function and p r i and p o j are the prior probabilities of the response and observation s respectively discrete data are typically required to learn the cpts describing a system and perform inference with them the discretization scheme of each variable determines its prior distribution and the conditional dependencies learnt by the bn and is therefore a key factor in bn model performance for a thorough introduction into bns the reader is referred to pearl 1988 and charniak 1991 2 2 discretization methods the main methods to discretizing continuous variables in bns can be classified as manual supervised and unsupervised chen and pollino 2012 within each method multiple algorithms have been proposed for developing effective bns across a range of modelling applications 2 2 1 manual discretization manual discretization also referred to as expert discretization involves a user manually selecting discretization thresholds based on physical meaningfulness theoretical knowledge or their expert interpretation of the problem domain chen and pollino 2012 manual discretization can be aided through the use of tools such as histograms or regression trees to better understand thresholds present in the data in a review of 128 published papers on bn applications in environmental modelling aguilera et al 2011 noted that manual discretization was the most common method used in studies that required discretization of continuous data this method is often favored because it allows continuous variables to be discretized at intervals interpretable and relevant to the data or model objectives uusitalo 2007 chen and pollino 2012 and no discretization algorithm or additional computation is required 2 2 2 unsupervised discretization unsupervised discretization is a method of discretizing continuous data based on the intrinsic data distribution of each individual variable it is commonly used to discretize continuous variables for bn applications when manual discretization is not available due to the absence of theoretical or expert knowledge of the data or system being modelled aguilera et al 2011 unsupervised discretization is popular because it is computationally simple and objective the equal width ew and equal frequency ef algorithms tested in this study are two of the most commonly used unsupervised discretization algorithms in environmental applications aguilera et al 2011 chen and pollino 2012 ew discretization divides continuous data into a predefined number of intervals of equal width this method can perform well with approximately uniform continuous distributions but generates inappropriate intervals of imbalanced probabilities when data is highly skewed or contains outliers chen and pollino 2012 ef discretization divides continuous data into a predefined number of intervals of equal frequency ef discretization generates a uniform non informative distribution of the continuous data which is useful for capturing the modes of the distribution nojavan et al 2017 but it can hide outliers in the data which are often of interest in environmental systems and in cases where there is a high frequency of the same value that value may be forced to split into different intervals chen and pollino 2012 using unsupervised discretization algorithms like ef binning that produce non informative prior distributions is often favored because the resulting bn is purely driven by the data i e states in the bn are not predisposed by a prior to apply ew or ef discretization the number of intervals to partition the data into must be specified most bn applications typically use between 2 and 10 intervals uusitalo 2007 2 2 3 supervised discretization supervised discretization is an informative method of discretization that utilizes the state of the output variable to inform and optimize the discretization of each individual input variable supervised discretization algorithms are frequently used in the computer science literature and have been shown to outperform unsupervised discretization when using bns on a variety of datasets dougherty et al 1995 despite this they remain largely absent from the environmental bn literature due in part to a lack of capability in the bn software packages commonly used for environmental bn modelling such as netica norsys software corporation 2017 as well as remaining knowledge gaps between environmental modelling and the broader computer science and machine learning literature like unsupervised discretization a drawback of supervised discretization algorithms is that the thresholds they produce are often physically meaningless in addition supervised algorithms may produce potentially spurious discretization thresholds that are fit to noise in the data rather than thresholds that increase bn predictive skill supervised discretization also requires a discrete output variable to inform the discretization of the continuous input variables this means that if the output variable is continuous a priori knowledge assumptions or an unsupervised discretization method would be required to discretize it before the input variables can be discretized using a supervised method while there are many supervised discretization algorithms available the fayyad irani f i fayyad and irani 1993 and kononenko ko kononenko 1995 algorithms are well tested and are available in commonly used software packages such as r and python both algorithms are based on entropy minimization and effectively iterate through discretization schemes to find the one that maximizes predictability of the output variable minimizes entropy input variables are optimized individually and can have different numbers of discretization intervals the ko algorithm differs from the f i algorithm in that it accounts for entropy bias introduced in multi variate settings when variables have different numbers of discretization intervals kononenko 1995 2 2 4 experimental design to demonstrate the effect of discretization on bn model performance different methods of manual unsupervised and supervised discretization were applied to a bn and evaluated the bn used for this purpose is shown in fig 1 and models the erosion impact of coastal storms on sandy beaches as a function of the pre storm beach volume storm wave power and storm water level each variable in the bn consists of 1600 data points with differing distributions fig 2 for a detailed description of this data the reader is referred to harley et al 2017 in this study for each of the different discretization methods outlined below the continuous data describing the bn fig 2 was discretized the bn was trained on the discretized data and its performance was evaluated as described in section 2 3 the open source software package weka v3 9 1 university of waikato 1999 2016 was used to discretize and evaluate each bn a total of 24 tests were conducted comprising different configurations of manual discretization unsupervised discretization algorithms equal width ew and equal frequency ef and supervised discretization algorithms f i fayyad and irani 1993 and ko kononenko 1995 algorithms as outlined below two different manual discretization schemes were specified from the data table 1 m1 a simpler scheme in which variables were discretized into 4 or fewer intervals and m2 a more complex scheme in which variables were discretized into 5 intervals or fewer the intervals selected for each variable in these schemes were based on expert judgement and are intuitive regularly spaced and hold physical and measurable meaning in the context of coastal engineering and management a total of six unsupervised discretization schemes were tested comprising three ew schemes and three ef schemes of 3 4 and 5 intervals each for the bn tested in this study fig 1 and the amount of data available 1600 data instances a greater number of intervals than this could lead to issues of model overfitting and reduced predictive skill while fewer intervals would produce an underfit model beuzen et al 2018 unsupervised tests are henceforth denoted as the method followed by the number of intervals specified e g ew4 examples of ew and ef discretization are shown in table 1 the supervised discretization algorithms tested here f i and ko methods require a discrete output variable to inform the discretization of the input variables therefore to test the full range of possible discretization schemes the output variable erosion volume fig 1 was first discretized using each of the manual and unsupervised schemes described above and then the two supervised algorithms were applied to discretize the remaining input variables this resulted in a total of 2 supervised methods x 6 unsupervised output discretizations 2 manual output discretization 16 supervised discretization tests supervised tests are henceforth denoted as the supervised algorithm followed by the method used to discretize the output variable e g f i ew4 examples of f i and ko discretization are shown in table 1 2 3 evaluation of discretization methods to demonstrate the effect of each of the 24 discretization schemes on the bn model fig 1 performance 10 repetitions of 10 fold cross validation i e 100 tests were performed and evaluation metrics were recorded cross validation is a widely used method of testing machine learning models that partition data into separate training and testing sets to give an unbiased evaluation of model predictive skill elsner and schmertmann 1994 to assess model performance two metrics were used 1 model accuracy the percent of cases for which the modelled and actual discretized intervals of the output variable are equal ranging from 0 no predictive power to 100 perfect model model accuracy is a measure of a bns predictive skill 2 f measure also known as f score the ratio of a model s recall and precision expressed here as a percentage ranging from 0 poor model to 100 perfect model broadly speaking the f measure provides an indication of how useful the model is over all of the discretized output variable states while accuracy is commonly used as a bn performance metric marcot 2012 it is important to consider what this accuracy actually represents i e does the model perform well over all output states a useful model or just very well on one of them not as useful of a model as shown in fig 3 the higher f measure of model 2 represents a model that performs well over the entire output domain and it is likely to be considered a more useful model than model 1 despite having lower accuracy in general a large imbalance between a model s accuracy and f measure indicates a spuriously predictive model that is only predicting a single discretization interval all of the time 3 results the bn shown in fig 1 was trained and tested with each of the 24 discretization schemes described in section 2 2 4 to evaluate how different discretization methods affected model performance fig 4 presents the average bn accuracy and f measure results from the 10 repetitions of 10 fold cv conducted for these tests demonstrating that there is variability both between and within the different methods of discretization used manual unsupervised or supervised fig 5 summarizes the results of fig 4 to compare these three discretization methods from fig 5 it can be seen that on average supervised discretization produced models with the highest accuracy 73 8 followed by manual discretization 69 0 and unsupervised discretization 64 8 in terms of the f measure the results of supervised and manual methods are comparable with f measures of 64 9 and 65 5 respectively and are higher than the unsupervised methods having an f measure of 51 3 similar variability in performance is observed when summarizing the results of each individual discretization algorithm tested in this study fig 6 of the unsupervised algorithms ew outperforms ef in terms of model accuracy with values of 68 2 and 61 4 respectively fig 6 however the higher ew performance is a result of the ew schemes predicting only the majority output bin all of the time e g fig 3 as evident by the lower f measure of 42 0 compared to an f measure of 60 3 for ef there is no obvious difference between the two different supervised algorithms f i and ko tested with both achieving high average model accuracy and usability similarly there was little variability between the two different manual discretization schemes tested with both also achieving good model accuracy and usability 4 discussion and conclusion at present the impact of different methods of discretization on bn model performance has not been well documented in the environmental bn modelling literature death et al 2015 nojavan 2017 furthermore the use of supervised discretization remains largely absent from environmental bn applications this study demonstrates the potential influence of manual unsupervised and supervised discretization on an environmental bn in our case study evaluating the performance of different manual unsupervised and supervised discretization methods for a bn modelling coastal erosion results reveal that the method of discretization used can influence the performance of the resultant bn as evaluated by its accuracy and f measure in this study supervised discretization produced bns that outperformed unsupervised and manual methods in terms of predictive skill this is consistent with results from studies comparing discretization methods in other fields e g dougherty et al 1995 and advocates for the use of supervised methods when predictive skill is a modelling priority manual discretization was the next best method followed by unsupervised discretization it should be noted that the performance of manual discretization schemes are subject to the quality of expert interpretation of the model domain and the selected discretization thresholds therefore manual discretization could not generally be said to outperform unsupervised methods in a predictive sense however the manual approach allows for expert information to be incorporated into the modelling process suggesting the resulting bn may be more easily interpretable or fit for purpose for the different algorithms of unsupervised ew and ef and supervised f i and ko discretization tested no one algorithm within a specific method outperformed the other in both evaluation criteria i e accuracy and f measure this is consistent with the comparison of unsupervised discretization algorithms conducted by nojavan et al 2017 and indicates that the category of discretization i e manual unsupervised or supervised may be more important than the algorithm used within that category as evident in fig 4 bn performance predictive accuracy and or f measure generally decreased here with an increased number of discretization bins in all unsupervised supervised and manual discretization tests this is a result of the amount of data available to train the bn the conditional probability table cpt of the output variable in a bn quantifies the probability of every single permutation of input variable and output variable bins states so its size increases considerably with the number of discretization bins in the bn 2 s i z e c p t o i 1 n i i where o is the number of bins of the output node and i i is the number of states in the ith input node when the size of the cpt is greater than the amount of data available to train it or when there is limited data available to populate each unique entry variable combination in the cpt model performance typically decreases because it is missing some conditional relationships or they have not been appropriately represented by the available data beuzen et al 2018 for example the ef3 scheme here had a cpt size of 31 all of which were quantified using an average of 15 data points strong representation from the 1600 available in this study and it performed better than the ef5 scheme fig 4 which had a cpt size of 625 54 of which were missing and the rest of which were quantified from only 4 data points on average weak representation therefore when discretizing continuous variables for use in a bn to maximize predictive performance it is important to consider the number of bins used and how this can affect the size and strength of the conditional relations in the output variable cpt learnt from the available data even when using supervised discretization many algorithms such as the fayyad and irani 1993 and kononenko 1995 algorithms used here discretize univariately e g one input variable one at a time to maximize predictability of the output variable and do not consider the number of bins in other variables or the resultant size of the output variable cpt as such cpts generated using supervised methods can be large although the discretization schemes produced by supervised methods generally outperform unsupervised methods figs 4 and 5 because the bins themselves have high predictive utility predictive skill can potentially be further increased by using an expert to reduce the number of bins i e reduce the size of the cpt following supervised discretization in environmental models the interpretability and physical meaning of a discretization scheme is often as important as the performance of a model in this study both supervised and manual discretization performed well however these two methods have quite different practical applications supervised discretization provides an efficient method of discretizing many variables autonomously and maximizing the predictive skill of a model however the thresholds it determines may appear random and physically meaningless particularly in environmental modelling e g table 1 in contrast in manual discretization more physically meaningful discretization thresholds are specified which may be of more importance to environmental modellers and stakeholders a combination of supervised and manual discretization as was tested in some schemes here e g f i m1 ko m1 can provide a balance between a predictive model with physical meaningfulness however a limitation of manual discretization is that it requires a priori expert knowledge of the problem domain unsupervised methods must be used in the absence of this knowledge either to discretize all variables in the bn or to discretize the output node to prepare it for supervised discretization in this study unsupervised discretization algorithms produced the poorest performing bns figs 3 5 however they are computationally simple are generally robust to noisy data and there are many different algorithms available that can be useful for exploring and manipulating the intrinsic properties of a continuous distribution and how this can affect the predictive power of a bn it should be noted that the performance of the different unsupervised discretization algorithms tested here equal width and equal frequency and in the study by nojavan et al 2017 was highly variable and it is recommended that several alternative algorithms be evaluated if using unsupervised discretization to ensure that bn results are not an artefact of the discretization algorithm based on the results and discussion in this study a summary and practical guide for the use of manual unsupervised and supervised discretization in environmental bn modelling is presented in table 2 while bns that incorporate continuous variables and avoid discretization continue to be studied they remain limited in their capabilities and application korb and nicholson 2010 in addition the use of discrete variables rather than continuous is often desirable because they allow for rapid and efficient bn model building produce models that are interpretable and can be efficiently used for probabilistic inference and decision making and can improve the signal to noise ratio in the data by using bins that smooth out small scale fluctuations in the data discretization will therefore continue to be a key aspect of bn modelling in environmental applications it is anticipated that the work presented here will aid in the future application of bns applied to environmental modelling acknowledgements data for this research was partially funded by ongoing support by northern beaches council the australian research council lp04555157 lp100200348 dp150101339 and the nsw environmental trust environmental research program rd 2015 0128 wave and tide data was kindly provided by manly hydraulics laboratory under the nsw coastal data network program managed by the office of environment and heritage oeh the 2nd author is additionally supported through an australian research council future fellowship ft120100269 the lead author is funded under the australian postgraduate research training program 
26342,bayesian networks bns are an increasingly popular method for modelling environmental systems the discretization of continuous variables is often required to use bns there are three main methods of discretization manual unsupervised and supervised here we compare and demonstrate each approach with a bn that predicts coastal erosion results reveal that supervised discretization methods produced bns of the highest average predictive skill 73 8 followed by manual discretization 69 0 and unsupervised discretization 64 8 however each method has specific advantages that may make them more suitable for particular applications manual methods can produce physical meaningful bns which is favorable in environmental modelling supervised methods can autonomously and optimally discretize variables and may be preferred when predictive skill is a modelling priority unsupervised methods are computationally simple and versatile the optimal discretization scheme should consider both the performance and practicality of the scheme 1 introduction bayesian networks bns are probabilistic graphical models that can be used to represent causal systems pearl 1988 bns have several key features that make them useful for environmental modelling they can easily handle non linear systems have low computational cost can deal with missing data and data from different sources explicitly include uncertainties and have a simple and intuitive graphical structure that is easily understood by non technical users uusitalo 2007 chen and pollino 2012 as a result bns are an increasingly popular method of environmental modelling aguilera et al 2011 and have been used in a variety of applications to date for example modelling and supporting decision making in water resource management castelletti and soncini sessa 2007 conducting ecological risk assessments pollino et al 2007 and modelling wildlife habitat and population viability marcot et al 2001 modelling coastal vulnerability to sea level rise gutierrez et al 2011 and integrated modelling of socioeconomic and biophysical processes for natural resource management kragt et al 2011 despite their wide applicability to environmental modelling most commonly used bn software packages and algorithms require discrete data which is a limitation because environmental systems are often characterized by continuous attributes this means that discretization of continuous data is often necessary to effectively and efficiently use bns for environmental modelling there are three main methods of discretizing continuous data for use in bns 1 manual in which discretization is specified by an expert user 2 supervised in which the value of the output variable s is used to automatically optimize discretization of other variables in the system and 3 unsupervised in which information about the output variables is not available or not used and discretization is based on the distribution of each individual variable dougherty et al 1995 the process of discretizing continuous data for use in a bn can result in a loss of information from the system friedman and goldszmidt 1996 and can significantly influence bn model performance fienen and plant 2015 nojavan et al 2017 despite this the impacts of different methods of discretization on bn performance have not been well discussed in the literature death et al 2015 nojavan et al 2017 and discretization has been an overlooked and undocumented process in many environmental bn applications to date for example in a recent review of bn applications in environmental modelling aguilera et al 2011 noted that approximately 50 of studies that discretized continuous data for use in a bn did not discuss the discretization method used to begin to address this issue a recent study by nojavan et al 2017 compared different algorithms of unsupervised discretization and found that while no one algorithm consistently outperformed others the method of discretization could influence model performance the study by nojavan et al 2017 did not evaluate manual or supervised discretization supervised discretization algorithms are of particular interest here because unlike manual and unsupervised methods they remain largely unused in environmental bn applications this is surprising as supervised discretization algorithms are an efficient method of autonomously discretizing continuous data to maximize predictability of the output variable and have been shown to produce more predictive models than unsupervised discretization algorithms e g fayyad and irani 1993 dougherty et al 1995 the present study aims to extend the work of nojavan et al 2017 by comparing the effect of manual unsupervised and supervised discretization on the performance of an environmental bn used to predict coastal erosion from storms each method of discretization is evaluated and practical guidelines for their use in future bn studies are proposed 2 methodology 2 1 bayesian networks a bn is a graphical representation of the joint probability distribution of a system the structure of a bn is formally known as a directed acyclic graph and is composed of nodes representing variables in the system and arcs representing causality between nodes e g fig 1 conditional dependencies amongst variables in the system are quantified in conditional probability tables cpts to make predictions of the system these conditional dependencies and the prior distribution of variables are used with bayes theorem 1 p r i o j p o j r i p r i p o j where p r i o j is the predicted or posterior probability of a response r i conditioned on the observation s o j p o j r i is the likelihood function and p r i and p o j are the prior probabilities of the response and observation s respectively discrete data are typically required to learn the cpts describing a system and perform inference with them the discretization scheme of each variable determines its prior distribution and the conditional dependencies learnt by the bn and is therefore a key factor in bn model performance for a thorough introduction into bns the reader is referred to pearl 1988 and charniak 1991 2 2 discretization methods the main methods to discretizing continuous variables in bns can be classified as manual supervised and unsupervised chen and pollino 2012 within each method multiple algorithms have been proposed for developing effective bns across a range of modelling applications 2 2 1 manual discretization manual discretization also referred to as expert discretization involves a user manually selecting discretization thresholds based on physical meaningfulness theoretical knowledge or their expert interpretation of the problem domain chen and pollino 2012 manual discretization can be aided through the use of tools such as histograms or regression trees to better understand thresholds present in the data in a review of 128 published papers on bn applications in environmental modelling aguilera et al 2011 noted that manual discretization was the most common method used in studies that required discretization of continuous data this method is often favored because it allows continuous variables to be discretized at intervals interpretable and relevant to the data or model objectives uusitalo 2007 chen and pollino 2012 and no discretization algorithm or additional computation is required 2 2 2 unsupervised discretization unsupervised discretization is a method of discretizing continuous data based on the intrinsic data distribution of each individual variable it is commonly used to discretize continuous variables for bn applications when manual discretization is not available due to the absence of theoretical or expert knowledge of the data or system being modelled aguilera et al 2011 unsupervised discretization is popular because it is computationally simple and objective the equal width ew and equal frequency ef algorithms tested in this study are two of the most commonly used unsupervised discretization algorithms in environmental applications aguilera et al 2011 chen and pollino 2012 ew discretization divides continuous data into a predefined number of intervals of equal width this method can perform well with approximately uniform continuous distributions but generates inappropriate intervals of imbalanced probabilities when data is highly skewed or contains outliers chen and pollino 2012 ef discretization divides continuous data into a predefined number of intervals of equal frequency ef discretization generates a uniform non informative distribution of the continuous data which is useful for capturing the modes of the distribution nojavan et al 2017 but it can hide outliers in the data which are often of interest in environmental systems and in cases where there is a high frequency of the same value that value may be forced to split into different intervals chen and pollino 2012 using unsupervised discretization algorithms like ef binning that produce non informative prior distributions is often favored because the resulting bn is purely driven by the data i e states in the bn are not predisposed by a prior to apply ew or ef discretization the number of intervals to partition the data into must be specified most bn applications typically use between 2 and 10 intervals uusitalo 2007 2 2 3 supervised discretization supervised discretization is an informative method of discretization that utilizes the state of the output variable to inform and optimize the discretization of each individual input variable supervised discretization algorithms are frequently used in the computer science literature and have been shown to outperform unsupervised discretization when using bns on a variety of datasets dougherty et al 1995 despite this they remain largely absent from the environmental bn literature due in part to a lack of capability in the bn software packages commonly used for environmental bn modelling such as netica norsys software corporation 2017 as well as remaining knowledge gaps between environmental modelling and the broader computer science and machine learning literature like unsupervised discretization a drawback of supervised discretization algorithms is that the thresholds they produce are often physically meaningless in addition supervised algorithms may produce potentially spurious discretization thresholds that are fit to noise in the data rather than thresholds that increase bn predictive skill supervised discretization also requires a discrete output variable to inform the discretization of the continuous input variables this means that if the output variable is continuous a priori knowledge assumptions or an unsupervised discretization method would be required to discretize it before the input variables can be discretized using a supervised method while there are many supervised discretization algorithms available the fayyad irani f i fayyad and irani 1993 and kononenko ko kononenko 1995 algorithms are well tested and are available in commonly used software packages such as r and python both algorithms are based on entropy minimization and effectively iterate through discretization schemes to find the one that maximizes predictability of the output variable minimizes entropy input variables are optimized individually and can have different numbers of discretization intervals the ko algorithm differs from the f i algorithm in that it accounts for entropy bias introduced in multi variate settings when variables have different numbers of discretization intervals kononenko 1995 2 2 4 experimental design to demonstrate the effect of discretization on bn model performance different methods of manual unsupervised and supervised discretization were applied to a bn and evaluated the bn used for this purpose is shown in fig 1 and models the erosion impact of coastal storms on sandy beaches as a function of the pre storm beach volume storm wave power and storm water level each variable in the bn consists of 1600 data points with differing distributions fig 2 for a detailed description of this data the reader is referred to harley et al 2017 in this study for each of the different discretization methods outlined below the continuous data describing the bn fig 2 was discretized the bn was trained on the discretized data and its performance was evaluated as described in section 2 3 the open source software package weka v3 9 1 university of waikato 1999 2016 was used to discretize and evaluate each bn a total of 24 tests were conducted comprising different configurations of manual discretization unsupervised discretization algorithms equal width ew and equal frequency ef and supervised discretization algorithms f i fayyad and irani 1993 and ko kononenko 1995 algorithms as outlined below two different manual discretization schemes were specified from the data table 1 m1 a simpler scheme in which variables were discretized into 4 or fewer intervals and m2 a more complex scheme in which variables were discretized into 5 intervals or fewer the intervals selected for each variable in these schemes were based on expert judgement and are intuitive regularly spaced and hold physical and measurable meaning in the context of coastal engineering and management a total of six unsupervised discretization schemes were tested comprising three ew schemes and three ef schemes of 3 4 and 5 intervals each for the bn tested in this study fig 1 and the amount of data available 1600 data instances a greater number of intervals than this could lead to issues of model overfitting and reduced predictive skill while fewer intervals would produce an underfit model beuzen et al 2018 unsupervised tests are henceforth denoted as the method followed by the number of intervals specified e g ew4 examples of ew and ef discretization are shown in table 1 the supervised discretization algorithms tested here f i and ko methods require a discrete output variable to inform the discretization of the input variables therefore to test the full range of possible discretization schemes the output variable erosion volume fig 1 was first discretized using each of the manual and unsupervised schemes described above and then the two supervised algorithms were applied to discretize the remaining input variables this resulted in a total of 2 supervised methods x 6 unsupervised output discretizations 2 manual output discretization 16 supervised discretization tests supervised tests are henceforth denoted as the supervised algorithm followed by the method used to discretize the output variable e g f i ew4 examples of f i and ko discretization are shown in table 1 2 3 evaluation of discretization methods to demonstrate the effect of each of the 24 discretization schemes on the bn model fig 1 performance 10 repetitions of 10 fold cross validation i e 100 tests were performed and evaluation metrics were recorded cross validation is a widely used method of testing machine learning models that partition data into separate training and testing sets to give an unbiased evaluation of model predictive skill elsner and schmertmann 1994 to assess model performance two metrics were used 1 model accuracy the percent of cases for which the modelled and actual discretized intervals of the output variable are equal ranging from 0 no predictive power to 100 perfect model model accuracy is a measure of a bns predictive skill 2 f measure also known as f score the ratio of a model s recall and precision expressed here as a percentage ranging from 0 poor model to 100 perfect model broadly speaking the f measure provides an indication of how useful the model is over all of the discretized output variable states while accuracy is commonly used as a bn performance metric marcot 2012 it is important to consider what this accuracy actually represents i e does the model perform well over all output states a useful model or just very well on one of them not as useful of a model as shown in fig 3 the higher f measure of model 2 represents a model that performs well over the entire output domain and it is likely to be considered a more useful model than model 1 despite having lower accuracy in general a large imbalance between a model s accuracy and f measure indicates a spuriously predictive model that is only predicting a single discretization interval all of the time 3 results the bn shown in fig 1 was trained and tested with each of the 24 discretization schemes described in section 2 2 4 to evaluate how different discretization methods affected model performance fig 4 presents the average bn accuracy and f measure results from the 10 repetitions of 10 fold cv conducted for these tests demonstrating that there is variability both between and within the different methods of discretization used manual unsupervised or supervised fig 5 summarizes the results of fig 4 to compare these three discretization methods from fig 5 it can be seen that on average supervised discretization produced models with the highest accuracy 73 8 followed by manual discretization 69 0 and unsupervised discretization 64 8 in terms of the f measure the results of supervised and manual methods are comparable with f measures of 64 9 and 65 5 respectively and are higher than the unsupervised methods having an f measure of 51 3 similar variability in performance is observed when summarizing the results of each individual discretization algorithm tested in this study fig 6 of the unsupervised algorithms ew outperforms ef in terms of model accuracy with values of 68 2 and 61 4 respectively fig 6 however the higher ew performance is a result of the ew schemes predicting only the majority output bin all of the time e g fig 3 as evident by the lower f measure of 42 0 compared to an f measure of 60 3 for ef there is no obvious difference between the two different supervised algorithms f i and ko tested with both achieving high average model accuracy and usability similarly there was little variability between the two different manual discretization schemes tested with both also achieving good model accuracy and usability 4 discussion and conclusion at present the impact of different methods of discretization on bn model performance has not been well documented in the environmental bn modelling literature death et al 2015 nojavan 2017 furthermore the use of supervised discretization remains largely absent from environmental bn applications this study demonstrates the potential influence of manual unsupervised and supervised discretization on an environmental bn in our case study evaluating the performance of different manual unsupervised and supervised discretization methods for a bn modelling coastal erosion results reveal that the method of discretization used can influence the performance of the resultant bn as evaluated by its accuracy and f measure in this study supervised discretization produced bns that outperformed unsupervised and manual methods in terms of predictive skill this is consistent with results from studies comparing discretization methods in other fields e g dougherty et al 1995 and advocates for the use of supervised methods when predictive skill is a modelling priority manual discretization was the next best method followed by unsupervised discretization it should be noted that the performance of manual discretization schemes are subject to the quality of expert interpretation of the model domain and the selected discretization thresholds therefore manual discretization could not generally be said to outperform unsupervised methods in a predictive sense however the manual approach allows for expert information to be incorporated into the modelling process suggesting the resulting bn may be more easily interpretable or fit for purpose for the different algorithms of unsupervised ew and ef and supervised f i and ko discretization tested no one algorithm within a specific method outperformed the other in both evaluation criteria i e accuracy and f measure this is consistent with the comparison of unsupervised discretization algorithms conducted by nojavan et al 2017 and indicates that the category of discretization i e manual unsupervised or supervised may be more important than the algorithm used within that category as evident in fig 4 bn performance predictive accuracy and or f measure generally decreased here with an increased number of discretization bins in all unsupervised supervised and manual discretization tests this is a result of the amount of data available to train the bn the conditional probability table cpt of the output variable in a bn quantifies the probability of every single permutation of input variable and output variable bins states so its size increases considerably with the number of discretization bins in the bn 2 s i z e c p t o i 1 n i i where o is the number of bins of the output node and i i is the number of states in the ith input node when the size of the cpt is greater than the amount of data available to train it or when there is limited data available to populate each unique entry variable combination in the cpt model performance typically decreases because it is missing some conditional relationships or they have not been appropriately represented by the available data beuzen et al 2018 for example the ef3 scheme here had a cpt size of 31 all of which were quantified using an average of 15 data points strong representation from the 1600 available in this study and it performed better than the ef5 scheme fig 4 which had a cpt size of 625 54 of which were missing and the rest of which were quantified from only 4 data points on average weak representation therefore when discretizing continuous variables for use in a bn to maximize predictive performance it is important to consider the number of bins used and how this can affect the size and strength of the conditional relations in the output variable cpt learnt from the available data even when using supervised discretization many algorithms such as the fayyad and irani 1993 and kononenko 1995 algorithms used here discretize univariately e g one input variable one at a time to maximize predictability of the output variable and do not consider the number of bins in other variables or the resultant size of the output variable cpt as such cpts generated using supervised methods can be large although the discretization schemes produced by supervised methods generally outperform unsupervised methods figs 4 and 5 because the bins themselves have high predictive utility predictive skill can potentially be further increased by using an expert to reduce the number of bins i e reduce the size of the cpt following supervised discretization in environmental models the interpretability and physical meaning of a discretization scheme is often as important as the performance of a model in this study both supervised and manual discretization performed well however these two methods have quite different practical applications supervised discretization provides an efficient method of discretizing many variables autonomously and maximizing the predictive skill of a model however the thresholds it determines may appear random and physically meaningless particularly in environmental modelling e g table 1 in contrast in manual discretization more physically meaningful discretization thresholds are specified which may be of more importance to environmental modellers and stakeholders a combination of supervised and manual discretization as was tested in some schemes here e g f i m1 ko m1 can provide a balance between a predictive model with physical meaningfulness however a limitation of manual discretization is that it requires a priori expert knowledge of the problem domain unsupervised methods must be used in the absence of this knowledge either to discretize all variables in the bn or to discretize the output node to prepare it for supervised discretization in this study unsupervised discretization algorithms produced the poorest performing bns figs 3 5 however they are computationally simple are generally robust to noisy data and there are many different algorithms available that can be useful for exploring and manipulating the intrinsic properties of a continuous distribution and how this can affect the predictive power of a bn it should be noted that the performance of the different unsupervised discretization algorithms tested here equal width and equal frequency and in the study by nojavan et al 2017 was highly variable and it is recommended that several alternative algorithms be evaluated if using unsupervised discretization to ensure that bn results are not an artefact of the discretization algorithm based on the results and discussion in this study a summary and practical guide for the use of manual unsupervised and supervised discretization in environmental bn modelling is presented in table 2 while bns that incorporate continuous variables and avoid discretization continue to be studied they remain limited in their capabilities and application korb and nicholson 2010 in addition the use of discrete variables rather than continuous is often desirable because they allow for rapid and efficient bn model building produce models that are interpretable and can be efficiently used for probabilistic inference and decision making and can improve the signal to noise ratio in the data by using bins that smooth out small scale fluctuations in the data discretization will therefore continue to be a key aspect of bn modelling in environmental applications it is anticipated that the work presented here will aid in the future application of bns applied to environmental modelling acknowledgements data for this research was partially funded by ongoing support by northern beaches council the australian research council lp04555157 lp100200348 dp150101339 and the nsw environmental trust environmental research program rd 2015 0128 wave and tide data was kindly provided by manly hydraulics laboratory under the nsw coastal data network program managed by the office of environment and heritage oeh the 2nd author is additionally supported through an australian research council future fellowship ft120100269 the lead author is funded under the australian postgraduate research training program 
26343,coupling watershed and lake models is a powerful approach for providing lake or reservoir inputs and for simulating how land use scenarios may impact lake ecosystems swat soil and water assessment tool is one of the most widely used watershed models for this purpose however as outlets for watershed delineation in swat should be in the river network modellers may when modelling the inputs to a waterbody boundary make choices leading to imprecise results the impact of these choices on the accuracy of a coupled model can be highly relevant as they have a direct effect on the size of the delineated watershed in this paper we describe the design and development of swat2lake a tool that remedies such imprecise delineation of watersheds to lakes and reservoirs swat2lake has been designed as a plugin for the qgis software that works in parallel with qswat and it is available through wet au dk graphical abstract image keywords lake qgis plugin swat swat2lake watershed delineation wet software availability name of software swat2lake developers anders nielsen eugenio molina navarro software license gnu general public license contact address department of bioscience aarhus university vejlsvej 25 8600 silkeborg denmark email wet info wet au dk availability wet au dk registration is required 1 introduction during the last century the water quality of lakes and reservoirs worldwide has degraded because of multiple stressors such as climate change increased demand for water abstraction and high nutrient levels caused by agricultural expansion and point sources kristensen 2012 tsakiris 2015 modelling tools have become very useful in the assessment of the status of and the risks presented to aquatic ecosystems as well as in the development of mitigation tools for degraded waterbodies models allow water managers to simulate an array of potential future scenarios and they thereby support policy and decision making trolle et al 2012 multiple lake and reservoir models are described in the scientific literature including gotm fabm pclake bruggeman and bolding 2014 hu et al 2016 this model has recently been incorporated into wet water ecosystems tool a graphical user interface gui designed to aid model users nielsen et al 2017 as measured input data for lake models are not always available wet offers several methods for choosing inputs of water and nutrients one of these is to feed in simulated water and nutrients from the soil and water assessment tool swat arnold et al 1998 which requires an existing swat model for the targeted watershed swat has become one of the most popular eco hydrological models at watershed scale gassman et al 2010 thus more than 3000 peer reviewed journal articles were recorded from 1998 to 2017 in the swat literature database www card iastate edu swat articles its capability of simulating streamflow sediment and nutrient transport renders it a good candidate for deriving the required inputs to models of lakes or similar waterbodies reservoirs or lagoons see for instance plus et al 2006 trolle et al 2015 and as mentioned also to wet several studies before ours have coupled swat and lake models where the entire aquatic system consisting of a lake or reservoir in the following referred to as lake and its drainage area has been modelled e g plus et al 2006 yazdi and moridi 2017 nielsen et al 2013 molina navarro et al 2014 a holistic coupled watershed lake model approach is a powerful tool not only because it automatically generates the required input data for a lake model but especially because it allows simulation of a vast range of scenarios in the watershed i a land use change agricultural management or climate change and how these may impact a lake ecosystem trolle et al 2015 bucak et al 2018 however in swat user interfaces the delineation of the watershed area draining into a lake require users to draw the watershed outlet or outlets in a certain point of the river network those areas in between river s subbasins that flow directly into the lake are therefore excluded to the best of our knowledge this problem has not yet been addressed in the scientific literature on swat typically the areas in between rivers have been ignored and only the major streams flowing into the waterbody have been considered e g plus et al 2006 or in some cases only the main input e g yazdi and moridi 2017 depending on the swat interface used this can be done either by adding final outlets where each stream enters in the lake or by delineating the catchment including the lake area adding those points and deleting the outlets generated inside the lake although the areas in between rivers do not feature streams relevant hydrological processes in the watershed such as evapotranspiration or overland and or groundwater flow into the lake are not taken into account if they are disregarded some other authors have included the lake area within the delineated watershed selecting the lake outlet as the final outflow point of the watershed and not parameterising it within swat e g nielsen et al 2013 molina navarro et al 2014 palazn and navas 2014 bucak et al 2017 however this option is not satisfactory for modelling the inputs ideally the lake area should be excluded swat also allows simulation of some reservoir processes i e water sediments nutrients and pesticide routing arnold et al 1998 at a watershed outlet but in swat the reservoir is added as a point feature which receives inputs from the entire upstream watershed regardless of the size of the area covered by the waterbody the swim model soil and water integrated model developed on the basis of swat and matsalu models has a reservoir module which integrates a reservoir as an additional subbasin koch et al 2013 thus allowing an optimal integration of a waterbody and its watershed however such an option is not available in swat the choices made regarding watershed delineation affect the accuracy of the modelled water sediment and nutrient inputs the effect might be negligible when the ratio between the lake area and the watershed area is very small or when the lake has an elongated shape with just one main inflow stream however in larger lakes relative to watershed size and or in lakes with several major inflow streams the choices and thus assumptions might lead to considerable lack of accuracy this paper presents and describes the design and development of swat2lake a qgis plugin to tailor the watershed of a lake to its outline as illustration a practical application is presented for the danish lake arreskov watershed 2 coupling swat and lake models an illustration of current state we use the lake arreskov watershed to illustrate the common approaches in using swat to model inputs to a lake fig 1 lake arreskov located in the upper part of the odense river watershed on the island of funen has a surface area of 3 17 km2 and a mean depth of 1 9 m its watershed excluding the lake has an area of 24 76 km2 the lake is the largest in the region in the 1950 1980s the lake suffered from severe eutrophication which led to the implementation of measures for its restoration nielsen et al 2014 the different approaches for watershed delineation fig 1 were carried out in qswat 1 5 dile et al 2018 using a 10 m lidar based digital elevation model and a stream creation threshold of 50 ha because the stream network generated based on this threshold closely resembled reality for the first swat delineation approach i e use of the final watershed output as input in an eventual lake model fig 1a the modelled watershed was almost 13 larger than the actual watershed flowing into the lake fig 1c in contrast the second delineation approach fig 1b rendered a watershed area that was approx 8 smaller than the actual area fig 1c for the third most desirable approach fig 1c the watershed was tailored to the lake area the variations in the area covered are translated into differences in the catchment water balance annual average precipitation discharge and actual evapotranspiration in an uncalibrated model run with observed climate for the lake tailored approach during the period 2006 2015 were 20 7 10 2 and 10 9 hm3 respectively with the same model set up and following the first approach precipitation discharge and actual evapotranspiration were respectively 13 0 2 and 24 higher following the second approach however the annual average of the three water balance components was 8 lower than in the lake tailored approach these differences would have a subsequent impact in nutrient loads to tailor the watershed to the lake area we developed the swat2lake tool available as a plugin for the qgis software when applied in parallel with the watershed delineation step in qswat dile et al 2016 2018 the modeller can delineate the entire watershed flowing into a lake and in this way tailor the watershed delineation to the outline of the waterbody moreover the concept of tailoring a watershed to a lake is intended to assist in the application of wet nielsen et al 2017 a tool already developed for qgis it should be acknowledged that a delineation very close to this approach could be achieved by using a very low stream creation threshold however a very detailed stream network would be created far from reality in most of the cases thus not adequately representing some model processes e g channel processes 3 swat2lake tool development 3 1 technical implementation the swat model is available through two main gis interfaces the arcgis www esri com and the qgis platform www qgis org to support the path of open source we chose to integrate our tool in qgis which is a recognised open source gis software package chen et al 2010 in this way others may expand and develop the tool even further with functionalities beyond the current developments the interface of swat2lake was developed using qt designer www qt io and the associated core functionality in python 2 7 www python org all the developed functionalities can be run on a standard qgis installation and requires no additional python library package installations or upgrades a limitation is however that the swat interface in qgis qswat only works in a ms windows operating system in parallel with qswat swat2lake connects to the microsoft access database within a qswat project and analogous with qswat the plugin must be operated in a 32 bit version of qgis however the plugin is compatible with the latest stable release of qgis currently version 2 18 qgis and qswat must be installed on the computer prior to the installation of swat2lake instructions are available at www wet au dk 3 2 process description the plugin operates within an opened qswat project a qswat project must thus be launched in qgis from where relevant paths and files are automatically determined and initialised by the plugin when using the plugin it creates a new dedicated directory within the qswat project and stores all swat2lake related files here the main task of the plugin is to replace the original qswat subbasin file with a tailored version to be used by qswat the plugin performs the required gis processing to burn in the lake in the temporary subbasins delineation the area of the tailored subbasins is then determined and the database is upgraded with the new information along with an update of the routing of water and the length of rivers moreover the relevant qswat shapefiles are updated to sustain homogeneity within the qswat project files based on the qswat procedure on watershed delineation application of the swat2lake tool essentially involves three steps figs 2 and 3 step 1 adding a lake or reservoir shapefile after creating the stream within watershed delineation in qswat the modeller is to add a single lake shapefile at the moment swat2lake is designed to manage only one lake which watershed is to be modelled step 2 tailoring the watershed delineation to the waterbody outline having added the lake shapefile the modeller has to select as final outlet of the watershed the outflow of the lake and as intermediate outlets every inflow point of the stream network into the lake besides other locations potentially desired e g monitoring stations fig 2a once the preliminary subbasins have been created before merging of small subbasins fig 2b the swat2lake tool removes the area corresponding to the lake from the subbasins layer fig 2c using the qgis tool difference in vector geoprocessing tools and re calculates the area of each subbasin in the attribute table swat2lake also works if the lake falls within a single subbasin to ensure right execution of this step the tool verifies beforehand that no subbasin is located entirely within the lake area finally when the new subbasin map without the lake area has been created fig 2c the modeller may continue the watershed delineation within qswat and merge if desired small sized subbasins which might be convenient since the modified subbasins flowing directly into the lake may be small fig 2c and d and finish the delineation step 3 adjusting the flow routing in the stream network once the watershed delineation is finished and before the hydrological response units are defined the streamflow routing must be configured this routing is based on the first network created by qswat which includes streams within the lake area and routes all the flow to a unique outlet point fig 2a however after having conducted step 2 some subbasins that originally flowed into another subbasin now flow directly into the lake i e out of the watershed reflecting the reality requiring configuration for this purpose the table of the project database that contains the information of the reaches associated to each subbasin reach table has to be edited the plugin assigns a value of 0 to the table fields defining to which subbasin a stream flows into to node and subbasinr besides the cumulated drainage area for each subbasin areac field in the table is automatically edited to reflect the new delineation this process will create swat input files particularly the watershed configuration file fig fig which defines the routing network in the watershed arnold et al 2014 that reflect the new catchment configuration step 3 includes an additional operation in swat every subbasin must have a stream in subbasins located partially within the lake area in the initial delineation the whole stream might in fact be located within the lake area therefore after removing the area corresponding to the lake such subbasins which are now in direct contact with the lake contour have no stream fig 2d the plugin edits adjusts the length of the stream in the affected subbasins accordingly in the reach table in the project database in these subbasins flow enters in the lake via sheet flow however swat does not simulate sheet flow and requires a stream in all subbasins to function properly to deal with this the swat2lake tool assigns a stream length value of just 1 m channel width and depth default values remain making able to simulate flow in the subbasin but minimizing non existent processes such as channel processes this operation also ensures a correct value of the length of the main stream in the subbasin parameter ch l2 in the swat main channel input files rte files the attribute table of the stream network vector layer created by qswat riv1 shp contains the same information as the reach table in the database the content of the attribute table does not interfere with the subsequent swat execution the current swat2lake version does not edit the riv1 shp attribute table but an update that amend this table to ensure coherence is planned 4 conclusions the existing watershed delineation procedure in swat user interfaces requires to draw the outlets for subbasins and the final watershed outlet or outlets in a certain point of the river network and when delineating lake or reservoir watersheds modellers have therefore been forced to choose potentially imprecise approaches that consider only major streams or include the waterbody area within the watershed these approaches lead to substantial inaccuracy by modelling an area smaller or larger than the actual watershed flowing into the lake and this is not be desirable especially when the results of the watershed modelling are to serve as input in a subsequent lake or reservoir model to remedy this we have developed the swat2lake tool to assist in the delineation of lake or reservoir watersheds using swat the tool has been designed as a plugin for the qgis software and can be accessed at wet au dk working in parallel with qswat it allows the user to tailor a watershed to the waterbody outline and thus account for the entire drainage area flowing into the waterbody the plugin will be upgraded to support newer versions of qgis whenever qswat migrates to other database concepts e g sqlite acknowledgements this work was supported by the european research area for climate services 690462 as part of the jpi climate project watexr integration of climate seasonal prediction and ecosystem impact modelling for an efficient adaptation of water resources management to increasing climate extreme events we thank chris george for inspiring us to develop the swat2lake tool and anne mette poulsen for valuable editorial comments 
26343,coupling watershed and lake models is a powerful approach for providing lake or reservoir inputs and for simulating how land use scenarios may impact lake ecosystems swat soil and water assessment tool is one of the most widely used watershed models for this purpose however as outlets for watershed delineation in swat should be in the river network modellers may when modelling the inputs to a waterbody boundary make choices leading to imprecise results the impact of these choices on the accuracy of a coupled model can be highly relevant as they have a direct effect on the size of the delineated watershed in this paper we describe the design and development of swat2lake a tool that remedies such imprecise delineation of watersheds to lakes and reservoirs swat2lake has been designed as a plugin for the qgis software that works in parallel with qswat and it is available through wet au dk graphical abstract image keywords lake qgis plugin swat swat2lake watershed delineation wet software availability name of software swat2lake developers anders nielsen eugenio molina navarro software license gnu general public license contact address department of bioscience aarhus university vejlsvej 25 8600 silkeborg denmark email wet info wet au dk availability wet au dk registration is required 1 introduction during the last century the water quality of lakes and reservoirs worldwide has degraded because of multiple stressors such as climate change increased demand for water abstraction and high nutrient levels caused by agricultural expansion and point sources kristensen 2012 tsakiris 2015 modelling tools have become very useful in the assessment of the status of and the risks presented to aquatic ecosystems as well as in the development of mitigation tools for degraded waterbodies models allow water managers to simulate an array of potential future scenarios and they thereby support policy and decision making trolle et al 2012 multiple lake and reservoir models are described in the scientific literature including gotm fabm pclake bruggeman and bolding 2014 hu et al 2016 this model has recently been incorporated into wet water ecosystems tool a graphical user interface gui designed to aid model users nielsen et al 2017 as measured input data for lake models are not always available wet offers several methods for choosing inputs of water and nutrients one of these is to feed in simulated water and nutrients from the soil and water assessment tool swat arnold et al 1998 which requires an existing swat model for the targeted watershed swat has become one of the most popular eco hydrological models at watershed scale gassman et al 2010 thus more than 3000 peer reviewed journal articles were recorded from 1998 to 2017 in the swat literature database www card iastate edu swat articles its capability of simulating streamflow sediment and nutrient transport renders it a good candidate for deriving the required inputs to models of lakes or similar waterbodies reservoirs or lagoons see for instance plus et al 2006 trolle et al 2015 and as mentioned also to wet several studies before ours have coupled swat and lake models where the entire aquatic system consisting of a lake or reservoir in the following referred to as lake and its drainage area has been modelled e g plus et al 2006 yazdi and moridi 2017 nielsen et al 2013 molina navarro et al 2014 a holistic coupled watershed lake model approach is a powerful tool not only because it automatically generates the required input data for a lake model but especially because it allows simulation of a vast range of scenarios in the watershed i a land use change agricultural management or climate change and how these may impact a lake ecosystem trolle et al 2015 bucak et al 2018 however in swat user interfaces the delineation of the watershed area draining into a lake require users to draw the watershed outlet or outlets in a certain point of the river network those areas in between river s subbasins that flow directly into the lake are therefore excluded to the best of our knowledge this problem has not yet been addressed in the scientific literature on swat typically the areas in between rivers have been ignored and only the major streams flowing into the waterbody have been considered e g plus et al 2006 or in some cases only the main input e g yazdi and moridi 2017 depending on the swat interface used this can be done either by adding final outlets where each stream enters in the lake or by delineating the catchment including the lake area adding those points and deleting the outlets generated inside the lake although the areas in between rivers do not feature streams relevant hydrological processes in the watershed such as evapotranspiration or overland and or groundwater flow into the lake are not taken into account if they are disregarded some other authors have included the lake area within the delineated watershed selecting the lake outlet as the final outflow point of the watershed and not parameterising it within swat e g nielsen et al 2013 molina navarro et al 2014 palazn and navas 2014 bucak et al 2017 however this option is not satisfactory for modelling the inputs ideally the lake area should be excluded swat also allows simulation of some reservoir processes i e water sediments nutrients and pesticide routing arnold et al 1998 at a watershed outlet but in swat the reservoir is added as a point feature which receives inputs from the entire upstream watershed regardless of the size of the area covered by the waterbody the swim model soil and water integrated model developed on the basis of swat and matsalu models has a reservoir module which integrates a reservoir as an additional subbasin koch et al 2013 thus allowing an optimal integration of a waterbody and its watershed however such an option is not available in swat the choices made regarding watershed delineation affect the accuracy of the modelled water sediment and nutrient inputs the effect might be negligible when the ratio between the lake area and the watershed area is very small or when the lake has an elongated shape with just one main inflow stream however in larger lakes relative to watershed size and or in lakes with several major inflow streams the choices and thus assumptions might lead to considerable lack of accuracy this paper presents and describes the design and development of swat2lake a qgis plugin to tailor the watershed of a lake to its outline as illustration a practical application is presented for the danish lake arreskov watershed 2 coupling swat and lake models an illustration of current state we use the lake arreskov watershed to illustrate the common approaches in using swat to model inputs to a lake fig 1 lake arreskov located in the upper part of the odense river watershed on the island of funen has a surface area of 3 17 km2 and a mean depth of 1 9 m its watershed excluding the lake has an area of 24 76 km2 the lake is the largest in the region in the 1950 1980s the lake suffered from severe eutrophication which led to the implementation of measures for its restoration nielsen et al 2014 the different approaches for watershed delineation fig 1 were carried out in qswat 1 5 dile et al 2018 using a 10 m lidar based digital elevation model and a stream creation threshold of 50 ha because the stream network generated based on this threshold closely resembled reality for the first swat delineation approach i e use of the final watershed output as input in an eventual lake model fig 1a the modelled watershed was almost 13 larger than the actual watershed flowing into the lake fig 1c in contrast the second delineation approach fig 1b rendered a watershed area that was approx 8 smaller than the actual area fig 1c for the third most desirable approach fig 1c the watershed was tailored to the lake area the variations in the area covered are translated into differences in the catchment water balance annual average precipitation discharge and actual evapotranspiration in an uncalibrated model run with observed climate for the lake tailored approach during the period 2006 2015 were 20 7 10 2 and 10 9 hm3 respectively with the same model set up and following the first approach precipitation discharge and actual evapotranspiration were respectively 13 0 2 and 24 higher following the second approach however the annual average of the three water balance components was 8 lower than in the lake tailored approach these differences would have a subsequent impact in nutrient loads to tailor the watershed to the lake area we developed the swat2lake tool available as a plugin for the qgis software when applied in parallel with the watershed delineation step in qswat dile et al 2016 2018 the modeller can delineate the entire watershed flowing into a lake and in this way tailor the watershed delineation to the outline of the waterbody moreover the concept of tailoring a watershed to a lake is intended to assist in the application of wet nielsen et al 2017 a tool already developed for qgis it should be acknowledged that a delineation very close to this approach could be achieved by using a very low stream creation threshold however a very detailed stream network would be created far from reality in most of the cases thus not adequately representing some model processes e g channel processes 3 swat2lake tool development 3 1 technical implementation the swat model is available through two main gis interfaces the arcgis www esri com and the qgis platform www qgis org to support the path of open source we chose to integrate our tool in qgis which is a recognised open source gis software package chen et al 2010 in this way others may expand and develop the tool even further with functionalities beyond the current developments the interface of swat2lake was developed using qt designer www qt io and the associated core functionality in python 2 7 www python org all the developed functionalities can be run on a standard qgis installation and requires no additional python library package installations or upgrades a limitation is however that the swat interface in qgis qswat only works in a ms windows operating system in parallel with qswat swat2lake connects to the microsoft access database within a qswat project and analogous with qswat the plugin must be operated in a 32 bit version of qgis however the plugin is compatible with the latest stable release of qgis currently version 2 18 qgis and qswat must be installed on the computer prior to the installation of swat2lake instructions are available at www wet au dk 3 2 process description the plugin operates within an opened qswat project a qswat project must thus be launched in qgis from where relevant paths and files are automatically determined and initialised by the plugin when using the plugin it creates a new dedicated directory within the qswat project and stores all swat2lake related files here the main task of the plugin is to replace the original qswat subbasin file with a tailored version to be used by qswat the plugin performs the required gis processing to burn in the lake in the temporary subbasins delineation the area of the tailored subbasins is then determined and the database is upgraded with the new information along with an update of the routing of water and the length of rivers moreover the relevant qswat shapefiles are updated to sustain homogeneity within the qswat project files based on the qswat procedure on watershed delineation application of the swat2lake tool essentially involves three steps figs 2 and 3 step 1 adding a lake or reservoir shapefile after creating the stream within watershed delineation in qswat the modeller is to add a single lake shapefile at the moment swat2lake is designed to manage only one lake which watershed is to be modelled step 2 tailoring the watershed delineation to the waterbody outline having added the lake shapefile the modeller has to select as final outlet of the watershed the outflow of the lake and as intermediate outlets every inflow point of the stream network into the lake besides other locations potentially desired e g monitoring stations fig 2a once the preliminary subbasins have been created before merging of small subbasins fig 2b the swat2lake tool removes the area corresponding to the lake from the subbasins layer fig 2c using the qgis tool difference in vector geoprocessing tools and re calculates the area of each subbasin in the attribute table swat2lake also works if the lake falls within a single subbasin to ensure right execution of this step the tool verifies beforehand that no subbasin is located entirely within the lake area finally when the new subbasin map without the lake area has been created fig 2c the modeller may continue the watershed delineation within qswat and merge if desired small sized subbasins which might be convenient since the modified subbasins flowing directly into the lake may be small fig 2c and d and finish the delineation step 3 adjusting the flow routing in the stream network once the watershed delineation is finished and before the hydrological response units are defined the streamflow routing must be configured this routing is based on the first network created by qswat which includes streams within the lake area and routes all the flow to a unique outlet point fig 2a however after having conducted step 2 some subbasins that originally flowed into another subbasin now flow directly into the lake i e out of the watershed reflecting the reality requiring configuration for this purpose the table of the project database that contains the information of the reaches associated to each subbasin reach table has to be edited the plugin assigns a value of 0 to the table fields defining to which subbasin a stream flows into to node and subbasinr besides the cumulated drainage area for each subbasin areac field in the table is automatically edited to reflect the new delineation this process will create swat input files particularly the watershed configuration file fig fig which defines the routing network in the watershed arnold et al 2014 that reflect the new catchment configuration step 3 includes an additional operation in swat every subbasin must have a stream in subbasins located partially within the lake area in the initial delineation the whole stream might in fact be located within the lake area therefore after removing the area corresponding to the lake such subbasins which are now in direct contact with the lake contour have no stream fig 2d the plugin edits adjusts the length of the stream in the affected subbasins accordingly in the reach table in the project database in these subbasins flow enters in the lake via sheet flow however swat does not simulate sheet flow and requires a stream in all subbasins to function properly to deal with this the swat2lake tool assigns a stream length value of just 1 m channel width and depth default values remain making able to simulate flow in the subbasin but minimizing non existent processes such as channel processes this operation also ensures a correct value of the length of the main stream in the subbasin parameter ch l2 in the swat main channel input files rte files the attribute table of the stream network vector layer created by qswat riv1 shp contains the same information as the reach table in the database the content of the attribute table does not interfere with the subsequent swat execution the current swat2lake version does not edit the riv1 shp attribute table but an update that amend this table to ensure coherence is planned 4 conclusions the existing watershed delineation procedure in swat user interfaces requires to draw the outlets for subbasins and the final watershed outlet or outlets in a certain point of the river network and when delineating lake or reservoir watersheds modellers have therefore been forced to choose potentially imprecise approaches that consider only major streams or include the waterbody area within the watershed these approaches lead to substantial inaccuracy by modelling an area smaller or larger than the actual watershed flowing into the lake and this is not be desirable especially when the results of the watershed modelling are to serve as input in a subsequent lake or reservoir model to remedy this we have developed the swat2lake tool to assist in the delineation of lake or reservoir watersheds using swat the tool has been designed as a plugin for the qgis software and can be accessed at wet au dk working in parallel with qswat it allows the user to tailor a watershed to the waterbody outline and thus account for the entire drainage area flowing into the waterbody the plugin will be upgraded to support newer versions of qgis whenever qswat migrates to other database concepts e g sqlite acknowledgements this work was supported by the european research area for climate services 690462 as part of the jpi climate project watexr integration of climate seasonal prediction and ecosystem impact modelling for an efficient adaptation of water resources management to increasing climate extreme events we thank chris george for inspiring us to develop the swat2lake tool and anne mette poulsen for valuable editorial comments 
26344,china is anticipated to face reductions in crop yield due to climate change combining a process based agricultural model apsim with a simplified empirical model we investigate the association between wheat productivity and climate variables in the north china plain ncp over the period 1960 2010 the results show that the spatial distribution of wheat productivity across ncp is negatively related to daily maximum temperature but positively to daily minimum temperature solar radiation and precipitation for most parts of the ncp partial correlation analysis based on the apsim simulation outputs indicates that the increasing daily minimum temperature could lead to higher productivity but the effects are counteracted by increases in daily maximum temperature in the drier northern ncp wheat productivity is largely discounted due to decreases in precipitation while in the humid southern edge of the ncp the productivity stimulation effect of warming is offset by dimming keywords wheat productivity climate change the north china plain apsim software availability name of software agricultural production systems simulator apsim developer apsim initiative contact address 203 tor street toowoomba qld 4350 australia telephone 61 7 4688 1596 email apsim daff qld gov au year of first available 1994 hardware required any recent pc with a minimum of 2 gb of ram 32 or 64 bit software required windows xp vista windows 8 or windows 10 mono 2 10 for linux availability https www apsim info cost free for non commercial use 1 introduction food security is one of the largest global concerns with regard to the impact of global climate change beddington et al 2012 campbell et al 2016 ipcc 2014 lobell and asseng 2017 all of the four dimensions of food security i e availability accessibility utilization and stability could be affected by climate change directly or indirectly battisti and naylor 2009 fao 2008 godfray et al 2011 lesk et al 2016 rosenzweig et al 2014 tai et al 2014 wheeler and von braun 2013 the effects of climate change on crop and food production are becoming evident in a number of regions around the world and showing more commonly to be negative than positive although the negative impacts of climate change could be offset by the effect of co2 fertilization to some extent e g betts et al 2007 gedney et al 2006 lobell et al 2011 long et al 2006 porter et al 2014 it is reported that due to climate change agriculture output in developing countries may decline by 20 on average by 2080 while agriculture output in industrialized countries is expected to decrease by 6 fao 2008 among the countries most affected by climate change china is anticipated to face more frequent extreme weather events like droughts and floods which could result in the reduction of crop yields in addition with the increasing food demand due to a growing population and rising living standards food security in china may become more challenging and the international grain markets could be deeply reshaped as china is the world s largest wheat producer and consumer godfray et al 2010 ipcc 2014 piao et al 2010 turral et al 2011 zhang et al 2013 as a pivot subject of climate change mitigation and adaptation the responses of crop yields to climate change have been investigated in numerous research articles many of which have come up with important conclusions showing the association between crop production and climate change e g asseng et al 2013 2015 battisti and naylor 2009 butler and huybers 2013 conway et al 2014 lobell et al 2011 urban et al 2015 wheeler and von braun 2013 in general a 1 c rise in temperature could lower crop yields by up to 10 except in high latitude regions lobell et al 2011 for high latitude regions like scotland however the measured warming could contribute more than 20 of the increased crop yields over the period 1960 2006 gregory and marshall 2015 based on a meta analysis of around 1700 model simulations the most recent ipcc assessment demonstrated that on average global mean crop yields i e rice wheat and maize are projected to decrease between 3 and 10 per degree of warming above historical levels challinor et al 2014 among which the global wheat reduction rate is estimated to be 6 per degree of warming asseng et al 2015 for the response of crop yield to precipitation change the yields of nearly all crops show a positive relationship with precipitation but could be undermined by flooding and water logging caused by extremely high precipitation events falloon and betts 2009 lobell et al 2011 many of the largest falls in crop productivity have been attributed to anomalously low precipitation events kumar et al 2004 sivakumar et al 2005 indeed crop yield is quite sensitive to precipitation lobell and burke 2008 estimated that a one standard deviation change in precipitation in the growing season could lead to about a 10 change in millet production in south asia additionally research has also shown that decline in solar radiation and change in radiation composition could have significant effects on plant growth and crop productivity chameides et al 1999 rodriguez and sadras 2007 swain et al 2007 wild et al 2012 when investigating the association between crop productivity and climate change agricultural models are often used challinor et al 2014 cohn et al 2016 holzworth et al 2015 lobell and burke 2010 lobell et al 2011 rosenzweig et al 2014 such agricultural models which represent mathematically the relations between crop productivity and climate variables could be strongly process based e g apsim epic ceres and dssat etc or simplified statistical ones e g asseng et al 2011 challinor et al 2014 chen et al 2010 liu et al 2010 lobell and burke 2010 piao et al 2010 ray et al 2015 tao et al 2012 wang et al 2012 the process based model type has advantages in characterizing the dynamics of crop growth and development in sufficient detail and being able to evaluate the effects of climate change on crop yield when it is well calibrated lobell and burke 2010 lobell and asseng 2017 wilcox and makowski 2014 however the process based model requires routine meteorological inputs like precipitation temperature solar radiation relative humidity etc it becomes difficult if not impossible for the application of the model when faced with limited inputs moreover since the process based model usually operates at a daily time step while crop yield is output as an annual total the relations between crop yield and climate factors are not explicitly shown at annual scale but require additional efforts to be identified lobell and asseng 2017 peng et al 2004 in contrast the statistical model type is easily applicable and can directly summarize with less effort the relationship between crop yield and climate factors at an annual scale which is practicable for farm planning and decision making under changing climate deryng et al 2011 lobell and asseng 2017 makowski et al 2015 wilcox and makowski 2014 however the statistical relations between crop yield and climate variables for most of the statistical models are set up based on crop yield census data which could be at a regional average scale and mismatched in space with the climate observations the relations can be further confounded as the measured crop yield could also be affected by other factors e g changes in cultivars agronomic management practice and technological advancement not represented in the statistical model to enhance the reliability of the statistical model improved crop production datasets and novel approaches are needed to better understand the association between crop yield and climate lobell et al 2011 schlenker and roberts 2009 welch et al 2010 in china both the process based and the statistical model types have been used to assess the impact of climate change on crop production e g chen et al 2010 liu et al 2010 piao et al 2010 tao et al 2006 2012 wang et al 2009 2012 xiao et al 2012 it has been reported that rising temperatures since the 1980s have negatively affected crop production in the north china plain this has arisen because of the shortening of the length of the crop growth period accelerating the physiological development rate and diminishing the growth duration and rate of photosynthesis and biomass accumulation e g liu et al 2010 porter and gawith 1999 tao et al 2012 xiao et al 2012 zhang et al 2013 meanwhile declining solar radiation and precipitation have further reduced crop yields in the north china plain chen et al 2010 tao et al 2012 yang et al 2013 however the spatial association between crop yield and climatic variables remains inconclusive and the response of crop yield to each climate variable is not yet well understood liu et al 2010 lobell et al 2011 olesen et al 2011 tao et al 2012 in this paper we aim to investigate the association between winter wheat productivity and climate variables across the north china plain ncp for the period 1960 to 2010 the process based agricultural model apsim agricultural production systems simulator is used firstly to simulate wheat productivity on the basis of apsim simulations a statistical model is then developed to reflect explicitly the underlying climate factors shaping the spatial distribution of wheat productivity it is used as a surrogate of the apsim model to estimate wheat productivity where daily meteorological inputs are not available thereafter the relation between wheat productivity and climate factors for each site is further investigated to define the dominant climate factor affecting wheat productivity changes across the north china plain during the period 1960 to 2010 2 data and methods 2 1 study area the north china plain ncp is known as the barn of china contributing 50 of the nation s total wheat production liu et al 2010 jeong et al 2014 located at 31 42 40 30 n and 110 24 122 42 e it covers seven provinces in china fig 1 the ncp has a warm temperate monsoon climate with great seasonal and annual variation in temperature and precipitation where winter is cold and dry but summer is hot and wet the observed mean daily temperature in the region varies from 10 7 c in the north to 15 6 c in the south mean annual precipitation ranges from 478 mm in the northwest to 1114 mm in the southeast farmland accounts for 72 of total land area in the ncp of which around 82 is irrigated water shortage in the ncp is an important constraint on the cropping system increasing evidence suggests that climate change in the ncp e g higher temperature less solar radiation and precipitation has adverse impacts on the cropping system chen et al 2010 liu et al 2010 lobell et al 2011 ray et al 2015 in addition shrinking farmland as a consequence of urbanization makes it more challenging to meet the increasing food demand yue et al 2010 2 2 climate and phenology data to investigate the association between winter wheat yield and climate daily meteorological datasets provided by the china meteorological administration are used the dataset includes daily air temperatures maximum and minimum precipitation and sunshine hours for 73 stations in the ncp over the period 1960 to 2010 solar radiation observations one of the required inputs to apsim are calculated from sunshine hours by using the angstrom equation angstrom 1924 among the 73 meteorological sites 15 sites have phenology observations which are used to evaluate the performance of apsim model the phenology dataset includes sowing date emergence date jointing date and maturity date of winter wheat for the period 1991 to 2010 e g xiao et al 2012 2 3 methods apsim agricultural production systems simulator developed by the agricultural production systems research unit in australia is a daily process based model of agricultural systems holzworth et al 2014 keating et al 2003 it can simulate the growth and yield for many crop species in response to climate and management conditions and predict the long term consequence of cropping systems on soil physical and chemical conditions keating et al 2003 it has been widely used and validated in australia china the united states and africa research indicates that the apsim model simulates very well wheat growth yield and water consumption in the ncp e g chen et al 2010 li et al 2014 xiao and tao 2014 wang et al 2009 2012 zhang et al 2012 in this study the apsim model version 7 6 is used to simulate rain fed winter wheat growth in the ncp the minimum required climatic inputs include daily maximum t m a x and minimum t m i n temperature radiation r a d and precipitation p as described in the apsim wheat module there are seven parameters that need to be determined table 1 the seven parameters not only control phenology development of winter wheat but also influence biomass accumulation and harvested grain among them parameters p 1 p 4 are mainly related to wheat phenology while parameters p 5 p 7 are associated with yield the larger the values of p 1 and p 2 the more sensitive the cultivar is to vernalization and photoperiod cultivars with high p 1 and p 2 are a strong winter variety requiring more vernalization days parameters p 3 and p 4 are required thermal duration of the phase from grain filling to maturity and from emergence to end of juvenile respectively higher values of p 3 and p 4 suggest that a crop would go through the grain filling stage and juvenile stage much more slowly more details of the apsim model can be found at the website https www apsim info generally the parameters are calibrated by using field observations to optimize apsim model performance with a trial and error method li et al 2014 xiao and tao 2014 zhang et al 2012 in the calibration procedures usually the parameters for phenology development are determined first and then the parameters for grain development proper simulation of phenology is the first priority to calibrate the crop model since phenology captures much of the genotypic variation and drives many crop related processes in simulation models archontoulis et al 2014 the proper simulation of crop phenology is a necessary condition for the crop model to produce reliable estimation of biomass production and grain yield robertson et al 2002 if the model performs well in simulating phenology it is usually expected to have strong potential to provide reliable yield predictions the model can also be calibrated against observed wheat yield when the aim of the study is to simulate actual productivity for given environmental and management conditions however since this study focuses on the changes in wheat productivity from climate it is not essential to compare the climatic productivity against the actual one in this study different to the trial and error method the parameters are determined based on parameter candidates selected via literature review the candidates are selected in accordance with the principles of representativeness reliability and reproducibility as shown in table 1 there are eight parameter candidates applicable for winter wheat simulation across the ncp it is worth noting that the parameter set is cultivar dependent which means that the parameter set could not be the same for different cropping sites however since the scope of this study focuses on the association between crop yield and climate variables only to remove the effect of the difference in cultivar we assume the cultivar is the same for all sites across the ncp therefore the parameter set should be identical for all sites as well the single parameter set suitable for all sites in the region is then selected from the eight candidates according to their overall performance at the 15 sites against phenology observations available in setting up of the apsim wheat module in this study the sowing date is assumed to be the date when there are three continuous days with daily mean air temperature lower than 18 c mo et al 2009 the simulations also assume that soil texture of the whole study area is uniform and with sufficient fertilizer inputs thus neglecting the differences in soil physical and chemical properties like water holding capacity soil water infiltration root penetration soil aeration and soil salinization the soil up to a 1 5 m beneath the surface is divided into six layers and values of soil properties for each layer are set according to the results of zhang et al 2012 as we focus on the climatic productivity of winter wheat to eliminate the impacts of other environmental and management factors the cropping system in the region is assumed to be wheat only though wheat maize rotation is more common the model runs continuously for the simulation period but different to the simulation of wheat maize rotation cropping it assumes that there are no planting activities at the growing season of maize thereafter consecutive simulations of winter wheat productivity are conducted for all 73 stations across the ncp over the period 1960 2010 3 results and discussions 3 1 performance of apsim wheat module fig 2 shows the performance of apsim wheat modelling for the 15 phenology sites using all of the eight parameter candidates measures of performance invoked see bennett et al 2013 were the determination coefficient r 2 and root mean square error r m s e calculated according to simulated and observed dates of maturity for 1990 2010 as seen from fig 2 in general simulated and observed dates of maturity agree well for all 15 sites the differences r m s e between simulated and observed dates of maturity for the eight parameter candidates range from 3 38 to 5 15 days at all 15 sites but average at 4 20 days the determination coefficient r 2 is about 0 44 0 64 with an average of 0 53 for all 15 sites and there are nine sites with an r 2 greater than 0 5 up to 0 84 comparably the overall performance of the parameter candidates relative to cultivar nongda211 see table 1 is the best for its higher r 2 about 0 48 0 89 with an average of 0 66 and lower r m s e about 2 05 5 84 days with an average of 2 97 days for most of the sites fig 3 further shows the range of ensemble simulations of the rain fed wheat productivity using all the eight parameter candidates it shows clearly that simulations from different parameter candidates are not distributed uniformly for a specific year as the median could be quite different to the mean the range of the ensemble simulations varies among the simulation periods for some years the performance among the different parameter candidates is quite consistent smaller range it is worth noting that there is no systematic underestimate overestimate of wheat productivity for a specific parameter set as compared against the others which means that the simulated productivity of a parameter set may be lower than the other parameter sets in a specific year but it is not necessarily lower in the other years the results mean that the productivity of different cultivar varies with climate condition to maximize wheat productivity it implies that it could be beneficial to change the cultivar in adaptation to climate variation actually wheat cultivar in the ncp has been renewed about every 3 5 years and approximately 347 cultivars were released and introduced into the national seed market in china from 1984 to 2010 xiao and tao 2014 those crop cultivars have different genetic characteristics and hence their response and adaption to climate change are distinct existing research suggests that cultivar renewal and improved crop management practices have offset negative effects of climatic change on crop yields liu et al 2010 tao et al 2012 wang et al 2012 as shown the response of crop yield to climate varies among cultivars to demonstrate the responses of wheat productivity to climate conditions across the ncp for simplification the parameter set for cultivar nongda211 is selected in this study it is applied for all sites across the ncp because it provides the overall best phenology simulation fig 2 and is much closer to the median productivity simulations based on all the parameter candidates fig 3 3 2 spatial variation of wheat productivity in association with climate fig 4 presents the spatial variation of the mean winter wheat productivity under the rain fed condition for the period 1960 to 2010 in general the annual wheat productivity yield decreases from 11575 kg ha in the southeast to 2868 kg ha in the northwest the high productivity area i e yield 6000 kg ha accounts for 49 06 for the total farmland area in the ncp while the relatively low productivity area i e yield 3000 kg ha is about 11 76 of the total farmland area to estimate the regional mean productivity based on the simulations of the 73 sites we use the kriging method to estimate the productivity of each farmland pixel at the resolution of 1 km 1 km the result shows that the mean regional productivity of the ncp is around 6673 kg ha and coefficient of variation is 0 33 fig 4 also shows the spatial pattern of the climate variables closely related to wheat cropping where a i is the aridity index being the ratio of potential evapotranspiration to precipitation from the southeast to the northwest of the ncp there is a clear decrease in precipitation but an increase in a i meanwhile both t m i n and t m a x increase from north to south but r a d gradually intensifies from the southwest to the northeast comparing the spatial patterns of wheat productivity in the ncp it is found that the rain fed productivity is highly related to precipitation where there is higher precipitation it tends to have higher productivity the lowest wheat productivity is found in places with the highest a i greater than 5 5 while the highest wheat productivity is at the lowest a i productivity is also affected by available solar radiation r a d and temperature in the actual growing season in the north winter wheat growth is often stressed by frost damage and less precipitation and hence productivity is lower in contrast in the south more precipitation and higher temperatures provide sufficient water and heat resources for wheat growth and grain formation and hence bring about higher wheat productivity it is recorded that across the ncp wheat is typically planted in late autumn and harvested in early summer xiao et al 2012 growth of winter wheat in the ncp is often stressed by low temperature and less precipitation low temperature especially in the north often leads to frost damage which could affect wheat growth and may even kill wheat plants in winter and early spring jin 1996 drought stress is another important climate condition affecting growth development and production of plants in the ncp chen et al 2010 water deficit before anthesis may lead to a loss in yield by reducing spike and spikelet number and the fertility of surviving spikelets after anthesis water deficit especially if accompanied with high temperatures could hasten leaf senescence reduce grain filling rate and hence reduce mean kernel weight and dry matter accumulation giunta et al 1993 royo et al 1999 to further investigate the spatial association between rain fed wheat productivity and climate condition across the ncp stepwise regression was implemented to represent the relationship between crop yield and related climate factors where the crop yield used is from the simulation from apsim the regression analysis is based on the simulated crop yield from apsim where all dependent and independent variables are standardized by the correspondingly maximums to eliminate the effects of dimension the regression analysis is valuable to identify the dominant climate variables affecting the spatial pattern of crop yield the result shows that the annual wheat productivity of a specific site is a function of annual precipitation and solar radiation and mean daily minimum and maximum temperature of the actual growing season which can be expressed as 1 k y 2 31 k p 1 54 k p 2 0 45 k r a d 0 36 k t m i n 0 76 k t m a x 0 013 where k y is standardized wheat productivity k p and k r a d are standardized total precipitation and radiation of the actual growing season while k t m i n and k t m a x are standardized mean daily minimum and maximum air temperature in the actual growing season the regression function is at a confidence level of 99 the correlation coefficient between simulated wheat productivity from apsim and the regression model reaches 0 96 fig 5 the regression model eq 1 indicates that the dominant climate factors for wheat productivity in the ncp are precipitation solar radiation and daily minimum and maximum air temperature according to the regression model it can be seen that spatially a non linear effect exists between rain fed wheat productivity and precipitation implying that crop yield is not necessarily increasing decreasing with increase decrease in precipitation it is easy to derive from eq 1 that the partial derivative of wheat productivity to precipitation is given as 2 g p k y k p 2 31 3 08 k p where g p represents the spatial gradient of rain fed wheat productivity against precipitation p it is obvious that g p is not a constant but depends on k p g p decreases with increasing k p which implies that for drier regions the gradient is larger than for wetter regions moreover g p can also be negative if k p is higher than 0 75 implying a negative effect of higher precipitation on wheat productivity in the humid region where precipitation in the wheat growing season is higher than 195 5 mm as seen from eq 1 the spatial gradient represented by partial derivative and equal to the regression coefficient of wheat productivity against r a d t m a x and t m i n are all constants it is noticeable that the spatial variation of wheat productivity is positively related to t m i n but negatively related to t m a x which means that wheat productivity tends to increase spatially with higher t m i n but lower t m a x solar radiation also shows a positive effect on wheat productivity as an increase of 40 38 mj m2 in r a d could result in an increase of 5206 53 kg ha in wheat productivity 3 3 long term change of wheat productivity in association with climate averaging across the ncp for the period from 1960 to 2010 the standard deviation sd of regional wheat productivity is around 892 kg ha and the coefficient of variation cv sd mean is around 0 13 implying rather low inter annual variation of wheat production for the same period according to the result of a mann kendall trend test kendall 1975 mann 1945 it is found that there is a slight but not statistically significant increasing trend in the regional rain fed wheat productivity in the ncp as shown in table 2 the increasing rate of regional wheat productivity is only around 4 70 kg ha a table 2 also shows the long term trends of climate variables in the growing season averaging across the ncp which includes p t m i n t m a x r a d and aridity index a i in the growing season for the period 1960 to 2010 no significant trend of precipitation is detected but t m i n is found to increase significantly while r a d is seen to decrease significantly in practice higher t m i n could lead to higher wheat productivity on the contrary less solar radiation could result in lower wheat productivity in the ncp the results imply that the stimulation effect owing to warming could have been offset by the reduction of solar radiation and therefore result in an insignificant change of wheat productivity in the ncp spatially the trend of wheat productivity varies significantly as shown in fig 6 the long term change rate of wheat productivity varies from 43 1 54 4 kg ha a across the ncp for the period 1960 to 2010 among the 73 sites 14 sites show noticeable reductions in productivity and 20 sites experience unnoticeable reductions meanwhile 39 sites show increasing trends in which only one site has a statistically significant trend it is recognized that the sites with relatively higher wheat productivity which are mainly located in the southern and northeastern ncp have been experiencing reductions in wheat productivity as a consequence of climate change on the contrary those sites located at the middle and northern part of ncp with relatively lower wheat yield have been experiencing upward trends in wheat productivity the results suggest that the effect of climate change on wheat productivity varies among the climate regions e g chen et al 2010 yang et al 2013 the spatial variance of the long term trend of wheat productivity owing to climate change is determined by two underlying issues i e the spatial difference in the climate trend and the different reaction of wheat growth to climate variables to further explore the spatial difference of the response of wheat productivity to climate change the long term trends fig 6 of the dominant climate variables in the growing season are detected by the mann kendall test and the partial correlation analysis fig 7 between wheat productivity and climate variables in the growing season is conducted for all sites across the ncp the partial correlation analysis is preferred herein because it is capable of identifying the individual effects of each climate variable while eliminating the cross correlation between them fig 7 shows the relationship between wheat productivity and four dominant climate factors in the growing season for each site as described by the partial correlation coefficient it is found that variation of wheat productivity is positively related to change in precipitation across most sites in the semi arid region but insignificant negative relationship is found at five sites in the southern edge of the ncp which belongs to the humid region a i 1 5 the close relation between precipitation and wheat productivity means that the growth of winter wheat is under a water constraint for most parts of the ncp to improve wheat productivity across the ncp therefore one of the most effective measures is to promote and extend the application of irrigation however the results here are not being used to promote further increases in irrigation water in the region as over withdrawal of groundwater for irrigation has caused critical environmental problems in the ncp mo et al 2009 given the scarcity of water resources it is more important to improve irrigation efficiency via optimal irrigation scheduling such as timely applications of irrigation based on crop requirements and soil conditions liang et al 2011 in terms of the impact of precipitation change as presented in fig 6 there is no significant trend with respect to precipitation across the ncp which implies that the contribution of precipitation to wheat productivity change across the ncp is limited for the same period a significant increase of t m i n is found almost at all sites across the ncp fig 6 meanwhile t m a x shows a significant upward trend at 22 sites largely situated at the eastern coastal zone of the ncp fig 6 as demonstrated at fig 7 positive correlations between wheat productivity and t m i n are observed at most sites except for 4 sites of the ncp adversely t m a x is negatively related to productivity at almost all sites the results indicate that for most part of ncp the increasing t m i n could benefit wheat productivity as it could potentially alleviate frost damage during the growing season of the winter wheat sun et al 2015 while higher t m a x could harm wheat productivity since it would intensify the drought stress on wheat growth and grain formation especially in the semi arid area with a i greater than 1 5 although increasing t m i n alleviates frost damage more frequent drought stress at the reproductive stage could have induced wheat plant floret sterility tian et al 2009 in the relatively wetter region as in the southern edge of the ncp however higher t m a x could boost wheat growth owing to more available heat given sufficient water resources it is worth noting that the warming climate could shorten the growth duration of current wheat which leads to lower wheat productivity across the ncp in adaption to a warming climate cultivars with longer growing periods or those that are drought resistant and tolerant of high temperatures would be better choices in the ncp alternatively the cultivars with faster development rates and earlier flowering times that can extend the grain filling duration might be adopted under the changing climate zhang et al 2010 fig 6 also shows that r a d decreases significantly at almost all sites across the ncp in fig 7 the correlation between wheat productivity and solar radiation r a d in the growing season is found to be negative across 36 sites but most of them are not statistically significant at the level of  0 1 while remarkably a positive correlation is found at 16 sites the sites with positive correlation are mostly located at the southern edge and eastern coastal region of the ncp and have a lower a i 2 0 at the humid southern edge r a d is an influential factor controlling wheat productivity since higher r a d would further boost wheat development and grain formation owing to better soil moisture conditions as a result at the southern edge of the ncp wheat productivity decreased concurrently with the decline of solar radiation e g chen et al 2010 on the contrary in the dry north the decreasing solar radiation promotes wheat productivity to some extent since it could reduce the evaporation loss of soil water and consequently mitigate water stress with the consideration of the close correlation between solar radiation with wheat productivity particularly in the southern edge the cultivars that could maintain higher photosynthesis rates under weak radiation might be top choices in wheat cropping for climate change adaptation 4 conclusions global food security over the next few decades will be under challenge owing to climate change and socio economic development china is one of the countries most affected thus by climate change and may experience serious reduction of crop yield if without effective climate change mitigation and adaptation strategies the strategies rely deeply on the knowledge about the association between crop yield and climate in this study we have investigated the association across the north china plain which is known as the barn of china for the period 1960 2010 with the application of the process based agricultural model apsim in combination with an empirical approach the results have shown that the annual winter wheat productivity generally decreases from southeast to northwest across the ncp overall the spatial variation of wheat productivity is negatively related to daily maximum temperature but positively related to daily minimum temperature solar radiation and precipitation in the humid southern edge of ncp too much rainfall in the growing season may result in wheat yield reduction the spatial association between wheat yield and climate has been shown here to be useful to evaluate the productivity of the sites across the ncp even for those without any daily meteorological observations during the past 50 years 1960 2010 there has been no significant change of precipitation in the ncp however t m i n has increased significantly while solar radiation has decreased significantly as a consequence of climate change though there is no significant trend in the rain fed wheat productivity on average across the ncp the southern edge and eastern coastal part of the ncp have been experiencing reductions of wheat productivity while the northern part of the ncp has been experiencing an upward trend for most parts of the ncp the increasing t m i n could benefit wheat productivity while increasing t m a x plays an adverse role in the humid southern edge of the ncp however the increasing t m a x and solar radiation could boost wheat growth as more energy is available and there is little constraint on water availability in the drier north of the ncp wheat productivity is mainly discounted by less precipitation acknowledgement this research is partly supported by the australian government research training program agrtp we are very grateful to the china meteorological administration for providing the meteorological data 
26344,china is anticipated to face reductions in crop yield due to climate change combining a process based agricultural model apsim with a simplified empirical model we investigate the association between wheat productivity and climate variables in the north china plain ncp over the period 1960 2010 the results show that the spatial distribution of wheat productivity across ncp is negatively related to daily maximum temperature but positively to daily minimum temperature solar radiation and precipitation for most parts of the ncp partial correlation analysis based on the apsim simulation outputs indicates that the increasing daily minimum temperature could lead to higher productivity but the effects are counteracted by increases in daily maximum temperature in the drier northern ncp wheat productivity is largely discounted due to decreases in precipitation while in the humid southern edge of the ncp the productivity stimulation effect of warming is offset by dimming keywords wheat productivity climate change the north china plain apsim software availability name of software agricultural production systems simulator apsim developer apsim initiative contact address 203 tor street toowoomba qld 4350 australia telephone 61 7 4688 1596 email apsim daff qld gov au year of first available 1994 hardware required any recent pc with a minimum of 2 gb of ram 32 or 64 bit software required windows xp vista windows 8 or windows 10 mono 2 10 for linux availability https www apsim info cost free for non commercial use 1 introduction food security is one of the largest global concerns with regard to the impact of global climate change beddington et al 2012 campbell et al 2016 ipcc 2014 lobell and asseng 2017 all of the four dimensions of food security i e availability accessibility utilization and stability could be affected by climate change directly or indirectly battisti and naylor 2009 fao 2008 godfray et al 2011 lesk et al 2016 rosenzweig et al 2014 tai et al 2014 wheeler and von braun 2013 the effects of climate change on crop and food production are becoming evident in a number of regions around the world and showing more commonly to be negative than positive although the negative impacts of climate change could be offset by the effect of co2 fertilization to some extent e g betts et al 2007 gedney et al 2006 lobell et al 2011 long et al 2006 porter et al 2014 it is reported that due to climate change agriculture output in developing countries may decline by 20 on average by 2080 while agriculture output in industrialized countries is expected to decrease by 6 fao 2008 among the countries most affected by climate change china is anticipated to face more frequent extreme weather events like droughts and floods which could result in the reduction of crop yields in addition with the increasing food demand due to a growing population and rising living standards food security in china may become more challenging and the international grain markets could be deeply reshaped as china is the world s largest wheat producer and consumer godfray et al 2010 ipcc 2014 piao et al 2010 turral et al 2011 zhang et al 2013 as a pivot subject of climate change mitigation and adaptation the responses of crop yields to climate change have been investigated in numerous research articles many of which have come up with important conclusions showing the association between crop production and climate change e g asseng et al 2013 2015 battisti and naylor 2009 butler and huybers 2013 conway et al 2014 lobell et al 2011 urban et al 2015 wheeler and von braun 2013 in general a 1 c rise in temperature could lower crop yields by up to 10 except in high latitude regions lobell et al 2011 for high latitude regions like scotland however the measured warming could contribute more than 20 of the increased crop yields over the period 1960 2006 gregory and marshall 2015 based on a meta analysis of around 1700 model simulations the most recent ipcc assessment demonstrated that on average global mean crop yields i e rice wheat and maize are projected to decrease between 3 and 10 per degree of warming above historical levels challinor et al 2014 among which the global wheat reduction rate is estimated to be 6 per degree of warming asseng et al 2015 for the response of crop yield to precipitation change the yields of nearly all crops show a positive relationship with precipitation but could be undermined by flooding and water logging caused by extremely high precipitation events falloon and betts 2009 lobell et al 2011 many of the largest falls in crop productivity have been attributed to anomalously low precipitation events kumar et al 2004 sivakumar et al 2005 indeed crop yield is quite sensitive to precipitation lobell and burke 2008 estimated that a one standard deviation change in precipitation in the growing season could lead to about a 10 change in millet production in south asia additionally research has also shown that decline in solar radiation and change in radiation composition could have significant effects on plant growth and crop productivity chameides et al 1999 rodriguez and sadras 2007 swain et al 2007 wild et al 2012 when investigating the association between crop productivity and climate change agricultural models are often used challinor et al 2014 cohn et al 2016 holzworth et al 2015 lobell and burke 2010 lobell et al 2011 rosenzweig et al 2014 such agricultural models which represent mathematically the relations between crop productivity and climate variables could be strongly process based e g apsim epic ceres and dssat etc or simplified statistical ones e g asseng et al 2011 challinor et al 2014 chen et al 2010 liu et al 2010 lobell and burke 2010 piao et al 2010 ray et al 2015 tao et al 2012 wang et al 2012 the process based model type has advantages in characterizing the dynamics of crop growth and development in sufficient detail and being able to evaluate the effects of climate change on crop yield when it is well calibrated lobell and burke 2010 lobell and asseng 2017 wilcox and makowski 2014 however the process based model requires routine meteorological inputs like precipitation temperature solar radiation relative humidity etc it becomes difficult if not impossible for the application of the model when faced with limited inputs moreover since the process based model usually operates at a daily time step while crop yield is output as an annual total the relations between crop yield and climate factors are not explicitly shown at annual scale but require additional efforts to be identified lobell and asseng 2017 peng et al 2004 in contrast the statistical model type is easily applicable and can directly summarize with less effort the relationship between crop yield and climate factors at an annual scale which is practicable for farm planning and decision making under changing climate deryng et al 2011 lobell and asseng 2017 makowski et al 2015 wilcox and makowski 2014 however the statistical relations between crop yield and climate variables for most of the statistical models are set up based on crop yield census data which could be at a regional average scale and mismatched in space with the climate observations the relations can be further confounded as the measured crop yield could also be affected by other factors e g changes in cultivars agronomic management practice and technological advancement not represented in the statistical model to enhance the reliability of the statistical model improved crop production datasets and novel approaches are needed to better understand the association between crop yield and climate lobell et al 2011 schlenker and roberts 2009 welch et al 2010 in china both the process based and the statistical model types have been used to assess the impact of climate change on crop production e g chen et al 2010 liu et al 2010 piao et al 2010 tao et al 2006 2012 wang et al 2009 2012 xiao et al 2012 it has been reported that rising temperatures since the 1980s have negatively affected crop production in the north china plain this has arisen because of the shortening of the length of the crop growth period accelerating the physiological development rate and diminishing the growth duration and rate of photosynthesis and biomass accumulation e g liu et al 2010 porter and gawith 1999 tao et al 2012 xiao et al 2012 zhang et al 2013 meanwhile declining solar radiation and precipitation have further reduced crop yields in the north china plain chen et al 2010 tao et al 2012 yang et al 2013 however the spatial association between crop yield and climatic variables remains inconclusive and the response of crop yield to each climate variable is not yet well understood liu et al 2010 lobell et al 2011 olesen et al 2011 tao et al 2012 in this paper we aim to investigate the association between winter wheat productivity and climate variables across the north china plain ncp for the period 1960 to 2010 the process based agricultural model apsim agricultural production systems simulator is used firstly to simulate wheat productivity on the basis of apsim simulations a statistical model is then developed to reflect explicitly the underlying climate factors shaping the spatial distribution of wheat productivity it is used as a surrogate of the apsim model to estimate wheat productivity where daily meteorological inputs are not available thereafter the relation between wheat productivity and climate factors for each site is further investigated to define the dominant climate factor affecting wheat productivity changes across the north china plain during the period 1960 to 2010 2 data and methods 2 1 study area the north china plain ncp is known as the barn of china contributing 50 of the nation s total wheat production liu et al 2010 jeong et al 2014 located at 31 42 40 30 n and 110 24 122 42 e it covers seven provinces in china fig 1 the ncp has a warm temperate monsoon climate with great seasonal and annual variation in temperature and precipitation where winter is cold and dry but summer is hot and wet the observed mean daily temperature in the region varies from 10 7 c in the north to 15 6 c in the south mean annual precipitation ranges from 478 mm in the northwest to 1114 mm in the southeast farmland accounts for 72 of total land area in the ncp of which around 82 is irrigated water shortage in the ncp is an important constraint on the cropping system increasing evidence suggests that climate change in the ncp e g higher temperature less solar radiation and precipitation has adverse impacts on the cropping system chen et al 2010 liu et al 2010 lobell et al 2011 ray et al 2015 in addition shrinking farmland as a consequence of urbanization makes it more challenging to meet the increasing food demand yue et al 2010 2 2 climate and phenology data to investigate the association between winter wheat yield and climate daily meteorological datasets provided by the china meteorological administration are used the dataset includes daily air temperatures maximum and minimum precipitation and sunshine hours for 73 stations in the ncp over the period 1960 to 2010 solar radiation observations one of the required inputs to apsim are calculated from sunshine hours by using the angstrom equation angstrom 1924 among the 73 meteorological sites 15 sites have phenology observations which are used to evaluate the performance of apsim model the phenology dataset includes sowing date emergence date jointing date and maturity date of winter wheat for the period 1991 to 2010 e g xiao et al 2012 2 3 methods apsim agricultural production systems simulator developed by the agricultural production systems research unit in australia is a daily process based model of agricultural systems holzworth et al 2014 keating et al 2003 it can simulate the growth and yield for many crop species in response to climate and management conditions and predict the long term consequence of cropping systems on soil physical and chemical conditions keating et al 2003 it has been widely used and validated in australia china the united states and africa research indicates that the apsim model simulates very well wheat growth yield and water consumption in the ncp e g chen et al 2010 li et al 2014 xiao and tao 2014 wang et al 2009 2012 zhang et al 2012 in this study the apsim model version 7 6 is used to simulate rain fed winter wheat growth in the ncp the minimum required climatic inputs include daily maximum t m a x and minimum t m i n temperature radiation r a d and precipitation p as described in the apsim wheat module there are seven parameters that need to be determined table 1 the seven parameters not only control phenology development of winter wheat but also influence biomass accumulation and harvested grain among them parameters p 1 p 4 are mainly related to wheat phenology while parameters p 5 p 7 are associated with yield the larger the values of p 1 and p 2 the more sensitive the cultivar is to vernalization and photoperiod cultivars with high p 1 and p 2 are a strong winter variety requiring more vernalization days parameters p 3 and p 4 are required thermal duration of the phase from grain filling to maturity and from emergence to end of juvenile respectively higher values of p 3 and p 4 suggest that a crop would go through the grain filling stage and juvenile stage much more slowly more details of the apsim model can be found at the website https www apsim info generally the parameters are calibrated by using field observations to optimize apsim model performance with a trial and error method li et al 2014 xiao and tao 2014 zhang et al 2012 in the calibration procedures usually the parameters for phenology development are determined first and then the parameters for grain development proper simulation of phenology is the first priority to calibrate the crop model since phenology captures much of the genotypic variation and drives many crop related processes in simulation models archontoulis et al 2014 the proper simulation of crop phenology is a necessary condition for the crop model to produce reliable estimation of biomass production and grain yield robertson et al 2002 if the model performs well in simulating phenology it is usually expected to have strong potential to provide reliable yield predictions the model can also be calibrated against observed wheat yield when the aim of the study is to simulate actual productivity for given environmental and management conditions however since this study focuses on the changes in wheat productivity from climate it is not essential to compare the climatic productivity against the actual one in this study different to the trial and error method the parameters are determined based on parameter candidates selected via literature review the candidates are selected in accordance with the principles of representativeness reliability and reproducibility as shown in table 1 there are eight parameter candidates applicable for winter wheat simulation across the ncp it is worth noting that the parameter set is cultivar dependent which means that the parameter set could not be the same for different cropping sites however since the scope of this study focuses on the association between crop yield and climate variables only to remove the effect of the difference in cultivar we assume the cultivar is the same for all sites across the ncp therefore the parameter set should be identical for all sites as well the single parameter set suitable for all sites in the region is then selected from the eight candidates according to their overall performance at the 15 sites against phenology observations available in setting up of the apsim wheat module in this study the sowing date is assumed to be the date when there are three continuous days with daily mean air temperature lower than 18 c mo et al 2009 the simulations also assume that soil texture of the whole study area is uniform and with sufficient fertilizer inputs thus neglecting the differences in soil physical and chemical properties like water holding capacity soil water infiltration root penetration soil aeration and soil salinization the soil up to a 1 5 m beneath the surface is divided into six layers and values of soil properties for each layer are set according to the results of zhang et al 2012 as we focus on the climatic productivity of winter wheat to eliminate the impacts of other environmental and management factors the cropping system in the region is assumed to be wheat only though wheat maize rotation is more common the model runs continuously for the simulation period but different to the simulation of wheat maize rotation cropping it assumes that there are no planting activities at the growing season of maize thereafter consecutive simulations of winter wheat productivity are conducted for all 73 stations across the ncp over the period 1960 2010 3 results and discussions 3 1 performance of apsim wheat module fig 2 shows the performance of apsim wheat modelling for the 15 phenology sites using all of the eight parameter candidates measures of performance invoked see bennett et al 2013 were the determination coefficient r 2 and root mean square error r m s e calculated according to simulated and observed dates of maturity for 1990 2010 as seen from fig 2 in general simulated and observed dates of maturity agree well for all 15 sites the differences r m s e between simulated and observed dates of maturity for the eight parameter candidates range from 3 38 to 5 15 days at all 15 sites but average at 4 20 days the determination coefficient r 2 is about 0 44 0 64 with an average of 0 53 for all 15 sites and there are nine sites with an r 2 greater than 0 5 up to 0 84 comparably the overall performance of the parameter candidates relative to cultivar nongda211 see table 1 is the best for its higher r 2 about 0 48 0 89 with an average of 0 66 and lower r m s e about 2 05 5 84 days with an average of 2 97 days for most of the sites fig 3 further shows the range of ensemble simulations of the rain fed wheat productivity using all the eight parameter candidates it shows clearly that simulations from different parameter candidates are not distributed uniformly for a specific year as the median could be quite different to the mean the range of the ensemble simulations varies among the simulation periods for some years the performance among the different parameter candidates is quite consistent smaller range it is worth noting that there is no systematic underestimate overestimate of wheat productivity for a specific parameter set as compared against the others which means that the simulated productivity of a parameter set may be lower than the other parameter sets in a specific year but it is not necessarily lower in the other years the results mean that the productivity of different cultivar varies with climate condition to maximize wheat productivity it implies that it could be beneficial to change the cultivar in adaptation to climate variation actually wheat cultivar in the ncp has been renewed about every 3 5 years and approximately 347 cultivars were released and introduced into the national seed market in china from 1984 to 2010 xiao and tao 2014 those crop cultivars have different genetic characteristics and hence their response and adaption to climate change are distinct existing research suggests that cultivar renewal and improved crop management practices have offset negative effects of climatic change on crop yields liu et al 2010 tao et al 2012 wang et al 2012 as shown the response of crop yield to climate varies among cultivars to demonstrate the responses of wheat productivity to climate conditions across the ncp for simplification the parameter set for cultivar nongda211 is selected in this study it is applied for all sites across the ncp because it provides the overall best phenology simulation fig 2 and is much closer to the median productivity simulations based on all the parameter candidates fig 3 3 2 spatial variation of wheat productivity in association with climate fig 4 presents the spatial variation of the mean winter wheat productivity under the rain fed condition for the period 1960 to 2010 in general the annual wheat productivity yield decreases from 11575 kg ha in the southeast to 2868 kg ha in the northwest the high productivity area i e yield 6000 kg ha accounts for 49 06 for the total farmland area in the ncp while the relatively low productivity area i e yield 3000 kg ha is about 11 76 of the total farmland area to estimate the regional mean productivity based on the simulations of the 73 sites we use the kriging method to estimate the productivity of each farmland pixel at the resolution of 1 km 1 km the result shows that the mean regional productivity of the ncp is around 6673 kg ha and coefficient of variation is 0 33 fig 4 also shows the spatial pattern of the climate variables closely related to wheat cropping where a i is the aridity index being the ratio of potential evapotranspiration to precipitation from the southeast to the northwest of the ncp there is a clear decrease in precipitation but an increase in a i meanwhile both t m i n and t m a x increase from north to south but r a d gradually intensifies from the southwest to the northeast comparing the spatial patterns of wheat productivity in the ncp it is found that the rain fed productivity is highly related to precipitation where there is higher precipitation it tends to have higher productivity the lowest wheat productivity is found in places with the highest a i greater than 5 5 while the highest wheat productivity is at the lowest a i productivity is also affected by available solar radiation r a d and temperature in the actual growing season in the north winter wheat growth is often stressed by frost damage and less precipitation and hence productivity is lower in contrast in the south more precipitation and higher temperatures provide sufficient water and heat resources for wheat growth and grain formation and hence bring about higher wheat productivity it is recorded that across the ncp wheat is typically planted in late autumn and harvested in early summer xiao et al 2012 growth of winter wheat in the ncp is often stressed by low temperature and less precipitation low temperature especially in the north often leads to frost damage which could affect wheat growth and may even kill wheat plants in winter and early spring jin 1996 drought stress is another important climate condition affecting growth development and production of plants in the ncp chen et al 2010 water deficit before anthesis may lead to a loss in yield by reducing spike and spikelet number and the fertility of surviving spikelets after anthesis water deficit especially if accompanied with high temperatures could hasten leaf senescence reduce grain filling rate and hence reduce mean kernel weight and dry matter accumulation giunta et al 1993 royo et al 1999 to further investigate the spatial association between rain fed wheat productivity and climate condition across the ncp stepwise regression was implemented to represent the relationship between crop yield and related climate factors where the crop yield used is from the simulation from apsim the regression analysis is based on the simulated crop yield from apsim where all dependent and independent variables are standardized by the correspondingly maximums to eliminate the effects of dimension the regression analysis is valuable to identify the dominant climate variables affecting the spatial pattern of crop yield the result shows that the annual wheat productivity of a specific site is a function of annual precipitation and solar radiation and mean daily minimum and maximum temperature of the actual growing season which can be expressed as 1 k y 2 31 k p 1 54 k p 2 0 45 k r a d 0 36 k t m i n 0 76 k t m a x 0 013 where k y is standardized wheat productivity k p and k r a d are standardized total precipitation and radiation of the actual growing season while k t m i n and k t m a x are standardized mean daily minimum and maximum air temperature in the actual growing season the regression function is at a confidence level of 99 the correlation coefficient between simulated wheat productivity from apsim and the regression model reaches 0 96 fig 5 the regression model eq 1 indicates that the dominant climate factors for wheat productivity in the ncp are precipitation solar radiation and daily minimum and maximum air temperature according to the regression model it can be seen that spatially a non linear effect exists between rain fed wheat productivity and precipitation implying that crop yield is not necessarily increasing decreasing with increase decrease in precipitation it is easy to derive from eq 1 that the partial derivative of wheat productivity to precipitation is given as 2 g p k y k p 2 31 3 08 k p where g p represents the spatial gradient of rain fed wheat productivity against precipitation p it is obvious that g p is not a constant but depends on k p g p decreases with increasing k p which implies that for drier regions the gradient is larger than for wetter regions moreover g p can also be negative if k p is higher than 0 75 implying a negative effect of higher precipitation on wheat productivity in the humid region where precipitation in the wheat growing season is higher than 195 5 mm as seen from eq 1 the spatial gradient represented by partial derivative and equal to the regression coefficient of wheat productivity against r a d t m a x and t m i n are all constants it is noticeable that the spatial variation of wheat productivity is positively related to t m i n but negatively related to t m a x which means that wheat productivity tends to increase spatially with higher t m i n but lower t m a x solar radiation also shows a positive effect on wheat productivity as an increase of 40 38 mj m2 in r a d could result in an increase of 5206 53 kg ha in wheat productivity 3 3 long term change of wheat productivity in association with climate averaging across the ncp for the period from 1960 to 2010 the standard deviation sd of regional wheat productivity is around 892 kg ha and the coefficient of variation cv sd mean is around 0 13 implying rather low inter annual variation of wheat production for the same period according to the result of a mann kendall trend test kendall 1975 mann 1945 it is found that there is a slight but not statistically significant increasing trend in the regional rain fed wheat productivity in the ncp as shown in table 2 the increasing rate of regional wheat productivity is only around 4 70 kg ha a table 2 also shows the long term trends of climate variables in the growing season averaging across the ncp which includes p t m i n t m a x r a d and aridity index a i in the growing season for the period 1960 to 2010 no significant trend of precipitation is detected but t m i n is found to increase significantly while r a d is seen to decrease significantly in practice higher t m i n could lead to higher wheat productivity on the contrary less solar radiation could result in lower wheat productivity in the ncp the results imply that the stimulation effect owing to warming could have been offset by the reduction of solar radiation and therefore result in an insignificant change of wheat productivity in the ncp spatially the trend of wheat productivity varies significantly as shown in fig 6 the long term change rate of wheat productivity varies from 43 1 54 4 kg ha a across the ncp for the period 1960 to 2010 among the 73 sites 14 sites show noticeable reductions in productivity and 20 sites experience unnoticeable reductions meanwhile 39 sites show increasing trends in which only one site has a statistically significant trend it is recognized that the sites with relatively higher wheat productivity which are mainly located in the southern and northeastern ncp have been experiencing reductions in wheat productivity as a consequence of climate change on the contrary those sites located at the middle and northern part of ncp with relatively lower wheat yield have been experiencing upward trends in wheat productivity the results suggest that the effect of climate change on wheat productivity varies among the climate regions e g chen et al 2010 yang et al 2013 the spatial variance of the long term trend of wheat productivity owing to climate change is determined by two underlying issues i e the spatial difference in the climate trend and the different reaction of wheat growth to climate variables to further explore the spatial difference of the response of wheat productivity to climate change the long term trends fig 6 of the dominant climate variables in the growing season are detected by the mann kendall test and the partial correlation analysis fig 7 between wheat productivity and climate variables in the growing season is conducted for all sites across the ncp the partial correlation analysis is preferred herein because it is capable of identifying the individual effects of each climate variable while eliminating the cross correlation between them fig 7 shows the relationship between wheat productivity and four dominant climate factors in the growing season for each site as described by the partial correlation coefficient it is found that variation of wheat productivity is positively related to change in precipitation across most sites in the semi arid region but insignificant negative relationship is found at five sites in the southern edge of the ncp which belongs to the humid region a i 1 5 the close relation between precipitation and wheat productivity means that the growth of winter wheat is under a water constraint for most parts of the ncp to improve wheat productivity across the ncp therefore one of the most effective measures is to promote and extend the application of irrigation however the results here are not being used to promote further increases in irrigation water in the region as over withdrawal of groundwater for irrigation has caused critical environmental problems in the ncp mo et al 2009 given the scarcity of water resources it is more important to improve irrigation efficiency via optimal irrigation scheduling such as timely applications of irrigation based on crop requirements and soil conditions liang et al 2011 in terms of the impact of precipitation change as presented in fig 6 there is no significant trend with respect to precipitation across the ncp which implies that the contribution of precipitation to wheat productivity change across the ncp is limited for the same period a significant increase of t m i n is found almost at all sites across the ncp fig 6 meanwhile t m a x shows a significant upward trend at 22 sites largely situated at the eastern coastal zone of the ncp fig 6 as demonstrated at fig 7 positive correlations between wheat productivity and t m i n are observed at most sites except for 4 sites of the ncp adversely t m a x is negatively related to productivity at almost all sites the results indicate that for most part of ncp the increasing t m i n could benefit wheat productivity as it could potentially alleviate frost damage during the growing season of the winter wheat sun et al 2015 while higher t m a x could harm wheat productivity since it would intensify the drought stress on wheat growth and grain formation especially in the semi arid area with a i greater than 1 5 although increasing t m i n alleviates frost damage more frequent drought stress at the reproductive stage could have induced wheat plant floret sterility tian et al 2009 in the relatively wetter region as in the southern edge of the ncp however higher t m a x could boost wheat growth owing to more available heat given sufficient water resources it is worth noting that the warming climate could shorten the growth duration of current wheat which leads to lower wheat productivity across the ncp in adaption to a warming climate cultivars with longer growing periods or those that are drought resistant and tolerant of high temperatures would be better choices in the ncp alternatively the cultivars with faster development rates and earlier flowering times that can extend the grain filling duration might be adopted under the changing climate zhang et al 2010 fig 6 also shows that r a d decreases significantly at almost all sites across the ncp in fig 7 the correlation between wheat productivity and solar radiation r a d in the growing season is found to be negative across 36 sites but most of them are not statistically significant at the level of  0 1 while remarkably a positive correlation is found at 16 sites the sites with positive correlation are mostly located at the southern edge and eastern coastal region of the ncp and have a lower a i 2 0 at the humid southern edge r a d is an influential factor controlling wheat productivity since higher r a d would further boost wheat development and grain formation owing to better soil moisture conditions as a result at the southern edge of the ncp wheat productivity decreased concurrently with the decline of solar radiation e g chen et al 2010 on the contrary in the dry north the decreasing solar radiation promotes wheat productivity to some extent since it could reduce the evaporation loss of soil water and consequently mitigate water stress with the consideration of the close correlation between solar radiation with wheat productivity particularly in the southern edge the cultivars that could maintain higher photosynthesis rates under weak radiation might be top choices in wheat cropping for climate change adaptation 4 conclusions global food security over the next few decades will be under challenge owing to climate change and socio economic development china is one of the countries most affected thus by climate change and may experience serious reduction of crop yield if without effective climate change mitigation and adaptation strategies the strategies rely deeply on the knowledge about the association between crop yield and climate in this study we have investigated the association across the north china plain which is known as the barn of china for the period 1960 2010 with the application of the process based agricultural model apsim in combination with an empirical approach the results have shown that the annual winter wheat productivity generally decreases from southeast to northwest across the ncp overall the spatial variation of wheat productivity is negatively related to daily maximum temperature but positively related to daily minimum temperature solar radiation and precipitation in the humid southern edge of ncp too much rainfall in the growing season may result in wheat yield reduction the spatial association between wheat yield and climate has been shown here to be useful to evaluate the productivity of the sites across the ncp even for those without any daily meteorological observations during the past 50 years 1960 2010 there has been no significant change of precipitation in the ncp however t m i n has increased significantly while solar radiation has decreased significantly as a consequence of climate change though there is no significant trend in the rain fed wheat productivity on average across the ncp the southern edge and eastern coastal part of the ncp have been experiencing reductions of wheat productivity while the northern part of the ncp has been experiencing an upward trend for most parts of the ncp the increasing t m i n could benefit wheat productivity while increasing t m a x plays an adverse role in the humid southern edge of the ncp however the increasing t m a x and solar radiation could boost wheat growth as more energy is available and there is little constraint on water availability in the drier north of the ncp wheat productivity is mainly discounted by less precipitation acknowledgement this research is partly supported by the australian government research training program agrtp we are very grateful to the china meteorological administration for providing the meteorological data 
