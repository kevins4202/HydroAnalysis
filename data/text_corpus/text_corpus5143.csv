index,text
25715,a recently introduced multiscale model for representing the effects of small scale hyporheic zone biogeochemical processes is extended from the reach scale to river network scales the model uses advection dispersion reaction equations for the channel network and one dimensional advection reaction subgrid models for the hyporheic zone we summarize implementation in the integrated surface subsurface hydrology modeling system amanzi ats the extension uses topologically defined meshes to represent stream river networks and associates a hyporheic subgrid model with each channel grid cell biogeochemical reaction modeling capability residing in community software is accessed through an application programming interface the implementation is verified against independent numerical solutions on a single reach mesh convergence studies show that commonly used semi distributed representations can introduce significant spatial discretization error denitrification of farm runoff in a subbasin of the portage river basin in ohio usa is used to demonstrate the general purpose reactive transport capability keywords reactive transport models hyporheic zone river networks surface subsurface hydrological models multiscale models 1 introduction hyporheic zones the regions of water saturated sediment adjacent to the flowing channels of rivers and streams are biogeochemical hotspots where transformation and retention of carbon nutrients metals pesticides and pharmaceuticals are often elevated relative to the flowing channel hyporheic exchange flows the movement of water through hyporheic zones and back to the flowing channel allow solutes to access those reactive zones and thus play an important role in regulating water quality in rivers as anthropogenic stresses on river water quality increase globally there is a significant need for quantitative modeling tools that represent the combined effects of hyporheic exchange flows and biogeochemical processes in the hyporheic zone while still remaining tractable at basin scales two and three dimensional multicomponent reactive transport models of the hyporheic zone have contributed much to understanding biogeochemical processing in the hyporheic zone boano et al 2010 marzadri et al 2011 bardini et al 2012 marzadri et al 2012 azizian et al 2017 dwivedi et al 2018 however biogeochemical conditions especially redox conditions can vary rapidly over very short distances in the hyporheic zone and this variability is an important determinant of overall function hedin et al 1998 as a result computational meshes with high spatial resolution are required and explicit spatial representations of multicomponent reactive transport are only tractable at relatively small scales typically the scale of single hydrogeomorphic feature at reach scales a common strategy for solute transport is to use the transient storage model tsm bencala and walters 1983 with sorption and biodegradation represented as first order transformations with effective rate constants gooseff et al 2004 gooseff et al 2005 haggerty et al 2009 kim et al 2021 the tsm has also been coupled to multicomponent equilibrium speciation models broshears et al 1996 runkel et al 1996 runkel et al 1999 both tsm based approaches conceptualize the hyporheic zone as a small volume reactor that is well mixed and coupled to the stream channel by first order bidirectional mass exchange a significant simplification that does not account for important spatial variability such as redox zonation moreover the tsm does not adequately account for flowpath diversity in the hyporheic zone specifically the tsm implies an exponential distribution of hyporheic residence times and often fails to reproduce late time tailing of breakthrough curves haggerty et al 2002 gooseff et al 2003 marion et al 2003 zaramella et al 2003 the long residence times associated with the tail of the distribution can be biogeochemical important as those flowpaths may have sufficient time to become anoxic resulting in different biogeochemical function e g denitrification zarnetske et al 2011 zarnetske et al 2012 multirate generalizations of the tsm that are capable of representing late time tailing of breakthrough curves have been used for transport in river stream corridors silva et al 2009 anderson and phanikumar 2011 haggerty 2013 and extended to reactive transport fang et al 2020 however the many transient storage zones in the multirate tsm conceptualization are each exchanging oxygen with the channel which suppresses the important process of redox zonation in the multirate tsm models painter 2021 moving from reach scales to river network scales the modeling challenges are only amplified although network based models for reactive transport at watershed and river basin scales have a long history e g arnold et al 1998 whitehead et al 1998 seitzinger et al 2002 wade et al 2002 mulholland et al 2008 alexander et al 2009 bertuzzo et al 2017 czuba et al 2018 diamantini et al 2019 yang et al 2019 the majority of network based models do not explicitly account for mass transfer limitations between the flowing stream channel and biogeochemically active hyporheic zones without explicit separation of in channel and out of channel processes reaction rates become effective rates that incorporate the effects of biogeochemical processes mass transfer limitations and advective delays and flowpath diversity in the hyporheic zone as a result the link to laboratory and site scale investigations of biogeochemical processes is tenuous at best and reach scale calibration becomes necessary the tsm model has been linked to network scale models ye et al 2012 fang et al 2020 femeena et al 2020 and a few models seitzinger et al 2002 mulholland et al 2008 alexander et al 2009 implicitly account for the effects of mass transfer limitations by making effective reaction rates dependent on concentration and discharge using empirical relationships but network models more commonly use effective rate constants without consideration of how those effective rates link to different biogeochemical and mass transfer processes challenges associated with modeling biogeochemical dynamics at river network scales were explored by helton et al 2011 who evaluated common modeling approaches for modeling biogeochemistry in stream networks using scaling of in situ denitrification from headwater streams to river networks as a concrete example they point out that limitations inherent in current model approaches restrict our ability to simulate biogeochemical dynamics among diverse river networks among the needed advancements they identify representations of hydrologic exchanges between the stream channel and subsurface waters coupled to more mechanistic representations of coupled biogeochemical cycles with those challenges in mind we recently introduced a multiscale modeling strategy painter 2018 2021 to allow hyporheic zone biogeochemical processes to be represented in reach scale models while accounting for hyporheic exchange flow geochemical gradients within the hyporheic zone and diversity of hyporheic flowpaths that multiscale approach adels advection dispersion equation with lagrangian subgrids uses a one dimensional advection dispersion reaction equation for the channel and couples that equation at each channel location to a one dimensional advection reaction subgrid model representing an ensemble of streamlines that are diverted into the hyporheic zone before returning to the channel the adels multiscale representation was shown to be equivalent to previous convolution based approaches haggerty et al 2002 wörman et al 2002 gooseff et al 2003 marion et al 2008 liao and cirpka 2011 lemke et al 2013 liao et al 2013 that have been highly successful for representing stream tracer tests in the situation of steady flow and non reactive transport the value in the adels formulation comes from the fact that it generalizes to multicomponent reactive transport unlike convolution based models computational requirements for reactive transport simulations in large stream networks are vastly reduced compared with explicit two or three dimensional representations of the hyporheic zone because the multiscale adels approach allows for a relatively course discretization of the stream channel while still capturing the effects of fine scale geochemical variability within the hyporheic zone here we extend the multiscale adels model to stream networks describe its implementation in the integrated surface subsurface hydrology modeling system amanzi ats coon et al 2019 and demonstrate on network scale models using denitrification as an example the implementation uses topologically defined meshes to represent stream river networks and takes advantage of ats s capability to handle multiple computational meshes garimella et al 2014 to associate a one dimensional hyporheic subgrid model to each channel grid cell the application programming interface alquimia https github com lbl eesa alquimia dev allows access to biogeochemical reaction modeling capability residing in community software such as pflotran hammond et al 2014 2 methods 2 1 adels model reach scale model accounting for hyporheic exchange flux at the reach scale the adels model uses a one dimensional advection dispersion reaction equation for the channel and couples that equation at each channel location to a one dimensional advection reaction subgrid model representing an ensemble of streamlines that are diverted into the hyporheic zone before returning to the channel a schematic is shown in fig 1 the hyporheic subgrid model is written in lagrangian form with hyporheic age τ replacing the spatial distance thus distance along that streamline is parameterized by the hyporheic age τ with 0 τ τ where τ is the hyporheic lifetime or travel time the ensemble of streamlines is characterized by the lifetime probability density φ τ because the streamlines all have the same upstream boundary condition which is governed by the concentration in the stream channel the concentration profiles are the same along the streamlines computationally we thus need only consider reactive transport along one representative streamline solute flux from the hyporheic zone to the stream channel can then be computed from results at intermediate locations along the single streamline properly weighted by the lifetime probability density the concentration in the channel c c h is then a function of position x along the reach and time t c c h c c h t x whereas the concentration c h z in the hyporheic zone is a function of x t and the hyporheic age residence time c h z c h z t x τ we consider a reach with cross sectional area a x m2 and stream discharge m3 s q x q 0 0 x q l x d x where q 0 is the discharge at the beginning of the reach in question and q l x m2 s is the volumetric lateral inflow outflow per unit stream length which represents the water gained or lost to the groundwater system as well as water gained from overland flow and unmapped small streams defining a transport operator t c x q c x a d c x where d is dispersion coefficient m2 s the reactive transport system can be written in compact form as painter 2018 2021 1 a c c h t t b c c h a α h z b c c h γ h z t x γ l t x a p c h c c h a s c h 2 c h z t b c h z τ p h z c h z 3 c h z t x 0 c c h t x 4 γ h z t x a α h z b 0 c h z t x τ φ τ d τ 5 γ l t x q l b c l t x for q l 0 q l b c c h t x for q l 0 eq 1 is the conservation equation for solutes in the channel eq 2 represents transport and reaction processes in the hyporheic zone bold quantities represent column vectors and c h z t x τ contain the species concentrations in the channel and hyporheic zone respectively the symbol denotes element wise multiplication the column vector b is 1 for mobile species and 0 for immobile species multiplication by b ensures that immobile species are not transported the first term on the right in eq 1 represents the loss of solutes to the hyporheic zone and γ h z with explicit form eq 4 is the return flux from the hyporheic zone eq 3 and γ h z provide the coupling between the channel and hyporheic zone with γ h z acting as a solute source to the channel and the channel concentration providing the upstream boundary condition on the canonical streamline for the hyporheic zone strength of the coupling is quantified by the rate constant for hyporheic exchange α 1 s which is the volumetric flowrate for recirculating water in the hyporheic zone per unit water volume in the channel γ l is the solute source due to lateral flow which represents the combined effect of groundwater inflows overland flow and flow from unmapped tributaries to the reach in question its explicit form in eq 5 is different for gaining and losing streams depending on the sign of q l c l is the concentration in incoming water for gaining streams in eqs 1 and 2 p c h c c h and p h z c h z describe the transformation production consumption rates caused by reactions as functions of local concentrations note that each species depends on all the other species through the reaction terms s c h in eq 1 is the direct source of solute to the channel that could represent for example exchange of oxygen with the atmosphere for numerical implementation it is convenient to approximate the integral in eq 4 with a quadrature a computationally efficient choice painter 2018 2021 is to select quadrature points t i that are equally spaced in probability as illustrated in fig 2 which makes the quadrature weights the same and equal to 1 n where n is the number of quadrature points with that choice the return flux becomes 6 γ h z t x a α h z n b i 1 n c h z t x τ i with 7 τ i φ 1 i 1 2 n here φ τ of the cumulative distribution of hyporheic lifetime which has inverse φ 1 eqs 1 3 and 5 7 represent the system solved in our implementation of adels in amanzi ats 2 2 adels extension to watershed scales here we describe an extension of the above multiscale model to represent stream river networks and its implementation in the amanzi ats or simply ats software version 1 1 2 2 1 implementation in the ats software amanzi ats coon et al 2019 is an integrated hydrology simulator that couples flow energy transport and solute transport in the surface and subsurface ats s processes are implemented within the amanzi toolkit which includes core functionality for solving partial differential equations pdes describing multiphysics applications amanzi is highly parallel by domain decomposition and leverages the trilinos heroux et al 2003 framework for mpi based data parallelism multiple spatial discretization and time evolution schemes are provided here we have used first order spatial discretization donor upwind for advection and finite volume for diffusion which is second order in this case and a first order imex temporal scheme which is explicit for advection and implicit for diffusion however amanzi also provides high order transport advection schemes that are of interest to maintain sharp fronts when physical dispersion is minimal multiple linear iterative solvers and preconditioners are available in amanzi for our simulations we have used pcg preconditioned conjugate gradient with hypre amg falgout and yang 2002 algebraic multigrid for solving the diffusive term the software design of the combined amanzi ats facilitates implementation of multiscale models like that presented here by providing tools for managing multiple process models defined on many coupled domains in addition the specialized spatial structure of our multiscale model is facilitated by amanzi s mesh infrastructure garimella et al 2014 jan et al 2018 which provides a framework for manipulating and managing multiple domains and their underlying discrete representation in a mesh multiphysics complexity across multiple domains is managed in amanzi ats using a framework called arcos coon et al 2016 arcos was originally designed to manage multiple models of physical processes known as process kernels pks pks are selected at runtime and coupled together using generic or application specific couplers called multi process couplers mpcs mpcs couple one or more pks and also implement the pk interface allowing an mpc to appear as a pk from the perspective of other mpcs this property allows the user to define complex hierarchical model structures at runtime jan et al 2018 extended the arcos framework to coordinate many indexed domains that concept has been formalized in arcos through the introduction of pks across domain sets in addition to pks implementing different processes in amanzi ats a domain is a submanifold on which a pk may be solved the discrete form of a domain is a mesh each domain has a mesh and many domains may share a common flyweight mesh that extension enables the implementation of the adels model in amanzi ats the pk tree for that model configuration is shown in fig 3 transport and reactions for both the channel system and the subgrid model are split and weakly or sequentially coupled through an mpc reactions are not implemented natively in amanzi or ats instead geochemical reaction models in the open source pflotran hammond et al 2014 code are called through the application programming interface alquimia molins et al 2018 which abstracts multiple geochemical reaction models to a common interface note the subgrid models are mutually independent and trivially parallelizable each one takes concentrations from the channel as a time dependent boundary condition and returns calculated solution chemistry as function of hyporheic age the mpc hyporheic zone does the bookkeeping to sum the return flux eq 6 as a source to the channel transport system for each domain topologically defined meshes are used to prescribe the discrete representation of the domain on which the pde is solved topologically defined meshes are meshes whose node positions are not necessarily specified explicitly in space as compared to geometrically defined meshes whose nodes are defined explicitly as points within r n and all other geometric quantities can be derived from these positions in a topologic mesh mesh topology is defined by specifying a cell adjacency list for each channel grid cell in addition to mesh topology a few geometric quantities are required to fully specify differential equations on the mesh these include the area of a face and the distance from a cell s centroid to each of its faces this topologic mesh concept allows both stream networks which are 1 d submanifolds whose coordinate is defined as distance along the network and the subgrid mesh which is a 1d submanifold whose coordinate is in travel time to be expressed in amanzi s existing mesh infrastructure 2 2 2 a topologically defined unstructured mesh for the stream network to move from the single reach form of the above equations to a stream network we discretize the stream channel so that each computational cell denotes a segment of a single stream channel as with the single reach model a hyporheic subgrid model can then be attached to each computational cell in the stream network in the coarsest resolution of the stream channel network each channel cell would run from junction to junction as shown schematically in the supplemental fig s2 in general finer resolution is of interest and those junction to junction segments would be further divided into multiple computational cells with a hyporheic subgrid model attached to each the mesh network topology is a directed acyclic graph starting at the outlet and proceeding to the beginning of each first order stream leafs of the tree a cell in this topology can have one two or three neighbors a cell that is not downstream of a junction will have one upstream neighbor unless it is the first cell in a first order stream which has no upstream neighbors cells downstream of a junction have two upstream neighbors all cells have a single downstream neighbor except for the single cell at the outlet which has none 2 2 3 representation of the subgrid system the subgrid model associated with each channel grid cell is a 1 d advection reaction model written in lagragian form with hyporheic age replacing the spatial coordinate therefore the subgrid mesh defines τ as the topological mesh s coordinate and face areas are given uniformly as 1 coupling between each subgrid system and its associated channel grid cell is bidirectional the channel provides upstream boundary conditions for the subgrid model and results from intermediate locations on the subgrid model are summed to form the return flux term eq 7 the calculation of the return flux can be facilitated by using a computational mesh for each subgrid model that is non uniform with the downstream face of each cell coinciding with one of the quadrature points τ i as given by eq 6 2 3 site description two watersheds were chosen to demonstrate the implementation of adels in ats the east fork poplar creek bear creek efpc bc and south branch portage river middle branch portage river sbpr mbpr located in tennessee and ohio usa respectively fig 4 the efpc bc 35 95 latitude and 84 38 longitude is located in oak ridge tn the upper part of efpc has legacy mercury contamination from past activities at the y 12 national security complex y 12 complex the sbpr mbpr 41 51 latitude and 84 93 longitude is predominantly agricultural land 78 nutrient nitrate and phosphorus loading to streams from these lands have depleted water quality and affected fish communities the efpc bc 060102070302 and sbpr mbpr 0410001002 are 12 and 10 digit hydrologic unit codes huc watersheds respectively colors in fig 4 right column present annual average stream discharge extracted from the national hydrography database nhdplus buto and anderson 2020 and mapped onto stream river networks for the corresponding watersheds as described later a map of the river networks without mapped discharge is provided in the electronic supplement fig s1 2 4 flow characterization from the nhdplus database here we limit our scope to solute transport and reactions using prescribed and steady channel flow extensions of this work to accommodate unsteady flow will be described in a future publication following the convention of the nhdplus database buto and anderson 2020 we specify the channel discharge at the downstream end of each stream reach just upstream of each junction discharge just downstream of each junction is then determined by flow continuity that is by summing the discharges from the contributing reaches discharges on the upstream side of the first order reaches must also be specified and are presumed to be zero neglecting spring fed reaches when specifying channel discharges this way from the nhdplus database or similar sources discharge at the upstream and downstream end of a reach will in general not be the same an increase in discharge over the length of a reach indicates groundwater inflows or the time averaged contribution of overland flow or unmapped ephemeral streams gaining reach similarly a decrease in discharge indicates a losing reach that implied source or sink is then distributed equally among channel grid cells in the gaining or losing reaches as lateral inflow outflow the annual average channel discharge extracted from the nhdplus database and mapped to river networks of efpc bc top right and sbpr mbpr bottom right watersheds are shown in fig 4 right column the channel network is discretized into cells 10 m long in these examples for illustration purpose only note that a coarser resolution is used in our numerical scenarios presented later also a sensitivity study on stream spatial resolution will be presented to show that high resolution spatial mesh is needed in some special cases but not always 2 5 example biogeochemical reaction system we demonstrate the model through a fairly standard denitrification example this denitrification example tracks a single representative dissolved organic carbon doc species with chemical composition ch2o dissolved oxygen and nitrate with aerobic respiration and denitrification c h 2 o o 2 a q c o 2 a q h 2 o c h 2 o 4 5 n o 3 4 5 h 4 10 n 2 a q c o 2 a q 7 5 h 2 o the microbially mediated denitrification reactions are assumed to occur in the subgrid system but not in the channels reaction rates are described by dual monod kinetics with oxygen inhibiting denitrification reaction rates are represented as 23 r d o c r o 2 5 4 r n o 3 24 r o 2 r a e r μ c o 2 k a e r o 2 μ c d o c k a e r d o c 25 r n o 3 r d e n μ c n o 3 k d e n n o 3 μ c d o c k d e n d o c μ k d e n o 2 i n h c o 2 here μ a b a a b represents monod kinetics or and inhibition function depending on the order of concentration and model parameter in the argument list that is if c is a concentration and k is a constant then μ c k represents monod kinetics with monod half saturation k switching the order of the arguments μ k c defines an inhibition function with the constant k taking the role of inhibition constant symbol definitions and reaction parameter values for the reference case are defined in table 1 the maximum rates r a e r and r d e n were selected to make the half lives for oxygen consumption and denitrification to be 1 h and 5 10 h respectively gomez velez et al 2015 2 6 numerical experiments description to demonstrate adels implementation in ats at network scales we have considered two watersheds shown in fig 4 for our numerical experiments on efpc bc stream network we performed a conservative tracer experiment and a denitrification experiment to demonstrate the sensitivity of results to spatial discretization several stream spatial resolutions are considered with two different stream hyporheic exchange rate constants several spatial resolutions are considered the coarsest resolution has one channel cell between each stream junction as is typical for semi distributed models like swat arnold et al 1998 the finest resolution has a maximum size of 250 m for the channel grid cells a pulse denitrification numerical experiment with high and low nitrate loading is carried out on sbpr mbpr watershed the system is initialized with a low nitrate concentration 50 μ m and continued low nitrate watershed loading for 10 days to get the system in a steady state and avoid the effect of dilution a pulse of high nitrate 400 μ m loading at the first order reaches is introduced for the next 15 days here we will refer to low and high nitrate cases as low nutrient loading and high nutrient loading note that loading of other species are unchanged in these scenarios two numerical tests were performed for sbpr mbpr one with and one without considering biogeochemical reactions in the hyporheic zone and denoted by bgc and nobgc respectively in both cases reactions were not considered in the stream network itself note that the two cases are identical in terms of the amount and timing of nutrient loading the only difference is the use of reactions versus no reactions in the hyporheic zone model inputs such as stream discharge segment length and channel width are extracted from the nhdplus database and do not vary among scenarios or during the course of simulations moreover the implied source of water gaining losing reaches is also identical in all numerical scenarios for a given watershed although amanzi ats accommodates full heterogeneity in the input parameters we avoid that complication in these demonstration simulations and use spatially and temporally constant values for the exchange rate between the stream and hyporheic zone a log normal distribution with mean of 5 h and log variance of 1 was assumed for the hyporheic lifetime distribution also constant in space and time the sensitivity studies presented here aimed at demonstrating 1 the impact of spatial discretization on nutrient breakthrough curves 2 the influence of stream hyporheic exchange rate on nitrate removal and 2 the impact of the reaeration rate constant on dissolved oxygen in streams and the associated nitrate consumption in the hyporheic zone all these cases are designed to showcase the biogeochemical process complexity that can be accommodated at scales large enough to be relevant to water quality the low and high exchange rate constants used in the conservative tracer test are provided in table 1 the denitrification simulations with low medium and high hyporheic exchange rate constants are denoted by le me and he respectively the me refers to the reference case exchange value the low and high exchange values are 2 times smaller and greater than the reference case value respectively all numerical simulations are summarized in table 2 3 results 3 1 ats verification tests on a single reach to verify the adels implementation in the ats and build confidence in using topological meshes we compare breakthrough curves of concentrations doc n and dissolved oxygen obtained in ats denitrification experiment with an independent simulation coded in the mathematica software wolfram research 2021 results for a 2 km long reach with total simulated time of 3 day and with 1 day nitrate pulse injection at the inlet boundary cell are shown in fig 5 the breakthrough time and tailing behavior are in very good agreement which confirms that the adels method is correctly implemented in ats slight differences at the leading edge of the doc breakthrough curve and at the end of the nitrate plateau are judged to be caused by differences in the time evolution schemes although not reported here extensive testing has been done on a single reach and three reaches y type configuration to build confidence in the adels implementation within ats before its use for simulating larger stream river networks moreover the breakthrough curve of a conversative tracer obtained in ats simulation is also compared against independent solution fig s3 in the supplemental 3 2 mesh convergence in network scale simulations we performed conservative tracer and denitrification experiments on the efpc watershed top right in fig 4 to study the sensitivity of breakthrough curves and nitrate removal to stream spatial discretization in the conservative pulse tracer test with initially tracer free water in the stream and hyporheic zone we inject tracer for 6 days in the first order streams and monitor the tracer concentration at the outlet of the watershed after 6 days of tracer injection the system is flushed with tracer free water for another 30 days fig 6 left shows breakthrough curves obtained in high and low resolution simulations with low and high hyporheic exchange rate constants the breakthrough curve depends weakly on resolution for the low exchange case but becomes more sensitive with the higher exchange in our denitrification example a pulse injection with high nitrate loading is carried out the system is initialized with low nutrient concentrations values provided in table 1 and low nitrate watershed loading for 10 days after 10 days a high nitrate loading at the first order reaches is introduced for 15 days fig 6 right depicts the relative percentage difference between coarse resolution stream length and high resolution of 250 m it is evident that the nitrate removal especially in low order streams is strongly affected by the spatial discretization we also compared a 500 m case to the 250 m case and the relative difference is much smaller indicating convergence as spatial resolution is increased fig s4 in the supplemental these results suggest that nutrient processing is likely to be sensitive to spatial resolution when exchange between stream and hyporheic zone is strong and the hyporheic zone biogeochemical system is reactive moreover these results show that semi distributed models that use a single stream channel between adjacent stream junctions may introduce significant spatial discretization error however the discretization error may be less significant if the effect of interest is breakthrough curves obtained at locations far away from biogeochemical hotspots and or in highly diluted zones e g the watershed outlet in the example here 3 3 numerical demonstration experiments on denitrification in the sbpr mbpr watershed in the denitrification example in the sbpr mbpr watershed the streams and hyporheic zone are initialized with low nutrient concentrations in a 25 day simulation period the first 10 days represent a period of low nitrate loading to all reaches by end of day 10 the stream and hyporheic zone concentrations are nearly in a steady state for the next 15 days the low order streams are subject to high nitrate loading while the rest of the watershed loading is unchanged in fig 7 we show the concentrations of dissolved organic carbon doc nitrate and dissolved oxygen do at the watershed outlet with bgc scenario and without reactions nobgc scenario with low medium and high stream hyporheic exchange rate constants the solid and dashed lines represent breakthrough curves at the watershed outlet in the nobgc and bgc cases respectively because the initial conditions are arbitrarily chosen and far from equilibrium the first 10 days are essentially model spinup during that period biogeochemical reactions cause a sudden drop in all concentrations until a consistent steady state is approached at 10 days when the nitrate loading is increased at 10 days the nitrate concentration at the outlet also increases by comparing the nobgc and bgc cases it can be seen that more than one half the nitrate is removed through denitrification reactions as the stream hyporheic exchange rate increases the removal of nitrate by denitrification also increases fig 7 middle fig 8 shows nitrate concentration in the reference case at the end of simulation we show nitrate removal percent per km mapped onto the stream network in the same case in fig 9 spatial variability in stream size discharge and lateral inflow caused significant spatial heterogeneity in the nitrate consumption while the details of the spatial patterns seen in figs 9 and 10 would depend on various assumptions this model can represent significant spatial complexity in biogeochemical processing including localized hot spots of denitrification most of the nitrate consumption by denitrification happens in the low order reaches in this example reaches with higher discharge all things being equal show lower nitrate removal efficiencies because net denitrification is the result of a competition between downstream transport and hyporheic exchange and reactions higher discharge shifts the balance more toward downstream transport and reduces the net denitrification rate however it is important to note that we have not correlated the hyporheic exchange rate to the discharge in this initial demonstration and the relative strength of denitrification with stream order may change with more realistic representation of hyporheic exchange the doc nitrate and dissolved oxygen concentrations at the watershed outlet at the end of simulation are shown in fig 10 for the nobgc and bgc scenarios with low medium and high exchange rates results from an additional numerical experiment bgc with low reaeration rate are also shown in fig 10 for all combinations of hyporheic exchange and reaeration rate constants including biogeochemical reactions in the hyporheic zone decreases the concentrations as expected doc concentrations are strongly dependent on the hyporheic exchange rate constant nitrate concentrations are less sensitive because denitrification becomes carbon limited with higher exchange rates dissolved oxygen concentrations are insensitive to the hyporheic exchange rate constant for the reference case reaeration rate constant that insensitivity is the result of competing processes on the one hand more exchange causes more doc and oxygen to be consumed on the other hand increased consumption of doc in the low order streams results in carbon limitations in the downstream reaches which reduces the consumption of the oxygen that is introduced by reaeration carbon consumption is reduced in the low reaeration case which reduces the carbon limitations and makes nitrate and dissolved oxygen concentrations more sensitive to hyporheic exchange 4 summary and future work this work is part of a larger effort to enable detailed biogeochemical reactive transport modeling capability in watershed scale models although reactive transport modeling has long history and proven value in advancing mechanistic understanding steefel et al 2005 application of rtms at watershed scales is relatively rare li et al 2021 central among the many challenges in bringing rtms to the watershed scale is the representation of biogeochemical process understanding gained from laboratory or small scale field investigations in much larger models those scaling challenges are unlikely to be addressed through brute force simulation instead fit for purpose model structures informed by cross disciplinary theoretical understanding are needed li et al 2021 here we address on one aspect of that greater challenge the representation of hyporheic zone biogeochemical processing in river network scale simulations the multiscale modeling strategy presented here provides an alternative to explicit three dimensional representations of the hyporheic zone biogeochemical processes that are impracticable at scale a recently proposed multiscale model adels for simulating hyporheic exchange flows and hyporheic zone biogeochemical processes was extended to river network scales and implemented in the integrated surface subsurface thermal hydrology modeling system amanzi ats version 1 1 the adels implementation in amanzi ats uses topologically defined meshes to represent river networks and the corresponding hyporheic zone subgrid models amanzi ats s meshing workflow has been extended to filter river networks from a given shape file or huc number and extract data from nhdplus database to characterize river networks the hyporheic zone biogeochemical processes are incorporated in the model through the software interface alquimia which provides amanzi ats access to chemical reaction engines such as pflotran the amanzi ats results on a single reach were verified against independent solutions and build confidence in the implementation of the adels model further we demonstrated simulations on river networks huc 12 and huc 10 considering conservative tracer transport and denitrification as examples a mesh convergence study shows that simplified representation of stream characteristics as done in semi distributed models can lead to significant discretization errors which are larger with more biogeochemical activity in the hyporheic zone and with stronger stream hyporheic zone interactions we demonstrated that the removal of nitrate through denitrification is highly correlated with hyporheic exchange rate doc availability and reaeration rate in addition to the nitrate availability which are consistent with published works the capability presented here is intended for numerical experiments to help understand a variety of questions about geochemical processing in stream river networks the capability is currently limited to steady discharge and to the river corridor itself and thus requires water and solute fluxes from the larger watershed be specified a few extensions are required to make the capability compatible with a full 3d watershed reactive transport capability o combine amanzi ats s meshing workflow a pre processing step for stream and hyporheic zone model inputs with the nexss model gomez velez and harvey 2014 to account for spatially variable stream hyporheic zone exchange rates and travel times in ats simulations spatial heterogeneity in the exchange rate and travel times can significantly impact nitrate removal and may have consequences for biogeochemical hotspots o extend the multiscale adels model to accommodate unsteady stream discharge dynamic stream discharge can be specified in two ways 1 sequentially couple amanzi ats s stream flow pk to the ats stream transport pk to get a dynamic stream flow field before advancing transport and the reaction system through alquimia or 2 provide a time series of stream discharge and update the associated quantities such as hyporheic exchange rate ground water source etc dynamically before the transport pk advances in time regardless of how the dynamic stream discharge is obtained minor modifications to the adels framework are needed to accommodate unsteady stream discharge o combine the multiscale adels model with amanzi ats s 3d integrated surface subsurface hydrology capability in that approach 3d watershed models with spatial resolution on the order of 10s of meters or larger would be used to simulate flow and reactive transport in groundwater on the surface and in the stream channel while the adels conceptualization would be used to represent hyporheic zone processing in hydrogeomorphic features smaller than the grid cells code and data availability the advanced terrestrial simulator ats coon et al 2019 is open source under the bsd 3 clause license and is publicly available at https github com amanzi ats and https doi org 10 11578 dc 20190911 1 simulations were conducted using version 1 1 0 modeling data input files scripts for pre and post processing and scripts for extracting nhdplus data for river networks characterization are publicly available at https doi org 10 12769 1785972 author contributions numerical simulations were performed by aj with guidance from slp etc and aj contributed to the development of the model with help from slp for debugging and benchmarking all authors contributed to design of the research and to the manuscript preparation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful to saubhagya rathore for careful review of the manuscript this work was funded by the u s department of energy office of science biological and environmental research environmental systems science program and is a product of the ideas watersheds project and the critical interfaces science focus area sfa at ornl this research used resources of the compute and data environment for science cades at the oak ridge national laboratory which is supported by the office of science of the u s department of energy under contract no de ac05 00or22725 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105166 
25715,a recently introduced multiscale model for representing the effects of small scale hyporheic zone biogeochemical processes is extended from the reach scale to river network scales the model uses advection dispersion reaction equations for the channel network and one dimensional advection reaction subgrid models for the hyporheic zone we summarize implementation in the integrated surface subsurface hydrology modeling system amanzi ats the extension uses topologically defined meshes to represent stream river networks and associates a hyporheic subgrid model with each channel grid cell biogeochemical reaction modeling capability residing in community software is accessed through an application programming interface the implementation is verified against independent numerical solutions on a single reach mesh convergence studies show that commonly used semi distributed representations can introduce significant spatial discretization error denitrification of farm runoff in a subbasin of the portage river basin in ohio usa is used to demonstrate the general purpose reactive transport capability keywords reactive transport models hyporheic zone river networks surface subsurface hydrological models multiscale models 1 introduction hyporheic zones the regions of water saturated sediment adjacent to the flowing channels of rivers and streams are biogeochemical hotspots where transformation and retention of carbon nutrients metals pesticides and pharmaceuticals are often elevated relative to the flowing channel hyporheic exchange flows the movement of water through hyporheic zones and back to the flowing channel allow solutes to access those reactive zones and thus play an important role in regulating water quality in rivers as anthropogenic stresses on river water quality increase globally there is a significant need for quantitative modeling tools that represent the combined effects of hyporheic exchange flows and biogeochemical processes in the hyporheic zone while still remaining tractable at basin scales two and three dimensional multicomponent reactive transport models of the hyporheic zone have contributed much to understanding biogeochemical processing in the hyporheic zone boano et al 2010 marzadri et al 2011 bardini et al 2012 marzadri et al 2012 azizian et al 2017 dwivedi et al 2018 however biogeochemical conditions especially redox conditions can vary rapidly over very short distances in the hyporheic zone and this variability is an important determinant of overall function hedin et al 1998 as a result computational meshes with high spatial resolution are required and explicit spatial representations of multicomponent reactive transport are only tractable at relatively small scales typically the scale of single hydrogeomorphic feature at reach scales a common strategy for solute transport is to use the transient storage model tsm bencala and walters 1983 with sorption and biodegradation represented as first order transformations with effective rate constants gooseff et al 2004 gooseff et al 2005 haggerty et al 2009 kim et al 2021 the tsm has also been coupled to multicomponent equilibrium speciation models broshears et al 1996 runkel et al 1996 runkel et al 1999 both tsm based approaches conceptualize the hyporheic zone as a small volume reactor that is well mixed and coupled to the stream channel by first order bidirectional mass exchange a significant simplification that does not account for important spatial variability such as redox zonation moreover the tsm does not adequately account for flowpath diversity in the hyporheic zone specifically the tsm implies an exponential distribution of hyporheic residence times and often fails to reproduce late time tailing of breakthrough curves haggerty et al 2002 gooseff et al 2003 marion et al 2003 zaramella et al 2003 the long residence times associated with the tail of the distribution can be biogeochemical important as those flowpaths may have sufficient time to become anoxic resulting in different biogeochemical function e g denitrification zarnetske et al 2011 zarnetske et al 2012 multirate generalizations of the tsm that are capable of representing late time tailing of breakthrough curves have been used for transport in river stream corridors silva et al 2009 anderson and phanikumar 2011 haggerty 2013 and extended to reactive transport fang et al 2020 however the many transient storage zones in the multirate tsm conceptualization are each exchanging oxygen with the channel which suppresses the important process of redox zonation in the multirate tsm models painter 2021 moving from reach scales to river network scales the modeling challenges are only amplified although network based models for reactive transport at watershed and river basin scales have a long history e g arnold et al 1998 whitehead et al 1998 seitzinger et al 2002 wade et al 2002 mulholland et al 2008 alexander et al 2009 bertuzzo et al 2017 czuba et al 2018 diamantini et al 2019 yang et al 2019 the majority of network based models do not explicitly account for mass transfer limitations between the flowing stream channel and biogeochemically active hyporheic zones without explicit separation of in channel and out of channel processes reaction rates become effective rates that incorporate the effects of biogeochemical processes mass transfer limitations and advective delays and flowpath diversity in the hyporheic zone as a result the link to laboratory and site scale investigations of biogeochemical processes is tenuous at best and reach scale calibration becomes necessary the tsm model has been linked to network scale models ye et al 2012 fang et al 2020 femeena et al 2020 and a few models seitzinger et al 2002 mulholland et al 2008 alexander et al 2009 implicitly account for the effects of mass transfer limitations by making effective reaction rates dependent on concentration and discharge using empirical relationships but network models more commonly use effective rate constants without consideration of how those effective rates link to different biogeochemical and mass transfer processes challenges associated with modeling biogeochemical dynamics at river network scales were explored by helton et al 2011 who evaluated common modeling approaches for modeling biogeochemistry in stream networks using scaling of in situ denitrification from headwater streams to river networks as a concrete example they point out that limitations inherent in current model approaches restrict our ability to simulate biogeochemical dynamics among diverse river networks among the needed advancements they identify representations of hydrologic exchanges between the stream channel and subsurface waters coupled to more mechanistic representations of coupled biogeochemical cycles with those challenges in mind we recently introduced a multiscale modeling strategy painter 2018 2021 to allow hyporheic zone biogeochemical processes to be represented in reach scale models while accounting for hyporheic exchange flow geochemical gradients within the hyporheic zone and diversity of hyporheic flowpaths that multiscale approach adels advection dispersion equation with lagrangian subgrids uses a one dimensional advection dispersion reaction equation for the channel and couples that equation at each channel location to a one dimensional advection reaction subgrid model representing an ensemble of streamlines that are diverted into the hyporheic zone before returning to the channel the adels multiscale representation was shown to be equivalent to previous convolution based approaches haggerty et al 2002 wörman et al 2002 gooseff et al 2003 marion et al 2008 liao and cirpka 2011 lemke et al 2013 liao et al 2013 that have been highly successful for representing stream tracer tests in the situation of steady flow and non reactive transport the value in the adels formulation comes from the fact that it generalizes to multicomponent reactive transport unlike convolution based models computational requirements for reactive transport simulations in large stream networks are vastly reduced compared with explicit two or three dimensional representations of the hyporheic zone because the multiscale adels approach allows for a relatively course discretization of the stream channel while still capturing the effects of fine scale geochemical variability within the hyporheic zone here we extend the multiscale adels model to stream networks describe its implementation in the integrated surface subsurface hydrology modeling system amanzi ats coon et al 2019 and demonstrate on network scale models using denitrification as an example the implementation uses topologically defined meshes to represent stream river networks and takes advantage of ats s capability to handle multiple computational meshes garimella et al 2014 to associate a one dimensional hyporheic subgrid model to each channel grid cell the application programming interface alquimia https github com lbl eesa alquimia dev allows access to biogeochemical reaction modeling capability residing in community software such as pflotran hammond et al 2014 2 methods 2 1 adels model reach scale model accounting for hyporheic exchange flux at the reach scale the adels model uses a one dimensional advection dispersion reaction equation for the channel and couples that equation at each channel location to a one dimensional advection reaction subgrid model representing an ensemble of streamlines that are diverted into the hyporheic zone before returning to the channel a schematic is shown in fig 1 the hyporheic subgrid model is written in lagrangian form with hyporheic age τ replacing the spatial distance thus distance along that streamline is parameterized by the hyporheic age τ with 0 τ τ where τ is the hyporheic lifetime or travel time the ensemble of streamlines is characterized by the lifetime probability density φ τ because the streamlines all have the same upstream boundary condition which is governed by the concentration in the stream channel the concentration profiles are the same along the streamlines computationally we thus need only consider reactive transport along one representative streamline solute flux from the hyporheic zone to the stream channel can then be computed from results at intermediate locations along the single streamline properly weighted by the lifetime probability density the concentration in the channel c c h is then a function of position x along the reach and time t c c h c c h t x whereas the concentration c h z in the hyporheic zone is a function of x t and the hyporheic age residence time c h z c h z t x τ we consider a reach with cross sectional area a x m2 and stream discharge m3 s q x q 0 0 x q l x d x where q 0 is the discharge at the beginning of the reach in question and q l x m2 s is the volumetric lateral inflow outflow per unit stream length which represents the water gained or lost to the groundwater system as well as water gained from overland flow and unmapped small streams defining a transport operator t c x q c x a d c x where d is dispersion coefficient m2 s the reactive transport system can be written in compact form as painter 2018 2021 1 a c c h t t b c c h a α h z b c c h γ h z t x γ l t x a p c h c c h a s c h 2 c h z t b c h z τ p h z c h z 3 c h z t x 0 c c h t x 4 γ h z t x a α h z b 0 c h z t x τ φ τ d τ 5 γ l t x q l b c l t x for q l 0 q l b c c h t x for q l 0 eq 1 is the conservation equation for solutes in the channel eq 2 represents transport and reaction processes in the hyporheic zone bold quantities represent column vectors and c h z t x τ contain the species concentrations in the channel and hyporheic zone respectively the symbol denotes element wise multiplication the column vector b is 1 for mobile species and 0 for immobile species multiplication by b ensures that immobile species are not transported the first term on the right in eq 1 represents the loss of solutes to the hyporheic zone and γ h z with explicit form eq 4 is the return flux from the hyporheic zone eq 3 and γ h z provide the coupling between the channel and hyporheic zone with γ h z acting as a solute source to the channel and the channel concentration providing the upstream boundary condition on the canonical streamline for the hyporheic zone strength of the coupling is quantified by the rate constant for hyporheic exchange α 1 s which is the volumetric flowrate for recirculating water in the hyporheic zone per unit water volume in the channel γ l is the solute source due to lateral flow which represents the combined effect of groundwater inflows overland flow and flow from unmapped tributaries to the reach in question its explicit form in eq 5 is different for gaining and losing streams depending on the sign of q l c l is the concentration in incoming water for gaining streams in eqs 1 and 2 p c h c c h and p h z c h z describe the transformation production consumption rates caused by reactions as functions of local concentrations note that each species depends on all the other species through the reaction terms s c h in eq 1 is the direct source of solute to the channel that could represent for example exchange of oxygen with the atmosphere for numerical implementation it is convenient to approximate the integral in eq 4 with a quadrature a computationally efficient choice painter 2018 2021 is to select quadrature points t i that are equally spaced in probability as illustrated in fig 2 which makes the quadrature weights the same and equal to 1 n where n is the number of quadrature points with that choice the return flux becomes 6 γ h z t x a α h z n b i 1 n c h z t x τ i with 7 τ i φ 1 i 1 2 n here φ τ of the cumulative distribution of hyporheic lifetime which has inverse φ 1 eqs 1 3 and 5 7 represent the system solved in our implementation of adels in amanzi ats 2 2 adels extension to watershed scales here we describe an extension of the above multiscale model to represent stream river networks and its implementation in the amanzi ats or simply ats software version 1 1 2 2 1 implementation in the ats software amanzi ats coon et al 2019 is an integrated hydrology simulator that couples flow energy transport and solute transport in the surface and subsurface ats s processes are implemented within the amanzi toolkit which includes core functionality for solving partial differential equations pdes describing multiphysics applications amanzi is highly parallel by domain decomposition and leverages the trilinos heroux et al 2003 framework for mpi based data parallelism multiple spatial discretization and time evolution schemes are provided here we have used first order spatial discretization donor upwind for advection and finite volume for diffusion which is second order in this case and a first order imex temporal scheme which is explicit for advection and implicit for diffusion however amanzi also provides high order transport advection schemes that are of interest to maintain sharp fronts when physical dispersion is minimal multiple linear iterative solvers and preconditioners are available in amanzi for our simulations we have used pcg preconditioned conjugate gradient with hypre amg falgout and yang 2002 algebraic multigrid for solving the diffusive term the software design of the combined amanzi ats facilitates implementation of multiscale models like that presented here by providing tools for managing multiple process models defined on many coupled domains in addition the specialized spatial structure of our multiscale model is facilitated by amanzi s mesh infrastructure garimella et al 2014 jan et al 2018 which provides a framework for manipulating and managing multiple domains and their underlying discrete representation in a mesh multiphysics complexity across multiple domains is managed in amanzi ats using a framework called arcos coon et al 2016 arcos was originally designed to manage multiple models of physical processes known as process kernels pks pks are selected at runtime and coupled together using generic or application specific couplers called multi process couplers mpcs mpcs couple one or more pks and also implement the pk interface allowing an mpc to appear as a pk from the perspective of other mpcs this property allows the user to define complex hierarchical model structures at runtime jan et al 2018 extended the arcos framework to coordinate many indexed domains that concept has been formalized in arcos through the introduction of pks across domain sets in addition to pks implementing different processes in amanzi ats a domain is a submanifold on which a pk may be solved the discrete form of a domain is a mesh each domain has a mesh and many domains may share a common flyweight mesh that extension enables the implementation of the adels model in amanzi ats the pk tree for that model configuration is shown in fig 3 transport and reactions for both the channel system and the subgrid model are split and weakly or sequentially coupled through an mpc reactions are not implemented natively in amanzi or ats instead geochemical reaction models in the open source pflotran hammond et al 2014 code are called through the application programming interface alquimia molins et al 2018 which abstracts multiple geochemical reaction models to a common interface note the subgrid models are mutually independent and trivially parallelizable each one takes concentrations from the channel as a time dependent boundary condition and returns calculated solution chemistry as function of hyporheic age the mpc hyporheic zone does the bookkeeping to sum the return flux eq 6 as a source to the channel transport system for each domain topologically defined meshes are used to prescribe the discrete representation of the domain on which the pde is solved topologically defined meshes are meshes whose node positions are not necessarily specified explicitly in space as compared to geometrically defined meshes whose nodes are defined explicitly as points within r n and all other geometric quantities can be derived from these positions in a topologic mesh mesh topology is defined by specifying a cell adjacency list for each channel grid cell in addition to mesh topology a few geometric quantities are required to fully specify differential equations on the mesh these include the area of a face and the distance from a cell s centroid to each of its faces this topologic mesh concept allows both stream networks which are 1 d submanifolds whose coordinate is defined as distance along the network and the subgrid mesh which is a 1d submanifold whose coordinate is in travel time to be expressed in amanzi s existing mesh infrastructure 2 2 2 a topologically defined unstructured mesh for the stream network to move from the single reach form of the above equations to a stream network we discretize the stream channel so that each computational cell denotes a segment of a single stream channel as with the single reach model a hyporheic subgrid model can then be attached to each computational cell in the stream network in the coarsest resolution of the stream channel network each channel cell would run from junction to junction as shown schematically in the supplemental fig s2 in general finer resolution is of interest and those junction to junction segments would be further divided into multiple computational cells with a hyporheic subgrid model attached to each the mesh network topology is a directed acyclic graph starting at the outlet and proceeding to the beginning of each first order stream leafs of the tree a cell in this topology can have one two or three neighbors a cell that is not downstream of a junction will have one upstream neighbor unless it is the first cell in a first order stream which has no upstream neighbors cells downstream of a junction have two upstream neighbors all cells have a single downstream neighbor except for the single cell at the outlet which has none 2 2 3 representation of the subgrid system the subgrid model associated with each channel grid cell is a 1 d advection reaction model written in lagragian form with hyporheic age replacing the spatial coordinate therefore the subgrid mesh defines τ as the topological mesh s coordinate and face areas are given uniformly as 1 coupling between each subgrid system and its associated channel grid cell is bidirectional the channel provides upstream boundary conditions for the subgrid model and results from intermediate locations on the subgrid model are summed to form the return flux term eq 7 the calculation of the return flux can be facilitated by using a computational mesh for each subgrid model that is non uniform with the downstream face of each cell coinciding with one of the quadrature points τ i as given by eq 6 2 3 site description two watersheds were chosen to demonstrate the implementation of adels in ats the east fork poplar creek bear creek efpc bc and south branch portage river middle branch portage river sbpr mbpr located in tennessee and ohio usa respectively fig 4 the efpc bc 35 95 latitude and 84 38 longitude is located in oak ridge tn the upper part of efpc has legacy mercury contamination from past activities at the y 12 national security complex y 12 complex the sbpr mbpr 41 51 latitude and 84 93 longitude is predominantly agricultural land 78 nutrient nitrate and phosphorus loading to streams from these lands have depleted water quality and affected fish communities the efpc bc 060102070302 and sbpr mbpr 0410001002 are 12 and 10 digit hydrologic unit codes huc watersheds respectively colors in fig 4 right column present annual average stream discharge extracted from the national hydrography database nhdplus buto and anderson 2020 and mapped onto stream river networks for the corresponding watersheds as described later a map of the river networks without mapped discharge is provided in the electronic supplement fig s1 2 4 flow characterization from the nhdplus database here we limit our scope to solute transport and reactions using prescribed and steady channel flow extensions of this work to accommodate unsteady flow will be described in a future publication following the convention of the nhdplus database buto and anderson 2020 we specify the channel discharge at the downstream end of each stream reach just upstream of each junction discharge just downstream of each junction is then determined by flow continuity that is by summing the discharges from the contributing reaches discharges on the upstream side of the first order reaches must also be specified and are presumed to be zero neglecting spring fed reaches when specifying channel discharges this way from the nhdplus database or similar sources discharge at the upstream and downstream end of a reach will in general not be the same an increase in discharge over the length of a reach indicates groundwater inflows or the time averaged contribution of overland flow or unmapped ephemeral streams gaining reach similarly a decrease in discharge indicates a losing reach that implied source or sink is then distributed equally among channel grid cells in the gaining or losing reaches as lateral inflow outflow the annual average channel discharge extracted from the nhdplus database and mapped to river networks of efpc bc top right and sbpr mbpr bottom right watersheds are shown in fig 4 right column the channel network is discretized into cells 10 m long in these examples for illustration purpose only note that a coarser resolution is used in our numerical scenarios presented later also a sensitivity study on stream spatial resolution will be presented to show that high resolution spatial mesh is needed in some special cases but not always 2 5 example biogeochemical reaction system we demonstrate the model through a fairly standard denitrification example this denitrification example tracks a single representative dissolved organic carbon doc species with chemical composition ch2o dissolved oxygen and nitrate with aerobic respiration and denitrification c h 2 o o 2 a q c o 2 a q h 2 o c h 2 o 4 5 n o 3 4 5 h 4 10 n 2 a q c o 2 a q 7 5 h 2 o the microbially mediated denitrification reactions are assumed to occur in the subgrid system but not in the channels reaction rates are described by dual monod kinetics with oxygen inhibiting denitrification reaction rates are represented as 23 r d o c r o 2 5 4 r n o 3 24 r o 2 r a e r μ c o 2 k a e r o 2 μ c d o c k a e r d o c 25 r n o 3 r d e n μ c n o 3 k d e n n o 3 μ c d o c k d e n d o c μ k d e n o 2 i n h c o 2 here μ a b a a b represents monod kinetics or and inhibition function depending on the order of concentration and model parameter in the argument list that is if c is a concentration and k is a constant then μ c k represents monod kinetics with monod half saturation k switching the order of the arguments μ k c defines an inhibition function with the constant k taking the role of inhibition constant symbol definitions and reaction parameter values for the reference case are defined in table 1 the maximum rates r a e r and r d e n were selected to make the half lives for oxygen consumption and denitrification to be 1 h and 5 10 h respectively gomez velez et al 2015 2 6 numerical experiments description to demonstrate adels implementation in ats at network scales we have considered two watersheds shown in fig 4 for our numerical experiments on efpc bc stream network we performed a conservative tracer experiment and a denitrification experiment to demonstrate the sensitivity of results to spatial discretization several stream spatial resolutions are considered with two different stream hyporheic exchange rate constants several spatial resolutions are considered the coarsest resolution has one channel cell between each stream junction as is typical for semi distributed models like swat arnold et al 1998 the finest resolution has a maximum size of 250 m for the channel grid cells a pulse denitrification numerical experiment with high and low nitrate loading is carried out on sbpr mbpr watershed the system is initialized with a low nitrate concentration 50 μ m and continued low nitrate watershed loading for 10 days to get the system in a steady state and avoid the effect of dilution a pulse of high nitrate 400 μ m loading at the first order reaches is introduced for the next 15 days here we will refer to low and high nitrate cases as low nutrient loading and high nutrient loading note that loading of other species are unchanged in these scenarios two numerical tests were performed for sbpr mbpr one with and one without considering biogeochemical reactions in the hyporheic zone and denoted by bgc and nobgc respectively in both cases reactions were not considered in the stream network itself note that the two cases are identical in terms of the amount and timing of nutrient loading the only difference is the use of reactions versus no reactions in the hyporheic zone model inputs such as stream discharge segment length and channel width are extracted from the nhdplus database and do not vary among scenarios or during the course of simulations moreover the implied source of water gaining losing reaches is also identical in all numerical scenarios for a given watershed although amanzi ats accommodates full heterogeneity in the input parameters we avoid that complication in these demonstration simulations and use spatially and temporally constant values for the exchange rate between the stream and hyporheic zone a log normal distribution with mean of 5 h and log variance of 1 was assumed for the hyporheic lifetime distribution also constant in space and time the sensitivity studies presented here aimed at demonstrating 1 the impact of spatial discretization on nutrient breakthrough curves 2 the influence of stream hyporheic exchange rate on nitrate removal and 2 the impact of the reaeration rate constant on dissolved oxygen in streams and the associated nitrate consumption in the hyporheic zone all these cases are designed to showcase the biogeochemical process complexity that can be accommodated at scales large enough to be relevant to water quality the low and high exchange rate constants used in the conservative tracer test are provided in table 1 the denitrification simulations with low medium and high hyporheic exchange rate constants are denoted by le me and he respectively the me refers to the reference case exchange value the low and high exchange values are 2 times smaller and greater than the reference case value respectively all numerical simulations are summarized in table 2 3 results 3 1 ats verification tests on a single reach to verify the adels implementation in the ats and build confidence in using topological meshes we compare breakthrough curves of concentrations doc n and dissolved oxygen obtained in ats denitrification experiment with an independent simulation coded in the mathematica software wolfram research 2021 results for a 2 km long reach with total simulated time of 3 day and with 1 day nitrate pulse injection at the inlet boundary cell are shown in fig 5 the breakthrough time and tailing behavior are in very good agreement which confirms that the adels method is correctly implemented in ats slight differences at the leading edge of the doc breakthrough curve and at the end of the nitrate plateau are judged to be caused by differences in the time evolution schemes although not reported here extensive testing has been done on a single reach and three reaches y type configuration to build confidence in the adels implementation within ats before its use for simulating larger stream river networks moreover the breakthrough curve of a conversative tracer obtained in ats simulation is also compared against independent solution fig s3 in the supplemental 3 2 mesh convergence in network scale simulations we performed conservative tracer and denitrification experiments on the efpc watershed top right in fig 4 to study the sensitivity of breakthrough curves and nitrate removal to stream spatial discretization in the conservative pulse tracer test with initially tracer free water in the stream and hyporheic zone we inject tracer for 6 days in the first order streams and monitor the tracer concentration at the outlet of the watershed after 6 days of tracer injection the system is flushed with tracer free water for another 30 days fig 6 left shows breakthrough curves obtained in high and low resolution simulations with low and high hyporheic exchange rate constants the breakthrough curve depends weakly on resolution for the low exchange case but becomes more sensitive with the higher exchange in our denitrification example a pulse injection with high nitrate loading is carried out the system is initialized with low nutrient concentrations values provided in table 1 and low nitrate watershed loading for 10 days after 10 days a high nitrate loading at the first order reaches is introduced for 15 days fig 6 right depicts the relative percentage difference between coarse resolution stream length and high resolution of 250 m it is evident that the nitrate removal especially in low order streams is strongly affected by the spatial discretization we also compared a 500 m case to the 250 m case and the relative difference is much smaller indicating convergence as spatial resolution is increased fig s4 in the supplemental these results suggest that nutrient processing is likely to be sensitive to spatial resolution when exchange between stream and hyporheic zone is strong and the hyporheic zone biogeochemical system is reactive moreover these results show that semi distributed models that use a single stream channel between adjacent stream junctions may introduce significant spatial discretization error however the discretization error may be less significant if the effect of interest is breakthrough curves obtained at locations far away from biogeochemical hotspots and or in highly diluted zones e g the watershed outlet in the example here 3 3 numerical demonstration experiments on denitrification in the sbpr mbpr watershed in the denitrification example in the sbpr mbpr watershed the streams and hyporheic zone are initialized with low nutrient concentrations in a 25 day simulation period the first 10 days represent a period of low nitrate loading to all reaches by end of day 10 the stream and hyporheic zone concentrations are nearly in a steady state for the next 15 days the low order streams are subject to high nitrate loading while the rest of the watershed loading is unchanged in fig 7 we show the concentrations of dissolved organic carbon doc nitrate and dissolved oxygen do at the watershed outlet with bgc scenario and without reactions nobgc scenario with low medium and high stream hyporheic exchange rate constants the solid and dashed lines represent breakthrough curves at the watershed outlet in the nobgc and bgc cases respectively because the initial conditions are arbitrarily chosen and far from equilibrium the first 10 days are essentially model spinup during that period biogeochemical reactions cause a sudden drop in all concentrations until a consistent steady state is approached at 10 days when the nitrate loading is increased at 10 days the nitrate concentration at the outlet also increases by comparing the nobgc and bgc cases it can be seen that more than one half the nitrate is removed through denitrification reactions as the stream hyporheic exchange rate increases the removal of nitrate by denitrification also increases fig 7 middle fig 8 shows nitrate concentration in the reference case at the end of simulation we show nitrate removal percent per km mapped onto the stream network in the same case in fig 9 spatial variability in stream size discharge and lateral inflow caused significant spatial heterogeneity in the nitrate consumption while the details of the spatial patterns seen in figs 9 and 10 would depend on various assumptions this model can represent significant spatial complexity in biogeochemical processing including localized hot spots of denitrification most of the nitrate consumption by denitrification happens in the low order reaches in this example reaches with higher discharge all things being equal show lower nitrate removal efficiencies because net denitrification is the result of a competition between downstream transport and hyporheic exchange and reactions higher discharge shifts the balance more toward downstream transport and reduces the net denitrification rate however it is important to note that we have not correlated the hyporheic exchange rate to the discharge in this initial demonstration and the relative strength of denitrification with stream order may change with more realistic representation of hyporheic exchange the doc nitrate and dissolved oxygen concentrations at the watershed outlet at the end of simulation are shown in fig 10 for the nobgc and bgc scenarios with low medium and high exchange rates results from an additional numerical experiment bgc with low reaeration rate are also shown in fig 10 for all combinations of hyporheic exchange and reaeration rate constants including biogeochemical reactions in the hyporheic zone decreases the concentrations as expected doc concentrations are strongly dependent on the hyporheic exchange rate constant nitrate concentrations are less sensitive because denitrification becomes carbon limited with higher exchange rates dissolved oxygen concentrations are insensitive to the hyporheic exchange rate constant for the reference case reaeration rate constant that insensitivity is the result of competing processes on the one hand more exchange causes more doc and oxygen to be consumed on the other hand increased consumption of doc in the low order streams results in carbon limitations in the downstream reaches which reduces the consumption of the oxygen that is introduced by reaeration carbon consumption is reduced in the low reaeration case which reduces the carbon limitations and makes nitrate and dissolved oxygen concentrations more sensitive to hyporheic exchange 4 summary and future work this work is part of a larger effort to enable detailed biogeochemical reactive transport modeling capability in watershed scale models although reactive transport modeling has long history and proven value in advancing mechanistic understanding steefel et al 2005 application of rtms at watershed scales is relatively rare li et al 2021 central among the many challenges in bringing rtms to the watershed scale is the representation of biogeochemical process understanding gained from laboratory or small scale field investigations in much larger models those scaling challenges are unlikely to be addressed through brute force simulation instead fit for purpose model structures informed by cross disciplinary theoretical understanding are needed li et al 2021 here we address on one aspect of that greater challenge the representation of hyporheic zone biogeochemical processing in river network scale simulations the multiscale modeling strategy presented here provides an alternative to explicit three dimensional representations of the hyporheic zone biogeochemical processes that are impracticable at scale a recently proposed multiscale model adels for simulating hyporheic exchange flows and hyporheic zone biogeochemical processes was extended to river network scales and implemented in the integrated surface subsurface thermal hydrology modeling system amanzi ats version 1 1 the adels implementation in amanzi ats uses topologically defined meshes to represent river networks and the corresponding hyporheic zone subgrid models amanzi ats s meshing workflow has been extended to filter river networks from a given shape file or huc number and extract data from nhdplus database to characterize river networks the hyporheic zone biogeochemical processes are incorporated in the model through the software interface alquimia which provides amanzi ats access to chemical reaction engines such as pflotran the amanzi ats results on a single reach were verified against independent solutions and build confidence in the implementation of the adels model further we demonstrated simulations on river networks huc 12 and huc 10 considering conservative tracer transport and denitrification as examples a mesh convergence study shows that simplified representation of stream characteristics as done in semi distributed models can lead to significant discretization errors which are larger with more biogeochemical activity in the hyporheic zone and with stronger stream hyporheic zone interactions we demonstrated that the removal of nitrate through denitrification is highly correlated with hyporheic exchange rate doc availability and reaeration rate in addition to the nitrate availability which are consistent with published works the capability presented here is intended for numerical experiments to help understand a variety of questions about geochemical processing in stream river networks the capability is currently limited to steady discharge and to the river corridor itself and thus requires water and solute fluxes from the larger watershed be specified a few extensions are required to make the capability compatible with a full 3d watershed reactive transport capability o combine amanzi ats s meshing workflow a pre processing step for stream and hyporheic zone model inputs with the nexss model gomez velez and harvey 2014 to account for spatially variable stream hyporheic zone exchange rates and travel times in ats simulations spatial heterogeneity in the exchange rate and travel times can significantly impact nitrate removal and may have consequences for biogeochemical hotspots o extend the multiscale adels model to accommodate unsteady stream discharge dynamic stream discharge can be specified in two ways 1 sequentially couple amanzi ats s stream flow pk to the ats stream transport pk to get a dynamic stream flow field before advancing transport and the reaction system through alquimia or 2 provide a time series of stream discharge and update the associated quantities such as hyporheic exchange rate ground water source etc dynamically before the transport pk advances in time regardless of how the dynamic stream discharge is obtained minor modifications to the adels framework are needed to accommodate unsteady stream discharge o combine the multiscale adels model with amanzi ats s 3d integrated surface subsurface hydrology capability in that approach 3d watershed models with spatial resolution on the order of 10s of meters or larger would be used to simulate flow and reactive transport in groundwater on the surface and in the stream channel while the adels conceptualization would be used to represent hyporheic zone processing in hydrogeomorphic features smaller than the grid cells code and data availability the advanced terrestrial simulator ats coon et al 2019 is open source under the bsd 3 clause license and is publicly available at https github com amanzi ats and https doi org 10 11578 dc 20190911 1 simulations were conducted using version 1 1 0 modeling data input files scripts for pre and post processing and scripts for extracting nhdplus data for river networks characterization are publicly available at https doi org 10 12769 1785972 author contributions numerical simulations were performed by aj with guidance from slp etc and aj contributed to the development of the model with help from slp for debugging and benchmarking all authors contributed to design of the research and to the manuscript preparation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful to saubhagya rathore for careful review of the manuscript this work was funded by the u s department of energy office of science biological and environmental research environmental systems science program and is a product of the ideas watersheds project and the critical interfaces science focus area sfa at ornl this research used resources of the compute and data environment for science cades at the oak ridge national laboratory which is supported by the office of science of the u s department of energy under contract no de ac05 00or22725 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105166 
25716,increased model complexity and data quantities have raised the computing power requirement for efficient evapotranspiration et estimation a cloud based service is presented to encapsulate and publish the etwatch modeling algorithms as web application programming interfaces apis in a consistent style to provide extensible model calculation service and data storage service in a cloud platform for water managers and stakeholders the prototype system named etwatch cloud allows users to rapidly and easily set up an et generation project for any region of interest by invoking apis directly to produce et data using a web browser or local integrated development environment the case study demonstrates that etwatch cloud can provide a highly scalable and interoperable et generation tool for stakeholders from et community helping to facilitate the application of remote sensing based et algorithms for water management in hydrology sector keywords actual evapotranspiration remote sensing web api cloud computing 1 introduction evapotranspiration et including evaporation from soil and water bodies and transpiration from vegetated surfaces through plant leaves is a water cycle component that represents real water loss to the atmosphere allen et al 1998 the long term dynamics of et are crucial to regional water balance research water resource utilization planning agricultural water resource management climate change research and even sustainable development of the social economy wu et al 2014 qiu et al 2011 zhang et al 2015 yassen et al 2020 therefore for water resource management accurate monitoring of et is as important as precipitation observations but is significantly more difficult to achieve remote sensing rs makes it possible to obtain surface characteristic parameters for large scale heterogeneous surfaces zhang et al 2016 many rs based algorithms have been developed to estimate the actual et of land surfaces su 2002 bastiaanssen et al 1998 allen et al 2007 teixeira 2010 wu et al 2020 ma et al 2021 typically algorithms for generating et are implemented as independent software modules or packages that run on desktop computers which are accessed by a limited number of users and or operators one example is the standalone version of etwatch which integrates multiple parameters and makes full use of multisource rs data to reflect the spatial and temporal characteristics of land surfaces and water heat fluxes in the main energy balance involved in et estimation wu et al 2020 another example is the r evapotranspiration package which includes 17 modules and uses one or several climate variables to estimate et at a single location guo et al 2016 both approaches require users to have their own it infrastructure including servers computers databases and routers with accessary computing storage and networking services furthermore the increasing complexity of model algorithms and the use of large quantities of multi source data require substantial computing power to generate et data efficiently these factors can prevent some users from producing et data because of insufficient capacity or financial resources for it infrastructure such conditions are not conducive to the effective management of water resources cloud computing is one of the most significant development in modern information and communications technology ict it provides a powerful architecture for performing large scale and complex big data computing several applications based on various cloud platforms have been develop in agricultural monitoring and geographic applications for example cropwatch integrates rs data with field data to determine global major crop production indicators more efficiently with cloud computing wu et al 2015a the cloud based web processing service wps framework has been used to improve the performance of the community multiscale air quality cmaq model supporting earth sciences research zhang et al 2019 the open geographic modeling and simulation system opengms aims to support the sharing of models data and computing power as reusable resources for geographic applications in an open web environment yue et al 2015 chen et al 2020 within the opengms research framework a data sharing method was proposed to improve the usability of hydrological data based on the universal data exchange udx model with the aim of addressing the gap between models and data resources wang et al 2018 2020 the trend of migrating huge quantities of geographic and remote sensing data and models to cloud platform for sharing and integration has grown more and more obvious recently three applications include irrisat earth engine evapotranspiration flux eeflux and openet to provide et data based on the google earth engine gee hornbuckle et al 2015 gorelick et al 2017 allen et al 2015 irrisat was developed to access crop water use etc and crop coefficient kc data through four application programming interfaces apis eeflux is based on the surface energy balance model algorithm and generates maps of evapotranspiration at 30 m resolution derived from thermal and reflected landsat imagery data allen et al 2007 although eeflux is a web based gee app the api of the model has not been open assessed to the public so users are unable to calibrate their region of interest roi or integrate for their own process chain openet was developed as an online platform for mapping evapotranspiration from multiple models at the scale of individual fields https openetdata org the apis and custom reporting tools in openet still wait to be provided currently in this study etwatch cloud is developed to provide an et data generation service based on the ict infrastructure in alibaba cloud a set of algorithms for generating regional actual et at multiple resolutions from 30 m to 1 km using key ground based meteorological variables and multiple satellite data from landsat 8 fy 2 and modis images are deployed in etwatch cloud a web interface enables users to organize projects and produce the required et data without coding or with less coding users can also invoke etwatch apis in python language with an associated web ide integrated development environment that enables rapid development prototyping this all in one platform allows users from various backgrounds such as technical users business users and citizen developers to build et monitoring systems without bearing the cost of ict infrastructure and technical barriers the remainder of this paper is organized as follows section 2 introduces the web service framework and describes the design principles section 3 implements the etwatch cloud based service and provides a case study of model publication and execution data publication and visualization and regional et generation section 4 discusses the pros and cons of etwatch cloud and compares its products with other existing et products and the final section provides conclusions and outlines future work 2 etwatch cloud framework 2 1 role analysis etwatch cloud is a web service that publishes algorithms as apis and shares data as shown in fig 1 there are four types of roles in the service model providers data providers administrators and end users and or operators the model providers are responsible for preparing model resources to generate et data with a standard form and interface encapsulation can reduce the complexity and heterogeneity of these models in terms of how they are presented to end users the model providers also prepare the required input and output parameters and model description so that the end users can understand what the model is for and how to use the model each etwatch model resource is deployed on etwatch cloud for publishing as an api in the rest representational state transfer style which is a hybrid derived from several network based architectural styles fielding 2000 the data providers prepare data resources in a standard form for et data generation or analysis in etwatch cloud the rs data files should be processed from raw data files and saved with the same projection and format to keep data compatible with each other to ensure the identification and uniqueness of the files and the automatic operation of the api data files are named according to the same rules users can access the required data file by name to ensure automatic processing which improves the efficiency of data access data resources are stored in cloud storage and published as service which can be accessed or downloaded on the web the administrators of the service manage the apis data users and cloud resources and monitor logs of model runs input output data and performances once the etwatch model resources are deployed on etwatch cloud administrators monitor the status of each api to ensure that the api can be assessed and used properly by the end users administrators also monitor the storage resources occupied by data and apply for storage space in a timely manner when the storage space is insufficient administrators need to process and respond to users orders for data downloads or production immediately to meet the needs of different levels of users the etwatch cloud provides different online interacting ways for message exchanges between users and the model apis etwatch cloud provides a web interface for no coding end users to add or delete apis without coding in their projects and control the order in which the api runs to produce the required et data users with programming ability can program online to invoke the api and control the workflow in the web programming environment provided by etwatch cloud through these interfaces the log information and output of the model api are sent to the end users 2 2 framework of cloud based web service the cloud based web service framework is designed to share et models and generate et data fig 2 a complete frontend backend separation architecture is used to split etwatch cloud into 4 components an etwatch portal a user center an administration console administrator and a cloud it infrastructure platform a clear and concise api is designed for each module to achieve data transfer and interaction between the database and the frontend so that the service has the characteristics of a short development cycle low coupling high scalability clear hierarchy clear logic and easy maintenance all ict infrastructure resources are provided by a public cloud platform used to store transmit and process data an object storage service oss is used to cost effectively store back up and archive large amounts of rs data in the cloud a relational database service provides users with access to a database without the need to set up physical hardware install software or configure the service for optimal performance block storage devices for elastic compute service ecs instances include cloud disks based on a distributed storage architecture and local disks located on the physical machines where the ecs instances are hosted computational services including ecs elastic container service and function compute service allow users to develop and run their applications in powerful computational clouds these services are monitored and managed by the administrator to ensure efficient cloud resource use the administration console management is designed to manage the apis data users and cloud resources and to monitor performance jobs and cloud resource status the api gateway provides complete life cycle management of api including api publishing maintenance monitoring and unpublishing for different users the user manager allocates a different app id and app key to pass the identity and permission authentication process through the api gateway requests exceeding the normal frequency or with invalid parameter types or values are denied by the api gateway if the request is valid the job scheduler applies for resources from the resource manager to perform model operations the resource manager determines whether to use an idle resource or a newly requested resource to complete a job based on resource usage monitored by the resource monitor the etwatch portal offers a direct interactive and responsive way to view and query information about model apis or data each api represents a model that is needed to produce or analyze et data the api description for each model such as operation names parameter names and data types is shown to end users in the portal these apis are opened for users to invoke remotely in their own et generation projects the data metadata describe the temporal and spatial extent format resolution source projection query api and file list of the data the data api provides a rest interface to retrieve metadata for the data users additionally the model and data resources are described in the javascript object notation json string which is not only easy for humans to read and write but also easy for machines to parse and generate bray 2014 a user guide and project demonstrations provide instructions and examples of api and data usage to speed up user learning the user center has four components the personal information component order component project component and collection component users can collect apis or order data and organize these as a project interactively to minimize coding and produce et data for rois the basic information of a roi including the name image zone id coordination range and time zone is provided by the user to define the project all apis and data are added to the workflow interactively using a web based graphical user interface gui to control the operation of model apis and upload the specific data to be used in addition webide is provided for developers to build customized applications to produce et data online with less python code with the input data presented in json data interchange format 2 3 etwatch model encapsulation 2 3 1 overview of the etwatch model etwatch is a set of models and algorithms that characterize the land surface soil vegetation and atmospheric boundary layer to quantify fluxes between soil vegetation and the boundary layer and to determine the individual components of the surface energy balance with multisource and multiscale rs data wu et al 2012 2016b 2020 fig 3 the inputs of etwatch cloud include polar satellite data such as terra aqua moderate resolution imaging spectroradiometer modis data geostationary meteorological satellite data such as fy2 satellite cloud data products resource satellites such as landsat 8 sentinel 2 and gf1 2 atmospheric infrared sounder airs data microwave rs data from radarsat sentinel 1 and gf3 and meteorological data etwatch is based on the surface energy balance equation with 10 modules to estimate the daily net radiation rn surface soil heat flux g0 sensible heat flux and latent heat flux et with the support of water vapor deficit vpd aerodynamic roughness length atmospheric boundary layer abl canopy conductance and surface resistance the final output of etwatch includes the et for a clear day daily et and monthly et wu et al 2020 multisource satellite data are used to develop algorithms calculating the individual components of the surface energy balance and gap filling table 1 lists all input data and intermediate variables in etwatch cloud can be obtained from rs data meteorological observations and related model estimates 2 3 2 model description encapsulation of the etwatch models is intended to conform each module to a standard api a json model description file provides a format for describing model attributes input data output data and runtime demands with the guidance of this format the etwatch models are encapsulated as standardized apis in addition model users can prepare input data and use the model more easily here the net radiation module is used as an example to introduce the method of describing a model in json format as shown in fig 4 the model description file consists of three objects 1 the attribute object consists of the properties of the module including the name brief description provider abstract and links to related papers 2 the io object is designed to explain the module input and output data names and types provides a brief introduction lists examples and states whether the data are required all the attributes of the input data and the output data are stored as an array in the io object 3 the runtime object describes model deployment information including hardware dependency software dependency and library dependency the net radiation module needs a 2 0 ghz cpu main frequency and 4 gb ram and 10 gb storage disks constitute the minimum hardware dependencies 2 3 3 restful model api etwatch consists of many modules all of which are wrapped in interactive data language idl that cannot be invoked in a web environment by end users an encapsulated etwatch model needs to be further published as a web service using the hypertext transfer protocol http and the api follows the rest approach wherein web services are invoked as uniform resource locator url based requests a restful api defines one or more url endpoints with a domain port and path the urls of these submodules are different and include the submodule names to indicate access to a unique resource for example the net radiation submodule api is http etwatch cn api net radiation the basic operations are querying and running of the model and the http requests get and post correspond to these operations the restful model api is statelessness that helps extend the api to millions of concurrent users by deploying it to multiple servers in the cloud platform using the get method the api returns a json encoded response detailing the information of the submodule and api using the post method the api returns a json encoded response result of submodule execution the rest api uses the status code response message to inform the client of the overall result of its request the status code element is a 4 digit integer result code of the attempt to understand and satisfy the request the first digit of the status code defines the class of response status codes fall into the following two categories 2xxx success the action was successfully received understood and accepted 4xxx error the request contains bad syntax or cannot be fulfilled seven standard status codes are defined that can be used to communicate the results of client requests the design of the status code for the rest api is shown in table 2 3 implementation to implement the etwatch cloud service multiple state of the art techniques libraries and standard protocols were used to develop the apis share data and organize the project based on the elastic cloud computing power of alibaba cloud users can access and utilize the full capability of the etwatch cloud service through a web browser with an internet connection in this way the requirement of computing facilities are greatly reduced 3 1 service implementation 3 1 1 etwatch portal the open source toolkit bootstrap and the jquery javascript libraries were used to develop the etwatch portal the portal is a responsive user friendly web application that works across a multitude of web browsers including firefox and chrome without any additional plug in installations fig 5 shows the following pages of the etwatch portal homepage model information data information and user guide end users can easily query and view published models and data in the portal the method request parameters and response information for each api are presented on the api information webpage users can test the api with example requests and view the output of the processing on the test webpage as shown in fig 6 when clicking the post button the request parameters are transmitted as a body by sending a single json encoded data string if the deserialization process works json data will be sent to the api gateway which processes the data and returns a result response the typical response data are also json encoded and include the status code message and result both the request and the response are encoded using the utf 8 character set 3 1 2 user center the open source toolkit bootstrap and jquery libraries were also used to build the user center as shown in fig 7 end users can manage project order collection and personal information through the corresponding webpages via browsers for newly registered users a 5 gb data storage space is allocated to project data storage if the data storage space is not adequate users can submit a request for capacity expansion in the user center the service stores all model results and provides download options through the interface of the user center to allow data analysis or integration with other software users without programming experience can use the web gui to produce et data more easily as shown in fig 8 a dynamic tabbed interface is used as a container for basic information on the api and data for the et generation project when the user activates one of the other tab elements the previously displayed tab panel is hidden the tab panel associated with the activated tab becomes visible and the tab is considered active all apis can be added to the project and their order can be changed by dragging and dropping they can be invoked one by one from left to right and from top to bottom to ensure that outputs from one api are useable as inputs for another 3 1 3 administration console a web gui was built to enable the administrator model provider or data provider to manage their resources on the internet as shown in fig 9 the left sidebar of the administration console gui shows the management entries for all resource types including apis data projects orders users logs and cloud resources the icons in the upper right of the page can be used for rapid access to subpages including the api invoking log lists api lists data lists and project lists the graphs in the middle and bottom of the page show the statistical information about api invoking disk usage and the status of the ecs as shown in fig 10 the backend of the administration console is implemented to create allocate and release the ict computing and storage resources provided by alibaba cloud the idl runtime environment and package for each etwatch module are added to a container image which can become a container when it runs on an elastic container instance eci to isolate dependencies and simplify deployment by default an eci with 1 vcpu and 2 gb memory is used to create an image cache and save time in pulling images from a remote image repository all ict computing and storage resources are monitored and managed to use cloud resources efficiently the api gateway verifies the validity of the request s permissions and parameters if the request is valid and there is no idle eci the resource manager will create a new eci for a new valid request and the resource manager will extend the oss if there is insufficient storage space for new data in addition idle computing resources will be released by the resource manager in time to save costs when new data are stored in the oss the function computing service for data publication will be triggered and then the url of the result will be used for any user request via the api gateway all api information file paths user information and logs are stored in a postgresql relationship database 3 2 model publication and invocation the powerful and flexible django rest framework drf was used to rapidly map the various submodules to api endpoints based on python thirty six apis were implemented to produce the consistent and universal data required for the accurate estimation of et these names and urls can be obtained from table a1 in the appendix we used the net radiation submodule as an example to show all procedures of model publication and invocation as shown in fig 11 once the model resources of etwatch have been deployed the model providers can publish the api of each submodule online after the api is published it is shared on the internet and every user in the portal can use the api provided by etwatch cloud anytime anywhere as shown in fig 12 users can prepare and upload the input data for submodule invocation in the web gui on the webpage users can check the status of a submodule run and obtain output data once the run is finished etwatch apis can also be invoked in all general purpose programming languages such as java python and php thus allowing users to use etwatch apis with their preferred languages webide which includes a file manager python code editor and response viewer is provided in the user center for rapid prototyping online fig 13 3 3 data publication and visualization the open source server program geoserver is used for publishing and sharing spatial data geoserver uses the rest interface s simple http method to create a styled map for users the geoserver manager library was used to programmatically configure geoserver through its rest api as shown in fig 14 once the etwatch data resources have been uploaded the data providers can publish the data online after the data are published the geoserver rest publisher class handles the request and the upper level logic to modify the catalog publish and add styles feature types or coverages every user in the portal can download and use the data provided by etwatch cloud webmap is built based on an open geospatial consortium ogc standard interface web map service wms and provides an interactive and easy webgis tool to visualize geospatial data with the leaflet javascript library a browser client can send a getmap request to obtain rendered map layers and a getfeatureinfo request to query the detailed information on speciﬁed features and the response can then be added to webmap fig 15 users can examine the input rs data and results using the webmap visual interface 3 4 et data generation 3 4 1 study area the hetao irrigation district hid a major grain producing region of china in the inner mongolia autonomous region was chosen to evaluate the performance of etwatch cloud it is located on the north bank of the yellow river 40 13 41 18 n 106 52 109 03 e the average annual temperatures range from 6 to 10 c with maximums in july and minimums in january zhang et al 2021 the annual average precipitation is less than 250 mm and most precipitation events concentrated during june to september most of the area is farmland with lake wuliangsuhai located in the west fig 16 three dominant crops maize sunflower and wheat occupy over 80 of the cropland wen et al 2019 3 4 2 data preparation for this case study rs and ground meteorological data for 2019 were used as inputs for the etwatch model the rs data include the daily modis 1b clear sky products mod09 mod11 mod13 and myd07 and airs data downloaded from the level 1 and atmosphere archive distribution system https ladsweb modaps eosdis nasa gov daily water vapor profile data with different layers were extracted directly from the myd07 and airs products the modis surface temperature and ndvi data at an 1 km resolution were coupled with another rs product the daily amsr e brightness temperature data at a 25 km resolution to estimate the daily soil moisture content fengyun geostationary meteorological satellite fy 2d e f hourly cloud products at a 5 km resolution which were downloaded from the national satellite meteorological center nsmc fengyun satellite data center were also included the ground meteorological data comprising daily data for the maximum air temperature minimum air temperature average air pressure average air humidity average wind speed and sunshine hours were downloaded from the china meteorological data service center http data cma cn two different spatial interpolation methods were used to interpolate these point data into raster data at an 1 km resolution for correspondence with the satellite data the inverse distance squared method was used for the air temperature and air pressure data based on a digital elevation model dem while the thin plate spline method was used for the air humidity wind speed and sunshine hours data franke 1982 lu et al 2008 these data processing steps were integrated into different apis two et products were used to evaluate the et estimates one is based on the operational simplified surface energy balance ssebop model and the second is based on the mod16 data which relies upon the penman monteith equation senay 2018 running et al 2017 3 4 3 et generation to validate the service against its objectives the etwatch cloud project center was used to apply the etwatch apis to the rs and meteorological data process for the hid using 22 apis et data at an 1 km resolution for each day of each month in 2019 at the basin scale and et data at a 30 m resolution for each month in 2019 over the entire hid were produced fig 17 is a screenshot from the etwatch project center of the apis used for data processing and et estimation in the hid case study after all apis have been executed the results can be viewed online in webmap or the data can be obtained via wcs for display and further analysis with local gis software such as qgis fig 18 was generated with qgis 3 16 mapping tools using the 30 m resolution et data provided by etwatch cloud high values are located in the wuliangsuhai lake area while low values occur in the urban areas in wulateqianqi and linhe similar high and low values occur in fig 19 which shows statistical summaries of the et data by land use for the hid in 2019 4 discussion etwatch cloud an et data generation service was developed using the cloud computing power of alibaba cloud to address the challenging resource requirements in computing storage and maintenance posed by et data generation based on multisource rs data the framework of a cloud based web service was applied for generating and sharing et data via the etwatch model we designed a json model description file to ensure the consistency of model encapsulation and designed the api to follow the restful approach to provide flexible api invocation for users computing storage programming and algorithm development are already incorporated into the service this approach is especially useful for users who need to generate their own et data but lack financial resources or professional background table 3 shows the advantages of the etwatch cloud service compared with existing et generation systems there are two types of applications in etwatch cloud enabling users to choose the one with which they are familiar the et project is defined on the webpage by the user which is especially useful for beginners with little coding experience the rest style apis provided in etwatch cloud simplify calling procedures into a consistent process creating a quick and autonomous way to create projects with little coding only basic information is needed to define the project such as the extent of the study area and the time period to be monitored in such a project the user can call apis from the service as private project apis calibrate apis with their own ground data upload rs data for the project area and conduct data processing to generate et data users manage data themselves in this project mode they can download the intermediate and final data products and delete the data and the project they can also delete the data but keep the project for future use alternatively users can use the python runtime environment provided by the service or other programming language environments on their local computers to call apis and organize processing workflows the advantages of etwatch cloud over the standalone version of etwatch include high scalability and easy interoperability because cloud computing and the storage capacity are scalable users with different purposes and capacities can use the scalable etwatch cloud to monitor et at anywhere and at any time without bearing insufficient computing or storage capacities if an et algorithm is updated users can access the new api without reinstalling the software insufficient storage can be addressed through a simple space expansion request rather than a hardware upgrade this permits users to focus on choosing model data combinations and analyzing the results for water resource management the etwatch cloud is a refurbished version of etwatch standalone both can generate et data with the same accuracy and quality fig 20 shows monthly et data for 2019 from etwatch cloud the ssebop model and mod16a2 data the etwatch data show low values in winter and spring and substantially higher values in late summer and early autumn since most cultivated land in the hid is planted with single season crops the monthly et peaks during the peak growth period for these crops in july and august however the current prototypical implementation of etwatch cloud still has some limitations the output from the etwatch cloud service is formatted as raster files tif format and published as a wms layer so it can be easily used as input for other rs or gis approaches for further analyses however more convenient and flexible et data analysis functions remains to be implemented in the service to ensure the quality of et data it is desirable to calibrate etwatch model parameters using ground observation data in the study region calibration should be carried out on key variables such as the surface temperature ground temperature difference shortwave and longwave radiation water heat flux and sensible heat flux using ground measurement data xiong et al 2011 however ground data sources often contain anomalies and may be provided in different formats this makes the integration of data from ground observations into etwatch cloud a complex task an efficient extraction transformation and loading etl tool together with calibration techniques needs to be incorporated into the service in the future 5 conclusion this paper introduces etwatch cloud service which is a model collection migrated from a local pc to the cloud platform for et generation using readily available rs and standard ground based meteorological observations etwatch cloud provides a general solution for the publication execution and management of modules in the cloud apis for the calculation and analysis of et are encapsulated as functional modules and open assessed in a restful style to provide web services through this method each model and its algorithm are shared for model reuse potentially this cloud based method could be used for migrating other rs models to the cloud for data generation services a prototype service was implemented with open source software libraries and standard protocols to publish the model and data based on alibaba cloud et data generation for the hid was used as a case study of the use of etwatch cloud the et estimates for the hid were demonstrated and suggest that the service could also be used in other regions or for other data to model workflows future developments of etwatch cloud include an etl api to support data input for local calibration and enhanced online visualization and analysis capabilities for data output software and data availability software etwatch cloud developers fangming wu bingfang wu contact information wubf aircas ac cn hardware required general purpose computer software required internet browser firefox chrome etc program language java python server and client script technologies javascript json etc availability all apis and data can be accessed via http etwatch cn declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the meteorological and radiation datasets were collected free of charge from the china national meteorological bureau the geostationary meteorological satellite data fy 2d e were collected free of charge from the national satellite meteorological center nsmc fengyun satellite data center modis datasets can be downloaded from the united states nasa websites the authors thank the anonymous reviewers for their critical comments and constructive suggestions which signiﬁcantly improved the quality of the paper funding for this research was provided by the national natural science foundation of china grant numbers 41991232 41701403 and 41601463 the key research program of frontier science cas grant no qyzdy ssw dqc014 and the qinghai science and technology plan grant no 2019 sf 155 appendix a table a1 apis in etwatch cloud table a1 name class url maximum temperature interpolation ground weather http etwatch cn api meteo tmax minimum temperature interpolation ground weather http etwatch cn api meteo tmin average temperature interpolation ground weather http etwatch cn api meteo tavg sunshine hour interpolation ground weather http etwatch cn api meteo sunt humidity interpolation ground weather http etwatch cn api rs api meteo humd winv interpolation ground weather http etwatch cn api meteo winv atmospheric pressure interpolation ground weather http etwatch cn api meteo pre rainfall interpolation ground weather http etwatch cn api meteo rain height of atmospheric boundary layer boundary layer http etwatch cn api airs wind speed in the atmospheric boundary layer boundary layer http etwatch cn api airs wind daily net shortwave radiation surface parameter http etwatch cn api net radiation rswn daily net longwave radiation surface parameters http www etwatch cn api net radiation rlwn solar radiation surface parameters http etwatch cn api net radiation rswd ndvi gapfilling surface parameters http etwatch cn api ndvi rebuild integrated aerodynamic roughness surface parameters http etwatch cn api ad roughness rs gapfilling surface parameter http etwatch cn api rs rebuild albedo gapfilling surface parameter http etwatch cn api albedo rebuild sunshine duration fy surface parameters http etwatch cn api fy sunt api rs smooth surface parameters http etwatch cn api smooth rs surface resistance pm inversion surface parameters http etwatch cn api surface resistance pm inversion sunshine duration msg surface parameters http etwatch cn api msg sunt et fuse sd surface flux http etwatch cn api et fuse sd et fuse downscaling surface flux http etwatch cn api et fuse downscaling instantaneous sensible heat flux parameterization method surface flux http etwatch cn api oshfp instantaneous soil heat flux in satellite transit surface flux http etwatch cn api soil hf instantaneous sensible heat flux sebs surface flux http etwatch cn api instantaneous sensible heat flux sebs et fuse starfm surface flux http etwatch cn api et fuse starfm net radiation surface flux http etwatch cn api net radiation instantaneous netradiation surface flux http etwatch cn api instantaneous netradiation clear day et surface flux http etwatch cn api clear day et sebs daily et0 surface flux http etwatch cn api dayly calculate et0 daily soil heat flux surface flux http etwatch cn api soil hf daily daily et calculate surface flux http etwatch cn api dayly calculate cumulative calculation time and space statistics http etwatch cn api composite space statistics time and space statistics http etwatch cn api statistic 
25716,increased model complexity and data quantities have raised the computing power requirement for efficient evapotranspiration et estimation a cloud based service is presented to encapsulate and publish the etwatch modeling algorithms as web application programming interfaces apis in a consistent style to provide extensible model calculation service and data storage service in a cloud platform for water managers and stakeholders the prototype system named etwatch cloud allows users to rapidly and easily set up an et generation project for any region of interest by invoking apis directly to produce et data using a web browser or local integrated development environment the case study demonstrates that etwatch cloud can provide a highly scalable and interoperable et generation tool for stakeholders from et community helping to facilitate the application of remote sensing based et algorithms for water management in hydrology sector keywords actual evapotranspiration remote sensing web api cloud computing 1 introduction evapotranspiration et including evaporation from soil and water bodies and transpiration from vegetated surfaces through plant leaves is a water cycle component that represents real water loss to the atmosphere allen et al 1998 the long term dynamics of et are crucial to regional water balance research water resource utilization planning agricultural water resource management climate change research and even sustainable development of the social economy wu et al 2014 qiu et al 2011 zhang et al 2015 yassen et al 2020 therefore for water resource management accurate monitoring of et is as important as precipitation observations but is significantly more difficult to achieve remote sensing rs makes it possible to obtain surface characteristic parameters for large scale heterogeneous surfaces zhang et al 2016 many rs based algorithms have been developed to estimate the actual et of land surfaces su 2002 bastiaanssen et al 1998 allen et al 2007 teixeira 2010 wu et al 2020 ma et al 2021 typically algorithms for generating et are implemented as independent software modules or packages that run on desktop computers which are accessed by a limited number of users and or operators one example is the standalone version of etwatch which integrates multiple parameters and makes full use of multisource rs data to reflect the spatial and temporal characteristics of land surfaces and water heat fluxes in the main energy balance involved in et estimation wu et al 2020 another example is the r evapotranspiration package which includes 17 modules and uses one or several climate variables to estimate et at a single location guo et al 2016 both approaches require users to have their own it infrastructure including servers computers databases and routers with accessary computing storage and networking services furthermore the increasing complexity of model algorithms and the use of large quantities of multi source data require substantial computing power to generate et data efficiently these factors can prevent some users from producing et data because of insufficient capacity or financial resources for it infrastructure such conditions are not conducive to the effective management of water resources cloud computing is one of the most significant development in modern information and communications technology ict it provides a powerful architecture for performing large scale and complex big data computing several applications based on various cloud platforms have been develop in agricultural monitoring and geographic applications for example cropwatch integrates rs data with field data to determine global major crop production indicators more efficiently with cloud computing wu et al 2015a the cloud based web processing service wps framework has been used to improve the performance of the community multiscale air quality cmaq model supporting earth sciences research zhang et al 2019 the open geographic modeling and simulation system opengms aims to support the sharing of models data and computing power as reusable resources for geographic applications in an open web environment yue et al 2015 chen et al 2020 within the opengms research framework a data sharing method was proposed to improve the usability of hydrological data based on the universal data exchange udx model with the aim of addressing the gap between models and data resources wang et al 2018 2020 the trend of migrating huge quantities of geographic and remote sensing data and models to cloud platform for sharing and integration has grown more and more obvious recently three applications include irrisat earth engine evapotranspiration flux eeflux and openet to provide et data based on the google earth engine gee hornbuckle et al 2015 gorelick et al 2017 allen et al 2015 irrisat was developed to access crop water use etc and crop coefficient kc data through four application programming interfaces apis eeflux is based on the surface energy balance model algorithm and generates maps of evapotranspiration at 30 m resolution derived from thermal and reflected landsat imagery data allen et al 2007 although eeflux is a web based gee app the api of the model has not been open assessed to the public so users are unable to calibrate their region of interest roi or integrate for their own process chain openet was developed as an online platform for mapping evapotranspiration from multiple models at the scale of individual fields https openetdata org the apis and custom reporting tools in openet still wait to be provided currently in this study etwatch cloud is developed to provide an et data generation service based on the ict infrastructure in alibaba cloud a set of algorithms for generating regional actual et at multiple resolutions from 30 m to 1 km using key ground based meteorological variables and multiple satellite data from landsat 8 fy 2 and modis images are deployed in etwatch cloud a web interface enables users to organize projects and produce the required et data without coding or with less coding users can also invoke etwatch apis in python language with an associated web ide integrated development environment that enables rapid development prototyping this all in one platform allows users from various backgrounds such as technical users business users and citizen developers to build et monitoring systems without bearing the cost of ict infrastructure and technical barriers the remainder of this paper is organized as follows section 2 introduces the web service framework and describes the design principles section 3 implements the etwatch cloud based service and provides a case study of model publication and execution data publication and visualization and regional et generation section 4 discusses the pros and cons of etwatch cloud and compares its products with other existing et products and the final section provides conclusions and outlines future work 2 etwatch cloud framework 2 1 role analysis etwatch cloud is a web service that publishes algorithms as apis and shares data as shown in fig 1 there are four types of roles in the service model providers data providers administrators and end users and or operators the model providers are responsible for preparing model resources to generate et data with a standard form and interface encapsulation can reduce the complexity and heterogeneity of these models in terms of how they are presented to end users the model providers also prepare the required input and output parameters and model description so that the end users can understand what the model is for and how to use the model each etwatch model resource is deployed on etwatch cloud for publishing as an api in the rest representational state transfer style which is a hybrid derived from several network based architectural styles fielding 2000 the data providers prepare data resources in a standard form for et data generation or analysis in etwatch cloud the rs data files should be processed from raw data files and saved with the same projection and format to keep data compatible with each other to ensure the identification and uniqueness of the files and the automatic operation of the api data files are named according to the same rules users can access the required data file by name to ensure automatic processing which improves the efficiency of data access data resources are stored in cloud storage and published as service which can be accessed or downloaded on the web the administrators of the service manage the apis data users and cloud resources and monitor logs of model runs input output data and performances once the etwatch model resources are deployed on etwatch cloud administrators monitor the status of each api to ensure that the api can be assessed and used properly by the end users administrators also monitor the storage resources occupied by data and apply for storage space in a timely manner when the storage space is insufficient administrators need to process and respond to users orders for data downloads or production immediately to meet the needs of different levels of users the etwatch cloud provides different online interacting ways for message exchanges between users and the model apis etwatch cloud provides a web interface for no coding end users to add or delete apis without coding in their projects and control the order in which the api runs to produce the required et data users with programming ability can program online to invoke the api and control the workflow in the web programming environment provided by etwatch cloud through these interfaces the log information and output of the model api are sent to the end users 2 2 framework of cloud based web service the cloud based web service framework is designed to share et models and generate et data fig 2 a complete frontend backend separation architecture is used to split etwatch cloud into 4 components an etwatch portal a user center an administration console administrator and a cloud it infrastructure platform a clear and concise api is designed for each module to achieve data transfer and interaction between the database and the frontend so that the service has the characteristics of a short development cycle low coupling high scalability clear hierarchy clear logic and easy maintenance all ict infrastructure resources are provided by a public cloud platform used to store transmit and process data an object storage service oss is used to cost effectively store back up and archive large amounts of rs data in the cloud a relational database service provides users with access to a database without the need to set up physical hardware install software or configure the service for optimal performance block storage devices for elastic compute service ecs instances include cloud disks based on a distributed storage architecture and local disks located on the physical machines where the ecs instances are hosted computational services including ecs elastic container service and function compute service allow users to develop and run their applications in powerful computational clouds these services are monitored and managed by the administrator to ensure efficient cloud resource use the administration console management is designed to manage the apis data users and cloud resources and to monitor performance jobs and cloud resource status the api gateway provides complete life cycle management of api including api publishing maintenance monitoring and unpublishing for different users the user manager allocates a different app id and app key to pass the identity and permission authentication process through the api gateway requests exceeding the normal frequency or with invalid parameter types or values are denied by the api gateway if the request is valid the job scheduler applies for resources from the resource manager to perform model operations the resource manager determines whether to use an idle resource or a newly requested resource to complete a job based on resource usage monitored by the resource monitor the etwatch portal offers a direct interactive and responsive way to view and query information about model apis or data each api represents a model that is needed to produce or analyze et data the api description for each model such as operation names parameter names and data types is shown to end users in the portal these apis are opened for users to invoke remotely in their own et generation projects the data metadata describe the temporal and spatial extent format resolution source projection query api and file list of the data the data api provides a rest interface to retrieve metadata for the data users additionally the model and data resources are described in the javascript object notation json string which is not only easy for humans to read and write but also easy for machines to parse and generate bray 2014 a user guide and project demonstrations provide instructions and examples of api and data usage to speed up user learning the user center has four components the personal information component order component project component and collection component users can collect apis or order data and organize these as a project interactively to minimize coding and produce et data for rois the basic information of a roi including the name image zone id coordination range and time zone is provided by the user to define the project all apis and data are added to the workflow interactively using a web based graphical user interface gui to control the operation of model apis and upload the specific data to be used in addition webide is provided for developers to build customized applications to produce et data online with less python code with the input data presented in json data interchange format 2 3 etwatch model encapsulation 2 3 1 overview of the etwatch model etwatch is a set of models and algorithms that characterize the land surface soil vegetation and atmospheric boundary layer to quantify fluxes between soil vegetation and the boundary layer and to determine the individual components of the surface energy balance with multisource and multiscale rs data wu et al 2012 2016b 2020 fig 3 the inputs of etwatch cloud include polar satellite data such as terra aqua moderate resolution imaging spectroradiometer modis data geostationary meteorological satellite data such as fy2 satellite cloud data products resource satellites such as landsat 8 sentinel 2 and gf1 2 atmospheric infrared sounder airs data microwave rs data from radarsat sentinel 1 and gf3 and meteorological data etwatch is based on the surface energy balance equation with 10 modules to estimate the daily net radiation rn surface soil heat flux g0 sensible heat flux and latent heat flux et with the support of water vapor deficit vpd aerodynamic roughness length atmospheric boundary layer abl canopy conductance and surface resistance the final output of etwatch includes the et for a clear day daily et and monthly et wu et al 2020 multisource satellite data are used to develop algorithms calculating the individual components of the surface energy balance and gap filling table 1 lists all input data and intermediate variables in etwatch cloud can be obtained from rs data meteorological observations and related model estimates 2 3 2 model description encapsulation of the etwatch models is intended to conform each module to a standard api a json model description file provides a format for describing model attributes input data output data and runtime demands with the guidance of this format the etwatch models are encapsulated as standardized apis in addition model users can prepare input data and use the model more easily here the net radiation module is used as an example to introduce the method of describing a model in json format as shown in fig 4 the model description file consists of three objects 1 the attribute object consists of the properties of the module including the name brief description provider abstract and links to related papers 2 the io object is designed to explain the module input and output data names and types provides a brief introduction lists examples and states whether the data are required all the attributes of the input data and the output data are stored as an array in the io object 3 the runtime object describes model deployment information including hardware dependency software dependency and library dependency the net radiation module needs a 2 0 ghz cpu main frequency and 4 gb ram and 10 gb storage disks constitute the minimum hardware dependencies 2 3 3 restful model api etwatch consists of many modules all of which are wrapped in interactive data language idl that cannot be invoked in a web environment by end users an encapsulated etwatch model needs to be further published as a web service using the hypertext transfer protocol http and the api follows the rest approach wherein web services are invoked as uniform resource locator url based requests a restful api defines one or more url endpoints with a domain port and path the urls of these submodules are different and include the submodule names to indicate access to a unique resource for example the net radiation submodule api is http etwatch cn api net radiation the basic operations are querying and running of the model and the http requests get and post correspond to these operations the restful model api is statelessness that helps extend the api to millions of concurrent users by deploying it to multiple servers in the cloud platform using the get method the api returns a json encoded response detailing the information of the submodule and api using the post method the api returns a json encoded response result of submodule execution the rest api uses the status code response message to inform the client of the overall result of its request the status code element is a 4 digit integer result code of the attempt to understand and satisfy the request the first digit of the status code defines the class of response status codes fall into the following two categories 2xxx success the action was successfully received understood and accepted 4xxx error the request contains bad syntax or cannot be fulfilled seven standard status codes are defined that can be used to communicate the results of client requests the design of the status code for the rest api is shown in table 2 3 implementation to implement the etwatch cloud service multiple state of the art techniques libraries and standard protocols were used to develop the apis share data and organize the project based on the elastic cloud computing power of alibaba cloud users can access and utilize the full capability of the etwatch cloud service through a web browser with an internet connection in this way the requirement of computing facilities are greatly reduced 3 1 service implementation 3 1 1 etwatch portal the open source toolkit bootstrap and the jquery javascript libraries were used to develop the etwatch portal the portal is a responsive user friendly web application that works across a multitude of web browsers including firefox and chrome without any additional plug in installations fig 5 shows the following pages of the etwatch portal homepage model information data information and user guide end users can easily query and view published models and data in the portal the method request parameters and response information for each api are presented on the api information webpage users can test the api with example requests and view the output of the processing on the test webpage as shown in fig 6 when clicking the post button the request parameters are transmitted as a body by sending a single json encoded data string if the deserialization process works json data will be sent to the api gateway which processes the data and returns a result response the typical response data are also json encoded and include the status code message and result both the request and the response are encoded using the utf 8 character set 3 1 2 user center the open source toolkit bootstrap and jquery libraries were also used to build the user center as shown in fig 7 end users can manage project order collection and personal information through the corresponding webpages via browsers for newly registered users a 5 gb data storage space is allocated to project data storage if the data storage space is not adequate users can submit a request for capacity expansion in the user center the service stores all model results and provides download options through the interface of the user center to allow data analysis or integration with other software users without programming experience can use the web gui to produce et data more easily as shown in fig 8 a dynamic tabbed interface is used as a container for basic information on the api and data for the et generation project when the user activates one of the other tab elements the previously displayed tab panel is hidden the tab panel associated with the activated tab becomes visible and the tab is considered active all apis can be added to the project and their order can be changed by dragging and dropping they can be invoked one by one from left to right and from top to bottom to ensure that outputs from one api are useable as inputs for another 3 1 3 administration console a web gui was built to enable the administrator model provider or data provider to manage their resources on the internet as shown in fig 9 the left sidebar of the administration console gui shows the management entries for all resource types including apis data projects orders users logs and cloud resources the icons in the upper right of the page can be used for rapid access to subpages including the api invoking log lists api lists data lists and project lists the graphs in the middle and bottom of the page show the statistical information about api invoking disk usage and the status of the ecs as shown in fig 10 the backend of the administration console is implemented to create allocate and release the ict computing and storage resources provided by alibaba cloud the idl runtime environment and package for each etwatch module are added to a container image which can become a container when it runs on an elastic container instance eci to isolate dependencies and simplify deployment by default an eci with 1 vcpu and 2 gb memory is used to create an image cache and save time in pulling images from a remote image repository all ict computing and storage resources are monitored and managed to use cloud resources efficiently the api gateway verifies the validity of the request s permissions and parameters if the request is valid and there is no idle eci the resource manager will create a new eci for a new valid request and the resource manager will extend the oss if there is insufficient storage space for new data in addition idle computing resources will be released by the resource manager in time to save costs when new data are stored in the oss the function computing service for data publication will be triggered and then the url of the result will be used for any user request via the api gateway all api information file paths user information and logs are stored in a postgresql relationship database 3 2 model publication and invocation the powerful and flexible django rest framework drf was used to rapidly map the various submodules to api endpoints based on python thirty six apis were implemented to produce the consistent and universal data required for the accurate estimation of et these names and urls can be obtained from table a1 in the appendix we used the net radiation submodule as an example to show all procedures of model publication and invocation as shown in fig 11 once the model resources of etwatch have been deployed the model providers can publish the api of each submodule online after the api is published it is shared on the internet and every user in the portal can use the api provided by etwatch cloud anytime anywhere as shown in fig 12 users can prepare and upload the input data for submodule invocation in the web gui on the webpage users can check the status of a submodule run and obtain output data once the run is finished etwatch apis can also be invoked in all general purpose programming languages such as java python and php thus allowing users to use etwatch apis with their preferred languages webide which includes a file manager python code editor and response viewer is provided in the user center for rapid prototyping online fig 13 3 3 data publication and visualization the open source server program geoserver is used for publishing and sharing spatial data geoserver uses the rest interface s simple http method to create a styled map for users the geoserver manager library was used to programmatically configure geoserver through its rest api as shown in fig 14 once the etwatch data resources have been uploaded the data providers can publish the data online after the data are published the geoserver rest publisher class handles the request and the upper level logic to modify the catalog publish and add styles feature types or coverages every user in the portal can download and use the data provided by etwatch cloud webmap is built based on an open geospatial consortium ogc standard interface web map service wms and provides an interactive and easy webgis tool to visualize geospatial data with the leaflet javascript library a browser client can send a getmap request to obtain rendered map layers and a getfeatureinfo request to query the detailed information on speciﬁed features and the response can then be added to webmap fig 15 users can examine the input rs data and results using the webmap visual interface 3 4 et data generation 3 4 1 study area the hetao irrigation district hid a major grain producing region of china in the inner mongolia autonomous region was chosen to evaluate the performance of etwatch cloud it is located on the north bank of the yellow river 40 13 41 18 n 106 52 109 03 e the average annual temperatures range from 6 to 10 c with maximums in july and minimums in january zhang et al 2021 the annual average precipitation is less than 250 mm and most precipitation events concentrated during june to september most of the area is farmland with lake wuliangsuhai located in the west fig 16 three dominant crops maize sunflower and wheat occupy over 80 of the cropland wen et al 2019 3 4 2 data preparation for this case study rs and ground meteorological data for 2019 were used as inputs for the etwatch model the rs data include the daily modis 1b clear sky products mod09 mod11 mod13 and myd07 and airs data downloaded from the level 1 and atmosphere archive distribution system https ladsweb modaps eosdis nasa gov daily water vapor profile data with different layers were extracted directly from the myd07 and airs products the modis surface temperature and ndvi data at an 1 km resolution were coupled with another rs product the daily amsr e brightness temperature data at a 25 km resolution to estimate the daily soil moisture content fengyun geostationary meteorological satellite fy 2d e f hourly cloud products at a 5 km resolution which were downloaded from the national satellite meteorological center nsmc fengyun satellite data center were also included the ground meteorological data comprising daily data for the maximum air temperature minimum air temperature average air pressure average air humidity average wind speed and sunshine hours were downloaded from the china meteorological data service center http data cma cn two different spatial interpolation methods were used to interpolate these point data into raster data at an 1 km resolution for correspondence with the satellite data the inverse distance squared method was used for the air temperature and air pressure data based on a digital elevation model dem while the thin plate spline method was used for the air humidity wind speed and sunshine hours data franke 1982 lu et al 2008 these data processing steps were integrated into different apis two et products were used to evaluate the et estimates one is based on the operational simplified surface energy balance ssebop model and the second is based on the mod16 data which relies upon the penman monteith equation senay 2018 running et al 2017 3 4 3 et generation to validate the service against its objectives the etwatch cloud project center was used to apply the etwatch apis to the rs and meteorological data process for the hid using 22 apis et data at an 1 km resolution for each day of each month in 2019 at the basin scale and et data at a 30 m resolution for each month in 2019 over the entire hid were produced fig 17 is a screenshot from the etwatch project center of the apis used for data processing and et estimation in the hid case study after all apis have been executed the results can be viewed online in webmap or the data can be obtained via wcs for display and further analysis with local gis software such as qgis fig 18 was generated with qgis 3 16 mapping tools using the 30 m resolution et data provided by etwatch cloud high values are located in the wuliangsuhai lake area while low values occur in the urban areas in wulateqianqi and linhe similar high and low values occur in fig 19 which shows statistical summaries of the et data by land use for the hid in 2019 4 discussion etwatch cloud an et data generation service was developed using the cloud computing power of alibaba cloud to address the challenging resource requirements in computing storage and maintenance posed by et data generation based on multisource rs data the framework of a cloud based web service was applied for generating and sharing et data via the etwatch model we designed a json model description file to ensure the consistency of model encapsulation and designed the api to follow the restful approach to provide flexible api invocation for users computing storage programming and algorithm development are already incorporated into the service this approach is especially useful for users who need to generate their own et data but lack financial resources or professional background table 3 shows the advantages of the etwatch cloud service compared with existing et generation systems there are two types of applications in etwatch cloud enabling users to choose the one with which they are familiar the et project is defined on the webpage by the user which is especially useful for beginners with little coding experience the rest style apis provided in etwatch cloud simplify calling procedures into a consistent process creating a quick and autonomous way to create projects with little coding only basic information is needed to define the project such as the extent of the study area and the time period to be monitored in such a project the user can call apis from the service as private project apis calibrate apis with their own ground data upload rs data for the project area and conduct data processing to generate et data users manage data themselves in this project mode they can download the intermediate and final data products and delete the data and the project they can also delete the data but keep the project for future use alternatively users can use the python runtime environment provided by the service or other programming language environments on their local computers to call apis and organize processing workflows the advantages of etwatch cloud over the standalone version of etwatch include high scalability and easy interoperability because cloud computing and the storage capacity are scalable users with different purposes and capacities can use the scalable etwatch cloud to monitor et at anywhere and at any time without bearing insufficient computing or storage capacities if an et algorithm is updated users can access the new api without reinstalling the software insufficient storage can be addressed through a simple space expansion request rather than a hardware upgrade this permits users to focus on choosing model data combinations and analyzing the results for water resource management the etwatch cloud is a refurbished version of etwatch standalone both can generate et data with the same accuracy and quality fig 20 shows monthly et data for 2019 from etwatch cloud the ssebop model and mod16a2 data the etwatch data show low values in winter and spring and substantially higher values in late summer and early autumn since most cultivated land in the hid is planted with single season crops the monthly et peaks during the peak growth period for these crops in july and august however the current prototypical implementation of etwatch cloud still has some limitations the output from the etwatch cloud service is formatted as raster files tif format and published as a wms layer so it can be easily used as input for other rs or gis approaches for further analyses however more convenient and flexible et data analysis functions remains to be implemented in the service to ensure the quality of et data it is desirable to calibrate etwatch model parameters using ground observation data in the study region calibration should be carried out on key variables such as the surface temperature ground temperature difference shortwave and longwave radiation water heat flux and sensible heat flux using ground measurement data xiong et al 2011 however ground data sources often contain anomalies and may be provided in different formats this makes the integration of data from ground observations into etwatch cloud a complex task an efficient extraction transformation and loading etl tool together with calibration techniques needs to be incorporated into the service in the future 5 conclusion this paper introduces etwatch cloud service which is a model collection migrated from a local pc to the cloud platform for et generation using readily available rs and standard ground based meteorological observations etwatch cloud provides a general solution for the publication execution and management of modules in the cloud apis for the calculation and analysis of et are encapsulated as functional modules and open assessed in a restful style to provide web services through this method each model and its algorithm are shared for model reuse potentially this cloud based method could be used for migrating other rs models to the cloud for data generation services a prototype service was implemented with open source software libraries and standard protocols to publish the model and data based on alibaba cloud et data generation for the hid was used as a case study of the use of etwatch cloud the et estimates for the hid were demonstrated and suggest that the service could also be used in other regions or for other data to model workflows future developments of etwatch cloud include an etl api to support data input for local calibration and enhanced online visualization and analysis capabilities for data output software and data availability software etwatch cloud developers fangming wu bingfang wu contact information wubf aircas ac cn hardware required general purpose computer software required internet browser firefox chrome etc program language java python server and client script technologies javascript json etc availability all apis and data can be accessed via http etwatch cn declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the meteorological and radiation datasets were collected free of charge from the china national meteorological bureau the geostationary meteorological satellite data fy 2d e were collected free of charge from the national satellite meteorological center nsmc fengyun satellite data center modis datasets can be downloaded from the united states nasa websites the authors thank the anonymous reviewers for their critical comments and constructive suggestions which signiﬁcantly improved the quality of the paper funding for this research was provided by the national natural science foundation of china grant numbers 41991232 41701403 and 41601463 the key research program of frontier science cas grant no qyzdy ssw dqc014 and the qinghai science and technology plan grant no 2019 sf 155 appendix a table a1 apis in etwatch cloud table a1 name class url maximum temperature interpolation ground weather http etwatch cn api meteo tmax minimum temperature interpolation ground weather http etwatch cn api meteo tmin average temperature interpolation ground weather http etwatch cn api meteo tavg sunshine hour interpolation ground weather http etwatch cn api meteo sunt humidity interpolation ground weather http etwatch cn api rs api meteo humd winv interpolation ground weather http etwatch cn api meteo winv atmospheric pressure interpolation ground weather http etwatch cn api meteo pre rainfall interpolation ground weather http etwatch cn api meteo rain height of atmospheric boundary layer boundary layer http etwatch cn api airs wind speed in the atmospheric boundary layer boundary layer http etwatch cn api airs wind daily net shortwave radiation surface parameter http etwatch cn api net radiation rswn daily net longwave radiation surface parameters http www etwatch cn api net radiation rlwn solar radiation surface parameters http etwatch cn api net radiation rswd ndvi gapfilling surface parameters http etwatch cn api ndvi rebuild integrated aerodynamic roughness surface parameters http etwatch cn api ad roughness rs gapfilling surface parameter http etwatch cn api rs rebuild albedo gapfilling surface parameter http etwatch cn api albedo rebuild sunshine duration fy surface parameters http etwatch cn api fy sunt api rs smooth surface parameters http etwatch cn api smooth rs surface resistance pm inversion surface parameters http etwatch cn api surface resistance pm inversion sunshine duration msg surface parameters http etwatch cn api msg sunt et fuse sd surface flux http etwatch cn api et fuse sd et fuse downscaling surface flux http etwatch cn api et fuse downscaling instantaneous sensible heat flux parameterization method surface flux http etwatch cn api oshfp instantaneous soil heat flux in satellite transit surface flux http etwatch cn api soil hf instantaneous sensible heat flux sebs surface flux http etwatch cn api instantaneous sensible heat flux sebs et fuse starfm surface flux http etwatch cn api et fuse starfm net radiation surface flux http etwatch cn api net radiation instantaneous netradiation surface flux http etwatch cn api instantaneous netradiation clear day et surface flux http etwatch cn api clear day et sebs daily et0 surface flux http etwatch cn api dayly calculate et0 daily soil heat flux surface flux http etwatch cn api soil hf daily daily et calculate surface flux http etwatch cn api dayly calculate cumulative calculation time and space statistics http etwatch cn api composite space statistics time and space statistics http etwatch cn api statistic 
25717,pooled testing also known as group testing where diagnostic tests are performed on pooled samples has broad applications in the surveillance of diseases in animals and humans an increasingly common use case is molecular xenomonitoring mx where surveillance of vector borne diseases is conducted by capturing and testing large numbers of vectors e g mosquitoes the r package pooltestr was developed to meet the needs of increasingly large and complex molecular xenomonitoring surveys but can be applied to analyse any data involving pooled testing pooltestr includes simple and flexible tools to estimate prevalence and fit fixed and mixed effect generalised linear models for pooled data in frequentist and bayesian frameworks mixed effect models allow users to account for the hierarchical sampling designs that are often employed in surveys including mx we demonstrate the utility of pooltestr by applying it to a large synthetic dataset that emulates a mx survey with a hierarchical sampling design keywords r group testing molecular xenomonitoring open source software pooled testing mixed effect regression 1 introduction pooled testing also known as group testing where diagnostic tests are performed on pooled samples has broad applications for the detection of traits defects or diseases with low prevalence recently pooled testing has been used to efficiently and rapidly test for sars cov 2019 sunjaya and sunjaya 2020 but this approach has long been used for surveillance of other infectious diseases e g to detect pathogens in food animals arnold et al 2011 and conduct surveillance of vector borne diseases pilotte et al 2017 rodríguez pérez et al 2006 the world health organization has for decades been running global elimination programs to reduce the impact of many vector borne diseases including lymphatic filariasis lf world health organisation 2019 a key challenge for programs of this scale is to optimise the efficiency and accuracy of surveillance especially as the disease becomes rarer or more localised over time molecular xenomonitoring mx the surveillance of pathogens or molecular markers in vector populations is emerging as an alternative or adjunct to human based surveillance of vector borne diseases such as lf and typically employs pooled testing of mosquitoes for filarial dna lau et al 2016 rao et al 2016 schmaedick et al 2014 subramanian et al 2020 in the right setting mx with pooled testing could potentially be very sensitive avoid or reduce the need to test humans and provide insights to assist in vector control strategies large scale mx surveys such as those needed to support decision making for elimination programs involve capturing and testing large numbers of vectors it is not always feasible to test every vector captured individually so vectors are routinely pooled to reduce cost and improve efficiency if using a sufficiently sensitive and specific test each pool returns a positive result if any individual in the pool is positive and a negative result otherwise the presence absence of infected vectors provides a proxy measure of ongoing transmission and could potentially help identify geographic areas where further surveillance or interventions may be required the prevalence of infection amongst vectors provides useful information for decision makers when assessing and communicating risk and can be used to prioritise the allocation of resources observe the effects of interventions and identify spatial and temporal trends however estimating prevalence from pooled samples requires specialised statistical methods chen and swallow 1990 farrington 1992 hepworth 2005 walter et al 1980 many mx surveys employ a hierarchical sampling structure for instance to conduct a mx study in a particular region one may select a number of representative villages in the region followed by a number of representative sites within each village while hierarchical sampling designs like this provide an effective means of collecting a representative sample of vectors across the study area they call for specialised analytical methods to accurately estimate infection prevalence analytical methods that do not account for hierarchical sampling structures will tend to underestimate the uncertainty in prevalence when applied to data collected using these sampling methods birkner et al 2013 there are a number of extant software packages for working with pooled testing models e g poolscreen katholi and unnasch 2006 the excel add in pooledinfrate biggerstaff 2009 and the r packages pooling van domelen 2020 bingroup zhang et al 2018 and its successor bingroup2 hitt et al 2020 poolscreen is a stand alone application that has been used in many mx projects and provides a graphical user interface for estimating prevalence in both frequentists and bayesian paradigms however poolscreen is impractical for very large and complex surveys where one may wish to estimate prevalence for many subsets of the data e g estimating prevalence by vector species by country region village by sampling year etc the microsoft excel add in pooledinfrate is able to automate some of these steps however neither poolscreen nor pooledinfrate have functionality for regression modelling the r package pooling is designed primarily for case control studies with pooled assays and not applicable to mx studies the r packages bingroup and its successor bingroup2 provide tools for fixed effect regression modelling with pooled data though only in a frequentist framework however none of these software have functionality to account for hierarchical sampling frames which are common in large scale mx surveys and therefore will tend to underestimate the uncertainty in prevalence estimates associated with the sampling design the lack of a tool that is tailored for large scale hierarchical mx surveys limits the efficiency of data analyses and the amount of information to be gleaned from these studies particularly in resource poor settings with limited technical capacity to fill this gap we developed pooltestr a package for the r language r core team 2020 which provides a user friendly and extensible framework for estimating prevalence with pooled data and performing fixed and mixed effect regression modelling for both hierarchical and non hierarchical surveys all analyses can be conducted in frequentist or bayesian frameworks though our package can be applied to generic pooled testing datasets we demonstrate its use through examples based on simulated mx data with known prevalence 2 pooled testing and the pooled binomial glmm suppose we have a molecular marker of infection in a population with unknown prevalence p we can estimate the prevalence by taking a random sample of binary outcomes showing whether or not the molecular marker is present in a sampled unit e g testing each mosquito caught if the molecular marker has a low prevalence in the population and the unit cost of each test is much more than the unit cost of procuring samples e g trapping mosquitoes pooled testing may be a more cost effective means of estimating prevalence for simplicity assume that the test can detect the marker with 100 sensitivity and specificity we also assume that the marker is independent across each individual in the sample and therefore also in the pools and that the total number of samples is much smaller than the population while larger pools may dilute the marker of interest and thus affect the sensitivity of the test for the purposes of our models we will assume that the dilution is insignificant for the range of pool sizes used under these assumptions for a pool of size s the probability of a positive result is φ s p 1 1 p s in some cases a fixed pool size is used however we consider the general case where there may be a mixture of pool sizes suppose we have a set of observations where we sample from a population where the marker of interest has prevalence p pool them into pools of size s i and observe the indicator outcomes y i with 1 indicating a positive test result and 0 indicating a negative test result given the pool sizes s the outcomes y is a sufficient statistic for the prevalence p and follows a pooled bernoulli distribution with probability mass function given by poolbern y s p i poolbern y i s i p i bern y i ϕ s i p 1 p i s i 1 y i i 1 1 p s i y i an equivalent formulation can be made in terms of the pooled binomial distribution described in o neill and mclure 2021 which reduces the sufficient statistic of interest to the counts of cases and counts of positive outcomes in each pool size other than in trivial situations where all the positive pools have the same size the maximum likelihood estimate mle for p does not have a closed form expression and is computed numerically under some weak regularity conditions the standardised mle converges in distribution to the standard normal distribution allowing for wald confidence intervals other confidence intervals for p have been proposed hepworth 2005 in our package we implement a generalised linear mixed model glmm using the pooled bernoulli distribution building on mixed modelling frameworks provided by the packages brms and lme4 we model the outcomes y of tests on pools of size s as p y s p i poolbern y i s i p i p f 1 η η x β j z j u j k s k x k u j n 0 σ j where f is a link function β are fixed population effect coefficients u j are random group effect coefficients associated with grouping factor j x is a design matrix for the fixed population effects z j are design matrices for each grouping factor and σ j are unknown covariance matrices in the simplest case σ j are diagonal matrices however in general we allow for random group effects to be correlated between covariates further details of possible structuring of these random group effects can be found in bürkner 2018 the s k x k are smooth functions of covariates x k either splines or gaussian processes with a squared exponential kernel our package accommodates two link functions for the prevalence parameter the logistic and complementary log log cll p log log 1 p functions the logistic link function produces more readily interpretable coefficients and is the default in our package however the cll link function leads to a simpler mathematical exposition since cll φ s p log s cll p allowing the separatation of the pool size from the prevalence parameter i e reducing the model to a glmm with a bernoulli response and offset of log s p y η i bern y i cll 1 η i η log s x β j z j u j k s k x k u j n 0 σ j our package provides functions for fitting these models in either frequentist or bayesian framework in the bayesian case the functions in our package have flexibility for the user to input any differentiable prior for any of the unknown model parameters or use default uninformative priors our model extends the generalised linear model discussed in farrington 1992 our model with a logistic link function is also similar to a bird nesting model used in shaffer 2004 the regression models implemented in the r package bingroup2 hitt et al 2020 and those presented in mcmahan et al 2017 and joyner et al 2020 are similar to our model but their models allow items within a single pool to have different covariates however the models in mcmahan et al 2017 and joyner et al 2020 omit the smooth terms s k x k and bingroup2 can only model fixed effects the covariates in these models can represent any characteristic of pools for a mx study of a mosquito borne disease this may include sample collection time and location or attributes of the site where the sample is collected e g interventions in place at the site altitude vegetation index distance to housing it may also include any attributes of the mosquitoes shared by the entire pool e g mosquito species in a survey design where trapped mosquitoes are sorted by species before being pooled mixed effect terms can be used to account for intra site variation not captured by other covariates in studies with hierarchical sampling frames 3 the pooltestr package the pooltestr package has been designed to be a simple user friendly and extensible way to analyse test results from pooled samples the package has four primary functions for the estimation of prevalence poolprev hierpoolprev poolreg and poolregbayes poolprev produces unadjusted estimates of prevalence of a marker based on the outcome of tests on pooled samples optionally stratifying the dataset by one or more user specified covariates hierpoolprev is like poolprev but allows users to adjust prevalence estimates for hierarchical structure in sampling frames poolreg and poolregbayes provide flexible and extensible frameworks to fit mixed or fixed effect regression models in either a frequentist or bayesian framework allowing users to identify variables associated with higher prevalence or fit predictive models while accounting for hierarchical sampling frames table 1 provides an overview of the differences between the four main functions the following sections provide more details of these functions boxes a and b provide example code and figs 1 and 2 compare the outputs of these functions when applied to a synthetic dataset 3 1 poolprev poolprev was designed to produce comparable results to the popular stand alone application poolscreen katholi and unnasch 2006 for familiarity to existing users of the software and to enable direct comparison of results from the many studies that used poolscreen e g helmy et al 2004 rodríguez pérez et al 2006 schmaedick et al 2014 subramanian et al 2020 tewari et al 2004 stratifying a dataset and calculating prevalence for each subgroup of the data using poolscreen requires many manual steps to import data run analyses and export results using our function poolprev this same task can be achieved in a few lines of r code with a simple syntax given a dataset containing the number of samples per pool and the test results for each pool poolprev returns bayesian and maximum likelihood estimates of the prevalence together with uncertainty intervals efficient bayesian inference is performed with hamiltonian markov chain monte carlo using the stan programming language stan development team 2020b and the r packages rstan stan development team 2020a and rstantools gabry et al 2020 users can specify their prior belief for the prevalence from the beta distribution or use the default uninformative jeffrey s prior users can also optionally specify the prior probability that the marker of interest is entirely absent from the population in which case poolprev also returns the probability of absence given the data as we assume the test performed on the pooled samples does not produce false positive or negatives the probability of absence is always zero if any of the pools test positive in most cases the credible interval cri of level γ e g 95 is the 1 γ 2 e g 2 5 and 1 γ 2 e g 97 5 quantiles of the posterior distribution however if all tests are positive the upper bound of the cri is 1 and the lower bound is the 1 γ e g 5 quantile of the posterior similarly if all tests are negative the lower bound of the cri is 0 and the upper bound is the γ e g 95 quantile of the posterior a confidence interval of level γ is calculated using the likelihood ratio method i e a wilk s confidence interval as with the bayesian cri the lower or upper bound of the confidence intervals are zero or one when all pools are negative or positive respectively all estimates can be optionally stratified by variables e g vector species or location by providing the name s of the columns in the dataset containing the variable s estimation of prevalence proceed independently for each subgroup of the data defined by the variable s box a demonstrates the use of poolprev on a synthetic dataset described in section 4 3 2 hierpoolprev heirpoolprev is designed to account for the hierarchical sampling structures that are common in mx studies it assumes that samples were taken from a number of sites across the study area and these sites can be nested within one or more hierarchical levels e g sites within villages villages within regions regions within provinces etc heirpoolprev estimates prevalence by fitting an intercept only hierarchical generalised linear mixed model with a logistic link function the syntax and outputs are very similar to poolprev there is only one additional argument hierarchy which requires the user to list the variables that encode the hierarchical structure of the sampling frame e g the names of columns containing site ids village ids etc the output provides the bayesian posterior mean and cri for the prevalence but unlike poolprev does not provide frequentist outputs i e maximum likelihood estimates or likelihood ratio confidence intervals as with poolprev users can specify their prior belief for the prevalence and specify variables that stratify the dataset into subgroups if subgroups of the data are specified estimation of prevalence and random effect variances proceed independently for each subgroup box a demonstrates the use of hierpoolprev on a synthetic dataset described in section 4 3 3 regression poolreg and poolreg bayes our package provides tools for mixed effect regression models in both frequentist and bayesian frameworks poolreg fits a frequentist mixed or fixed effect generalised linear model that adjusts for the sizes of pools building on glm from the stats package r core team 2020 for fixed effect models and the glmer function from the lme4 package bates et al 2015 for mixed effect models for a model with only fixed effects the output is an s3 object of class glmfit while the output for a model with random effects is an s4 object of class glmermod which supports that same methods e g summary predict plot confint anova as any other object returned by the glm or glmer functions poolregbayes provides functionality to perform the same analyses in a bayesian framework and returns a brmsfit object by building on these existing statistical packages pooltestr leverages the extensive suite of diagnostics tools available for working with models fitted with these functions and uses paradigms that will be familiar to existing users of r these frameworks allow for a very broad range of linear models e g polynomial regression spline regression gaussian process models in addition pooltestr includes the function getprevalence which provides a convenient way to extract estimates of prevalence from regression models fitted with poolreg or poolregbayes the function getprevalence is in many cases able to detect whether a model includes adjustments for hierarchical random group effect terms and automatically estimate prevalence at every level of the sampling hierarchy box b applies poolreg and poolregbayes to the same synthetic dataset used to demonstrate poolprev estimating the trend of decline in prevalence over time box c shows the model summaries for these regression models 4 comparison of methods on a synthetic dataset pooltestr provides a number of approaches to estimate prevalence frequentist or bayesian stratifying or adjusting for covariates adjusting for or ignoring hierarchical sampling frame table 1 we compare the approaches with a simulation study of 500 synthetic datasets each synthetic dataset emulated a large mx survey with mosquitoes sampled across three years with a realistic hierarchical sampling design e g schmaedick et al 2014 subramanian et al 2020 three regions ten randomly chosen villages per region and ten randomly chosen sites per village for an average of approximately 180 000 mosquitoes per dataset split across an average of 6 770 pools we assume as is common that the primary purpose of the surveys is to inform interventions or assessments that will be applied at the region level i e in sampled and unsampled villages therefore the primary outcome of interest is the overall prevalence in each region over time however region level estimates of prevalence will need to be adjusted for the hierarchical sampling frames used at the village and site level each synthetic dataset was generated by simulating samples taken from across three regions a b and c in which the vectors had a low 0 5 medium 2 and high 4 prevalence of the marker of interest we then emulated a multi level cluster survey with ten villages chosen randomly from each region and traps placed at ten random sites in each village we sampled from the same locations once a year over three years 0 1 and 2 prevalence was not uniform within each region or over time at baseline year 0 prevalence varied between villages within each region standard deviation on the log odds scale 0 5 and prevalence varied between sites within each village standard deviation on the log odds scale 0 5 consequently though the prevalence was different for each site two sites within the same village were likely to have a more similar prevalence than two sites in different villages or two sites in different regions on average the prevalence was declining over time odds ratio of 0 8 per year or equivalently a coefficient for year of 0 22 on the log odds scale however the rate of change in prevalence varied between villages standard deviation on log odds scale 0 2 consequently two sites in different villages with similar prevalence at baseline typically had different prevalence by the third year and prevalence even went up in some villages we modelled the total number of mosquitoes trapped at each site and each year as independent negative binomial random variables mean 200 dispersion 5 of vectors though a wide range of pool sizes may lead to better estimates of prevalence gu et al 2004 we simulated a simple and practical pooling strategy similar to those used in practice e g schmaedick et al 2014 subramanian et al 2020 each year the catches at each site were pooled into groups of 25 with an additional pool for any remainder e g a catch of 53 vectors would be pooled into two pools of 25 and one pool of three every pool was tested once for the marker of interest using a test with perfect sensitivity and specificity the code for generating these synthetic datasets and the first of these datasets accessible as exampledata are distributed with the pooltestr package the example dataset has been used to illustrate the package in boxes a b and c box a demonstrates the use of the functions poolprev and hierpoolprev by estimating prevalence stratified by year and region with or without adjustments for the hierarchical sampling frame box b demonstrates the functions poolreg and poolregbayes and fits logistic type regression models with year and region as covariates with and without adjustment for sampling hierarchy in frequentist and bayesian frameworks box c shows the model summaries for the simple frequentist fixed effect regression model for region and year and a more complex bayesian model with fixed population effects for region and year and random group effects for village and site while both correctly identified that prevalence declined over the three sampling years i e negative coefficient for year and that baseline prevalence was lowest in region a i e positive coefficients for regions b and c the mixed effect model also estimated the degree of variation between villages and sites resulting in differing point estimates figs 1 and 2 compare these different approaches for estimating prevalence on these simulated datasets since our example had adequate sample size the estimates using a frequentist framework were very similar to estimates in a bayesian framework using non informative priors compare frequentist and bayesian outputs of poolprev in fig 1 as true prevalence in the synthetic datasets was moderately variable between sites and villages methods that did not account for hierarchical sampling frame resulted in confidence credible intervals that included the true value for 50 of synthetic datasets meanwhile methods that accounted for hierarchical sampling frames resulted in 95 intervals that included the true value for approximately 95 of simulated datasets fig 1a this difference can be seen whether stratifying other covariates the approach in box a or adjusting for them the approach in box b stratifying the data by year and region produced estimates with wider confidence credible intervals than in a regression framework compare in fig 1b poolprev to poolregbayes without adjustment for hierarchy or the results of hierpoolprev to poolregbayes with adjustments for hierarchy this effect is particularly pronounced where prevalence is low e g region a consequently without adjustments for hierarchical sampling using a regression framework further reduced the fraction of the confidence credible intervals that contain the true value fig 1a however the regression model with adjustments for hierarchical sampling frame had the narrowest intervals that included the true value in approximately 95 of simulated datasets the maximum likelihood estimates of prevalence from pooled samples are known to have positive bias hepworth and biggerstaff 2017 and other regression models of pool tested data are known to produce biased estimates bilder and tebbs 2009 consequently it is not surprising that the estimates of prevalence in our simulation study also exhibited positive bias fig 2 the normalised bias bias divided by true value increased with decreasing prevalence however the bias was minimised in nearly all cases by adjusting for sampling frame hierarchy and adjusting for covariates rather than stratifying in bayesian analyses bias was further reduced by using the posterior median rather than the posterior mean as the point estimate moreover while normalised bias of the posterior median in regression analyses adjusting for sampling frame hierarchy was consistently 4 the bias of point estimates from models without these adjustments was sensitive to true prevalence ranging from approximately 0 normalised bias for region c in year 0 true prevalence 4 0 mean estimated prevalence 4 0 to approximately 32 normalised bias for region a in year 2 true prevalence 0 32 mean estimated prevalence 0 42 6 discussion pooltestr is a cross platform user friendly flexible and extensible r package for estimating prevalence and regression modelling with tests on pooled samples pooltestr offers substantial advantages over existing software for pooled testing such as poolscreen katholi and unnasch 2006 and pooledinfrate biggerstaff 2009 especially for hierarchical sampling designs such as those used in mx surveys while each analysis in poolscreen requires many manual steps to import data and export results pooltestr integrates with diverse ecosystem of r packages simplifying the importation of data visualisation and exportation of results to a number of common formats e g csv xls existing r packages with some functionality to work with pool tested data include binomsamsize höhle 2017 pegrouptesting zhang and li 2016 bingroup zhang et al 2018 bingroup2 hitt et al 2020 and pooling van domelen 2020 binomsamsize can only accommodate equal sized pools and neither pegrouptesting nor binomsamsize has functionality for regression modelling pooling bingroup and its successor bingroup2 have functionality for simple regression models but cannot fit mixed effect models none of these software or r packages are able to account for hierarchical sampling frames however other authors have published mixed effect regression models for pooled data sometimes accompanied by software e g joyner et al 2020 mcmahan et al 2017 however these software have included closed source platform specific components or otherwise have not been designed for ease of use for non programming specialists when conducting mx surveys collecting a simple random sample of vectors across a large area is operationally infeasible many mx studies will therefore involve a hierarchical sampling frame involving representative sample sites distributed across the study area if the study area and the distance between traps are smaller than the movement range of the vector being studied it may be fair to assume that all traps are sampling from the same population and that there is no variation in prevalence between trap sites in such cases the method implemented in poolscreen pooledinfrate and the poolprev function in our package are appropriate for estimating prevalence however when aggregating data to estimate prevalence in a study area substantially larger than the typical movement range of vectors these methods which do not account for heterogeneity between sample sites may have unreasonably narrow confidence intervals that often fail to contain the true value birkner et al 2013 instead the function hierpoolprev or a hierarchical mixed effect regression model using poolreg or poolregbayes should be preferred in these situations while accounting for hierarchical sampling frames will increase the width of confidence intervals for prevalence estimates failing to do so may result in confidence intervals which frequently fail to include the true prevalence value molecular xenomonitoring surveys utilising pooled testing are often paired with human surveys utilising un pooled testing pilotte et al 2017 though regression modelling is commonly used in the human components of these surveys e g subramanian et al 2020 regression modelling with pooled mx data has been hampered by the lack of suitable software the only method for looking at differences by groups in poolscreen is to manually stratify the data and re run the analysis and the regression models in bingroup2 cannot account for hierarchical sampling frames the regression functions in the pooltestr package fill this gap allowing users to identify variables associated with infection e g region survey year vector species environmental covariates test the statistical significance of these associations and produce predictive models moreover where appropriate regression models can produce more precise estimates narrower confidence intervals compared to simple stratification regression models could be used for predictive prevalence mapping however further development is required to allow for models with spatial correlation to be easily accessible to users one limitation of the class of regression models implemented in our package is that covariates must be equal for every individual in a pool for instance to use our package to model possible differences between vector species in an mx study each pool must include only vectors of a single species the r packages bingroup and bingroup2 can handle cases where covariates my differ between individuals in a single pool but only for a restricted set of fixed effect regression models however study designs where covariates are the same for all members of the pool allow for better estimates of prevalence and regression coefficients so should be preferred where practical bilder and tebbs 2009 there are currently no tools that readily allow for the comparison or synthesis of both the human and mx components of surveys e g model predictions of prevalence in humans based on prevalence in vectors this functionality may be added in future releases of pooltestr maximum likelihood estimates of prevalence based on pool tested data are known to positively biased bilder and tebbs 2009 hepworth and biggerstaff 2017 a number of bias corrected estimates have been proposed hepworth and biggerstaff 2017 and these may be incorporated in future releases of the package bias is not typically used to assess estimators in a bayesian context where point estimates depend not only on the model but also on the choice of prior however posterior mean prevalence when using the default uninformative priors in our package will likely be positively biased in many settings this bias can be alleviated by using the posterior median instead of the posterior mean and or an informative prior appropriate to the study setting as with all models estimates made with pooltestr will be unreliable if the implicit assumptions about the test characteristics sampling frame population or covariates are substantially violated all the models in our package currently assume that the tests applied to each pool have perfect sensitivity and specificity while tests may be imperfectly sensitive or specific even when testing individual samples test sensitivity and specificity may also decline with pool size statistical methods that estimate test sensitivity or specificity from the data test for the existence of diluting effect in larger pools or otherwise adjust for imperfect test specificity and sensitivity have been proposed tu et al 1995 and may be incorporated in future versions of pooltestr all of our models also assume that vector catch numbers are either fixed by the sampling design or random and independent of the prevalence of the marker of interest and any modelled covariates one common survey design is to set out traps for a fixed period of time and test all vectors trapped at each site the relationship between vector density transmission rate and prevalence is dependent on complex host agent and environment relationships and so there may be correlation between catch numbers and disease prevalence at a given sampling site however we anticipate that this kind of correlation if not accounted for may bias estimates if sample sizes are not fixed ahead of time while a predetermined sample size for each site could avoid this bias it may require sampling to be prolonged at some sites and vectors to be discarded at others the best way to detect and adjust for bias related to sampling designs that do not use a predetermined sample size remains an open question another key consideration in mx studies is the appropriate sample size and pooling strategy katholi and unnasch 2006 when designing a sampling strategy using pooled samples there is a trade off between cost and precision using fewer larger pools makes it cheaper and faster to conduct laboratory tests but greater numbers of smaller pools improves the power of the data and the precision of estimates for a fixed number of pools distributing the specimens into a number of fixed size pools is likely to result in poorer estimates than using pools of various sizes gu et al 2004 however there are currently no practical rules or tools for determining an optimal or near optimal strategies for sampling or pooling a tool that given a sampling design testing constraints and catch size determines the optimal number of pools and the optimal distribution of samples across these pools would further improve the cost effectiveness of pooled mx surveys and may be incorporated in future updates of pooltestr 7 conclusion pooltestr is a software package born out of the need for a simple flexible and freely available tool to analyse large and complex datasets to estimate infection prevalence from pooled samples pooltestr allows users to conduct the most common analyses required for mx whilst to being able to adjust for hierarchical sampling design and conduct a broad range of regression analyses mx is increasingly being used as a surveillance method around the world and we hope that pooltestr can assist researchers and program managers in disease surveillance in a range of control settings and other contexts using pooled data software availability name of software pooltestr type of software add on package for r first available 2020 programming languages r stan license gpl 3 code repository cran https cran r project org web packages pooltestr index html github https github com angusmclure pooltestr developers angus mclure contact address angus mclure anu edu au declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements angus mclure was supported by an australian research council discovery project grant dp180100246 colleen lau was supported by the national health and medical research council fellowships 1109035 1193826 this work received financial support from the coalition for operational research on neglected tropical diseases cor ntd grant number opp1053230 which is funded at the task force for global health primarily by the bill melinda gates foundation by the uk aid from the british government and by the united states agency for international development through its neglected tropical diseases program the funders had no role in study design data collection and analysis decision to publish or preparation of the manuscript 
25717,pooled testing also known as group testing where diagnostic tests are performed on pooled samples has broad applications in the surveillance of diseases in animals and humans an increasingly common use case is molecular xenomonitoring mx where surveillance of vector borne diseases is conducted by capturing and testing large numbers of vectors e g mosquitoes the r package pooltestr was developed to meet the needs of increasingly large and complex molecular xenomonitoring surveys but can be applied to analyse any data involving pooled testing pooltestr includes simple and flexible tools to estimate prevalence and fit fixed and mixed effect generalised linear models for pooled data in frequentist and bayesian frameworks mixed effect models allow users to account for the hierarchical sampling designs that are often employed in surveys including mx we demonstrate the utility of pooltestr by applying it to a large synthetic dataset that emulates a mx survey with a hierarchical sampling design keywords r group testing molecular xenomonitoring open source software pooled testing mixed effect regression 1 introduction pooled testing also known as group testing where diagnostic tests are performed on pooled samples has broad applications for the detection of traits defects or diseases with low prevalence recently pooled testing has been used to efficiently and rapidly test for sars cov 2019 sunjaya and sunjaya 2020 but this approach has long been used for surveillance of other infectious diseases e g to detect pathogens in food animals arnold et al 2011 and conduct surveillance of vector borne diseases pilotte et al 2017 rodríguez pérez et al 2006 the world health organization has for decades been running global elimination programs to reduce the impact of many vector borne diseases including lymphatic filariasis lf world health organisation 2019 a key challenge for programs of this scale is to optimise the efficiency and accuracy of surveillance especially as the disease becomes rarer or more localised over time molecular xenomonitoring mx the surveillance of pathogens or molecular markers in vector populations is emerging as an alternative or adjunct to human based surveillance of vector borne diseases such as lf and typically employs pooled testing of mosquitoes for filarial dna lau et al 2016 rao et al 2016 schmaedick et al 2014 subramanian et al 2020 in the right setting mx with pooled testing could potentially be very sensitive avoid or reduce the need to test humans and provide insights to assist in vector control strategies large scale mx surveys such as those needed to support decision making for elimination programs involve capturing and testing large numbers of vectors it is not always feasible to test every vector captured individually so vectors are routinely pooled to reduce cost and improve efficiency if using a sufficiently sensitive and specific test each pool returns a positive result if any individual in the pool is positive and a negative result otherwise the presence absence of infected vectors provides a proxy measure of ongoing transmission and could potentially help identify geographic areas where further surveillance or interventions may be required the prevalence of infection amongst vectors provides useful information for decision makers when assessing and communicating risk and can be used to prioritise the allocation of resources observe the effects of interventions and identify spatial and temporal trends however estimating prevalence from pooled samples requires specialised statistical methods chen and swallow 1990 farrington 1992 hepworth 2005 walter et al 1980 many mx surveys employ a hierarchical sampling structure for instance to conduct a mx study in a particular region one may select a number of representative villages in the region followed by a number of representative sites within each village while hierarchical sampling designs like this provide an effective means of collecting a representative sample of vectors across the study area they call for specialised analytical methods to accurately estimate infection prevalence analytical methods that do not account for hierarchical sampling structures will tend to underestimate the uncertainty in prevalence when applied to data collected using these sampling methods birkner et al 2013 there are a number of extant software packages for working with pooled testing models e g poolscreen katholi and unnasch 2006 the excel add in pooledinfrate biggerstaff 2009 and the r packages pooling van domelen 2020 bingroup zhang et al 2018 and its successor bingroup2 hitt et al 2020 poolscreen is a stand alone application that has been used in many mx projects and provides a graphical user interface for estimating prevalence in both frequentists and bayesian paradigms however poolscreen is impractical for very large and complex surveys where one may wish to estimate prevalence for many subsets of the data e g estimating prevalence by vector species by country region village by sampling year etc the microsoft excel add in pooledinfrate is able to automate some of these steps however neither poolscreen nor pooledinfrate have functionality for regression modelling the r package pooling is designed primarily for case control studies with pooled assays and not applicable to mx studies the r packages bingroup and its successor bingroup2 provide tools for fixed effect regression modelling with pooled data though only in a frequentist framework however none of these software have functionality to account for hierarchical sampling frames which are common in large scale mx surveys and therefore will tend to underestimate the uncertainty in prevalence estimates associated with the sampling design the lack of a tool that is tailored for large scale hierarchical mx surveys limits the efficiency of data analyses and the amount of information to be gleaned from these studies particularly in resource poor settings with limited technical capacity to fill this gap we developed pooltestr a package for the r language r core team 2020 which provides a user friendly and extensible framework for estimating prevalence with pooled data and performing fixed and mixed effect regression modelling for both hierarchical and non hierarchical surveys all analyses can be conducted in frequentist or bayesian frameworks though our package can be applied to generic pooled testing datasets we demonstrate its use through examples based on simulated mx data with known prevalence 2 pooled testing and the pooled binomial glmm suppose we have a molecular marker of infection in a population with unknown prevalence p we can estimate the prevalence by taking a random sample of binary outcomes showing whether or not the molecular marker is present in a sampled unit e g testing each mosquito caught if the molecular marker has a low prevalence in the population and the unit cost of each test is much more than the unit cost of procuring samples e g trapping mosquitoes pooled testing may be a more cost effective means of estimating prevalence for simplicity assume that the test can detect the marker with 100 sensitivity and specificity we also assume that the marker is independent across each individual in the sample and therefore also in the pools and that the total number of samples is much smaller than the population while larger pools may dilute the marker of interest and thus affect the sensitivity of the test for the purposes of our models we will assume that the dilution is insignificant for the range of pool sizes used under these assumptions for a pool of size s the probability of a positive result is φ s p 1 1 p s in some cases a fixed pool size is used however we consider the general case where there may be a mixture of pool sizes suppose we have a set of observations where we sample from a population where the marker of interest has prevalence p pool them into pools of size s i and observe the indicator outcomes y i with 1 indicating a positive test result and 0 indicating a negative test result given the pool sizes s the outcomes y is a sufficient statistic for the prevalence p and follows a pooled bernoulli distribution with probability mass function given by poolbern y s p i poolbern y i s i p i bern y i ϕ s i p 1 p i s i 1 y i i 1 1 p s i y i an equivalent formulation can be made in terms of the pooled binomial distribution described in o neill and mclure 2021 which reduces the sufficient statistic of interest to the counts of cases and counts of positive outcomes in each pool size other than in trivial situations where all the positive pools have the same size the maximum likelihood estimate mle for p does not have a closed form expression and is computed numerically under some weak regularity conditions the standardised mle converges in distribution to the standard normal distribution allowing for wald confidence intervals other confidence intervals for p have been proposed hepworth 2005 in our package we implement a generalised linear mixed model glmm using the pooled bernoulli distribution building on mixed modelling frameworks provided by the packages brms and lme4 we model the outcomes y of tests on pools of size s as p y s p i poolbern y i s i p i p f 1 η η x β j z j u j k s k x k u j n 0 σ j where f is a link function β are fixed population effect coefficients u j are random group effect coefficients associated with grouping factor j x is a design matrix for the fixed population effects z j are design matrices for each grouping factor and σ j are unknown covariance matrices in the simplest case σ j are diagonal matrices however in general we allow for random group effects to be correlated between covariates further details of possible structuring of these random group effects can be found in bürkner 2018 the s k x k are smooth functions of covariates x k either splines or gaussian processes with a squared exponential kernel our package accommodates two link functions for the prevalence parameter the logistic and complementary log log cll p log log 1 p functions the logistic link function produces more readily interpretable coefficients and is the default in our package however the cll link function leads to a simpler mathematical exposition since cll φ s p log s cll p allowing the separatation of the pool size from the prevalence parameter i e reducing the model to a glmm with a bernoulli response and offset of log s p y η i bern y i cll 1 η i η log s x β j z j u j k s k x k u j n 0 σ j our package provides functions for fitting these models in either frequentist or bayesian framework in the bayesian case the functions in our package have flexibility for the user to input any differentiable prior for any of the unknown model parameters or use default uninformative priors our model extends the generalised linear model discussed in farrington 1992 our model with a logistic link function is also similar to a bird nesting model used in shaffer 2004 the regression models implemented in the r package bingroup2 hitt et al 2020 and those presented in mcmahan et al 2017 and joyner et al 2020 are similar to our model but their models allow items within a single pool to have different covariates however the models in mcmahan et al 2017 and joyner et al 2020 omit the smooth terms s k x k and bingroup2 can only model fixed effects the covariates in these models can represent any characteristic of pools for a mx study of a mosquito borne disease this may include sample collection time and location or attributes of the site where the sample is collected e g interventions in place at the site altitude vegetation index distance to housing it may also include any attributes of the mosquitoes shared by the entire pool e g mosquito species in a survey design where trapped mosquitoes are sorted by species before being pooled mixed effect terms can be used to account for intra site variation not captured by other covariates in studies with hierarchical sampling frames 3 the pooltestr package the pooltestr package has been designed to be a simple user friendly and extensible way to analyse test results from pooled samples the package has four primary functions for the estimation of prevalence poolprev hierpoolprev poolreg and poolregbayes poolprev produces unadjusted estimates of prevalence of a marker based on the outcome of tests on pooled samples optionally stratifying the dataset by one or more user specified covariates hierpoolprev is like poolprev but allows users to adjust prevalence estimates for hierarchical structure in sampling frames poolreg and poolregbayes provide flexible and extensible frameworks to fit mixed or fixed effect regression models in either a frequentist or bayesian framework allowing users to identify variables associated with higher prevalence or fit predictive models while accounting for hierarchical sampling frames table 1 provides an overview of the differences between the four main functions the following sections provide more details of these functions boxes a and b provide example code and figs 1 and 2 compare the outputs of these functions when applied to a synthetic dataset 3 1 poolprev poolprev was designed to produce comparable results to the popular stand alone application poolscreen katholi and unnasch 2006 for familiarity to existing users of the software and to enable direct comparison of results from the many studies that used poolscreen e g helmy et al 2004 rodríguez pérez et al 2006 schmaedick et al 2014 subramanian et al 2020 tewari et al 2004 stratifying a dataset and calculating prevalence for each subgroup of the data using poolscreen requires many manual steps to import data run analyses and export results using our function poolprev this same task can be achieved in a few lines of r code with a simple syntax given a dataset containing the number of samples per pool and the test results for each pool poolprev returns bayesian and maximum likelihood estimates of the prevalence together with uncertainty intervals efficient bayesian inference is performed with hamiltonian markov chain monte carlo using the stan programming language stan development team 2020b and the r packages rstan stan development team 2020a and rstantools gabry et al 2020 users can specify their prior belief for the prevalence from the beta distribution or use the default uninformative jeffrey s prior users can also optionally specify the prior probability that the marker of interest is entirely absent from the population in which case poolprev also returns the probability of absence given the data as we assume the test performed on the pooled samples does not produce false positive or negatives the probability of absence is always zero if any of the pools test positive in most cases the credible interval cri of level γ e g 95 is the 1 γ 2 e g 2 5 and 1 γ 2 e g 97 5 quantiles of the posterior distribution however if all tests are positive the upper bound of the cri is 1 and the lower bound is the 1 γ e g 5 quantile of the posterior similarly if all tests are negative the lower bound of the cri is 0 and the upper bound is the γ e g 95 quantile of the posterior a confidence interval of level γ is calculated using the likelihood ratio method i e a wilk s confidence interval as with the bayesian cri the lower or upper bound of the confidence intervals are zero or one when all pools are negative or positive respectively all estimates can be optionally stratified by variables e g vector species or location by providing the name s of the columns in the dataset containing the variable s estimation of prevalence proceed independently for each subgroup of the data defined by the variable s box a demonstrates the use of poolprev on a synthetic dataset described in section 4 3 2 hierpoolprev heirpoolprev is designed to account for the hierarchical sampling structures that are common in mx studies it assumes that samples were taken from a number of sites across the study area and these sites can be nested within one or more hierarchical levels e g sites within villages villages within regions regions within provinces etc heirpoolprev estimates prevalence by fitting an intercept only hierarchical generalised linear mixed model with a logistic link function the syntax and outputs are very similar to poolprev there is only one additional argument hierarchy which requires the user to list the variables that encode the hierarchical structure of the sampling frame e g the names of columns containing site ids village ids etc the output provides the bayesian posterior mean and cri for the prevalence but unlike poolprev does not provide frequentist outputs i e maximum likelihood estimates or likelihood ratio confidence intervals as with poolprev users can specify their prior belief for the prevalence and specify variables that stratify the dataset into subgroups if subgroups of the data are specified estimation of prevalence and random effect variances proceed independently for each subgroup box a demonstrates the use of hierpoolprev on a synthetic dataset described in section 4 3 3 regression poolreg and poolreg bayes our package provides tools for mixed effect regression models in both frequentist and bayesian frameworks poolreg fits a frequentist mixed or fixed effect generalised linear model that adjusts for the sizes of pools building on glm from the stats package r core team 2020 for fixed effect models and the glmer function from the lme4 package bates et al 2015 for mixed effect models for a model with only fixed effects the output is an s3 object of class glmfit while the output for a model with random effects is an s4 object of class glmermod which supports that same methods e g summary predict plot confint anova as any other object returned by the glm or glmer functions poolregbayes provides functionality to perform the same analyses in a bayesian framework and returns a brmsfit object by building on these existing statistical packages pooltestr leverages the extensive suite of diagnostics tools available for working with models fitted with these functions and uses paradigms that will be familiar to existing users of r these frameworks allow for a very broad range of linear models e g polynomial regression spline regression gaussian process models in addition pooltestr includes the function getprevalence which provides a convenient way to extract estimates of prevalence from regression models fitted with poolreg or poolregbayes the function getprevalence is in many cases able to detect whether a model includes adjustments for hierarchical random group effect terms and automatically estimate prevalence at every level of the sampling hierarchy box b applies poolreg and poolregbayes to the same synthetic dataset used to demonstrate poolprev estimating the trend of decline in prevalence over time box c shows the model summaries for these regression models 4 comparison of methods on a synthetic dataset pooltestr provides a number of approaches to estimate prevalence frequentist or bayesian stratifying or adjusting for covariates adjusting for or ignoring hierarchical sampling frame table 1 we compare the approaches with a simulation study of 500 synthetic datasets each synthetic dataset emulated a large mx survey with mosquitoes sampled across three years with a realistic hierarchical sampling design e g schmaedick et al 2014 subramanian et al 2020 three regions ten randomly chosen villages per region and ten randomly chosen sites per village for an average of approximately 180 000 mosquitoes per dataset split across an average of 6 770 pools we assume as is common that the primary purpose of the surveys is to inform interventions or assessments that will be applied at the region level i e in sampled and unsampled villages therefore the primary outcome of interest is the overall prevalence in each region over time however region level estimates of prevalence will need to be adjusted for the hierarchical sampling frames used at the village and site level each synthetic dataset was generated by simulating samples taken from across three regions a b and c in which the vectors had a low 0 5 medium 2 and high 4 prevalence of the marker of interest we then emulated a multi level cluster survey with ten villages chosen randomly from each region and traps placed at ten random sites in each village we sampled from the same locations once a year over three years 0 1 and 2 prevalence was not uniform within each region or over time at baseline year 0 prevalence varied between villages within each region standard deviation on the log odds scale 0 5 and prevalence varied between sites within each village standard deviation on the log odds scale 0 5 consequently though the prevalence was different for each site two sites within the same village were likely to have a more similar prevalence than two sites in different villages or two sites in different regions on average the prevalence was declining over time odds ratio of 0 8 per year or equivalently a coefficient for year of 0 22 on the log odds scale however the rate of change in prevalence varied between villages standard deviation on log odds scale 0 2 consequently two sites in different villages with similar prevalence at baseline typically had different prevalence by the third year and prevalence even went up in some villages we modelled the total number of mosquitoes trapped at each site and each year as independent negative binomial random variables mean 200 dispersion 5 of vectors though a wide range of pool sizes may lead to better estimates of prevalence gu et al 2004 we simulated a simple and practical pooling strategy similar to those used in practice e g schmaedick et al 2014 subramanian et al 2020 each year the catches at each site were pooled into groups of 25 with an additional pool for any remainder e g a catch of 53 vectors would be pooled into two pools of 25 and one pool of three every pool was tested once for the marker of interest using a test with perfect sensitivity and specificity the code for generating these synthetic datasets and the first of these datasets accessible as exampledata are distributed with the pooltestr package the example dataset has been used to illustrate the package in boxes a b and c box a demonstrates the use of the functions poolprev and hierpoolprev by estimating prevalence stratified by year and region with or without adjustments for the hierarchical sampling frame box b demonstrates the functions poolreg and poolregbayes and fits logistic type regression models with year and region as covariates with and without adjustment for sampling hierarchy in frequentist and bayesian frameworks box c shows the model summaries for the simple frequentist fixed effect regression model for region and year and a more complex bayesian model with fixed population effects for region and year and random group effects for village and site while both correctly identified that prevalence declined over the three sampling years i e negative coefficient for year and that baseline prevalence was lowest in region a i e positive coefficients for regions b and c the mixed effect model also estimated the degree of variation between villages and sites resulting in differing point estimates figs 1 and 2 compare these different approaches for estimating prevalence on these simulated datasets since our example had adequate sample size the estimates using a frequentist framework were very similar to estimates in a bayesian framework using non informative priors compare frequentist and bayesian outputs of poolprev in fig 1 as true prevalence in the synthetic datasets was moderately variable between sites and villages methods that did not account for hierarchical sampling frame resulted in confidence credible intervals that included the true value for 50 of synthetic datasets meanwhile methods that accounted for hierarchical sampling frames resulted in 95 intervals that included the true value for approximately 95 of simulated datasets fig 1a this difference can be seen whether stratifying other covariates the approach in box a or adjusting for them the approach in box b stratifying the data by year and region produced estimates with wider confidence credible intervals than in a regression framework compare in fig 1b poolprev to poolregbayes without adjustment for hierarchy or the results of hierpoolprev to poolregbayes with adjustments for hierarchy this effect is particularly pronounced where prevalence is low e g region a consequently without adjustments for hierarchical sampling using a regression framework further reduced the fraction of the confidence credible intervals that contain the true value fig 1a however the regression model with adjustments for hierarchical sampling frame had the narrowest intervals that included the true value in approximately 95 of simulated datasets the maximum likelihood estimates of prevalence from pooled samples are known to have positive bias hepworth and biggerstaff 2017 and other regression models of pool tested data are known to produce biased estimates bilder and tebbs 2009 consequently it is not surprising that the estimates of prevalence in our simulation study also exhibited positive bias fig 2 the normalised bias bias divided by true value increased with decreasing prevalence however the bias was minimised in nearly all cases by adjusting for sampling frame hierarchy and adjusting for covariates rather than stratifying in bayesian analyses bias was further reduced by using the posterior median rather than the posterior mean as the point estimate moreover while normalised bias of the posterior median in regression analyses adjusting for sampling frame hierarchy was consistently 4 the bias of point estimates from models without these adjustments was sensitive to true prevalence ranging from approximately 0 normalised bias for region c in year 0 true prevalence 4 0 mean estimated prevalence 4 0 to approximately 32 normalised bias for region a in year 2 true prevalence 0 32 mean estimated prevalence 0 42 6 discussion pooltestr is a cross platform user friendly flexible and extensible r package for estimating prevalence and regression modelling with tests on pooled samples pooltestr offers substantial advantages over existing software for pooled testing such as poolscreen katholi and unnasch 2006 and pooledinfrate biggerstaff 2009 especially for hierarchical sampling designs such as those used in mx surveys while each analysis in poolscreen requires many manual steps to import data and export results pooltestr integrates with diverse ecosystem of r packages simplifying the importation of data visualisation and exportation of results to a number of common formats e g csv xls existing r packages with some functionality to work with pool tested data include binomsamsize höhle 2017 pegrouptesting zhang and li 2016 bingroup zhang et al 2018 bingroup2 hitt et al 2020 and pooling van domelen 2020 binomsamsize can only accommodate equal sized pools and neither pegrouptesting nor binomsamsize has functionality for regression modelling pooling bingroup and its successor bingroup2 have functionality for simple regression models but cannot fit mixed effect models none of these software or r packages are able to account for hierarchical sampling frames however other authors have published mixed effect regression models for pooled data sometimes accompanied by software e g joyner et al 2020 mcmahan et al 2017 however these software have included closed source platform specific components or otherwise have not been designed for ease of use for non programming specialists when conducting mx surveys collecting a simple random sample of vectors across a large area is operationally infeasible many mx studies will therefore involve a hierarchical sampling frame involving representative sample sites distributed across the study area if the study area and the distance between traps are smaller than the movement range of the vector being studied it may be fair to assume that all traps are sampling from the same population and that there is no variation in prevalence between trap sites in such cases the method implemented in poolscreen pooledinfrate and the poolprev function in our package are appropriate for estimating prevalence however when aggregating data to estimate prevalence in a study area substantially larger than the typical movement range of vectors these methods which do not account for heterogeneity between sample sites may have unreasonably narrow confidence intervals that often fail to contain the true value birkner et al 2013 instead the function hierpoolprev or a hierarchical mixed effect regression model using poolreg or poolregbayes should be preferred in these situations while accounting for hierarchical sampling frames will increase the width of confidence intervals for prevalence estimates failing to do so may result in confidence intervals which frequently fail to include the true prevalence value molecular xenomonitoring surveys utilising pooled testing are often paired with human surveys utilising un pooled testing pilotte et al 2017 though regression modelling is commonly used in the human components of these surveys e g subramanian et al 2020 regression modelling with pooled mx data has been hampered by the lack of suitable software the only method for looking at differences by groups in poolscreen is to manually stratify the data and re run the analysis and the regression models in bingroup2 cannot account for hierarchical sampling frames the regression functions in the pooltestr package fill this gap allowing users to identify variables associated with infection e g region survey year vector species environmental covariates test the statistical significance of these associations and produce predictive models moreover where appropriate regression models can produce more precise estimates narrower confidence intervals compared to simple stratification regression models could be used for predictive prevalence mapping however further development is required to allow for models with spatial correlation to be easily accessible to users one limitation of the class of regression models implemented in our package is that covariates must be equal for every individual in a pool for instance to use our package to model possible differences between vector species in an mx study each pool must include only vectors of a single species the r packages bingroup and bingroup2 can handle cases where covariates my differ between individuals in a single pool but only for a restricted set of fixed effect regression models however study designs where covariates are the same for all members of the pool allow for better estimates of prevalence and regression coefficients so should be preferred where practical bilder and tebbs 2009 there are currently no tools that readily allow for the comparison or synthesis of both the human and mx components of surveys e g model predictions of prevalence in humans based on prevalence in vectors this functionality may be added in future releases of pooltestr maximum likelihood estimates of prevalence based on pool tested data are known to positively biased bilder and tebbs 2009 hepworth and biggerstaff 2017 a number of bias corrected estimates have been proposed hepworth and biggerstaff 2017 and these may be incorporated in future releases of the package bias is not typically used to assess estimators in a bayesian context where point estimates depend not only on the model but also on the choice of prior however posterior mean prevalence when using the default uninformative priors in our package will likely be positively biased in many settings this bias can be alleviated by using the posterior median instead of the posterior mean and or an informative prior appropriate to the study setting as with all models estimates made with pooltestr will be unreliable if the implicit assumptions about the test characteristics sampling frame population or covariates are substantially violated all the models in our package currently assume that the tests applied to each pool have perfect sensitivity and specificity while tests may be imperfectly sensitive or specific even when testing individual samples test sensitivity and specificity may also decline with pool size statistical methods that estimate test sensitivity or specificity from the data test for the existence of diluting effect in larger pools or otherwise adjust for imperfect test specificity and sensitivity have been proposed tu et al 1995 and may be incorporated in future versions of pooltestr all of our models also assume that vector catch numbers are either fixed by the sampling design or random and independent of the prevalence of the marker of interest and any modelled covariates one common survey design is to set out traps for a fixed period of time and test all vectors trapped at each site the relationship between vector density transmission rate and prevalence is dependent on complex host agent and environment relationships and so there may be correlation between catch numbers and disease prevalence at a given sampling site however we anticipate that this kind of correlation if not accounted for may bias estimates if sample sizes are not fixed ahead of time while a predetermined sample size for each site could avoid this bias it may require sampling to be prolonged at some sites and vectors to be discarded at others the best way to detect and adjust for bias related to sampling designs that do not use a predetermined sample size remains an open question another key consideration in mx studies is the appropriate sample size and pooling strategy katholi and unnasch 2006 when designing a sampling strategy using pooled samples there is a trade off between cost and precision using fewer larger pools makes it cheaper and faster to conduct laboratory tests but greater numbers of smaller pools improves the power of the data and the precision of estimates for a fixed number of pools distributing the specimens into a number of fixed size pools is likely to result in poorer estimates than using pools of various sizes gu et al 2004 however there are currently no practical rules or tools for determining an optimal or near optimal strategies for sampling or pooling a tool that given a sampling design testing constraints and catch size determines the optimal number of pools and the optimal distribution of samples across these pools would further improve the cost effectiveness of pooled mx surveys and may be incorporated in future updates of pooltestr 7 conclusion pooltestr is a software package born out of the need for a simple flexible and freely available tool to analyse large and complex datasets to estimate infection prevalence from pooled samples pooltestr allows users to conduct the most common analyses required for mx whilst to being able to adjust for hierarchical sampling design and conduct a broad range of regression analyses mx is increasingly being used as a surveillance method around the world and we hope that pooltestr can assist researchers and program managers in disease surveillance in a range of control settings and other contexts using pooled data software availability name of software pooltestr type of software add on package for r first available 2020 programming languages r stan license gpl 3 code repository cran https cran r project org web packages pooltestr index html github https github com angusmclure pooltestr developers angus mclure contact address angus mclure anu edu au declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements angus mclure was supported by an australian research council discovery project grant dp180100246 colleen lau was supported by the national health and medical research council fellowships 1109035 1193826 this work received financial support from the coalition for operational research on neglected tropical diseases cor ntd grant number opp1053230 which is funded at the task force for global health primarily by the bill melinda gates foundation by the uk aid from the british government and by the united states agency for international development through its neglected tropical diseases program the funders had no role in study design data collection and analysis decision to publish or preparation of the manuscript 
25718,retrofitting watersheds with sensing and control technologies promises to enable autonomous water systems which control themselves in real time to improve water quality to realize this vision there is a need to improve the degree of fidelity in the underlying representation of pollutant processes this paper presents an open source python package stormreactor which integrates the stormwater management model s water balance engine with a new water quality module stormreactor includes a variety of predefined pollutant generation and treatment processes while allowing users to implement additional processes on their own to demonstrate the range of possible water quality methodologies that can be modeled we simulated suspended solids and nitrates in a real and anonymized stormwater network to illustrate stormreactor s real time control capabilities a control strategy was implemented to maximize denitrification case study results indicate a controlled asset can achieve the same pollutant improvements as an uncontrolled asset in a quarter of the spatial footprint graphical abstract image 1 keywords water quality water balance model integration real time control stormwater 1 introduction a reliable and cost effective method for treating stormwater pollutants is real time control sun et al 2020 garofalo et al 2017 shishegar et al 2019 retrofitting stormwater assets with sensing and control technologies enables watersheds to adapt in real time to individual storms or pollutant loads persaud et al 2019 zhang et al 2018 these smart stormwater assets can be coordinated at the watershed scale to maximize pollutant treatment kerkez et al 2016 eggimann et al 2017 berglund et al 2020 in essence this supports the analogy of transforming our natural or urbanized watersheds into distributed treatment plants by combining knowledge from stormwater systems and process control mullapudi et al 2017 to realize this vision we must first be able to model both pollutant transformations and the impact of real time control actions on water quality at the watershed scale wong et al 2006 garcía et al 2015 berglund et al 2020 this can be achieved with integrated environmental modeling integrated environmental modeling dynamically links distinctly separate models during run time to better understand the environmental system s response to human and natural stressors laniak et al 2013 sutherland et al 2017 recently integrated environmental modeling has been used to combine climate and streamflow data with a water budget model and a dynamic groundwater model shuler and mariner 2020 simulate the hydrological effects of land use changes on karst systems bittner et al 2020 link precipitation forecasts with real time hydrological and hydraulic modeling for urban flood forecasting brendel et al 2020 couple hydrodynamic and closed nutrient cycle ecological models to predict dissolved oxygen do in surface waters suarez et al 2019 and create a catchment scale water quality modeling and monitoring framework wang et al 2019 integrated environmental modeling of stormwater requires the coupling of water quantity and quality models this necessitates simulating a number of underlying processes including precipitation runoff climatic variables land use flow and pollutant routing and pollutant transformations deletic and maksimovic 1998 egodawatta et al 2007 mccarthy et al 2007 while a number of existing models are able to represent these individual components effectively at a granular scale an all in one modeling package is still lacking given the complexity of stormwater specifically its nonlinear dynamics overton and meadows 2013 garcía et al 2015 most existing models understandably seem to draw a line between flow and quality obropta and kardos 2007 bach et al 2014 there has been a stated need to integrate these two types of environmental models mullapudi et al 2017 wang et al 2019 tuomela et al 2018 to that end the specific contributions of this paper are 1 stormreactor a new water quality package implemented as an extension of the popular us environmental protection agency s epa stormwater management model swmm which provides an open source python programming interface for simulating complex pollutant generation treatment and real time control processes 2 an evaluation of the package s ability to model complex pollutant transformations and real time control actions using two case studies these contributions provide researchers and practitioners more flexibility in simulating water quality processes and pollutant based real time control at site and watershed scales 2 state of stormwater quality modeling existing stormwater models can be broadly grouped into two categories water quantity models and water quality models most stormwater models primarily focus on coupled hydrologic hydraulic processes with limited capabilities for modeling water quality e g mike urban dr3m storm music swmm obropta and kardos 2007 bach et al 2014 however some stormwater models do focus on high resolution water quality processes these finite element models e g hydrus cwmi fitovert simulate complex pollutant transformations within individual sites rizzo et al 2014 pálfy and langergraber 2014 giraldi et al 2010 unfortunately scaling from site to watershed scale becomes very difficult due to the input data requirements and the difficulty of parameterization the chasm between these two types of stormwater models forces a trade off between either comprehensively modeling water quality at the site scale or less comprehensively modeling watershed scale processes to avoid this tradeoff researchers have modified existing stormwater models like swmm to expand their pollutant modeling capabilities swmm widely used in the us stormwater community is an open source urban stormwater model rossman 2015 swmm s water quality model provides users the ability to introduce pollutants and pollutant treatment while also routing and calculating mass balance for each pollutant rossman and huber 2016 swmm tss modified swmm to simulate total suspended solids tss transport accumulation and erosion in sewers and retention tanks sun et al 2017 as its name implies this modification is only for tss baek et al 2020 modified swmm s water quality module for low impact development lid to include straining decay and decomposition of pollutants however this modification does not work for stormwater storage assets or links talbot et al modified pcswmm a licensed version of swmm to simulate sediment loading due to soil erosion talbot et al 2016 this modification is not open source and thus not open for exploration or expansion by the community all of these packages are very useful for specific modeling tasks however they do not offer general water quality modeling solutions although these packages provide additional functionality swmm has many remaining pollutant modeling limitations that must be addressed the water quality module is limited by the range of treatment measures that can be modeled wong et al 2006 specifically limited nutrient treatment capabilities inside storage nodes e g basins wetlands troitsky et al 2019 niazi et al 2017 swmm cannot simulate pollutant treatment inside links e g conduits channels or pollutant generation processes e g resuspension erosion inside any stormwater asset pollutant treatment cannot be turned on or off based on site conditions or other parameters requiring treatment to run for the entire simulation all of these constraints limit a user s ability to model complex pollutant transformations necessitating a more generalizable and scalable approach aside from water quality limitations many stormwater quality models have limited or no ability to simulate real time control real time control is made possible through the installation of sensors which can monitor the flow and quality parameters and actuators which can control the flow of water kerkez et al 2016 schu et al 2004 to realize the goal of autonomous watersheds we must be able to model real time control strategies garcía et al 2015 vanrolleghem et al 2005 one open source and popular real time control package is pyswmm a python wrapper for the swmm computational engine pyswmm queries stormwater states directly from swmm which is used to apply control actions by setting the control parameters for valves gates and pumps in real time mcdonnell et al 2020 however pyswmm presently only enables real time control decisions to be made based on water quantity parameters e g flow head depth volume therefore there is a need for a comprehensive package that can both simulate water quality processes and real time control 3 new package for modeling stormwater quality a watershed scale pollutant transformation model is comprised of the water quantity and water quality representations of the stormwater network fig 1 these representations provide insight into which sub components are already well addressed by existing models and which others should be expanded or developed the water quantity representation focuses on the conveyance of water through the network of links e g channels conduits and nodes e g detention basins retention basins wetlands the hydrologic and hydraulic processes which underpin the water quantity sub component are well established in stormwater models obropta and kardos 2007 bach et al 2014 the water quality representation includes the pollutant generation and treatment processes that occur in stormwater assets e g wetland as a continuously tank reactor cstr retention basin as a settling tank often this sub component is significantly simplified e g first order decay models instead of drawing from water treatment process literature mullapudi et al 2017 leaving room for expansion guided by the state of these sub components in current stormwater models we developed stormreactor a new water quality python package coupled with swmm the choice to build a module for swmm was based on a number of factors first swmm has a verified hydraulic solver which is critically important for accurately modeling flow and pollutant routing rossman 2015 in addition building upon swmm s popularity engages a large user base ensuring it is accessible to more people finally swmm is open source which enables modification of its code and the use of popular python wrappers such as pyswmm section 3 1 and section 3 2 detail the development and structure of stormreactor stormreactor was created by i modifying the swmm and pyswmm source code to allow water quality states to be modified and ii building an additional python library to interface water quality modeling with these popular tools 3 1 swmm and pyswmm to address the limitations of swmm s water quality module we modified swmm s c source code 1 1 https github com openwateranalytics stormwater management model by introducing getters and setters to allow for real time access of the model states during simulation table 1 a getter enables a user to access a variable while a setter enables a user to change the value of a variable we then modified pyswmm s python source code 2 2 https github com openwateranalytics pyswmm to gain access to swmm water quality states and to provide the convenience of modeling in a popular scripting language while pyswmm already allowed for the interaction with swmm s quantity states e g flows depths it needed to be expanded to support interaction with water quality states table 1 now a user can interact with a pollutant s concentration in any node or link during any routing time step in this way swmm is used to transport pollutants using its reliable hydraulic and routing engine pyswmm is used to support python interaction with swmm s c engine and stormreactor adds supplementary support for water quality modeling fig 2 3 2 stormreactor stormreactor enables users to model water quality while fully leveraging the well validated swmm functionality for flow and routing stormreactor provides a high level programming interface that removes the user from the complex interactions between swmm pyswmm and stormreactor and only requires a few python command statements to model pollutant transformations users have the ability to select a water treatment method in any stormwater asset and specify the routing time steps across which to carry out simulations to promote uptake by an existing community of modelers a user can select any of the already existing swmm treatment functions outlined in the swmm reference manual volume iii water quality table 2 rossman and huber 2016 users can also select from a library of our new water quality methods including reactor models and stream processes such as erosion table 2 more importantly users can implement their own custom pollutant models using a python interface section 3 2 3 these custom pollutant models can be built upon states of the various water quantity and quality parameters in swmm e g flow depth volume concentration as well as interact with other python packages e g scipy readers are directed to zenodo for stormreactor s source code and documentation mason and mullapudi 2021 3 2 1 user experience stormreactor can be installed using pip 3 3 https pypi org project stormreactor to use stormreactor first import both stormreactor and pyswmm fig 3 next define a configuration dictionary stating at which nodes and links water quality will be modeled as well as the desired pollutants water quality methods and the parameters required for each method then create an instance of the water quality class by calling waterquality which takes two arguments config the configuration dictionary and sim a pyswmm simulation object which encapsulates all the swmm simulation functionality e g start stop simulation get set attributes finally call the class instance method updatewqstate to run the desired water quality method once initialized stormreactor executes the simulation loop first stormreactor queries the necessary water quantity and quality parameters e g water depth pollutant concentration for specific stormwater assets at the current routing time step next it uses the queried parameters to compute and set the new pollutant concentration using a predefined or custom water quality method if a water quality computation requires a time parameter the length of the routing time step is used if real time control is being modeled selected water quality and or quantity data are used to calculate the control decisions swmm then enacts the real time control decisions and routes the pollutant s and flows through the network this process can be repeated at any or every routing time step the simulation loop terminates after the number of desired routing time steps or the swmm model is complete 3 2 2 architecture stormreactor s architecture follows an object oriented programming paradigm this matches already popular python conventions and maximizes potential for user customization stormreactor begins by defining a class waterquality the class has an init method which takes three parameters self an instance of the class sim the pyswmm simulation object and config the configuration dictionary when an instance of the class is created it automatically calls the init method which does the following 1 initializes the asset flag 2 calls the pyswmm method sim start time to get the start time of the simulation 3 initializes the variable last timestep to aid in calculating the length of the routing time step 4 initializes the ordinary differential equation ode solver for the cstr water quality method and 5 defines the callable names of the water quality instance methods the waterquality class also defines two important methods updatewqstate and updatewqstate cstr which update the pollutant concentrations during a swmm simulation for non cstr and cstr methods respectively the class also has a collection of python instance methods which specify the various treatment and generation processes that can be performed on a pollutant table 2 time steps are handled by stormreactor by relying on swmm many of the treatment methods do not require a time parameter e g event mean concentration constant removal k c method stormreactor handles these methods just as they would be handled in native swmm these methods grab the current pollutant concentration and then calculates and sets the new concentration at the end of the current routing time step for the methods that do require a time parameter e g n th order reaction kinetics erosion gravity settling stormreactor computes the routing time step length dt using the same method as swmm to calculate dt stormreactor calls the pyswmm function sim current time to get the current simulation time subtracts the previous routing time step saved in the variable last timestep and then converts it to seconds in this way stormreactor is dependent on swmm to get dt once dt is calculated and the current concentration is queried the new concentration is computed and set at the end of the current routing time step this new concentration then becomes the concentration at the beginning of the next routing time step routing time steps are usually on the order of seconds whereas water quality processes may take much longer therefore users must also parameterize water quality coefficients on the order of seconds 3 2 3 implementing custom pollutant models to implement a new custom pollutant model users can either 1 add their new class instance method to stormreactor s code base or 2 build their model directly in their python script using the appropriate getters and setters table 1 we recommend the first option if code is to be more seamlessly shared with others to add a new method to the code base a user must 1 define the new method using the following convention newmethod self id pollutantid parameters flag non public python instance methods should always start with an underscore the new method requires five parameters self an instance of the class id the node or link name in swmm pollutantid the pollutant index in swmm parameters the water quality method parameters and flag used to determine if the method is for a link or node 2 provide a text description of the method including the water quality method parameters and their required units be sure to note if the method is for links nodes or both 3 write the pollutant transformation code for the new method a define any variables that may be needed for the pollutant transformation calculations b query swmm variables that are necessary for the computation e g pollutant concentration water depth current simulation time using pyswmm getters c compute the pollutant transformation concentration d set the new pollutant concentration using pyswmm setters 4 define the callable name in the init method 5 write unit tests for the new method and add them to test links py and or test nodes py in the tests folder once the new method is added to stormreactor s code base the user can then use it following the steps outlined in section 3 2 1 4 water quality case studies the study area is a 7 8 km2 urban separated stormwater network fig 1 located in michigan which suffers from erosion problems due to high flashy flows in this network stormwater first flows through a detention basin into a long channel a detention basin has its outlet at the bottom of the basin so between storms it is usually dry the long channel then flows into a retention basin a retention basin has its outlet at a higher point so it tends to retain a permanent pool of water if the height of the water in the retention basin is less than a specified threshold water flows directly into a constructed treatment wetland otherwise water bypasses the wetland and overflows into another channel water leaving the wetland flows into the same channel as the overflow from the retention basin the end of this channel is considered the outfall of the stormwater network for the two case studies we isolated the network described above from a calibrated swmm model of the larger regional stormwater network since we removed the upstream assets from the model we added inflows to simulate the real system response the network was forced with a 5 year 12 h storm which corresponds with design guidelines in the study region wong and kerkez 2018 readers are directed to zenodo for the swmm input files and simulation code mason 2021a b we provide these case studies to illustrate the following capabilities of stormreactor 1 stormreactor can model swmm s pollutant treatment equations as if we used swmm s water quality module directly 2 stormreactor can model new water quality processes e g channel erosion cstrs in series and 3 stormreactor enables water quality based real time control actions the first case study uses tss to illustrate the first two capabilities section 4 1 and the second case study uses nitrate to demonstrate the third capability section 4 2 4 1 tss case study tss often measured as concentration in mg l is a commonly monitored pollutant because it negatively impacts water quality these impacts include increasing turbidity inhibiting plant growth reducing species diversity as well as providing transportation for nutrients and heavy metals shammaa and zhu 2001 schilling et al 2017 dong et al 1984 to mitigate these negative impacts researchers and practitioners must be able to model deposition erosion and transport processes section 4 1 1 details how stormreactor was used to model these tss processes and section 4 1 2 provides the simulation results and discussion 4 1 1 tss methods gravity settling was assumed to occur in the wetland basins and channels we selected the gravity settling equation from the swmm reference manual volume iii water quality to illustrate how stormreactor allows users to model and match existing swmm treatment equations rossman and huber 2016 the gravity settling equation is defined as 1 c c c c e x p k δ t d the values for the steady state concentration c 21 mg l and the settling velocity k 0 0005 m s were selected based on prior monitoring campaigns in the region at each routing time step δt depth d was queried from swmm and the current concentration c was computed along with gravity settling erosion was also assumed to occur in both channels many equations exist for modeling erosion and sediment transport many of which can be implemented in our library for illustration purposes we selected the engelund hansen sediment transport formula engelund and hansen 1967 the formula of engelund and hansen formula 1967 can be expressed as 2 f φ 0 1 0 5 2 where 3 f 2 g d s o v 2 4 θ d s o s s 1 d 50 5 q t φ s s 1 g d 50 3 1 2 where f is a friction factor φ is a dimensionless sediment transport function θ is a dimensionless shear parameter g is gravitational acceleration d is hydraulic depth s o is channel slope v is mean channel velocity s s is specific gravity of sediment d 50 is mean particle diameter and q t is total bed material sediment discharge by weight per unit width usda 1983 wu et al 2004 the values for mean particle diameter d 50 0 04 mm sediment specific gravity s s 1 6 and channel slope 0 037 1 8 m m were selected based on site data at each routing time step the required parameter values were queried from swmm the sediment discharge concentration was computed and the new tss concentration was set in swmm root mean square error was used to validate both settling and erosion in the nodes and links for gravity settling in the nodes root mean square error was calculated for the cumulative tss load from the stormreactor simulation and a native swmm simulation fig 4 the root mean squared error was zero for all three nodes since treatment in swmm links is a new feature of stormreactor gravity settling and erosion in the channels had to be validated differently the load leaving the channel was compared to the load entering the outfall the root mean squared error was 6 19e 13 tss concentrations measured directly downstream of our outfall average 21 mg l during steady state conditions and 175 mg l during storm conditions for our simulation tss was assumed to follow an event mean concentration emc wash off model rossman 2015 since this network is dominated by channel erosion and not subcatchment wash off the steady state emc was used in the wash off model the additional tss needed to match storm event concentrations was provided by the erosion model 4 1 2 tss results and discussion results show that this system is dominated by erosion processes with only small reductions due to gravity settling fig 5 the detention basin s tss concentration averaged 13 mg l due to the small emc used in the wash off model the retention basin saw higher concentrations throughout the simulation with an average tss concentration of 121 mg l this was a result of significant erosion occurring in the channel that connects the two basins the wetland s tss concentration was lower than in the retention basin but still averaged 100 mg l during the simulation the reduction was due to settling in the wetland the outfall s average tss concentration was 107 mg l the increase in concentration at the outfall was again due to channel erosion occurring between the wetland and the outfall stormreactor improved tss process representation by including channel erosion prior to stormreactor users could not model pollutant generation processes unless they modified the parameters in the swmm build up and wash off equations in our case study this would have not reflected reality because it would have resulted in high tss concentrations in the detention basin since most of the tss added to this system comes from downstream channel erosion high tss concentrations should only be found in the downstream assets stormreactor now provides the ability to model pollutant generation processes in the assets in which they occur the tss simulation took 42 35 s on a 2018 macbook pro processor 2 2 ghz 6 core intel core i7 memory 16 gb 2400 mhz ddr4 as compared to 6 75 s without the tss model as we scale to larger networks future work must evaluate the computational efficiency of stormreactor 4 2 nitrate case study excess nitrogen can cause water quality impairments such as eutrophication harmful algal blooms and fish kills conley et al 2009 howarth and paerl 2008 in order to mitigate these negative impacts researchers and practitioners must be able to model the nitrogen cycle this is presently not possible in models like swmm because the multiphase multicomponent reactions which are affected by the aerobic anoxic conditions in the network cannot be simulated troitsky et al 2019 niazi et al 2017 section 4 2 1 details how stormreactor was used to model nitrate section 4 2 2 explains the addition of real time control which will control the stormwater network in response to water quality states to our knowledge this case study is the first to model nitrate treatment through real time control at the scale of an entire stormwater network 4 2 1 nitrate methods modeling nitrogen interactions in stormwater is difficult because nitrogen exists in various forms e g nitrate nitrite particulate nitrogen ammonia ammonium dissolved organic nitrogen nitrogen gas and undergoes numerous transformations e g denitrification nitrification ammonification fixation and dissimilatory reduction troitsky et al 2019 in stormwater basins and wetlands nitrogen is typically removed through three main mechanisms assimilation sedimentation and denitrification however the primary mechanism is denitrification yang and lusk 2018 high denitrification rates are a result of high nitrate concentrations low do concentrations and readily available sources of carbon e g decaying plants and grass kadlec and wallace 2009 perryman et al 2011 for this case study we focused only on nitrogen in the form of nitrate and therefore denitrification as the primary removal mechanism we selected nitrate because site data and other studies indicate runoff is dominated by this form of nitrogen kadlec and wallace 2009 denitrification was assumed to occur only in the wetland because wetlands tend to have large quantities of biomass and thus higher denitrification capacity than other storage nodes white and reddy 2009 scholes et al 2008 since this case study assumed high nitrate concentrations and readily available sources of carbon do became the limiting factor for denitrification necessitating us to model do concentrations as well the wetland do model was implemented using the cstr method in stormreactor based on findings by kadlec 2010 we assumed the wetland functioned as three cstrs in series we selected cstrs to illustrate how stormreactor enables wastewater treatment process models often cstrs are modeled assuming steady state conditions where the influent concentration inflow rate and outflow rate are constant and therefore the concentration in the control volume is also constant steady state condition allows for a closed form solution to the cstr equation however in a wetland influent concentration and flows are dynamic and therefore the cstr should be assumed to be unsteady we solved the unsteady cstr with an ode solver to show how stormreactor integrates with other computational python packages we selected the scipy ode numerical solver using the explicit runge kutta method 4 4 https docs scipy org doc scipy reference generated scipy integrate ode html virtanen et al 2020 the cstr equation is defined as 6 d c d t v q i n c i n q o u t c k c v based on data collected in this network the influent do concentration c in to the wetland was assumed to be 9 6 mg l the reaction rate constant k do was assumed to be 0 2 hr reddy and patrick 1984 at each routing time step the dynamic parameters were queried from swmm q in q out v and the ode solver computed the current concentration c since the do concentration was only relevant to triggering denitrification in the wetland do was tracked only in python and therefore the new do concentration did not need to be set in swmm i e do was not added as a pollutant in the swmm input file nitrate treatment was triggered when the do concentration dropped below 1 mg l signaling anoxic conditions nitrate treatment in the wetland was also modeled in stormreactor using three cstrs in series kadlec and wallace 2009 the nitrate concentration in the real stormwater network averages less than 1 mg l during steady state and storm conditions although this low level may exceed recommended water quality criteria epa 2002 assuming a larger concentration will result in higher rates of denitrification for simulation purposes therefore for our simulation nitrate was added to the system using swmm s wash off model assuming an emc of 10 mg l which aligns with 13 of stream sites monitored by mueller and spahr 2005 the nitrate reaction rate constant k no was assumed to be 1 5 day reddy and patrick 1984 at each routing time step the dynamic parameters were queried from swmm q in q out c in v the ode solver computed the current concentration c and that concentration was then set in swmm to validate the cstrs in series model stormreactor s steady state concentration at the end of the simulation was compared with the steady state analytical solution the wetland s nitrate concentration from stormreactor converged to the computed steady state analytical solution 5 7 error 4 2 2 nitrate real time control strategy a water quality based controller was constructed to maximize denitrification without flooding the wetland algorithm 1 the controller held water in the wetland until the nitrate was treated or flooding was imminent it also held water in the upstream detention basin until the downstream wetland had sufficient storage capacity to handle more inflow when the controller opened a valve it regulated the size of the opening 0 100 to release water at a rate proportional to the asset s water level by solving the submerged orifice equation rossman 2015 it was assumed that the network had the necessary water quantity and quality sensors and the outlets of the detention basin and the wetland had controllable valves to reflect real world implementation control decisions were constrained to every 15 min the controlled scenario was compared against a baseline uncontrolled scenario to determine the effectiveness of the controller algorithm 1 the controller s objective was to maximize denitrification without flooding the wetland the controller computed the valve s percent opening for the detention basin valve db and wetland valve w water was released proportionally by solving the submerged orifice equation q m a x c a 2 g d for c the discharge coefficient where q max was the maximum flow rate desired q max 2 m 3 s a was the completely open orifice area g was acceleration due to gravity and d was water depth q max was the flow rate threshold at which downstream sediments were assumed to re suspend mullapudi et al 2017 the computed value for c was multiplied by a scaling factor f f 1 75 in this study image 1 4 2 3 nitrate results and discussion the controller met the control objective of maximizing denitrification fig 6 the controlled scenario saw a 95 nitrate load reduction at the outfall as compared to the uncontrolled scenario the load reduction was a result of keeping the valves closed when either the wetland was oxic or the wetland s nitrate concentration was too high the controller used both the wetland and the upstream basin for storage until the conditions were appropriate to release flows to put this load reduction into context swmm was used to determine how large the studied wetland would need to be to obtain the same load reduction without real time control after incrementally increasing the area of the wetland and rerunning the swmm simulation several times it was determined that the wetland would need to be four times as large to obtain the same load reduction the controller also ensured that flooding did not occur in any of the assets fig 6 the water depths in the detention basin and wetland were kept below their flooding thresholds these two assets did not flood because the detention basin had significant storage capacity and the controller opened the wetland valve whenever it was close to its maximum capacity in both scenarios the retention basin depth resulted in some flows bypassing the wetland unfortunately this is because of how the retention basin wetland system was designed if a control valve was installed or the bypass height was increased on the retention basin these bypass flows could have been reduced stormreactor provided the ability to implement a water quality based controller in swmm prior to this package users trying to meet water quality goals with controllers could only access water quantity states now users can access water quality states and build a pollutant concentration based controller with only a few lines of python code the real time controlled nitrate simulation took 86 84 s on a 2018 macbook pro processor 2 2 ghz 6 core intel core i7 memory 16 gb 2400 mhz ddr4 the nitrate simulation without real time control took 86 22 s as compared to the simulation without water quality or real time control which took 12 30 s the increased computational time was a result of the longer simulation twelve days instead of five and the ode solver therefore to increase computational efficiency in the future a discrete form update could be used instead of an ode solver 5 discussion as shown in the case studies stormreactor improved water quality process representation at both the site and watershed scale rather than implementing an all in one quality quantity model we coupled the popular water quantity features of swmm with stormreactor s water quality model to illustrate the fidelity of stormreactor we showed how a variety of pollutant transformations e g erosion settling cstr matched expectations from established models and methods therefore stormreactor was shown to be an effective tool for modeling water quality to the best of our knowledge our modular framework supports many of the features seen in advanced hydraulic and water quality packages for advanced users stormreactor s integration with python will support numerical solvers and packages higher order reaction kinetics wastewater process models e g asm 1 and combined sewer networks in its present implementation stormreactor poses a few constraints which users need to be aware of before choosing to use it in their stormwater studies it does not presently support lid i e green infrastructure water quality processes because swmm handles lid water quality outside of its link and node data structures in addition stormreactor does not support high spatial resolution water quality processes e g advection diffusion dispersion both lid access and high spatial resolution models can be added and are proposed as future work aside from these limitations stormreactor provides a general water quality modeling solution that is flexible and expandable the nitrate case study points to the potential of using real time control or smart stormwater systems for ecological benefits watershed water quality goals can be acheived by tuning real time control the ability to model complex water quality interactions enables the development and testing of real time control algorithms that use pollutant concentration load and sensor data we can now utilize formal control theory e g pid mpc genetic algorithms to explore emergent behavior stability and optimal control strategies at both the site and watershed scale we can then use this information to optimize asset treatment performance pushing our watersheds to behave like distributed water treatment plants and ultimately improve watershed water quality 6 conclusions stormreactor improves the fidelity of modeling pollutant transformations and pollutant based real time control moving us a step closer to realizing the goal of controlling entire watersheds as real time distributed treatment plants additional fidelity could be gained by adding lid access and high spatial resolution models to stormreactor the flexibility of stormreactor gives researchers and practitioners immense freedom in modeling water quality we hope that this package will become a community driven resource we see opportunities for the research community to collaborate on the development of stormreactor by contributing their own pollutant generation and treatment methods as we scale to larger networks future work must evaluate the computational efficiency of stormreactor in addition significant future research stands to be enabled through the use of holistic frameworks such as those posed in this paper in particular future studies have the potential to evaluate how to control entire watersheds in response to ecological objectives software availability name of software stormreactor developers brooke mason abhiram mullapudi year first available 2020 operating system osx windows or linux software required python 3 6 0 pyswmm 1 0 1 numpy 1 21 0 scipy 1 7 0 availability and online documentation https github com klabum stormreactor a snapshot of the github repository consistent with the description in this paper is available in zenodo mason and mullapudi 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the u s national science foundation award numbers 1750744 and 1737432 
25718,retrofitting watersheds with sensing and control technologies promises to enable autonomous water systems which control themselves in real time to improve water quality to realize this vision there is a need to improve the degree of fidelity in the underlying representation of pollutant processes this paper presents an open source python package stormreactor which integrates the stormwater management model s water balance engine with a new water quality module stormreactor includes a variety of predefined pollutant generation and treatment processes while allowing users to implement additional processes on their own to demonstrate the range of possible water quality methodologies that can be modeled we simulated suspended solids and nitrates in a real and anonymized stormwater network to illustrate stormreactor s real time control capabilities a control strategy was implemented to maximize denitrification case study results indicate a controlled asset can achieve the same pollutant improvements as an uncontrolled asset in a quarter of the spatial footprint graphical abstract image 1 keywords water quality water balance model integration real time control stormwater 1 introduction a reliable and cost effective method for treating stormwater pollutants is real time control sun et al 2020 garofalo et al 2017 shishegar et al 2019 retrofitting stormwater assets with sensing and control technologies enables watersheds to adapt in real time to individual storms or pollutant loads persaud et al 2019 zhang et al 2018 these smart stormwater assets can be coordinated at the watershed scale to maximize pollutant treatment kerkez et al 2016 eggimann et al 2017 berglund et al 2020 in essence this supports the analogy of transforming our natural or urbanized watersheds into distributed treatment plants by combining knowledge from stormwater systems and process control mullapudi et al 2017 to realize this vision we must first be able to model both pollutant transformations and the impact of real time control actions on water quality at the watershed scale wong et al 2006 garcía et al 2015 berglund et al 2020 this can be achieved with integrated environmental modeling integrated environmental modeling dynamically links distinctly separate models during run time to better understand the environmental system s response to human and natural stressors laniak et al 2013 sutherland et al 2017 recently integrated environmental modeling has been used to combine climate and streamflow data with a water budget model and a dynamic groundwater model shuler and mariner 2020 simulate the hydrological effects of land use changes on karst systems bittner et al 2020 link precipitation forecasts with real time hydrological and hydraulic modeling for urban flood forecasting brendel et al 2020 couple hydrodynamic and closed nutrient cycle ecological models to predict dissolved oxygen do in surface waters suarez et al 2019 and create a catchment scale water quality modeling and monitoring framework wang et al 2019 integrated environmental modeling of stormwater requires the coupling of water quantity and quality models this necessitates simulating a number of underlying processes including precipitation runoff climatic variables land use flow and pollutant routing and pollutant transformations deletic and maksimovic 1998 egodawatta et al 2007 mccarthy et al 2007 while a number of existing models are able to represent these individual components effectively at a granular scale an all in one modeling package is still lacking given the complexity of stormwater specifically its nonlinear dynamics overton and meadows 2013 garcía et al 2015 most existing models understandably seem to draw a line between flow and quality obropta and kardos 2007 bach et al 2014 there has been a stated need to integrate these two types of environmental models mullapudi et al 2017 wang et al 2019 tuomela et al 2018 to that end the specific contributions of this paper are 1 stormreactor a new water quality package implemented as an extension of the popular us environmental protection agency s epa stormwater management model swmm which provides an open source python programming interface for simulating complex pollutant generation treatment and real time control processes 2 an evaluation of the package s ability to model complex pollutant transformations and real time control actions using two case studies these contributions provide researchers and practitioners more flexibility in simulating water quality processes and pollutant based real time control at site and watershed scales 2 state of stormwater quality modeling existing stormwater models can be broadly grouped into two categories water quantity models and water quality models most stormwater models primarily focus on coupled hydrologic hydraulic processes with limited capabilities for modeling water quality e g mike urban dr3m storm music swmm obropta and kardos 2007 bach et al 2014 however some stormwater models do focus on high resolution water quality processes these finite element models e g hydrus cwmi fitovert simulate complex pollutant transformations within individual sites rizzo et al 2014 pálfy and langergraber 2014 giraldi et al 2010 unfortunately scaling from site to watershed scale becomes very difficult due to the input data requirements and the difficulty of parameterization the chasm between these two types of stormwater models forces a trade off between either comprehensively modeling water quality at the site scale or less comprehensively modeling watershed scale processes to avoid this tradeoff researchers have modified existing stormwater models like swmm to expand their pollutant modeling capabilities swmm widely used in the us stormwater community is an open source urban stormwater model rossman 2015 swmm s water quality model provides users the ability to introduce pollutants and pollutant treatment while also routing and calculating mass balance for each pollutant rossman and huber 2016 swmm tss modified swmm to simulate total suspended solids tss transport accumulation and erosion in sewers and retention tanks sun et al 2017 as its name implies this modification is only for tss baek et al 2020 modified swmm s water quality module for low impact development lid to include straining decay and decomposition of pollutants however this modification does not work for stormwater storage assets or links talbot et al modified pcswmm a licensed version of swmm to simulate sediment loading due to soil erosion talbot et al 2016 this modification is not open source and thus not open for exploration or expansion by the community all of these packages are very useful for specific modeling tasks however they do not offer general water quality modeling solutions although these packages provide additional functionality swmm has many remaining pollutant modeling limitations that must be addressed the water quality module is limited by the range of treatment measures that can be modeled wong et al 2006 specifically limited nutrient treatment capabilities inside storage nodes e g basins wetlands troitsky et al 2019 niazi et al 2017 swmm cannot simulate pollutant treatment inside links e g conduits channels or pollutant generation processes e g resuspension erosion inside any stormwater asset pollutant treatment cannot be turned on or off based on site conditions or other parameters requiring treatment to run for the entire simulation all of these constraints limit a user s ability to model complex pollutant transformations necessitating a more generalizable and scalable approach aside from water quality limitations many stormwater quality models have limited or no ability to simulate real time control real time control is made possible through the installation of sensors which can monitor the flow and quality parameters and actuators which can control the flow of water kerkez et al 2016 schu et al 2004 to realize the goal of autonomous watersheds we must be able to model real time control strategies garcía et al 2015 vanrolleghem et al 2005 one open source and popular real time control package is pyswmm a python wrapper for the swmm computational engine pyswmm queries stormwater states directly from swmm which is used to apply control actions by setting the control parameters for valves gates and pumps in real time mcdonnell et al 2020 however pyswmm presently only enables real time control decisions to be made based on water quantity parameters e g flow head depth volume therefore there is a need for a comprehensive package that can both simulate water quality processes and real time control 3 new package for modeling stormwater quality a watershed scale pollutant transformation model is comprised of the water quantity and water quality representations of the stormwater network fig 1 these representations provide insight into which sub components are already well addressed by existing models and which others should be expanded or developed the water quantity representation focuses on the conveyance of water through the network of links e g channels conduits and nodes e g detention basins retention basins wetlands the hydrologic and hydraulic processes which underpin the water quantity sub component are well established in stormwater models obropta and kardos 2007 bach et al 2014 the water quality representation includes the pollutant generation and treatment processes that occur in stormwater assets e g wetland as a continuously tank reactor cstr retention basin as a settling tank often this sub component is significantly simplified e g first order decay models instead of drawing from water treatment process literature mullapudi et al 2017 leaving room for expansion guided by the state of these sub components in current stormwater models we developed stormreactor a new water quality python package coupled with swmm the choice to build a module for swmm was based on a number of factors first swmm has a verified hydraulic solver which is critically important for accurately modeling flow and pollutant routing rossman 2015 in addition building upon swmm s popularity engages a large user base ensuring it is accessible to more people finally swmm is open source which enables modification of its code and the use of popular python wrappers such as pyswmm section 3 1 and section 3 2 detail the development and structure of stormreactor stormreactor was created by i modifying the swmm and pyswmm source code to allow water quality states to be modified and ii building an additional python library to interface water quality modeling with these popular tools 3 1 swmm and pyswmm to address the limitations of swmm s water quality module we modified swmm s c source code 1 1 https github com openwateranalytics stormwater management model by introducing getters and setters to allow for real time access of the model states during simulation table 1 a getter enables a user to access a variable while a setter enables a user to change the value of a variable we then modified pyswmm s python source code 2 2 https github com openwateranalytics pyswmm to gain access to swmm water quality states and to provide the convenience of modeling in a popular scripting language while pyswmm already allowed for the interaction with swmm s quantity states e g flows depths it needed to be expanded to support interaction with water quality states table 1 now a user can interact with a pollutant s concentration in any node or link during any routing time step in this way swmm is used to transport pollutants using its reliable hydraulic and routing engine pyswmm is used to support python interaction with swmm s c engine and stormreactor adds supplementary support for water quality modeling fig 2 3 2 stormreactor stormreactor enables users to model water quality while fully leveraging the well validated swmm functionality for flow and routing stormreactor provides a high level programming interface that removes the user from the complex interactions between swmm pyswmm and stormreactor and only requires a few python command statements to model pollutant transformations users have the ability to select a water treatment method in any stormwater asset and specify the routing time steps across which to carry out simulations to promote uptake by an existing community of modelers a user can select any of the already existing swmm treatment functions outlined in the swmm reference manual volume iii water quality table 2 rossman and huber 2016 users can also select from a library of our new water quality methods including reactor models and stream processes such as erosion table 2 more importantly users can implement their own custom pollutant models using a python interface section 3 2 3 these custom pollutant models can be built upon states of the various water quantity and quality parameters in swmm e g flow depth volume concentration as well as interact with other python packages e g scipy readers are directed to zenodo for stormreactor s source code and documentation mason and mullapudi 2021 3 2 1 user experience stormreactor can be installed using pip 3 3 https pypi org project stormreactor to use stormreactor first import both stormreactor and pyswmm fig 3 next define a configuration dictionary stating at which nodes and links water quality will be modeled as well as the desired pollutants water quality methods and the parameters required for each method then create an instance of the water quality class by calling waterquality which takes two arguments config the configuration dictionary and sim a pyswmm simulation object which encapsulates all the swmm simulation functionality e g start stop simulation get set attributes finally call the class instance method updatewqstate to run the desired water quality method once initialized stormreactor executes the simulation loop first stormreactor queries the necessary water quantity and quality parameters e g water depth pollutant concentration for specific stormwater assets at the current routing time step next it uses the queried parameters to compute and set the new pollutant concentration using a predefined or custom water quality method if a water quality computation requires a time parameter the length of the routing time step is used if real time control is being modeled selected water quality and or quantity data are used to calculate the control decisions swmm then enacts the real time control decisions and routes the pollutant s and flows through the network this process can be repeated at any or every routing time step the simulation loop terminates after the number of desired routing time steps or the swmm model is complete 3 2 2 architecture stormreactor s architecture follows an object oriented programming paradigm this matches already popular python conventions and maximizes potential for user customization stormreactor begins by defining a class waterquality the class has an init method which takes three parameters self an instance of the class sim the pyswmm simulation object and config the configuration dictionary when an instance of the class is created it automatically calls the init method which does the following 1 initializes the asset flag 2 calls the pyswmm method sim start time to get the start time of the simulation 3 initializes the variable last timestep to aid in calculating the length of the routing time step 4 initializes the ordinary differential equation ode solver for the cstr water quality method and 5 defines the callable names of the water quality instance methods the waterquality class also defines two important methods updatewqstate and updatewqstate cstr which update the pollutant concentrations during a swmm simulation for non cstr and cstr methods respectively the class also has a collection of python instance methods which specify the various treatment and generation processes that can be performed on a pollutant table 2 time steps are handled by stormreactor by relying on swmm many of the treatment methods do not require a time parameter e g event mean concentration constant removal k c method stormreactor handles these methods just as they would be handled in native swmm these methods grab the current pollutant concentration and then calculates and sets the new concentration at the end of the current routing time step for the methods that do require a time parameter e g n th order reaction kinetics erosion gravity settling stormreactor computes the routing time step length dt using the same method as swmm to calculate dt stormreactor calls the pyswmm function sim current time to get the current simulation time subtracts the previous routing time step saved in the variable last timestep and then converts it to seconds in this way stormreactor is dependent on swmm to get dt once dt is calculated and the current concentration is queried the new concentration is computed and set at the end of the current routing time step this new concentration then becomes the concentration at the beginning of the next routing time step routing time steps are usually on the order of seconds whereas water quality processes may take much longer therefore users must also parameterize water quality coefficients on the order of seconds 3 2 3 implementing custom pollutant models to implement a new custom pollutant model users can either 1 add their new class instance method to stormreactor s code base or 2 build their model directly in their python script using the appropriate getters and setters table 1 we recommend the first option if code is to be more seamlessly shared with others to add a new method to the code base a user must 1 define the new method using the following convention newmethod self id pollutantid parameters flag non public python instance methods should always start with an underscore the new method requires five parameters self an instance of the class id the node or link name in swmm pollutantid the pollutant index in swmm parameters the water quality method parameters and flag used to determine if the method is for a link or node 2 provide a text description of the method including the water quality method parameters and their required units be sure to note if the method is for links nodes or both 3 write the pollutant transformation code for the new method a define any variables that may be needed for the pollutant transformation calculations b query swmm variables that are necessary for the computation e g pollutant concentration water depth current simulation time using pyswmm getters c compute the pollutant transformation concentration d set the new pollutant concentration using pyswmm setters 4 define the callable name in the init method 5 write unit tests for the new method and add them to test links py and or test nodes py in the tests folder once the new method is added to stormreactor s code base the user can then use it following the steps outlined in section 3 2 1 4 water quality case studies the study area is a 7 8 km2 urban separated stormwater network fig 1 located in michigan which suffers from erosion problems due to high flashy flows in this network stormwater first flows through a detention basin into a long channel a detention basin has its outlet at the bottom of the basin so between storms it is usually dry the long channel then flows into a retention basin a retention basin has its outlet at a higher point so it tends to retain a permanent pool of water if the height of the water in the retention basin is less than a specified threshold water flows directly into a constructed treatment wetland otherwise water bypasses the wetland and overflows into another channel water leaving the wetland flows into the same channel as the overflow from the retention basin the end of this channel is considered the outfall of the stormwater network for the two case studies we isolated the network described above from a calibrated swmm model of the larger regional stormwater network since we removed the upstream assets from the model we added inflows to simulate the real system response the network was forced with a 5 year 12 h storm which corresponds with design guidelines in the study region wong and kerkez 2018 readers are directed to zenodo for the swmm input files and simulation code mason 2021a b we provide these case studies to illustrate the following capabilities of stormreactor 1 stormreactor can model swmm s pollutant treatment equations as if we used swmm s water quality module directly 2 stormreactor can model new water quality processes e g channel erosion cstrs in series and 3 stormreactor enables water quality based real time control actions the first case study uses tss to illustrate the first two capabilities section 4 1 and the second case study uses nitrate to demonstrate the third capability section 4 2 4 1 tss case study tss often measured as concentration in mg l is a commonly monitored pollutant because it negatively impacts water quality these impacts include increasing turbidity inhibiting plant growth reducing species diversity as well as providing transportation for nutrients and heavy metals shammaa and zhu 2001 schilling et al 2017 dong et al 1984 to mitigate these negative impacts researchers and practitioners must be able to model deposition erosion and transport processes section 4 1 1 details how stormreactor was used to model these tss processes and section 4 1 2 provides the simulation results and discussion 4 1 1 tss methods gravity settling was assumed to occur in the wetland basins and channels we selected the gravity settling equation from the swmm reference manual volume iii water quality to illustrate how stormreactor allows users to model and match existing swmm treatment equations rossman and huber 2016 the gravity settling equation is defined as 1 c c c c e x p k δ t d the values for the steady state concentration c 21 mg l and the settling velocity k 0 0005 m s were selected based on prior monitoring campaigns in the region at each routing time step δt depth d was queried from swmm and the current concentration c was computed along with gravity settling erosion was also assumed to occur in both channels many equations exist for modeling erosion and sediment transport many of which can be implemented in our library for illustration purposes we selected the engelund hansen sediment transport formula engelund and hansen 1967 the formula of engelund and hansen formula 1967 can be expressed as 2 f φ 0 1 0 5 2 where 3 f 2 g d s o v 2 4 θ d s o s s 1 d 50 5 q t φ s s 1 g d 50 3 1 2 where f is a friction factor φ is a dimensionless sediment transport function θ is a dimensionless shear parameter g is gravitational acceleration d is hydraulic depth s o is channel slope v is mean channel velocity s s is specific gravity of sediment d 50 is mean particle diameter and q t is total bed material sediment discharge by weight per unit width usda 1983 wu et al 2004 the values for mean particle diameter d 50 0 04 mm sediment specific gravity s s 1 6 and channel slope 0 037 1 8 m m were selected based on site data at each routing time step the required parameter values were queried from swmm the sediment discharge concentration was computed and the new tss concentration was set in swmm root mean square error was used to validate both settling and erosion in the nodes and links for gravity settling in the nodes root mean square error was calculated for the cumulative tss load from the stormreactor simulation and a native swmm simulation fig 4 the root mean squared error was zero for all three nodes since treatment in swmm links is a new feature of stormreactor gravity settling and erosion in the channels had to be validated differently the load leaving the channel was compared to the load entering the outfall the root mean squared error was 6 19e 13 tss concentrations measured directly downstream of our outfall average 21 mg l during steady state conditions and 175 mg l during storm conditions for our simulation tss was assumed to follow an event mean concentration emc wash off model rossman 2015 since this network is dominated by channel erosion and not subcatchment wash off the steady state emc was used in the wash off model the additional tss needed to match storm event concentrations was provided by the erosion model 4 1 2 tss results and discussion results show that this system is dominated by erosion processes with only small reductions due to gravity settling fig 5 the detention basin s tss concentration averaged 13 mg l due to the small emc used in the wash off model the retention basin saw higher concentrations throughout the simulation with an average tss concentration of 121 mg l this was a result of significant erosion occurring in the channel that connects the two basins the wetland s tss concentration was lower than in the retention basin but still averaged 100 mg l during the simulation the reduction was due to settling in the wetland the outfall s average tss concentration was 107 mg l the increase in concentration at the outfall was again due to channel erosion occurring between the wetland and the outfall stormreactor improved tss process representation by including channel erosion prior to stormreactor users could not model pollutant generation processes unless they modified the parameters in the swmm build up and wash off equations in our case study this would have not reflected reality because it would have resulted in high tss concentrations in the detention basin since most of the tss added to this system comes from downstream channel erosion high tss concentrations should only be found in the downstream assets stormreactor now provides the ability to model pollutant generation processes in the assets in which they occur the tss simulation took 42 35 s on a 2018 macbook pro processor 2 2 ghz 6 core intel core i7 memory 16 gb 2400 mhz ddr4 as compared to 6 75 s without the tss model as we scale to larger networks future work must evaluate the computational efficiency of stormreactor 4 2 nitrate case study excess nitrogen can cause water quality impairments such as eutrophication harmful algal blooms and fish kills conley et al 2009 howarth and paerl 2008 in order to mitigate these negative impacts researchers and practitioners must be able to model the nitrogen cycle this is presently not possible in models like swmm because the multiphase multicomponent reactions which are affected by the aerobic anoxic conditions in the network cannot be simulated troitsky et al 2019 niazi et al 2017 section 4 2 1 details how stormreactor was used to model nitrate section 4 2 2 explains the addition of real time control which will control the stormwater network in response to water quality states to our knowledge this case study is the first to model nitrate treatment through real time control at the scale of an entire stormwater network 4 2 1 nitrate methods modeling nitrogen interactions in stormwater is difficult because nitrogen exists in various forms e g nitrate nitrite particulate nitrogen ammonia ammonium dissolved organic nitrogen nitrogen gas and undergoes numerous transformations e g denitrification nitrification ammonification fixation and dissimilatory reduction troitsky et al 2019 in stormwater basins and wetlands nitrogen is typically removed through three main mechanisms assimilation sedimentation and denitrification however the primary mechanism is denitrification yang and lusk 2018 high denitrification rates are a result of high nitrate concentrations low do concentrations and readily available sources of carbon e g decaying plants and grass kadlec and wallace 2009 perryman et al 2011 for this case study we focused only on nitrogen in the form of nitrate and therefore denitrification as the primary removal mechanism we selected nitrate because site data and other studies indicate runoff is dominated by this form of nitrogen kadlec and wallace 2009 denitrification was assumed to occur only in the wetland because wetlands tend to have large quantities of biomass and thus higher denitrification capacity than other storage nodes white and reddy 2009 scholes et al 2008 since this case study assumed high nitrate concentrations and readily available sources of carbon do became the limiting factor for denitrification necessitating us to model do concentrations as well the wetland do model was implemented using the cstr method in stormreactor based on findings by kadlec 2010 we assumed the wetland functioned as three cstrs in series we selected cstrs to illustrate how stormreactor enables wastewater treatment process models often cstrs are modeled assuming steady state conditions where the influent concentration inflow rate and outflow rate are constant and therefore the concentration in the control volume is also constant steady state condition allows for a closed form solution to the cstr equation however in a wetland influent concentration and flows are dynamic and therefore the cstr should be assumed to be unsteady we solved the unsteady cstr with an ode solver to show how stormreactor integrates with other computational python packages we selected the scipy ode numerical solver using the explicit runge kutta method 4 4 https docs scipy org doc scipy reference generated scipy integrate ode html virtanen et al 2020 the cstr equation is defined as 6 d c d t v q i n c i n q o u t c k c v based on data collected in this network the influent do concentration c in to the wetland was assumed to be 9 6 mg l the reaction rate constant k do was assumed to be 0 2 hr reddy and patrick 1984 at each routing time step the dynamic parameters were queried from swmm q in q out v and the ode solver computed the current concentration c since the do concentration was only relevant to triggering denitrification in the wetland do was tracked only in python and therefore the new do concentration did not need to be set in swmm i e do was not added as a pollutant in the swmm input file nitrate treatment was triggered when the do concentration dropped below 1 mg l signaling anoxic conditions nitrate treatment in the wetland was also modeled in stormreactor using three cstrs in series kadlec and wallace 2009 the nitrate concentration in the real stormwater network averages less than 1 mg l during steady state and storm conditions although this low level may exceed recommended water quality criteria epa 2002 assuming a larger concentration will result in higher rates of denitrification for simulation purposes therefore for our simulation nitrate was added to the system using swmm s wash off model assuming an emc of 10 mg l which aligns with 13 of stream sites monitored by mueller and spahr 2005 the nitrate reaction rate constant k no was assumed to be 1 5 day reddy and patrick 1984 at each routing time step the dynamic parameters were queried from swmm q in q out c in v the ode solver computed the current concentration c and that concentration was then set in swmm to validate the cstrs in series model stormreactor s steady state concentration at the end of the simulation was compared with the steady state analytical solution the wetland s nitrate concentration from stormreactor converged to the computed steady state analytical solution 5 7 error 4 2 2 nitrate real time control strategy a water quality based controller was constructed to maximize denitrification without flooding the wetland algorithm 1 the controller held water in the wetland until the nitrate was treated or flooding was imminent it also held water in the upstream detention basin until the downstream wetland had sufficient storage capacity to handle more inflow when the controller opened a valve it regulated the size of the opening 0 100 to release water at a rate proportional to the asset s water level by solving the submerged orifice equation rossman 2015 it was assumed that the network had the necessary water quantity and quality sensors and the outlets of the detention basin and the wetland had controllable valves to reflect real world implementation control decisions were constrained to every 15 min the controlled scenario was compared against a baseline uncontrolled scenario to determine the effectiveness of the controller algorithm 1 the controller s objective was to maximize denitrification without flooding the wetland the controller computed the valve s percent opening for the detention basin valve db and wetland valve w water was released proportionally by solving the submerged orifice equation q m a x c a 2 g d for c the discharge coefficient where q max was the maximum flow rate desired q max 2 m 3 s a was the completely open orifice area g was acceleration due to gravity and d was water depth q max was the flow rate threshold at which downstream sediments were assumed to re suspend mullapudi et al 2017 the computed value for c was multiplied by a scaling factor f f 1 75 in this study image 1 4 2 3 nitrate results and discussion the controller met the control objective of maximizing denitrification fig 6 the controlled scenario saw a 95 nitrate load reduction at the outfall as compared to the uncontrolled scenario the load reduction was a result of keeping the valves closed when either the wetland was oxic or the wetland s nitrate concentration was too high the controller used both the wetland and the upstream basin for storage until the conditions were appropriate to release flows to put this load reduction into context swmm was used to determine how large the studied wetland would need to be to obtain the same load reduction without real time control after incrementally increasing the area of the wetland and rerunning the swmm simulation several times it was determined that the wetland would need to be four times as large to obtain the same load reduction the controller also ensured that flooding did not occur in any of the assets fig 6 the water depths in the detention basin and wetland were kept below their flooding thresholds these two assets did not flood because the detention basin had significant storage capacity and the controller opened the wetland valve whenever it was close to its maximum capacity in both scenarios the retention basin depth resulted in some flows bypassing the wetland unfortunately this is because of how the retention basin wetland system was designed if a control valve was installed or the bypass height was increased on the retention basin these bypass flows could have been reduced stormreactor provided the ability to implement a water quality based controller in swmm prior to this package users trying to meet water quality goals with controllers could only access water quantity states now users can access water quality states and build a pollutant concentration based controller with only a few lines of python code the real time controlled nitrate simulation took 86 84 s on a 2018 macbook pro processor 2 2 ghz 6 core intel core i7 memory 16 gb 2400 mhz ddr4 the nitrate simulation without real time control took 86 22 s as compared to the simulation without water quality or real time control which took 12 30 s the increased computational time was a result of the longer simulation twelve days instead of five and the ode solver therefore to increase computational efficiency in the future a discrete form update could be used instead of an ode solver 5 discussion as shown in the case studies stormreactor improved water quality process representation at both the site and watershed scale rather than implementing an all in one quality quantity model we coupled the popular water quantity features of swmm with stormreactor s water quality model to illustrate the fidelity of stormreactor we showed how a variety of pollutant transformations e g erosion settling cstr matched expectations from established models and methods therefore stormreactor was shown to be an effective tool for modeling water quality to the best of our knowledge our modular framework supports many of the features seen in advanced hydraulic and water quality packages for advanced users stormreactor s integration with python will support numerical solvers and packages higher order reaction kinetics wastewater process models e g asm 1 and combined sewer networks in its present implementation stormreactor poses a few constraints which users need to be aware of before choosing to use it in their stormwater studies it does not presently support lid i e green infrastructure water quality processes because swmm handles lid water quality outside of its link and node data structures in addition stormreactor does not support high spatial resolution water quality processes e g advection diffusion dispersion both lid access and high spatial resolution models can be added and are proposed as future work aside from these limitations stormreactor provides a general water quality modeling solution that is flexible and expandable the nitrate case study points to the potential of using real time control or smart stormwater systems for ecological benefits watershed water quality goals can be acheived by tuning real time control the ability to model complex water quality interactions enables the development and testing of real time control algorithms that use pollutant concentration load and sensor data we can now utilize formal control theory e g pid mpc genetic algorithms to explore emergent behavior stability and optimal control strategies at both the site and watershed scale we can then use this information to optimize asset treatment performance pushing our watersheds to behave like distributed water treatment plants and ultimately improve watershed water quality 6 conclusions stormreactor improves the fidelity of modeling pollutant transformations and pollutant based real time control moving us a step closer to realizing the goal of controlling entire watersheds as real time distributed treatment plants additional fidelity could be gained by adding lid access and high spatial resolution models to stormreactor the flexibility of stormreactor gives researchers and practitioners immense freedom in modeling water quality we hope that this package will become a community driven resource we see opportunities for the research community to collaborate on the development of stormreactor by contributing their own pollutant generation and treatment methods as we scale to larger networks future work must evaluate the computational efficiency of stormreactor in addition significant future research stands to be enabled through the use of holistic frameworks such as those posed in this paper in particular future studies have the potential to evaluate how to control entire watersheds in response to ecological objectives software availability name of software stormreactor developers brooke mason abhiram mullapudi year first available 2020 operating system osx windows or linux software required python 3 6 0 pyswmm 1 0 1 numpy 1 21 0 scipy 1 7 0 availability and online documentation https github com klabum stormreactor a snapshot of the github repository consistent with the description in this paper is available in zenodo mason and mullapudi 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the u s national science foundation award numbers 1750744 and 1737432 
25719,to support equitable planning model based analyses can be used to explore inequality patterns arising from different scenarios scenario discovery is increasingly used to extract insights from ensembles of simulation here we apply two scenario discovery approaches for unraveling inequality patterns and their drivers with an application to spatial inequality of farms profitability in the vietnam mekong delta first we follow an established sequential approach where we begin with clustering the inequality patterns from the simulation results and next identify model input subspaces that best explain each cluster second we propose a novel concurrent approach using multivariate regression trees to simultaneously classify inequality patterns and identify their corresponding input subspaces both approaches have comparable output space separability performance the concurrent approach yields significantly better input space separability but this comes at the expense of having a larger number of subspaces requiring analysts to make extra effort to distill policy relevant insights keywords model based decision support fairness adaptation deep uncertainty equity 1 introduction recent model based studies for supporting climate planning have advocated for assessing distributional outcomes of alternative policies see e g gourevitch et al 2020 kind et al 2017 rao 2013 this is because evaluating policies using aggregate metrics can be misleading as a policy that is optimal from an aggregate point of view might actually benefit some people at the expense of the others hansson 2007 rao et al 2017 sayers et al 2018 looking only from an aggregate point of view can introduce or even exacerbate inequalities furthermore there exists uncertainty in not only the magnitude and the spatial distribution of climate change but also in the differential exposure vulnerability and adaptive capacity of the people and how these factors evolve over time green 2016 o neill et al 2017 thomas et al 2019 this makes it even more crucial to assess ex ante the distributional consequences of adaptation and mitigation policies there are two types of analyses for assessing distributional outcomes the first one is normative analysis here the aim is to identify a policy that best satisfies a moral principle for instance in climate change mitigation the polluters pay principle and the equal per capita entitlements are two often used imperatives for allocating mitigation responsibility gardiner 2010 okereke 2010 in adaptation the use of differentiated historical responsibility has been proposed for determining funding responsibility grasso 2007 whereas putting the most vulnerable first has been proposed for distributing benefits paavola and adger 2006 these principles can be operationalized for use in quantitative model based studies for example adler et al 2017 operationalize the prioritarian principle giving higher weights to outcomes experienced by worse off people for calculating the social cost of carbon the second type is explorative analysis rather than putting value judgements on whether the distribution of outcomes is morally acceptable explorative analysis aims to identify groups who become better off and worse off because of the implementation of policies there are various ways to define population subgroups for example ciullo et al 2020 look at the distribution of flood risk reduction benefits across people living in different locations i e dike rings by identifying potential winners and losers explorative analysis can help planners in anticipating unintended distributive consequences and ameliorating potential injustices for instance by preparing compensation measures to worse off actors when performing explorative analysis the analyst faces an interpretation problem arising out of two concerns jafino et al 2021b first identifying inequality patterns requires calculating the outcomes experienced by individual actors leading to a larger number of performance indicators this sometimes requires a modification to the model structure rao et al 2017 and how model outputs are treated franssen 2005 kasprzyk et al 2016 second the fact that distributional outcomes can vary substantially under different futures necessitates the exploration of inequality pattern across a large ensemble of scenarios schweizer 2018 taconet et al 2020 taken together the large ensemble of scenarios and the high dimensionality of the output space make it hard to distill policy relevant insights about the different plausible modes of inequality patterns and the associated policies and uncertainties under which the different modes arise scenario discovery is an approach for deriving policy relevant insights from large ensembles of simulation results bryant and lempert 2010 groves and lempert 2007 scenario discovery process begins with generating simulation results database through running the model under a large number of scenarios bankes 1993 moallemi et al 2020 and proceeds with identifying combinations of driving forces that lead to a certain pattern of model outcomes scenario discovery answers the question under which conditions or scenarios do the model outcomes behave in a certain way scenario discovery by now is a recognized approach to deal with deep uncertainty in model based planning for climate change and to make sense of large scale computational experiment see e g guivarch et al 2016 herman et al 2015 knox et al 2018 lamontagne et al 2018 moallemi et al 2017 rozenberg et al 2014 weaver et al 2013 traditional applications of scenario discovery include policy stress testing and vulnerability analysis e g eker and van daalen 2015 halim et al 2015 hidayatno et al 2020 shortridge and zaitchik 2018 as part of many objective robust decision making bartholomew and kwakkel 2020 kasprzyk et al 2013 the main objective here is identifying conditions under which a policy fails to meet its objectives this requires users to set a threshold for classifying policy success if the performance of the policy exceeds or goes below in case of a maximization problem the threshold the policy is deemed to fail in reaching its objectives in this established application of scenario discovery one applies a binary classification to the model output space from the simulation results database by dividing the output space into a region where the policy performance meets the minimal requirement and a region where it fails to do so a rule induction algorithm is then applied to identify combinations of input parameters that lead to the vulnerable region in the output space in this study we investigate the merits of using multiclass scenario discovery an extension of the standard binary class scenario discovery for performing explorative analysis of distributional outcomes in multiclass scenario discovery the model output space is partitioned into multiple clusters and the input subspaces for each cluster are then identified multiclass scenario discovery is appropriate for explorative analysis of distributional outcomes as there might be numerous modes of inequality in the future we cannot simply impose a binary classification on the distributional outcomes distinctive inequality patterns might emerge but due to system complexity and non linearity similar patterns might arise from completely distinct uncertainty and policy scenarios jafino et al 2021a we explore two alternative approaches to multiclass scenario discovery first we adapt the cluster then identify approach as has been used in previous multiclass scenario discovery studies gerst et al 2013 rozenberg et al 2014 steinmann et al 2020 in this approach the clustering of the model output space is performed first followed by the identification of input subspaces for each cluster separately this can negatively affect interpretability because different clusters in the output space might be linked to overlapping subspaces of the input space to address this we propose and test the use of multivariate regression tree mrt for multiclass scenario discovery in this second approach the output space clustering and input subspace identification are solved concurrently through the mrt algorithm we apply both the established sequential and the novel concurrent approach for multiclass scenario discovery to an agriculture adaptation planning problem for the upper vietnam mekong delta vmd we explore spatial inequality of district level farms profitability resulting from different realizations of uncertainties and implementation of adaptation measures the rest of the paper is structured as follows in section 2 we describe the two approaches of multiclass scenario discovery and explain further the concept of input and output space separability in section 3 we provide the background of the case study and introduce the model that is being used in section 4 we present the results of the two approaches in section 5 we discuss the merits of each approach i e their performance in terms of input and output space separability as well as the resulting scenario narratives identified by each approach in section 6 we summarize our main findings and insights 2 methods 2 1 multiclass scenario discovery there are a number of scenario discovery applications that extend the output space partitioning from binary classification to multiclass classification gerst et al 2013 kwakkel and jaxa rozen 2016 rozenberg et al 2014 steinmann et al 2020 a major difference between traditional scenario discovery and multiclass scenario discovery lies in the characterization of the output space in traditional scenario discovery the output space is partitioned into only two classes those which are of interest and those which are not kwakkel et al 2013 in contrast in multiclass scenario discovery the output space is partitioned into more than two classes multiclass scenario discovery involves two tasks the output space has to be partitioned into multiple distinct classes and for each class input subspaces which are highly predictive for it have to be identified the highly predictive input subspaces form the narrative behind each class in the output space for the first task partitioning the output space various approaches for specifying the classes have been used classification can be performed by either manually imposing a threshold on the outcome variables e g guivarch et al 2016 rozenberg et al 2014 or by using a clustering algorithm to automatically identify the classes e g berntsen and trutnevyte 2017 gerst et al 2013 moallemi et al 2017 steinmann et al 2020 in the manual threshold approach the analyst has full control over how the output space is partitioned thus enhancing the interpretability of the resulting classes however the task becomes increasingly complex with increasing number of outcome variables in contrast clustering algorithms can handle a larger set of outcome variables but at the expense of worsening interpretability for the second task identifying highly predictive input subspaces both the patient rule induction method prim friedman and fisher 1999 and classification and regression tree cart algorithms breiman et al 1984 have been widely used for multiclass scenario discovery prim is iteratively and independently applied to each cluster of the output space see e g rozenberg et al 2014 in contrast cart can identify highly predictive input subspaces for multiple clusters of the output space simultaneously by predicting the membership of each scenario in one of the identified clusters see e g gerst et al 2013 the partitioning of the output space and the identification of highly predictive input subspaces are traditionally performed sequentially in this study we propose the use of multivariate regression tree mrt for multiclass scenario discovery to concurrently perform these two tasks mrt is an extension of cart where multiple dependent variables are being used to characterize the impurity of a decision node de ath 2002 mrt has previously been used for model based analysis such as for unraveling tradeoffs and synergies between management objectives ndong et al 2020 smith et al 2019 for multiclass scenario discovery the input parameters of the simulation model become the independent variables of the mrt while the outcome variables of interest become the dependent variables the leaves resulting from the regression tree then act as the clusters of the output space the variables being used in each decision node and their corresponding splitting values form the narrative behind each cluster of output space scenario discovery enables the extraction of policy relevant insights e g exploring plausible modes of inequality patterns from large scale computational experiments by making the large ensemble of simulation results interpretable the interpretability of multiclass scenario discovery can be evaluated using three criteria the first criterion is output space separability which is similar to the objective of clustering algorithms hastie et al 2009 jain 2010 after clustering the output space members within the same cluster should have similar outcome characteristics e g spatial inequality patterns while members from different clusters should be dissimilar the second criterion is input space separability steinmann et al 2020 which focuses on the rule induction part of scenario discovery each class of outcome should originate from distinct and non overlapping subspaces in the input space as illustrated in fig 1 scenario discovery results are ideal if the identified input and output subspaces are completely separable i e if each cluster in the output space is distinctive from the other clusters and is driven by distinctive subspaces in the input space the third criterion is the resulting number of scenario narratives having a larger number of clusters generally leads to better output space separability hastie et al 2009 but it comes at the expense of having more complicated narratives to be communicated to decision makers 2 2 sequential approach cluster then identify 2 2 1 clustering phase the clustering phase aims to find distinctive patterns of outcomes within the simulation results clustering performance is evaluated by the explained variance e v k 1 k 1 k s s e k s s e a l l where evk is the explained variance of the algorithm with k clusters sse k is the sum of squared error of members in cluster k and sse all is the sum of squared error of the entire dataset explained variance generally increases with the number of clusters the more clusters are used the smaller the differences between members within each cluster will be we use the elbow method to select the optimal number of clusters ketchen and shook 1996 here we calculate the difference of the explained variance δ e v k e v k e v k 1 we can then set a threshold t and determine the number of clusters where an additional cluster would yield δ e v k t as an optimal number of clusters for the particular algorithm we consider five clustering algorithms that are commonly used in model based analysis bandaru et al 2017 bárcena et al 2015 moallemi et al 2018 rohmer et al 2018 szekely and rizzo 2005 k means clustering k medoids clustering gaussian mixture model agglomerative clustering with complete linkage and agglomerative clustering with average linkage the combination of clustering algorithm and corresponding optimal number of clusters that yields the highest explained variance is selected for further analysis 2 2 2 input subspace identification phase we adopt the boosted trees algorithm to induce subspaces conditional on each class of the output space trindade et al 2019 boosted trees build upon cart by generating an ensemble of classification trees where each tree tries to minimize the impurity in the dataset by iteratively splitting the dataset into leaves de ath 2007 hastie et al 2009 schapire and freund 2012 a leaf is impure if it contains mixes of data points from different classes or in our case simulation results from different clusters we use the gini impurity criterion i g m k 1 k p m k 1 p m k where i g m is the gini impurity of leaf or node m k is the total number of classes of the output space and p m k is the proportion of scenarios with class k in node m in each iteration a classification tree looks for all possible splits across the input features and selects the one that yields the highest reduction in impurity boosted trees employ an ensemble of weak classification trees through multiple boosting iterations in each boosting iteration the algorithm readjusts the weights of misclassified data that are to be inputted to the weak classifier in the successive iterations freund and schapire 1997 hastie et al 2009 users control the algorithm by setting the maximum number of boosting iteration and limiting the complexity of individual trees pedregosa et al 2011 zhu et al 2009 the setup of boosted trees allows for calculating the relative importance of each input feature in each splitting iteration a classification tree uses one input feature to separate a parent node into two child nodes the importance of an input feature can be estimated as a function of how often a given feature is selected as the splitting variable and how much impurity reduction is achieved specifically the importance is measured by the normalized percentage of total impurity loss across all trees due to splits using the input feature finally for scenario discovery the most influential input features are mapped back to the identified clusters of the output space a technique often coined factor mapping trindade et al 2019 the factor maps can be used to visually construct rules or scenario narratives i e combinations of input parameters for each cluster of output space 2 3 concurrent approach multivariate regression trees mrts are an extension of univariate regression tree where multiple response variables are being used simultaneously to find candidate splits in each decision node de ath 2002 in each iteration mrt looks for the best split in the input features that leads to the largest reduction of impurity in the child nodes for regression problems the impurity of a node in terms of a single response variable is calculated as the summed euclidean distance between each data point to the mean of the response variable accordingly in mrt the total impurity of a node also termed the error of the node for regression trees is calculated as the summation of the impurity of each response variable e m n 1 n m j 1 j y i j y j m 2 where e m is the error or impurity of node m n m is the total number of data points in node m j is the total number of response variables y i j is the value of response variable j from data point i and y j m is the mean of response variable j across data points in node m the algorithm looks for the optimal split in the input space that yields the lowest sum of errors from the two child nodes in our application the leaves from the tree will directly turn into the clusters of inequality patterns this is because the splitting criterion in mrt is intended to minimize the similarity of outcome variables between the child nodes while maximizing the similarity within the child nodes to maintain interpretability it is important to balance the size of the tree with the purity of the tree the size of the tree the tree depth in an mrt is externally determined by the user by specifying a stopping criterion such as the maximum number of leaves or the minimum impurity of the leaves breiman et al 1984 de ath 2002 pedregosa et al 2011 we use a 10 fold cross validation technique to decide the appropriate depth of the tree larsen and speckman 2004 in each fold the algorithm is trained on 90 of the data and the accuracy of the resulting tree is tested on the rest 10 of the data the accuracy is indicated by the coefficient of determination score r 2 1 s s e r e s s s e a l l where s s e r e s is the sum of squared error between the predicted values and the actual values of the response variables while s s e a l l is the sum of squared error between the actual values and the mean values of the response variables in the entire dataset the accuracy of an mrt will increase with the depth of the tree hence we also calculate the changes in accuracy and attempt to balance this with the complexity of the tree ndong et al 2020 smith et al 2019 the selected tree depth is the one that has changes in accuracy smaller than a specified threshold t the resulting decision tree can be analyzed and visually inspected starting from either the leaves or the root smith et al 2019 in leaves first analysis users begin with looking for the leaf that contains certain patterns of interest the analysis then goes up the decision tree to understand conditions i e combinations of input parameters and their values that lead to the leaf of interest in root first analysis users start from the very first decision node at the top of the tree and go down the tree to explore a specific scenario leaves first analysis is a bottom up approach to reading a decision tree while root first analysis is a top down approach note that leaves first and root first analyses are concerned with how we read the mrt results hence the choice between these two does not alter the results of the algorithm itself fig 2 summarizes how the two main steps in multiclass scenario discovery i e output space partitioning and input subspaces identification are carried out in the sequential and the concurrent approach through iteratively minimizing the impurity of the child nodes the mrt partitions the output space to find distinctive patterns of outcomes at the same time the input features used to split each parent node as well as the splitting value of these features are used to construct narratives behind each final child nodes of the tree 3 case study 3 1 adaptation planning in the upper vietnam mekong delta vmd the vmd located in the southern part of the country is one of the largest deltas in the world the delta supplies 55 of total rice production and 85 of total rice export of vietnam gso 2019 toan 2014 the upstream part of the delta including an giang and dong thap provinces see fig 3 a is subject to annual monsoon flooding which could be worsened by climate change hoang et al 2019 triet et al 2020 flood risks are further exacerbated by land subsidence of which 7 17 mm year has been attributed to agricultural activities minderhoud et al 2018 sediment starvation puts another pressure on the delta further development of hydropower dams in cambodia which is located upstream of the vmd reduces sediment concentration in the river which has been one of the main sources of free nutrients for farmers in the vmd lauri et al 2012 manh et al 2015 the agricultural sector in the vmd has experienced several transitions in the past decades the construction of water resources infrastructure allowed farmers to harvest twice a year double rice cropping the winter spring crop between december and march and the summer autumn crop between april and july ngan et al 2018 son et al 2013 dikes of around 2 m high were initially constructed but they do not protect the paddy fields against flooding during the annual peak discharges in the monsoon season to facilitate further intensification of the agriculture sector the government has been constructing high dikes of 4 5 m since the early 2000s protecting the fields against monsoon flooding and thus enabling farmers to have a third cropping season triple rice cropping see fig 3b recently it was found that the high dikes expansion policy has unintended consequences for environmental sustainability garschagen et al 2012 tran et al 2018 and for inequality between richer and poorer farmers chapman et al 2016 in this study we evaluate the spatial inequality of farm profitability specifically we look at how different spatial inequality patterns at a district level emerge from different combinations of anthropogenic pressure climatic change and implementation of alternative adaptation policies this allows us to provide spatially explicit policy advice and administrative area based recommendations for local decision makers our study complements previous inequality studies in the region that focus on the distributional outcomes from a household point of view i e comparing poor and rich farmers at an individual household level chapman and darby 2016 chapman et al 2016 3 2 integrated assessment metamodel we used a spatially explicit integrated assessment metamodel to simulate the profitability of the farmers in an giang and dong thap provinces combining previously established complex models jafino et al 2021a fig 4 shows the general conceptualization of the model in short the model operates with a spatial resolution of 200m where each cell is represented by a particular land use function e g single rice double rice triple rice orchard plantation or aquaculture profitability is then calculated for each cell based on income from selling rice and cost of purchasing fertilizer we assume that nutrients are the limiting factors of rice yield which is the case in most southeast asian countries sattari et al 2014 witt et al 1999 inundation plays two opposing roles on the one hand unintentional inundation for instance due to extremely high discharge in monsoon seasons reduces the total annual rice yield on the other hand inundation supplies free nutrients through floodplain sedimentation flood risks are reduced in areas with higher dikes and are increased by land subsidence which in turn is dependent on the land use dynamics finally the land use module simulates farmers behavior of changing cropping practices especially between double rice cropping triple rice cropping orchard plantation and aquaculture future distributional outcomes are evaluated at a district level hence the cell level profitability is aggregated for each of the 23 districts in an giang and dong thap provinces the model is run with an annual time step from 2012 to 2050 the detailed model description validation and fit for purpose assessment are described in jafino et al 2021a 3 2 1 adaptation measures we tested both hard infrastructural and soft non infrastructural policies that affect the different modules within the model the infrastructural policies are related to dike de construction these policies are drawn from the recent flood control debates in the region either further expansion of high dikes or deconstructing all established high dikes into low dikes käkönen 2008 tran et al 2018 triet et al 2020 these policies are applied in an giang and dong thap independently resulting in a total of four alternative policies the first soft policy is a seeds upgrade policy we assume that by using a better seed variety the crops become more resilient to floods we model this by reducing the steepness of the stage damage curve dutta et al 2003 triet et al 2018 so that the same level of inundation results in a lower yield reduction the second policy is fertilizer subsidies where 50 kg of free fertilizer are distributed to farmers in each cropping season free fertilizer is given to farmers located far from the river as they get a significantly lower nutrients concentration from floodplain sedimentation manh et al 2013 2014 3 2 2 uncertainties there are five key uncertain factors affecting the productivity of the agricultural sector in the upper vmd the first uncertain factor is future annual peak discharge that affects flood risk we use synthetic future hydrographs of the mekong river generated by a global hydrological model driven by climatic data from two scenarios rcp 4 5 and 8 5 sutanudjaja et al 2018 winsemius et al 2013 the second uncertain factor is the hydropower dam development upstream in cambodia this factor affects the annual peak discharge and reduces total sediment supply to the vmd as the dams trap the sediment upstream we use five dam development scenarios as worked out by lauri et al 2012 and manh et al 2015 the next two factors are the productivity gap among the three seasons the winter spring season that starts in december just after the wet monsoon season is the most productive season the summer autumn season and the autumn winter season are less productive due to the limited water content in the soil in the former and the high degree of precipitation in the latter in 2002 the summer autumn season and the autumn winter season in dong thap produced 38 and 50 fewer yield per hectare respectively in 2016 the productivity gap has been reduced to only 26 and 35 for the summer autumn and autumn winter season respectively in this study we consider a wide range of plausible future productivity gap between 15 and 45 the last uncertain factor is the society s preference toward the different rice cropping system and the spatial plan for the region this factor affects future land use demand which in turn is spatially allocated by the land use change module we consider four scenarios based on the competing narratives of agriculture intensification in the vmd as well as based on the mekong delta plan mekong delta plan consortium 2013 tran et al 2018 triet et al 2018 continuing intensification higher triple rice cropping demand and lower double rice cropping demand reverting to double rice the opposite of the first scenario rising non rice preferences higher demand for alternative livelihoods such as orchard plantation aquaculture and shrimp rice farming and increasing urbanization higher demand for residential area 3 2 3 experimental setup the setup of the case study is summarized using the xlrm framework lempert et al 2003 in fig 5 to allow for an exhaustive exploration of plausible combinations of uncertainties and policies we apply full factorial sampling to input factors that are categorical and ordinal i e we sample all possible combinations of categorical and ordinal input factors these factors include the six policy variables and some of the uncertain variables i e river discharge and farming practice preference we combine the full factorial sample with a latin hypercube sampling of the productivity gap uncertainties as the values for these uncertainties take a continuous range this experimental setup results in a total of 43200 computational experiments the exploratory modelling workbench kwakkel 2017 is used to perform these experiments 3 3 post processing of simulation results the clustering phase in the sequential approach and the calculation of error in the concurrent approach require the computation of distance between the outcomes of each scenario in the simulation results database to avoid having one outcome variable dictating the distance calculation the values of each outcome variable are usually normalized to 0 1 across the scenarios e g giudici et al 2020 smith et al 2019 normalization of each outcome variable across the entire scenarios when doing explorative analysis of distributional outcomes is problematic the outcome variables are the outcomes for each district by doing a normalization we lose sights of the relative performance of each district compared to all other districts within each scenario see fig 6 a and b hence we calculate instead the relative profitability of each district i e the 0 1 normalization is applied between the performance of each district within each scenario instead of across scenarios see fig 6c in this way we maintain the information regarding the relative winners and losers in each scenario as a result the clustering algorithm is forced to look for distinctive inequality patterns 4 results 4 1 sequential approach the first step in the sequential approach is clustering the output space into a number of representative inequality patterns we test five alternative clustering algorithms while varying the number of clusters see appendix a for details we find that the k means algorithm with seven clusters yields the most satisfactory performance which balances the explained variance and the number of final clusters the remainder of the sequential approach is thus based on the results from this clustering setup fig 7a shows the seven representative inequality patterns from each cluster of the output space the representative scenario is taken from the medoid of the corresponding cluster that is the scenario which outcomes have the smallest euclidean distance to all other scenarios in the cluster at a glance we can see that cluster 2 3 6 and 7 have similar inequality patterns where three districts located around the mid northeastern part of the region have a higher relative profitability of higher than 0 7 the patterns are different once we inspect them in more detail for example in cluster 3 the district located in the top northwestern part of the region is not relatively better off in cluster 6 this district is significantly better off compared to the others relative profitability 1 next we use the boosted trees algorithm to first identify the most critical input features that best explain the seven clusters of inequality patterns fig 7b shows the results of the input feature scoring the most important input feature is the degree of upstream dam development followed by three dikes construction policies expansion of high dikes in an giang in dong thap and reverting back to low dikes in an giang the other input features have substantially lower importance scores we use the four most important input features to map back the input space to the seven clusters of output space the importance scores of these four features add up to 0 705 implying that these features contribute to 70 5 of the total impurity reduction in the entire ensemble of trees fig 7c shows the factor map for each cluster where the cluster numbering corresponds to the seven inequality patterns in fig 7a since three of the four most important features are related to dike construction policies we combine them into a single axis i e the vertical axis on fig 7c the numbers underlying the heatmap correspond to the fraction of scenarios in that particular cluster for example 20 of the 7879 scenarios in cluster 1 are scenarios with high upstream dam development while maintaining the current dikes configuration in the vmd another 20 of the scenarios have a combination of high upstream dam development and low dikes policy in an giang fig 7c shows that cluster 1 which has one of the more distinctive inequality patterns see fig 7a is primarily induced through a combination of high sediment trapping due to upstream dam development and either expansion of high dikes in an giang or the preservation of current dikes inequality pattern as exemplified by cluster 4 is caused by the transformation of high dikes back into low dikes in an giang together with a high degree of upstream dam development cluster 2 and 7 which have similar inequality patterns emerge if upstream dam development is relatively low and either the low dikes policy in an giang is enacted or the current dikes system is maintained further construction of high dikes in an giang in combination with relatively low upstream dam development would lead to inequality patterns as depicted either in cluster 3 or 6 what do these results imply for adaptation planning in the vmd the most important insight is that the interaction between what the vmd government does in terms of dikes de construction and what the cambodian government does in terms of hydropower dams development has non linear effects on the emerging spatial distribution of farms profitability for instance a relatively small degree of upstream dam development would make provinces that expand their high dikes worse off this follows from comparing cluster 3 and 6 with cluster 5 in cluster 3 and 6 the high dikes policy in an giang is enacted and this makes districts within an giang relatively worse off in cluster 5 the high dikes policy in dong thap is also enacted and this leads to districts in dong thap becoming worse off this stresses the importance of having transboundary basin management in order to ensure equitable future for the vmd farmers 4 2 concurrent approach the first step in the concurrent approach is growing the regression tree and selecting an appropriate tree size we iteratively grow the tree from three to 40 leaves and observe the evolution of the cross validation scores see appendix b for details we find that the tree with 18 leaves yields the most satisfactory cross validation score and proceed with this tree size in the remainder of the concurrent approach for visualization purpose we separate the entire regression tree into two figures fig 8 and fig 9 together make up the entirety of the regression tree the first splitting variable identified by the mrt is the expansion of high dikes in an giang fig 8 shows the left branch of the tree high dikes in an giang is expanded while fig 9 shows the right branch of the tree high dikes in an giang is not expanded the number of scenarios and the representative inequality pattern from all scenarios in each leaf are provided at the bottom of the figures similar to the sequential approach before the medoid scenario in each leaf is assigned to be its representative inequality pattern it is important to restate here that in each scenario the profitability of the districts is normalized between 0 and 1 where darker green color means higher relative profitability here we illustrate how we can use either leaves first or root first analysis to interpret the results of the mrt we will use root first analysis to analyze the left branch of the tree and leaves first analysis for the right branch of the tree the left branch of the tree as shown in fig 8 contains scenarios where the high dikes policy in an giang is implemented for illustration we approach this side of the tree using root first analysis the subsequent decision node here is the degree of upstream dam development with a cutoff point of 2 5 we have six levels of upstream dam development with 0 2 being no to medium degree of upstream development and 3 5 being higher degrees of development if upstream dam development is relatively small the next deciding factors are the dikes policy in dong thap low dikes policy leads to inequality pattern in cluster 1 high dikes policy leads to cluster 2 while maintaining the current dikes distribution in dong thap leads to cluster 3 it is interesting to compare cluster 2 and cluster 4 as from the root first perspective the only difference is the degree of upstream dam development if high dikes are expanded in both provinces and many upstream dams are eventually built districts alongside the river will become substantially better off cluster 4 however a smaller degree of upstream dam construction will lead to a less striking difference in relative profitability cluster 2 cluster 6 although having a different narrative has a similar inequality pattern as cluster 4 even without expansion of high dikes in dong thap a very large degree of upstream dam development in combination with high dikes policy in an giang still make districts alongside the river better off the right branch of the tree contains the remaining 12 leaves fig 9 we approach the interpretation of this branch by following leaves first analysis we focus on three visually distinct inequality patterns the first pattern is typified by the higher relative profitability of districts alongside the river as observed in cluster 15 and 18 both clusters actually have a similar narrative where no dikes policy is taken and upstream dam development is relatively large the second distinct pattern is exemplified by cluster 8 and 9 in both clusters districts located to the north of the river have smaller relative profitability both clusters have a similar narrative of medium degree of upstream dam development and with high dikes being expanded in dong thap the third distinct pattern is observed in cluster 10 14 and cluster 16 the main pattern here is that there are three districts located to the north of the river three districts located to the south of the river and one district on the northwest corner of the region who are better off this pattern can emerge from multiple future conditions for example a condition for cluster 10 14 to materialize is no extremely high upstream dam development however cluster 16 shows that even if all planned upstream dams are built a similar inequality pattern could emerge if all dikes in both an giang and dong thap are reverted back into low dikes what can the vmd government learn from the concurrent approach through combining both root first and leaves first analyses we can clearly see that similar narratives could lead to distinctive patterns of spatial distribution at the same time similar distribution patterns could emerge from distinctive narratives the decision tree can easily help the government in understanding plausible inequality patterns and pathways that lead to those patterns and thus preparing additional measures to compensate worse off districts 5 discussion 5 1 input space and output space separability the induced input subspaces though scenario discovery have a perfect separability if each subspace is mutually exclusive with the others while traditionally this has been quantified through the density and coverage indicators bryant and lempert 2010 the use of these indicators is not applicable for sequential multiclass scenario discovery as presented here this is because in the sequential approach we do not set a strict boundary on the identified subspaces however from a visual inspection we can see that some of the identified subspaces are overlapping with each other see fig 7c for example the combination of current dikes and low to medium upstream dam development exists in the identified subspaces of cluster 2 and 7 in contrast the concurrent approach produces completely separable input subspaces as each end leaf has unique scenario narratives i e combination of the scenario variables therefore the concurrent approach leads to better input space separability compared to the sequential approach to quantify output space separability we calculate the average euclidean distance between the relative profitability of the 23 districts in all scenarios within each cluster within class dissimilarity as well as between scenarios from different clusters between class dissimilarity a better separability of output space thus entails low within cluster distance and high between cluster distance from table 1 we can see that neither approach is superior to the other the concurrent approach has better within cluster average distance compared to the sequential approach this can be explained by the more granular separation of the output space so that each cluster consists of more similar simulation results in contrast the sequential approach has better between cluster average distance compared to the concurrent approach this is explained by looking at the representative inequality patterns from the concurrent approach on figs 8 and 9 where there are many clusters that have similar inequality patterns to this end it is interesting to observe in more details the dis similarity of the resulting narratives from the two approaches 5 2 comparison of the resulting scenarios in this section we compare the clusters of inequality patterns from the two approaches as well as the narratives behind each cluster first we see that the clusters of inequality patterns from the concurrent approach have a higher degree of variation as there are several clusters that have a comparable pattern however most of the patterns identified from the concurrent approach are also present in the sequential approach for example the inequality patterns of cluster 4 6 15 and 18 from the concurrent approach are comparable to the inequality pattern of cluster 1 from the sequential approach table 2 lists the other pairs of similar inequality patterns identified by the two approaches two exceptions worth noting are cluster 15 and 16 from the concurrent approach in general cluster 16 has a similar inequality pattern to cluster 2 from the sequential approach however the most profitable districts in cluster 16 are the two districts in the westernmost part of the region furthermore the easternmost district is also slightly better off than many of the other districts cluster 15 has similar inequality pattern to cluster 1 from the sequential approach the difference is that many districts to the north of the river are also relatively better off in cluster 15 most of the narratives behind each inequality pattern identified by the two approaches are also comparable see table 2 for example cluster 4 and 5 from the sequential approach have similar narratives to their counterparts from the concurrent approach since in the concurrent approach we do not limit our analysis to only four most important factors this approach yields slightly richer and more detailed narratives for some of the clusters in the concurrent approach the low dikes policy in dong thap is identified as an important part of the narratives for some of the clusters i e cluster 1 10 and 16 from the concurrent approach the more aggregated results of the sequential approach conceal some diversity within the scenarios for example maintaining the current dike system in combination with a medium to high degree of upstream dam development is part of the narrative for cluster 6 from the sequential approach however the same narrative leads to different inequality patterns if we follow the decision tree from the concurrent approach i e cluster 14 and 15 this is because combining the inequality patterns from cluster 14 and 15 of the concurrent approach will average out the profitability of districts that are better off in each cluster resulting in a more equal distribution as exemplified by the representative inequality pattern of cluster 6 from the sequential approach 5 3 reflection for practice the comparisons above show that the concurrent approach outperforms the sequential approach however it comes with a caveat of having a larger number of final narratives this raises the question of whether the benefit of better input space separability in the concurrent approach does outweigh the drawback of having more clusters and narratives to answer this we need to first revisit the main purpose of scenario discovery itself which is to craft narratives about system outcomes under certain combinations of uncertainties polices bryant and lempert 2010 greeven et al 2016 lempert et al 2006 in particular attention needs to be given on the decision making contexts and on the use case of the narratives generated from the multiclass scenario discovery exercise past scenario discovery studies have a varying level of stakeholder involvement some studies indicate a relatively low degree of interactions with stakeholder e g hidayatno et al 2020 lamontagne et al 2018 moallemi et al 2017 in these studies the generated narratives are mainly aimed at defining plausible future pathways which are to be used by other institutions in other contexts other studies indicate a more frequent and thorough interactions e g groves et al 2019 hamarat et al 2013 trindade et al 2019 the aim of such studies is often more specific such as for stress testing alternative policies and identifying vulnerabilities accordingly narratives generated in such use cases are used solely for the purpose of the project the sequential approach with a relatively lower number of narratives is more suitable for the former type of use cases relatively little stakeholder engagement narratives to be transferred for other contexts the concurrent approach with better separability performance but more narratives is more suitable for the latter type of use cases more intense stakeholder engagement more focused analysis in addition to the characteristics of the use cases there are two further important points to note first an important strength of scenario discovery is to facilitate deliberation and this obviously requires thorough engagements with clients and stakeholders accordingly narratives from scenario discovery should not be shared as is with stakeholders rather the analyst should always be at the interface between the policy problem and the tool used to support the policy analysis cuppen et al 2021 so the issue with having a larger number of scenarios is that the analyst might have to do more work to distill the message from the analysis before conveying it to others and this can be done through consultation with the stakeholders second as some clusters from the concurrent approach have similar inequality patterns presenting them simultaneously might not be appropriate without the help of the results from the sequential approach the regrouping of similar clusters from the concurrent approach can be performed through either root first or leaves first analysis smith et al 2019 in leaves first analysis the key step is to identify clusters with similar representative inequality patterns this can be done qualitatively through visual inspection as done in table 2 or through consultation with stakeholders to aid this process the analyst can calculate the average distance between any pair of clusters and combine those with relatively low distance the final choice of the number of narratives should not be the analyst s call but instead decided in a participatory and interactive setting with stakeholders if what is of more interest is the narratives instead of the resulting inequality patterns the root first analysis can be followed instead 6 conclusion adaptation policies and uncertainties and the interactions between them almost unavoidably yield unequal consequences to different people the task of exploring future inequality patterns and understanding their drivers fits the nature of scenario discovery in scenario discovery one maps back the output space of a model in this case inequality patterns with its input space policy levers and exogenous uncertain factors in this study we contribute to the advancement of scenario discovery in two ways first we propose two novel criteria to evaluate the quality of multiclass scenario discovery results output space separability and the number of resulting narratives second we propose a novel concurrent approach for multiclass scenario discovery by using multivariate regression trees mrt using agriculture adaptation planning for the vietnam mekong delta as a case study we demonstrate the application of both the established sequential and the novel concurrent approach for multiclass scenario discovery we find that the concurrent approach performs considerably better in terms of input space separability the mrt algorithm guarantees a perfect separation of the input space when clustering the simulation results this however does result in a larger number of clusters of output space and subsequently narratives while the sequential approach results in seven scenarios the concurrent approach produces eighteen scenarios both approaches have a fairly comparable output space separability performance with the sequential approach results in better between cluster dissimilarity and the concurrent approach results in better within class similarity despite the differences in performance we show how most of the narratives and representative inequality patterns identified by the two approaches are similar with some exceptions the concurrent approach provides richer insights as it unravels two additional representative inequality patterns that are not captured by the sequential approach based on the case study results we argue for the use of the concurrent approach for future multiclass scenario discovery the concurrent approach guarantees perfect input space separability without sacrificing too much in terms of output space separability furthermore the concurrent approach captures richer and more distinctive trade off patterns between outcome variables in our case inequality patterns compared to the sequential approach one caveat is that the concurrent approach requires one to make extra effort to distill insights from these richer results in light of the presented results we see several directions for future research the first one is related to the selection of representative inequality patterns in this study we take a pragmatic approach by using the medoid scenario in each cluster other approaches include averaging the relative profitability of each actor across all scenarios in a cluster or selecting the scenario which has the most distinctive inequality pattern relative to the other clusters carlsen et al 2016 the second direction is assessing the limits and scalability of clustering when a higher number of stakeholders which leads to a larger number of outcome variables is considered while alternative high dimensional clustering techniques are available kriegel et al 2009 xu and tian 2015 their usefulness in the context of scenario discovery remains to be evaluated the third direction is to assess the impacts of different spatial aggregations as we aggregate farms profitability at a district level within district inequality is ignored the statistical bias resulting from spatial aggregation well known as the modifiable areal unit problem fotheringham and wong 1991 can have profound implications for the emerging spatial pattern sensitivity or robustness analysis could be applied to understand the stability of the representative inequality patterns under different aggregation levels inequalities can be viewed from various dimensions across people in different locations interregional with different income different socioeconomic background or across actors and variables inequality of profitability benefits from policies exposure to and impacts of climate change harrison et al 2016 jafino et al 2021b rao et al 2017 irrespective of the dimension and variable of inequality there is still a methodological need to explore plausible inequality patterns to support equitable adaptation planning for the purpose of showing the merits of multiclass scenario discovery for this methodological need we used one dimension of inequality interregional inequality of profitability without loss of generality the sequential and concurrent approaches could be applied to other conceptualizations of inequality as we only need to slice the population differently based on our variables of interest however it is important to highlight the limitation of this approach in planning for climate change distributional consequences can be seen from intra generational between people and assuming they live within the same generation and intergenerational between generations perspectives jafino et al 2021b multiclass scenario discovery is applicable for exploring intra generational but not intergenerational inequalities the topic of discounting is more applicable for the latter with recent works proposing alternative discounting methods that account for equity asheim 2017 dietz and asheim 2012 software and data availability the code for conducting both sequential and concurrent approaches as well as the data required to perform the analysis can be accessed at https github com bramkaarga inequality pattern exploration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by nwo top sector water call adaptation pathways for socially inclusive development of urbanizing deltas research number ond1362814 appendix a details of clustering results from the sequential approach the first step in the sequential approach is determining the clustering algorithm and the number of clusters to proceed with for each algorithm we perform clustering with an increasing number of clusters from 2 to 21 we calculate the explained variance for each number of cluster fig 10 by sweeping across different numbers of clusters we can observe the progression and the convergence of the explained variance at the end of the iteration i e with 21 clusters the explained variance from all algorithms clusters converges to 0 8 as explained in the methods section we set a threshold of 0 05 for the changes in explained variance in order to select an optimal number of clusters from each algorithm note that the selection of the threshold t for the changes in explained variance is a subjective choice we need to balance the explained variance of the selected number of clusters at which the threshold t is being met the potential gain in explained variance when using a higher number of clusters and the potential loss in interpretability when a higher number clusters is used table 3 shows the number of clusters from each clustering algorithm when the threshold t 0 05 is met and the corresponding explained variance k means algorithm yields the best performance it performs slightly better than k medoids and clearly outperforms the other clustering algorithms its explained variance of 0 711 is also not too distant from the overall explained variance convergence of 0 8 hence in the remainder of this sequential approach we proceed with the 7 clusters of output space as identified by the k means algorithm appendix b details of tree selection in the concurrent approach in the concurrent approach we start directly with determining the size of the tree based on the evolution of the cross validation scores fig 11 shows the increase of the 10 fold cross validation scores with increasing number of leaves similar to the clustering results in the sequential approach the cross validation score seems to converge to 0 8 however the cross validation score has not stagnated yet even after being grown to having 40 leaves as the score keeps increasing even after the tree has become quite complex it is advised to set a threshold of increase in cross validation scores in order to select an appropriate tree size smith et al 2019 we choose a threshold of 0 01 dashed line on fig 11 and this threshold is reached when the number of leaves is 18 
25719,to support equitable planning model based analyses can be used to explore inequality patterns arising from different scenarios scenario discovery is increasingly used to extract insights from ensembles of simulation here we apply two scenario discovery approaches for unraveling inequality patterns and their drivers with an application to spatial inequality of farms profitability in the vietnam mekong delta first we follow an established sequential approach where we begin with clustering the inequality patterns from the simulation results and next identify model input subspaces that best explain each cluster second we propose a novel concurrent approach using multivariate regression trees to simultaneously classify inequality patterns and identify their corresponding input subspaces both approaches have comparable output space separability performance the concurrent approach yields significantly better input space separability but this comes at the expense of having a larger number of subspaces requiring analysts to make extra effort to distill policy relevant insights keywords model based decision support fairness adaptation deep uncertainty equity 1 introduction recent model based studies for supporting climate planning have advocated for assessing distributional outcomes of alternative policies see e g gourevitch et al 2020 kind et al 2017 rao 2013 this is because evaluating policies using aggregate metrics can be misleading as a policy that is optimal from an aggregate point of view might actually benefit some people at the expense of the others hansson 2007 rao et al 2017 sayers et al 2018 looking only from an aggregate point of view can introduce or even exacerbate inequalities furthermore there exists uncertainty in not only the magnitude and the spatial distribution of climate change but also in the differential exposure vulnerability and adaptive capacity of the people and how these factors evolve over time green 2016 o neill et al 2017 thomas et al 2019 this makes it even more crucial to assess ex ante the distributional consequences of adaptation and mitigation policies there are two types of analyses for assessing distributional outcomes the first one is normative analysis here the aim is to identify a policy that best satisfies a moral principle for instance in climate change mitigation the polluters pay principle and the equal per capita entitlements are two often used imperatives for allocating mitigation responsibility gardiner 2010 okereke 2010 in adaptation the use of differentiated historical responsibility has been proposed for determining funding responsibility grasso 2007 whereas putting the most vulnerable first has been proposed for distributing benefits paavola and adger 2006 these principles can be operationalized for use in quantitative model based studies for example adler et al 2017 operationalize the prioritarian principle giving higher weights to outcomes experienced by worse off people for calculating the social cost of carbon the second type is explorative analysis rather than putting value judgements on whether the distribution of outcomes is morally acceptable explorative analysis aims to identify groups who become better off and worse off because of the implementation of policies there are various ways to define population subgroups for example ciullo et al 2020 look at the distribution of flood risk reduction benefits across people living in different locations i e dike rings by identifying potential winners and losers explorative analysis can help planners in anticipating unintended distributive consequences and ameliorating potential injustices for instance by preparing compensation measures to worse off actors when performing explorative analysis the analyst faces an interpretation problem arising out of two concerns jafino et al 2021b first identifying inequality patterns requires calculating the outcomes experienced by individual actors leading to a larger number of performance indicators this sometimes requires a modification to the model structure rao et al 2017 and how model outputs are treated franssen 2005 kasprzyk et al 2016 second the fact that distributional outcomes can vary substantially under different futures necessitates the exploration of inequality pattern across a large ensemble of scenarios schweizer 2018 taconet et al 2020 taken together the large ensemble of scenarios and the high dimensionality of the output space make it hard to distill policy relevant insights about the different plausible modes of inequality patterns and the associated policies and uncertainties under which the different modes arise scenario discovery is an approach for deriving policy relevant insights from large ensembles of simulation results bryant and lempert 2010 groves and lempert 2007 scenario discovery process begins with generating simulation results database through running the model under a large number of scenarios bankes 1993 moallemi et al 2020 and proceeds with identifying combinations of driving forces that lead to a certain pattern of model outcomes scenario discovery answers the question under which conditions or scenarios do the model outcomes behave in a certain way scenario discovery by now is a recognized approach to deal with deep uncertainty in model based planning for climate change and to make sense of large scale computational experiment see e g guivarch et al 2016 herman et al 2015 knox et al 2018 lamontagne et al 2018 moallemi et al 2017 rozenberg et al 2014 weaver et al 2013 traditional applications of scenario discovery include policy stress testing and vulnerability analysis e g eker and van daalen 2015 halim et al 2015 hidayatno et al 2020 shortridge and zaitchik 2018 as part of many objective robust decision making bartholomew and kwakkel 2020 kasprzyk et al 2013 the main objective here is identifying conditions under which a policy fails to meet its objectives this requires users to set a threshold for classifying policy success if the performance of the policy exceeds or goes below in case of a maximization problem the threshold the policy is deemed to fail in reaching its objectives in this established application of scenario discovery one applies a binary classification to the model output space from the simulation results database by dividing the output space into a region where the policy performance meets the minimal requirement and a region where it fails to do so a rule induction algorithm is then applied to identify combinations of input parameters that lead to the vulnerable region in the output space in this study we investigate the merits of using multiclass scenario discovery an extension of the standard binary class scenario discovery for performing explorative analysis of distributional outcomes in multiclass scenario discovery the model output space is partitioned into multiple clusters and the input subspaces for each cluster are then identified multiclass scenario discovery is appropriate for explorative analysis of distributional outcomes as there might be numerous modes of inequality in the future we cannot simply impose a binary classification on the distributional outcomes distinctive inequality patterns might emerge but due to system complexity and non linearity similar patterns might arise from completely distinct uncertainty and policy scenarios jafino et al 2021a we explore two alternative approaches to multiclass scenario discovery first we adapt the cluster then identify approach as has been used in previous multiclass scenario discovery studies gerst et al 2013 rozenberg et al 2014 steinmann et al 2020 in this approach the clustering of the model output space is performed first followed by the identification of input subspaces for each cluster separately this can negatively affect interpretability because different clusters in the output space might be linked to overlapping subspaces of the input space to address this we propose and test the use of multivariate regression tree mrt for multiclass scenario discovery in this second approach the output space clustering and input subspace identification are solved concurrently through the mrt algorithm we apply both the established sequential and the novel concurrent approach for multiclass scenario discovery to an agriculture adaptation planning problem for the upper vietnam mekong delta vmd we explore spatial inequality of district level farms profitability resulting from different realizations of uncertainties and implementation of adaptation measures the rest of the paper is structured as follows in section 2 we describe the two approaches of multiclass scenario discovery and explain further the concept of input and output space separability in section 3 we provide the background of the case study and introduce the model that is being used in section 4 we present the results of the two approaches in section 5 we discuss the merits of each approach i e their performance in terms of input and output space separability as well as the resulting scenario narratives identified by each approach in section 6 we summarize our main findings and insights 2 methods 2 1 multiclass scenario discovery there are a number of scenario discovery applications that extend the output space partitioning from binary classification to multiclass classification gerst et al 2013 kwakkel and jaxa rozen 2016 rozenberg et al 2014 steinmann et al 2020 a major difference between traditional scenario discovery and multiclass scenario discovery lies in the characterization of the output space in traditional scenario discovery the output space is partitioned into only two classes those which are of interest and those which are not kwakkel et al 2013 in contrast in multiclass scenario discovery the output space is partitioned into more than two classes multiclass scenario discovery involves two tasks the output space has to be partitioned into multiple distinct classes and for each class input subspaces which are highly predictive for it have to be identified the highly predictive input subspaces form the narrative behind each class in the output space for the first task partitioning the output space various approaches for specifying the classes have been used classification can be performed by either manually imposing a threshold on the outcome variables e g guivarch et al 2016 rozenberg et al 2014 or by using a clustering algorithm to automatically identify the classes e g berntsen and trutnevyte 2017 gerst et al 2013 moallemi et al 2017 steinmann et al 2020 in the manual threshold approach the analyst has full control over how the output space is partitioned thus enhancing the interpretability of the resulting classes however the task becomes increasingly complex with increasing number of outcome variables in contrast clustering algorithms can handle a larger set of outcome variables but at the expense of worsening interpretability for the second task identifying highly predictive input subspaces both the patient rule induction method prim friedman and fisher 1999 and classification and regression tree cart algorithms breiman et al 1984 have been widely used for multiclass scenario discovery prim is iteratively and independently applied to each cluster of the output space see e g rozenberg et al 2014 in contrast cart can identify highly predictive input subspaces for multiple clusters of the output space simultaneously by predicting the membership of each scenario in one of the identified clusters see e g gerst et al 2013 the partitioning of the output space and the identification of highly predictive input subspaces are traditionally performed sequentially in this study we propose the use of multivariate regression tree mrt for multiclass scenario discovery to concurrently perform these two tasks mrt is an extension of cart where multiple dependent variables are being used to characterize the impurity of a decision node de ath 2002 mrt has previously been used for model based analysis such as for unraveling tradeoffs and synergies between management objectives ndong et al 2020 smith et al 2019 for multiclass scenario discovery the input parameters of the simulation model become the independent variables of the mrt while the outcome variables of interest become the dependent variables the leaves resulting from the regression tree then act as the clusters of the output space the variables being used in each decision node and their corresponding splitting values form the narrative behind each cluster of output space scenario discovery enables the extraction of policy relevant insights e g exploring plausible modes of inequality patterns from large scale computational experiments by making the large ensemble of simulation results interpretable the interpretability of multiclass scenario discovery can be evaluated using three criteria the first criterion is output space separability which is similar to the objective of clustering algorithms hastie et al 2009 jain 2010 after clustering the output space members within the same cluster should have similar outcome characteristics e g spatial inequality patterns while members from different clusters should be dissimilar the second criterion is input space separability steinmann et al 2020 which focuses on the rule induction part of scenario discovery each class of outcome should originate from distinct and non overlapping subspaces in the input space as illustrated in fig 1 scenario discovery results are ideal if the identified input and output subspaces are completely separable i e if each cluster in the output space is distinctive from the other clusters and is driven by distinctive subspaces in the input space the third criterion is the resulting number of scenario narratives having a larger number of clusters generally leads to better output space separability hastie et al 2009 but it comes at the expense of having more complicated narratives to be communicated to decision makers 2 2 sequential approach cluster then identify 2 2 1 clustering phase the clustering phase aims to find distinctive patterns of outcomes within the simulation results clustering performance is evaluated by the explained variance e v k 1 k 1 k s s e k s s e a l l where evk is the explained variance of the algorithm with k clusters sse k is the sum of squared error of members in cluster k and sse all is the sum of squared error of the entire dataset explained variance generally increases with the number of clusters the more clusters are used the smaller the differences between members within each cluster will be we use the elbow method to select the optimal number of clusters ketchen and shook 1996 here we calculate the difference of the explained variance δ e v k e v k e v k 1 we can then set a threshold t and determine the number of clusters where an additional cluster would yield δ e v k t as an optimal number of clusters for the particular algorithm we consider five clustering algorithms that are commonly used in model based analysis bandaru et al 2017 bárcena et al 2015 moallemi et al 2018 rohmer et al 2018 szekely and rizzo 2005 k means clustering k medoids clustering gaussian mixture model agglomerative clustering with complete linkage and agglomerative clustering with average linkage the combination of clustering algorithm and corresponding optimal number of clusters that yields the highest explained variance is selected for further analysis 2 2 2 input subspace identification phase we adopt the boosted trees algorithm to induce subspaces conditional on each class of the output space trindade et al 2019 boosted trees build upon cart by generating an ensemble of classification trees where each tree tries to minimize the impurity in the dataset by iteratively splitting the dataset into leaves de ath 2007 hastie et al 2009 schapire and freund 2012 a leaf is impure if it contains mixes of data points from different classes or in our case simulation results from different clusters we use the gini impurity criterion i g m k 1 k p m k 1 p m k where i g m is the gini impurity of leaf or node m k is the total number of classes of the output space and p m k is the proportion of scenarios with class k in node m in each iteration a classification tree looks for all possible splits across the input features and selects the one that yields the highest reduction in impurity boosted trees employ an ensemble of weak classification trees through multiple boosting iterations in each boosting iteration the algorithm readjusts the weights of misclassified data that are to be inputted to the weak classifier in the successive iterations freund and schapire 1997 hastie et al 2009 users control the algorithm by setting the maximum number of boosting iteration and limiting the complexity of individual trees pedregosa et al 2011 zhu et al 2009 the setup of boosted trees allows for calculating the relative importance of each input feature in each splitting iteration a classification tree uses one input feature to separate a parent node into two child nodes the importance of an input feature can be estimated as a function of how often a given feature is selected as the splitting variable and how much impurity reduction is achieved specifically the importance is measured by the normalized percentage of total impurity loss across all trees due to splits using the input feature finally for scenario discovery the most influential input features are mapped back to the identified clusters of the output space a technique often coined factor mapping trindade et al 2019 the factor maps can be used to visually construct rules or scenario narratives i e combinations of input parameters for each cluster of output space 2 3 concurrent approach multivariate regression trees mrts are an extension of univariate regression tree where multiple response variables are being used simultaneously to find candidate splits in each decision node de ath 2002 in each iteration mrt looks for the best split in the input features that leads to the largest reduction of impurity in the child nodes for regression problems the impurity of a node in terms of a single response variable is calculated as the summed euclidean distance between each data point to the mean of the response variable accordingly in mrt the total impurity of a node also termed the error of the node for regression trees is calculated as the summation of the impurity of each response variable e m n 1 n m j 1 j y i j y j m 2 where e m is the error or impurity of node m n m is the total number of data points in node m j is the total number of response variables y i j is the value of response variable j from data point i and y j m is the mean of response variable j across data points in node m the algorithm looks for the optimal split in the input space that yields the lowest sum of errors from the two child nodes in our application the leaves from the tree will directly turn into the clusters of inequality patterns this is because the splitting criterion in mrt is intended to minimize the similarity of outcome variables between the child nodes while maximizing the similarity within the child nodes to maintain interpretability it is important to balance the size of the tree with the purity of the tree the size of the tree the tree depth in an mrt is externally determined by the user by specifying a stopping criterion such as the maximum number of leaves or the minimum impurity of the leaves breiman et al 1984 de ath 2002 pedregosa et al 2011 we use a 10 fold cross validation technique to decide the appropriate depth of the tree larsen and speckman 2004 in each fold the algorithm is trained on 90 of the data and the accuracy of the resulting tree is tested on the rest 10 of the data the accuracy is indicated by the coefficient of determination score r 2 1 s s e r e s s s e a l l where s s e r e s is the sum of squared error between the predicted values and the actual values of the response variables while s s e a l l is the sum of squared error between the actual values and the mean values of the response variables in the entire dataset the accuracy of an mrt will increase with the depth of the tree hence we also calculate the changes in accuracy and attempt to balance this with the complexity of the tree ndong et al 2020 smith et al 2019 the selected tree depth is the one that has changes in accuracy smaller than a specified threshold t the resulting decision tree can be analyzed and visually inspected starting from either the leaves or the root smith et al 2019 in leaves first analysis users begin with looking for the leaf that contains certain patterns of interest the analysis then goes up the decision tree to understand conditions i e combinations of input parameters and their values that lead to the leaf of interest in root first analysis users start from the very first decision node at the top of the tree and go down the tree to explore a specific scenario leaves first analysis is a bottom up approach to reading a decision tree while root first analysis is a top down approach note that leaves first and root first analyses are concerned with how we read the mrt results hence the choice between these two does not alter the results of the algorithm itself fig 2 summarizes how the two main steps in multiclass scenario discovery i e output space partitioning and input subspaces identification are carried out in the sequential and the concurrent approach through iteratively minimizing the impurity of the child nodes the mrt partitions the output space to find distinctive patterns of outcomes at the same time the input features used to split each parent node as well as the splitting value of these features are used to construct narratives behind each final child nodes of the tree 3 case study 3 1 adaptation planning in the upper vietnam mekong delta vmd the vmd located in the southern part of the country is one of the largest deltas in the world the delta supplies 55 of total rice production and 85 of total rice export of vietnam gso 2019 toan 2014 the upstream part of the delta including an giang and dong thap provinces see fig 3 a is subject to annual monsoon flooding which could be worsened by climate change hoang et al 2019 triet et al 2020 flood risks are further exacerbated by land subsidence of which 7 17 mm year has been attributed to agricultural activities minderhoud et al 2018 sediment starvation puts another pressure on the delta further development of hydropower dams in cambodia which is located upstream of the vmd reduces sediment concentration in the river which has been one of the main sources of free nutrients for farmers in the vmd lauri et al 2012 manh et al 2015 the agricultural sector in the vmd has experienced several transitions in the past decades the construction of water resources infrastructure allowed farmers to harvest twice a year double rice cropping the winter spring crop between december and march and the summer autumn crop between april and july ngan et al 2018 son et al 2013 dikes of around 2 m high were initially constructed but they do not protect the paddy fields against flooding during the annual peak discharges in the monsoon season to facilitate further intensification of the agriculture sector the government has been constructing high dikes of 4 5 m since the early 2000s protecting the fields against monsoon flooding and thus enabling farmers to have a third cropping season triple rice cropping see fig 3b recently it was found that the high dikes expansion policy has unintended consequences for environmental sustainability garschagen et al 2012 tran et al 2018 and for inequality between richer and poorer farmers chapman et al 2016 in this study we evaluate the spatial inequality of farm profitability specifically we look at how different spatial inequality patterns at a district level emerge from different combinations of anthropogenic pressure climatic change and implementation of alternative adaptation policies this allows us to provide spatially explicit policy advice and administrative area based recommendations for local decision makers our study complements previous inequality studies in the region that focus on the distributional outcomes from a household point of view i e comparing poor and rich farmers at an individual household level chapman and darby 2016 chapman et al 2016 3 2 integrated assessment metamodel we used a spatially explicit integrated assessment metamodel to simulate the profitability of the farmers in an giang and dong thap provinces combining previously established complex models jafino et al 2021a fig 4 shows the general conceptualization of the model in short the model operates with a spatial resolution of 200m where each cell is represented by a particular land use function e g single rice double rice triple rice orchard plantation or aquaculture profitability is then calculated for each cell based on income from selling rice and cost of purchasing fertilizer we assume that nutrients are the limiting factors of rice yield which is the case in most southeast asian countries sattari et al 2014 witt et al 1999 inundation plays two opposing roles on the one hand unintentional inundation for instance due to extremely high discharge in monsoon seasons reduces the total annual rice yield on the other hand inundation supplies free nutrients through floodplain sedimentation flood risks are reduced in areas with higher dikes and are increased by land subsidence which in turn is dependent on the land use dynamics finally the land use module simulates farmers behavior of changing cropping practices especially between double rice cropping triple rice cropping orchard plantation and aquaculture future distributional outcomes are evaluated at a district level hence the cell level profitability is aggregated for each of the 23 districts in an giang and dong thap provinces the model is run with an annual time step from 2012 to 2050 the detailed model description validation and fit for purpose assessment are described in jafino et al 2021a 3 2 1 adaptation measures we tested both hard infrastructural and soft non infrastructural policies that affect the different modules within the model the infrastructural policies are related to dike de construction these policies are drawn from the recent flood control debates in the region either further expansion of high dikes or deconstructing all established high dikes into low dikes käkönen 2008 tran et al 2018 triet et al 2020 these policies are applied in an giang and dong thap independently resulting in a total of four alternative policies the first soft policy is a seeds upgrade policy we assume that by using a better seed variety the crops become more resilient to floods we model this by reducing the steepness of the stage damage curve dutta et al 2003 triet et al 2018 so that the same level of inundation results in a lower yield reduction the second policy is fertilizer subsidies where 50 kg of free fertilizer are distributed to farmers in each cropping season free fertilizer is given to farmers located far from the river as they get a significantly lower nutrients concentration from floodplain sedimentation manh et al 2013 2014 3 2 2 uncertainties there are five key uncertain factors affecting the productivity of the agricultural sector in the upper vmd the first uncertain factor is future annual peak discharge that affects flood risk we use synthetic future hydrographs of the mekong river generated by a global hydrological model driven by climatic data from two scenarios rcp 4 5 and 8 5 sutanudjaja et al 2018 winsemius et al 2013 the second uncertain factor is the hydropower dam development upstream in cambodia this factor affects the annual peak discharge and reduces total sediment supply to the vmd as the dams trap the sediment upstream we use five dam development scenarios as worked out by lauri et al 2012 and manh et al 2015 the next two factors are the productivity gap among the three seasons the winter spring season that starts in december just after the wet monsoon season is the most productive season the summer autumn season and the autumn winter season are less productive due to the limited water content in the soil in the former and the high degree of precipitation in the latter in 2002 the summer autumn season and the autumn winter season in dong thap produced 38 and 50 fewer yield per hectare respectively in 2016 the productivity gap has been reduced to only 26 and 35 for the summer autumn and autumn winter season respectively in this study we consider a wide range of plausible future productivity gap between 15 and 45 the last uncertain factor is the society s preference toward the different rice cropping system and the spatial plan for the region this factor affects future land use demand which in turn is spatially allocated by the land use change module we consider four scenarios based on the competing narratives of agriculture intensification in the vmd as well as based on the mekong delta plan mekong delta plan consortium 2013 tran et al 2018 triet et al 2018 continuing intensification higher triple rice cropping demand and lower double rice cropping demand reverting to double rice the opposite of the first scenario rising non rice preferences higher demand for alternative livelihoods such as orchard plantation aquaculture and shrimp rice farming and increasing urbanization higher demand for residential area 3 2 3 experimental setup the setup of the case study is summarized using the xlrm framework lempert et al 2003 in fig 5 to allow for an exhaustive exploration of plausible combinations of uncertainties and policies we apply full factorial sampling to input factors that are categorical and ordinal i e we sample all possible combinations of categorical and ordinal input factors these factors include the six policy variables and some of the uncertain variables i e river discharge and farming practice preference we combine the full factorial sample with a latin hypercube sampling of the productivity gap uncertainties as the values for these uncertainties take a continuous range this experimental setup results in a total of 43200 computational experiments the exploratory modelling workbench kwakkel 2017 is used to perform these experiments 3 3 post processing of simulation results the clustering phase in the sequential approach and the calculation of error in the concurrent approach require the computation of distance between the outcomes of each scenario in the simulation results database to avoid having one outcome variable dictating the distance calculation the values of each outcome variable are usually normalized to 0 1 across the scenarios e g giudici et al 2020 smith et al 2019 normalization of each outcome variable across the entire scenarios when doing explorative analysis of distributional outcomes is problematic the outcome variables are the outcomes for each district by doing a normalization we lose sights of the relative performance of each district compared to all other districts within each scenario see fig 6 a and b hence we calculate instead the relative profitability of each district i e the 0 1 normalization is applied between the performance of each district within each scenario instead of across scenarios see fig 6c in this way we maintain the information regarding the relative winners and losers in each scenario as a result the clustering algorithm is forced to look for distinctive inequality patterns 4 results 4 1 sequential approach the first step in the sequential approach is clustering the output space into a number of representative inequality patterns we test five alternative clustering algorithms while varying the number of clusters see appendix a for details we find that the k means algorithm with seven clusters yields the most satisfactory performance which balances the explained variance and the number of final clusters the remainder of the sequential approach is thus based on the results from this clustering setup fig 7a shows the seven representative inequality patterns from each cluster of the output space the representative scenario is taken from the medoid of the corresponding cluster that is the scenario which outcomes have the smallest euclidean distance to all other scenarios in the cluster at a glance we can see that cluster 2 3 6 and 7 have similar inequality patterns where three districts located around the mid northeastern part of the region have a higher relative profitability of higher than 0 7 the patterns are different once we inspect them in more detail for example in cluster 3 the district located in the top northwestern part of the region is not relatively better off in cluster 6 this district is significantly better off compared to the others relative profitability 1 next we use the boosted trees algorithm to first identify the most critical input features that best explain the seven clusters of inequality patterns fig 7b shows the results of the input feature scoring the most important input feature is the degree of upstream dam development followed by three dikes construction policies expansion of high dikes in an giang in dong thap and reverting back to low dikes in an giang the other input features have substantially lower importance scores we use the four most important input features to map back the input space to the seven clusters of output space the importance scores of these four features add up to 0 705 implying that these features contribute to 70 5 of the total impurity reduction in the entire ensemble of trees fig 7c shows the factor map for each cluster where the cluster numbering corresponds to the seven inequality patterns in fig 7a since three of the four most important features are related to dike construction policies we combine them into a single axis i e the vertical axis on fig 7c the numbers underlying the heatmap correspond to the fraction of scenarios in that particular cluster for example 20 of the 7879 scenarios in cluster 1 are scenarios with high upstream dam development while maintaining the current dikes configuration in the vmd another 20 of the scenarios have a combination of high upstream dam development and low dikes policy in an giang fig 7c shows that cluster 1 which has one of the more distinctive inequality patterns see fig 7a is primarily induced through a combination of high sediment trapping due to upstream dam development and either expansion of high dikes in an giang or the preservation of current dikes inequality pattern as exemplified by cluster 4 is caused by the transformation of high dikes back into low dikes in an giang together with a high degree of upstream dam development cluster 2 and 7 which have similar inequality patterns emerge if upstream dam development is relatively low and either the low dikes policy in an giang is enacted or the current dikes system is maintained further construction of high dikes in an giang in combination with relatively low upstream dam development would lead to inequality patterns as depicted either in cluster 3 or 6 what do these results imply for adaptation planning in the vmd the most important insight is that the interaction between what the vmd government does in terms of dikes de construction and what the cambodian government does in terms of hydropower dams development has non linear effects on the emerging spatial distribution of farms profitability for instance a relatively small degree of upstream dam development would make provinces that expand their high dikes worse off this follows from comparing cluster 3 and 6 with cluster 5 in cluster 3 and 6 the high dikes policy in an giang is enacted and this makes districts within an giang relatively worse off in cluster 5 the high dikes policy in dong thap is also enacted and this leads to districts in dong thap becoming worse off this stresses the importance of having transboundary basin management in order to ensure equitable future for the vmd farmers 4 2 concurrent approach the first step in the concurrent approach is growing the regression tree and selecting an appropriate tree size we iteratively grow the tree from three to 40 leaves and observe the evolution of the cross validation scores see appendix b for details we find that the tree with 18 leaves yields the most satisfactory cross validation score and proceed with this tree size in the remainder of the concurrent approach for visualization purpose we separate the entire regression tree into two figures fig 8 and fig 9 together make up the entirety of the regression tree the first splitting variable identified by the mrt is the expansion of high dikes in an giang fig 8 shows the left branch of the tree high dikes in an giang is expanded while fig 9 shows the right branch of the tree high dikes in an giang is not expanded the number of scenarios and the representative inequality pattern from all scenarios in each leaf are provided at the bottom of the figures similar to the sequential approach before the medoid scenario in each leaf is assigned to be its representative inequality pattern it is important to restate here that in each scenario the profitability of the districts is normalized between 0 and 1 where darker green color means higher relative profitability here we illustrate how we can use either leaves first or root first analysis to interpret the results of the mrt we will use root first analysis to analyze the left branch of the tree and leaves first analysis for the right branch of the tree the left branch of the tree as shown in fig 8 contains scenarios where the high dikes policy in an giang is implemented for illustration we approach this side of the tree using root first analysis the subsequent decision node here is the degree of upstream dam development with a cutoff point of 2 5 we have six levels of upstream dam development with 0 2 being no to medium degree of upstream development and 3 5 being higher degrees of development if upstream dam development is relatively small the next deciding factors are the dikes policy in dong thap low dikes policy leads to inequality pattern in cluster 1 high dikes policy leads to cluster 2 while maintaining the current dikes distribution in dong thap leads to cluster 3 it is interesting to compare cluster 2 and cluster 4 as from the root first perspective the only difference is the degree of upstream dam development if high dikes are expanded in both provinces and many upstream dams are eventually built districts alongside the river will become substantially better off cluster 4 however a smaller degree of upstream dam construction will lead to a less striking difference in relative profitability cluster 2 cluster 6 although having a different narrative has a similar inequality pattern as cluster 4 even without expansion of high dikes in dong thap a very large degree of upstream dam development in combination with high dikes policy in an giang still make districts alongside the river better off the right branch of the tree contains the remaining 12 leaves fig 9 we approach the interpretation of this branch by following leaves first analysis we focus on three visually distinct inequality patterns the first pattern is typified by the higher relative profitability of districts alongside the river as observed in cluster 15 and 18 both clusters actually have a similar narrative where no dikes policy is taken and upstream dam development is relatively large the second distinct pattern is exemplified by cluster 8 and 9 in both clusters districts located to the north of the river have smaller relative profitability both clusters have a similar narrative of medium degree of upstream dam development and with high dikes being expanded in dong thap the third distinct pattern is observed in cluster 10 14 and cluster 16 the main pattern here is that there are three districts located to the north of the river three districts located to the south of the river and one district on the northwest corner of the region who are better off this pattern can emerge from multiple future conditions for example a condition for cluster 10 14 to materialize is no extremely high upstream dam development however cluster 16 shows that even if all planned upstream dams are built a similar inequality pattern could emerge if all dikes in both an giang and dong thap are reverted back into low dikes what can the vmd government learn from the concurrent approach through combining both root first and leaves first analyses we can clearly see that similar narratives could lead to distinctive patterns of spatial distribution at the same time similar distribution patterns could emerge from distinctive narratives the decision tree can easily help the government in understanding plausible inequality patterns and pathways that lead to those patterns and thus preparing additional measures to compensate worse off districts 5 discussion 5 1 input space and output space separability the induced input subspaces though scenario discovery have a perfect separability if each subspace is mutually exclusive with the others while traditionally this has been quantified through the density and coverage indicators bryant and lempert 2010 the use of these indicators is not applicable for sequential multiclass scenario discovery as presented here this is because in the sequential approach we do not set a strict boundary on the identified subspaces however from a visual inspection we can see that some of the identified subspaces are overlapping with each other see fig 7c for example the combination of current dikes and low to medium upstream dam development exists in the identified subspaces of cluster 2 and 7 in contrast the concurrent approach produces completely separable input subspaces as each end leaf has unique scenario narratives i e combination of the scenario variables therefore the concurrent approach leads to better input space separability compared to the sequential approach to quantify output space separability we calculate the average euclidean distance between the relative profitability of the 23 districts in all scenarios within each cluster within class dissimilarity as well as between scenarios from different clusters between class dissimilarity a better separability of output space thus entails low within cluster distance and high between cluster distance from table 1 we can see that neither approach is superior to the other the concurrent approach has better within cluster average distance compared to the sequential approach this can be explained by the more granular separation of the output space so that each cluster consists of more similar simulation results in contrast the sequential approach has better between cluster average distance compared to the concurrent approach this is explained by looking at the representative inequality patterns from the concurrent approach on figs 8 and 9 where there are many clusters that have similar inequality patterns to this end it is interesting to observe in more details the dis similarity of the resulting narratives from the two approaches 5 2 comparison of the resulting scenarios in this section we compare the clusters of inequality patterns from the two approaches as well as the narratives behind each cluster first we see that the clusters of inequality patterns from the concurrent approach have a higher degree of variation as there are several clusters that have a comparable pattern however most of the patterns identified from the concurrent approach are also present in the sequential approach for example the inequality patterns of cluster 4 6 15 and 18 from the concurrent approach are comparable to the inequality pattern of cluster 1 from the sequential approach table 2 lists the other pairs of similar inequality patterns identified by the two approaches two exceptions worth noting are cluster 15 and 16 from the concurrent approach in general cluster 16 has a similar inequality pattern to cluster 2 from the sequential approach however the most profitable districts in cluster 16 are the two districts in the westernmost part of the region furthermore the easternmost district is also slightly better off than many of the other districts cluster 15 has similar inequality pattern to cluster 1 from the sequential approach the difference is that many districts to the north of the river are also relatively better off in cluster 15 most of the narratives behind each inequality pattern identified by the two approaches are also comparable see table 2 for example cluster 4 and 5 from the sequential approach have similar narratives to their counterparts from the concurrent approach since in the concurrent approach we do not limit our analysis to only four most important factors this approach yields slightly richer and more detailed narratives for some of the clusters in the concurrent approach the low dikes policy in dong thap is identified as an important part of the narratives for some of the clusters i e cluster 1 10 and 16 from the concurrent approach the more aggregated results of the sequential approach conceal some diversity within the scenarios for example maintaining the current dike system in combination with a medium to high degree of upstream dam development is part of the narrative for cluster 6 from the sequential approach however the same narrative leads to different inequality patterns if we follow the decision tree from the concurrent approach i e cluster 14 and 15 this is because combining the inequality patterns from cluster 14 and 15 of the concurrent approach will average out the profitability of districts that are better off in each cluster resulting in a more equal distribution as exemplified by the representative inequality pattern of cluster 6 from the sequential approach 5 3 reflection for practice the comparisons above show that the concurrent approach outperforms the sequential approach however it comes with a caveat of having a larger number of final narratives this raises the question of whether the benefit of better input space separability in the concurrent approach does outweigh the drawback of having more clusters and narratives to answer this we need to first revisit the main purpose of scenario discovery itself which is to craft narratives about system outcomes under certain combinations of uncertainties polices bryant and lempert 2010 greeven et al 2016 lempert et al 2006 in particular attention needs to be given on the decision making contexts and on the use case of the narratives generated from the multiclass scenario discovery exercise past scenario discovery studies have a varying level of stakeholder involvement some studies indicate a relatively low degree of interactions with stakeholder e g hidayatno et al 2020 lamontagne et al 2018 moallemi et al 2017 in these studies the generated narratives are mainly aimed at defining plausible future pathways which are to be used by other institutions in other contexts other studies indicate a more frequent and thorough interactions e g groves et al 2019 hamarat et al 2013 trindade et al 2019 the aim of such studies is often more specific such as for stress testing alternative policies and identifying vulnerabilities accordingly narratives generated in such use cases are used solely for the purpose of the project the sequential approach with a relatively lower number of narratives is more suitable for the former type of use cases relatively little stakeholder engagement narratives to be transferred for other contexts the concurrent approach with better separability performance but more narratives is more suitable for the latter type of use cases more intense stakeholder engagement more focused analysis in addition to the characteristics of the use cases there are two further important points to note first an important strength of scenario discovery is to facilitate deliberation and this obviously requires thorough engagements with clients and stakeholders accordingly narratives from scenario discovery should not be shared as is with stakeholders rather the analyst should always be at the interface between the policy problem and the tool used to support the policy analysis cuppen et al 2021 so the issue with having a larger number of scenarios is that the analyst might have to do more work to distill the message from the analysis before conveying it to others and this can be done through consultation with the stakeholders second as some clusters from the concurrent approach have similar inequality patterns presenting them simultaneously might not be appropriate without the help of the results from the sequential approach the regrouping of similar clusters from the concurrent approach can be performed through either root first or leaves first analysis smith et al 2019 in leaves first analysis the key step is to identify clusters with similar representative inequality patterns this can be done qualitatively through visual inspection as done in table 2 or through consultation with stakeholders to aid this process the analyst can calculate the average distance between any pair of clusters and combine those with relatively low distance the final choice of the number of narratives should not be the analyst s call but instead decided in a participatory and interactive setting with stakeholders if what is of more interest is the narratives instead of the resulting inequality patterns the root first analysis can be followed instead 6 conclusion adaptation policies and uncertainties and the interactions between them almost unavoidably yield unequal consequences to different people the task of exploring future inequality patterns and understanding their drivers fits the nature of scenario discovery in scenario discovery one maps back the output space of a model in this case inequality patterns with its input space policy levers and exogenous uncertain factors in this study we contribute to the advancement of scenario discovery in two ways first we propose two novel criteria to evaluate the quality of multiclass scenario discovery results output space separability and the number of resulting narratives second we propose a novel concurrent approach for multiclass scenario discovery by using multivariate regression trees mrt using agriculture adaptation planning for the vietnam mekong delta as a case study we demonstrate the application of both the established sequential and the novel concurrent approach for multiclass scenario discovery we find that the concurrent approach performs considerably better in terms of input space separability the mrt algorithm guarantees a perfect separation of the input space when clustering the simulation results this however does result in a larger number of clusters of output space and subsequently narratives while the sequential approach results in seven scenarios the concurrent approach produces eighteen scenarios both approaches have a fairly comparable output space separability performance with the sequential approach results in better between cluster dissimilarity and the concurrent approach results in better within class similarity despite the differences in performance we show how most of the narratives and representative inequality patterns identified by the two approaches are similar with some exceptions the concurrent approach provides richer insights as it unravels two additional representative inequality patterns that are not captured by the sequential approach based on the case study results we argue for the use of the concurrent approach for future multiclass scenario discovery the concurrent approach guarantees perfect input space separability without sacrificing too much in terms of output space separability furthermore the concurrent approach captures richer and more distinctive trade off patterns between outcome variables in our case inequality patterns compared to the sequential approach one caveat is that the concurrent approach requires one to make extra effort to distill insights from these richer results in light of the presented results we see several directions for future research the first one is related to the selection of representative inequality patterns in this study we take a pragmatic approach by using the medoid scenario in each cluster other approaches include averaging the relative profitability of each actor across all scenarios in a cluster or selecting the scenario which has the most distinctive inequality pattern relative to the other clusters carlsen et al 2016 the second direction is assessing the limits and scalability of clustering when a higher number of stakeholders which leads to a larger number of outcome variables is considered while alternative high dimensional clustering techniques are available kriegel et al 2009 xu and tian 2015 their usefulness in the context of scenario discovery remains to be evaluated the third direction is to assess the impacts of different spatial aggregations as we aggregate farms profitability at a district level within district inequality is ignored the statistical bias resulting from spatial aggregation well known as the modifiable areal unit problem fotheringham and wong 1991 can have profound implications for the emerging spatial pattern sensitivity or robustness analysis could be applied to understand the stability of the representative inequality patterns under different aggregation levels inequalities can be viewed from various dimensions across people in different locations interregional with different income different socioeconomic background or across actors and variables inequality of profitability benefits from policies exposure to and impacts of climate change harrison et al 2016 jafino et al 2021b rao et al 2017 irrespective of the dimension and variable of inequality there is still a methodological need to explore plausible inequality patterns to support equitable adaptation planning for the purpose of showing the merits of multiclass scenario discovery for this methodological need we used one dimension of inequality interregional inequality of profitability without loss of generality the sequential and concurrent approaches could be applied to other conceptualizations of inequality as we only need to slice the population differently based on our variables of interest however it is important to highlight the limitation of this approach in planning for climate change distributional consequences can be seen from intra generational between people and assuming they live within the same generation and intergenerational between generations perspectives jafino et al 2021b multiclass scenario discovery is applicable for exploring intra generational but not intergenerational inequalities the topic of discounting is more applicable for the latter with recent works proposing alternative discounting methods that account for equity asheim 2017 dietz and asheim 2012 software and data availability the code for conducting both sequential and concurrent approaches as well as the data required to perform the analysis can be accessed at https github com bramkaarga inequality pattern exploration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by nwo top sector water call adaptation pathways for socially inclusive development of urbanizing deltas research number ond1362814 appendix a details of clustering results from the sequential approach the first step in the sequential approach is determining the clustering algorithm and the number of clusters to proceed with for each algorithm we perform clustering with an increasing number of clusters from 2 to 21 we calculate the explained variance for each number of cluster fig 10 by sweeping across different numbers of clusters we can observe the progression and the convergence of the explained variance at the end of the iteration i e with 21 clusters the explained variance from all algorithms clusters converges to 0 8 as explained in the methods section we set a threshold of 0 05 for the changes in explained variance in order to select an optimal number of clusters from each algorithm note that the selection of the threshold t for the changes in explained variance is a subjective choice we need to balance the explained variance of the selected number of clusters at which the threshold t is being met the potential gain in explained variance when using a higher number of clusters and the potential loss in interpretability when a higher number clusters is used table 3 shows the number of clusters from each clustering algorithm when the threshold t 0 05 is met and the corresponding explained variance k means algorithm yields the best performance it performs slightly better than k medoids and clearly outperforms the other clustering algorithms its explained variance of 0 711 is also not too distant from the overall explained variance convergence of 0 8 hence in the remainder of this sequential approach we proceed with the 7 clusters of output space as identified by the k means algorithm appendix b details of tree selection in the concurrent approach in the concurrent approach we start directly with determining the size of the tree based on the evolution of the cross validation scores fig 11 shows the increase of the 10 fold cross validation scores with increasing number of leaves similar to the clustering results in the sequential approach the cross validation score seems to converge to 0 8 however the cross validation score has not stagnated yet even after being grown to having 40 leaves as the score keeps increasing even after the tree has become quite complex it is advised to set a threshold of increase in cross validation scores in order to select an appropriate tree size smith et al 2019 we choose a threshold of 0 01 dashed line on fig 11 and this threshold is reached when the number of leaves is 18 
