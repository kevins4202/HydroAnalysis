index,text
6480,precipitation data plays an important role in investigation of water related fields of research such as water resources management hydraulic structure design and groundwater quantity quality parameters due to its high variability in space and time to evaluate and investigate precipitation pattern a set of well designed rain gauge stations can substantially reduce the cost and increase the estimation accuracy in the present research a new methodology of spatiotemporal optimization is developed for a rain gauge monitoring network and the results are compared to the optimized network based on spatial variations of precipitation the optimization process consists of two main steps of application of multi attribute decision making and heuristic approaches the entropy is chosen as the multi attribute decision making approach to determine optimum number of stations then the optimization process is implemented via coupling of genetic algorithm ga and geostatistical methods to identify the best rain gauge network configuration to determine the spatiotemporal structure of precipitation spatiotemporal variography and geostatistical methods known as ordinary kriging ok and bayesian maximum entropy bme have been undertaken thirty years of annual precipitation data from 105 rain gauge stations within and near namak lake watershed in the central part of iran are utilized in this research to optimize rain gauge stations spatially and temporally results showed that spatiotemporal network design considerably differs from spatial optimal rain gauge stations optimal locations of rain gauge stations resulted from spatiotemporal approach are almost two times better than spatial configuration to reduce rain gauge costs installation operation maintenance etc by avoiding data redundancy more efficiently in addition bme based network design method outperformed ok based network keywords genetic algorithm ordinary kriging bayesian maximum entropy rain gauge monitoring network spatiotemporal precipitation namak lake watershed 1 introduction estimation of precipitation usually faces uncertainties associated with spatial and or spatiotemporal variations of precipitation and non uniform distribution of rain gauge networks these uncertainties greatly affect information required for flood forecasting and design of hydraulic structures hence selection of a reliable and proper network of rain gauge stations has consequently received more attention than before because of water scarcity and dominant local or even regional droughts a well designed network of rain gauges is the best network configuration appropriate numbers and locations with the least number and best location of stations which reflects the reliable and acceptable spatial spatiotemporal variability of precipitation over a watershed this purpose can be achieved by eliminating the redundant stations rodriguez iturbe and mejía 1974 dong et al 2005 nunes et al 2004 and augmentation of existing network cheng et al 2008 chebbi et al 2011 delmelle and goovaerts 2009 delmelle 2014 adhikary et al 2015 nazaripour and daneshvar 2017 generally various approaches can be employed to find optimal configuration of stations these approaches can be categorized into four folds known as statistical geostatistical multi attribute decision making madm and heuristic frameworks in table 1 studies related to these approaches are categorized into spatial and spatiotemporal network design studies as this table shows geostatistical sampling has been received more attention as a reliable and useful tool for monitoring network design which is utilized in three parts of the table known as purely geostatistical the second row of the table combination of madm and geostatistical the fourth row of the table and combination of heuristic and geostatistical the fifth row of the table that is because of the fact that geostatistical approach considers variations of the studied variable both spatially and spatiotemporally it provides best linear unbiased estimator blue and the estimated value can be calculated along with its estimation error variance eev most of the previous rain gauge monitoring network designs have been aimed at prioritizing optimizing stations according to their spatial or temporal lumped variations of the precipitation all spatial studies in table 1 investigate spatial variations of precipitation modeling spatial variation individually may lack some valuable information in time scales due to possible gaps in spatiotemporal records of precipitation data spatiotemporal investigation of precipitation has received high attention in recent years bayat et al 2013 pulkkinen et al 2016 it seems that no research has been done to investigate the effectiveness of rainfall network design based on spatiotemporal simulation in other fields of environmental research especially groundwater monitoring network there are some studies that have investigated spatiotemporal variations of variable which are presented in spatiotemporal studies in table 1 different scenarios dealing with the spatiotemporal aspects of variables spatiotemporal studies of table 1 can be categorized into the following sections 1 the first scenario is to deal with space and time aspects separately nunes et al 2004 mogheir et al 2005 chadalavada and datta 2008 mogheir et al 2009 owlia et al 2011 dhar and patil 2012 tanos et al 2015 maymandi et al 2018 nunes et al 2004 used three optimization criteria maximization of spatial accuracy minimization of temporal redundancy and combination of these criteria to achieve the best subset of stations in groundwater monitoring network they used simulated annealing sa method to optimize variance estimation coupled with only spatial geostatistical approach chadalavada and datta 2008 eliminated temporal redundancy in detecting groundwater pollution in a hypothetical contaminated aquifer their selected monitoring network design was implemented for each period of time separately in their sequential approach the solution of previous management period was considered as an input for the subsequent management time mogheir et al 2009 assessed and redesigned spatial and temporal frequency of groundwater quality monitoring network in the northern part of gaza strip using entropy theory the temporal frequency was tested using transinformation model by increasing it to once a year and once every two years owlia et al 2011 recommended a methodology to redesign groundwater quality monitoring network in an aquifer located in iran using information content of transinformation distance t d curve extracted from entropy theory they measured some statistical factors in each station to find stations with similar temporal trend these included magnitude of variable in each period direction of change higher concentration slope trend and finally dispersion and homogeneity of data around the mean 2 the second scenario is based on considering spatiotemporal aspect of information and using spatiotemporal simulation method heuvelink et al 2012 júnez ferreira and herrera 2013 hosseini and kerachian 2017 heuvelink et al 2012 optimized temperature network design via sa coupled with spatiotemporal geostatistics their objective function was to minimize the average universal kriging variance they examined three scenarios of considering both static fixed location and mobile with different starting time configuration of stations júnez ferreira and herrera 2013 proposed a methodology to optimize a hydraulic head monitoring network in an aquifer located in mexico considering spatiotemporal variations they used combinations of coupled spatiotemporal variogram and kalman filter kf to assimilate missing wells data in their methodology optimal positions of wells were obtained yearly and separately by eliminating the redundant spatiotemporal samples hosseini and kerachian 2017 optimized spatiotemporal groundwater level monitoring network using a combination of bayesian maximum entropy bme and ordered weighted averaging owa as a madm method firstly they determined the priority number of each station to be removed then they have tested different time lags and starting points to obtain temporal frequencies in most spatiotemporal researches spatiotemporal variations have been considered as spatial configuration combined with temporal variation of their temporal snapshot separately without any spatiotemporal correlation scenario one in the second scenario it was concluded that the procedures and goals of spatiotemporal optimization were different from the methodology applied in this research in detail júnez ferreira and herrera 2013 reached the optimum structure for each year separately hosseini and kerachian 2017 defined optimal location and frequency of stations using madm approach they introduced various scenarios regarding to the hexagons side lengths time lags and starting points for temporal sampling which can make the optimization problem more time consuming and complex in these two researches time as mean as sampling frequency was considered as another feature which was optimized throughout the research time period in fact in the second scenario total number of space time records has been optimized this approach may not lead to reduction in existing stations the main objective in this research was to investigate optimal positions of rain gauge stations while a new framework of spatiotemporal variations of precipitation were considered in fact dimension of time is considered as the third aspect which is coupled with spatial dimensions longitude and latitude as the most noble point of the current paper and also to achieve comprehensive spatiotemporal dynamics of precipitation spatiotemporal geostatistical methods known as ordinary kriging ok and bme were used due for reliable and accurate estimations in the application of inherent spatiotemporal variations of precipitation one of the important points of the current approach is its independency to missing data at the rain gauge stations this research is organized into five sections the first of them is the materials and methods which consist of the study area and description of the applied methods and proposed spatiotemporal methodology the second and third parts are related to the algorithm and performance indicators respectively the fourth section is the results which categorize into optimal configuration of rain gauge stations obtained from spatiotemporal modeling and also the comparison between spatial and spatiotemporal network design the last section is dedicated to conclusions and discussion of network design based on spatiotemporal simulation 2 materials and methods 2 1 study area and data description namak lake watershed is located between 51 and 52 east longitudes and 39 and 40 north latitudes the altitude values range from 752 m in eastern areas up to 4330 m in the northeastern regions its area is approximately 90 000 km2 this watershed is regarded as one of the most important watersheds in iran because tehran capital of iran is located in it this altitude variation leads to high variability of precipitation amounts over the watershed for example large precipitation amounts more than 1000 mm are observed in the north eastern part of the watershed western and south western areas receive less intense precipitation lower than 100 mm and the remaining regions have median range of precipitation in the current paper the rain gauge network has been divided into two sections for calibration and validation purposes which are illustrated in fig 1 digital elevation model dem plotted in this figure shows that the majority of the watershed has elevation ranging from 1000 m to 2000 m there are very few areas located in north eastern parts with elevation higher than 3000 m eastern regions of the watershed are lowlands with elevation lower than 1000 m annual precipitation of 30 year period 1976 2005 are considered in this research 2 1 1 calibration network the first fold of the stations known as calibration rain gauge stations consists of 105 stations 87 stations located inside and the rest are scattered outside the watershed in fact the optimization process in this research is implemented on calibration network as seen in fig 1 outside stations are entirely located in northern and northeastern parts of the watershed 2 1 2 validation network the second fold of stations accounts for 30 of the calibration network with 31 rain gauges which are chosen randomly so that at least one station is considered in each region of the watershed introducing of this network is for evaluation purposes of the optimized networks apart from calibration network validation stations are more uniformly distributed throughout the watershed 2 2 methodology in this research genetic algorithm ga as a heuristic method and entropy theory as a madm have been used as useful tools to reach optimal network configuration in the current section spatiotemporal geostatistical framework entropy theory ga and the proposed spatiotemporal methodology are briefly described 2 2 1 geostatistical framework combining spatial spatiotemporal structure of random variables via statistical approaches have been received more attention snepvangers et al 2003 heuvelink and griffith 2010 and different models have been developed to analyze spatial spatiotemporal random structures e g goovaerts 2010 cameletti 2012 pebesma 2012 generally three objectives can be applied regarding to optimal spatial temporal sampling variogram estimation geostatistical formulation and sampling in the multivariate field reliable variogram modeling plays an important role in spatial estimation accuracy in the second case the goal is to minimize kriging variance sampling from the multivariate field third case the secondary information can be applied in case of difficult and expensive measurement of the primary variable delmelle 2014 a realization of spatiotemporal random function rf z s t represents continuous spatiotemporal domain where z demonstrates attribute value at location s for example any station at time t the experimental space time variogram γ st is given by 1 γ st h s h t 0 5 v a r z s h s t h t z s t where hs and ht are spatial and temporal increments and var represents variogram finding suitable and accurate mathematical variogram structure to represent spatiotemporal domain accurately plays crucial role in geostatistical estimation discourse to achieve this goal two types of mathematical spatiotemporal variogram models have been proposed known as separable and non separable spatiotemporal variograms earlier attempts used separable models to construct spatiotemporal covariance functions with some simplified assumptions these models separate the dependence of space and time development of non separable covariance functions first began by dimitrakopoulos and luo 1994 different researchers have utilized various types of covariance functions which are known as sum product and product sum mathematical forms de iaco et al 2001 de cesare et al 2001a 2001b the next step after determination of spatiotemporal variogram covariance is to implement geostatistical methods ok and bme as classic and modern geostatistics which are utilized in the current research the estimated variable in geostatistical framework is represented as a weighted linear combination of observed values as follows 2 z k s t i 1 n λ i s t z i s t where z k s t is estimated value at unknown point k z i s t represents the observed value at sampled points i n is total number of observed points and λ i s t indicates the weight associated with sample point i ordinary kriging ok the ok estimation method known as classical geostatistics is derived by solving the following system equations 3 j 1 n λ j γ s i s j μ γ s i s k i 1 2 n i 1 n λ i 1 where γ s i s j is the variogram between two observed points i and j γ s i s k represents the variogram between observed and unknown points i and k µ demonstrates the lagrange multiplier the eev is expressed as 4 σ r 2 i 1 n λ i γ s i s k μ for more details about the ok method readers are referred to kitanidis 1997 and goovaerts 1997 bayesian maximum entropy bme the bme method known as modern geostatistics provides a mathematical probabilistic framework to accurately estimate the stochastic variable in space and time along with uncertainty information it takes two knowledge bases general knowledge base g and site specific knowledge s the former demonstrates general characterization of space time random function such as mean and covariance functions the latter considers both hard monitoring data and soft measurement errors model prediction uncertain information etc data which can be considered as the difference from ok briefly speaking bme consists of three main stages at the prior stage the general knowledge base is characterized to form prior probability density function pdf at this stage the concept of maximum entropy is applied at the meta prior stage the site specific knowledge is organized into hard and soft data finally at the integration posterior stage posterior pdf is constructed by blending g and s at this stage the bayesian conditionalization rule is applied from the posterior pdf bmemode the most likely value at the estimation value and bmemean minimizes the mean square estimation error can be calculated in this paper bmelib package is implemented to analyze space and space time geostatistical section of monitoring network design for more details about the bme method readers are referred to christakos et al 2002 2 2 2 entropy theory entropy theory has been widely used for achieving optimal rainfall gauge network related papers are presented in madm type of network design in table 1 entropy theory is characterized based on information content of station s distributions which can also describe uncertainty the main objective in entropy theory is to minimize the redundant information or to maximize information transmitted by minimum number of stations which is described by transinformation t function the discretized format of transinformation between two rain gauge stations x and y is expressed as 5 t x y i 1 n j 1 n p x i y j ln p x i y j p x i p y j where n is the number of stations p xi and p xj are the discrete probabilities of occurrence of two rain gauge stations xi and yj and p xi yj is the joint or conditional probability the joint probability is measured from a contingency table which is characterized from marginal frequencies of rainfall time series the procedure to construct t d relationship is fully described by mogheir et al 2006 2 2 3 genetic algorithm the concept and procedure of ga is firstly introduced by goldberg 1989 it is a random based evolutionary algorithm ea searching through possible solutions to find the optimal one one of the major advantages in ga is to deal with discrete variables with infinite number of possible values which is the purpose of this study ga consists of three main operators known as initial population mutation and crossover a binary code to represent the combination of rain gauge stations has been used different values of ga parameters including crossover probability pc and mutation probability pm have been evaluated these parameters range from 0 7 to 0 9 and 0 01 0 1 respectively the algorithm is stopped when reaches a constant iteration or when no improvement has been observed between a set of iterations 2 2 4 the proposed spatiotemporal framework as stated in the introduction section the new methodology of considering time dimension is utilized in this paper which differs from other approaches in concept and structure in this approach the thirty year annual precipitation data are considered jointly and simultaneously with their missing values the spatiotemporal covariance is fitted to the existing datasets and then spatiotemporal network design is implemented based on the optimization procedures applied in this paper after identification of the optimum positions of rain gauge stations precipitation on grid points are estimated yearly based on the selected geostatistical estimators ok and bme then grid wise mean annual precipitation over the time period has been calculated fig 2 illustrates schematic spatiotemporal structure of the proposed methodology in 2d 1 spatial temporal dimensions the main difference between the proposed framework and the previous studies is that the selected removed stations are considered to be constant for each time period time series of annual precipitation for two stations are also drawn in fig 2 which shows that stations may consist of missing data during studied time period 3 modeling procedure achieving the best configuration of network via network design methods is based on two different strategies one of them is founded on adding new station s to fill its gap of information and the second strategy is based on removing stations s to reduce cost or redundant information in the current paper the second area has been chosen to evaluate presented spatiotemporal network design procedure fig 3 illustrates the process of spatiotemporal optimization to find optimal configuration of rain gauge network as shown in this flowchart it consists of six main steps which are described in the following steps step one relates to entropy theory and other steps demonstrate ga process for more comprehensive representation of the flowchart an example of n total stations with n removed stations are also presented under each step in this example i and j represent removed stations and chosen chromosomes in ga respectively the presented example is supposed to be one of the chosen chromosomes for instance j 1 in this example circular and triangle symbols represent remaining and removed stations in order step 1 achieving the optimum number of rain gauge stations the main goal of the heuristic optimization process in network design is to identify the best network configuration number of stations and their locations in this paper firstly the removed number of stations is determined based on the application of entropy theory a widely used method especially in rainfall monitoring network design the procedure of entropy based optimization applied in this paper is in line with that of mogheir et al 2006 who used t d transinformation distance curve t d curve and its parameter play important role in selection of optimal stations after calculation of pair wise transinformations and distances in rain gauge stations t d points are drawn and then the best function is fitted to experimental values exponential decay curve is selected as the best fitting function from this curve the optimum distance l and then optimum grid size a can be identified after fitting the best grid network to the watershed and calculation of one station per grid size based on geostatistical framework the optimum positions of rain gauge stations can be identified mogheir et al 2009 bayat et al 2018 the output of this step is the final number of stations which are recommended to be removed n the detailed process of step 1 approach is fully presented in bayat et al 2018 step 2 selection of removed stations at ga population stage as the objective in this paper is to find redundant stations with little information ga initial population stage is applied on removed stations n from the pool of calibration stations n which are fully described in the study area and data description section then for each chromosome a set of random n stations were chosen as can be seen from the example below step 2 in fig 3 stations 5 to n from n stations are assumed to be removed for the sample chromosome number 1 j 1 step 3 removing of entire n stations and returning of them individually after determination of the selected removed n stations in this step firstly the whole selected stations are omitted then these removed stations are returned into remaining stations individually for this configuration of stations estimation error variance eev and also annual precipitation of the individual removed station est are calculated for each time step t via ok and bme estimators after calculation of these required parameters eev and est for the selected removed station this station is being removed from remaining stations and the next station is added this process is repeated for all the stations the proposed procedure is also illustrated under this step for the three removed stations i as an example obs in this example is represented as observed value which is known as depicted in fig 3 geostatistical framework is implemented to simulate the attribute value in space and time the precipitation information is classified into two classes known as hard and soft datasets among the entire stations 105 and simulation periods 30 years 105 30 3150 records 2903 records of annual precipitation are available to consider as hard data and the remaining part is soft data spatiotemporal variogram is calculated based on available annual precipitation for experimental spatiotemporal covariance hard data of 30 year time period are collected consecutively to fit the best covariance function over experimental covariance various separable product theoretical covariance models for spatial and temporal structures spherical gaussian etc are tested and finally the best mathematical spatiotemporal variogram is chosen based on the highest goodness of fit the best combination of exponential and spherical for space and time are selected the following equation is assumed for the covariance function 6 cov r t n u g g e t s i l l exp 3 r s r 2 1 1 5 t t r 0 5 t t r 3 where r and t are spatial and temporal lags nugget is spatiotemporal covariance nugget effect sill is spatiotemporal covariance sill variance of spatiotemporal sr is spatial covariance range and tr is temporal covariance range the 3d covariance function is illustrated in fig 4 as shown in this figure theoretical covariance has good relationship with experimental covariance values the values of fitted parameters are nugget 3078 sill 2 37 10 4 sr 1 18 10 5 m and tr 200 years step 4 objective functions after determining the removed stations three objective functions ofs are calculated over the time of simulation t as follows 1 estimation error variance eev must be minimized 2 mean square error mse must be minimized 3 coefficient of determination r2 must be maximized 7 eev j m a x m a x e e v i t t 1 t 1 i 1 n 8 mse j 1 n i 1 n 1 t 2 t 1 t 2 e s t i t o b s i t 2 9 r j 1 n i 1 n t 2 t 1 t 2 e s t i t o b s i t t 1 t 2 es t i t t 1 t 2 ob s i t t 2 t 1 t 2 e s t i t 2 t 1 t 2 es t i t 2 t 2 t 1 t 2 o b s i t 2 t 1 t 2 ob s i t 2 where ee v i t estimation error variance of the removed station i in time step t based on the remaining stations n n es t i t estimation of precipitation in removed station i in time step of t based on the remaining stations n n ob s i t observed precipitation value in the removed station i in time step of t n total number of stations n total number of removed stations t1 total interested periods of time t2 total time of available observed precipitation values the first of eev relates to spatial estimation variances obtained from existing stations which depends on the structure and the location of the stations the second and third ofs are in relation with the accuracy of the candidate removed stations step 5 calculation of ofs in ga crossover and mutation stages after construction of initial population in ga and related ofs crossover and mutation stages should be implemented the procedure for stations configuration and of calculations is the same as steps 3 and 4 step 6 selection of the best of in this step the calculated population crossover and mutation ofs are put together and the best individuals are obtained based on elitism selection strategy 4 evaluation of the proposed methodology to evaluate the performance of proposed network design approach considering spatiotemporal variations of annual precipitation and also the three ofs validation network which are fully described in study area and data description section criteria are applied after determination of estimated est s i and observed obs s i precipitation at validation stations s s 1 s 2 mean absolute error mae normalized mean square error nmse and coefficient of correlation cc are calculated as follows 10 mae 1 v i 1 v a b s obs s i e s t s i 11 nmse 1 s 2 v i 1 v obs s i e s t s i 2 12 cc x y cov x y σ x σ y where v is the total number of validation dataset abs operator returns absolute value s 2 represents the variance of precipitation values of the validation network cov demonstrates covariance and σx is standard deviation of variable x 5 results and discussion as stated in the modeling section the required number of removed rain gauge stations was obtained via entropy theory approach bayat et al 2018 comprehensively described the procedure and pre results of number of removed stations according to bayat et al 2018 optimum number of removed stations was equal to 15 which will be used in network design based on spatiotemporal simulation coupled with ga with different ofs eev mse and r2 to achieve the comprehensive perspective two spatiotemporal geostatistical methods ok and bme were also employed to validate the proposed spatiotemporal network design as well as stated earlier the following results of spatiotemporal estimation of precipitation over grid points of the watershed have been averaged throughout time for the sake of simplification while comparing the results space and time averaged spatiotemporal variations of annual precipitation were labeled as s and st in the current section firstly the outcomes of spatiotemporal network design section 5 1 are presented and secondly similarity dissimilarity of the spatial vs spatiotemporal section 5 2 approaches is discussed 5 1 results of spatiotemporal network design based on the proposed procedure of spatiotemporal network design the best set of removed stations considering two geostatistical estimators ok and bme and three ofs eev mse and r2 have been presented in table 2 in this table optimum ga parameters optimum fitness function and run time are also viewed which have been obtained by evaluating various ga parameters as the fitness function in all ofs values shows bme results outperformed ok generally run time of ok vs bme and spatial s vs spatiotemporal st has shown considerable lower run time of ok against bme and s against st the statistical results of the three ofs eev mse and r2 were compared separately according to the validation network indicators known as mae nmse and cc then the final conclusion was drawn after analyzing the results the results are shown in table 3 discussions of the results only for st network design part are as follows comparison of mae nmse and cc values implied the relatively low superiority of mse against eev and r2 the mae nmse and cc for ok mse st were 35 3 0 209 and 0 89 respectively these values for the worst of ok r2 st were 38 6 0 241 and 0 87 respectively which have shown 9 13 and 3 greater accuracy of ok mse st than those obtained from ok r2 st these conclusions were also true for bme estimator which has shown around 8 10 and 2 superiority of bme mse st the best of compared with bme r2 st the worst of comparing the results of ok and bme estimators have shown that bme experienced better results which were on average 7 7 21 8 and 3 3 for mae nmse and cc respectively as an example in mae indicator mae value for ok eev st and bme eev st were 37 6 and 34 3 a superiority of 8 58 nmse values for ok r2 st and bme r2 st were 0 241 and 0 187 a progress of 22 4 finally cc values for ok r2 st and bme r2 st were 0 87 and 0 90 which have shown 3 94 superiority as can be seen from the described evaluation remarks it can be totally concluded that mse outperformed others and r2 received the lowest accuracy 5 2 spatial and spatiotemporal similarities and dissimilarities in the current section network design based on spatial s and time averaged spatiotemporal st approaches have been compared the records of validation network indicators in table 3 show that the differences between the two approaches ware highly significant and st outcomes are almost two times better than s records the superiority of st than s in mae and nmse indicators averaged on the whole ofs was observed 70 and 72 by means of ok estimator respectively these percentages were also confirmed by bme estimator which was reported as 80 and 91 superiority of st vs s in mae and nmse indicators respectively as an example in ok estimator mae values in ok mse s and ok mse st were reported 59 9 and 35 3 which have been shown 69 7 more accuracy of the st relative to s as an another case in bme method nmse values in bme r2 s and bme r2 st were equal to 0 376 and 0 187 which implies 103 2 higher performance of st against s considering cc values they were higher in st approach compared to s the greatest increase in ok estimator was stated by ok eev s vs ok eev st which were reported as 0 77 and 0 88 a surge of 0 11 unit this approach based on bme was between bme r2 s and bme r2 st which was the same enhancement of 0 11 unit comparison of ok and bme estimators by means of cc values also confirmed the superiority of bme against ok method the most recognized case was related to ok eev s vs bme eev s which was recorded 0 04 unit more rigorous results of bme than ok distribution of the removed rain gauge stations for s and st optimization approaches are illustrated in figs 5 and 6 in which square cross and circle symbols are represented as eev mse and r2 respectively in these figures left and right figures relate to ok and bme estimators respectively as shown in both s and st distributions and whole ofs the spread of removed rain gauge stations estimated by both ok and bme methods were almost the same more specifically removed stations in eev s eev st and r2 st both ok and bme were more located in northern regions of the watershed which were mountainous areas others mse s mse st and r2 s both ok and bme were more scattered the comparison between three ofs of the removed stations in s approach fig 5 demonstrates that mse and r2 have almost the same spread of stations and extended into the whole areas on the other hand removed stations based on eev of were more concentrated in northern parts of the watershed than the other ofs locations of mse based removed stations in st approach fig 6 have shown that they were mostly distributed in central and northern regions of the watershed distribution of both eev based and r2 based removed stations have demonstrated that they were more localized in northern parts of the study area it can be generally concluded that mse based removed stations were more scattered compared to eev based and r2 based removed stations for more comprehensive analyses regarding the removed stations table 4 demonstrates the percentage of similarity in removed rain gauge stations discussion about optimization approaches s st and ofs eev mse and r2 in this table can be demonstrated from two points of view in the first view of table 4 the main focus is to compare the three ofs in each estimator optimization approach separately ok s bme s ok st and bme st in ok s the highest percentage was among mse and r2 ok mse s and ok r2 s with 33 3 similarity this conclusion was also proven in spatial removed stations distribution presented in fig 5a in bme s whole of the ofs experienced the same similarity of 13 3 the records from ok st and bme st indicated that eev and r2 had the highest relationship with 40 and 33 3 similarities these findings were also identified from fig 6a and b for ok st and bme st methods sequentially in the second view of table 4 the percentage of similarity in the same of ok s vs bme s ok st vs bme st ok s vs ok st and bme s vs bme st was presented as can be seen from this table ok s and bme s had the highest similarities which were 46 7 73 3 and 73 3 for eev mse and r2 in order ok st and bme st received the second highest similarities which were 53 3 60 and 66 7 for eev mse and r2 respectively the comparison of ok ok s vs ok st and bme bme s vs bme st results considering whole ofs have shown higher similarity of bme estimator as an example bme eev s vs bme eev st percentage of similarity was 33 3 while this percentage for ok eev s vs ok eev st was stated 26 7 as the previous results presented mse was considered as the most accurate in both s and st approaches and also both ok and bme estimators table 4 results also confirm the same conclusions so that mse in whole cases ok s vs bme s ok st vs bme st ok s vs ok st and bme s vs bme st received the highest percentage of similarity the percentages are reported as 73 3 60 53 3 and 73 3 respectively 5 2 1 annual precipitation pattern in this part spatial pattern of annual precipitation for the best ofs is depicted to investigate the effect of removed rain gauge stations on contour lines of precipitation as described earlier mse has been considered as the best of in both s and st network design approaches and also both ok and bme estimators which were extracted from table 3 as an example spatial precipitation patterns of bme mse s and bme mse st are illustrated in fig 7 which have demonstrated the highest differences in these figures white circles and black squares are the optimized combination of removed and remaining stations respectively the three most recognizable differences in contour lines are spotted by square in these locations the effect of rain gauge removed stations on contour lines can be identified clearly these observations have demonstrated that how loss of one or two stations can change precipitation information and pattern in those areas 5 2 2 distribution assessment of spatial versus spatiotemporal results to assess distribution of the results q q plots of s versus time averaged st values of annual precipitation are illustrated in fig 8 these figures were related to the best configurations resulted from ok ok mse s vs ok mse st and bme bme mse s vs bme mse st estimators the inferred points of ok q q plot fig 8a have shown that s versus time averaged st had almost the same statistical probability of exceedance up to 450 mm of annual precipitation after this critical point the values of time averaged st in ok estimator have been shifted upward about 50 mm and have got away from the bisector line but in bme estimator precipitation values of more than 450 mm are fluctuating around the bisector line this shows more compatibility of s and st precipitation values in bme method compared to ok grid points of precipitation values more than 450 mm were all located in north eastern parts of the watershed with the highest elevation data 6 conclusions in this research a new framework of rain gauge monitoring network optimization considering spatiotemporal variations of annual precipitation is presented the methodology was applied to namak lake watershed located approximately in central part of iran the proposed methodology in the current paper is systematically and methodologically different from previous approaches in the context of rainfall network design methods in the previous researches of spatiotemporal network design temporal frequency and spatial position of stations were separately identified for each time lag however in the current paper spatial and temporal dimensions were considered as a unified information structure the proposed optimization process is the hybrid of ga and spatiotemporal geostatistical method to identify optimal position of rain gauge stations in terms of three objective functions known as minimizing the eev mse and maximizing the r2 these ofs mainly correspond to the variances and accuracy of the model the most crucial issue in developed methodology is that no gap filling was implemented throughout the optimization process not to include another source of error related to estimation process in this research different configurations of rain gauge stations obtained from spatial and spatiotemporal precipitation variations can be considered as a controversial issue among decision makers in the developed approach spatiotemporal annual precipitation was modeled considering spatiotemporal covariance function and then estimation process was implemented via classical and modern geostatistical methods known as ok and bme it was concluded that spatiotemporal framework of rain gauge network design can substantially improve the accuracy of estimation compared with the network design with spatial variations of precipitation records of validation network indicators have indicated that optimized network obtained from spatiotemporal precipitation variations were almost two times better than spatial variations this issue can be challenged into two folds the first is the fact that spatiotemporal variations of annual precipitation can be modeled more reliably and truly by considering time series of precipitation with missing data while spatial variations of annual precipitation are averaged during time period in the second challenge decision makers should keep in mind the accuracy computational time and complexity of each optimization approaches and select the best approach based on the their project goals among spatial and spatiotemporal objective functions mse performed the best in both ok and bme simulations this conclusion can also be confirmed by similarity percentages in table 4 which were reported as 73 3 53 3 73 3 and 60 in ok s vs bme s ok s vs ok st bme s vs bme st and ok st vs bme st respectively as a further investigation of this research other heuristic methods and objective functions can be dedicated to investigate the effects of searching algorithms presented in other optimization methods another area for future investigation is evaluation of the proposed spatial and spatiotemporal network design considering collocated geostatistical methods such as co kriging and also space spatiotemporal clustering to assess their effects on network design in addition to quantify uncertainties related to the observed and estimated precipitation fuzzy mathematics can be recommended declaration of interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement authors would like to thank the anonymous reviewers for their invaluable comments and recommendations that improved the quality and clarity of the manuscript 
6480,precipitation data plays an important role in investigation of water related fields of research such as water resources management hydraulic structure design and groundwater quantity quality parameters due to its high variability in space and time to evaluate and investigate precipitation pattern a set of well designed rain gauge stations can substantially reduce the cost and increase the estimation accuracy in the present research a new methodology of spatiotemporal optimization is developed for a rain gauge monitoring network and the results are compared to the optimized network based on spatial variations of precipitation the optimization process consists of two main steps of application of multi attribute decision making and heuristic approaches the entropy is chosen as the multi attribute decision making approach to determine optimum number of stations then the optimization process is implemented via coupling of genetic algorithm ga and geostatistical methods to identify the best rain gauge network configuration to determine the spatiotemporal structure of precipitation spatiotemporal variography and geostatistical methods known as ordinary kriging ok and bayesian maximum entropy bme have been undertaken thirty years of annual precipitation data from 105 rain gauge stations within and near namak lake watershed in the central part of iran are utilized in this research to optimize rain gauge stations spatially and temporally results showed that spatiotemporal network design considerably differs from spatial optimal rain gauge stations optimal locations of rain gauge stations resulted from spatiotemporal approach are almost two times better than spatial configuration to reduce rain gauge costs installation operation maintenance etc by avoiding data redundancy more efficiently in addition bme based network design method outperformed ok based network keywords genetic algorithm ordinary kriging bayesian maximum entropy rain gauge monitoring network spatiotemporal precipitation namak lake watershed 1 introduction estimation of precipitation usually faces uncertainties associated with spatial and or spatiotemporal variations of precipitation and non uniform distribution of rain gauge networks these uncertainties greatly affect information required for flood forecasting and design of hydraulic structures hence selection of a reliable and proper network of rain gauge stations has consequently received more attention than before because of water scarcity and dominant local or even regional droughts a well designed network of rain gauges is the best network configuration appropriate numbers and locations with the least number and best location of stations which reflects the reliable and acceptable spatial spatiotemporal variability of precipitation over a watershed this purpose can be achieved by eliminating the redundant stations rodriguez iturbe and mejía 1974 dong et al 2005 nunes et al 2004 and augmentation of existing network cheng et al 2008 chebbi et al 2011 delmelle and goovaerts 2009 delmelle 2014 adhikary et al 2015 nazaripour and daneshvar 2017 generally various approaches can be employed to find optimal configuration of stations these approaches can be categorized into four folds known as statistical geostatistical multi attribute decision making madm and heuristic frameworks in table 1 studies related to these approaches are categorized into spatial and spatiotemporal network design studies as this table shows geostatistical sampling has been received more attention as a reliable and useful tool for monitoring network design which is utilized in three parts of the table known as purely geostatistical the second row of the table combination of madm and geostatistical the fourth row of the table and combination of heuristic and geostatistical the fifth row of the table that is because of the fact that geostatistical approach considers variations of the studied variable both spatially and spatiotemporally it provides best linear unbiased estimator blue and the estimated value can be calculated along with its estimation error variance eev most of the previous rain gauge monitoring network designs have been aimed at prioritizing optimizing stations according to their spatial or temporal lumped variations of the precipitation all spatial studies in table 1 investigate spatial variations of precipitation modeling spatial variation individually may lack some valuable information in time scales due to possible gaps in spatiotemporal records of precipitation data spatiotemporal investigation of precipitation has received high attention in recent years bayat et al 2013 pulkkinen et al 2016 it seems that no research has been done to investigate the effectiveness of rainfall network design based on spatiotemporal simulation in other fields of environmental research especially groundwater monitoring network there are some studies that have investigated spatiotemporal variations of variable which are presented in spatiotemporal studies in table 1 different scenarios dealing with the spatiotemporal aspects of variables spatiotemporal studies of table 1 can be categorized into the following sections 1 the first scenario is to deal with space and time aspects separately nunes et al 2004 mogheir et al 2005 chadalavada and datta 2008 mogheir et al 2009 owlia et al 2011 dhar and patil 2012 tanos et al 2015 maymandi et al 2018 nunes et al 2004 used three optimization criteria maximization of spatial accuracy minimization of temporal redundancy and combination of these criteria to achieve the best subset of stations in groundwater monitoring network they used simulated annealing sa method to optimize variance estimation coupled with only spatial geostatistical approach chadalavada and datta 2008 eliminated temporal redundancy in detecting groundwater pollution in a hypothetical contaminated aquifer their selected monitoring network design was implemented for each period of time separately in their sequential approach the solution of previous management period was considered as an input for the subsequent management time mogheir et al 2009 assessed and redesigned spatial and temporal frequency of groundwater quality monitoring network in the northern part of gaza strip using entropy theory the temporal frequency was tested using transinformation model by increasing it to once a year and once every two years owlia et al 2011 recommended a methodology to redesign groundwater quality monitoring network in an aquifer located in iran using information content of transinformation distance t d curve extracted from entropy theory they measured some statistical factors in each station to find stations with similar temporal trend these included magnitude of variable in each period direction of change higher concentration slope trend and finally dispersion and homogeneity of data around the mean 2 the second scenario is based on considering spatiotemporal aspect of information and using spatiotemporal simulation method heuvelink et al 2012 júnez ferreira and herrera 2013 hosseini and kerachian 2017 heuvelink et al 2012 optimized temperature network design via sa coupled with spatiotemporal geostatistics their objective function was to minimize the average universal kriging variance they examined three scenarios of considering both static fixed location and mobile with different starting time configuration of stations júnez ferreira and herrera 2013 proposed a methodology to optimize a hydraulic head monitoring network in an aquifer located in mexico considering spatiotemporal variations they used combinations of coupled spatiotemporal variogram and kalman filter kf to assimilate missing wells data in their methodology optimal positions of wells were obtained yearly and separately by eliminating the redundant spatiotemporal samples hosseini and kerachian 2017 optimized spatiotemporal groundwater level monitoring network using a combination of bayesian maximum entropy bme and ordered weighted averaging owa as a madm method firstly they determined the priority number of each station to be removed then they have tested different time lags and starting points to obtain temporal frequencies in most spatiotemporal researches spatiotemporal variations have been considered as spatial configuration combined with temporal variation of their temporal snapshot separately without any spatiotemporal correlation scenario one in the second scenario it was concluded that the procedures and goals of spatiotemporal optimization were different from the methodology applied in this research in detail júnez ferreira and herrera 2013 reached the optimum structure for each year separately hosseini and kerachian 2017 defined optimal location and frequency of stations using madm approach they introduced various scenarios regarding to the hexagons side lengths time lags and starting points for temporal sampling which can make the optimization problem more time consuming and complex in these two researches time as mean as sampling frequency was considered as another feature which was optimized throughout the research time period in fact in the second scenario total number of space time records has been optimized this approach may not lead to reduction in existing stations the main objective in this research was to investigate optimal positions of rain gauge stations while a new framework of spatiotemporal variations of precipitation were considered in fact dimension of time is considered as the third aspect which is coupled with spatial dimensions longitude and latitude as the most noble point of the current paper and also to achieve comprehensive spatiotemporal dynamics of precipitation spatiotemporal geostatistical methods known as ordinary kriging ok and bme were used due for reliable and accurate estimations in the application of inherent spatiotemporal variations of precipitation one of the important points of the current approach is its independency to missing data at the rain gauge stations this research is organized into five sections the first of them is the materials and methods which consist of the study area and description of the applied methods and proposed spatiotemporal methodology the second and third parts are related to the algorithm and performance indicators respectively the fourth section is the results which categorize into optimal configuration of rain gauge stations obtained from spatiotemporal modeling and also the comparison between spatial and spatiotemporal network design the last section is dedicated to conclusions and discussion of network design based on spatiotemporal simulation 2 materials and methods 2 1 study area and data description namak lake watershed is located between 51 and 52 east longitudes and 39 and 40 north latitudes the altitude values range from 752 m in eastern areas up to 4330 m in the northeastern regions its area is approximately 90 000 km2 this watershed is regarded as one of the most important watersheds in iran because tehran capital of iran is located in it this altitude variation leads to high variability of precipitation amounts over the watershed for example large precipitation amounts more than 1000 mm are observed in the north eastern part of the watershed western and south western areas receive less intense precipitation lower than 100 mm and the remaining regions have median range of precipitation in the current paper the rain gauge network has been divided into two sections for calibration and validation purposes which are illustrated in fig 1 digital elevation model dem plotted in this figure shows that the majority of the watershed has elevation ranging from 1000 m to 2000 m there are very few areas located in north eastern parts with elevation higher than 3000 m eastern regions of the watershed are lowlands with elevation lower than 1000 m annual precipitation of 30 year period 1976 2005 are considered in this research 2 1 1 calibration network the first fold of the stations known as calibration rain gauge stations consists of 105 stations 87 stations located inside and the rest are scattered outside the watershed in fact the optimization process in this research is implemented on calibration network as seen in fig 1 outside stations are entirely located in northern and northeastern parts of the watershed 2 1 2 validation network the second fold of stations accounts for 30 of the calibration network with 31 rain gauges which are chosen randomly so that at least one station is considered in each region of the watershed introducing of this network is for evaluation purposes of the optimized networks apart from calibration network validation stations are more uniformly distributed throughout the watershed 2 2 methodology in this research genetic algorithm ga as a heuristic method and entropy theory as a madm have been used as useful tools to reach optimal network configuration in the current section spatiotemporal geostatistical framework entropy theory ga and the proposed spatiotemporal methodology are briefly described 2 2 1 geostatistical framework combining spatial spatiotemporal structure of random variables via statistical approaches have been received more attention snepvangers et al 2003 heuvelink and griffith 2010 and different models have been developed to analyze spatial spatiotemporal random structures e g goovaerts 2010 cameletti 2012 pebesma 2012 generally three objectives can be applied regarding to optimal spatial temporal sampling variogram estimation geostatistical formulation and sampling in the multivariate field reliable variogram modeling plays an important role in spatial estimation accuracy in the second case the goal is to minimize kriging variance sampling from the multivariate field third case the secondary information can be applied in case of difficult and expensive measurement of the primary variable delmelle 2014 a realization of spatiotemporal random function rf z s t represents continuous spatiotemporal domain where z demonstrates attribute value at location s for example any station at time t the experimental space time variogram γ st is given by 1 γ st h s h t 0 5 v a r z s h s t h t z s t where hs and ht are spatial and temporal increments and var represents variogram finding suitable and accurate mathematical variogram structure to represent spatiotemporal domain accurately plays crucial role in geostatistical estimation discourse to achieve this goal two types of mathematical spatiotemporal variogram models have been proposed known as separable and non separable spatiotemporal variograms earlier attempts used separable models to construct spatiotemporal covariance functions with some simplified assumptions these models separate the dependence of space and time development of non separable covariance functions first began by dimitrakopoulos and luo 1994 different researchers have utilized various types of covariance functions which are known as sum product and product sum mathematical forms de iaco et al 2001 de cesare et al 2001a 2001b the next step after determination of spatiotemporal variogram covariance is to implement geostatistical methods ok and bme as classic and modern geostatistics which are utilized in the current research the estimated variable in geostatistical framework is represented as a weighted linear combination of observed values as follows 2 z k s t i 1 n λ i s t z i s t where z k s t is estimated value at unknown point k z i s t represents the observed value at sampled points i n is total number of observed points and λ i s t indicates the weight associated with sample point i ordinary kriging ok the ok estimation method known as classical geostatistics is derived by solving the following system equations 3 j 1 n λ j γ s i s j μ γ s i s k i 1 2 n i 1 n λ i 1 where γ s i s j is the variogram between two observed points i and j γ s i s k represents the variogram between observed and unknown points i and k µ demonstrates the lagrange multiplier the eev is expressed as 4 σ r 2 i 1 n λ i γ s i s k μ for more details about the ok method readers are referred to kitanidis 1997 and goovaerts 1997 bayesian maximum entropy bme the bme method known as modern geostatistics provides a mathematical probabilistic framework to accurately estimate the stochastic variable in space and time along with uncertainty information it takes two knowledge bases general knowledge base g and site specific knowledge s the former demonstrates general characterization of space time random function such as mean and covariance functions the latter considers both hard monitoring data and soft measurement errors model prediction uncertain information etc data which can be considered as the difference from ok briefly speaking bme consists of three main stages at the prior stage the general knowledge base is characterized to form prior probability density function pdf at this stage the concept of maximum entropy is applied at the meta prior stage the site specific knowledge is organized into hard and soft data finally at the integration posterior stage posterior pdf is constructed by blending g and s at this stage the bayesian conditionalization rule is applied from the posterior pdf bmemode the most likely value at the estimation value and bmemean minimizes the mean square estimation error can be calculated in this paper bmelib package is implemented to analyze space and space time geostatistical section of monitoring network design for more details about the bme method readers are referred to christakos et al 2002 2 2 2 entropy theory entropy theory has been widely used for achieving optimal rainfall gauge network related papers are presented in madm type of network design in table 1 entropy theory is characterized based on information content of station s distributions which can also describe uncertainty the main objective in entropy theory is to minimize the redundant information or to maximize information transmitted by minimum number of stations which is described by transinformation t function the discretized format of transinformation between two rain gauge stations x and y is expressed as 5 t x y i 1 n j 1 n p x i y j ln p x i y j p x i p y j where n is the number of stations p xi and p xj are the discrete probabilities of occurrence of two rain gauge stations xi and yj and p xi yj is the joint or conditional probability the joint probability is measured from a contingency table which is characterized from marginal frequencies of rainfall time series the procedure to construct t d relationship is fully described by mogheir et al 2006 2 2 3 genetic algorithm the concept and procedure of ga is firstly introduced by goldberg 1989 it is a random based evolutionary algorithm ea searching through possible solutions to find the optimal one one of the major advantages in ga is to deal with discrete variables with infinite number of possible values which is the purpose of this study ga consists of three main operators known as initial population mutation and crossover a binary code to represent the combination of rain gauge stations has been used different values of ga parameters including crossover probability pc and mutation probability pm have been evaluated these parameters range from 0 7 to 0 9 and 0 01 0 1 respectively the algorithm is stopped when reaches a constant iteration or when no improvement has been observed between a set of iterations 2 2 4 the proposed spatiotemporal framework as stated in the introduction section the new methodology of considering time dimension is utilized in this paper which differs from other approaches in concept and structure in this approach the thirty year annual precipitation data are considered jointly and simultaneously with their missing values the spatiotemporal covariance is fitted to the existing datasets and then spatiotemporal network design is implemented based on the optimization procedures applied in this paper after identification of the optimum positions of rain gauge stations precipitation on grid points are estimated yearly based on the selected geostatistical estimators ok and bme then grid wise mean annual precipitation over the time period has been calculated fig 2 illustrates schematic spatiotemporal structure of the proposed methodology in 2d 1 spatial temporal dimensions the main difference between the proposed framework and the previous studies is that the selected removed stations are considered to be constant for each time period time series of annual precipitation for two stations are also drawn in fig 2 which shows that stations may consist of missing data during studied time period 3 modeling procedure achieving the best configuration of network via network design methods is based on two different strategies one of them is founded on adding new station s to fill its gap of information and the second strategy is based on removing stations s to reduce cost or redundant information in the current paper the second area has been chosen to evaluate presented spatiotemporal network design procedure fig 3 illustrates the process of spatiotemporal optimization to find optimal configuration of rain gauge network as shown in this flowchart it consists of six main steps which are described in the following steps step one relates to entropy theory and other steps demonstrate ga process for more comprehensive representation of the flowchart an example of n total stations with n removed stations are also presented under each step in this example i and j represent removed stations and chosen chromosomes in ga respectively the presented example is supposed to be one of the chosen chromosomes for instance j 1 in this example circular and triangle symbols represent remaining and removed stations in order step 1 achieving the optimum number of rain gauge stations the main goal of the heuristic optimization process in network design is to identify the best network configuration number of stations and their locations in this paper firstly the removed number of stations is determined based on the application of entropy theory a widely used method especially in rainfall monitoring network design the procedure of entropy based optimization applied in this paper is in line with that of mogheir et al 2006 who used t d transinformation distance curve t d curve and its parameter play important role in selection of optimal stations after calculation of pair wise transinformations and distances in rain gauge stations t d points are drawn and then the best function is fitted to experimental values exponential decay curve is selected as the best fitting function from this curve the optimum distance l and then optimum grid size a can be identified after fitting the best grid network to the watershed and calculation of one station per grid size based on geostatistical framework the optimum positions of rain gauge stations can be identified mogheir et al 2009 bayat et al 2018 the output of this step is the final number of stations which are recommended to be removed n the detailed process of step 1 approach is fully presented in bayat et al 2018 step 2 selection of removed stations at ga population stage as the objective in this paper is to find redundant stations with little information ga initial population stage is applied on removed stations n from the pool of calibration stations n which are fully described in the study area and data description section then for each chromosome a set of random n stations were chosen as can be seen from the example below step 2 in fig 3 stations 5 to n from n stations are assumed to be removed for the sample chromosome number 1 j 1 step 3 removing of entire n stations and returning of them individually after determination of the selected removed n stations in this step firstly the whole selected stations are omitted then these removed stations are returned into remaining stations individually for this configuration of stations estimation error variance eev and also annual precipitation of the individual removed station est are calculated for each time step t via ok and bme estimators after calculation of these required parameters eev and est for the selected removed station this station is being removed from remaining stations and the next station is added this process is repeated for all the stations the proposed procedure is also illustrated under this step for the three removed stations i as an example obs in this example is represented as observed value which is known as depicted in fig 3 geostatistical framework is implemented to simulate the attribute value in space and time the precipitation information is classified into two classes known as hard and soft datasets among the entire stations 105 and simulation periods 30 years 105 30 3150 records 2903 records of annual precipitation are available to consider as hard data and the remaining part is soft data spatiotemporal variogram is calculated based on available annual precipitation for experimental spatiotemporal covariance hard data of 30 year time period are collected consecutively to fit the best covariance function over experimental covariance various separable product theoretical covariance models for spatial and temporal structures spherical gaussian etc are tested and finally the best mathematical spatiotemporal variogram is chosen based on the highest goodness of fit the best combination of exponential and spherical for space and time are selected the following equation is assumed for the covariance function 6 cov r t n u g g e t s i l l exp 3 r s r 2 1 1 5 t t r 0 5 t t r 3 where r and t are spatial and temporal lags nugget is spatiotemporal covariance nugget effect sill is spatiotemporal covariance sill variance of spatiotemporal sr is spatial covariance range and tr is temporal covariance range the 3d covariance function is illustrated in fig 4 as shown in this figure theoretical covariance has good relationship with experimental covariance values the values of fitted parameters are nugget 3078 sill 2 37 10 4 sr 1 18 10 5 m and tr 200 years step 4 objective functions after determining the removed stations three objective functions ofs are calculated over the time of simulation t as follows 1 estimation error variance eev must be minimized 2 mean square error mse must be minimized 3 coefficient of determination r2 must be maximized 7 eev j m a x m a x e e v i t t 1 t 1 i 1 n 8 mse j 1 n i 1 n 1 t 2 t 1 t 2 e s t i t o b s i t 2 9 r j 1 n i 1 n t 2 t 1 t 2 e s t i t o b s i t t 1 t 2 es t i t t 1 t 2 ob s i t t 2 t 1 t 2 e s t i t 2 t 1 t 2 es t i t 2 t 2 t 1 t 2 o b s i t 2 t 1 t 2 ob s i t 2 where ee v i t estimation error variance of the removed station i in time step t based on the remaining stations n n es t i t estimation of precipitation in removed station i in time step of t based on the remaining stations n n ob s i t observed precipitation value in the removed station i in time step of t n total number of stations n total number of removed stations t1 total interested periods of time t2 total time of available observed precipitation values the first of eev relates to spatial estimation variances obtained from existing stations which depends on the structure and the location of the stations the second and third ofs are in relation with the accuracy of the candidate removed stations step 5 calculation of ofs in ga crossover and mutation stages after construction of initial population in ga and related ofs crossover and mutation stages should be implemented the procedure for stations configuration and of calculations is the same as steps 3 and 4 step 6 selection of the best of in this step the calculated population crossover and mutation ofs are put together and the best individuals are obtained based on elitism selection strategy 4 evaluation of the proposed methodology to evaluate the performance of proposed network design approach considering spatiotemporal variations of annual precipitation and also the three ofs validation network which are fully described in study area and data description section criteria are applied after determination of estimated est s i and observed obs s i precipitation at validation stations s s 1 s 2 mean absolute error mae normalized mean square error nmse and coefficient of correlation cc are calculated as follows 10 mae 1 v i 1 v a b s obs s i e s t s i 11 nmse 1 s 2 v i 1 v obs s i e s t s i 2 12 cc x y cov x y σ x σ y where v is the total number of validation dataset abs operator returns absolute value s 2 represents the variance of precipitation values of the validation network cov demonstrates covariance and σx is standard deviation of variable x 5 results and discussion as stated in the modeling section the required number of removed rain gauge stations was obtained via entropy theory approach bayat et al 2018 comprehensively described the procedure and pre results of number of removed stations according to bayat et al 2018 optimum number of removed stations was equal to 15 which will be used in network design based on spatiotemporal simulation coupled with ga with different ofs eev mse and r2 to achieve the comprehensive perspective two spatiotemporal geostatistical methods ok and bme were also employed to validate the proposed spatiotemporal network design as well as stated earlier the following results of spatiotemporal estimation of precipitation over grid points of the watershed have been averaged throughout time for the sake of simplification while comparing the results space and time averaged spatiotemporal variations of annual precipitation were labeled as s and st in the current section firstly the outcomes of spatiotemporal network design section 5 1 are presented and secondly similarity dissimilarity of the spatial vs spatiotemporal section 5 2 approaches is discussed 5 1 results of spatiotemporal network design based on the proposed procedure of spatiotemporal network design the best set of removed stations considering two geostatistical estimators ok and bme and three ofs eev mse and r2 have been presented in table 2 in this table optimum ga parameters optimum fitness function and run time are also viewed which have been obtained by evaluating various ga parameters as the fitness function in all ofs values shows bme results outperformed ok generally run time of ok vs bme and spatial s vs spatiotemporal st has shown considerable lower run time of ok against bme and s against st the statistical results of the three ofs eev mse and r2 were compared separately according to the validation network indicators known as mae nmse and cc then the final conclusion was drawn after analyzing the results the results are shown in table 3 discussions of the results only for st network design part are as follows comparison of mae nmse and cc values implied the relatively low superiority of mse against eev and r2 the mae nmse and cc for ok mse st were 35 3 0 209 and 0 89 respectively these values for the worst of ok r2 st were 38 6 0 241 and 0 87 respectively which have shown 9 13 and 3 greater accuracy of ok mse st than those obtained from ok r2 st these conclusions were also true for bme estimator which has shown around 8 10 and 2 superiority of bme mse st the best of compared with bme r2 st the worst of comparing the results of ok and bme estimators have shown that bme experienced better results which were on average 7 7 21 8 and 3 3 for mae nmse and cc respectively as an example in mae indicator mae value for ok eev st and bme eev st were 37 6 and 34 3 a superiority of 8 58 nmse values for ok r2 st and bme r2 st were 0 241 and 0 187 a progress of 22 4 finally cc values for ok r2 st and bme r2 st were 0 87 and 0 90 which have shown 3 94 superiority as can be seen from the described evaluation remarks it can be totally concluded that mse outperformed others and r2 received the lowest accuracy 5 2 spatial and spatiotemporal similarities and dissimilarities in the current section network design based on spatial s and time averaged spatiotemporal st approaches have been compared the records of validation network indicators in table 3 show that the differences between the two approaches ware highly significant and st outcomes are almost two times better than s records the superiority of st than s in mae and nmse indicators averaged on the whole ofs was observed 70 and 72 by means of ok estimator respectively these percentages were also confirmed by bme estimator which was reported as 80 and 91 superiority of st vs s in mae and nmse indicators respectively as an example in ok estimator mae values in ok mse s and ok mse st were reported 59 9 and 35 3 which have been shown 69 7 more accuracy of the st relative to s as an another case in bme method nmse values in bme r2 s and bme r2 st were equal to 0 376 and 0 187 which implies 103 2 higher performance of st against s considering cc values they were higher in st approach compared to s the greatest increase in ok estimator was stated by ok eev s vs ok eev st which were reported as 0 77 and 0 88 a surge of 0 11 unit this approach based on bme was between bme r2 s and bme r2 st which was the same enhancement of 0 11 unit comparison of ok and bme estimators by means of cc values also confirmed the superiority of bme against ok method the most recognized case was related to ok eev s vs bme eev s which was recorded 0 04 unit more rigorous results of bme than ok distribution of the removed rain gauge stations for s and st optimization approaches are illustrated in figs 5 and 6 in which square cross and circle symbols are represented as eev mse and r2 respectively in these figures left and right figures relate to ok and bme estimators respectively as shown in both s and st distributions and whole ofs the spread of removed rain gauge stations estimated by both ok and bme methods were almost the same more specifically removed stations in eev s eev st and r2 st both ok and bme were more located in northern regions of the watershed which were mountainous areas others mse s mse st and r2 s both ok and bme were more scattered the comparison between three ofs of the removed stations in s approach fig 5 demonstrates that mse and r2 have almost the same spread of stations and extended into the whole areas on the other hand removed stations based on eev of were more concentrated in northern parts of the watershed than the other ofs locations of mse based removed stations in st approach fig 6 have shown that they were mostly distributed in central and northern regions of the watershed distribution of both eev based and r2 based removed stations have demonstrated that they were more localized in northern parts of the study area it can be generally concluded that mse based removed stations were more scattered compared to eev based and r2 based removed stations for more comprehensive analyses regarding the removed stations table 4 demonstrates the percentage of similarity in removed rain gauge stations discussion about optimization approaches s st and ofs eev mse and r2 in this table can be demonstrated from two points of view in the first view of table 4 the main focus is to compare the three ofs in each estimator optimization approach separately ok s bme s ok st and bme st in ok s the highest percentage was among mse and r2 ok mse s and ok r2 s with 33 3 similarity this conclusion was also proven in spatial removed stations distribution presented in fig 5a in bme s whole of the ofs experienced the same similarity of 13 3 the records from ok st and bme st indicated that eev and r2 had the highest relationship with 40 and 33 3 similarities these findings were also identified from fig 6a and b for ok st and bme st methods sequentially in the second view of table 4 the percentage of similarity in the same of ok s vs bme s ok st vs bme st ok s vs ok st and bme s vs bme st was presented as can be seen from this table ok s and bme s had the highest similarities which were 46 7 73 3 and 73 3 for eev mse and r2 in order ok st and bme st received the second highest similarities which were 53 3 60 and 66 7 for eev mse and r2 respectively the comparison of ok ok s vs ok st and bme bme s vs bme st results considering whole ofs have shown higher similarity of bme estimator as an example bme eev s vs bme eev st percentage of similarity was 33 3 while this percentage for ok eev s vs ok eev st was stated 26 7 as the previous results presented mse was considered as the most accurate in both s and st approaches and also both ok and bme estimators table 4 results also confirm the same conclusions so that mse in whole cases ok s vs bme s ok st vs bme st ok s vs ok st and bme s vs bme st received the highest percentage of similarity the percentages are reported as 73 3 60 53 3 and 73 3 respectively 5 2 1 annual precipitation pattern in this part spatial pattern of annual precipitation for the best ofs is depicted to investigate the effect of removed rain gauge stations on contour lines of precipitation as described earlier mse has been considered as the best of in both s and st network design approaches and also both ok and bme estimators which were extracted from table 3 as an example spatial precipitation patterns of bme mse s and bme mse st are illustrated in fig 7 which have demonstrated the highest differences in these figures white circles and black squares are the optimized combination of removed and remaining stations respectively the three most recognizable differences in contour lines are spotted by square in these locations the effect of rain gauge removed stations on contour lines can be identified clearly these observations have demonstrated that how loss of one or two stations can change precipitation information and pattern in those areas 5 2 2 distribution assessment of spatial versus spatiotemporal results to assess distribution of the results q q plots of s versus time averaged st values of annual precipitation are illustrated in fig 8 these figures were related to the best configurations resulted from ok ok mse s vs ok mse st and bme bme mse s vs bme mse st estimators the inferred points of ok q q plot fig 8a have shown that s versus time averaged st had almost the same statistical probability of exceedance up to 450 mm of annual precipitation after this critical point the values of time averaged st in ok estimator have been shifted upward about 50 mm and have got away from the bisector line but in bme estimator precipitation values of more than 450 mm are fluctuating around the bisector line this shows more compatibility of s and st precipitation values in bme method compared to ok grid points of precipitation values more than 450 mm were all located in north eastern parts of the watershed with the highest elevation data 6 conclusions in this research a new framework of rain gauge monitoring network optimization considering spatiotemporal variations of annual precipitation is presented the methodology was applied to namak lake watershed located approximately in central part of iran the proposed methodology in the current paper is systematically and methodologically different from previous approaches in the context of rainfall network design methods in the previous researches of spatiotemporal network design temporal frequency and spatial position of stations were separately identified for each time lag however in the current paper spatial and temporal dimensions were considered as a unified information structure the proposed optimization process is the hybrid of ga and spatiotemporal geostatistical method to identify optimal position of rain gauge stations in terms of three objective functions known as minimizing the eev mse and maximizing the r2 these ofs mainly correspond to the variances and accuracy of the model the most crucial issue in developed methodology is that no gap filling was implemented throughout the optimization process not to include another source of error related to estimation process in this research different configurations of rain gauge stations obtained from spatial and spatiotemporal precipitation variations can be considered as a controversial issue among decision makers in the developed approach spatiotemporal annual precipitation was modeled considering spatiotemporal covariance function and then estimation process was implemented via classical and modern geostatistical methods known as ok and bme it was concluded that spatiotemporal framework of rain gauge network design can substantially improve the accuracy of estimation compared with the network design with spatial variations of precipitation records of validation network indicators have indicated that optimized network obtained from spatiotemporal precipitation variations were almost two times better than spatial variations this issue can be challenged into two folds the first is the fact that spatiotemporal variations of annual precipitation can be modeled more reliably and truly by considering time series of precipitation with missing data while spatial variations of annual precipitation are averaged during time period in the second challenge decision makers should keep in mind the accuracy computational time and complexity of each optimization approaches and select the best approach based on the their project goals among spatial and spatiotemporal objective functions mse performed the best in both ok and bme simulations this conclusion can also be confirmed by similarity percentages in table 4 which were reported as 73 3 53 3 73 3 and 60 in ok s vs bme s ok s vs ok st bme s vs bme st and ok st vs bme st respectively as a further investigation of this research other heuristic methods and objective functions can be dedicated to investigate the effects of searching algorithms presented in other optimization methods another area for future investigation is evaluation of the proposed spatial and spatiotemporal network design considering collocated geostatistical methods such as co kriging and also space spatiotemporal clustering to assess their effects on network design in addition to quantify uncertainties related to the observed and estimated precipitation fuzzy mathematics can be recommended declaration of interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement authors would like to thank the anonymous reviewers for their invaluable comments and recommendations that improved the quality and clarity of the manuscript 
6481,vernal pools are seasonal wetlands that have a high diversity of endemic and native plant species yet they are threatened by agricultural conversion and urban development and face threats posed by climate change resulting from altered precipitation and temperature regimes we developed an approach to investigate the potential impacts of climate change on hydrology and vegetation communities of vernal pools by creating a mass balance hydrologic model that is coupled to a statistical model of plant community distribution the hydrologic and vegetative models were calibrated using field measurements from a vernal pool in northeastern california that experiences snow dominated hydrology and is larger than vernal pools in more studied areas like central california but representative of other northern california vernal pools using downscaled data from global climate models the coupled model suggests that warmer conditions will lead to the pool being inundated for a shorter time but with little change in maximum depth reduced hydroperiods suggest possible declines in vernal pool specialist species with future climate change the coupled model is an integrated approach for understanding the impact of altered environmental conditions on unique hydrology and plant community composition of vernal pool ecosystems but the model approach could be improved with longer term data and by applying it at more sites to broaden the applicability of the approach and to enable better process representation keywords ephemeral pool hydrologic modeling hydroperiod wetland plants seasonal wetland 1 introduction vernal pools are ephemeral wetlands that exist in local topographic depressions with relatively impermeable substrates keeley and zedler 1998 boone et al 2006 in temperate regions experiencing winter snowfall like northern california they are subject to four distinct seasons they typically 1 fill with snow in the winter 2 melt into inundated pools in the spring 3 become unsaturated and vegetated by summer and then 4 dry and become fully desiccated by autumn their main source of water is direct precipitation though some pools receive runoff from a very small watershed keeley and zedler 1998 the substrates of vernal pools can be bedrock such as volcanic mud or lava flows clay rich soils cemented mudflow and soils with hard clay pans or duripans hobson and dahlgren 1998 keeley and zedler 1998 smith and verrill 1998 boone et al 2006 all of these substrates have low hydraulic conductivities as these low permeability substrates restrict the infiltration of water small amounts of precipitation can produce perched zones of saturation and pooling of water rains et al 2008 the ephemeral nature of vernal pools and associated vegetation is also influenced by climate vernal pools are distinguished by the source and seasonality of their water inputs vernal pools associated with mediterranean climates can receive a majority of precipitation in the cooler months of the winter whereas summers are hot and dry keeley and zedler 1998 vernal pool vegetation distribution varies from year to year depending on climatic conditions such as precipitation amount and timing as well as air temperature brooks 2004 keeley and zedler 1998 because vernal pools have seasonally distinct climatic conditions i e alternating between inundation and desiccation for months at a time unique plant communities are found within the pools plant communities in vernal pools transition from one to another along subtle topographic gradients based on hydrologic thresholds keeley and zedler 1998 gosejohan et al 2017 vernal pool plant species must cope with inundated conditions during the time of seed germination and seedling establishment and also rapid desiccation in early summer many species of wetland plants are not found in vernal pools because of an inability to tolerate the severe soil desiccation and heat stress that occurs during summer conversely many upland grasses that can tolerate soil desiccation cannot tolerate inundation keeley and zedler 1998 perennial grass species are found more often above the high water line whereas vernal pool specialist species are usually found below the high water line bauder 2000 vernal pools in northeastern california and elsewhere may be threatened by climate change bauder 2005 brooks 2009 dettinger and cayan 1995 showed that air temperatures in winter months in the northern and central sierra nevada increased by roughly 2 c over their period of study 1940 1995 projections for the 21st century show temperatures increasing by 3 or 4 c for the region under a business as usual scenario stewart et al 2004 precipitation did not increase in this area between 1940 and 1995 dettinger and cayan 1995 and is not projected to change through the 21st century stewart et al 2004 maurer 2007 more recent research shows a projected increase in years with many atmospheric river precipitation events related to climate change scenarios in the northern sierra in the 21st century but also note that the uncertainty at regional scales is still large dettinger 2011 if these temperature and precipitation projections are correct water budgets of vernal pools in the region will likely be affected if all other variables are held constant a temperature increase without an increase in precipitation might lead to the pools being filled for a shorter amount of time because the increase in temperature would increase evaporation and more precipitation will fall as rain instead of snow leading to earlier snowmelt and more rapid drying of the pool substrate brooks 2009 berghuijs et al 2014 vernal pool plant communities are structured primarily by hydrologic influences such as hydroperiod inundation length and water depth e g emery et al 2009 gosejohan et al 2017 and therefore the species composition of vernal pools is expected to be sensitive to climate shifts that alter hydroperiod vernal pool vegetation may also be affected by changes in precipitation seasonality given that precipitation during winter and the early growing season has been observed to favor native vernal pool specialist species whereas late season precipitation favors non native plant species javornik and collinge 2016 because of the presence of endemic vegetation and distinct annual hydrologic patterns of vernal pools the goals of this research were to develop a modeling approach to examine if a changing climate has the potential to shorten the period of inundation hydroperiod and impact vegetation patterns of the vernal pools we coupled a hydrologic water balance model with a static model of plant community habitat associations to investigate the following hypotheses hypothesis 1 changes to precipitation and temperature regimes associated with climate change scenarios will decrease the period of inundation in vernal pools hypothesis 2 changes to precipitation and temperature regimes associated with climate change scenarios will decrease the maximum depth of water in vernal pools hypothesis 3 if hypotheses 1 or 2 are supported then changes to precipitation or temperature regimes associated with climate change will result in proportionally fewer vernal pool specialist species although vernal pools and their unique vegetation have been recognized for decades jepson 1925 keeley and zedler 1998 and there have been several studies of their vegetation bliss and zedler 1998 keeley 1990 spencer and riesberg 1998 as well as fauna baldwin et al 2006 simovich 1998 few attempts to model their hydrology have been made possibly because of the relatively small amount of water contained by vernal pools as well as their ephemeral nature furthermore previous hydrologic models of vernal pools did not link model outputs to expected changes in vegetation or fauna e g pyke 2004 boone et al 2006 maclean et al 2012 table 1 in the current study we linked the outputs of a hydrologic water balance model to known vernal plant community habitat associations to assess potential impacts of climate change on habitat availability for vernal pool specialist plants while our study focuses on one vernal pool in northern california and uses data from a limited time period our overarching objective is to demonstrate an approach that examines the hydrology and vegetation of vernal pools that can be applied in other landscapes with similar data limitations or that can be improved where more data are available in this article we describe the development of the hydrology model and its linkage to a vegetation model whose development is described in detail in gosejohan et al 2017 2 site description our vernal pool model was developed and calibrated for northeast coyote springs hereafter referred to as coyote springs in lassen national forest n 40 42 55 e 121 22 2 in northeastern california fig 1 as the location for this study the elevation of coyote springs is 1580 masl and the area is 4 6 ha unlike more frequently studied western u s vernal pools that are located in lower elevation valleys with mostly rain precipitation and often connected to other pools coyote springs is an isolated vernal pool that receives much of its precipitation as snow the pool is surrounded by coniferous forest composed primarily of calocedrus decurrens pinus ponderosa and pinus jeffreyi with an understory dominated by artemisia tridentata purshia tridentata and several perennial grass species dominant plant species within the vernal pool vary by hydroperiod and water depth with short inundated communities of vernal pool specialist species epilobium brachycarpum lotus purshianus microsteris gracilis var gracilis and trichostema oblongum and non native bromus hordeaceus and lactuca serriola moderately inundated communities of deschampsia danthonioides and grindelia nana cuscuta howelliana and epilobium densiflorum and long inundated communities of castilleja campestris ssp campestris eleocharis bella marsilea oligospora pilularia americana plagiobothrys stipitatus and psilocarphus brevissimus gosejohan 2012 gosejohan et al 2017 soils for coyote springs were identified as the skalan bobbitt families association web soil survey 2012 and the soils of the greater area are volcanic in origin norris and webb 1976 the morphology of soils in volcanic northern california vernal pools can be controlled by ferrolysis clay formation and translocation duripan formation and calcium carbonate formation rains et al 2008 ferrolysis occurs when soils alternate between aerobic and anaerobic conditions when the soils are inundated with water they become anaerobic which creates a reducing environment where free iron is reduced to ferrous iron fe2 and displaces base cations base cations silicates and bicarbonate move downward in the soil profile and precipitate out of solution as the pool dries this process forms a duripan that is thickest in the deepest parts of the pool and tapers towards the margins of the pool basin hobson and dahlgren 1998 fig 2 also the clay particles created during weathering move downward in the soil profile when the soil is saturated as the soil profile matures a gradient of soil texture where soil is coarser near the surface and finer soil is near the duripan develops hobson and dahlgren 1998 soil texture analysis of coyote springs soils taken at the surface and at depth reflected such morphology the aquitard of the duripan has two primary effects on the hydrology of vernal pools 1 it inhibits percolation from the pool above into the groundwater system and 2 it inhibits groundwater from entering the vernal pool 3 methods 3 1 stage gauges and field surveys water depths were measured at discrete locations within the pool using mounted game cameras and then extrapolated to the entire vernal pool at a daily time step using a detailed topographic survey gosejohan et al 2017 moultrie game spy i 65 digital cameras and two stage gauges were installed in december 2010 at coyote springs the stage gages were placed at the deepest part of the pool two were placed for redundancy in case one of the stage gages was damaged the cameras were programmed to take photographs of the stage gauges every four hours throughout the day the stage gauges were made from pvc pipe had tape markings at 2 cm intervals and were attached to steel rebar that was hammered into the ground stage measurements were recorded from the photographs from december 4 2010 to december 12 2011 in summer 2011 vegetation was surveyed in 1 m2 quadrats uniformly spaced at intersections within a 25 m grid and also every 10 m along two fine scale transects that ran the lengths of the major and minor axes of the pool in each 1 m2 quadrat all plant species were identified and cover classes estimated using a modified daubenmire 1959 classification please see gosejohan et al 2017 for more detail on the methods used for vegetation monitoring a topographic survey using a leica wild tc101 total station andtotal data systems recon with survey pro for windows xp version 3 8 1 data collector was also performed that included the vegetation survey points stage gauges and topographic features such as steep elevation gradients high points and the pool perimeter for a total of 350 survey points over approximately 5700 m2 the total station survey data were used to create a surface interpolation and stage area volume s a v relationship using kriging with 2 5 m grid resolution with surfer version 8 0 golden software 2002 the s a v relationship is a vertically discretized bathymetric surface that provides pool volume and area at a 2 cm vertical resolution the 2 cm vertical resolution of the hydrologic and vegetation model is conservatively constrained by the coarser of these two data sets i e stage gage as opposed to the total station survey the area and volume resolution was 1 m2 and 1 m3 respectively 3 2 coupled model development separate hydrologic and vegetation models were developed to work together by using hydrologic model output of hydroperiod and maximum depth as input to the vegetation model broadly speaking the hydrologic model was used to estimate the water depth and inundation duration spatially across the pool which were then used with the vegetation model to estimate vegetated community distribution 3 2 1 hydrologic model development the hydrologic model was built in excel with a time step of one quarter of a month the length of this time step was selected as a compromise between temporally disparate datasets that ranged in frequency between daily and monthly monthly datasets were linearly interpolated to quarterly data sets such as the publicly available monthly parameter elevation relationships on independent slopes model prism daly et al 1994 data whereas daily data sets were aggregated into quarters of a month such as the availability of vernal pool stage calibration data and water depth measurements from gosejohan et al 2017 on average one quarter of a month is 7 6 days inputs to the hydrologic model were mean air temperature and precipitation over water year wy 2011 see section 3 3 for more details on the climate inputs and the output was a time series of pool stage although the pool only had water for about 7 months the model covered the entire water year to ensure that the model simulated the beginning and ending of inundation appropriately the s a v relationship was used to calculate the water balance at each time step as well as the resulting pool stage at the deepest part of the pool as part of the water balance approach evapotranspiration was calculated for both the soil and the open water of the pool potential evapotranspiration was calculated using the hamon 1963 simplification of the thornthwaite method hamon 1963 thornthwaite 1948 potential evapotranspiration pet in mm was converted to actual evapotranspiration using coefficients unique to the soil and open water eqs 1 and 2 the coefficients αwater and αsoil were obtained through the calibration process described in section 3 2 2 although this approach does not directly account for water stress which can be important under arid conditions we applied this simplified approach because of limited data availability in future climate simulations 1 ae t soil 1000 α soil p e t aetsoil actual soil evapotranspiration m αsoil soil actual evapotranspiration coefficient unitless 2 ae t water 1000 α water p e t aetwater actual water evapotranspiration m αwater water actual evapotranspiration coefficient unitless the s a v relationship was used to calculate the evapotranspiration volume from aetwater that was then multiplied by the area of the pool from the previous time step eq 3 3 wve a e t water p o o l a r e a t 1 wve water volume evapotranspiration m3 pool areat 1 pool area from s a v used in the previous time step m2 to calculate the evapotranspiration from the soil aetsoil was multiplied by the maximum pool area minus the pool area from the previous time step eq 4 4 sve a e t soil maxpool poolare a t 1 sve soil volume evapotranspiration m3 maxpool maximum possible pool area m2 the maximum pool area was defined by the area of a polygon drawn around the interface of the surrounding trees and pool basin because the area adjacent to the pool is not much higher topographically and therefore contributes little water to the pool via runoff maxpool was determined to be 46 000 m2 precipitation volume input for each timestep was calculated by multiplying the precipitation for a quarter month by the maximum pool area eq 5 5 precipvolume p r e c i p m a x p o o l precipvolume water volume of precipitation m3 precip precipitation rate for a quarter of a month m quarter of a month the phase of the precipitation i e rain or snow was determined using the water and snow balance modeling system wasmod method xu 2002 in the wasmod method the phase of precipitation is a function of mean air temperature eqs 6 and 7 mean air temperature was calculated as the average of minimum temperature and maximum temperature for each timestep 6 s n t p t 1 e c t a 1 a 1 a 2 2 snt solid part of snow m3 pt precipitation m3 ct mean air temperature c a 1 air temperature above which all precipitation is rain c a 2 air temperature above which snow melting begins c 7 r t p t s n t rt rainfall m3 a 1 and a 2 were calibration parameters for the model rainfall enters directly into the soil and pool snow is added to the snowpack eq 8 and must melt before it is added to the soil and pool the melting function is also defined by the wasmod approach as a function of the a 1 and a 2 parameters eq 9 8 s p t s p t 1 s n t m t spt snowpack m3 mt snowmelt m3 spt 1 snowpack from previous time step m3 9 m t s p t 1 1 e c t a 2 a 1 a 2 2 water could leave the system by seepage through the duripan eq 10 the seepage rate was defined as a function of stage eq 11 10 seepage p o o l a r e a t 1 s e e p a g e r a t e seepage seepage volume m3 seepagerate rate of seepage m quarter of a month eq 11 11 seepagerate seepagecoeff c poolstage t 1 seepagecoeff calibration parameter m quarter of a month c conversion factor with a value of 1 m 1 poolstaget 1 pool stage from previous time step m the model was constructed such that when pool stage increased the volume of water associated with the stage increase conceptually entered two places the open water pool and the unwetted soil pores above the previous depth fig 2 the flux of water associated with water entering the unwetted soil pores was called the postpoolsoilwater eq 12 partitioning of water between the pool and postpoolsoilwater was determined by the basinsoilfactor and was calibrated to be a value between 0 and 1 the remaining fraction of precipitation entered the pool 12 postpoolsoilwater p r e c i p i t a t i o n b a s i n s o i l f a c t o r postpoolsoilwater volume of water diverted to soil when stage increases m3 basinsoilfactor calibrated factor that partitions precipitation between soil and pool when stage is increasing unitless before the pool could be created the soil pore volume between the lowest elevation in the pool basin see the horizontal black line in fig 2 and the duripan prepoolsoilstorage was required to be filled with water prepoolsoilstorage could range from zero to prepoolsoilstoragemax eq 13 13 prepoolsoilstorag e max m a x p o o l s o i l d e p t h p o r o s i t y prepoolsoilstoragemax maximum volume of storage in prepoolsoilstorage m3 soildepth calibrated average depth of soil between lowest elevation and duripan m porosity fraction of soil volume occupied by pores unitless the soildepth used to calculate prepoolsoilstorage was a calibration parameter maxpool and porosity were user specified values while a full texture analysis was not possible at coyote springs iubelt et al 2016 found surface soils in four nearby pools with similar morphology to have sandy loam loam and clay loam texture porosity was set to 0 5 which is consistent with silt loam 0 485 clay loam 0 476 and silty clay 0 492 clapp and hornberger 1978 as soil properties could not be measured using soildepth as a calibration parameter helped to approximate the volume of prepoolsoilstorage as a function of porosity and soildepth water was not routed into the pool until prepoolsoilstorage reached prepoolsoilstoragemax another water storage in the model was snowpack eq 8 precipitation that fell as snow was added to the snowpack water in the snowpack did not enter the prepoolsoilstorage or pool until it melted the model assumed the water did not re freeze a third water storage was the open water pool referred to as the pool water was allowed to exchange between the prepoolsoilstorage and the pool after the model calculated the balance of the fluxes from the system of equations for a timestep the balance was added positive or negative to the prepoolsoilstorage eq 14 14 prepoolsoilstorage m r s e e p a g e w v e s v e the model then evaluated if the prepoolsoilstorage was at maximum capacity if the prepoolsoilstorage was above maximum capacity excess water was routed to the pool otherwise the model routed water from the pool back to prepoolsoilstorage until prepoolsoilstoragemax was reached or left prepoolsoilstorage below prepoolsoilstoragemax if there was no water in the pool fig 3 3 2 2 hydrologic model calibration the calibration metrics used for fitting the model parameters were root mean squared error rmse for water depth over 33 temporal observations and hydroperiod difference at the stage gages eq 15 the rmse was only calculated over the period when there was water in the pool to be more conservative in calculating the rmse 15 hydroperiod d i f f o b s h y d r o p e r i o d m o d e l e d h y d r o p e r i o d obs h y d r o p e r i o d hydroperioddiff hydroperiod difference no of timesteps obs hydroperiod hydroperiod observed from stage gauges no of timesteps mod hydroperiod modeled hydroperiod no of timesteps hydroperioddiff was a calibration metric to penalize differences between modeled and observed pool hydroperiods where each timestep was one quarter of a month early calibration efforts using rmse alone resulted in parameter values that wetted the pool later and dried the pool earlier than was observed this disproportionately affected the deepest parts of the pool as vernal pool specialist species are typically found in the deeper parts of the pool bauder 2000 it was important that the parameter values selected model the period of inundation well for this portion of the pool initialsoilstorage is a parameter that is defined as the volume of water that is in the soil at the start of the model and is calibrated for table 2 a monte carlo approach was used because there can be multiple optimal parameter sets for this model random non integer values were selected from a uniform distribution of the specified range of values for each parameter and solver function in excel was executed to minimize the sum of rmse and hydroperiodlengthdiff by adjusting parameter values this procedure was executed 200 times the final calibrated parameter values were the values for the model run that had the smallest sum of rmse and hydroperiodlengthdiff of the 200 monte carlo runs table 2 3 2 3 vegetation model development the vegetation model was developed as a statistical model that relates the probability of occurrence for a given plant community type to the inundation length hydroperiod and maximum depth associated with a 1 m2 frame within which vegetation was sampled see section 3 1 for details the model was developed by coupling data gathered from the vegetation survey with stage gage data as described in gosejohan et al 2017 the overall approach was similar to that of auble et al 1994 where riparian plant community types were positioned along a gradient of inundation duration using a direct gradient approach allowing projections of how changes in river management would alter plant community composition gosejohan et al 2017 used non metric multidimensional scaling nms ordination analysis to determine that inundation length and maximum depth were predictive hydrologic variables for vernal pool plant species hierarchical agglomerative cluster and indicator species analyses dufrêne and legendre 1997 were used to group the sites according to plant species composition into five broad plant community types defined by annual hydroperiod and maximum water depth short term inundated edge shallow tolerant deep tolerant and long term inundated gosejohan et al 2017 only the edge shallow tolerant and long term inundated community types were identified in coyote springs classification tree analysis cart was used to model the probability of occurrence for each plant community group at coyote springs according to habitat requirement thresholds of hydroperiod and maximum water depth at an annual resolution because hydrologic data were collected for a single year whereas some of the plant species present were perennial species that persist for multiple growing seasons it was assumed that the year of sampling was broadly representative with respect to the habitat requirements of dominant plant species the hydroperiod model for coyote springs predicted the edge community group to occur at hydroperiods of less than 101 5 days the shallow tolerant community group to occur at hydroperiods of 101 5 209 5 days and the long term inundated community group to occur at greater than 209 5 days of inundation in the maximum depth model for coyote springs edge community shallow tolerant and long term inundated community species were associated with maximum water depths of less than 15 5 cm 15 5 26 87 cm and more than 26 87 cm respectively although the hard thresholds suggested by the cart model are an artifact of the statistical modeling approach and do not reflect the variability in vegetation hydrology relationships found in nature they provide useful information for projecting directional shifts of plant community types in response to hydrologic change therefore areal coverage of each of the three plant community groups was projected according to the cart thresholds given simulated estimates of the two hydrologic variables for various climate change scenarios to couple the vegetation data with stage gage data for linkage with the hydrologic model output a total station survey of the pool was used to create a bathymetric surface from which water elevations from the stage gage could be projected to any location within the pool see section 3 1 maximum depth was defined as the maximum depth of water above a given elevation within the pool for example if the maximum stage gauge reading for a given year was 0 24 m the maximum depth for the portion of the pool associated with an elevation 0 00 m on the stage gauge would have a maximum depth of 0 24 m whereas the portion of the pool associated with the elevation at 0 22 m on the stage gauge would have a maximum depth of 0 02 m thus the maximum depth for each 2 cm elevation increment was calculated by taking the difference between the maximum stage gauge reading for a given year and the elevation at the location with the deepest part of the pool having a datum value of zero hydroperiod was calculated by summing the number of quarter months that the pool stage was above a given discretized 2 cm elevation the number of quarter months was then multiplied by 7 6 days to determine days of inundation the model then determined which vegetation type to assign to a given area associated with each 2 cm elevation increment within the pool according to the hierarchical classification provided by the cart analysis the area associated within each 2 cm increment within the pool was greater than 1400 m2 for all increments the model output was vegetation community type as percentage of maxpool vernal pool specialist species are among the indicator species found in shallow tolerant and long term inundated communities therefore for the purposes of our analysis we considered increases in long term inundated and shallow tolerant communities as indicating increases in vernal pool specialist species habitat 3 3 climate data and climate change scenarios prism precipitation and temperature data were used as the climate inputs for the hydrologic model for calibration and historical runs prism data were used for precipitation and air temperature inputs to the model for calibration of water year 2011 by comparing model output to stage gauge data datasets of precipitation and minimum and maximum temperature for climate change scenarios were obtained from the bias correctionand constructed analogs bcca version 2 which were downscaled from the world climate research programme s coupled model intercomparison project phase 3 cmip3 multi model dataset maurer et al 2007 available at http gdo dcp ucllnl org bcca data are available at daily time increments and 1 8th of a degree grid size at the time this study was developed only cmip3 data were available we use all the downscaled gcms available by the bcca products for a complete list of gcms refer to maurer et al 2007 the b1 emission scenario that assumes an aggressive emission reduction policy and a2 emission scenario that assumes unconstrained growth scenarios were used from these models as they were respectively the most and least conservative of the commonly modeled emission scenarios from ipcc 2007 because the gcm datasets are gridded to much larger cell sizes 12 km than the prism dataset 1 km the prism dataset was used for additional bias correction to bias correct temperature mean temperature was calculated as the average of maximum and minimum temperature for the prism dataset and each gcm dataset for the period 1980 2000 the difference between the prism and gcm mean temperature for 1980 2000 was added to each gcm dataset to bias correct precipitation mean annual precipitation was calculated for the prism dataset and each gcm dataset for the period 1980 2000 the difference between the prism and gcm mean annual precipitation was calculated as a percentage difference and was used to scale the gcm precipitation values the hydrologic and vegetation see section 3 2 models were run for two ten year time periods 1990 1999 historical and 2090 2099 future for 1990 1999 prism and each bias corrected gcm dataset was used for a total of nineteen historical model runs for 2090 2099 the datasets for the a2 and b1 scenarios for each bias corrected gcm were used for a total of thirty six future model runs differences were calculated between the 10 year means of the hydrologic model results for hydroperiod and maximum depth between historic 1990 1999 and future 2090 2099 time segments for each gcm and emission scenario results of vegetation models were compared using box plots for each gcm and averages of the gcms 4 results 4 1 hydrology the parameter values for the hydrologic model that produced the smallest amount of error for coyote springs yielded an rmse hydroperiodlengthdiff of 0 025 m 0 timesteps and an r2 value of 0 962 fig 4 indicating the model performed well in reproducing the hydrology of the vernal pool the rmse of 0 025 m was 8 9 of maximum depth model results from runs with the 18 gcm inputs indicate that the average percentage of precipitation that was rain increased from 81 5 in the historic period to 94 3 for the a2 scenario and 89 9 for the b1 scenario fig 5 average hydroperiods for the historical time period 1991 2000 a2 scenario and b1 scenario were 225 range 207 252 193 range 144 226 and 204 range 172 223 days respectively fig 6 a the historical period had longer hydroperiods than either of the two climate change scenarios fig 6a there was little change in maximum depth for decadal averages among all emissions scenarios fig 6b the historic mean was 26 cm range 23 28 cm the a2 mean was 27 cm range 22 37 cm and the b1 mean was 27 cm range 23 32 cm 4 2 vegetation because the predicted change in maximum depth did not differ significantly among emission scenarios fig 6b we used only hydroperiod and not maximum depth to simulate changes in the vegetation community when averaging gcm results by emission scenario the mean historical 1990 1999 edge community type occurred in 44 5 of the pool fig 7 and increased for future emission scenarios table 3 the model indicated a transition from the long term inundated community to the edge and shallow tolerant communities for both climate scenarios as compared to historical conditions 5 discussion 5 1 susceptibility of vernal pool hydrology and vegetation to climate change scenarios winter 2000 noted that because many wetlands rely on precipitation as their main source of water input they can be more susceptible to climate change vernal pools are seasonal wetlands and the pools studied also have little watershed inputs besides precipitation precipitation for the study area is projected to remain unchanged in the next century dettinger and cayan 1995 and the cmip3 gcm data for our study area showed very similar timing and amounts of precipitation in future emission scenarios when compared to the historical time period this resulted in little change in maximum depth values between the historical and future scenarios because similar amounts of water entered the pool primarily in the winter months when most precipitation occurs and et is minimal our simulations therefore indicated more response to temperature in the summer because it affects evaporation air temperature was higher for both future emission scenarios with a2 being more pronounced which resulted in faster drying of the pools shorter periods of inundation and greater percentages of rain versus snow precipitation figs 5 and 6 the 18 gcms in our study only had an average of 7 and 4 increase in precipitation for the a2 and b1 scenarios respectively as compared to historical conditions which may have contributed to the stronger responses to air temperature in our simulations we note that updated climate change scenarios from cmip5 taylor et al 2012 indicate that upper atmospheric large scale dynamics can lead to an increase in northern california winter precipitation in the late 21st century neelin et al 2013 future work is necessary to understand potential impacts and uncertainty of such large scale dynamic changes over northern california s complex terrain as such changes could affect precipitation climate projections pyke 2005 modeled climate change scenarios that increased precipitation by 10 for each 1 c increase in air temperature for central valley vernal pools and concluded that hydroperiod increased and consequently conditions for branchiopod reproduction improved with climate change precipitation in the pyke 2005 study was increased by as much as 30 whereas the gcms we used had average increased precipitation for our study area of 2 1 for a2 and 2 5 for b1 scenarios per degree c increase in air temperature which could explain the differences between that study s results and ours for hydroperiod we argue that our approach is more robust because it uses more recent climate scenarios and downscaled gcm data rather than assuming a constant increase in temperature and precipitation our approach suggests that climate change scenario conditions for vernal pool specialists may become more challenging than those shown by pyke 2005 our simulations suggest that both climate change projections considered would result in sharp reductions in hydroperiod which in turn would lead to declines of the long term inundated community whereas edge and shallow tolerant communities would increase the vernal pool specialist species of primary conservation concern are more commonly found in the long term inundated community where extended seasonal periods of inundation act as an ecological filter to exclude both wetland generalists and invasive plant species javornik and collinge 2016 gosejohan et al 2017 previous studies have also found the hydroperiod to be critically important for determining plant community structure in vernal pool ecosystems bauder 2000 deil 2005 emery et al 2009 gosejohan et al 2017 5 2 hydrologic modeling approach the objective of this study was to develop and apply a modeling approach linking climate hydrology and vegetation dynamics for vernal pools the simple water balance approach performed reasonably well for the coyote springs vernal pool in terms of modeling hydroperiod and water depth over time although pyke 2004 suggested that less than daily time steps would be better for modeling vernal pool hydrologic processes in his models of central valley and coastal vernal pools in california we had similar limitations in data availability to the pyke 2004 study and found the weekly time step to be appropriate for this vernal pool in northern california given the uncertainties of the data used to model the system we prioritized fitting the timing and duration of the hydroperiod in testing model fit because of the objective of linking hydrology and vegetation the coyote springs hydrologic model used surveyed topography to develop a relationship between pool volume and water depth that was more geometrically resolved than the representation used by boone et al 2006 and pyke 2004 thus the model represented the more complex geometry of coyote springs where deeper regions of the vernal pool had steeper slopes than the edge of the pool unlike boone et al 2006 who modeled vernal pools in minnesota we did not explicitly include surface water inflow from runoff to the pool because of the very small watershed surrounding the pool but we did include calculated snowmelt at each timestep 5 2 1 limitations of the hydrologic modeling approach the et method used for this study was simple and based on air temperature only while there is some evidence that temperature based et methods such as hamon 1963 are better than radiation based estimates such as the priestly taylor approach in humid environments lu et al 2005 it would be preferable to use penman monteith or a fully physical et model such as that used in donohue et al 2010 given the dry conditions at the site in addition the use of a constant relationship between aet and pet in the model does not represent some dynamics associated with soil drying that could affect et and vegetation however when considering the period of interest in this study is 2090 2099 there is large uncertainty associated with many of the penman monteith input parameters e g solar radiation that is dependent on cloud cover as well as other input parameters that are unavailable ground heat flux from climate models furthermore the large difference in size between climate model cells and the coyote springs site makes downscaling many of the parameters inappropriate mcvicar et al 2012 found broad decreases in wind across the globe in recent decades that resulted in penman monteith overestimating et given the scope of the study and current climate data available the use of a temperature based model was most appropriate some models have used a water stress factor that is a function of soil depth and water content to estimate aet from pet e g arnold et al 1998 which could be an appropriate modification of our approach to better represent the relationship between aet and pet however our approach did include separate calibration parameters for a soil factor and an open water factor eqs 1 and 2 and table 2 which may somewhat indirectly account for water stress the modeling approach used in this study could also be strengthened when climate parameters such as wind speed and vapor pressure become available with more certainty in the climate modeling data set so that radiation and physically based et methods could be applied we also acknowledge that there is uncertainty in the future projections of temperature and precipitation and hence our estimates of et because of that uncertainty we used all 18 gcms and two emissions scenarios i e a2 and b1 to examine future conditions regarding snow accumulation and melt the parameter for which snow begins melting a 2 was calibrated to be 10 8 c while this temperature is considerably lower than the freezing point of water snowmelt is a function not only of temperature but also of short and longwave radiation coyote springs is free from large vegetation and other objects that could provide shade from solar radiation which could lead to snowmelt occurring at temperatures below the freezing point the use of a constant partitioning between soil water and open water was a simplification that could be improved upon with more data as pool stage increases per unit of stage increase requires more water to both wet the soil and fill the pool because of the expanding pool perimeter and area while partitioning of water between the soil and the pool in reality is a head dependent process determination of the head dependence would require a duripan survey which was not possible for this study 5 3 data limitations model development calibration and validation were limited by data availability we were only able to collect data over one year at one vernal pool coyote springs so the model was only calibrated for that particular year and location ideally the model would have multiple years of data for calibration while all processes simulated within the model would benefit from more calibration data parameters that directly affect initial pool formation for a given year e g soildepth would in particular benefit from additional data initial pool formation is responsive to antecedent hydrologic factors therefore multiple years of data would be preferable for calibrating the model data were not available for an independent validation of the model assessment of model generalizability will require applying the model to additional vernal pools that vary in soils morphology climate and other variables this in turn would benefit from long term data collection for hydrologic variables and plant community structure across a diverse network of monitoring sites 5 4 suggestions for future work overall the coupling of hydrologic and vegetative models is a promising approach to quantify changes in vernal pool plant communities under a variety of climate scenarios previous studies have applied similar approaches to couple process based hydrologic models with empirical models of vegetation response to hydrology for riverine systems and wet meadow communities auble et al 1994 springer et al 1999 rains et al 2004 hammersmark et al 2010 in this study we used output of water depth with a statistically derived vegetation model to examine the potential impacts of climate change projections on vernal pool vegetation communities we do have several suggestions for future work that could improve this approach we used statistics with field data to establish the relationships between vernal pool vegetation and hydrology but more mechanistic information on this relationship would strengthen process modeling efforts while some studies have tried to isolate the effects of hydrology on the germination process of vernal pool plants in a controlled laboratory setting e g bliss and zedler 1998 more laboratory experiments would be beneficial for determining hydrologic thresholds while controlling for confounding variables e g isolating the effect of hydroperiod from that of seasonally varying water depths in addition many factors that are species specific besides hydrology can influence plant community structure a coupled model that considers other variables in addition to hydroperiod and maximum depth would likely improve modeled results possibilities for model variables include ph water temperature and soil texture we also encourage collecting more data and developing a more mechanistic modeling approach data collection from multiple pools over longer periods of time to allow more robust testing and calibration of hydrologic and vegetation models such data would also enable the development of a mechanistic hydrologic modeling approach for a more generalized process based understanding of vernal pool hydrology in addition better data would enable the use of more physically based et approaches 6 conclusion previous studies bauder 2000 gosejohan et al 2017 etc have indicated that hydrology is a central driving environmental factor for determining ecological community distribution within vernal pools in this study a modeling approach that combined hydrology and vegetation models was developed and used to investigate the potential implications of climate change on vernal pool hydrology and specialist species in northern california results indicate that climate change could lead to shorter hydroperiod which could be challenging for vernal pool specialist species additional studies of vernal pools with coupled hydrology and vegetation and faunal models would enable better understanding of implications of climate change for vernal pools in wetter or warmer climate regions such coupled models can help resource managers to assess the relative effectiveness of alternative hydrologic restoration scenarios for mitigating climate change effects on these unique ecosystems declaration of interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded in part by the nevada agricultural experiment station and the usda forest service the authors thank mirte iubelt michael rosen chris daly dan mcevoy louis provencher alexis montrone and robert blank for their assistance with the project thanks are extended to the desert research institute division of atmospheric sciences for partially supporting j f mejia we acknowledge the modeling groups the program for climate model diagnosis and intercomparison pcmdi and the wcrp s working group on coupled modelling wgcm for their roles in making available the wcrp cmip3 multi model dataset support of this dataset is provided by the office of science u s department of energy we also thank four anonymous reviewers and the journal of hydrology editorial team for their helpful review and comments 
6481,vernal pools are seasonal wetlands that have a high diversity of endemic and native plant species yet they are threatened by agricultural conversion and urban development and face threats posed by climate change resulting from altered precipitation and temperature regimes we developed an approach to investigate the potential impacts of climate change on hydrology and vegetation communities of vernal pools by creating a mass balance hydrologic model that is coupled to a statistical model of plant community distribution the hydrologic and vegetative models were calibrated using field measurements from a vernal pool in northeastern california that experiences snow dominated hydrology and is larger than vernal pools in more studied areas like central california but representative of other northern california vernal pools using downscaled data from global climate models the coupled model suggests that warmer conditions will lead to the pool being inundated for a shorter time but with little change in maximum depth reduced hydroperiods suggest possible declines in vernal pool specialist species with future climate change the coupled model is an integrated approach for understanding the impact of altered environmental conditions on unique hydrology and plant community composition of vernal pool ecosystems but the model approach could be improved with longer term data and by applying it at more sites to broaden the applicability of the approach and to enable better process representation keywords ephemeral pool hydrologic modeling hydroperiod wetland plants seasonal wetland 1 introduction vernal pools are ephemeral wetlands that exist in local topographic depressions with relatively impermeable substrates keeley and zedler 1998 boone et al 2006 in temperate regions experiencing winter snowfall like northern california they are subject to four distinct seasons they typically 1 fill with snow in the winter 2 melt into inundated pools in the spring 3 become unsaturated and vegetated by summer and then 4 dry and become fully desiccated by autumn their main source of water is direct precipitation though some pools receive runoff from a very small watershed keeley and zedler 1998 the substrates of vernal pools can be bedrock such as volcanic mud or lava flows clay rich soils cemented mudflow and soils with hard clay pans or duripans hobson and dahlgren 1998 keeley and zedler 1998 smith and verrill 1998 boone et al 2006 all of these substrates have low hydraulic conductivities as these low permeability substrates restrict the infiltration of water small amounts of precipitation can produce perched zones of saturation and pooling of water rains et al 2008 the ephemeral nature of vernal pools and associated vegetation is also influenced by climate vernal pools are distinguished by the source and seasonality of their water inputs vernal pools associated with mediterranean climates can receive a majority of precipitation in the cooler months of the winter whereas summers are hot and dry keeley and zedler 1998 vernal pool vegetation distribution varies from year to year depending on climatic conditions such as precipitation amount and timing as well as air temperature brooks 2004 keeley and zedler 1998 because vernal pools have seasonally distinct climatic conditions i e alternating between inundation and desiccation for months at a time unique plant communities are found within the pools plant communities in vernal pools transition from one to another along subtle topographic gradients based on hydrologic thresholds keeley and zedler 1998 gosejohan et al 2017 vernal pool plant species must cope with inundated conditions during the time of seed germination and seedling establishment and also rapid desiccation in early summer many species of wetland plants are not found in vernal pools because of an inability to tolerate the severe soil desiccation and heat stress that occurs during summer conversely many upland grasses that can tolerate soil desiccation cannot tolerate inundation keeley and zedler 1998 perennial grass species are found more often above the high water line whereas vernal pool specialist species are usually found below the high water line bauder 2000 vernal pools in northeastern california and elsewhere may be threatened by climate change bauder 2005 brooks 2009 dettinger and cayan 1995 showed that air temperatures in winter months in the northern and central sierra nevada increased by roughly 2 c over their period of study 1940 1995 projections for the 21st century show temperatures increasing by 3 or 4 c for the region under a business as usual scenario stewart et al 2004 precipitation did not increase in this area between 1940 and 1995 dettinger and cayan 1995 and is not projected to change through the 21st century stewart et al 2004 maurer 2007 more recent research shows a projected increase in years with many atmospheric river precipitation events related to climate change scenarios in the northern sierra in the 21st century but also note that the uncertainty at regional scales is still large dettinger 2011 if these temperature and precipitation projections are correct water budgets of vernal pools in the region will likely be affected if all other variables are held constant a temperature increase without an increase in precipitation might lead to the pools being filled for a shorter amount of time because the increase in temperature would increase evaporation and more precipitation will fall as rain instead of snow leading to earlier snowmelt and more rapid drying of the pool substrate brooks 2009 berghuijs et al 2014 vernal pool plant communities are structured primarily by hydrologic influences such as hydroperiod inundation length and water depth e g emery et al 2009 gosejohan et al 2017 and therefore the species composition of vernal pools is expected to be sensitive to climate shifts that alter hydroperiod vernal pool vegetation may also be affected by changes in precipitation seasonality given that precipitation during winter and the early growing season has been observed to favor native vernal pool specialist species whereas late season precipitation favors non native plant species javornik and collinge 2016 because of the presence of endemic vegetation and distinct annual hydrologic patterns of vernal pools the goals of this research were to develop a modeling approach to examine if a changing climate has the potential to shorten the period of inundation hydroperiod and impact vegetation patterns of the vernal pools we coupled a hydrologic water balance model with a static model of plant community habitat associations to investigate the following hypotheses hypothesis 1 changes to precipitation and temperature regimes associated with climate change scenarios will decrease the period of inundation in vernal pools hypothesis 2 changes to precipitation and temperature regimes associated with climate change scenarios will decrease the maximum depth of water in vernal pools hypothesis 3 if hypotheses 1 or 2 are supported then changes to precipitation or temperature regimes associated with climate change will result in proportionally fewer vernal pool specialist species although vernal pools and their unique vegetation have been recognized for decades jepson 1925 keeley and zedler 1998 and there have been several studies of their vegetation bliss and zedler 1998 keeley 1990 spencer and riesberg 1998 as well as fauna baldwin et al 2006 simovich 1998 few attempts to model their hydrology have been made possibly because of the relatively small amount of water contained by vernal pools as well as their ephemeral nature furthermore previous hydrologic models of vernal pools did not link model outputs to expected changes in vegetation or fauna e g pyke 2004 boone et al 2006 maclean et al 2012 table 1 in the current study we linked the outputs of a hydrologic water balance model to known vernal plant community habitat associations to assess potential impacts of climate change on habitat availability for vernal pool specialist plants while our study focuses on one vernal pool in northern california and uses data from a limited time period our overarching objective is to demonstrate an approach that examines the hydrology and vegetation of vernal pools that can be applied in other landscapes with similar data limitations or that can be improved where more data are available in this article we describe the development of the hydrology model and its linkage to a vegetation model whose development is described in detail in gosejohan et al 2017 2 site description our vernal pool model was developed and calibrated for northeast coyote springs hereafter referred to as coyote springs in lassen national forest n 40 42 55 e 121 22 2 in northeastern california fig 1 as the location for this study the elevation of coyote springs is 1580 masl and the area is 4 6 ha unlike more frequently studied western u s vernal pools that are located in lower elevation valleys with mostly rain precipitation and often connected to other pools coyote springs is an isolated vernal pool that receives much of its precipitation as snow the pool is surrounded by coniferous forest composed primarily of calocedrus decurrens pinus ponderosa and pinus jeffreyi with an understory dominated by artemisia tridentata purshia tridentata and several perennial grass species dominant plant species within the vernal pool vary by hydroperiod and water depth with short inundated communities of vernal pool specialist species epilobium brachycarpum lotus purshianus microsteris gracilis var gracilis and trichostema oblongum and non native bromus hordeaceus and lactuca serriola moderately inundated communities of deschampsia danthonioides and grindelia nana cuscuta howelliana and epilobium densiflorum and long inundated communities of castilleja campestris ssp campestris eleocharis bella marsilea oligospora pilularia americana plagiobothrys stipitatus and psilocarphus brevissimus gosejohan 2012 gosejohan et al 2017 soils for coyote springs were identified as the skalan bobbitt families association web soil survey 2012 and the soils of the greater area are volcanic in origin norris and webb 1976 the morphology of soils in volcanic northern california vernal pools can be controlled by ferrolysis clay formation and translocation duripan formation and calcium carbonate formation rains et al 2008 ferrolysis occurs when soils alternate between aerobic and anaerobic conditions when the soils are inundated with water they become anaerobic which creates a reducing environment where free iron is reduced to ferrous iron fe2 and displaces base cations base cations silicates and bicarbonate move downward in the soil profile and precipitate out of solution as the pool dries this process forms a duripan that is thickest in the deepest parts of the pool and tapers towards the margins of the pool basin hobson and dahlgren 1998 fig 2 also the clay particles created during weathering move downward in the soil profile when the soil is saturated as the soil profile matures a gradient of soil texture where soil is coarser near the surface and finer soil is near the duripan develops hobson and dahlgren 1998 soil texture analysis of coyote springs soils taken at the surface and at depth reflected such morphology the aquitard of the duripan has two primary effects on the hydrology of vernal pools 1 it inhibits percolation from the pool above into the groundwater system and 2 it inhibits groundwater from entering the vernal pool 3 methods 3 1 stage gauges and field surveys water depths were measured at discrete locations within the pool using mounted game cameras and then extrapolated to the entire vernal pool at a daily time step using a detailed topographic survey gosejohan et al 2017 moultrie game spy i 65 digital cameras and two stage gauges were installed in december 2010 at coyote springs the stage gages were placed at the deepest part of the pool two were placed for redundancy in case one of the stage gages was damaged the cameras were programmed to take photographs of the stage gauges every four hours throughout the day the stage gauges were made from pvc pipe had tape markings at 2 cm intervals and were attached to steel rebar that was hammered into the ground stage measurements were recorded from the photographs from december 4 2010 to december 12 2011 in summer 2011 vegetation was surveyed in 1 m2 quadrats uniformly spaced at intersections within a 25 m grid and also every 10 m along two fine scale transects that ran the lengths of the major and minor axes of the pool in each 1 m2 quadrat all plant species were identified and cover classes estimated using a modified daubenmire 1959 classification please see gosejohan et al 2017 for more detail on the methods used for vegetation monitoring a topographic survey using a leica wild tc101 total station andtotal data systems recon with survey pro for windows xp version 3 8 1 data collector was also performed that included the vegetation survey points stage gauges and topographic features such as steep elevation gradients high points and the pool perimeter for a total of 350 survey points over approximately 5700 m2 the total station survey data were used to create a surface interpolation and stage area volume s a v relationship using kriging with 2 5 m grid resolution with surfer version 8 0 golden software 2002 the s a v relationship is a vertically discretized bathymetric surface that provides pool volume and area at a 2 cm vertical resolution the 2 cm vertical resolution of the hydrologic and vegetation model is conservatively constrained by the coarser of these two data sets i e stage gage as opposed to the total station survey the area and volume resolution was 1 m2 and 1 m3 respectively 3 2 coupled model development separate hydrologic and vegetation models were developed to work together by using hydrologic model output of hydroperiod and maximum depth as input to the vegetation model broadly speaking the hydrologic model was used to estimate the water depth and inundation duration spatially across the pool which were then used with the vegetation model to estimate vegetated community distribution 3 2 1 hydrologic model development the hydrologic model was built in excel with a time step of one quarter of a month the length of this time step was selected as a compromise between temporally disparate datasets that ranged in frequency between daily and monthly monthly datasets were linearly interpolated to quarterly data sets such as the publicly available monthly parameter elevation relationships on independent slopes model prism daly et al 1994 data whereas daily data sets were aggregated into quarters of a month such as the availability of vernal pool stage calibration data and water depth measurements from gosejohan et al 2017 on average one quarter of a month is 7 6 days inputs to the hydrologic model were mean air temperature and precipitation over water year wy 2011 see section 3 3 for more details on the climate inputs and the output was a time series of pool stage although the pool only had water for about 7 months the model covered the entire water year to ensure that the model simulated the beginning and ending of inundation appropriately the s a v relationship was used to calculate the water balance at each time step as well as the resulting pool stage at the deepest part of the pool as part of the water balance approach evapotranspiration was calculated for both the soil and the open water of the pool potential evapotranspiration was calculated using the hamon 1963 simplification of the thornthwaite method hamon 1963 thornthwaite 1948 potential evapotranspiration pet in mm was converted to actual evapotranspiration using coefficients unique to the soil and open water eqs 1 and 2 the coefficients αwater and αsoil were obtained through the calibration process described in section 3 2 2 although this approach does not directly account for water stress which can be important under arid conditions we applied this simplified approach because of limited data availability in future climate simulations 1 ae t soil 1000 α soil p e t aetsoil actual soil evapotranspiration m αsoil soil actual evapotranspiration coefficient unitless 2 ae t water 1000 α water p e t aetwater actual water evapotranspiration m αwater water actual evapotranspiration coefficient unitless the s a v relationship was used to calculate the evapotranspiration volume from aetwater that was then multiplied by the area of the pool from the previous time step eq 3 3 wve a e t water p o o l a r e a t 1 wve water volume evapotranspiration m3 pool areat 1 pool area from s a v used in the previous time step m2 to calculate the evapotranspiration from the soil aetsoil was multiplied by the maximum pool area minus the pool area from the previous time step eq 4 4 sve a e t soil maxpool poolare a t 1 sve soil volume evapotranspiration m3 maxpool maximum possible pool area m2 the maximum pool area was defined by the area of a polygon drawn around the interface of the surrounding trees and pool basin because the area adjacent to the pool is not much higher topographically and therefore contributes little water to the pool via runoff maxpool was determined to be 46 000 m2 precipitation volume input for each timestep was calculated by multiplying the precipitation for a quarter month by the maximum pool area eq 5 5 precipvolume p r e c i p m a x p o o l precipvolume water volume of precipitation m3 precip precipitation rate for a quarter of a month m quarter of a month the phase of the precipitation i e rain or snow was determined using the water and snow balance modeling system wasmod method xu 2002 in the wasmod method the phase of precipitation is a function of mean air temperature eqs 6 and 7 mean air temperature was calculated as the average of minimum temperature and maximum temperature for each timestep 6 s n t p t 1 e c t a 1 a 1 a 2 2 snt solid part of snow m3 pt precipitation m3 ct mean air temperature c a 1 air temperature above which all precipitation is rain c a 2 air temperature above which snow melting begins c 7 r t p t s n t rt rainfall m3 a 1 and a 2 were calibration parameters for the model rainfall enters directly into the soil and pool snow is added to the snowpack eq 8 and must melt before it is added to the soil and pool the melting function is also defined by the wasmod approach as a function of the a 1 and a 2 parameters eq 9 8 s p t s p t 1 s n t m t spt snowpack m3 mt snowmelt m3 spt 1 snowpack from previous time step m3 9 m t s p t 1 1 e c t a 2 a 1 a 2 2 water could leave the system by seepage through the duripan eq 10 the seepage rate was defined as a function of stage eq 11 10 seepage p o o l a r e a t 1 s e e p a g e r a t e seepage seepage volume m3 seepagerate rate of seepage m quarter of a month eq 11 11 seepagerate seepagecoeff c poolstage t 1 seepagecoeff calibration parameter m quarter of a month c conversion factor with a value of 1 m 1 poolstaget 1 pool stage from previous time step m the model was constructed such that when pool stage increased the volume of water associated with the stage increase conceptually entered two places the open water pool and the unwetted soil pores above the previous depth fig 2 the flux of water associated with water entering the unwetted soil pores was called the postpoolsoilwater eq 12 partitioning of water between the pool and postpoolsoilwater was determined by the basinsoilfactor and was calibrated to be a value between 0 and 1 the remaining fraction of precipitation entered the pool 12 postpoolsoilwater p r e c i p i t a t i o n b a s i n s o i l f a c t o r postpoolsoilwater volume of water diverted to soil when stage increases m3 basinsoilfactor calibrated factor that partitions precipitation between soil and pool when stage is increasing unitless before the pool could be created the soil pore volume between the lowest elevation in the pool basin see the horizontal black line in fig 2 and the duripan prepoolsoilstorage was required to be filled with water prepoolsoilstorage could range from zero to prepoolsoilstoragemax eq 13 13 prepoolsoilstorag e max m a x p o o l s o i l d e p t h p o r o s i t y prepoolsoilstoragemax maximum volume of storage in prepoolsoilstorage m3 soildepth calibrated average depth of soil between lowest elevation and duripan m porosity fraction of soil volume occupied by pores unitless the soildepth used to calculate prepoolsoilstorage was a calibration parameter maxpool and porosity were user specified values while a full texture analysis was not possible at coyote springs iubelt et al 2016 found surface soils in four nearby pools with similar morphology to have sandy loam loam and clay loam texture porosity was set to 0 5 which is consistent with silt loam 0 485 clay loam 0 476 and silty clay 0 492 clapp and hornberger 1978 as soil properties could not be measured using soildepth as a calibration parameter helped to approximate the volume of prepoolsoilstorage as a function of porosity and soildepth water was not routed into the pool until prepoolsoilstorage reached prepoolsoilstoragemax another water storage in the model was snowpack eq 8 precipitation that fell as snow was added to the snowpack water in the snowpack did not enter the prepoolsoilstorage or pool until it melted the model assumed the water did not re freeze a third water storage was the open water pool referred to as the pool water was allowed to exchange between the prepoolsoilstorage and the pool after the model calculated the balance of the fluxes from the system of equations for a timestep the balance was added positive or negative to the prepoolsoilstorage eq 14 14 prepoolsoilstorage m r s e e p a g e w v e s v e the model then evaluated if the prepoolsoilstorage was at maximum capacity if the prepoolsoilstorage was above maximum capacity excess water was routed to the pool otherwise the model routed water from the pool back to prepoolsoilstorage until prepoolsoilstoragemax was reached or left prepoolsoilstorage below prepoolsoilstoragemax if there was no water in the pool fig 3 3 2 2 hydrologic model calibration the calibration metrics used for fitting the model parameters were root mean squared error rmse for water depth over 33 temporal observations and hydroperiod difference at the stage gages eq 15 the rmse was only calculated over the period when there was water in the pool to be more conservative in calculating the rmse 15 hydroperiod d i f f o b s h y d r o p e r i o d m o d e l e d h y d r o p e r i o d obs h y d r o p e r i o d hydroperioddiff hydroperiod difference no of timesteps obs hydroperiod hydroperiod observed from stage gauges no of timesteps mod hydroperiod modeled hydroperiod no of timesteps hydroperioddiff was a calibration metric to penalize differences between modeled and observed pool hydroperiods where each timestep was one quarter of a month early calibration efforts using rmse alone resulted in parameter values that wetted the pool later and dried the pool earlier than was observed this disproportionately affected the deepest parts of the pool as vernal pool specialist species are typically found in the deeper parts of the pool bauder 2000 it was important that the parameter values selected model the period of inundation well for this portion of the pool initialsoilstorage is a parameter that is defined as the volume of water that is in the soil at the start of the model and is calibrated for table 2 a monte carlo approach was used because there can be multiple optimal parameter sets for this model random non integer values were selected from a uniform distribution of the specified range of values for each parameter and solver function in excel was executed to minimize the sum of rmse and hydroperiodlengthdiff by adjusting parameter values this procedure was executed 200 times the final calibrated parameter values were the values for the model run that had the smallest sum of rmse and hydroperiodlengthdiff of the 200 monte carlo runs table 2 3 2 3 vegetation model development the vegetation model was developed as a statistical model that relates the probability of occurrence for a given plant community type to the inundation length hydroperiod and maximum depth associated with a 1 m2 frame within which vegetation was sampled see section 3 1 for details the model was developed by coupling data gathered from the vegetation survey with stage gage data as described in gosejohan et al 2017 the overall approach was similar to that of auble et al 1994 where riparian plant community types were positioned along a gradient of inundation duration using a direct gradient approach allowing projections of how changes in river management would alter plant community composition gosejohan et al 2017 used non metric multidimensional scaling nms ordination analysis to determine that inundation length and maximum depth were predictive hydrologic variables for vernal pool plant species hierarchical agglomerative cluster and indicator species analyses dufrêne and legendre 1997 were used to group the sites according to plant species composition into five broad plant community types defined by annual hydroperiod and maximum water depth short term inundated edge shallow tolerant deep tolerant and long term inundated gosejohan et al 2017 only the edge shallow tolerant and long term inundated community types were identified in coyote springs classification tree analysis cart was used to model the probability of occurrence for each plant community group at coyote springs according to habitat requirement thresholds of hydroperiod and maximum water depth at an annual resolution because hydrologic data were collected for a single year whereas some of the plant species present were perennial species that persist for multiple growing seasons it was assumed that the year of sampling was broadly representative with respect to the habitat requirements of dominant plant species the hydroperiod model for coyote springs predicted the edge community group to occur at hydroperiods of less than 101 5 days the shallow tolerant community group to occur at hydroperiods of 101 5 209 5 days and the long term inundated community group to occur at greater than 209 5 days of inundation in the maximum depth model for coyote springs edge community shallow tolerant and long term inundated community species were associated with maximum water depths of less than 15 5 cm 15 5 26 87 cm and more than 26 87 cm respectively although the hard thresholds suggested by the cart model are an artifact of the statistical modeling approach and do not reflect the variability in vegetation hydrology relationships found in nature they provide useful information for projecting directional shifts of plant community types in response to hydrologic change therefore areal coverage of each of the three plant community groups was projected according to the cart thresholds given simulated estimates of the two hydrologic variables for various climate change scenarios to couple the vegetation data with stage gage data for linkage with the hydrologic model output a total station survey of the pool was used to create a bathymetric surface from which water elevations from the stage gage could be projected to any location within the pool see section 3 1 maximum depth was defined as the maximum depth of water above a given elevation within the pool for example if the maximum stage gauge reading for a given year was 0 24 m the maximum depth for the portion of the pool associated with an elevation 0 00 m on the stage gauge would have a maximum depth of 0 24 m whereas the portion of the pool associated with the elevation at 0 22 m on the stage gauge would have a maximum depth of 0 02 m thus the maximum depth for each 2 cm elevation increment was calculated by taking the difference between the maximum stage gauge reading for a given year and the elevation at the location with the deepest part of the pool having a datum value of zero hydroperiod was calculated by summing the number of quarter months that the pool stage was above a given discretized 2 cm elevation the number of quarter months was then multiplied by 7 6 days to determine days of inundation the model then determined which vegetation type to assign to a given area associated with each 2 cm elevation increment within the pool according to the hierarchical classification provided by the cart analysis the area associated within each 2 cm increment within the pool was greater than 1400 m2 for all increments the model output was vegetation community type as percentage of maxpool vernal pool specialist species are among the indicator species found in shallow tolerant and long term inundated communities therefore for the purposes of our analysis we considered increases in long term inundated and shallow tolerant communities as indicating increases in vernal pool specialist species habitat 3 3 climate data and climate change scenarios prism precipitation and temperature data were used as the climate inputs for the hydrologic model for calibration and historical runs prism data were used for precipitation and air temperature inputs to the model for calibration of water year 2011 by comparing model output to stage gauge data datasets of precipitation and minimum and maximum temperature for climate change scenarios were obtained from the bias correctionand constructed analogs bcca version 2 which were downscaled from the world climate research programme s coupled model intercomparison project phase 3 cmip3 multi model dataset maurer et al 2007 available at http gdo dcp ucllnl org bcca data are available at daily time increments and 1 8th of a degree grid size at the time this study was developed only cmip3 data were available we use all the downscaled gcms available by the bcca products for a complete list of gcms refer to maurer et al 2007 the b1 emission scenario that assumes an aggressive emission reduction policy and a2 emission scenario that assumes unconstrained growth scenarios were used from these models as they were respectively the most and least conservative of the commonly modeled emission scenarios from ipcc 2007 because the gcm datasets are gridded to much larger cell sizes 12 km than the prism dataset 1 km the prism dataset was used for additional bias correction to bias correct temperature mean temperature was calculated as the average of maximum and minimum temperature for the prism dataset and each gcm dataset for the period 1980 2000 the difference between the prism and gcm mean temperature for 1980 2000 was added to each gcm dataset to bias correct precipitation mean annual precipitation was calculated for the prism dataset and each gcm dataset for the period 1980 2000 the difference between the prism and gcm mean annual precipitation was calculated as a percentage difference and was used to scale the gcm precipitation values the hydrologic and vegetation see section 3 2 models were run for two ten year time periods 1990 1999 historical and 2090 2099 future for 1990 1999 prism and each bias corrected gcm dataset was used for a total of nineteen historical model runs for 2090 2099 the datasets for the a2 and b1 scenarios for each bias corrected gcm were used for a total of thirty six future model runs differences were calculated between the 10 year means of the hydrologic model results for hydroperiod and maximum depth between historic 1990 1999 and future 2090 2099 time segments for each gcm and emission scenario results of vegetation models were compared using box plots for each gcm and averages of the gcms 4 results 4 1 hydrology the parameter values for the hydrologic model that produced the smallest amount of error for coyote springs yielded an rmse hydroperiodlengthdiff of 0 025 m 0 timesteps and an r2 value of 0 962 fig 4 indicating the model performed well in reproducing the hydrology of the vernal pool the rmse of 0 025 m was 8 9 of maximum depth model results from runs with the 18 gcm inputs indicate that the average percentage of precipitation that was rain increased from 81 5 in the historic period to 94 3 for the a2 scenario and 89 9 for the b1 scenario fig 5 average hydroperiods for the historical time period 1991 2000 a2 scenario and b1 scenario were 225 range 207 252 193 range 144 226 and 204 range 172 223 days respectively fig 6 a the historical period had longer hydroperiods than either of the two climate change scenarios fig 6a there was little change in maximum depth for decadal averages among all emissions scenarios fig 6b the historic mean was 26 cm range 23 28 cm the a2 mean was 27 cm range 22 37 cm and the b1 mean was 27 cm range 23 32 cm 4 2 vegetation because the predicted change in maximum depth did not differ significantly among emission scenarios fig 6b we used only hydroperiod and not maximum depth to simulate changes in the vegetation community when averaging gcm results by emission scenario the mean historical 1990 1999 edge community type occurred in 44 5 of the pool fig 7 and increased for future emission scenarios table 3 the model indicated a transition from the long term inundated community to the edge and shallow tolerant communities for both climate scenarios as compared to historical conditions 5 discussion 5 1 susceptibility of vernal pool hydrology and vegetation to climate change scenarios winter 2000 noted that because many wetlands rely on precipitation as their main source of water input they can be more susceptible to climate change vernal pools are seasonal wetlands and the pools studied also have little watershed inputs besides precipitation precipitation for the study area is projected to remain unchanged in the next century dettinger and cayan 1995 and the cmip3 gcm data for our study area showed very similar timing and amounts of precipitation in future emission scenarios when compared to the historical time period this resulted in little change in maximum depth values between the historical and future scenarios because similar amounts of water entered the pool primarily in the winter months when most precipitation occurs and et is minimal our simulations therefore indicated more response to temperature in the summer because it affects evaporation air temperature was higher for both future emission scenarios with a2 being more pronounced which resulted in faster drying of the pools shorter periods of inundation and greater percentages of rain versus snow precipitation figs 5 and 6 the 18 gcms in our study only had an average of 7 and 4 increase in precipitation for the a2 and b1 scenarios respectively as compared to historical conditions which may have contributed to the stronger responses to air temperature in our simulations we note that updated climate change scenarios from cmip5 taylor et al 2012 indicate that upper atmospheric large scale dynamics can lead to an increase in northern california winter precipitation in the late 21st century neelin et al 2013 future work is necessary to understand potential impacts and uncertainty of such large scale dynamic changes over northern california s complex terrain as such changes could affect precipitation climate projections pyke 2005 modeled climate change scenarios that increased precipitation by 10 for each 1 c increase in air temperature for central valley vernal pools and concluded that hydroperiod increased and consequently conditions for branchiopod reproduction improved with climate change precipitation in the pyke 2005 study was increased by as much as 30 whereas the gcms we used had average increased precipitation for our study area of 2 1 for a2 and 2 5 for b1 scenarios per degree c increase in air temperature which could explain the differences between that study s results and ours for hydroperiod we argue that our approach is more robust because it uses more recent climate scenarios and downscaled gcm data rather than assuming a constant increase in temperature and precipitation our approach suggests that climate change scenario conditions for vernal pool specialists may become more challenging than those shown by pyke 2005 our simulations suggest that both climate change projections considered would result in sharp reductions in hydroperiod which in turn would lead to declines of the long term inundated community whereas edge and shallow tolerant communities would increase the vernal pool specialist species of primary conservation concern are more commonly found in the long term inundated community where extended seasonal periods of inundation act as an ecological filter to exclude both wetland generalists and invasive plant species javornik and collinge 2016 gosejohan et al 2017 previous studies have also found the hydroperiod to be critically important for determining plant community structure in vernal pool ecosystems bauder 2000 deil 2005 emery et al 2009 gosejohan et al 2017 5 2 hydrologic modeling approach the objective of this study was to develop and apply a modeling approach linking climate hydrology and vegetation dynamics for vernal pools the simple water balance approach performed reasonably well for the coyote springs vernal pool in terms of modeling hydroperiod and water depth over time although pyke 2004 suggested that less than daily time steps would be better for modeling vernal pool hydrologic processes in his models of central valley and coastal vernal pools in california we had similar limitations in data availability to the pyke 2004 study and found the weekly time step to be appropriate for this vernal pool in northern california given the uncertainties of the data used to model the system we prioritized fitting the timing and duration of the hydroperiod in testing model fit because of the objective of linking hydrology and vegetation the coyote springs hydrologic model used surveyed topography to develop a relationship between pool volume and water depth that was more geometrically resolved than the representation used by boone et al 2006 and pyke 2004 thus the model represented the more complex geometry of coyote springs where deeper regions of the vernal pool had steeper slopes than the edge of the pool unlike boone et al 2006 who modeled vernal pools in minnesota we did not explicitly include surface water inflow from runoff to the pool because of the very small watershed surrounding the pool but we did include calculated snowmelt at each timestep 5 2 1 limitations of the hydrologic modeling approach the et method used for this study was simple and based on air temperature only while there is some evidence that temperature based et methods such as hamon 1963 are better than radiation based estimates such as the priestly taylor approach in humid environments lu et al 2005 it would be preferable to use penman monteith or a fully physical et model such as that used in donohue et al 2010 given the dry conditions at the site in addition the use of a constant relationship between aet and pet in the model does not represent some dynamics associated with soil drying that could affect et and vegetation however when considering the period of interest in this study is 2090 2099 there is large uncertainty associated with many of the penman monteith input parameters e g solar radiation that is dependent on cloud cover as well as other input parameters that are unavailable ground heat flux from climate models furthermore the large difference in size between climate model cells and the coyote springs site makes downscaling many of the parameters inappropriate mcvicar et al 2012 found broad decreases in wind across the globe in recent decades that resulted in penman monteith overestimating et given the scope of the study and current climate data available the use of a temperature based model was most appropriate some models have used a water stress factor that is a function of soil depth and water content to estimate aet from pet e g arnold et al 1998 which could be an appropriate modification of our approach to better represent the relationship between aet and pet however our approach did include separate calibration parameters for a soil factor and an open water factor eqs 1 and 2 and table 2 which may somewhat indirectly account for water stress the modeling approach used in this study could also be strengthened when climate parameters such as wind speed and vapor pressure become available with more certainty in the climate modeling data set so that radiation and physically based et methods could be applied we also acknowledge that there is uncertainty in the future projections of temperature and precipitation and hence our estimates of et because of that uncertainty we used all 18 gcms and two emissions scenarios i e a2 and b1 to examine future conditions regarding snow accumulation and melt the parameter for which snow begins melting a 2 was calibrated to be 10 8 c while this temperature is considerably lower than the freezing point of water snowmelt is a function not only of temperature but also of short and longwave radiation coyote springs is free from large vegetation and other objects that could provide shade from solar radiation which could lead to snowmelt occurring at temperatures below the freezing point the use of a constant partitioning between soil water and open water was a simplification that could be improved upon with more data as pool stage increases per unit of stage increase requires more water to both wet the soil and fill the pool because of the expanding pool perimeter and area while partitioning of water between the soil and the pool in reality is a head dependent process determination of the head dependence would require a duripan survey which was not possible for this study 5 3 data limitations model development calibration and validation were limited by data availability we were only able to collect data over one year at one vernal pool coyote springs so the model was only calibrated for that particular year and location ideally the model would have multiple years of data for calibration while all processes simulated within the model would benefit from more calibration data parameters that directly affect initial pool formation for a given year e g soildepth would in particular benefit from additional data initial pool formation is responsive to antecedent hydrologic factors therefore multiple years of data would be preferable for calibrating the model data were not available for an independent validation of the model assessment of model generalizability will require applying the model to additional vernal pools that vary in soils morphology climate and other variables this in turn would benefit from long term data collection for hydrologic variables and plant community structure across a diverse network of monitoring sites 5 4 suggestions for future work overall the coupling of hydrologic and vegetative models is a promising approach to quantify changes in vernal pool plant communities under a variety of climate scenarios previous studies have applied similar approaches to couple process based hydrologic models with empirical models of vegetation response to hydrology for riverine systems and wet meadow communities auble et al 1994 springer et al 1999 rains et al 2004 hammersmark et al 2010 in this study we used output of water depth with a statistically derived vegetation model to examine the potential impacts of climate change projections on vernal pool vegetation communities we do have several suggestions for future work that could improve this approach we used statistics with field data to establish the relationships between vernal pool vegetation and hydrology but more mechanistic information on this relationship would strengthen process modeling efforts while some studies have tried to isolate the effects of hydrology on the germination process of vernal pool plants in a controlled laboratory setting e g bliss and zedler 1998 more laboratory experiments would be beneficial for determining hydrologic thresholds while controlling for confounding variables e g isolating the effect of hydroperiod from that of seasonally varying water depths in addition many factors that are species specific besides hydrology can influence plant community structure a coupled model that considers other variables in addition to hydroperiod and maximum depth would likely improve modeled results possibilities for model variables include ph water temperature and soil texture we also encourage collecting more data and developing a more mechanistic modeling approach data collection from multiple pools over longer periods of time to allow more robust testing and calibration of hydrologic and vegetation models such data would also enable the development of a mechanistic hydrologic modeling approach for a more generalized process based understanding of vernal pool hydrology in addition better data would enable the use of more physically based et approaches 6 conclusion previous studies bauder 2000 gosejohan et al 2017 etc have indicated that hydrology is a central driving environmental factor for determining ecological community distribution within vernal pools in this study a modeling approach that combined hydrology and vegetation models was developed and used to investigate the potential implications of climate change on vernal pool hydrology and specialist species in northern california results indicate that climate change could lead to shorter hydroperiod which could be challenging for vernal pool specialist species additional studies of vernal pools with coupled hydrology and vegetation and faunal models would enable better understanding of implications of climate change for vernal pools in wetter or warmer climate regions such coupled models can help resource managers to assess the relative effectiveness of alternative hydrologic restoration scenarios for mitigating climate change effects on these unique ecosystems declaration of interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded in part by the nevada agricultural experiment station and the usda forest service the authors thank mirte iubelt michael rosen chris daly dan mcevoy louis provencher alexis montrone and robert blank for their assistance with the project thanks are extended to the desert research institute division of atmospheric sciences for partially supporting j f mejia we acknowledge the modeling groups the program for climate model diagnosis and intercomparison pcmdi and the wcrp s working group on coupled modelling wgcm for their roles in making available the wcrp cmip3 multi model dataset support of this dataset is provided by the office of science u s department of energy we also thank four anonymous reviewers and the journal of hydrology editorial team for their helpful review and comments 
6482,distributions of daily average water levels prior to ice jams and in jam free periods during spring breakups in the lena river were constructed the results showed that during periods of water rising in jam free years water levels were distributed in a random manner in accordance with an exponential poisson like function in contrast water level distributions during periods preceding ice jams followed a power law similar to various multiscale hydrological phenomena such as flood occurrences return periods of rainfall arctic drifting ice dynamics and other nonequilibrium natural processes however the complexity of ice jam floods is of a particular kind in many cases the power exponent in a water level distribution referred to as the b value in a gutenberg richter relation increased sharply a few days before an ice jam in other words a plot of the entire power law distribution usually consists of two portions differing in their slope in terms of thermodynamics an increase of the b value leads to a decrease of the system conservation to explain this effect one should take into account that ice jams are frequently preceded by advances of ice which changes the energy balance in the water ice system as a result the power exponent in water level distributions appears to be higher in the partially fragmented ice cover keywords spring backup ice jam water level distribution b value 1 introduction flooding is a complex phenomenon caused by a number of factors that can be associated with local regional and hemispheric climatic processes kundzewicz et al 2005 in cold regions river ice break up and related floods are the most important hydrological events of the year kääb et al 2013 compared to when water rises because of snowmelt or rainfall ice jam flooding continues to be a highly underexplored phenomenon due to the complexity of ice processes and multifactor interaction characteristics in the water ice system turcotte and morse 2013 floods of this kind produce particularly severe impacts on civil structures because they are accompanied with an ejection of ice on the shore agafonova et al 2017 more severe flooding causes damage that affects the local population takakura et al 2018 in many cases the local evacuation of the public is required and even a transfer of the most affected settlements is considered in extraordinary circumstances filippova 2011 accurately forecasting ice jam behavior is complicated by its abrupt onset and the lack of current controls on ice behavior despite substantial efforts no reliable flood forecast model is yet available mahabir et al 2002 as a rule the proposed models contain many parameters beven 1989 and often provide satisfactory reproductions of peak water levels for a particular site buzin 2009 beltaos 2003 buzin et al 2006 more standardized models of ice jam flooding lindenschmidt 2017 lindenschmidt et al 2012 still require any direct considerations in the problem of prediction in addition to be universal the adequate numerical models should cover quite different spatial and temporal scale levels including those that are much larger or much smaller than the observation scale more than two decades ago blöschl and sivapalan 1995 wrote accordingly to bridge this gap scaling is needed while gutknecht 1993 examined how the hydrological processes often show aspects of both self organization and randomness the behavior of large scale hydrological phenomena such as the occurrence of floods mazzarella and rapetti 2004 malamud and turcotte 2006 alipour et al 2016 and return periods of rainfall gupta and dawdy 1995 peñate et al 2013 furey and gupta 2007 exhibit self similarity at different scales and the distribution of their spatial temporal and energy parameters follows a power law a power law gutenberg richter type distribution signifies scale invariance self similarity over the range considered malamud and turcotte 2006 without providing an instrument for specifying any characteristic event s i e a dangerous flooding among other events over an energy time scale in a similar way the empirical gutenberg richter law describes self similar energy distributions of earthquakes but does not predict their realization in energy time coordinates however variations of the power law exponent in a given distribution if ever may contain some indirect information about the short term thermodynamic state of the system such as information about its conservation that in our case would characterize a degree of ice debris consolidation here this problem is considered to be relevant to the dynamics of ice jam flood formation the evolution of a partially consolidated system of water ice is regarded as a statistical multiplicity of interdependent motions we present time series analyses of rising water levels prior to ice jams and in jam free periods of spring ice breakups in the middle reach of the lena river the statistical approach to the behavior of overbank flooding is limited by the multifactor character of hydrological processes therefore extracting persistent consequences of events changing selected characteristics has rarely been successful aich et al 2016 however ice jam floods have a specificity determined by the critical impact of instability of ice cover during a breakup it is well established that ice in various formations and scales from laboratory samples weiss 2003 to the arctic ice pack chmel et al 2007 exhibits a scale invariant response with respect to deformation and fragmentation the distributions of daily average heights of water level h were considered from the viewpoint of their complying with either random poissonian type or correlated gutenberg richter type events during various types of spring backups that is events that proceeded with ice jam free flooding as well as those that occurred with ice jam formation 2 study site characterization data were collected from three hydrometric stations hs situated at a distance of approximately 2500 km from the river mouth covering periods of a few spring floods from 2008 to 2015 a schematic map of the lena river basin and the geographical positions of the hs whose databases were processed is depicted in fig 1 a and b fig 1b shows a satellite imagery of the study site the frequency of ice jams over this river reach 269 km in length is approximately 95 percent the identifying features of ice jam conditions are solid ice cover up to 200 cm thick high midstream speed 1 1 5 m s and a considerable celerity of flood wave up to 100 km day the river slope of the stretch under consideration is the highest and the stream velocity during the flood period is at a maximum during ice drift the increase of river width compared to that in low water seasons is usually as small as 10 percent therefore spring flooding occurs in conditions featuring a relatively slow speed of drifting ice takakura et al 2018 when large jams form the entire width of the river is observed to be tightly packed with ice in such cases the flow rate often decreases kil myaninov 2003 the efficiency of various factors on ice jam formation is determined to a large extent by the morphometric characteristics of the riverbed in an upper part of the reach under consideration hs 3028 the riverbed is moderately sinuous and braided downstream hs 3030 hs 3031 the flood plain is to the left at hs 3031 the river bed is low sloping a stony right bank is steep with a height up to 10 m a concave gently sloping left bank is sandy 3 ice jams locations table 1 shows the locations of a few floods with or without ice jams that occurred over the years at the river reach described above to determine the locations of the ice jams the water stage over the reach was calculated using the saint venant system of equations which describe a nonsteady water current banshchikova 2008 then the calculated h values versus time t and distance to the river mouth l were plotted fig 2 the isolines connecting points of equal water levels were drawn the constructed h versus t l plot is convenient intuitive and meaningful in respect to the variation of water levels for the given date along the reach at locations of ice jams the divergence of isolines can be seen upper fragments of the lines shift to the left while the inferior ones move in the opposite direction the water levels of free stream boundaries exhibit a well pronounced splash downstream fig 2 4 statistics of water level variation 4 1 spring breakup without ice jam fig 3 shows a distribution of the daily average water levels recorded from the beginning of the rush of water until the day of the maximum level near hs 3028 in the 2009 period of spring breakup which was not followed by an ice jam the distribution of the levels was calculated using the formula n h h versus h dependence where n is the number of days that the water level h was higher than the value of h which goes through the values of all water levels registered in the nearest hs horizontal coordinate and the number of days for which the water level h exceeded a current h value is shown on the vertical axis the same data were plotted in two coordinates the linear coordinates were utilized in an upper panel fig 3a while a lower panel fig 3b shows the data using semilogarithmic coordinates with a linear scale along a horizontal axis it can be seen that the distribution plotted in linear coordinates represents a descending singularity free curve in semilogarithmic coordinates the data points fall upon a straight line with a slope a 1 log 10 h h a h the relation 1 is equivalent to the exponential law of poissonian type 1a n h h e a h which is indicative of random events occurring independently from each other in general the exponential distributions appeared in all cases during which the water rise terminated without an ice jam in contrast in breakups with ice jam formation the water level increased in a random manner in only 3 cases out of 20 analyzed floods in the lena river not shown 4 2 spring breakup with ice jam fig 4 shows various kinds of n h h versus h distributions which were constructed for spring breakups that culminated both with and without ice jams it can be seen that only breakups without jams exhibit the exponential height distribution in semilogarithmic coordinates fig 4a left panel the distributions for breakups accompanied by ice jams are represented in these coordinates by complex descending curves fig 4b c just as in the preceding section 4 1 the distributions were plotted in coordinates of two different kinds however this time the semilogarithmic coordinates fig 4a c were supplemented with double log ones fig 4d f it can be seen that the water height distributions plotted in log log coordinates exhibit log linear dependences in the case of breakups with ice jams fig 4e f 2 log 10 h h b log 10 h where b is the slope of straight portions in addition analyses of spring breakups documented during diverse years in a few hs along the lena river have shown that the vast majority approximately 80 of height distributions caused by ice jams demonstrate a combination of two log linear portions with different slopes as shown in fig 4e as a rule the b value increased a few days before ice jamming working from logarithms in the relation 2 we obtain the same distribution in the form of a power law 2a n h h h b contrary to the quick descending exponential function the power law provides long range interactions between elements events which are interdependent both in space and time blöschl and sivapalan 1995 after the breakup of the ice jam the water ice system decomposes and the above described analysis of water level variations becomes inapplicable to free flowing water fig 5 shows the distribution of water levels before and after an ice jam presented both in log log coordinates a histogram of the level variation is depicted in fig 6 the n h h versus h plots for water rise prior to the ice jam follow the power law 4a with a change in the power exponent at a certain stage a plot for the slowly varying water level after the jam appears to have an almost vertical dependency 5 discussion the power law dependence 4a evidences the identity of the considered hydrological process at different scale levels self similarity because the function n h is a single resolution of the scaling equation 3 n λ h λ b n h where λ is the scaling factor the parameter b slope of straight lines in fig 5 characterizes the relative contributions of large and small events the lower the slope the higher the contribution of high water rises a term b value appeared first as a log linear constant in the formulation of the statistical distribution of magnitudes m of earthquakes given by gutenberg and richter 1954 4 log 10 n m m b m where n is the number of earthquakes whose magnitude m is higher than m the magnitude is proportional to the logarithm of released energy with a factor tending to 1 ekström and dziewonski 1988 a relation 2 found for the water height distributions prior to ice jams is statistically equivalent to eq 4 both are specific for dynamics of open nonequilibrium multi element multi event systems at the same time the scale invariant behavior during ice consolidation could be regarded as an opposite process to the tectonic disruption it must be emphasized that the energy distribution 4 as well as any other number vs intensity dependences of gutenberg richter type including water height distributions presented in fig 5 are inefficient for forecasting any transformational changes in the system water ice this is an indirect consequence of the principal postulate of the statistical conjecture that the scale invariant processes are considered as structureless in our case it would be impossible to assess further flood dynamics on the basis of the primary part of the n h h versus h dependence the gutenberg richter law is based on the essentially a posteriori analysis of the intensity of past events at the same time the ice jam floods exhibit a feature that distinguishes them from other scale invariant geophysical phenomena in many cases the power exponent in a water level distribution increased sharply a few days before an ice jam in other words a plot of the entire power law distribution usually consists of two portions differing in their slope fig 5 in terms of thermodynamics an increase of the b value leads to a decrease of the system conservation to explain this effect one should take into consideration that ice jams are frequently preceded by advances of ice which changes the energy balance in the water ice system as a result the power exponent in the water level distributions appears to be higher in the partially fragmented ice cover the power law scenario was realized only in flood induced ice jams in relatively rare cases the log linear dependence was characterized by a single slope b fig 4e this pattern is specific for many natural processes such as the gutenberg richter law for the dependence of the number of earthquakes on their magnitude bak et al 2002 or the number of drifting ice accelerations on their intensity chmel et al 2007 however in most cases of ice jams the b value exhibits a sudden increase several days before the jam figs 4c and 5a the power law number vs intensity dependences are connected to the actual physical conditions and processes furey and gupta 1995 for example gupta and dawdy 1995 reported that snowmelt generated floods exhibit simple scaling whereas rainfall generated floods exhibit multiscaling power exponent variation the crucial condition of the power law dependence for the number of events on their intensities is the nonequilibrium state of the system in an equilibrium system that does not have any energy intake possible fluctuations of the state decay quickly in accordance with the exponential law in the latter case the system s dynamics proceed in a random manner in this study distributions of this kind of water level were obtained for jam free floods to explain changes in the b value in our case one should take into account that to realize scale invariance the system must be not only nonequilibrium but conservation must also be sufficient to be able to retain the incoming energy in one version of the spring block models used for the simulation of seismic activity christensen and olami 1992 a possibility was included to change the energy conservation in the system by tuning a certain parameter α which served as a measure of conservation their main finding had a clear physical correlation the lower the level of conservation the higher the b value there are two ultimate cases in a fully non conservative system α 0 the scaling is impossible due to the lack of the interactions memory between components events this case is readily visible in fig 5 where the b value increased by two orders of magnitude in the water height distribution calculated for the drawdown period when the free floating ice affected the level variation negligibly on the other hand in a fully conservative system α the b value should be a universal constant to be the same at any energy income which is not the case in real natural systems where a dissipative constituent to the energy redistribution always takes place in the case of ice jam formation the energy of the water ice system grows due to elastic deformation of the ice cover at the first stage of jam formation however the long term data shows that prior to almost every jam initial ice movements and openings appear the ice cover partially loses its connectedness and the conservation of the water ice system reduces as a result the fragmentation of the ice cover in front of a jam leads to an increase of the b value which manifests itself in a break of the log linear dependence 6 conclusion ice jam floods follow a trend to scale invariant behavior correspondingly the daily distributions of water levels follow the power law like many natural large scale processes distinct from other catastrophic phenomena earthquakes rainfall floods jam related floods exhibit a particular feature in the majority of cases the power law exponent increases steeply a few days before the jam this effect is caused by the variation of energy balance in the water ice system as almost each ice jam is preceded by ice advances the fragmentation of ice reduces the ice cover connectedness the relevant increase of the power exponent in the water level distribution is consistent with the expected response of a nonequilibrium statistical system on the reduction of its conservation declaration of competing interest none declared 
6482,distributions of daily average water levels prior to ice jams and in jam free periods during spring breakups in the lena river were constructed the results showed that during periods of water rising in jam free years water levels were distributed in a random manner in accordance with an exponential poisson like function in contrast water level distributions during periods preceding ice jams followed a power law similar to various multiscale hydrological phenomena such as flood occurrences return periods of rainfall arctic drifting ice dynamics and other nonequilibrium natural processes however the complexity of ice jam floods is of a particular kind in many cases the power exponent in a water level distribution referred to as the b value in a gutenberg richter relation increased sharply a few days before an ice jam in other words a plot of the entire power law distribution usually consists of two portions differing in their slope in terms of thermodynamics an increase of the b value leads to a decrease of the system conservation to explain this effect one should take into account that ice jams are frequently preceded by advances of ice which changes the energy balance in the water ice system as a result the power exponent in water level distributions appears to be higher in the partially fragmented ice cover keywords spring backup ice jam water level distribution b value 1 introduction flooding is a complex phenomenon caused by a number of factors that can be associated with local regional and hemispheric climatic processes kundzewicz et al 2005 in cold regions river ice break up and related floods are the most important hydrological events of the year kääb et al 2013 compared to when water rises because of snowmelt or rainfall ice jam flooding continues to be a highly underexplored phenomenon due to the complexity of ice processes and multifactor interaction characteristics in the water ice system turcotte and morse 2013 floods of this kind produce particularly severe impacts on civil structures because they are accompanied with an ejection of ice on the shore agafonova et al 2017 more severe flooding causes damage that affects the local population takakura et al 2018 in many cases the local evacuation of the public is required and even a transfer of the most affected settlements is considered in extraordinary circumstances filippova 2011 accurately forecasting ice jam behavior is complicated by its abrupt onset and the lack of current controls on ice behavior despite substantial efforts no reliable flood forecast model is yet available mahabir et al 2002 as a rule the proposed models contain many parameters beven 1989 and often provide satisfactory reproductions of peak water levels for a particular site buzin 2009 beltaos 2003 buzin et al 2006 more standardized models of ice jam flooding lindenschmidt 2017 lindenschmidt et al 2012 still require any direct considerations in the problem of prediction in addition to be universal the adequate numerical models should cover quite different spatial and temporal scale levels including those that are much larger or much smaller than the observation scale more than two decades ago blöschl and sivapalan 1995 wrote accordingly to bridge this gap scaling is needed while gutknecht 1993 examined how the hydrological processes often show aspects of both self organization and randomness the behavior of large scale hydrological phenomena such as the occurrence of floods mazzarella and rapetti 2004 malamud and turcotte 2006 alipour et al 2016 and return periods of rainfall gupta and dawdy 1995 peñate et al 2013 furey and gupta 2007 exhibit self similarity at different scales and the distribution of their spatial temporal and energy parameters follows a power law a power law gutenberg richter type distribution signifies scale invariance self similarity over the range considered malamud and turcotte 2006 without providing an instrument for specifying any characteristic event s i e a dangerous flooding among other events over an energy time scale in a similar way the empirical gutenberg richter law describes self similar energy distributions of earthquakes but does not predict their realization in energy time coordinates however variations of the power law exponent in a given distribution if ever may contain some indirect information about the short term thermodynamic state of the system such as information about its conservation that in our case would characterize a degree of ice debris consolidation here this problem is considered to be relevant to the dynamics of ice jam flood formation the evolution of a partially consolidated system of water ice is regarded as a statistical multiplicity of interdependent motions we present time series analyses of rising water levels prior to ice jams and in jam free periods of spring ice breakups in the middle reach of the lena river the statistical approach to the behavior of overbank flooding is limited by the multifactor character of hydrological processes therefore extracting persistent consequences of events changing selected characteristics has rarely been successful aich et al 2016 however ice jam floods have a specificity determined by the critical impact of instability of ice cover during a breakup it is well established that ice in various formations and scales from laboratory samples weiss 2003 to the arctic ice pack chmel et al 2007 exhibits a scale invariant response with respect to deformation and fragmentation the distributions of daily average heights of water level h were considered from the viewpoint of their complying with either random poissonian type or correlated gutenberg richter type events during various types of spring backups that is events that proceeded with ice jam free flooding as well as those that occurred with ice jam formation 2 study site characterization data were collected from three hydrometric stations hs situated at a distance of approximately 2500 km from the river mouth covering periods of a few spring floods from 2008 to 2015 a schematic map of the lena river basin and the geographical positions of the hs whose databases were processed is depicted in fig 1 a and b fig 1b shows a satellite imagery of the study site the frequency of ice jams over this river reach 269 km in length is approximately 95 percent the identifying features of ice jam conditions are solid ice cover up to 200 cm thick high midstream speed 1 1 5 m s and a considerable celerity of flood wave up to 100 km day the river slope of the stretch under consideration is the highest and the stream velocity during the flood period is at a maximum during ice drift the increase of river width compared to that in low water seasons is usually as small as 10 percent therefore spring flooding occurs in conditions featuring a relatively slow speed of drifting ice takakura et al 2018 when large jams form the entire width of the river is observed to be tightly packed with ice in such cases the flow rate often decreases kil myaninov 2003 the efficiency of various factors on ice jam formation is determined to a large extent by the morphometric characteristics of the riverbed in an upper part of the reach under consideration hs 3028 the riverbed is moderately sinuous and braided downstream hs 3030 hs 3031 the flood plain is to the left at hs 3031 the river bed is low sloping a stony right bank is steep with a height up to 10 m a concave gently sloping left bank is sandy 3 ice jams locations table 1 shows the locations of a few floods with or without ice jams that occurred over the years at the river reach described above to determine the locations of the ice jams the water stage over the reach was calculated using the saint venant system of equations which describe a nonsteady water current banshchikova 2008 then the calculated h values versus time t and distance to the river mouth l were plotted fig 2 the isolines connecting points of equal water levels were drawn the constructed h versus t l plot is convenient intuitive and meaningful in respect to the variation of water levels for the given date along the reach at locations of ice jams the divergence of isolines can be seen upper fragments of the lines shift to the left while the inferior ones move in the opposite direction the water levels of free stream boundaries exhibit a well pronounced splash downstream fig 2 4 statistics of water level variation 4 1 spring breakup without ice jam fig 3 shows a distribution of the daily average water levels recorded from the beginning of the rush of water until the day of the maximum level near hs 3028 in the 2009 period of spring breakup which was not followed by an ice jam the distribution of the levels was calculated using the formula n h h versus h dependence where n is the number of days that the water level h was higher than the value of h which goes through the values of all water levels registered in the nearest hs horizontal coordinate and the number of days for which the water level h exceeded a current h value is shown on the vertical axis the same data were plotted in two coordinates the linear coordinates were utilized in an upper panel fig 3a while a lower panel fig 3b shows the data using semilogarithmic coordinates with a linear scale along a horizontal axis it can be seen that the distribution plotted in linear coordinates represents a descending singularity free curve in semilogarithmic coordinates the data points fall upon a straight line with a slope a 1 log 10 h h a h the relation 1 is equivalent to the exponential law of poissonian type 1a n h h e a h which is indicative of random events occurring independently from each other in general the exponential distributions appeared in all cases during which the water rise terminated without an ice jam in contrast in breakups with ice jam formation the water level increased in a random manner in only 3 cases out of 20 analyzed floods in the lena river not shown 4 2 spring breakup with ice jam fig 4 shows various kinds of n h h versus h distributions which were constructed for spring breakups that culminated both with and without ice jams it can be seen that only breakups without jams exhibit the exponential height distribution in semilogarithmic coordinates fig 4a left panel the distributions for breakups accompanied by ice jams are represented in these coordinates by complex descending curves fig 4b c just as in the preceding section 4 1 the distributions were plotted in coordinates of two different kinds however this time the semilogarithmic coordinates fig 4a c were supplemented with double log ones fig 4d f it can be seen that the water height distributions plotted in log log coordinates exhibit log linear dependences in the case of breakups with ice jams fig 4e f 2 log 10 h h b log 10 h where b is the slope of straight portions in addition analyses of spring breakups documented during diverse years in a few hs along the lena river have shown that the vast majority approximately 80 of height distributions caused by ice jams demonstrate a combination of two log linear portions with different slopes as shown in fig 4e as a rule the b value increased a few days before ice jamming working from logarithms in the relation 2 we obtain the same distribution in the form of a power law 2a n h h h b contrary to the quick descending exponential function the power law provides long range interactions between elements events which are interdependent both in space and time blöschl and sivapalan 1995 after the breakup of the ice jam the water ice system decomposes and the above described analysis of water level variations becomes inapplicable to free flowing water fig 5 shows the distribution of water levels before and after an ice jam presented both in log log coordinates a histogram of the level variation is depicted in fig 6 the n h h versus h plots for water rise prior to the ice jam follow the power law 4a with a change in the power exponent at a certain stage a plot for the slowly varying water level after the jam appears to have an almost vertical dependency 5 discussion the power law dependence 4a evidences the identity of the considered hydrological process at different scale levels self similarity because the function n h is a single resolution of the scaling equation 3 n λ h λ b n h where λ is the scaling factor the parameter b slope of straight lines in fig 5 characterizes the relative contributions of large and small events the lower the slope the higher the contribution of high water rises a term b value appeared first as a log linear constant in the formulation of the statistical distribution of magnitudes m of earthquakes given by gutenberg and richter 1954 4 log 10 n m m b m where n is the number of earthquakes whose magnitude m is higher than m the magnitude is proportional to the logarithm of released energy with a factor tending to 1 ekström and dziewonski 1988 a relation 2 found for the water height distributions prior to ice jams is statistically equivalent to eq 4 both are specific for dynamics of open nonequilibrium multi element multi event systems at the same time the scale invariant behavior during ice consolidation could be regarded as an opposite process to the tectonic disruption it must be emphasized that the energy distribution 4 as well as any other number vs intensity dependences of gutenberg richter type including water height distributions presented in fig 5 are inefficient for forecasting any transformational changes in the system water ice this is an indirect consequence of the principal postulate of the statistical conjecture that the scale invariant processes are considered as structureless in our case it would be impossible to assess further flood dynamics on the basis of the primary part of the n h h versus h dependence the gutenberg richter law is based on the essentially a posteriori analysis of the intensity of past events at the same time the ice jam floods exhibit a feature that distinguishes them from other scale invariant geophysical phenomena in many cases the power exponent in a water level distribution increased sharply a few days before an ice jam in other words a plot of the entire power law distribution usually consists of two portions differing in their slope fig 5 in terms of thermodynamics an increase of the b value leads to a decrease of the system conservation to explain this effect one should take into consideration that ice jams are frequently preceded by advances of ice which changes the energy balance in the water ice system as a result the power exponent in the water level distributions appears to be higher in the partially fragmented ice cover the power law scenario was realized only in flood induced ice jams in relatively rare cases the log linear dependence was characterized by a single slope b fig 4e this pattern is specific for many natural processes such as the gutenberg richter law for the dependence of the number of earthquakes on their magnitude bak et al 2002 or the number of drifting ice accelerations on their intensity chmel et al 2007 however in most cases of ice jams the b value exhibits a sudden increase several days before the jam figs 4c and 5a the power law number vs intensity dependences are connected to the actual physical conditions and processes furey and gupta 1995 for example gupta and dawdy 1995 reported that snowmelt generated floods exhibit simple scaling whereas rainfall generated floods exhibit multiscaling power exponent variation the crucial condition of the power law dependence for the number of events on their intensities is the nonequilibrium state of the system in an equilibrium system that does not have any energy intake possible fluctuations of the state decay quickly in accordance with the exponential law in the latter case the system s dynamics proceed in a random manner in this study distributions of this kind of water level were obtained for jam free floods to explain changes in the b value in our case one should take into account that to realize scale invariance the system must be not only nonequilibrium but conservation must also be sufficient to be able to retain the incoming energy in one version of the spring block models used for the simulation of seismic activity christensen and olami 1992 a possibility was included to change the energy conservation in the system by tuning a certain parameter α which served as a measure of conservation their main finding had a clear physical correlation the lower the level of conservation the higher the b value there are two ultimate cases in a fully non conservative system α 0 the scaling is impossible due to the lack of the interactions memory between components events this case is readily visible in fig 5 where the b value increased by two orders of magnitude in the water height distribution calculated for the drawdown period when the free floating ice affected the level variation negligibly on the other hand in a fully conservative system α the b value should be a universal constant to be the same at any energy income which is not the case in real natural systems where a dissipative constituent to the energy redistribution always takes place in the case of ice jam formation the energy of the water ice system grows due to elastic deformation of the ice cover at the first stage of jam formation however the long term data shows that prior to almost every jam initial ice movements and openings appear the ice cover partially loses its connectedness and the conservation of the water ice system reduces as a result the fragmentation of the ice cover in front of a jam leads to an increase of the b value which manifests itself in a break of the log linear dependence 6 conclusion ice jam floods follow a trend to scale invariant behavior correspondingly the daily distributions of water levels follow the power law like many natural large scale processes distinct from other catastrophic phenomena earthquakes rainfall floods jam related floods exhibit a particular feature in the majority of cases the power law exponent increases steeply a few days before the jam this effect is caused by the variation of energy balance in the water ice system as almost each ice jam is preceded by ice advances the fragmentation of ice reduces the ice cover connectedness the relevant increase of the power exponent in the water level distribution is consistent with the expected response of a nonequilibrium statistical system on the reduction of its conservation declaration of competing interest none declared 
6483,water quality data in the sewer system are indispensable for modelling but rarely available as measurements in sewers are challenging to conduct optimal experimental design oed is a powerful tool to identify and maximize the information content of measurement data this paper adopts a model based oed methodology to efficiently plan a measurement campaign for final model calibration and validation of a new sewer water quality model to do so a preliminary calibrated model of the case study is used to evaluate the information content of different potential measurement locations and scenarios for suspended solids as measured variable the case study first demonstrates how oed can identify the best measurement location within a complex sewer network it secondly demonstrates that measuring the beginning of a big rain event results in the most information rich data among all scenarios evaluated thirdly it analyses in detail the information content of dry weather flow dwf data in comparison to previous studies the methodology is improved by considering the actual measurement error characteristics when calculating the information content of measurement data keywords efficient data collection model based planning sewer sediments measurement campaign 1 introduction tackling the question of total suspended solids tss in sewer systems goes beyond simply understanding the fate of particulates throughout the sewer system itself tss is known as a carrier of nutrients but also as a carrier of heavy metals pesticides and pathogens among others moreover it is the cause of organic and inorganic pollution tss can thus be considered an indicator substance vanrolleghem et al 2018 developing models for the prediction of the tss flux for the control of overflow structures or the optimal management of the wastewater resource recovery facility wrrf has therefore far reaching benefits water quality modelling for particulates remains challenging in the sewer a main reason are the complex processes involved when it comes to tss as they greatly transcend mere advective transport particles can settle and resuspend depending on the condition they can flocculate aggregate or break and once settled they can be consolidated on the bottom of the sewer pipes in addition different physical chemical and biological processes take place both in the water phase and the sewer sediments ashley et al 2004 modelling developments are on going focusing on one or several processes to improve the understanding of those specific processes e g for gross solids penn et al 2018 or for bed load mohtar et al 2018 but even if models are currently not able to incorporate all the processes involved they help to understand the behaviour of tss in sewer systems independent of the tss process modelled data are indispensable for model calibration and validation in particular calibrating dynamic models benefits from high resolution data as provided by online sensors unfortunately these data are rarely available as measuring in the sewer system is not very widespread and ambitious to conduct furthermore they are distributed systems that may require measurements at multiple locations vanrolleghem et al 1999 measurements in sewers not only require a considerable investment in the equipment and the set up of the measurement site but also require a tight and intensive maintenance schedule to ensure measurements of reliable quality the sensors need to be manually cleaned calibrated and validated which includes labour intensive laboratory experiments ledergerber et al 2017 moreover not all data have the same information content for model calibration and validation in order to calibrate a parameter the parameter has to be influential during the time period when the data are collected dochain and vanrolleghem 2001 if a data set is available for calibration and validation of a complex model where overparameterization might be an issue identifiability analysis is a method to assess the parameters that can be estimated from the given measurement data set freni et al 2011 if however a new data set is to be collected model based optimal experimental design oed can evaluate prior to the measurement campaign which potential experiment of a set of proposed experiments contains the most information for model calibration vanrolleghem et al 1995 oed has mostly been applied in laboratory scale experiments and therefore in a controlled environment see for example vanrolleghem and coen 1995 oed utilizes a preliminary model that has been calibrated on an initial set of data first different experiments are proposed and simulated with the preliminary model then simulation results are evaluated in terms of their information content de pauw and vanrolleghem 2006a the information content of an experiment can be calculated from the fisher information matrix fim indicated in eq 1 1 fim k 1 n y i θ j t k t q t k y i θ j t k the fim links the information content of an experiment to the sensitivity of the model output corresponding to the measurements y i with respect to the parameters studied θ j for the timesteps t k during the period of the experiment time step 1 to n and a square matrix q t k with user supplied weighting coefficients de pauw and vanrolleghem 2006a q t k is typically chosen as the inverse of the measurement error covariance or as the identity matrix vanrolleghem et al 1995 an important characteristic of the fim is the fact that the inverse of the fim corresponds to the parameter estimation error covariance matrix v dochain and vanrolleghem 2001 thus allowing the direct assessment of the confidence region of the calibrated parameters different criteria exist to evaluate the information content of the proposed experiments see for example mehra 1974 this paper transcends the laboratory scale use of oed by applying it to design a measurement campaign in a full scale sewer environment it studies how the potential information content of a new measurement data set can be optimized it starts from the point that a first measurement campaign has already been conducted without oed this allows calibrating and validating a preliminary model and approximating the measurement error a preliminary model is required for planning the second measurement campaign with oed in the most efficient way making optimal use of the investments such as measurement equipment and working hours for maintenance in addition the results of the first measurement campaign permit approximating the measurement error for the weighting coefficients of the square matrix q t k rather than working with the identity matrix assumption as previous studies vanrolleghem et al 1995 freni and mannina 2012 this paper illustrates how oed can be adopted considering an approximation of the measurement error to select the best location for measurements and to the best timing of the measurement scenarios in the end a more detailed analysis of the experiments performed under dry weather flow dwf conditions highlight the importance of considering the measurement error 2 material and methods 2 1 case study 2 1 1 description the studied sewer system of the wrrf clos de hilde cdh is located in the southern parts of bordeaux france the wrrf has a treatment capacity of approximately 400 000 pe and the catchment covers about 8 000 ha the catchment is shown in fig 1 and is located on both sides of the river garonne which is the main receiving water the sewer system consists of both combined and separate sewers for more details see ledergerber et al 2017 local regulations for the wrrf include tss cod and bod5 concentrations thresholds at the effluent jorf 2015 and importantly this recent decree jorf 2015 sets for the first time combined sewer overflow cso water quality standards that cities will have to apply in the near future 2 1 2 available data and models data regarding water quantity flow are available throughout the sewer system and provided by the local utility several continuous flow measurements exist mainly at pumping stations in addition a calibrated hydraulic model implemented in mike urban dhi denmark is available for the studied catchment water quality data is generally collected by the utility only at the wrrf in the framework of this project a first measurement campaign to obtain an initial data set for water quality in the sewer system was conducted in 2017 2 1 3 measurement equipment and installation two automatic measurement stations ams rsm30 primodal systems hamilton on canada are available on site rieger and vanrolleghem 2008 both are equipped with two tss sensors based on different measurement principles one sensor is a turbidity meter visoturb 700 iq wtw weilheim germany which measures the intensity of light scattered at 90 degrees as a beam of light passes through a water sample the second sensor is a spectrometer spectro lyser s can messtechnik vienna austria measuring absorption spectra in the uv vis range the sensors are calibrated and validated with laboratory tss samples to collect the data for the preliminary model during the first measurement campaign one ams was installed at the inlet of the wrrf cdh while the other ams was installed at the major pumping chamber noutary nt the location of the sensors is indicated in fig 1 for more details on the installation at the measurement sites see ledergerber et al 2017 the data quality of the measurements was assured by applying a univariate data quality assessment method alferes et al 2013 resulting in validated high frequency data minimum one value per three minutes 2 2 preliminary model 2 2 1 modelling approach and software the preliminary model with which the oed is applied includes the catchments and the sewer network of the wrrf cdh the catchment model used is the kosim west model with the extensions proposed by pieper 2017 it includes sub models for flow and pollution generation during wet weather flow wwf on the one hand and dwf on the other hand the two main extensions regard firstly the possibility of routing dwf with linear reservoirs in series and secondly the splitting of the wwf in two parallel sets of linear reservoirs which allows the user to capture the varying dynamics especially in the context of larger aggregated models pieper 2017 the sewer model uses the so called particle settling velocity distribution psvd linear reservoirs in series model maruéjouls et al 2015 from a hydraulic perspective it is a comparably simple modelling approach but allows considering advective transport settling and resuspension of ten different particle classes keeping the computational time very competitive the distribution of settling characteristics of a tss sample can be measured with the vicas experiment resulting in the psvd curve of a sample chebbo and gromaire 2009 based on these measurements the total tss is fractionated into the ten classes of the sewer model the particles of a certain class settle with the settling velocity characteristic of their class the resuspension rate r resusp d 1 is calculated using sigmoidal eq 2 where q in m3 d is the inflow of the linear reservoir r resusp max d 1 corresponds to the maximum resuspension rate reached for big inflows q half m3 d represents the inflow at which half of the resuspension rate is reached and n is the exponent defining steepness of the sigmoidal curve the parameters r resusp max q half and n have to be calibrated for each sewer stretch the model is implemented in the software west by dhi horsholm dk 2 r resusp t r resusp max q in n t q in n t q half n 2 2 2 calibration and validation of preliminary model the preliminary model for oed was calibrated for water quantity flow and water quality tss the water quantity model for wwf was calibrated and validated rmse 0 058 m3 s on the existing calibrated mike urban by dhi model only the dwf parameters had to be recalibrated on actual flow measurements as the mike urban model focused on wwf the validation period on actual flow measurements during a 4 day period including wwf is indicated in fig 2 left hand side and resulted in a rmse of 0 064 m3 s the preliminary water quality calibration and validation was conducted using validated tss data obtained during the first measurement campaign the time series were chosen based on an analysis of quality of the available data a coherent measurement set was selected in which high quality data from both ams were available at the same time the model was calibrated on a 10 day period including two rain events and was validated for the same 4 day period as the data set used for water quantity model validation also indicated in fig 2 validation resulted in a rmse of 58 mg tss l 2 3 oed methodology 2 3 1 overview of methodology as mentioned in the previous section 1 the core of oed is the calculation of the fim and the evaluation of the experiments for a specific criterion however oed has to be viewed in a broader context fig 3 shows the oed methodology applied for this case study it was inspired by many previous studies e g de pauw and vanrolleghem 2006a vanrolleghem and coen 1995 vanrolleghem et al 1995 the methodology is divided into two phases the preparation phase left and the actual experimental design phase right 2 3 2 preparation phase the intended outcome of the preparation phase is a preliminary model with which a future planned experiments can be evaluated b the measurement error can be characterized and c initial values can be assigned to the set of parameters for which the oed and ultimately the measurement campaign is conducted the preparation phase consists of three steps the first of which is inherent to all modelling tasks dochain and vanrolleghem 2001 identifying the purpose of the model and with it the modelling objectives as well as the choice of corresponding model structure the second step is to evaluate the data requirements for model calibration and validation the data for the preliminary model have to be collected which can include either obtaining available data or conducting a first measurement campaign if not sufficient data are available an important aspect of this step is to analyse the measurement data and characterize the measurement error the third step of the preparatory phase is to build calibrate and validate the preliminary model and identify the parameter set for oed manifold references to calibration and validation criteria exist see for example hauduc et al 2015 2 3 3 experimental design phase the output of the second phase the actual experimental design phase is the optimally designed measurement campaign in view of recalibration of the identified parameters this phase consists of three different steps the first step is to propose experiments that could potentially be conducted in the context of a sewer system this step is better described as the identification of the measurement scenarios as unlike in laboratory conditions the measurement conditions as such cannot be influenced but the timing and location of the measurements can be chosen the proposed scenarios have to be realistic with respect to the available measurement equipment the duration of the campaign and the work required for implementation of the experiment the second step is then to evaluate the proposed scenarios in terms of expected information content taking the proper measurement error into account given the results of the oed evaluation criteria for the different scenarios proposed the accordingly optimized measurement campaign can be implemented 2 4 additional information calculation 2 4 1 considered parameters for oed the overall aim of the oed for this model is to improve tss predictions at the wrrf and during wwf also at csos as water quantity parameters are constrained by the water balance and were felt to be already well calibrated on the basis of the mike urban model only the parameters for water quality were considered for oed the 29 parameters affecting water quality are present both in the catchment and the sewer sub model the parameters to be recalibrated are the following parameters i in the catchment sub model the mean concentrations under dwf wwf and infiltration flow and ii in the sewer sub model the re suspension parameters in each of the series of linear reservoirs r resusp max q half and n as the particle setting velocity distributions were obtained from good quality laboratory vicas experiments they were not considered as parameters to be included in the design of the second measurement campaign also in contrast to other oed studies e g vanrolleghem et al 1995 the initial conditions of all state variables of the model were not part of the oed indeed they were not relevant for the simulation results as several days of warm up were used in the simulations before the actual scenarios were simulated 2 4 2 calculation of parameter sensitivity to complete the fisher information matrix elements the local sensitivity was calculated at each anticipated measurement point for the second measurement campaign as the central difference according to eq 3 de pauw and vanrolleghem 2006b the perturbation factor δ θ j to calculate the sensitivity was 1 of the parameter value for each of the parameters θ j 3 y i θ j t y i t θ j δ θ j y i t θ j δ θ j 2 δ θ j 2 4 3 definition of measurement error of tss as indicated in section 2 1 3 the data obtained in the first measurement campaign of 2017 was filtered using the method developed by alferes et al 2013 the comparison of the raw and the filtered data allows characterizing the measurement error ε meas tss which is defined as the absolute difference of the filtered data filtered and the raw data data raw as in eq 4 4 ε meas tss t data filtered t data raw t 2 4 4 evaluation criterion for oed for this study the d optimal design criterion in eq 5 is chosen among the available scalars that can be calculated from the fim mehra 1974 as the fim is the inverse of the parameter estimation error covariance matrix maximising the determinant of the fim results in minimizing the volume of the covariance matrix thus minimizing the parameter estimation error dochain and vanrolleghem 2001 5 max det fim 3 results first some general information about the results is presented in section 3 1 in the following section 3 2 the optimal location of the ams is evaluated and then in section 3 3 different scenarios for the chosen optimal location are studied in the final section 3 4 dwf measurements are studied in more detail 3 1 general results 3 1 1 measurement error estimation of tss based on the assumption that the relative measurement error α does not depend on the tss concentration conc tss the absolute measurement error ε meas tss as defined in section 2 4 3 depends linearly on conc tss 6 ε meas tss α conc tss this relationship can be observed in fig 4 for the tss measurements at the pumping station nt the coefficient α was obtained by fitting the above equation using least squares regression for both measurement locations 0 09 for the inlet of the wrrf cdh and 0 18 for the pumping station nt this difference between locations confirms the experience of the far more challenging measurement conditions in the sewer than at the wrrf 3 1 2 degrees of freedom and constraints for oed in order to be able to identify the best possible experiments hereafter named scenarios for the reasons mentioned in section 2 3 3 the degrees of freedom of the measurement campaign had to be identified they include the location of the ams and thus the location of the measurement set up as well as the timing of the measurement differentiating between wwf and dwf and identifying different timings for both situations knowledge of the typical performance of the used measurement equipment and anticipation of different measurement conditions allow proposing measurement scenarios the experience of the first measurement campaign showed that maintenance of the sensors is of fundamental importance and that sensors give only reliable data for a relatively short period of time after a maintenance event ledergerber et al 2017 for the given case study it is assumed that in the worst case the reliable period lasts for only about 12 h after a maintenance event so scenarios of 12 h were planned with a measurement interval of three minutes corresponding to the storage interval for the online sensors different measurement scenarios were then created for the typical dwf pattern for dwf the day is split into day and night conditions splitting at 09 30 respectively 21 30 assuming that a workday starts at 09 00 at the site and sensors would have received their routine maintenance and be ready for use by 09 30 measurement scenarios were also defined for wwf conditions considering that in contrast to dwf conditions each rain event is different to have representative rain scenarios the proposed experiments were simulated using actual rain data of the previous measurement campaign conducted in 2017 one big summer storm cumulative 24 h rainfall 24 9 mm was chosen in which multiple overflows were taking place in addition to a smaller rain event cumulative 24 h rainfall 2 4 mm similar rains were observed several times over the summer of 2017 in total seven different scenarios were proposed summarized in table 1 3 2 oed for evaluation of measurement location for the planned measurement campaign the oed methodology was used to re evaluate the original location of the ams at nt and cdh with respect to the information content of the measurement data as mentioned initially two ams were available for the measurement campaigns during the first measurement campaign in 2017 which was designed based on expertise and practical experience one ams was installed at the inlet of the wrrf cdh and the second was placed at the pumping station nt in the sewer system the location of the ams in the sewer system was re evaluated using seven other locations as possible locations see fig 5 the evaluated locations correspond all to pumping stations or retention tanks upstream of the wrrf in the sewer system the first ams was always maintained at the inlet of the wrrf since this is the final outlet of the sewer system as preliminary measurements were only available at the pumping station nt and the inlet of the wrrf cdh the measurement error could only be estimated for those two locations for the oed calculations it was assumed that the measurement error found at nt was representative of the other seven locations in the sewer system for the evaluation of the location of the ams timing scenario 6 beginning of big rain event was chosen as this was expected to be the best scenario independent of the location chosen given the large tss dynamics occurring this will be confirmed in section 3 3 fig 6 illustrates how the value of the d optimal criterion changes according to the location of the ams for timing scenario 6 the location of the ams at the pumping station nt chosen for the first measurement campaign ranks third among the eight identified options the two places that rank better are located further upstream and contain information about the sewer system on the right bank of the garonne which nt lacks for the second measurement campaign a re location of the ams from nt to either cv jr or jr was therefore considered a closer evaluation of the sites however revealed that those locations were not practical either due to the elevated risk of vandalism jr or accessibility cv jr the location of the ams for the second measurement therefore remained at nt as it was the best possible location of those that were practically feasible 3 3 oed for evaluation of measurement scenarios having both location and timing as degrees of freedom see section 3 1 2 results in a two dimensional problem as the evaluation of the timing scenarios depends on the location of the ams chosen therefore it was first analysed how often a timing scenario is evaluated as the most information rich scenario for all possible measurement locations before analysing the results in more detail for the chosen location of the ams in the last step the influence of the distance from the measurement point to the series of linear reservoirs on the identifiability of their respective local parameters was studied 3 3 1 most information rich scenario fig 7 shows for how many locations a timing scenario was the most information rich among the eight different locations the analysis was conducted once for all dwf and wwf scenarios and once for the dwf scenarios only scenario 1 4 the results show that independent of the location chosen the best case scenario is always scenario 6 which corresponds to the beginning of a big rain event if the analysis is conducted for the dwf scenarios only the results show that in seven out of the eight possible locations scenario 4 night following wwf contains the most information rich data the actual information content value of the d optimal criterion depends on the location and is analysed in detail for the chosen location of the ams in the next section 3 3 2 3 3 2 detailed analysis for chosen location as concluded in section 3 2 the best practically feasible locations of the two available ams is the combination of nt and cdh for which the d criterion results are summarized in table 1 one will notice that the absolute values for the d criterion are very low in absolute terms but this is due to the units used for the variables and parameters of the model optimality of the experiment does not depend on the absolute values but on the ranking of the d values for the dwf scenarios only 28 parameters were considered as the wwf tss runoff concentration can only be estimated during wwf scenarios the tabulated results show that the values of the d criterion are by orders of magnitude higher for the wwf scenarios than for the dwf scenarios the measurement campaign has therefore to focus on capturing wet weather conditions it is also to be noted that the beginning of a big rain event is by far richer in information than the tail of the event in case of dwf only conditions the results show that data collected during night 21 30 09 30 are richer in information content than during day 09 30 21 30 as the inverse of the fim v corresponds to the parameter estimation error covariance matrix the relative parameter estimation error of each parameter can be evaluated the squared parameter estimation errors of the parameters j σ j 2 are the diagonal elements of the matrix v the resulting relative parameter estimation error σ j θ j is shown for three timing scenarios in fig 8 the worst case scenario 1 top the best case dwf scenario 2 middle and the overall best case scenario 6 the figure clearly shows that parameter identification with only dwf measurements even under the best conditions is very difficult many parameters remain unidentifiable with a parameter estimation error much larger than the parameter value itself σ j θ j 1 an example for this is parameter 5 the flow at which half of the resuspension rate is reached q half for an upstream series of linear reservoirs in the information poorest scenario 1 the parameter has a parameter estimation error approximately 12 times its own value while in the best case dwf this ration is only reduced to approximately 3 analysing scenario 6 in fig 8 shows on the contrary that the error on the parameter estimation reduces dramatically for most parameters when measurements are conducted under wwf conditions for instance parameters 1 6 can now be estimated with a parameter estimation error that is less than 20 of their respective parameter value an excellent result for water quality process parameters it must be accepted though that some parameters remain difficult to identify which is further studied in the next section 3 3 3 3 3 3 parameter estimation error and distance from the measurement point for the chosen location of the ams and the best case scenario 6 the parameter estimation error is studied with respect to where in the model the parameter occurs i e in which sewer stretch the parameter occurs in the model the hypothesis analysed here is that the farther the sewer stretch is from a measurement point the lower the information content relative to the parameter occurring in that stretch is the distance measure that is adopted here is the number of series of linear reservoirs between the sewer stretch of interest and the closest downstream measurement point for this the layout of the case study shown in fig 5 is schematized in fig 9 indicating the distance from a measurement point by levels which indicate the number of series of linear reservoirs global parameters are primarily the different tss concentrations in the catchments which determine the global mass balance and are thus considered to be level zero parameters in fig 10 the distance to the closest downstream measurement point is given for the parameters ordered with respect to their error estimation σ j θ j this figure shows a marked tendency for the parameter estimation error to increase with the distance to the closest downstream measurement point global parameters level zero and parameters close to a measurement point can generally be better estimated 3 4 oed for optimal 12 h measurement segment of dwf day following the analysis of the scenarios in the previous section this last analysis tackles the question of which 12 h segment during stable dwf flow conditions contains the most information fig 11 shows the d optimal value for every 12 h segment following the full hour of the day as the previous section already suggested the information content is highly variable during the day and is lower during measurement segments starting during day hours than during night hours the 12 h segment with the most information content starts at 02 00 4 discussion in the results section 3 2 this paper showed how oed can be used to identify the optimal measurement location the results showed that the chosen location of the measurement station in the sewer nt ranked third the better ranked locations with regard to information content cv jr and jr were unfortunately not feasible as location of the ams for practical reasons this finding about the location of the ams is consistent with the location of the sewer stretch in which parameters occur that have a large parameter estimation error see results section 3 3 it was demonstrated that parameters occurring in sewer stretches located at a large distance to the closest downstream measurement point high level are difficult to estimate in general those stretches are located on the right bank of the river garonne moving the second ams to cv jr or jr would indeed have allowed collecting more information about the parameters in those stretches in the results section 3 3 different measurement scenarios were analysed for the chosen location of the ams cdh and nt it illustrates that for the same effort i e a measurement campaign of 12 h data of markedly different information content can be obtained depending on the scenario it was shown that data collected under wwf conditions generally contain more information than those under dwf with the beginning of a big rain event containing the most information this is due to the important dynamics occurring during wwf conditions and the first flush phenomenon observed for the given case study the scenario analysis for dwf showed that the measurement campaign starting at 21 30 contains more information than the one starting at 9 30 this might be because the tss concentrations during the night are generally lower than during daytime which means that the measurement errors are smaller thus the information content per tss value is higher for the studied parameters the results section 3 4 analysed all theoretically possible 12 h measurement segments starting at a full hour of a dwf day those results confirmed the findings of the previous section and show that the later during a workday the 12 h measurement campaign starts the more information content the measurement data will have the analysis also demonstrated that the information content of a measurement segment starting at 02 00 would contain most information but those segments are not feasible from a practical point of view due to accessibility issues of pumping stations at night for the planning of the measurement campaign it was imposed that the maintenance of the sensors must be performed prior to rain events because if the sensors fail during a rain event no maintenance intervention can be conducted for safety reasons since the beginning of the rain event is the critically important this is quite acceptable in case dwf conditions prevail the measurement campaign has to start late during the workday in order to capture the information rich night values to the fullest extent practicable having all of this information allows for planning a measurement campaign in its most efficient way as it clearly indicates when the measurement campaign should be prepared and started this ensures the optimal use of the measurement equipment and the limited resources during the campaign 5 conclusions this research demonstrates that adaptation of model based oed to complex sewer models is possible it also shows that oed is a valuable tool for planning measurement campaigns in the challenging sewer environment as it allows making optimal use of the investments necessary for such a campaign in comparison to posterior analysis of a measured data set oed evaluates the best location and timing for measurements prior to the actual measurement campaign this would otherwise need to be planned by expert opinion only oed allows to objectively rank different measurement locations with respect to their information content this enables balancing how far upstream or downstream an ams should be placed with respect to the timing scenarios both expert opinion and oed identify wwf events as more important than dwf conditions however oed also differentiates between the importance of a small versus a big rain event and their beginning versus their tail from a methodological point of view it was demonstrated that considering the real measurement error in this case constant relative error affects the evaluation of the dwf scenarios this would not have been possible without the mathematical tools provided by oed declaration of competing interest none acknowledgements the authors would like to thank two anonymous reviewers for their valuable comments the authors acknowledge the financial support by a collaborative research and development grant of the natural sciences and engineering research council nserc and suez treatment solutions canada the authors thank bordeaux metropole and société de gestion de l assainissement de bordeaux métropole sgac for technical and financial support peter vanrolleghem holds the canada research chair in water quality modelling 
6483,water quality data in the sewer system are indispensable for modelling but rarely available as measurements in sewers are challenging to conduct optimal experimental design oed is a powerful tool to identify and maximize the information content of measurement data this paper adopts a model based oed methodology to efficiently plan a measurement campaign for final model calibration and validation of a new sewer water quality model to do so a preliminary calibrated model of the case study is used to evaluate the information content of different potential measurement locations and scenarios for suspended solids as measured variable the case study first demonstrates how oed can identify the best measurement location within a complex sewer network it secondly demonstrates that measuring the beginning of a big rain event results in the most information rich data among all scenarios evaluated thirdly it analyses in detail the information content of dry weather flow dwf data in comparison to previous studies the methodology is improved by considering the actual measurement error characteristics when calculating the information content of measurement data keywords efficient data collection model based planning sewer sediments measurement campaign 1 introduction tackling the question of total suspended solids tss in sewer systems goes beyond simply understanding the fate of particulates throughout the sewer system itself tss is known as a carrier of nutrients but also as a carrier of heavy metals pesticides and pathogens among others moreover it is the cause of organic and inorganic pollution tss can thus be considered an indicator substance vanrolleghem et al 2018 developing models for the prediction of the tss flux for the control of overflow structures or the optimal management of the wastewater resource recovery facility wrrf has therefore far reaching benefits water quality modelling for particulates remains challenging in the sewer a main reason are the complex processes involved when it comes to tss as they greatly transcend mere advective transport particles can settle and resuspend depending on the condition they can flocculate aggregate or break and once settled they can be consolidated on the bottom of the sewer pipes in addition different physical chemical and biological processes take place both in the water phase and the sewer sediments ashley et al 2004 modelling developments are on going focusing on one or several processes to improve the understanding of those specific processes e g for gross solids penn et al 2018 or for bed load mohtar et al 2018 but even if models are currently not able to incorporate all the processes involved they help to understand the behaviour of tss in sewer systems independent of the tss process modelled data are indispensable for model calibration and validation in particular calibrating dynamic models benefits from high resolution data as provided by online sensors unfortunately these data are rarely available as measuring in the sewer system is not very widespread and ambitious to conduct furthermore they are distributed systems that may require measurements at multiple locations vanrolleghem et al 1999 measurements in sewers not only require a considerable investment in the equipment and the set up of the measurement site but also require a tight and intensive maintenance schedule to ensure measurements of reliable quality the sensors need to be manually cleaned calibrated and validated which includes labour intensive laboratory experiments ledergerber et al 2017 moreover not all data have the same information content for model calibration and validation in order to calibrate a parameter the parameter has to be influential during the time period when the data are collected dochain and vanrolleghem 2001 if a data set is available for calibration and validation of a complex model where overparameterization might be an issue identifiability analysis is a method to assess the parameters that can be estimated from the given measurement data set freni et al 2011 if however a new data set is to be collected model based optimal experimental design oed can evaluate prior to the measurement campaign which potential experiment of a set of proposed experiments contains the most information for model calibration vanrolleghem et al 1995 oed has mostly been applied in laboratory scale experiments and therefore in a controlled environment see for example vanrolleghem and coen 1995 oed utilizes a preliminary model that has been calibrated on an initial set of data first different experiments are proposed and simulated with the preliminary model then simulation results are evaluated in terms of their information content de pauw and vanrolleghem 2006a the information content of an experiment can be calculated from the fisher information matrix fim indicated in eq 1 1 fim k 1 n y i θ j t k t q t k y i θ j t k the fim links the information content of an experiment to the sensitivity of the model output corresponding to the measurements y i with respect to the parameters studied θ j for the timesteps t k during the period of the experiment time step 1 to n and a square matrix q t k with user supplied weighting coefficients de pauw and vanrolleghem 2006a q t k is typically chosen as the inverse of the measurement error covariance or as the identity matrix vanrolleghem et al 1995 an important characteristic of the fim is the fact that the inverse of the fim corresponds to the parameter estimation error covariance matrix v dochain and vanrolleghem 2001 thus allowing the direct assessment of the confidence region of the calibrated parameters different criteria exist to evaluate the information content of the proposed experiments see for example mehra 1974 this paper transcends the laboratory scale use of oed by applying it to design a measurement campaign in a full scale sewer environment it studies how the potential information content of a new measurement data set can be optimized it starts from the point that a first measurement campaign has already been conducted without oed this allows calibrating and validating a preliminary model and approximating the measurement error a preliminary model is required for planning the second measurement campaign with oed in the most efficient way making optimal use of the investments such as measurement equipment and working hours for maintenance in addition the results of the first measurement campaign permit approximating the measurement error for the weighting coefficients of the square matrix q t k rather than working with the identity matrix assumption as previous studies vanrolleghem et al 1995 freni and mannina 2012 this paper illustrates how oed can be adopted considering an approximation of the measurement error to select the best location for measurements and to the best timing of the measurement scenarios in the end a more detailed analysis of the experiments performed under dry weather flow dwf conditions highlight the importance of considering the measurement error 2 material and methods 2 1 case study 2 1 1 description the studied sewer system of the wrrf clos de hilde cdh is located in the southern parts of bordeaux france the wrrf has a treatment capacity of approximately 400 000 pe and the catchment covers about 8 000 ha the catchment is shown in fig 1 and is located on both sides of the river garonne which is the main receiving water the sewer system consists of both combined and separate sewers for more details see ledergerber et al 2017 local regulations for the wrrf include tss cod and bod5 concentrations thresholds at the effluent jorf 2015 and importantly this recent decree jorf 2015 sets for the first time combined sewer overflow cso water quality standards that cities will have to apply in the near future 2 1 2 available data and models data regarding water quantity flow are available throughout the sewer system and provided by the local utility several continuous flow measurements exist mainly at pumping stations in addition a calibrated hydraulic model implemented in mike urban dhi denmark is available for the studied catchment water quality data is generally collected by the utility only at the wrrf in the framework of this project a first measurement campaign to obtain an initial data set for water quality in the sewer system was conducted in 2017 2 1 3 measurement equipment and installation two automatic measurement stations ams rsm30 primodal systems hamilton on canada are available on site rieger and vanrolleghem 2008 both are equipped with two tss sensors based on different measurement principles one sensor is a turbidity meter visoturb 700 iq wtw weilheim germany which measures the intensity of light scattered at 90 degrees as a beam of light passes through a water sample the second sensor is a spectrometer spectro lyser s can messtechnik vienna austria measuring absorption spectra in the uv vis range the sensors are calibrated and validated with laboratory tss samples to collect the data for the preliminary model during the first measurement campaign one ams was installed at the inlet of the wrrf cdh while the other ams was installed at the major pumping chamber noutary nt the location of the sensors is indicated in fig 1 for more details on the installation at the measurement sites see ledergerber et al 2017 the data quality of the measurements was assured by applying a univariate data quality assessment method alferes et al 2013 resulting in validated high frequency data minimum one value per three minutes 2 2 preliminary model 2 2 1 modelling approach and software the preliminary model with which the oed is applied includes the catchments and the sewer network of the wrrf cdh the catchment model used is the kosim west model with the extensions proposed by pieper 2017 it includes sub models for flow and pollution generation during wet weather flow wwf on the one hand and dwf on the other hand the two main extensions regard firstly the possibility of routing dwf with linear reservoirs in series and secondly the splitting of the wwf in two parallel sets of linear reservoirs which allows the user to capture the varying dynamics especially in the context of larger aggregated models pieper 2017 the sewer model uses the so called particle settling velocity distribution psvd linear reservoirs in series model maruéjouls et al 2015 from a hydraulic perspective it is a comparably simple modelling approach but allows considering advective transport settling and resuspension of ten different particle classes keeping the computational time very competitive the distribution of settling characteristics of a tss sample can be measured with the vicas experiment resulting in the psvd curve of a sample chebbo and gromaire 2009 based on these measurements the total tss is fractionated into the ten classes of the sewer model the particles of a certain class settle with the settling velocity characteristic of their class the resuspension rate r resusp d 1 is calculated using sigmoidal eq 2 where q in m3 d is the inflow of the linear reservoir r resusp max d 1 corresponds to the maximum resuspension rate reached for big inflows q half m3 d represents the inflow at which half of the resuspension rate is reached and n is the exponent defining steepness of the sigmoidal curve the parameters r resusp max q half and n have to be calibrated for each sewer stretch the model is implemented in the software west by dhi horsholm dk 2 r resusp t r resusp max q in n t q in n t q half n 2 2 2 calibration and validation of preliminary model the preliminary model for oed was calibrated for water quantity flow and water quality tss the water quantity model for wwf was calibrated and validated rmse 0 058 m3 s on the existing calibrated mike urban by dhi model only the dwf parameters had to be recalibrated on actual flow measurements as the mike urban model focused on wwf the validation period on actual flow measurements during a 4 day period including wwf is indicated in fig 2 left hand side and resulted in a rmse of 0 064 m3 s the preliminary water quality calibration and validation was conducted using validated tss data obtained during the first measurement campaign the time series were chosen based on an analysis of quality of the available data a coherent measurement set was selected in which high quality data from both ams were available at the same time the model was calibrated on a 10 day period including two rain events and was validated for the same 4 day period as the data set used for water quantity model validation also indicated in fig 2 validation resulted in a rmse of 58 mg tss l 2 3 oed methodology 2 3 1 overview of methodology as mentioned in the previous section 1 the core of oed is the calculation of the fim and the evaluation of the experiments for a specific criterion however oed has to be viewed in a broader context fig 3 shows the oed methodology applied for this case study it was inspired by many previous studies e g de pauw and vanrolleghem 2006a vanrolleghem and coen 1995 vanrolleghem et al 1995 the methodology is divided into two phases the preparation phase left and the actual experimental design phase right 2 3 2 preparation phase the intended outcome of the preparation phase is a preliminary model with which a future planned experiments can be evaluated b the measurement error can be characterized and c initial values can be assigned to the set of parameters for which the oed and ultimately the measurement campaign is conducted the preparation phase consists of three steps the first of which is inherent to all modelling tasks dochain and vanrolleghem 2001 identifying the purpose of the model and with it the modelling objectives as well as the choice of corresponding model structure the second step is to evaluate the data requirements for model calibration and validation the data for the preliminary model have to be collected which can include either obtaining available data or conducting a first measurement campaign if not sufficient data are available an important aspect of this step is to analyse the measurement data and characterize the measurement error the third step of the preparatory phase is to build calibrate and validate the preliminary model and identify the parameter set for oed manifold references to calibration and validation criteria exist see for example hauduc et al 2015 2 3 3 experimental design phase the output of the second phase the actual experimental design phase is the optimally designed measurement campaign in view of recalibration of the identified parameters this phase consists of three different steps the first step is to propose experiments that could potentially be conducted in the context of a sewer system this step is better described as the identification of the measurement scenarios as unlike in laboratory conditions the measurement conditions as such cannot be influenced but the timing and location of the measurements can be chosen the proposed scenarios have to be realistic with respect to the available measurement equipment the duration of the campaign and the work required for implementation of the experiment the second step is then to evaluate the proposed scenarios in terms of expected information content taking the proper measurement error into account given the results of the oed evaluation criteria for the different scenarios proposed the accordingly optimized measurement campaign can be implemented 2 4 additional information calculation 2 4 1 considered parameters for oed the overall aim of the oed for this model is to improve tss predictions at the wrrf and during wwf also at csos as water quantity parameters are constrained by the water balance and were felt to be already well calibrated on the basis of the mike urban model only the parameters for water quality were considered for oed the 29 parameters affecting water quality are present both in the catchment and the sewer sub model the parameters to be recalibrated are the following parameters i in the catchment sub model the mean concentrations under dwf wwf and infiltration flow and ii in the sewer sub model the re suspension parameters in each of the series of linear reservoirs r resusp max q half and n as the particle setting velocity distributions were obtained from good quality laboratory vicas experiments they were not considered as parameters to be included in the design of the second measurement campaign also in contrast to other oed studies e g vanrolleghem et al 1995 the initial conditions of all state variables of the model were not part of the oed indeed they were not relevant for the simulation results as several days of warm up were used in the simulations before the actual scenarios were simulated 2 4 2 calculation of parameter sensitivity to complete the fisher information matrix elements the local sensitivity was calculated at each anticipated measurement point for the second measurement campaign as the central difference according to eq 3 de pauw and vanrolleghem 2006b the perturbation factor δ θ j to calculate the sensitivity was 1 of the parameter value for each of the parameters θ j 3 y i θ j t y i t θ j δ θ j y i t θ j δ θ j 2 δ θ j 2 4 3 definition of measurement error of tss as indicated in section 2 1 3 the data obtained in the first measurement campaign of 2017 was filtered using the method developed by alferes et al 2013 the comparison of the raw and the filtered data allows characterizing the measurement error ε meas tss which is defined as the absolute difference of the filtered data filtered and the raw data data raw as in eq 4 4 ε meas tss t data filtered t data raw t 2 4 4 evaluation criterion for oed for this study the d optimal design criterion in eq 5 is chosen among the available scalars that can be calculated from the fim mehra 1974 as the fim is the inverse of the parameter estimation error covariance matrix maximising the determinant of the fim results in minimizing the volume of the covariance matrix thus minimizing the parameter estimation error dochain and vanrolleghem 2001 5 max det fim 3 results first some general information about the results is presented in section 3 1 in the following section 3 2 the optimal location of the ams is evaluated and then in section 3 3 different scenarios for the chosen optimal location are studied in the final section 3 4 dwf measurements are studied in more detail 3 1 general results 3 1 1 measurement error estimation of tss based on the assumption that the relative measurement error α does not depend on the tss concentration conc tss the absolute measurement error ε meas tss as defined in section 2 4 3 depends linearly on conc tss 6 ε meas tss α conc tss this relationship can be observed in fig 4 for the tss measurements at the pumping station nt the coefficient α was obtained by fitting the above equation using least squares regression for both measurement locations 0 09 for the inlet of the wrrf cdh and 0 18 for the pumping station nt this difference between locations confirms the experience of the far more challenging measurement conditions in the sewer than at the wrrf 3 1 2 degrees of freedom and constraints for oed in order to be able to identify the best possible experiments hereafter named scenarios for the reasons mentioned in section 2 3 3 the degrees of freedom of the measurement campaign had to be identified they include the location of the ams and thus the location of the measurement set up as well as the timing of the measurement differentiating between wwf and dwf and identifying different timings for both situations knowledge of the typical performance of the used measurement equipment and anticipation of different measurement conditions allow proposing measurement scenarios the experience of the first measurement campaign showed that maintenance of the sensors is of fundamental importance and that sensors give only reliable data for a relatively short period of time after a maintenance event ledergerber et al 2017 for the given case study it is assumed that in the worst case the reliable period lasts for only about 12 h after a maintenance event so scenarios of 12 h were planned with a measurement interval of three minutes corresponding to the storage interval for the online sensors different measurement scenarios were then created for the typical dwf pattern for dwf the day is split into day and night conditions splitting at 09 30 respectively 21 30 assuming that a workday starts at 09 00 at the site and sensors would have received their routine maintenance and be ready for use by 09 30 measurement scenarios were also defined for wwf conditions considering that in contrast to dwf conditions each rain event is different to have representative rain scenarios the proposed experiments were simulated using actual rain data of the previous measurement campaign conducted in 2017 one big summer storm cumulative 24 h rainfall 24 9 mm was chosen in which multiple overflows were taking place in addition to a smaller rain event cumulative 24 h rainfall 2 4 mm similar rains were observed several times over the summer of 2017 in total seven different scenarios were proposed summarized in table 1 3 2 oed for evaluation of measurement location for the planned measurement campaign the oed methodology was used to re evaluate the original location of the ams at nt and cdh with respect to the information content of the measurement data as mentioned initially two ams were available for the measurement campaigns during the first measurement campaign in 2017 which was designed based on expertise and practical experience one ams was installed at the inlet of the wrrf cdh and the second was placed at the pumping station nt in the sewer system the location of the ams in the sewer system was re evaluated using seven other locations as possible locations see fig 5 the evaluated locations correspond all to pumping stations or retention tanks upstream of the wrrf in the sewer system the first ams was always maintained at the inlet of the wrrf since this is the final outlet of the sewer system as preliminary measurements were only available at the pumping station nt and the inlet of the wrrf cdh the measurement error could only be estimated for those two locations for the oed calculations it was assumed that the measurement error found at nt was representative of the other seven locations in the sewer system for the evaluation of the location of the ams timing scenario 6 beginning of big rain event was chosen as this was expected to be the best scenario independent of the location chosen given the large tss dynamics occurring this will be confirmed in section 3 3 fig 6 illustrates how the value of the d optimal criterion changes according to the location of the ams for timing scenario 6 the location of the ams at the pumping station nt chosen for the first measurement campaign ranks third among the eight identified options the two places that rank better are located further upstream and contain information about the sewer system on the right bank of the garonne which nt lacks for the second measurement campaign a re location of the ams from nt to either cv jr or jr was therefore considered a closer evaluation of the sites however revealed that those locations were not practical either due to the elevated risk of vandalism jr or accessibility cv jr the location of the ams for the second measurement therefore remained at nt as it was the best possible location of those that were practically feasible 3 3 oed for evaluation of measurement scenarios having both location and timing as degrees of freedom see section 3 1 2 results in a two dimensional problem as the evaluation of the timing scenarios depends on the location of the ams chosen therefore it was first analysed how often a timing scenario is evaluated as the most information rich scenario for all possible measurement locations before analysing the results in more detail for the chosen location of the ams in the last step the influence of the distance from the measurement point to the series of linear reservoirs on the identifiability of their respective local parameters was studied 3 3 1 most information rich scenario fig 7 shows for how many locations a timing scenario was the most information rich among the eight different locations the analysis was conducted once for all dwf and wwf scenarios and once for the dwf scenarios only scenario 1 4 the results show that independent of the location chosen the best case scenario is always scenario 6 which corresponds to the beginning of a big rain event if the analysis is conducted for the dwf scenarios only the results show that in seven out of the eight possible locations scenario 4 night following wwf contains the most information rich data the actual information content value of the d optimal criterion depends on the location and is analysed in detail for the chosen location of the ams in the next section 3 3 2 3 3 2 detailed analysis for chosen location as concluded in section 3 2 the best practically feasible locations of the two available ams is the combination of nt and cdh for which the d criterion results are summarized in table 1 one will notice that the absolute values for the d criterion are very low in absolute terms but this is due to the units used for the variables and parameters of the model optimality of the experiment does not depend on the absolute values but on the ranking of the d values for the dwf scenarios only 28 parameters were considered as the wwf tss runoff concentration can only be estimated during wwf scenarios the tabulated results show that the values of the d criterion are by orders of magnitude higher for the wwf scenarios than for the dwf scenarios the measurement campaign has therefore to focus on capturing wet weather conditions it is also to be noted that the beginning of a big rain event is by far richer in information than the tail of the event in case of dwf only conditions the results show that data collected during night 21 30 09 30 are richer in information content than during day 09 30 21 30 as the inverse of the fim v corresponds to the parameter estimation error covariance matrix the relative parameter estimation error of each parameter can be evaluated the squared parameter estimation errors of the parameters j σ j 2 are the diagonal elements of the matrix v the resulting relative parameter estimation error σ j θ j is shown for three timing scenarios in fig 8 the worst case scenario 1 top the best case dwf scenario 2 middle and the overall best case scenario 6 the figure clearly shows that parameter identification with only dwf measurements even under the best conditions is very difficult many parameters remain unidentifiable with a parameter estimation error much larger than the parameter value itself σ j θ j 1 an example for this is parameter 5 the flow at which half of the resuspension rate is reached q half for an upstream series of linear reservoirs in the information poorest scenario 1 the parameter has a parameter estimation error approximately 12 times its own value while in the best case dwf this ration is only reduced to approximately 3 analysing scenario 6 in fig 8 shows on the contrary that the error on the parameter estimation reduces dramatically for most parameters when measurements are conducted under wwf conditions for instance parameters 1 6 can now be estimated with a parameter estimation error that is less than 20 of their respective parameter value an excellent result for water quality process parameters it must be accepted though that some parameters remain difficult to identify which is further studied in the next section 3 3 3 3 3 3 parameter estimation error and distance from the measurement point for the chosen location of the ams and the best case scenario 6 the parameter estimation error is studied with respect to where in the model the parameter occurs i e in which sewer stretch the parameter occurs in the model the hypothesis analysed here is that the farther the sewer stretch is from a measurement point the lower the information content relative to the parameter occurring in that stretch is the distance measure that is adopted here is the number of series of linear reservoirs between the sewer stretch of interest and the closest downstream measurement point for this the layout of the case study shown in fig 5 is schematized in fig 9 indicating the distance from a measurement point by levels which indicate the number of series of linear reservoirs global parameters are primarily the different tss concentrations in the catchments which determine the global mass balance and are thus considered to be level zero parameters in fig 10 the distance to the closest downstream measurement point is given for the parameters ordered with respect to their error estimation σ j θ j this figure shows a marked tendency for the parameter estimation error to increase with the distance to the closest downstream measurement point global parameters level zero and parameters close to a measurement point can generally be better estimated 3 4 oed for optimal 12 h measurement segment of dwf day following the analysis of the scenarios in the previous section this last analysis tackles the question of which 12 h segment during stable dwf flow conditions contains the most information fig 11 shows the d optimal value for every 12 h segment following the full hour of the day as the previous section already suggested the information content is highly variable during the day and is lower during measurement segments starting during day hours than during night hours the 12 h segment with the most information content starts at 02 00 4 discussion in the results section 3 2 this paper showed how oed can be used to identify the optimal measurement location the results showed that the chosen location of the measurement station in the sewer nt ranked third the better ranked locations with regard to information content cv jr and jr were unfortunately not feasible as location of the ams for practical reasons this finding about the location of the ams is consistent with the location of the sewer stretch in which parameters occur that have a large parameter estimation error see results section 3 3 it was demonstrated that parameters occurring in sewer stretches located at a large distance to the closest downstream measurement point high level are difficult to estimate in general those stretches are located on the right bank of the river garonne moving the second ams to cv jr or jr would indeed have allowed collecting more information about the parameters in those stretches in the results section 3 3 different measurement scenarios were analysed for the chosen location of the ams cdh and nt it illustrates that for the same effort i e a measurement campaign of 12 h data of markedly different information content can be obtained depending on the scenario it was shown that data collected under wwf conditions generally contain more information than those under dwf with the beginning of a big rain event containing the most information this is due to the important dynamics occurring during wwf conditions and the first flush phenomenon observed for the given case study the scenario analysis for dwf showed that the measurement campaign starting at 21 30 contains more information than the one starting at 9 30 this might be because the tss concentrations during the night are generally lower than during daytime which means that the measurement errors are smaller thus the information content per tss value is higher for the studied parameters the results section 3 4 analysed all theoretically possible 12 h measurement segments starting at a full hour of a dwf day those results confirmed the findings of the previous section and show that the later during a workday the 12 h measurement campaign starts the more information content the measurement data will have the analysis also demonstrated that the information content of a measurement segment starting at 02 00 would contain most information but those segments are not feasible from a practical point of view due to accessibility issues of pumping stations at night for the planning of the measurement campaign it was imposed that the maintenance of the sensors must be performed prior to rain events because if the sensors fail during a rain event no maintenance intervention can be conducted for safety reasons since the beginning of the rain event is the critically important this is quite acceptable in case dwf conditions prevail the measurement campaign has to start late during the workday in order to capture the information rich night values to the fullest extent practicable having all of this information allows for planning a measurement campaign in its most efficient way as it clearly indicates when the measurement campaign should be prepared and started this ensures the optimal use of the measurement equipment and the limited resources during the campaign 5 conclusions this research demonstrates that adaptation of model based oed to complex sewer models is possible it also shows that oed is a valuable tool for planning measurement campaigns in the challenging sewer environment as it allows making optimal use of the investments necessary for such a campaign in comparison to posterior analysis of a measured data set oed evaluates the best location and timing for measurements prior to the actual measurement campaign this would otherwise need to be planned by expert opinion only oed allows to objectively rank different measurement locations with respect to their information content this enables balancing how far upstream or downstream an ams should be placed with respect to the timing scenarios both expert opinion and oed identify wwf events as more important than dwf conditions however oed also differentiates between the importance of a small versus a big rain event and their beginning versus their tail from a methodological point of view it was demonstrated that considering the real measurement error in this case constant relative error affects the evaluation of the dwf scenarios this would not have been possible without the mathematical tools provided by oed declaration of competing interest none acknowledgements the authors would like to thank two anonymous reviewers for their valuable comments the authors acknowledge the financial support by a collaborative research and development grant of the natural sciences and engineering research council nserc and suez treatment solutions canada the authors thank bordeaux metropole and société de gestion de l assainissement de bordeaux métropole sgac for technical and financial support peter vanrolleghem holds the canada research chair in water quality modelling 
6484,accurate estimation of reference evapotranspiration et0 is critical for water resource management and irrigation scheduling this study evaluated the potential of a new machine learning algorithm using gradient boosting on decision trees with categorical features support i e catboost for accurately estimating daily et0 with limited meteorological data in humid regions of china two other commonly used machine learning algorithms random forests rf and support vector machine svm were also assessed for comparison eight input combinations of daily meteorological data including both complete and incomplete combinations of solar radiation rs maximum and minimum temperatures tmax and tmin relative humidity hr and wind speed u from five weather stations during 2001 2015 in south china were applied for model training and testing the results showed that all the three algorithms could achieve satisfactory accuracy for et0 estimation in subtropical china using rs tmax and tmin or u hr tmax and tmin as inputs under the circumstances of lacking complete meteorological parameters the increases in testing rmse and mape over training rmse and mape showed positive correlations with the number of input parameters to the machine learning models for the local models among the three algorithms svm offered the best prediction accuracy and stability with incomplete combinations of meteorological parameters as inputs while catboost performed best with the complete combination of parameters patterns of the generalized models were almost the same as the local models but the former ones showed less than 10 decreases in rmse or mape in comparison with the latter ones in addition the computing time and memory usage for data processing of catboost were much less than those of rf and svm overall as a tree based algorithm catboost made significant improvements in accuracy stability and computational cost when compared to rf therefore the catboost algorithm has a very high potential for et0 estimation in humid regions of china and even possibly in other parts of the world with similar humid climates keywords random forests support vector machine gradient boosting decision tree computational complexity model comparison catboost 1 introduction evapotranspiration et is the loss of water from the ground and vegetation into the atmosphere composed of evaporation from ground and vegetation surfaces plus transpiration through vegetation as an important component of the water cycle and energy balance et would not only affect plant growth and development but also influence the atmospheric circulation and consequently the climate therefore accurate estimation of et plays an important role in hydrological model development irrigation scheduling and regional water resource allocation although et can be directly measured by certain approaches such as weighting lysimeters or eddy covariance systems ding et al 2010 fan et al 2018c these approaches are generally expensive time consuming and not applicable at large spatial scales which impairs their broad application in reality especially for developing countries a widely used alternative for et estimation is to model et rates using reference evapotranspiration et0 and the crop conversion coefficient kc allen et al 1998 choudhury and singh 2016 reference evapotranspiration et0 is a semi mechanism and semi empirical formula independent of crop type and reproductive stage et0 is a function of climatic variables which are solar radiation air temperature wind speed and relative humidity these climatic variables are generally specific to the geographical location and climate type of the region concerned the penman monteith method was recommended by the food and agriculture organization of the united nations fao as the standard equation for et0 estimation due to its good precision and stability allen et al 1998 feng et al 2017a 2017b lei et al 2019 although the penman monteith fao 56 pmf 56 method has been applied widely in many fields the main drawback of this method is that it requires a large amount of meteorological data this could be a vital limitation for the application of pmf 56 in many far off locations especially in developing countries in such circumstances alternative methods relying on limited climatic data are needed mattar 2018 the calculation of et0 can be treated as a set of complicated and nonlinear regression processes that depend on many meteorological variables fan et al 2018a however it is usually hard to establish one empirical model to accurately describing all complex and non linear processes especially in the absence of some key information the most obvious limitation of empirical models is that their applicability is usually regional specific kisi 2016 therefore researchers proposed artificial intelligence algorithms for hydrological research fotovatikhah et al 2018 lu et al 2018 qasem et al 2019a 2019b including et0 estimation because they require nothing on knowledge of internal variables and provide simple solutions for non linear and multi variable functions the artificial intelligence models are well capable of dealing with nonlinear relationships between dependent and independent variables and have a strong modeling ability for some engineering problems and natural phenomena various artificial intelligence methods have been developed to predict et0 among which the most popular methods are artificial neural network ann models khoob 2008 compared hargreaves and ann models for estimating et0 at 12 weather stations in khuzestan plain iran where they found ann models predicted et0 better than the hargreaves method and the latter usually underestimated or overestimated et0 luo et al 2015 evaluated four ann models for et0 estimation based on public weather forecasts results of which showed that ann models were suitable for short term daily et0 forecasting shiri et al 2015 developed an adaptive neuro fuzzy inference system anfis and ann for estimating et0 through k folders cross validation methods shamshirband et al 2015 proposed a hybrid model cockoo search algorithm and ann to estimate et0 with data from twelve meteorological stations in serbia antonopoulos and antonopoulos 2017 compared ann models with the radiation and mass transfer based empirical methods in north greece gavili et al 2018 used three artificial intelligence models i e ann anfis and gene expression programming gep and five empirical models for estimating et0 in kurdistan iran in which ann showed the best prediction in this region similar results were also reported by yassin et al 2016 where ann presented higher accuracy than gep in et0 estimation kernel based algorithms have been widely used recently for their merits in accuracy and stability among which the most popular ones are support vector machine svm and extreme learning machine elm fan et al 2018b however it should be noted that not all elm models are kernel based and one kind of elm actually uses neural networks with randomly set number of neurons abdullah et al 2015 wen et al 2015 estimated daily et0 using an svm model with limited meteorological data in the extreme arid regions of china and they found that the svm model produced more accurate et0 estimates than the ann or empirical models similar work was also conducted in southwest china by feng et al 2017a 2017b with temperature data using elm kisi 2016 compared the performance of lssvm mars and m5 model tree for estimated et0 in the mediterranean region of turkey and concluded that the lssvm model outperformed the mars and the m5 model tree in the local application scenario but mars had higher accuracy in using other dataset scenarios svm and elm were also used to project future variability of et0 by downscaling in north china yin et al 2017 fan et al 2018a compared svm elm with other four tree based models in different climate zones of china and they found that svm and elm models had higher accuracies in predicting daily et0 but cost more computing time the svm and elm models have also been coupled with other algorithms such as the genetic algorithm ga yin et al 2017 mohammadrezapoura et al 2018 the firefly algorithm ffa gocić et al 2015 the bat algorithm wu et al 2018 the wavelet transform wt gocić et al 2015 and the wavelet gaussian process karbasi 2018 gradient boosting is a powerful machine learning technology that could achieve most advanced results in a variety of practical tasks lu et al 2018 fan et al 2019a it has been the primary method during the past decades for solving learning problems with heterogeneous characteristics noisy data and complex dependencies such as web searching dorogush et al 2018 weather forecasting kusiak et al 2009 global solar radiation prediction aler et al 2017 and et0 estimation fan et al 2018a theoretical results of the gradient boosting provide solid explanation on how iteration combines basic predictions weak models through a greedy process corresponding to gradient descent in function space catboost is a novel gradient boosting technology proposed by yandex company dorogush et al 2018 this algorithm makes many improvements in parallelism which means that it can complete the layout in less time and is easier to implement on the internet network in addition several improvements have been made to overcome the over fitting of model dorogush et al 2018 prokhorenkova et al 2018 it is clear that ann svm elm and many other machine learning models kisi and cengiz 2013 feng et al 2016 wu et al 2019 are frequently used in et0 modeling while tree based ensemble models especially catboost models are rarely used the comparison between simple tree based models and common support vector machine models is yet to be fully developed although high prediction accuracy is the main consideration when using artificial intelligence model good stability and less computational workload should also be considered hassan et al 2017 some models are inherently unstable and may produce less accurate estimates when forecasting with new data sets thus the aims of this study are to 1 evaluate accuracy stability and running time costs of the catboost model by comparing with rf and svm for modeling daily et0 using different meteorological datasets 2 evaluate the effects of different meteorological factors on et0 and 3 establish and evaluate the applicability of the general model in sub tropical humid regions 2 materials and methods 2 1 random forests rf random forests rf contains multiple decision tree classifiers and uses the breiman s bagging idea to ensemble many decision trees into a single but strong model breiman 2001 it uses the self help method i e the bootstrap resampling technology to generate new training sample sets from the original training samples of n by repeatedly selecting random k k n sets of samples during the overall selecting process some samples may be collected more than once in each round of the random sampling of bagging about 36 8 of the training data will not be sampled referring as out of bag oob data these uncollected data do not participate in the fitting of models during training but in turn can be used for detecting the generalization ability of models the training sample is used to generate k buffeting the decision or regression trees cart for the development of random forests and then classify the test sample by majority vote decision or use average as return values given the fact that randomness can effectively reduce model variance random forests in general can achieve good generalization ability and low variance resistance without additional pruning of course the fitting degree of model during training may get worse leading to higher bias but it is only relative however it is noted that the cart algorithm is a binary tree which means each non leaf node can only lead to two branches if a non leaf node is a multi level more than two discrete variable the variable is likely to be used multiple times at the same time if a non leaf node is a continuous variable the decision tree will treat it as a discrete variable the cart used in rf is based on the feature selection of the gini coefficient the gini coefficient is selected according to the criterion that each child node reaches its highest purity with all observations on that child node belonging to the same classification in that case the gini coefficient reaches its minimum value with lowest uncertainty but highest purity the gini coefficient of cart can be expressed as follows 1 gini p 2 p 1 p when traversing each segmentation point of each feature if feature a a is used d is divided into two parts namely d 1 the sample set that satisfies a a and d 2 the sample set that satisfies a a under the condition of feature a a the gini coefficient of d is 2 gini d a d 1 d g i n i d 1 d 2 d g i n i d 2 where gini d a represents the uncertainty of d every cart in rf is to find the segmentation point of the feature with smallest gini coefficients by continuously traversing all possible segmentation points of the feature in the tree and the dividing the data set into two subsets until the stop condition is met the cart algorithm in rf is different from other traditional algorithms firstly every feature selected for use in a rf tree is randomly generated from all m features which itself has reduced the risk and tendency of overfitting the model will not be determined by specific eigenvalues or feature combinations the increase of randomness will not improve the control of model s fitting ability indefinitely secondly unlike ordinary decision trees rf improves the establishment of decision trees for an ordinary decision tree choosing an optimal feature among all m sample features on the node to do the left and right subtree division of the decision tree is required however each tree of rf is actually a part of selected features among these few features an optimal feature is selected to divide the left and right subtrees of the decision tree expanding the effect of randomness and further enhancing the generalization ability of the model assuming that m sub features are selected for each tree the smaller the m sub is the worse the fitting degree of the model to the training set will be and the bias will increase but the generalization ability will be stronger and the variance of the model will decrease a larger m sub is the opposite in practice the value of m sub is generally considered as a parameter which will be constantly adjusted to obtain an appropriate value through tuning the structure of the rf algorithm is shown in fig 3 2 2 support vector machine svm to reduce the overfitting problem the support vector machine svm adopts the theory of structural risk minimization instead of empirical risk minimization svm can be applied for solving regression and classification problems for the classification problem the original problem is transformed into the convex quadratic programming problem when using svm given that the training data x y x i x r y i y r are randomly and independently generated by an unknown function svm can be translated into the following equation 3 f x ω ϕ x b where x is the input data ϕ x represents a function which can transfer the x into the high dimensional feature spaces ω and b are coefficients which are estimated by minimizing the regularized risk function 4 1 2 ω 2 c 1 n i 1 n l ε y i f x i where 1 2 ω 2 is a regularized term minimizing the regularized term can make a function close to minimizing empirical risk 1 n i 1 n l ε ξ i ξ i c is the regularization constant and ɛ is the threshold of acceptable minimum error by the svm which is set as 0 01 in this study l ε is expressed as 5 l ε y i f x i y i f x i ε y i f x i ε 0 y i f x i ε to estimate the ω and b eq 3 can be transformed into the primal objective function 6 by introducing the positive slack variables ξ i 6 1 2 ω 2 c 1 n i 1 n l ε ξ i ξ i lagrange multiplier method can solve the above optimization problem so the primal objective function can be written in its dual form 7 max ω α i α i i 1 n α i α i y i 1 2 i j 1 n α i α i α j α j ϕ x i ϕ x i 1 n α i α i ε where α i α i are lagrange multipliers ma et al 2019 thus the solution of eq 3 can be obtained as follows 8 f x i 1 n α i α i ϕ x i ϕ x b if the kernel function is introduced in eq 8 can be rewritten as 9 f x i 1 n α i α i k x i x b detailed principles and algorithms can be found in cortes and vapnik 1995 in this study svm is used for daily et0 prediction the kernel function uses the gaussian kernel function k x k x which is expressed as 10 k x k x exp γ x k x 2 where γ is the parameter of the kernel function another parameter the parameter c used in svm the equations not shown here is the regularization coefficient which is used to suppress the excessive model parameters and thereby alleviate the over fitting problem however there is no theoretical basis for the above two parameters which need to be determined by the trial and error method 2 3 categorical boosting catboost catboost is a new gradient boosting decision tree gbdt algorithm that can handle categorical features well this algorithm is different from traditional gbdt algorithms in the following aspects 1 dealing with categorical features during training time instead of preprocessing time catboost allows the use of whole dataset for training according to prokhorenkova et al 2018 target statistics ts is a very efficient method for handing categorical features with minimum information loss specifically for each example catboost performs a random permutation of the dataset and computes an average label value for the example with the same category value placed before the given one in the permutation if a permutation is θ σ 1 σ n n t it is substituted with 11 x σ p k j 1 p 1 x σ j k x σ p k y σ j β p j 1 p 1 x σ j k x σ p k β where p is a prior value and β is the weight of the prior for regression tasks the standard technique for calculating prior is to take the average label value in the dataset 2 feature combinations all the categorical features could be combined as a new one when constructing a new split for the tree catboost uses a greedy way to consider the combinations no combination is considered for the first split in the tree but for the second and the subsequent splits catboost combines all combinations preset with all categorical features in the dataset all splits selected in the tree are considered as a category with two values and used in combination 3 unbiased boosting with categorical features when using the ts method to convert categorical features into numerical values the distribution will be different from the original distribution and the deviation of this distribution will cause the deviation of the solution which is an inevitable problem for traditional gbdt methods prokhorenkova et al 2018 developed a new method through theoretical analysis to overcome the gradient bias which was named as ordered boosting the pseudo code of ordered boosting is expressed as follows algorithm ordered boosting input x k y k k 1 n ordered according to σ the number of trees i σ random permutation of 1 n m i 0 for i 1 n for t 1 to i do for i 1 to n do r i y i m σ i 1 x i for i 1 to n do δ m l e a r n m o d e l x i r j σ j i m i m i δ m return m n note that m i is trained without using the example x i and each m i shares the same tree structure in catboost random permutations of the training data are generated multiple permutations will be used to enhance the algorithm s robustness by sampling a random permutation and obtaining gradients on its basis these permutations are same to the ones applied for calculating statistics for categorical features for training distinct models different permutations will be used and hence the use of several permutations will not lead to overfitting for each permutation σ n different models mi will be trained as shown above suggesting that for building one tree it requires to store and recalculate o n 2 approximations for each permutation σ and update each model m i from m i x 1 to m i x i consequently the complexity of this operation is o s n 2 in practice an important trick that reduces the complexity of one tree construction to o s n is applied specifically for each permutation rather than storing and updating o n 2 values m i x i values m i x j i 1 log 2 n j 2 i 1 are maintained where m i x j is the approximation for the same j based on the first 2 i samples as a result the number of predictions m i x j will not be larger than 0 i log 2 n 2 i 1 4 n 4 fast scorer catboost uses oblivious trees as base predictors in which the same splitting criterion is used across an entire level of the tree kohavi and li 1995 such trees are balanced and less prone to overfitting in oblivious trees each leaf index is encoded as a binary vector with the length equal to the depth of the tree this principle is widely used in catboost model evaluators for the calculation of model predictions because all binaries use float statistics and one hot encoded features the structure of the catboost algorithm is shown in fig 4 2 4 case study and data in this study daily climatic data from 12 weather stations in south china were used for training and testing artificial intelligence models for estimating et0 this region is characterized as a subtropical monsoon climate wang et al 2017 however the climate in this region is complicated and vulnerable to the influence of el nino and typhoons and thus extreme floods and droughts often occur the region includes the yangtze river basin the largest river basin in china which often suffers from disastrous floods for instance the 1998 yangtze river flood affected over 200 million people resulting in more than 4 thousand dead and 20 billion direct economic loss for other areas in this region such as the yunnan province often suffer from droughts for example in 2009 a devastating drought occurred in yunnan and lasted for months with over 7 million people suffering from the scarcity of drinking water and over 2 million hectares of farmland lacking of irrigation water due to its complexity this region has become a hot spot for studying the distribution and dynamics in climates and water resources the 12 selected stations in this study range from the coast to the inland and cover various types of topography including plain plateau and basin which together can represent the major climate features in this region the climatic data were provided by the national meteorological information center nmic of china meteorological administration url http data cma cn continuous and long term series of observed daily solar radiation rs maximum and minimum air temperatures at 2 m height tmax and tmin relative humidity hr and wind speed at 2 m height u during 2001 2015 were collected the observed data were divided into two groups of which one group 2001 2010 was used for developing and training the artificial intelligence models and the other group 2011 2015 was used for model testing see fig 2 a detailed description of the 12 selected stations is listed in table 1 and fig 1 although these sites are in the same climate zone based on the köppen geiger climate classification method meteorological characteristics of the kunming station are quite different from the other stations mainly because this station locates at the yunnan guizhou plateau closer to the inland and less affected by the western pacific monsoon therefore kunming has the highest mean daily rs among the 12 stations in addition stations of mianyang and chongqing are located at the sichuan basin which have lower rs than the other 10 stations 2 5 input combinations and parameter calibration eight input combinations of daily meteorological variables including rs tmax and tmin hr and u were considered in this study for model training and testing table 2 the eight input combinations consisted of both the complete combination i e combination 8 with all meteorological variables included and the incomplete combinations i e combinations 1 7 with part of the variables combined see table 2 for detailed description the catboost rf and svm algorithms were performed with the catboost e1071 and randomforest r packages respectively using the r computing environment version 3 5 1 r core team 2018 the url for the catboost package is https github com catboost catboost tree master catboost r package all the simulations were performed in a computer with intel cpu i7 6700 3 4 4 0 ghz and 16 gb of ram memory primary parameters of the three machine learning models were optimized with the grid search method all parameter pairs were tried thoroughly and the one with the best performance was used for model training and testing according to ibrahim and khatib 2017 the main parameters of the rf algorithm are the maximum depth and the number of trees trees are more prone to be overfitting if they have larger maximum depths in this study the upper and lower limits of parameters were first determined by the trial and error method a grid was then created to search for the best combination of parameters by setting different step sizes for the rf model the maximum depth of trees varied between 2 and 16 at 2 intervals and the number of trees ranged from 100 to 1000 at 100 intervals catboost is also a tree based algorithm although it has many parameters the main parameters affecting model s accuracy and stability are the same as rf thus for the catboost model the number of rounds varied between 200 and 800 at 100 intervals and the maximum tree depth ranged from 2 to 10 at 2 intervals with the subset ratio of all datasets varying from 0 5 to 1 at 0 05 intervals for the svm model the most important parameters are the regularization coefficient and the parameter gamma of the kernel function kisi 2016 the gaussian kernel is usually more accurate than the linear kernel mehdizadeh et al 2017 therefore the gaussian function was selected as the radial basis kernel function in the svm model where the parameter c varied between 10 and 100 at 10 intervals and the parameter γ ranged from 5 to 50 at 5 intervals 2 6 reference evapotranspiration model according to the pmf 56 equation allen et al 1998 et0 mm d 1 is calculated as 12 e t 0 0 408 r n g γ 900 t a 273 u e s e a δ γ 1 0 34 u where rn mj m 2 day 1 is the net radiation at the crop surface which is usually calculated by rs global solar radiation g is the soil heat flux density mj m 2 day 1 ta is the mean daily air temperature at 2 m height c u is the wind speed at 2 m height m s 1 es and ea are the saturation and actual vapor pressures kpa respectively δ is the slope of vapor pressure curve kpa c 1 and γ is the air psychrometric constant kpa c 1 as in the daily time scale of this study g can be neglected details of the pmf 56 equation can be found in allen et al 1998 2 7 statistical evaluation four commonly employed statistical indicators were used to evaluate the models in this study fan et al 2019b wu et al 2019 ma 2019 including rmse r2 mbe and mape which are expressed as 13 r 2 i 1 n y i m y i m y i e y i e 2 i 1 n y i m y i m 2 i 1 n y i e y i e 2 14 rmse 1 n i 1 n y i m y i e 2 15 mbe 1 n i 1 n y i m y i e 16 mape 1 n i 1 n y i m y i e y i m where yi m yi e y i m and y i e are the measured estimated mean of measured mean of estimated et0 respectively n is the number of observations higher r2 values indicate better model performance and data fitting conversely lower values of rmse mbe and mape suggest better prediction accuracy to meet the requirements of the svm model raw weather data were normalized to 0 1 prior to training and testing using the following formula 17 z n z i z min z max z min where z n and z i are the normalized and raw values of the data z max and z min represent the minimum and maximum values of the raw data 3 results and discussion 3 1 prediction of machine learning models in this study capabilities of three machine learning approaches i e catboost rf and svm for predicting et0 with daily meteorological variables of tmax tmin rs hr and u were investigated meteorological data from 12 weather stations in south china were used since there were 4 statistical indicators used in this study and each of them showed inconsistent patterns among the 24 models 3 algorithms 8 input combinations and the 12 stations it was difficult to evaluate the overall performance of different models based on raw values of statistical indicators therefore the indicators were sorted according to a ranking procedure proposed by elagib and mansell 2000 specifically for each indicator in each station the 24 models were compared using the measures of accuracy i e errors including rmse mbe and mape and goodness of fit r2 and then each model was assigned with a rating score based on the individual measures for example the model with the smallest value of error or the highest r2 was ranked number 1 i e ranking score 1 while the model with the largest error or the lowest r2 was ranked number 24 i e ranking score 24 the sum of ranking scores was then adopted for evaluating the overall average ranking of models in such cases the best model was the one having the smallest score the outcome of the ranking process was summarized in two ways which were the sum of ranking scores of the 12 stations for each statistical indicator with each input combination during the training and testing stages table 3 and the sum of ranking scores of the different statistical indicators r2 rmse and mape in each station with each input combination table 4 in addition values of statistical indicators for each model are shown in the appended fig s1 among the three algorithms rf generally performed best with lowest ranking scores in r2 rmse and mape except in mbe during the training period but worst with highest ranking scores in r2 rmse and mape in the testing stage indicating that the rf models had some degree of overfitting table 3 for the specific input combination during the testing stage the goodness of model performance ranked as svm catboost rf in the combinations 1 6 and as catboost svm rf in the combination 8 while for the combination 7 the pros and cons of svm and catboost showed inconsistent patterns across statistical indicators table 3 the reason for the above phenomena may be related to the nature of tree algorithms tree based models employ a greedy algorithm to obtain the best training accuracy which means that such models will use input parameters to interpret the output results as much as possible if the input parameters are incomplete data sets a model cannot explain all the nonlinear relationships which inevitably leads to the overfitting problems while for the overall model performance based on ranking scores of r2 rmse and mape at each station table 4 the patterns were generally similar to the findings listed above table 3 for all stations except for hangzhou station 8 and guangzhou station 11 where the trends of model performance differed from the overall average rankings this was probably due to the variations in data between the training and testing stages to better describe the contributions of each meteorological parameter to the models a new diagram is drawn fig 5 the following is a brief introduction to the diagram the abscissa of each graph represents the different parameters which are dimensionless the vertical coordinate represents the ranking scores of each model multiple lines are drawn from each x coordinate to represent different meteorological parameters in which the individual vertical lines represent only one parameter as input e g temperature as the only input and the intersected lines represent combinations of various parameters to illustrate the difference in contributions of different meteorological parameters to model performance three stations including kunming inland plateau wuhan inland plain and fuzhou coastal plain representing the major types of terrains in the study region were selected for plotting overall for each weather station patterns of model ranking scores representing different parameter combinations among the three algorithms i e catboost rf and svm were almost identical fig 5 taking the catboost models at wuhan station as an example fig 5b the model with a complete combination of meteorological parameters i e t u hr and rs in this study as inputs had higher accuracy i e a lower ranking score than any other catboost models with an incomplete combination of parameters furthermore in terms of improving model accuracy adding hr to t and rs as a third input led to much better model performance than adding u but both of which showed higher accuracy than using the combination of t hr and u as inputs for models using two parameter input combinations on the basis of using t as the first input the effectiveness of the second input parameter on improving model accuracy was rank as rs hr u fig 4b patterns of the catboost model performance at the other two stations i e kunming and fuzhou fig 4a and b were both generally similar to those at the wuhan station with only slight differences in a few input combinations this may be attributed to the difference in data distribution pattern between the training and testing stages at the wuhan station where the relative humidity showed an increasing trend during the testing stage when compared with the training stage overall for the three parameters u hr and rs their importance for improving model performance on the basis of t was ranked as rs hr u for the three algorithms in this study furthermore the combination of t hr and u always showed much better model accuracy improvement than two parameter input combinations therefore considering that the radiation data are not always readily available it is recommended that a more readily available input parameter combination including t hr and u is used as an alternative for the region correlations between predicted et0 values by the rf catboost and svm models and their corresponding fao56 pm values i e pmf 56 et0 in the training and testing stages are shown as scatter plots figs s2 s7 taking the kunming station as an example for any input combination the rf model always produced less scattered estimates at the training stage but more scattered estimates at the testing stage when compared with the catboost or the svm model figs 5 10 indicating that catboost and svm models exhibited higher accuracy on daily et0 estimation this also confirmed that tree based ensemble methods could achieve significant improvement on the prediction of daily et0 and the tree based catboost model should be of high potential in this aspect furthermore estimations by either catboost or svm models in this study had r2 values over 0 80 with any input combination except combination 1 suggesting that tree based models can achieve decent precision on et0 prediction in humid regions of china with incomplete combinations of meteorological parameters as input fan et al 2018a have identified similar results and suggested that tree based ensemble models could obtain superior et0 estimates with only tmax tmin and rs instead of complete meteorological data as input in the tropical and subtropical zones of china in addition to this input combination it can be clearly seen that models using combination 5 tmax tmin u and hr as input could also describe the estimating trends well and yield very ideal accuracy at most weather stations feng et al 2017a 2017b developed an rf model at two stations of southwest china where the climate type was the same as that in this study and the r2 values were 0 984 and 0 988 with completed combination of meteorological parameters as input respectively in this study r2 values of the 12 stations for rf and svm models averaged at 0 986 and 0 987 respectively both of which were very close to the above findings in addition the corresponding r2 values of catboost models averaged at 0 992 were slightly better than either rf or svm wen et al 2015 evaluated the performance of svm in an extreme arid region of china they found that values of rmse and r2 were 0 262 mm d 1 and 0 590 with only daily temperature as input and were 0 262 mm d 1 and 0 903 with the combination of tmax tmin u2 and rs therefore it is possible that good model accuracy could be obtained with only air temperature as input in arid areas however unlike the arid areas air humidity changes rapidly in humid regions and therefore the influence of hr on model accuracy cannot be neglected 3 2 stability of machine learning models as shown in the previous sector based on ranking scores the rf model outperformed the corresponding catboost and svm models in the training stage but showed worse performance than the latter two in the testing stage table 3 according to the percentage increases in the testing statistical indicators over the training statistical indicators averaged from all 12 weather stations the rf models consistently exhibited substantially larger percentage increases in rmse and mape but decreases in r2 than their corresponding catboost or svm models fig 6 this indicates that when an rf model is applied to an unknown region for predicting et0 with incomplete combinations of meteorological parameters as input its error may be very huge among the three types of models the svm models showed most stable performance with smallest increases in the testing rmse from 4 8 to 37 4 and mape from 3 3 to 33 3 in the combinations 1 5 and 7 compared with the training stage slightly better than the catboost models 7 3 42 3 in rmse and 1 1 43 4 in mape for the other two input combinations i e the combinations 6 and 8 catboost models performed better than their corresponding rf ones with smaller increases in rmse and mape fig 6 overall these patterns suggest that the newly developed catboost algorithm with many improvements upon the traditional tree based rf models has high potential for overcoming the problems of overfitting 3 3 generalized machine learning models the ranking scores of statistical indicators of the generalized models during the training and testing stages and among the 12 stations are compared in tables 5 and 6 furthermore values of statistical indicators at the testing stage for the generalized models are shown in the appended fig s8 similar to previous results the rf models had best accuracies during the training period and worst accuracies during the testing period for the three algorithms during the testing stage the catboost model showed best accuracy with the complete combination of parameters as input while the svm models performed best with the other seven incomplete input combinations table 5 compared with the station specific model accuracies on estimating et0 at any of the 12 stations fig s1 the generalized models showed deceases in model accuracy however the magnitudes of decreases were no more than 10 in rmse or mape 3 4 computational complexity of machine learning models in addition to the accuracy and stability minimizing the complexity of models is also very important to model construction the time complexity of rf is o t r k n log 2 n n 0 632 n during the training stage while the time complexities for catboost and svm are o t r s n and o k 2 n 2 respectively prokhorenkova et al 2018 because both rf and catboost are capable of parallel computing their spatial complexity varies with the number of parallelism it is clear that the spatial complexities of rf and catboost for only one tree preserving the sorting and splitting points of data is more than o k n 2 and o s n respectively louppe 2014 prokhorenkova et al 2018 while for the svm its spatial complexity ranges from o k n 2 to o n sv 3 n n sv 2 k n n sv where n sv is the number of support vectors burges 1998 to verify whether there is any superiority for the catboost algorithm in computing efficiency a computational time cost and memory usage experiment with five levels of datasets was conducted computational time costs for the three machine learning models applied to a single station and all stations are presented in fig 7 to explore the relationships between training data size and computing time consumption for different algorithms five datasets with different levels of data size were used for testing the levels of data size were set as level 1 10 year data from one station level 2 10 year data from 12 stations level 3 20 year data from 12 stations level 4 30 year data from 12 stations and level 5 40 year data from 12 stations respectively it is clear that the average computing time was algorithm specific and depended on the size of the training data and that the correlation between training data size and computing time for each algorithm was not linear fig 7a across all the five datasets the average time consumed by the catboost algorithm was much less than that of the rf or the svm algorithm and among the latter two the computing time of rf models was equivalent to with levels 1 3 datasets or less than with levels 4 5 datasets svm models specifically with the level 1 dataset the catboost algorithm did not show any significant advantage in computational time cost when compared with the other two algorithms however as the size of training data increased the computing time costs of rf and svm increased sharply but the time cost of catboost did not change too much with datasets from level 2 to level 5 the computational costs of rf and svm were 13 0 34 4 and 10 9 52 7 times the cost of catboost which fully illustrated the advantages of the newly developed tree based algorithm catboost in parallel computing hassan et al 2017 have also indicated that the computational costs of traditional svm models were nearly 40 times the computational costs of tree based ensemble methods for predicting global solar radiation on an hourly basis tree based algorithms are generally capable of building decision trees in parallel which would more or less reduces the computing time however the paralleling efficiency of the rf algorithm in this study was far less than that of the catboost algorithm and therefore the rf models consumed much more time than the catboost models overall among the three algorithms in this study the catboost was fastest while the svm was slowest in terms of data processing the memory usage of rf models increased proportionally to the increasing size of input datasets which exceeded 1 gb at level 2 and reached 4 gb at level 5 fig 7b with the datasets of level 1 and 2 svm had the minimum memory usage less than 100 mb among the three algorithms and meanwhile catboost used slightly more memory than svm however when the size of datasets was greater than or equal to level 3 the memory footprint of catboost was smaller than svm therefore as the latest developed tree based model catboost has obvious optimization in time and space complexity especially when the size of input dataset is large 4 conclusions this study presented the application of a newly developed machine learning algorithm catboost for accurately estimating daily et0 in the subtropical humid zones of china the catboost algorithm along with two other commonly used machine learning algorithms rf and svm were trained and tested with meteorological data including rs tmax tmin hr and u from five weather stations during 2001 2015 in south china the results showed that all the three algorithms could achieve satisfactory accuracy for et0 estimation in subtropical china using rs tmax and tmin or u hr tmax and tmin as inputs under the circumstances of lacking complete meteorological parameters the increases in testing rmse and mape over training rmse and mape showed positive correlations with the number of input parameters to the machine learning models for the local models among the three algorithms svm offered the best predicting accuracy and stability with incomplete combinations of meteorological parameters as inputs while catboost performed best with the complete combination of parameters patterns of the generalized models were almost the same as those of the local models but the former ones showed less than 10 decreases in rmse or mape in comparison with the latter ones in addition the computing time and memory usage for data processing of catboost were much less than those of rf and svm overall as a tree based algorithm catboost made significant improvements in accuracy stability and computational cost when compared to rf therefore the catboost algorithm has a very high potential for et0 estimation in humid regions of china and even possibly in other parts of the world with similar humid climates conflict of interest statement we declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work there is no professional or other personal interest of any nature or kind in any product service and or company that could be construed as influencing the position presented in or the review of the manuscript entitled acknowledgements this study was jointly supported by the foundation of key laboratory of crop water use and regulation china firi2018 01 the national natural science foundation of china no 51709143 61703199 51790533 and the natural science foundation of jiangxi province china 20181bab206045 special thanks to the national meteorological information center of china meteorological administration for offering the meteorological data appendix a supplementary material supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 04 085 appendix a supplementary material the following are the supplementary data to this article supplementary data 1 
6484,accurate estimation of reference evapotranspiration et0 is critical for water resource management and irrigation scheduling this study evaluated the potential of a new machine learning algorithm using gradient boosting on decision trees with categorical features support i e catboost for accurately estimating daily et0 with limited meteorological data in humid regions of china two other commonly used machine learning algorithms random forests rf and support vector machine svm were also assessed for comparison eight input combinations of daily meteorological data including both complete and incomplete combinations of solar radiation rs maximum and minimum temperatures tmax and tmin relative humidity hr and wind speed u from five weather stations during 2001 2015 in south china were applied for model training and testing the results showed that all the three algorithms could achieve satisfactory accuracy for et0 estimation in subtropical china using rs tmax and tmin or u hr tmax and tmin as inputs under the circumstances of lacking complete meteorological parameters the increases in testing rmse and mape over training rmse and mape showed positive correlations with the number of input parameters to the machine learning models for the local models among the three algorithms svm offered the best prediction accuracy and stability with incomplete combinations of meteorological parameters as inputs while catboost performed best with the complete combination of parameters patterns of the generalized models were almost the same as the local models but the former ones showed less than 10 decreases in rmse or mape in comparison with the latter ones in addition the computing time and memory usage for data processing of catboost were much less than those of rf and svm overall as a tree based algorithm catboost made significant improvements in accuracy stability and computational cost when compared to rf therefore the catboost algorithm has a very high potential for et0 estimation in humid regions of china and even possibly in other parts of the world with similar humid climates keywords random forests support vector machine gradient boosting decision tree computational complexity model comparison catboost 1 introduction evapotranspiration et is the loss of water from the ground and vegetation into the atmosphere composed of evaporation from ground and vegetation surfaces plus transpiration through vegetation as an important component of the water cycle and energy balance et would not only affect plant growth and development but also influence the atmospheric circulation and consequently the climate therefore accurate estimation of et plays an important role in hydrological model development irrigation scheduling and regional water resource allocation although et can be directly measured by certain approaches such as weighting lysimeters or eddy covariance systems ding et al 2010 fan et al 2018c these approaches are generally expensive time consuming and not applicable at large spatial scales which impairs their broad application in reality especially for developing countries a widely used alternative for et estimation is to model et rates using reference evapotranspiration et0 and the crop conversion coefficient kc allen et al 1998 choudhury and singh 2016 reference evapotranspiration et0 is a semi mechanism and semi empirical formula independent of crop type and reproductive stage et0 is a function of climatic variables which are solar radiation air temperature wind speed and relative humidity these climatic variables are generally specific to the geographical location and climate type of the region concerned the penman monteith method was recommended by the food and agriculture organization of the united nations fao as the standard equation for et0 estimation due to its good precision and stability allen et al 1998 feng et al 2017a 2017b lei et al 2019 although the penman monteith fao 56 pmf 56 method has been applied widely in many fields the main drawback of this method is that it requires a large amount of meteorological data this could be a vital limitation for the application of pmf 56 in many far off locations especially in developing countries in such circumstances alternative methods relying on limited climatic data are needed mattar 2018 the calculation of et0 can be treated as a set of complicated and nonlinear regression processes that depend on many meteorological variables fan et al 2018a however it is usually hard to establish one empirical model to accurately describing all complex and non linear processes especially in the absence of some key information the most obvious limitation of empirical models is that their applicability is usually regional specific kisi 2016 therefore researchers proposed artificial intelligence algorithms for hydrological research fotovatikhah et al 2018 lu et al 2018 qasem et al 2019a 2019b including et0 estimation because they require nothing on knowledge of internal variables and provide simple solutions for non linear and multi variable functions the artificial intelligence models are well capable of dealing with nonlinear relationships between dependent and independent variables and have a strong modeling ability for some engineering problems and natural phenomena various artificial intelligence methods have been developed to predict et0 among which the most popular methods are artificial neural network ann models khoob 2008 compared hargreaves and ann models for estimating et0 at 12 weather stations in khuzestan plain iran where they found ann models predicted et0 better than the hargreaves method and the latter usually underestimated or overestimated et0 luo et al 2015 evaluated four ann models for et0 estimation based on public weather forecasts results of which showed that ann models were suitable for short term daily et0 forecasting shiri et al 2015 developed an adaptive neuro fuzzy inference system anfis and ann for estimating et0 through k folders cross validation methods shamshirband et al 2015 proposed a hybrid model cockoo search algorithm and ann to estimate et0 with data from twelve meteorological stations in serbia antonopoulos and antonopoulos 2017 compared ann models with the radiation and mass transfer based empirical methods in north greece gavili et al 2018 used three artificial intelligence models i e ann anfis and gene expression programming gep and five empirical models for estimating et0 in kurdistan iran in which ann showed the best prediction in this region similar results were also reported by yassin et al 2016 where ann presented higher accuracy than gep in et0 estimation kernel based algorithms have been widely used recently for their merits in accuracy and stability among which the most popular ones are support vector machine svm and extreme learning machine elm fan et al 2018b however it should be noted that not all elm models are kernel based and one kind of elm actually uses neural networks with randomly set number of neurons abdullah et al 2015 wen et al 2015 estimated daily et0 using an svm model with limited meteorological data in the extreme arid regions of china and they found that the svm model produced more accurate et0 estimates than the ann or empirical models similar work was also conducted in southwest china by feng et al 2017a 2017b with temperature data using elm kisi 2016 compared the performance of lssvm mars and m5 model tree for estimated et0 in the mediterranean region of turkey and concluded that the lssvm model outperformed the mars and the m5 model tree in the local application scenario but mars had higher accuracy in using other dataset scenarios svm and elm were also used to project future variability of et0 by downscaling in north china yin et al 2017 fan et al 2018a compared svm elm with other four tree based models in different climate zones of china and they found that svm and elm models had higher accuracies in predicting daily et0 but cost more computing time the svm and elm models have also been coupled with other algorithms such as the genetic algorithm ga yin et al 2017 mohammadrezapoura et al 2018 the firefly algorithm ffa gocić et al 2015 the bat algorithm wu et al 2018 the wavelet transform wt gocić et al 2015 and the wavelet gaussian process karbasi 2018 gradient boosting is a powerful machine learning technology that could achieve most advanced results in a variety of practical tasks lu et al 2018 fan et al 2019a it has been the primary method during the past decades for solving learning problems with heterogeneous characteristics noisy data and complex dependencies such as web searching dorogush et al 2018 weather forecasting kusiak et al 2009 global solar radiation prediction aler et al 2017 and et0 estimation fan et al 2018a theoretical results of the gradient boosting provide solid explanation on how iteration combines basic predictions weak models through a greedy process corresponding to gradient descent in function space catboost is a novel gradient boosting technology proposed by yandex company dorogush et al 2018 this algorithm makes many improvements in parallelism which means that it can complete the layout in less time and is easier to implement on the internet network in addition several improvements have been made to overcome the over fitting of model dorogush et al 2018 prokhorenkova et al 2018 it is clear that ann svm elm and many other machine learning models kisi and cengiz 2013 feng et al 2016 wu et al 2019 are frequently used in et0 modeling while tree based ensemble models especially catboost models are rarely used the comparison between simple tree based models and common support vector machine models is yet to be fully developed although high prediction accuracy is the main consideration when using artificial intelligence model good stability and less computational workload should also be considered hassan et al 2017 some models are inherently unstable and may produce less accurate estimates when forecasting with new data sets thus the aims of this study are to 1 evaluate accuracy stability and running time costs of the catboost model by comparing with rf and svm for modeling daily et0 using different meteorological datasets 2 evaluate the effects of different meteorological factors on et0 and 3 establish and evaluate the applicability of the general model in sub tropical humid regions 2 materials and methods 2 1 random forests rf random forests rf contains multiple decision tree classifiers and uses the breiman s bagging idea to ensemble many decision trees into a single but strong model breiman 2001 it uses the self help method i e the bootstrap resampling technology to generate new training sample sets from the original training samples of n by repeatedly selecting random k k n sets of samples during the overall selecting process some samples may be collected more than once in each round of the random sampling of bagging about 36 8 of the training data will not be sampled referring as out of bag oob data these uncollected data do not participate in the fitting of models during training but in turn can be used for detecting the generalization ability of models the training sample is used to generate k buffeting the decision or regression trees cart for the development of random forests and then classify the test sample by majority vote decision or use average as return values given the fact that randomness can effectively reduce model variance random forests in general can achieve good generalization ability and low variance resistance without additional pruning of course the fitting degree of model during training may get worse leading to higher bias but it is only relative however it is noted that the cart algorithm is a binary tree which means each non leaf node can only lead to two branches if a non leaf node is a multi level more than two discrete variable the variable is likely to be used multiple times at the same time if a non leaf node is a continuous variable the decision tree will treat it as a discrete variable the cart used in rf is based on the feature selection of the gini coefficient the gini coefficient is selected according to the criterion that each child node reaches its highest purity with all observations on that child node belonging to the same classification in that case the gini coefficient reaches its minimum value with lowest uncertainty but highest purity the gini coefficient of cart can be expressed as follows 1 gini p 2 p 1 p when traversing each segmentation point of each feature if feature a a is used d is divided into two parts namely d 1 the sample set that satisfies a a and d 2 the sample set that satisfies a a under the condition of feature a a the gini coefficient of d is 2 gini d a d 1 d g i n i d 1 d 2 d g i n i d 2 where gini d a represents the uncertainty of d every cart in rf is to find the segmentation point of the feature with smallest gini coefficients by continuously traversing all possible segmentation points of the feature in the tree and the dividing the data set into two subsets until the stop condition is met the cart algorithm in rf is different from other traditional algorithms firstly every feature selected for use in a rf tree is randomly generated from all m features which itself has reduced the risk and tendency of overfitting the model will not be determined by specific eigenvalues or feature combinations the increase of randomness will not improve the control of model s fitting ability indefinitely secondly unlike ordinary decision trees rf improves the establishment of decision trees for an ordinary decision tree choosing an optimal feature among all m sample features on the node to do the left and right subtree division of the decision tree is required however each tree of rf is actually a part of selected features among these few features an optimal feature is selected to divide the left and right subtrees of the decision tree expanding the effect of randomness and further enhancing the generalization ability of the model assuming that m sub features are selected for each tree the smaller the m sub is the worse the fitting degree of the model to the training set will be and the bias will increase but the generalization ability will be stronger and the variance of the model will decrease a larger m sub is the opposite in practice the value of m sub is generally considered as a parameter which will be constantly adjusted to obtain an appropriate value through tuning the structure of the rf algorithm is shown in fig 3 2 2 support vector machine svm to reduce the overfitting problem the support vector machine svm adopts the theory of structural risk minimization instead of empirical risk minimization svm can be applied for solving regression and classification problems for the classification problem the original problem is transformed into the convex quadratic programming problem when using svm given that the training data x y x i x r y i y r are randomly and independently generated by an unknown function svm can be translated into the following equation 3 f x ω ϕ x b where x is the input data ϕ x represents a function which can transfer the x into the high dimensional feature spaces ω and b are coefficients which are estimated by minimizing the regularized risk function 4 1 2 ω 2 c 1 n i 1 n l ε y i f x i where 1 2 ω 2 is a regularized term minimizing the regularized term can make a function close to minimizing empirical risk 1 n i 1 n l ε ξ i ξ i c is the regularization constant and ɛ is the threshold of acceptable minimum error by the svm which is set as 0 01 in this study l ε is expressed as 5 l ε y i f x i y i f x i ε y i f x i ε 0 y i f x i ε to estimate the ω and b eq 3 can be transformed into the primal objective function 6 by introducing the positive slack variables ξ i 6 1 2 ω 2 c 1 n i 1 n l ε ξ i ξ i lagrange multiplier method can solve the above optimization problem so the primal objective function can be written in its dual form 7 max ω α i α i i 1 n α i α i y i 1 2 i j 1 n α i α i α j α j ϕ x i ϕ x i 1 n α i α i ε where α i α i are lagrange multipliers ma et al 2019 thus the solution of eq 3 can be obtained as follows 8 f x i 1 n α i α i ϕ x i ϕ x b if the kernel function is introduced in eq 8 can be rewritten as 9 f x i 1 n α i α i k x i x b detailed principles and algorithms can be found in cortes and vapnik 1995 in this study svm is used for daily et0 prediction the kernel function uses the gaussian kernel function k x k x which is expressed as 10 k x k x exp γ x k x 2 where γ is the parameter of the kernel function another parameter the parameter c used in svm the equations not shown here is the regularization coefficient which is used to suppress the excessive model parameters and thereby alleviate the over fitting problem however there is no theoretical basis for the above two parameters which need to be determined by the trial and error method 2 3 categorical boosting catboost catboost is a new gradient boosting decision tree gbdt algorithm that can handle categorical features well this algorithm is different from traditional gbdt algorithms in the following aspects 1 dealing with categorical features during training time instead of preprocessing time catboost allows the use of whole dataset for training according to prokhorenkova et al 2018 target statistics ts is a very efficient method for handing categorical features with minimum information loss specifically for each example catboost performs a random permutation of the dataset and computes an average label value for the example with the same category value placed before the given one in the permutation if a permutation is θ σ 1 σ n n t it is substituted with 11 x σ p k j 1 p 1 x σ j k x σ p k y σ j β p j 1 p 1 x σ j k x σ p k β where p is a prior value and β is the weight of the prior for regression tasks the standard technique for calculating prior is to take the average label value in the dataset 2 feature combinations all the categorical features could be combined as a new one when constructing a new split for the tree catboost uses a greedy way to consider the combinations no combination is considered for the first split in the tree but for the second and the subsequent splits catboost combines all combinations preset with all categorical features in the dataset all splits selected in the tree are considered as a category with two values and used in combination 3 unbiased boosting with categorical features when using the ts method to convert categorical features into numerical values the distribution will be different from the original distribution and the deviation of this distribution will cause the deviation of the solution which is an inevitable problem for traditional gbdt methods prokhorenkova et al 2018 developed a new method through theoretical analysis to overcome the gradient bias which was named as ordered boosting the pseudo code of ordered boosting is expressed as follows algorithm ordered boosting input x k y k k 1 n ordered according to σ the number of trees i σ random permutation of 1 n m i 0 for i 1 n for t 1 to i do for i 1 to n do r i y i m σ i 1 x i for i 1 to n do δ m l e a r n m o d e l x i r j σ j i m i m i δ m return m n note that m i is trained without using the example x i and each m i shares the same tree structure in catboost random permutations of the training data are generated multiple permutations will be used to enhance the algorithm s robustness by sampling a random permutation and obtaining gradients on its basis these permutations are same to the ones applied for calculating statistics for categorical features for training distinct models different permutations will be used and hence the use of several permutations will not lead to overfitting for each permutation σ n different models mi will be trained as shown above suggesting that for building one tree it requires to store and recalculate o n 2 approximations for each permutation σ and update each model m i from m i x 1 to m i x i consequently the complexity of this operation is o s n 2 in practice an important trick that reduces the complexity of one tree construction to o s n is applied specifically for each permutation rather than storing and updating o n 2 values m i x i values m i x j i 1 log 2 n j 2 i 1 are maintained where m i x j is the approximation for the same j based on the first 2 i samples as a result the number of predictions m i x j will not be larger than 0 i log 2 n 2 i 1 4 n 4 fast scorer catboost uses oblivious trees as base predictors in which the same splitting criterion is used across an entire level of the tree kohavi and li 1995 such trees are balanced and less prone to overfitting in oblivious trees each leaf index is encoded as a binary vector with the length equal to the depth of the tree this principle is widely used in catboost model evaluators for the calculation of model predictions because all binaries use float statistics and one hot encoded features the structure of the catboost algorithm is shown in fig 4 2 4 case study and data in this study daily climatic data from 12 weather stations in south china were used for training and testing artificial intelligence models for estimating et0 this region is characterized as a subtropical monsoon climate wang et al 2017 however the climate in this region is complicated and vulnerable to the influence of el nino and typhoons and thus extreme floods and droughts often occur the region includes the yangtze river basin the largest river basin in china which often suffers from disastrous floods for instance the 1998 yangtze river flood affected over 200 million people resulting in more than 4 thousand dead and 20 billion direct economic loss for other areas in this region such as the yunnan province often suffer from droughts for example in 2009 a devastating drought occurred in yunnan and lasted for months with over 7 million people suffering from the scarcity of drinking water and over 2 million hectares of farmland lacking of irrigation water due to its complexity this region has become a hot spot for studying the distribution and dynamics in climates and water resources the 12 selected stations in this study range from the coast to the inland and cover various types of topography including plain plateau and basin which together can represent the major climate features in this region the climatic data were provided by the national meteorological information center nmic of china meteorological administration url http data cma cn continuous and long term series of observed daily solar radiation rs maximum and minimum air temperatures at 2 m height tmax and tmin relative humidity hr and wind speed at 2 m height u during 2001 2015 were collected the observed data were divided into two groups of which one group 2001 2010 was used for developing and training the artificial intelligence models and the other group 2011 2015 was used for model testing see fig 2 a detailed description of the 12 selected stations is listed in table 1 and fig 1 although these sites are in the same climate zone based on the köppen geiger climate classification method meteorological characteristics of the kunming station are quite different from the other stations mainly because this station locates at the yunnan guizhou plateau closer to the inland and less affected by the western pacific monsoon therefore kunming has the highest mean daily rs among the 12 stations in addition stations of mianyang and chongqing are located at the sichuan basin which have lower rs than the other 10 stations 2 5 input combinations and parameter calibration eight input combinations of daily meteorological variables including rs tmax and tmin hr and u were considered in this study for model training and testing table 2 the eight input combinations consisted of both the complete combination i e combination 8 with all meteorological variables included and the incomplete combinations i e combinations 1 7 with part of the variables combined see table 2 for detailed description the catboost rf and svm algorithms were performed with the catboost e1071 and randomforest r packages respectively using the r computing environment version 3 5 1 r core team 2018 the url for the catboost package is https github com catboost catboost tree master catboost r package all the simulations were performed in a computer with intel cpu i7 6700 3 4 4 0 ghz and 16 gb of ram memory primary parameters of the three machine learning models were optimized with the grid search method all parameter pairs were tried thoroughly and the one with the best performance was used for model training and testing according to ibrahim and khatib 2017 the main parameters of the rf algorithm are the maximum depth and the number of trees trees are more prone to be overfitting if they have larger maximum depths in this study the upper and lower limits of parameters were first determined by the trial and error method a grid was then created to search for the best combination of parameters by setting different step sizes for the rf model the maximum depth of trees varied between 2 and 16 at 2 intervals and the number of trees ranged from 100 to 1000 at 100 intervals catboost is also a tree based algorithm although it has many parameters the main parameters affecting model s accuracy and stability are the same as rf thus for the catboost model the number of rounds varied between 200 and 800 at 100 intervals and the maximum tree depth ranged from 2 to 10 at 2 intervals with the subset ratio of all datasets varying from 0 5 to 1 at 0 05 intervals for the svm model the most important parameters are the regularization coefficient and the parameter gamma of the kernel function kisi 2016 the gaussian kernel is usually more accurate than the linear kernel mehdizadeh et al 2017 therefore the gaussian function was selected as the radial basis kernel function in the svm model where the parameter c varied between 10 and 100 at 10 intervals and the parameter γ ranged from 5 to 50 at 5 intervals 2 6 reference evapotranspiration model according to the pmf 56 equation allen et al 1998 et0 mm d 1 is calculated as 12 e t 0 0 408 r n g γ 900 t a 273 u e s e a δ γ 1 0 34 u where rn mj m 2 day 1 is the net radiation at the crop surface which is usually calculated by rs global solar radiation g is the soil heat flux density mj m 2 day 1 ta is the mean daily air temperature at 2 m height c u is the wind speed at 2 m height m s 1 es and ea are the saturation and actual vapor pressures kpa respectively δ is the slope of vapor pressure curve kpa c 1 and γ is the air psychrometric constant kpa c 1 as in the daily time scale of this study g can be neglected details of the pmf 56 equation can be found in allen et al 1998 2 7 statistical evaluation four commonly employed statistical indicators were used to evaluate the models in this study fan et al 2019b wu et al 2019 ma 2019 including rmse r2 mbe and mape which are expressed as 13 r 2 i 1 n y i m y i m y i e y i e 2 i 1 n y i m y i m 2 i 1 n y i e y i e 2 14 rmse 1 n i 1 n y i m y i e 2 15 mbe 1 n i 1 n y i m y i e 16 mape 1 n i 1 n y i m y i e y i m where yi m yi e y i m and y i e are the measured estimated mean of measured mean of estimated et0 respectively n is the number of observations higher r2 values indicate better model performance and data fitting conversely lower values of rmse mbe and mape suggest better prediction accuracy to meet the requirements of the svm model raw weather data were normalized to 0 1 prior to training and testing using the following formula 17 z n z i z min z max z min where z n and z i are the normalized and raw values of the data z max and z min represent the minimum and maximum values of the raw data 3 results and discussion 3 1 prediction of machine learning models in this study capabilities of three machine learning approaches i e catboost rf and svm for predicting et0 with daily meteorological variables of tmax tmin rs hr and u were investigated meteorological data from 12 weather stations in south china were used since there were 4 statistical indicators used in this study and each of them showed inconsistent patterns among the 24 models 3 algorithms 8 input combinations and the 12 stations it was difficult to evaluate the overall performance of different models based on raw values of statistical indicators therefore the indicators were sorted according to a ranking procedure proposed by elagib and mansell 2000 specifically for each indicator in each station the 24 models were compared using the measures of accuracy i e errors including rmse mbe and mape and goodness of fit r2 and then each model was assigned with a rating score based on the individual measures for example the model with the smallest value of error or the highest r2 was ranked number 1 i e ranking score 1 while the model with the largest error or the lowest r2 was ranked number 24 i e ranking score 24 the sum of ranking scores was then adopted for evaluating the overall average ranking of models in such cases the best model was the one having the smallest score the outcome of the ranking process was summarized in two ways which were the sum of ranking scores of the 12 stations for each statistical indicator with each input combination during the training and testing stages table 3 and the sum of ranking scores of the different statistical indicators r2 rmse and mape in each station with each input combination table 4 in addition values of statistical indicators for each model are shown in the appended fig s1 among the three algorithms rf generally performed best with lowest ranking scores in r2 rmse and mape except in mbe during the training period but worst with highest ranking scores in r2 rmse and mape in the testing stage indicating that the rf models had some degree of overfitting table 3 for the specific input combination during the testing stage the goodness of model performance ranked as svm catboost rf in the combinations 1 6 and as catboost svm rf in the combination 8 while for the combination 7 the pros and cons of svm and catboost showed inconsistent patterns across statistical indicators table 3 the reason for the above phenomena may be related to the nature of tree algorithms tree based models employ a greedy algorithm to obtain the best training accuracy which means that such models will use input parameters to interpret the output results as much as possible if the input parameters are incomplete data sets a model cannot explain all the nonlinear relationships which inevitably leads to the overfitting problems while for the overall model performance based on ranking scores of r2 rmse and mape at each station table 4 the patterns were generally similar to the findings listed above table 3 for all stations except for hangzhou station 8 and guangzhou station 11 where the trends of model performance differed from the overall average rankings this was probably due to the variations in data between the training and testing stages to better describe the contributions of each meteorological parameter to the models a new diagram is drawn fig 5 the following is a brief introduction to the diagram the abscissa of each graph represents the different parameters which are dimensionless the vertical coordinate represents the ranking scores of each model multiple lines are drawn from each x coordinate to represent different meteorological parameters in which the individual vertical lines represent only one parameter as input e g temperature as the only input and the intersected lines represent combinations of various parameters to illustrate the difference in contributions of different meteorological parameters to model performance three stations including kunming inland plateau wuhan inland plain and fuzhou coastal plain representing the major types of terrains in the study region were selected for plotting overall for each weather station patterns of model ranking scores representing different parameter combinations among the three algorithms i e catboost rf and svm were almost identical fig 5 taking the catboost models at wuhan station as an example fig 5b the model with a complete combination of meteorological parameters i e t u hr and rs in this study as inputs had higher accuracy i e a lower ranking score than any other catboost models with an incomplete combination of parameters furthermore in terms of improving model accuracy adding hr to t and rs as a third input led to much better model performance than adding u but both of which showed higher accuracy than using the combination of t hr and u as inputs for models using two parameter input combinations on the basis of using t as the first input the effectiveness of the second input parameter on improving model accuracy was rank as rs hr u fig 4b patterns of the catboost model performance at the other two stations i e kunming and fuzhou fig 4a and b were both generally similar to those at the wuhan station with only slight differences in a few input combinations this may be attributed to the difference in data distribution pattern between the training and testing stages at the wuhan station where the relative humidity showed an increasing trend during the testing stage when compared with the training stage overall for the three parameters u hr and rs their importance for improving model performance on the basis of t was ranked as rs hr u for the three algorithms in this study furthermore the combination of t hr and u always showed much better model accuracy improvement than two parameter input combinations therefore considering that the radiation data are not always readily available it is recommended that a more readily available input parameter combination including t hr and u is used as an alternative for the region correlations between predicted et0 values by the rf catboost and svm models and their corresponding fao56 pm values i e pmf 56 et0 in the training and testing stages are shown as scatter plots figs s2 s7 taking the kunming station as an example for any input combination the rf model always produced less scattered estimates at the training stage but more scattered estimates at the testing stage when compared with the catboost or the svm model figs 5 10 indicating that catboost and svm models exhibited higher accuracy on daily et0 estimation this also confirmed that tree based ensemble methods could achieve significant improvement on the prediction of daily et0 and the tree based catboost model should be of high potential in this aspect furthermore estimations by either catboost or svm models in this study had r2 values over 0 80 with any input combination except combination 1 suggesting that tree based models can achieve decent precision on et0 prediction in humid regions of china with incomplete combinations of meteorological parameters as input fan et al 2018a have identified similar results and suggested that tree based ensemble models could obtain superior et0 estimates with only tmax tmin and rs instead of complete meteorological data as input in the tropical and subtropical zones of china in addition to this input combination it can be clearly seen that models using combination 5 tmax tmin u and hr as input could also describe the estimating trends well and yield very ideal accuracy at most weather stations feng et al 2017a 2017b developed an rf model at two stations of southwest china where the climate type was the same as that in this study and the r2 values were 0 984 and 0 988 with completed combination of meteorological parameters as input respectively in this study r2 values of the 12 stations for rf and svm models averaged at 0 986 and 0 987 respectively both of which were very close to the above findings in addition the corresponding r2 values of catboost models averaged at 0 992 were slightly better than either rf or svm wen et al 2015 evaluated the performance of svm in an extreme arid region of china they found that values of rmse and r2 were 0 262 mm d 1 and 0 590 with only daily temperature as input and were 0 262 mm d 1 and 0 903 with the combination of tmax tmin u2 and rs therefore it is possible that good model accuracy could be obtained with only air temperature as input in arid areas however unlike the arid areas air humidity changes rapidly in humid regions and therefore the influence of hr on model accuracy cannot be neglected 3 2 stability of machine learning models as shown in the previous sector based on ranking scores the rf model outperformed the corresponding catboost and svm models in the training stage but showed worse performance than the latter two in the testing stage table 3 according to the percentage increases in the testing statistical indicators over the training statistical indicators averaged from all 12 weather stations the rf models consistently exhibited substantially larger percentage increases in rmse and mape but decreases in r2 than their corresponding catboost or svm models fig 6 this indicates that when an rf model is applied to an unknown region for predicting et0 with incomplete combinations of meteorological parameters as input its error may be very huge among the three types of models the svm models showed most stable performance with smallest increases in the testing rmse from 4 8 to 37 4 and mape from 3 3 to 33 3 in the combinations 1 5 and 7 compared with the training stage slightly better than the catboost models 7 3 42 3 in rmse and 1 1 43 4 in mape for the other two input combinations i e the combinations 6 and 8 catboost models performed better than their corresponding rf ones with smaller increases in rmse and mape fig 6 overall these patterns suggest that the newly developed catboost algorithm with many improvements upon the traditional tree based rf models has high potential for overcoming the problems of overfitting 3 3 generalized machine learning models the ranking scores of statistical indicators of the generalized models during the training and testing stages and among the 12 stations are compared in tables 5 and 6 furthermore values of statistical indicators at the testing stage for the generalized models are shown in the appended fig s8 similar to previous results the rf models had best accuracies during the training period and worst accuracies during the testing period for the three algorithms during the testing stage the catboost model showed best accuracy with the complete combination of parameters as input while the svm models performed best with the other seven incomplete input combinations table 5 compared with the station specific model accuracies on estimating et0 at any of the 12 stations fig s1 the generalized models showed deceases in model accuracy however the magnitudes of decreases were no more than 10 in rmse or mape 3 4 computational complexity of machine learning models in addition to the accuracy and stability minimizing the complexity of models is also very important to model construction the time complexity of rf is o t r k n log 2 n n 0 632 n during the training stage while the time complexities for catboost and svm are o t r s n and o k 2 n 2 respectively prokhorenkova et al 2018 because both rf and catboost are capable of parallel computing their spatial complexity varies with the number of parallelism it is clear that the spatial complexities of rf and catboost for only one tree preserving the sorting and splitting points of data is more than o k n 2 and o s n respectively louppe 2014 prokhorenkova et al 2018 while for the svm its spatial complexity ranges from o k n 2 to o n sv 3 n n sv 2 k n n sv where n sv is the number of support vectors burges 1998 to verify whether there is any superiority for the catboost algorithm in computing efficiency a computational time cost and memory usage experiment with five levels of datasets was conducted computational time costs for the three machine learning models applied to a single station and all stations are presented in fig 7 to explore the relationships between training data size and computing time consumption for different algorithms five datasets with different levels of data size were used for testing the levels of data size were set as level 1 10 year data from one station level 2 10 year data from 12 stations level 3 20 year data from 12 stations level 4 30 year data from 12 stations and level 5 40 year data from 12 stations respectively it is clear that the average computing time was algorithm specific and depended on the size of the training data and that the correlation between training data size and computing time for each algorithm was not linear fig 7a across all the five datasets the average time consumed by the catboost algorithm was much less than that of the rf or the svm algorithm and among the latter two the computing time of rf models was equivalent to with levels 1 3 datasets or less than with levels 4 5 datasets svm models specifically with the level 1 dataset the catboost algorithm did not show any significant advantage in computational time cost when compared with the other two algorithms however as the size of training data increased the computing time costs of rf and svm increased sharply but the time cost of catboost did not change too much with datasets from level 2 to level 5 the computational costs of rf and svm were 13 0 34 4 and 10 9 52 7 times the cost of catboost which fully illustrated the advantages of the newly developed tree based algorithm catboost in parallel computing hassan et al 2017 have also indicated that the computational costs of traditional svm models were nearly 40 times the computational costs of tree based ensemble methods for predicting global solar radiation on an hourly basis tree based algorithms are generally capable of building decision trees in parallel which would more or less reduces the computing time however the paralleling efficiency of the rf algorithm in this study was far less than that of the catboost algorithm and therefore the rf models consumed much more time than the catboost models overall among the three algorithms in this study the catboost was fastest while the svm was slowest in terms of data processing the memory usage of rf models increased proportionally to the increasing size of input datasets which exceeded 1 gb at level 2 and reached 4 gb at level 5 fig 7b with the datasets of level 1 and 2 svm had the minimum memory usage less than 100 mb among the three algorithms and meanwhile catboost used slightly more memory than svm however when the size of datasets was greater than or equal to level 3 the memory footprint of catboost was smaller than svm therefore as the latest developed tree based model catboost has obvious optimization in time and space complexity especially when the size of input dataset is large 4 conclusions this study presented the application of a newly developed machine learning algorithm catboost for accurately estimating daily et0 in the subtropical humid zones of china the catboost algorithm along with two other commonly used machine learning algorithms rf and svm were trained and tested with meteorological data including rs tmax tmin hr and u from five weather stations during 2001 2015 in south china the results showed that all the three algorithms could achieve satisfactory accuracy for et0 estimation in subtropical china using rs tmax and tmin or u hr tmax and tmin as inputs under the circumstances of lacking complete meteorological parameters the increases in testing rmse and mape over training rmse and mape showed positive correlations with the number of input parameters to the machine learning models for the local models among the three algorithms svm offered the best predicting accuracy and stability with incomplete combinations of meteorological parameters as inputs while catboost performed best with the complete combination of parameters patterns of the generalized models were almost the same as those of the local models but the former ones showed less than 10 decreases in rmse or mape in comparison with the latter ones in addition the computing time and memory usage for data processing of catboost were much less than those of rf and svm overall as a tree based algorithm catboost made significant improvements in accuracy stability and computational cost when compared to rf therefore the catboost algorithm has a very high potential for et0 estimation in humid regions of china and even possibly in other parts of the world with similar humid climates conflict of interest statement we declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work there is no professional or other personal interest of any nature or kind in any product service and or company that could be construed as influencing the position presented in or the review of the manuscript entitled acknowledgements this study was jointly supported by the foundation of key laboratory of crop water use and regulation china firi2018 01 the national natural science foundation of china no 51709143 61703199 51790533 and the natural science foundation of jiangxi province china 20181bab206045 special thanks to the national meteorological information center of china meteorological administration for offering the meteorological data appendix a supplementary material supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 04 085 appendix a supplementary material the following are the supplementary data to this article supplementary data 1 
