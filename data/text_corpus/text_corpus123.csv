index,text
615,this study investigates the instability of pure newtonian fluid flow between two parallel plates where the bottom one coated with various porous media with permeability k and porosity ε we have applied normal modes to perturb the coupled flow system specifically the effects of some dimensionless parameters such as depth ratio permeability parameter and the porosity of the porous medium on the instability have been examined we found these parameters play a critical role in the instability depending on these parameters instability is initiated and dominated either by the fluid or by the porous region in particular we found that there are ranges of the depth ratio and the permeability parameter for each value of the porous media porosity that affect the instability of this coupled flow we have also determined a new parameter which specifies the potential dominance and the stability margin of each mode to validate our calculations we have also compared our results with the orr sommerfeld equation in addition we have examined three special extreme conditions to study the coupled flow system in more detail keywords coupled fluid porous flow instability brinkman equation stability margin 1 introduction the fluid flow over and inside a porous media has been received considerable attention during few decades because of its wide range of applications in industry from biology to geophysical problems for example battiato et al 2010 studied the carbon nanotube forests cnts by investigating the bending profile of cylindrical arrays weinbaum 1998 presented three different fundamental cellular level transport models by considering the thin layer of specialized matrix that cells produce at the surface of their plasmalemma membranes ghisalberti and nepf 2009 had experimentally examined aquatic flow over a submerged vegetation canopy gayev and hunt 2007 had investigated rivers flooding over plains with vegetation although thermal convection instability in the fluid porous system has been studied extensively nield 1977 chen and chen 1988 straughan 2002 thiele et al 2009 deepika and narayana 2015 deepika et al 2017 the hydrodynamic instability problem of such a system has been appreciated only in recent years to the best of the author s knowledge chang et al 2006 were the first to study the poiseuille flow instability problem in a fluid overlying a porous medium they employed darcy s law and the navier stokes equation as the governing equations in the fluid porous system i e the so called two domain approach they used the boundary conditions proposed by beavers and joseph 1967 which was known to be accurate in the studies of slow flow of a fluid passing over porous spheres they also used the orr sommerfeld orszag 1971 equation to formulate the eigenvalue problem and solved it based on the chebyshev tau method dongarra et al 1996 after chang s pioneering work many researchers studied the instability problem in a fluid porous geometry considering different flow conditions methodology and boundary conditions for example samanta 2017 applied chebyshev collocation method dolapçi 2004 to solve the orr sommerfeld type eigenvalue problem and used the no slip upper wall condition into a slip boundary condition deepu et al 2015 presented the linear stability analysis of the poiseuille flow in a fluid overlying a porous material with anisotropic and inhomogeneous permeability they found that the depth ratio the darcy number the anisotropy parameter and the inhomogeneity factor have impact on the instability of the system in this study we choose brinkman equation brinkman 1949 as an alternative equation for darcy s law in the two domain approach it is widely agreed that the brinkman equation is accurate for highly porosity materials battiato et al 2010 in addition there is a higher order term i e the viscous term in the brinkman equation which enables to couple the porous medium and free fluid region in a straightforward manner therefore it is reasonable to assume the continuity of stress and velocity as a boundary condition along the interface that also significantly reduce the analytical difficulties in the instability analysis this method has been applied by other researchers in the instability analysis liu et al 2008 liu and liu 2009 hill and straughan 2008 hill and straughan 2009 silin et al 2011 goyal et al 2013 straughan and harfash 2013 in particular the focus of this research is on examining the plane poiseuille flow instability problem more broadly and quantifying the impact of the porous layer thickness and properties on the fluid porous instability different from previous studies the present work considers a model to describe a wider parameter space which helps us to find different behaviors of the system instability unlike the study by liu et al 2008 the convective term i e u u in the momentum equation for the porous region has been considered in our analysis the laplacian term of our governing equations has also been defined based on our experimental measurements wu and mirbod 2018 which is different from the corresponding one in the previous studies thiele et al 2009 hill and straughan 2008 goyal et al 2013 lyubimova et al 2016 see section 2 1 for more details furthermore we determine a new dimensionless parameter which specifies the potential dominance of the coupled system instability depending on the porosity of the porous media we also analyze the behavior of the special extreme cases on the instability to the best of our knowledge none of these analyses have been attempted before to validate our code we used orr sommerfeld orszag 1971 and also compared our study with the previous results liu et al 2008 hill and straughan 2008 we expect these results will be a step forward in investigating the hydrodynamic instability problem of newtonian fluids overlying porous surfaces this model reveals the relation between the gap of the two parallel plates the physical properties and the thickness of a permeable layer to the fluid passing over it which is a first step towards further analyzing the instability of complex fluids flows over porous surfaces 2 problem formulation we consider a fully viscous incompressible flow driven by a pressure gradient dp dx 0 between two parallel plates as shown in fig 1 the bottom plate is covered with a rigid homogenous isotropic porous medium with permeability k and porosity ε the gap between the two plates is 2l 2h where 2h is the thickness of a porous medium and 2l is the height of the free flow region it should be noted that the thickness 2l and 2h are defined for the simplicity of the numerical calculations which will be explained in detail in section 3 the coordinate axis x and y are directed along and normal to the flow respectively and the origin is located at the fluid porous interface 2 1 governing equations the flow in the free flow region using the continuity and the incompressible navier stokes equations can be defined as 2 1 u 0 2 2 u t u u 1 ρ p μ ρ 2 u where u u v represents the velocity throughout the channel p is the pressure μ is the dynamic viscosity and ρ is the density of the fluid respectively in the porous medium region the continuity and momentum brinkman s equations can be given by 2 3 u m 0 2 4 1 ε u m t 1 ε 2 u m u m 1 ρ p m μ e ρ ε 2 u m μ ρ k u m the subscript m represents the corresponding parameters in the porous medium here μ e is the effective viscosity which takes into account the slip at the interface between the porous and fluid together with the porosity it should be noted that in our governing equations the laplacian term considered as μ e ρ ε 2 u m this is different from μ ρ ε 2 u m which has been used by the previous studies thiele et al 2009 hill and straughan 2008 goyal et al 2013 lyubimova et al 2016 where they considered the impact of only porosity on the viscosity ratio in order to generalize the governing equations in the present work we considered the laplacian term as μ e ρ ε 2 u m because we experimentally found that the ratio between the effective viscosity and the fluid viscosity depends on both porosity and the complex structures of porous media this has been discussed in detail by wu and mirbod 2018 although the effective viscosity has long been discussed koplik et al 1983 the only widely agreed conclusion of the previous studies is that for the high porosity material effective viscosity is the same as the dynamic viscosity of the fluid i e as ε 1 μ e μ the boundary conditions in this study by considering the continuity of velocity profiles and the stress tensors at the interface as well as no slip boundary conditions on the two plates can be expressed as at y 2l 2 5 u 0 v 0 at y 2h 2 6 u m 0 v m 0 at y 0 2 7 u u m v v m 2 8 μ u y v x μ e ε u m y v m x 2 9 p 2 μ v y p m 2 μ e ε v m y 2 2 the basic steady state flow to study the relative magnitudes of velocity we normalized the length in both fluid and porous regions by l and h respectively also a constant pressure gradient is considered in the x direction therefore the governing equations and boundary conditions in the basic flow can be expressed as 2 10 2 u c 2 11 m ε 2 u m α 2 u m c d 2 2 12 m μ e μ α 2 h 2 k d l h at y 2 2 13 u 0 at y 2 2 14 u m 0 at y 0 2 15 u u m 2 16 u y d m ε u m y where u u 0 and u m u m 0 are the velocity of the basic state flow here c l 2 μ p x is a constant value based on the pressure gradient in the basic state the property of porous medium and the system geometry m is the viscosity ratio α is the permeability parameter and d is the length ratio fig 2 shows the results of the normalized basic flow velocity profiles for different α d values and specific values of m 1 and ε 0 8 the velocity profiles in the free fluid region and the porous region are both normalized by v the maximum velocity u y in the free fluid region in addition the length in the free fluid region and in the porous region has been normalized by l and h respectively it can be seen that both the length ratio d and the permeability parameter α have a strong impact on the basic flow velocity profiles fig 2 a shows that for α 50 as the length ratio d increases the slip velocity at the fluid porous interface decreases which consequently causes the boundary condition at the interface goes close to no slip condition in other words the interface behaves as an impermeable surface this is because as the length ratio increases the effect of porous medium on the system becomes less significant the same procedure can be seen in fig 2 b when the permeability parameter α increases the core velocity profile is parabolic except near the interface where the velocity drops to very small values in other words for large values of α when the permeability of the porous material becomes small the velocity profile inside the porous layer shows the characteristic of plug flow except near the free fluid porous medium interface fig 2 b also shows that for d 0 1 as α increases the porous medium approaches to an impermeable solid surface thus the no slip condition will be achieved eventually similar results have been reported in our previous work mirbod et al 2017 2 3 linear stability analysis to study the linear stability of this problem we derived the perturbation equations by introducing the perturbed forms u u u v v v p p p u m u m u m v m v m v m p m p m p m we then non dimensionalized the equations with the length velocity time and pressure in the both fluid and porous regions these parameters are l v l v μv l in the fluid section and h vm the maximum of u m y h vm μvm h in the porous region the dimensionless and linearized form of equations are obtained as 2 17 u x v y 0 2 18 r e u t u u x v d u d y p x 2 u 2 19 r e v t u v x p y 2 v 2 20 u m x m v m y m 0 2 21 r e m 1 ε u m t m 1 ε 2 u m u m x m v m d u m d y m p m x m m ε 2 u m α 2 u m 2 22 r e m 1 ε v m t m 1 ε 2 u m v m x m p m y m m ε 2 v m α 2 v m where re ρvl μ is the reynolds number in the fluid region and rem ρvmh μ is the reynolds number inside the porous layer u u v and u m u m v m are the normalized basic flow velocity profile in the fluid and porous region respectively it should be noted that to explicitly separating the free flow and the porous region we denoted the time and the coordinates in the porous region as tm xm and ym then the normal modes can be introduced as 2 23 u v p u y v y p y e x p i k x λ t 2 24 u m v m p m u m y m v m y m p m y m e x p i k m x m λ m t m where k and km are the dimensionless wavenumbers for the two regions λ and λ m are complex parameters whose real part is the propagation velocity of the perturbation and their imaginary part determines the amplification or attenuation of the perturbation depending on their sign silin et al 2011 in addition k dkm r e m v m v d r e and λre dλ m rem therefore by considering d dy d 2 d 2 dy 2 dm d dym dm 2 d 2 dym 2 u du dy u m d u m d y m one can define the normal modes equations and boundary conditions as 2 25 i k u d v 0 2 26 r e i k λ u u i k u v u i k p d 2 k 2 u 2 27 r e i k λ v u i k v d p d 2 k 2 v 2 28 i k u m d m v m 0 2 29 r e m i k m ε λ m u m 1 ε 2 u m i k m u m 1 ε 2 v m u m i k m p m m ε d m 2 k m 2 u m α 2 u m 2 30 r e m i k m ε λ m v m 1 ε 2 u m i k m v m d m p m m ε d m 2 k m 2 v m α 2 v m at y 2 2 31 u 0 v 0 at y 2 2 32 u m 0 v m 0 at y 0 2 33 u u m v v m 2 34 d u i k v d m ε v m v d m u m i k v m 2 35 p 2 d v d v m v p m 2 d v m v m ε d m v m finally eqs 2 21 2 26 together with the corresponding boundary conditions i e eqs 2 27 2 31 form an eighth order eigenvalue problem in the next section we briefly explain the methodology to solve this eigenvalue problem 3 numerical method because the properties of chebyshev polynomials are very suitable for solving this type of boundary condition problem we selected the chebyshev collocation method to solve this eighth order eigenvalue problem the detailed procedures can be found in dolapçi s work dolapçi 2004 we first transferred the domain of the fluid 0 2 and the porous region 2 0 to the chebyshev domain 1 1 by introducing y y 1 and y m y m 1 then the perturbation amplitudes i e u y v y p y u m y m v m y m p m y m were approximated by using the chebyshev expansions as 3 1 u y n 0 n a n t n y v y n 0 n b n t n y p y n 0 n c n t n y 3 2 u m y m n 0 n a n m t n y m v m y m n 0 n b n m t n y m p m y m n 0 n c n m t n y m where tn y is the chebyshev polynomials defined as 3 3 t n y cos n c o s 1 y 1 y 1 3 4 y j cos n j n π therefore by arranging eqs 2 21 2 26 and the corresponding boundary conditions eqs 2 27 2 31 and using chebyshev expansions one can obtain the generalized eigenvalue problem as 3 5 a x λ b x this is a 6 n 1 6 n 1 matrix eigenvalue problem to determine the number n of chebyshev polynomials we then checked the convergence of the numerical results table 1 shows the leading eigenvalue of two different cases with n 50 60 70 80 case 1 corresponds to ε 0 5 d 0 2 α 10 re 10 000 k 1 and case 2 corresponds to ε 0 9 d 0 5 α 100 re 10 000 k 1 moreover λ r and λ i represent the real part and imaginary part of the leading eigenvalue respectively different values of n have been tested using our code and we found n 60 is adequate to obtain the accurate results 4 results and discussion 4 1 instability of the classical plane poiseuille flow to verify the accuracy of our calculation procedure and the code we first conducted the calculations of the classical plane poiseuille flow in a channel where a pure newtonian fluid flows inside it and compared our results with the work by orszag orszag 1971 this is a one layer system which is governed by navier stokes equation only orszag used chebyshev tau method to solve the orr sommerfeld equations we found the most unstable mode occurs at re 10 000 and k 1 is λ 0 23752649 0 00373967i which is the same as orszag s results fig 3 represents the neutral curve λ i 0 in the re k plane for the classical plane poiseuille flow in a channel the critical reynolds number was found to be 5773 which appears at k 1 02 4 2 the effects of depth ratio d fig 4 shows the neutral curves for various depth ratio d when m 1 α 50 and ε 0 8 depending on the values of d α and ε the curves are composed of two intersecting curves and have a bi modal structure in fig 4 a where the range of d varies from 0 1 to 0 5 the short wave branch i e the right side branch or the so called fluid branch chang et al 2006 liu et al 2008 hill and straughan 2009 dominates the instability of the system in all neutral curves i e the instability primarily occurs in the fluid domain for d 0 5 the long wave branch chang et al 2006 liu et al 2008 hill and straughan 2009 i e the porous branch almost disappears as the depth ratio decreases from 0 5 to 0 1 the effect of porous layer on the instability increases fig 4 a reveals that by decreasing the depth ratio d the flow becomes unstable on the contrary as can be seen in fig 4 b by further decreasing the depth ratio from 0 1 to 0 05 the flow becomes stable and the critical reynolds number increases in addition the effect of the fluid region on the instability decreases this inverse behavior of the neutral curves in two different length ratio ranges i e d 0 1 0 5 and d 0 05 0 1 have never been discussed in the previous literatures we also found that the critical depth ratio for m 1 α 50 and ε 0 8 is 0 0632 for d 0 0632 the porous mode dominates the instability and when the depth ratio reaches to 0 05 the fluid branch nearly disappears for d 0 0632 the fluid mode always dominates the instability in general these results show that the depth ratio is a critical parameter and affects the instability of the porous fluid system significantly in order to better understand the two critical modes in a neutral curve i e the points where the minimal reynolds numbers occurs in the fluid or porous branch following the other researchers chang et al 2006 liu et al 2008 hill and straughan 2009 we also present the normalized vertical velocity amplitude v y at the two minima in the bi modal structure of the neutral curve where d 0 1 m 1 α 50 ε 0 8 fig 5 a shows the eigenfunction i e the amplitude of vertical velocity of the most unstable mode in the porous branch the graph shows a flow reversal at the interface which indicates the strong influence of instability from the porous medium layer near the fluid layer this is because the limited space within the fluid layer in fig 5 b the flow reversal at the interface disappears this phenomenon proofs the fluid layer dominants the instability in the fluid porous system a similar scenario has been observed by other researchers in chang et al 2006 liu et al 2008 hill and straughan 2009 4 3 the impact of the permeability parameter α we then analyzed the impact of the permeability parameter α on the instability fig 6 shows the neutral curves at different permeability parameters α for m 1 d 0 1 and ε 0 8 first we considered the permeability parameter α less than 50 as can be seen in fig 6 a for α 25 the instability initiates in the porous region by increasing α from 25 to 50 the system becomes unstable and the impact of the porous layer reduces this is because larger α values represent the porous layer has lower permeability which lead to higher flow resistance in the porous layer it then appears that the instability in the fluid porous system especially at the fluid porous interface more likely occurs in addition the critical reynolds number decreases and switches from the porous region to the fluid region at α 31 6 the critical reynolds number in the fluid branch and the porous branch becomes equal we then considered the permeability parameter greater than 50 as shown in fig 6 b by increasing α from 50 to 150 an opposite behavior of the neutral curves was found where the critical reynolds number increases this represents the porous fluid system becomes more stable in addition the bi modal structure can be found in all these curves in fig 6 b where the short wave branch i e the fluid branch is always lower than the long wave branch i e the porous branch which indicates the instability is dominated by the fluid branch this is because as α increases the effect of porous layer on the instability becomes insignificant 4 4 the impact of the parameter γ by comparing the effect of the depth ratio and the permeability parameter on the instability system we found there is always a neutral curve with a critical value of α or d which has identical minimal reynolds number in both fluid branch and the porous branch this phenomenon has not been addressed before we then defined a new parameter γ which relates α and d as 4 1 γ α d l k fig 7 shows the neutral curves for γ 2 5 3 16 4 at m 1 ε 0 8 for each γ we have chosen two groups of α and d values as can be seen in fig 7 the neutral curves for two groups of α and d values are overlapped perfectly this indicates that γ is independent from the combination of the values of α and d in other words once we determine a γ value we can choose different combination of α and d values without changing the structure of the neutral curve thus instead of comparing α and d separately it is more convenient to distinguish the instability of the system by comparing only the γ value it should be noted that the parameter γ is not valid for the extreme conditions when α α 0 d and d 0 as can be seen in fig 7 γ γ c 3 16 is a critical value where the minimum value of the reynolds number in both the fluid and the porous branch are the same thus specifically for m 1 ε 0 8 when γ is less than γ c where the instability is dominated by the porous region the system becomes more stable and when γ is greater than γ c the instability is dominated by the fluid branch the values of the critical reynolds number decays and the system becomes unstable 4 5 the effect of the porosity ε to further examine the instability of the coupled flow in terms of the properties of the porous media the neutral curves for various porosity are shown in fig 8 for m 1 d 0 1 α 0 8 we found that the bi modal structure appears in all the neutral curves when the porosity ε increases the porous branch and the fluid branch both become more unstable especially at the region close to the intersection it should be noted that the analytical model used in studying the flow inside porous medium is brinkman s equation and in low porosity environments the laplacian term in the brinkman s equation is not valid hill and straughan 2009 therefore we could only consider the conditions for ε 0 3 generalization to the smaller porosity values is the subject of our current investigations we then investigated the effect of the porosity on the instability of the system by defining the critical parameter γ fig 9 shows the neutral curves at the critical parameter γ c for ε 0 3 ε 0 6 and ε 0 99 the critical parameter γ c for each porosity value obtained by a combination of two different values of α and d this confirms that the neutral curve presented in fig 7 can be employed for various porosity of the porous media as long as ε 0 3 also for m 1 the critical parameter γ c 3 1 0 1 which indicates that there is a weak effect of the porosity on the instability we would like to emphasize that the parameter γ is only valid in the normal condition we will discuss the special conditions in the next section 4 6 special cases to examine the special cases of the coupled flow system because of the numerical limitations we only consider large or small values of α and d in our analysis fig 10 shows the neutral curve and the corresponding basic flow velocity profile for d 104 α 10 4 m 1 ε 1 the resulting neutral curve shown in fig 10 a is the same as the classical plane poiseuille flow in a smooth channel see fig 3 we then assume d 104 as a constant value and vary α m and ε where no variation in the results was observed see fig 10 these results state that for very large values of the length ratio the effect of the porous medium on the instability is negligible this effect can also be seen in fig 10 b which shows the normalized basic flow velocity profile for d 104 α 10 4 m 1 ε 1 for large values of the depth ratio d the normalized velocity inside the porous layer reaches to very small values that is a characteristic of the plug flow inside the porous layer this is because the effect of the porous layer becomes insignificant resulting in the instability characterization of the classical plane poiseuille flow when the height of the channel is 2l we then considered the condition with α 104 ε 10 4 d 1 that is the case with an extremely low permeability and porosity of the porous medium we found the neutral curve and the corresponding basic flow velocity profile are the same as fig 10 this indicates that for very large permeability parameter the effect of the porous medium on the instability of the system is also negligible in other words a very large permeability parameter represents an impermeable surface in the channel furthermore the normalized basic flow velocity inside the porous layer is almost equal to zero as a result these graphs demonstrate that the instability of the system is determined by the free fluid layer when either the length ratio or the permeability parameter are extremely high fig 11 a shows the neutral curve for α 10 4 ε 1 d 10 4 m 1 this is the case with a very high permeability and porosity porous medium layer and an extremely low length ratio if we consider the neutral curve in the rem km plane using k m k d and r e m v m v d r e we can find that the critical reynolds number the corresponding wave number and the neutral curve in the porous medium region are the same as classical plane poiseuille flow in a no porous channel fig 11 b shows the corresponding basic flow velocity profile which is normalized by vm i e the maximum velocity inside the porous medium this figure clearly shows that the normalized velocity profile inside the free flow region is zero which confirms that the free flow region has no impact on instability of the system 5 conclusions in the present study we examined the linear stability problem between two parallel plates where the bottom one coated with various porous media with permeability k and porosity ε this work considers a model to describe a wider parameter space which helps us to find different behaviors of the system instability the brinkman s and navier stokes equations were coupled to describe the flow inside the porous medium and in the free fluid region respectively the laplacian term of our governing equations has also been defined based on our experimental measurements wu and mirbod 2018 which is different from the corresponding one in the previous studies thiele et al 2009 hill and straughan 2008 goyal et al 2013 lyubimova et al 2016 we first characterized the effect of the depth ratio d on the system in particular we found that there are ranges of the depth ratio and the permeability parameter that will affect the instability of this coupled flow we found that for d 0 1 α 50 m 1 and ε 80 as the length ratio increases the effect of the porous medium on the instability decreases also the entire system becomes more stable results in an increasing the critical reynolds number for d 0 1 as the depth ratio decreases the effect of the fluid branch on the instability decreases it appears that there is a critical value of d 0 0632 at which the instability is switched from the fluid mode to the porous mode we then investigated the effect of the permeability parameter α when m 1 and ε 80 it was found that for α 50 the system becomes more stable as α decreases when α becomes less than 31 6 the porous branch dominates the system for α 50 as α increases the system becomes stable and the fluid mode dominates the instability to describe the instability behavior of the coupled flow system more broadly we proposed a new parameter γ as the product of the length ratio d and the permeability parameter α we found for each γ value there is a neutral curve which is independent of the combination of α and d values in addition for each specific porosity value ε there exists a critical parameter γ c the neutral curve and the corresponding critical value could help to characterize the instability of the system and whether the system is dominated by the fluid branch or the porous branch we further studied the effect of porosity on the instability we compared the neutral curves at porosities ε 0 3 0 5 0 8 0 99 and observed that the system becomes slightly unstable when the porosity increases we then determined the critical γ values for ε 0 3 0 6 0 99 and we found that the critical γ always equals to 3 1 0 1 which indicates the porosity has little effect on the instability we have also examined and compared three special cases including 1 the extremely large d values 2 the extremely large α values and 3 the extremely small α d values for the former two cases the effect of the porous medium on the system is negligible and the system behaves the same as the classical plane poiseuille flow in a channel with impermeable surfaces for the last case the free fluid region has no effect on the fluid porous system and the instability behavior inside the porous medium is also the same as the classical plane poiseuille flow in a channel with impermeable surfaces because of the extremely high permeability and porosity condition these results agree well with the real physical phenomena these results will provide valuable insights into the analysis of the pure newtonian fluid overlying a porous media that are important in a wide range of industrial applications ranging from biological devices to geophysical problems it is expected that the results obtained in this study will provide a step forward in understanding and examining the complex fluids motion over permeable surfaces and analyzing the instability of such system the precise experiments to investigate the instability more deeply and validate our analytical model are subjects of our current investigations acknowledgement this research was supported by the army research office aro under award no w911nf 18 1 0356 
615,this study investigates the instability of pure newtonian fluid flow between two parallel plates where the bottom one coated with various porous media with permeability k and porosity ε we have applied normal modes to perturb the coupled flow system specifically the effects of some dimensionless parameters such as depth ratio permeability parameter and the porosity of the porous medium on the instability have been examined we found these parameters play a critical role in the instability depending on these parameters instability is initiated and dominated either by the fluid or by the porous region in particular we found that there are ranges of the depth ratio and the permeability parameter for each value of the porous media porosity that affect the instability of this coupled flow we have also determined a new parameter which specifies the potential dominance and the stability margin of each mode to validate our calculations we have also compared our results with the orr sommerfeld equation in addition we have examined three special extreme conditions to study the coupled flow system in more detail keywords coupled fluid porous flow instability brinkman equation stability margin 1 introduction the fluid flow over and inside a porous media has been received considerable attention during few decades because of its wide range of applications in industry from biology to geophysical problems for example battiato et al 2010 studied the carbon nanotube forests cnts by investigating the bending profile of cylindrical arrays weinbaum 1998 presented three different fundamental cellular level transport models by considering the thin layer of specialized matrix that cells produce at the surface of their plasmalemma membranes ghisalberti and nepf 2009 had experimentally examined aquatic flow over a submerged vegetation canopy gayev and hunt 2007 had investigated rivers flooding over plains with vegetation although thermal convection instability in the fluid porous system has been studied extensively nield 1977 chen and chen 1988 straughan 2002 thiele et al 2009 deepika and narayana 2015 deepika et al 2017 the hydrodynamic instability problem of such a system has been appreciated only in recent years to the best of the author s knowledge chang et al 2006 were the first to study the poiseuille flow instability problem in a fluid overlying a porous medium they employed darcy s law and the navier stokes equation as the governing equations in the fluid porous system i e the so called two domain approach they used the boundary conditions proposed by beavers and joseph 1967 which was known to be accurate in the studies of slow flow of a fluid passing over porous spheres they also used the orr sommerfeld orszag 1971 equation to formulate the eigenvalue problem and solved it based on the chebyshev tau method dongarra et al 1996 after chang s pioneering work many researchers studied the instability problem in a fluid porous geometry considering different flow conditions methodology and boundary conditions for example samanta 2017 applied chebyshev collocation method dolapçi 2004 to solve the orr sommerfeld type eigenvalue problem and used the no slip upper wall condition into a slip boundary condition deepu et al 2015 presented the linear stability analysis of the poiseuille flow in a fluid overlying a porous material with anisotropic and inhomogeneous permeability they found that the depth ratio the darcy number the anisotropy parameter and the inhomogeneity factor have impact on the instability of the system in this study we choose brinkman equation brinkman 1949 as an alternative equation for darcy s law in the two domain approach it is widely agreed that the brinkman equation is accurate for highly porosity materials battiato et al 2010 in addition there is a higher order term i e the viscous term in the brinkman equation which enables to couple the porous medium and free fluid region in a straightforward manner therefore it is reasonable to assume the continuity of stress and velocity as a boundary condition along the interface that also significantly reduce the analytical difficulties in the instability analysis this method has been applied by other researchers in the instability analysis liu et al 2008 liu and liu 2009 hill and straughan 2008 hill and straughan 2009 silin et al 2011 goyal et al 2013 straughan and harfash 2013 in particular the focus of this research is on examining the plane poiseuille flow instability problem more broadly and quantifying the impact of the porous layer thickness and properties on the fluid porous instability different from previous studies the present work considers a model to describe a wider parameter space which helps us to find different behaviors of the system instability unlike the study by liu et al 2008 the convective term i e u u in the momentum equation for the porous region has been considered in our analysis the laplacian term of our governing equations has also been defined based on our experimental measurements wu and mirbod 2018 which is different from the corresponding one in the previous studies thiele et al 2009 hill and straughan 2008 goyal et al 2013 lyubimova et al 2016 see section 2 1 for more details furthermore we determine a new dimensionless parameter which specifies the potential dominance of the coupled system instability depending on the porosity of the porous media we also analyze the behavior of the special extreme cases on the instability to the best of our knowledge none of these analyses have been attempted before to validate our code we used orr sommerfeld orszag 1971 and also compared our study with the previous results liu et al 2008 hill and straughan 2008 we expect these results will be a step forward in investigating the hydrodynamic instability problem of newtonian fluids overlying porous surfaces this model reveals the relation between the gap of the two parallel plates the physical properties and the thickness of a permeable layer to the fluid passing over it which is a first step towards further analyzing the instability of complex fluids flows over porous surfaces 2 problem formulation we consider a fully viscous incompressible flow driven by a pressure gradient dp dx 0 between two parallel plates as shown in fig 1 the bottom plate is covered with a rigid homogenous isotropic porous medium with permeability k and porosity ε the gap between the two plates is 2l 2h where 2h is the thickness of a porous medium and 2l is the height of the free flow region it should be noted that the thickness 2l and 2h are defined for the simplicity of the numerical calculations which will be explained in detail in section 3 the coordinate axis x and y are directed along and normal to the flow respectively and the origin is located at the fluid porous interface 2 1 governing equations the flow in the free flow region using the continuity and the incompressible navier stokes equations can be defined as 2 1 u 0 2 2 u t u u 1 ρ p μ ρ 2 u where u u v represents the velocity throughout the channel p is the pressure μ is the dynamic viscosity and ρ is the density of the fluid respectively in the porous medium region the continuity and momentum brinkman s equations can be given by 2 3 u m 0 2 4 1 ε u m t 1 ε 2 u m u m 1 ρ p m μ e ρ ε 2 u m μ ρ k u m the subscript m represents the corresponding parameters in the porous medium here μ e is the effective viscosity which takes into account the slip at the interface between the porous and fluid together with the porosity it should be noted that in our governing equations the laplacian term considered as μ e ρ ε 2 u m this is different from μ ρ ε 2 u m which has been used by the previous studies thiele et al 2009 hill and straughan 2008 goyal et al 2013 lyubimova et al 2016 where they considered the impact of only porosity on the viscosity ratio in order to generalize the governing equations in the present work we considered the laplacian term as μ e ρ ε 2 u m because we experimentally found that the ratio between the effective viscosity and the fluid viscosity depends on both porosity and the complex structures of porous media this has been discussed in detail by wu and mirbod 2018 although the effective viscosity has long been discussed koplik et al 1983 the only widely agreed conclusion of the previous studies is that for the high porosity material effective viscosity is the same as the dynamic viscosity of the fluid i e as ε 1 μ e μ the boundary conditions in this study by considering the continuity of velocity profiles and the stress tensors at the interface as well as no slip boundary conditions on the two plates can be expressed as at y 2l 2 5 u 0 v 0 at y 2h 2 6 u m 0 v m 0 at y 0 2 7 u u m v v m 2 8 μ u y v x μ e ε u m y v m x 2 9 p 2 μ v y p m 2 μ e ε v m y 2 2 the basic steady state flow to study the relative magnitudes of velocity we normalized the length in both fluid and porous regions by l and h respectively also a constant pressure gradient is considered in the x direction therefore the governing equations and boundary conditions in the basic flow can be expressed as 2 10 2 u c 2 11 m ε 2 u m α 2 u m c d 2 2 12 m μ e μ α 2 h 2 k d l h at y 2 2 13 u 0 at y 2 2 14 u m 0 at y 0 2 15 u u m 2 16 u y d m ε u m y where u u 0 and u m u m 0 are the velocity of the basic state flow here c l 2 μ p x is a constant value based on the pressure gradient in the basic state the property of porous medium and the system geometry m is the viscosity ratio α is the permeability parameter and d is the length ratio fig 2 shows the results of the normalized basic flow velocity profiles for different α d values and specific values of m 1 and ε 0 8 the velocity profiles in the free fluid region and the porous region are both normalized by v the maximum velocity u y in the free fluid region in addition the length in the free fluid region and in the porous region has been normalized by l and h respectively it can be seen that both the length ratio d and the permeability parameter α have a strong impact on the basic flow velocity profiles fig 2 a shows that for α 50 as the length ratio d increases the slip velocity at the fluid porous interface decreases which consequently causes the boundary condition at the interface goes close to no slip condition in other words the interface behaves as an impermeable surface this is because as the length ratio increases the effect of porous medium on the system becomes less significant the same procedure can be seen in fig 2 b when the permeability parameter α increases the core velocity profile is parabolic except near the interface where the velocity drops to very small values in other words for large values of α when the permeability of the porous material becomes small the velocity profile inside the porous layer shows the characteristic of plug flow except near the free fluid porous medium interface fig 2 b also shows that for d 0 1 as α increases the porous medium approaches to an impermeable solid surface thus the no slip condition will be achieved eventually similar results have been reported in our previous work mirbod et al 2017 2 3 linear stability analysis to study the linear stability of this problem we derived the perturbation equations by introducing the perturbed forms u u u v v v p p p u m u m u m v m v m v m p m p m p m we then non dimensionalized the equations with the length velocity time and pressure in the both fluid and porous regions these parameters are l v l v μv l in the fluid section and h vm the maximum of u m y h vm μvm h in the porous region the dimensionless and linearized form of equations are obtained as 2 17 u x v y 0 2 18 r e u t u u x v d u d y p x 2 u 2 19 r e v t u v x p y 2 v 2 20 u m x m v m y m 0 2 21 r e m 1 ε u m t m 1 ε 2 u m u m x m v m d u m d y m p m x m m ε 2 u m α 2 u m 2 22 r e m 1 ε v m t m 1 ε 2 u m v m x m p m y m m ε 2 v m α 2 v m where re ρvl μ is the reynolds number in the fluid region and rem ρvmh μ is the reynolds number inside the porous layer u u v and u m u m v m are the normalized basic flow velocity profile in the fluid and porous region respectively it should be noted that to explicitly separating the free flow and the porous region we denoted the time and the coordinates in the porous region as tm xm and ym then the normal modes can be introduced as 2 23 u v p u y v y p y e x p i k x λ t 2 24 u m v m p m u m y m v m y m p m y m e x p i k m x m λ m t m where k and km are the dimensionless wavenumbers for the two regions λ and λ m are complex parameters whose real part is the propagation velocity of the perturbation and their imaginary part determines the amplification or attenuation of the perturbation depending on their sign silin et al 2011 in addition k dkm r e m v m v d r e and λre dλ m rem therefore by considering d dy d 2 d 2 dy 2 dm d dym dm 2 d 2 dym 2 u du dy u m d u m d y m one can define the normal modes equations and boundary conditions as 2 25 i k u d v 0 2 26 r e i k λ u u i k u v u i k p d 2 k 2 u 2 27 r e i k λ v u i k v d p d 2 k 2 v 2 28 i k u m d m v m 0 2 29 r e m i k m ε λ m u m 1 ε 2 u m i k m u m 1 ε 2 v m u m i k m p m m ε d m 2 k m 2 u m α 2 u m 2 30 r e m i k m ε λ m v m 1 ε 2 u m i k m v m d m p m m ε d m 2 k m 2 v m α 2 v m at y 2 2 31 u 0 v 0 at y 2 2 32 u m 0 v m 0 at y 0 2 33 u u m v v m 2 34 d u i k v d m ε v m v d m u m i k v m 2 35 p 2 d v d v m v p m 2 d v m v m ε d m v m finally eqs 2 21 2 26 together with the corresponding boundary conditions i e eqs 2 27 2 31 form an eighth order eigenvalue problem in the next section we briefly explain the methodology to solve this eigenvalue problem 3 numerical method because the properties of chebyshev polynomials are very suitable for solving this type of boundary condition problem we selected the chebyshev collocation method to solve this eighth order eigenvalue problem the detailed procedures can be found in dolapçi s work dolapçi 2004 we first transferred the domain of the fluid 0 2 and the porous region 2 0 to the chebyshev domain 1 1 by introducing y y 1 and y m y m 1 then the perturbation amplitudes i e u y v y p y u m y m v m y m p m y m were approximated by using the chebyshev expansions as 3 1 u y n 0 n a n t n y v y n 0 n b n t n y p y n 0 n c n t n y 3 2 u m y m n 0 n a n m t n y m v m y m n 0 n b n m t n y m p m y m n 0 n c n m t n y m where tn y is the chebyshev polynomials defined as 3 3 t n y cos n c o s 1 y 1 y 1 3 4 y j cos n j n π therefore by arranging eqs 2 21 2 26 and the corresponding boundary conditions eqs 2 27 2 31 and using chebyshev expansions one can obtain the generalized eigenvalue problem as 3 5 a x λ b x this is a 6 n 1 6 n 1 matrix eigenvalue problem to determine the number n of chebyshev polynomials we then checked the convergence of the numerical results table 1 shows the leading eigenvalue of two different cases with n 50 60 70 80 case 1 corresponds to ε 0 5 d 0 2 α 10 re 10 000 k 1 and case 2 corresponds to ε 0 9 d 0 5 α 100 re 10 000 k 1 moreover λ r and λ i represent the real part and imaginary part of the leading eigenvalue respectively different values of n have been tested using our code and we found n 60 is adequate to obtain the accurate results 4 results and discussion 4 1 instability of the classical plane poiseuille flow to verify the accuracy of our calculation procedure and the code we first conducted the calculations of the classical plane poiseuille flow in a channel where a pure newtonian fluid flows inside it and compared our results with the work by orszag orszag 1971 this is a one layer system which is governed by navier stokes equation only orszag used chebyshev tau method to solve the orr sommerfeld equations we found the most unstable mode occurs at re 10 000 and k 1 is λ 0 23752649 0 00373967i which is the same as orszag s results fig 3 represents the neutral curve λ i 0 in the re k plane for the classical plane poiseuille flow in a channel the critical reynolds number was found to be 5773 which appears at k 1 02 4 2 the effects of depth ratio d fig 4 shows the neutral curves for various depth ratio d when m 1 α 50 and ε 0 8 depending on the values of d α and ε the curves are composed of two intersecting curves and have a bi modal structure in fig 4 a where the range of d varies from 0 1 to 0 5 the short wave branch i e the right side branch or the so called fluid branch chang et al 2006 liu et al 2008 hill and straughan 2009 dominates the instability of the system in all neutral curves i e the instability primarily occurs in the fluid domain for d 0 5 the long wave branch chang et al 2006 liu et al 2008 hill and straughan 2009 i e the porous branch almost disappears as the depth ratio decreases from 0 5 to 0 1 the effect of porous layer on the instability increases fig 4 a reveals that by decreasing the depth ratio d the flow becomes unstable on the contrary as can be seen in fig 4 b by further decreasing the depth ratio from 0 1 to 0 05 the flow becomes stable and the critical reynolds number increases in addition the effect of the fluid region on the instability decreases this inverse behavior of the neutral curves in two different length ratio ranges i e d 0 1 0 5 and d 0 05 0 1 have never been discussed in the previous literatures we also found that the critical depth ratio for m 1 α 50 and ε 0 8 is 0 0632 for d 0 0632 the porous mode dominates the instability and when the depth ratio reaches to 0 05 the fluid branch nearly disappears for d 0 0632 the fluid mode always dominates the instability in general these results show that the depth ratio is a critical parameter and affects the instability of the porous fluid system significantly in order to better understand the two critical modes in a neutral curve i e the points where the minimal reynolds numbers occurs in the fluid or porous branch following the other researchers chang et al 2006 liu et al 2008 hill and straughan 2009 we also present the normalized vertical velocity amplitude v y at the two minima in the bi modal structure of the neutral curve where d 0 1 m 1 α 50 ε 0 8 fig 5 a shows the eigenfunction i e the amplitude of vertical velocity of the most unstable mode in the porous branch the graph shows a flow reversal at the interface which indicates the strong influence of instability from the porous medium layer near the fluid layer this is because the limited space within the fluid layer in fig 5 b the flow reversal at the interface disappears this phenomenon proofs the fluid layer dominants the instability in the fluid porous system a similar scenario has been observed by other researchers in chang et al 2006 liu et al 2008 hill and straughan 2009 4 3 the impact of the permeability parameter α we then analyzed the impact of the permeability parameter α on the instability fig 6 shows the neutral curves at different permeability parameters α for m 1 d 0 1 and ε 0 8 first we considered the permeability parameter α less than 50 as can be seen in fig 6 a for α 25 the instability initiates in the porous region by increasing α from 25 to 50 the system becomes unstable and the impact of the porous layer reduces this is because larger α values represent the porous layer has lower permeability which lead to higher flow resistance in the porous layer it then appears that the instability in the fluid porous system especially at the fluid porous interface more likely occurs in addition the critical reynolds number decreases and switches from the porous region to the fluid region at α 31 6 the critical reynolds number in the fluid branch and the porous branch becomes equal we then considered the permeability parameter greater than 50 as shown in fig 6 b by increasing α from 50 to 150 an opposite behavior of the neutral curves was found where the critical reynolds number increases this represents the porous fluid system becomes more stable in addition the bi modal structure can be found in all these curves in fig 6 b where the short wave branch i e the fluid branch is always lower than the long wave branch i e the porous branch which indicates the instability is dominated by the fluid branch this is because as α increases the effect of porous layer on the instability becomes insignificant 4 4 the impact of the parameter γ by comparing the effect of the depth ratio and the permeability parameter on the instability system we found there is always a neutral curve with a critical value of α or d which has identical minimal reynolds number in both fluid branch and the porous branch this phenomenon has not been addressed before we then defined a new parameter γ which relates α and d as 4 1 γ α d l k fig 7 shows the neutral curves for γ 2 5 3 16 4 at m 1 ε 0 8 for each γ we have chosen two groups of α and d values as can be seen in fig 7 the neutral curves for two groups of α and d values are overlapped perfectly this indicates that γ is independent from the combination of the values of α and d in other words once we determine a γ value we can choose different combination of α and d values without changing the structure of the neutral curve thus instead of comparing α and d separately it is more convenient to distinguish the instability of the system by comparing only the γ value it should be noted that the parameter γ is not valid for the extreme conditions when α α 0 d and d 0 as can be seen in fig 7 γ γ c 3 16 is a critical value where the minimum value of the reynolds number in both the fluid and the porous branch are the same thus specifically for m 1 ε 0 8 when γ is less than γ c where the instability is dominated by the porous region the system becomes more stable and when γ is greater than γ c the instability is dominated by the fluid branch the values of the critical reynolds number decays and the system becomes unstable 4 5 the effect of the porosity ε to further examine the instability of the coupled flow in terms of the properties of the porous media the neutral curves for various porosity are shown in fig 8 for m 1 d 0 1 α 0 8 we found that the bi modal structure appears in all the neutral curves when the porosity ε increases the porous branch and the fluid branch both become more unstable especially at the region close to the intersection it should be noted that the analytical model used in studying the flow inside porous medium is brinkman s equation and in low porosity environments the laplacian term in the brinkman s equation is not valid hill and straughan 2009 therefore we could only consider the conditions for ε 0 3 generalization to the smaller porosity values is the subject of our current investigations we then investigated the effect of the porosity on the instability of the system by defining the critical parameter γ fig 9 shows the neutral curves at the critical parameter γ c for ε 0 3 ε 0 6 and ε 0 99 the critical parameter γ c for each porosity value obtained by a combination of two different values of α and d this confirms that the neutral curve presented in fig 7 can be employed for various porosity of the porous media as long as ε 0 3 also for m 1 the critical parameter γ c 3 1 0 1 which indicates that there is a weak effect of the porosity on the instability we would like to emphasize that the parameter γ is only valid in the normal condition we will discuss the special conditions in the next section 4 6 special cases to examine the special cases of the coupled flow system because of the numerical limitations we only consider large or small values of α and d in our analysis fig 10 shows the neutral curve and the corresponding basic flow velocity profile for d 104 α 10 4 m 1 ε 1 the resulting neutral curve shown in fig 10 a is the same as the classical plane poiseuille flow in a smooth channel see fig 3 we then assume d 104 as a constant value and vary α m and ε where no variation in the results was observed see fig 10 these results state that for very large values of the length ratio the effect of the porous medium on the instability is negligible this effect can also be seen in fig 10 b which shows the normalized basic flow velocity profile for d 104 α 10 4 m 1 ε 1 for large values of the depth ratio d the normalized velocity inside the porous layer reaches to very small values that is a characteristic of the plug flow inside the porous layer this is because the effect of the porous layer becomes insignificant resulting in the instability characterization of the classical plane poiseuille flow when the height of the channel is 2l we then considered the condition with α 104 ε 10 4 d 1 that is the case with an extremely low permeability and porosity of the porous medium we found the neutral curve and the corresponding basic flow velocity profile are the same as fig 10 this indicates that for very large permeability parameter the effect of the porous medium on the instability of the system is also negligible in other words a very large permeability parameter represents an impermeable surface in the channel furthermore the normalized basic flow velocity inside the porous layer is almost equal to zero as a result these graphs demonstrate that the instability of the system is determined by the free fluid layer when either the length ratio or the permeability parameter are extremely high fig 11 a shows the neutral curve for α 10 4 ε 1 d 10 4 m 1 this is the case with a very high permeability and porosity porous medium layer and an extremely low length ratio if we consider the neutral curve in the rem km plane using k m k d and r e m v m v d r e we can find that the critical reynolds number the corresponding wave number and the neutral curve in the porous medium region are the same as classical plane poiseuille flow in a no porous channel fig 11 b shows the corresponding basic flow velocity profile which is normalized by vm i e the maximum velocity inside the porous medium this figure clearly shows that the normalized velocity profile inside the free flow region is zero which confirms that the free flow region has no impact on instability of the system 5 conclusions in the present study we examined the linear stability problem between two parallel plates where the bottom one coated with various porous media with permeability k and porosity ε this work considers a model to describe a wider parameter space which helps us to find different behaviors of the system instability the brinkman s and navier stokes equations were coupled to describe the flow inside the porous medium and in the free fluid region respectively the laplacian term of our governing equations has also been defined based on our experimental measurements wu and mirbod 2018 which is different from the corresponding one in the previous studies thiele et al 2009 hill and straughan 2008 goyal et al 2013 lyubimova et al 2016 we first characterized the effect of the depth ratio d on the system in particular we found that there are ranges of the depth ratio and the permeability parameter that will affect the instability of this coupled flow we found that for d 0 1 α 50 m 1 and ε 80 as the length ratio increases the effect of the porous medium on the instability decreases also the entire system becomes more stable results in an increasing the critical reynolds number for d 0 1 as the depth ratio decreases the effect of the fluid branch on the instability decreases it appears that there is a critical value of d 0 0632 at which the instability is switched from the fluid mode to the porous mode we then investigated the effect of the permeability parameter α when m 1 and ε 80 it was found that for α 50 the system becomes more stable as α decreases when α becomes less than 31 6 the porous branch dominates the system for α 50 as α increases the system becomes stable and the fluid mode dominates the instability to describe the instability behavior of the coupled flow system more broadly we proposed a new parameter γ as the product of the length ratio d and the permeability parameter α we found for each γ value there is a neutral curve which is independent of the combination of α and d values in addition for each specific porosity value ε there exists a critical parameter γ c the neutral curve and the corresponding critical value could help to characterize the instability of the system and whether the system is dominated by the fluid branch or the porous branch we further studied the effect of porosity on the instability we compared the neutral curves at porosities ε 0 3 0 5 0 8 0 99 and observed that the system becomes slightly unstable when the porosity increases we then determined the critical γ values for ε 0 3 0 6 0 99 and we found that the critical γ always equals to 3 1 0 1 which indicates the porosity has little effect on the instability we have also examined and compared three special cases including 1 the extremely large d values 2 the extremely large α values and 3 the extremely small α d values for the former two cases the effect of the porous medium on the system is negligible and the system behaves the same as the classical plane poiseuille flow in a channel with impermeable surfaces for the last case the free fluid region has no effect on the fluid porous system and the instability behavior inside the porous medium is also the same as the classical plane poiseuille flow in a channel with impermeable surfaces because of the extremely high permeability and porosity condition these results agree well with the real physical phenomena these results will provide valuable insights into the analysis of the pure newtonian fluid overlying a porous media that are important in a wide range of industrial applications ranging from biological devices to geophysical problems it is expected that the results obtained in this study will provide a step forward in understanding and examining the complex fluids motion over permeable surfaces and analyzing the instability of such system the precise experiments to investigate the instability more deeply and validate our analytical model are subjects of our current investigations acknowledgement this research was supported by the army research office aro under award no w911nf 18 1 0356 
616,reliable flood forecasting systems are the prerequisite for proper flood warning systems currently satellite remote sensing srs observations are widely used to improve model forecasts although they provide distributed information they are sometimes unable to satisfy flood modellers needs due to low overpass frequencies and high measuring uncertainties this paper assesses the potential of sparsely distributed in situ floodplain water level sensors to provide accurate near real time flood information as a means to enhance flood predictions a synthetic twin experiment evaluates the assimilation of different sensor network configurations designed through time series clustering and voronoi spacing with spatio temporal rmses reaching up to 1 cm the study demonstrates great potential adequate sensor placement proved crucial for improved performance in practice observation locations should be chosen such that they are located rather close to the river to increase the likelihood of early flooding and thus acquiring valuable information at an early stage of flooding furthermore high measuring frequencies benefit the simulations though one should be careful not to overcorrect water levels as these may result in inconsistencies lastly a network size of 5 to 7 observations yields good results while an increasing number of observations generally diminishes the importance of extra observations our findings could greatly contribute to future flood observing systems to either compensate for ungauged areas or complement current srs practices keywords flood monitoring data assimilation observation network 1 introduction changing climate conditions have undeniably affected the extremity of weather events of which floods are only one out of many gaume et al 2016 bryndal et al 2017 ummenhofer and meehl 2017 in a constant search for better flood warning systems much effort has been put into the development and improvement of flood inundation models hybrid models that combine 1d channel flow with 2d floodplain routing such as lisflood fp bates and de roo 2000 neal et al 2012 and mike flood patro et al 2009 have proven to be successful yet every model is only an approximation of nature and is therefore imperfect the mike flood model includes the boussinesq approximation to reproduce real flood dynamics which contrasts with the simpler inertial approximation used by the lisflood fp model each of the models inability of capturing the true dynamics should be accounted for by quantifying their uncertainties bates et al 2014 factors contributing to the total predictive uncertainty may include amongst others model equations and parameters errors in the digital elevation model dem parameterisation of sub grid physics and domain discretisation as this uncertainty might prevent flood managers from taking targeted action morss et al 2001 it is of paramount importance to reduce it this can be achieved by integrating observational information into the prediction hence constraining the prediction by means of both model and observations it should however be noted that observational information is also associated with uncertainty and this should be accounted for whether this includes systematic errors instrument error or random errors noise neppel et al 2010 di baldassarre et al 2012 this technique is known as data assimilation da and is widely used in environmental sciences for predictive analysis reichle 2008 carrassi et al 2018 in short da merges model states and observations thereby computing a weighted average of both states which is then considered as a new best estimate multiple studies have shown the potential of the joint use of a flood inundation model and observations with respect to calibration validation studies aronica et al 2002 horritt and bates 2002 schumann et al 2014 gobeyn et al 2017 during the last 20 years data assimilation has gained much attention within the field of flood inundation modelling in particular studies first focussed on the use of river observations measured or derived from in situ gauging stations madsen et al 2003 assimilated water levels of three river gauging stations thereby updating initial conditions at forecast time the flood forecast was subsequently improved for lead times up to 48 h other studies have focussed on further improving the assimilation procedure by either describing the spatial distribution of error statistics madsen and skotner 2005 or more recently defining the impact of sensor locations on the forecast skill mazzoleni et al 2015 given that the use of in situ river observations has always been of common practice barthélémy et al 2017 mazzoleni et al 2017 ricci et al 2011 in situ floodplain observations have only been used occasionally for instance madsen et al 2006 used a set of river and floodplain measurements as a validation set for a synthetic assimilation study although hartnack et al 2005 have suggested the potential use of in situ floodplain observations in assimilation studies neal et al 2007 were the first to effectively assimilate a combination of river and floodplain water levels into a hydraulic model for a small scale da experiment different sampling designs improved overall accuracy albeit with varying degrees of success furthermore the complementarity of river and floodplain sensors was investigated showing that river stage observations slightly outperform floodplain observations for updating the hydraulic model to the authors knowledge no study has yet investigated the optimal placement of sparse in situ floodplain sensors following the global decline of river gauging stations fekete and vörösmarty 2007 along with the need for distributed knowledge bates 2004 attention was shifted towards the use of satellite remote sensing srs observations to date many studies have demonstrated the use of synthetic aperture radar sar observations in the river andreadis and schumann 2014 giustarini et al 2011 matgen et al 2007a 2010 and floodplain hostache et al 2010a 2010b 2018 cooper et al 2018 into a da framework for flood monitoring along with this increasing amount of data ridler et al 2014 the efficient use of srs observations has been of growing interest a large number of studies have been devoted to the use of so called thinning strategies where redundant information is removed from the assimilation process for reasons of high computational cost ochotta et al 2005 increased water level uncertainty di baldassarre et al 2011 or the lack of reliable estimates of observation error covariances bormann and bauer 2010 in spite of varying degrees of success when assimilating srs observations it is important to realize their use suffers from drawbacks first considerable errors remain when water levels are extracted from sar imagery di baldassarre et al 2009 stephens et al 2012 schumann et al 2008 matgen et al 2010 2007a garcía pintado et al 2015 dasgupta et al 2018 second in some cases the impact of assimilation vanishes a few hours after acquisition although matgen et al 2007b point out that for large basins with a slowly changing rising or recession limb the impact is more persistent last the forecast skill seems to be proportional to the time interval between satellite overpasses andreadis et al 2007 showing the need for frequent measurements if continuous flood monitoring is desired as a result several authors emphasize that remotely sensed observations should not replace in situ measurements neal et al 2009 instead a complementary use of both ground data and remotely sensed observations is suggested matgen et al 2010 schumann et al 2009a in summary the evolution towards the use of srs observations in da studies is still ongoing however it is clear that the potential of in situ measurements should not be underestimated whereas previous work has mainly focused on in situ river observations the impact of in situ floodplain observations on the forecast skill remains unexplored to fill this gap this paper presents the assimilation of different floodplain observation configurations to improve or complement current flood prediction practices our key objective is to evaluate the overall da performance of several configurations and thereby offering guidance on the allocation and size of such an in situ network in addition we provide insight into the individual contribution of each observation within a network both in time and space hence we are able to identify which networks contain either mutually reinforcing observations or redundant information and as such to explain why some networks perform better than others lastly the assimilation frequency is varied in order to identify the impact of observation frequency on the persistency of the improvement for different networks the proof of concept is elaborated by means of a synthetic twin experiment which allows us to theoretically test the da setup without the need for real observations altogether the explorative nature of this research thus seeks to offer a basis for real case implementations to conclude the purpose of our investigation is not to advocate a replacement of current river gauging networks but rather to provide an insight and recommendation on the use of in situ floodplain networks eventually one could use an in situ floodplain network either alongside with srs observations as a compensation for poorly gauged areas mazzoleni et al 2017 2018 or as a single observation source in case river gauging stations no longer operate due to extreme flash floods aronica et al 2002 this paper is organised as follows section 2 outlines the design of the study and describes the flood inundation model the available data and the data assimilation algorithm the results are presented in section 3 and conclusions are drawn in section 4 2 methodology 2 1 study site and data in the snowdonia region of north wales the river dee originates and flows eastward through the steep wooded valleys of its upper reach serving as a natural boundary between england and wales the lower reach meanders in a flat topography before crossing the city of chester and flowing into the irish sea the overall length of the river dee is 110 km and its catchment area covers 1816 8 km2 the river dee was the first uk river to be classified as a water protection zone following a pollution incident in 1999 nowadays the river dee provides drinking water for over 3 million persons the modelled area of this study covers 40 km2 and includes the confluence of an 8 km reach of the river alyn with a 10 km section of the river dee s lower reach the average discharge rate of this lower reach is around 30 m3 s the river alyn has an average width of 12 m while the river dee is 30 m wide in average in contrast to the hilly upper reach the downstream floodplains have an average slope of only 0 005 grimaldi et al 2016 and are characterized by steep side slopes resulting in a valley filling event when flooded gobeyn et al 2017 owing to this flat topography the river floodplain can stretch up to more than 2 km and is mainly covered by arable land a lidar dem of 20 m was provided by the ea environment agency of england and wales and post processed in order to eliminate artificial artefacts of the area fig 1 although the river dee is well regulated it has experienced frequent flood events macdonald and sangster 2017 in december 2006 a 2 year flood event occurred of which the peak river discharge on december 8 caused an area of 8 3 km2 to be flooded this event has been of particular interest to many researchers because of the availability of satellite sar images acquired just after peak flood di baldassarre et al 2009 stephens et al 2012 gobeyn et al 2017 a visual delineation of this image s flood extent by schumann et al 2009b is also depicted in fig 1 the flood event of 2006 will also be used for our research purpose gauged water flow and stage data from november 18 until december 29 up and downstream of the study area are provided by the ea with a time resolution of 15 min in total three gauging stations cover the study area farndon dee ironbridge dee and pont y capel alyn named after a nearby city or construction 2 2 experimental design in this study the potential of in situ flood monitoring for improving model predictions through data assimilation experiments is tested from river forcings a set of roughness parameters and topographic information a flood inundation model is able to simulate floodplain water levels our state variables of interest these state variables will be synchronously updated by water level observations from a sparse in situ observation network apart from the current river gauging stations no additional stations yet exist in the floodplain area of interest as a result no validation data are available on the water levels in the floodplain since these are crucial in performance testing a synthetic case is used to examine the study object the experimental design of this study is based on an identical twin experiment masutani et al 2010 privé et al 2013 it offers a controlled setting for the evaluation of a data assimilation setup typically such experiment consists of 3 model runs a control run truth a free forecast run without observation integration open loop or background and an assimilation run analysis raicich and rampazzo 2003 other than for real case scenarios it is possible to design this experiment in such a way that a number of key complexities can be avoided first if all runs are simulated by the same model model structural errors can be neglected for comparative analysis second when both observations and forecasts are derived from the truth run through perturbed model initialisations this setup may also eliminate the presence of a possible model observation bias sørensen et al 2004 synthetic experiments are therefore very skillful for experimental control it is possible to not only identify the behaviour of da systems matgen et al 2010 andreadis and schumann 2014 but also to create different designs for the observing system privé et al 2013 if the synthetic da setup is successfully evaluated by proof of principle it should be further tested with real case data in an operational context within the area of flood inundation modelling many studies have demonstrated the use of a synthetic twin experiment for different research purposes sørensen et al 2004 hartnack et al 2005 madsen et al 2006 andreadis et al 2007 matgen et al 2010 garcía pintado et al 2013 andreadis and schumann 2014 gobeyn et al 2017 cooper et al 2018 in this study we investigate the potential of in situ floodplain observations in a da framework since the states of interest are unobserved they are synthetically generated by a flood inundation model that simulates floodplain f dynamics in terms of water level states ψ using river r flow boundary conditions q fig 2 consequently the aforementioned runs that constitute the twin experiment can be simulated with the use of river measurements from 3 gauging stations see section 2 1 first the truth run comprises an input qtruth and output ψ truth state linked by a dynamical system i e the model no uncertainty is associated with the truth secondly in order to obtain a model forecast an ensemble of inputs qens derived from the truth input using perturbations p is run through the same model yielding an ensemble of output states ψ f the ensemble average constitutes the open loop model prediction along with its uncertainty represented by the ensemble variance in this study we will use 32 ensemble members lastly for this prediction to improve observations are integrated into the da system in our design the observations ψ obs correspond to the truth output to which a gaussian error is added in order to represent the impact of observational uncertainty at a fixed frequency the data assimilation procedure combines the background model state and the synthetically generated observations taking into account not only their numerical values but also their respective uncertainties outlined in sections 2 2 2 and 2 2 3 this results in improved model states ψ a that will replace the old model states for subsequent forecasting until new observations become available for the next assimilation stage 2 2 1 flood inundation modelling the river dee flood event is simulated with the coupled 1d 2d lisflood fp flood inundation model it was first developed by bates et al 2010 with the aim of accurately reproducing flood events using a minimal representation of dynamical processes river flow is simulated by an approximation of the 1d de st venant equations 1 q x a t q 2 h x n 2 p 4 3 q 2 a 10 3 s 0 where q m3s 1 is the discharge a m2 the water cross section area x m the distance measured along the thalweg t s the time q m3s 1 additional flow from other sources to be set 0 for this case study s 0 the channel bed slope also approximated by the water surface slope n the manning friction coefficient p m the wetted perimeter for simplification the channel cross sections are assumed rectangular which is reasonable for rivers that are wide and shallow neal et al 2015 floodplain routing between two adjacent cells is numerically computed on a 2d raster grid by the manning formula horritt and bates 2002 3 q x i j h f l o w 5 3 n f p h i 1 j h i j δ x 1 2 δ y where h i j m is the water free surface height at node i j of the grid and hflow is defined as the difference between the highest water free surface of both cells and the highest bed elevation nfp is the manning floodplain coefficient and δx m and δy m are grid dimensions for the y dimension q y i j is defined in a similar way when the river water reaches bankful depth water flows into the adjacent floodplain cells along the river reach water is then routed through the floodplain by gravitational forcing using information from the dem each grid cell acts as a storage volume for which a mass balance is computed at every time step bates et al 2010 as a result the lisflood fp model computes water level time series for each grid in the study area using forcing variables parameters and topographic information both forcing variables as well as topographic information were provided by the ea the time resolution of the input forcing is 15 min whereas results were only retrieved on an hourly basis to reduce storage costs the spatial resolution of the model was restricted by the dem resolution and was set at 20 m following guidelines in chow et al 1988 and a former calibration study that used the lisflood fp model for the same case study true channel and floodplain manning coefficients were fixed at 0 06 and 0 12 respectively gobeyn et al 2017 wood et al 2016 according to hostache et al 2010a spatially distributed manning coefficients do not improve results significantly therefore both coefficients are considered to be constant in space and time the model was run in subgrid channel mode making it possible for subgrid scale river sections to be modelled neal et al 2012 2 2 2 generation of the ensemble model state assessment of the model state uncertainty usually requires the definition of many sources of uncertainty such as input state uncertainty parameter uncertainty and model structural error although errors in the dem and bathymetry could significantly affect model results these errors are disregarded in this synthetic study instead we focus on the river input and boundary state uncertainty as well as parameter uncertainty and assume a perfect model structure bathymetry and dem similar to gobeyn et al 2017 as such representation of the model state uncertainty requires model simulations with perturbed parameter sets and perturbed input states toth and kalnay 1993 the true parameter set adopted from section 2 2 1 is perturbed by gaussian noise with a standard deviation for the channel and floodplain manning coefficients of 0 01 respectively 0 02 perturbation of the true input states is performed using an autoregressive ar error model evensen 2003 4 ϵ j α ϵ j 1 1 α 2 w j where ϵ is the input error at any time j 1 given that the initial value of ϵ0 is known α is the temporal autocorrelation coefficient and w j n 0 σ w 2 is a white noise term like many error models this ar model consists of a deterministic component relating to the memory of the process red noise and a stochastic component white noise their accompanying weights determine how they relate both weights depend on the temporal autocorrelation coefficient α represented by an exponential decay garcía pintado et al 2013 5 α e δ t j τ where δtj s is the length of the time interval between two time steps j 1 and j and τ s is the decorrelation length of the forcing variable in this study it is assumed that the temporal autocorrelation of the flow error is equal to that of the flow for time series of river flow and river level data the decorrelation length is the length of a time frame where starting and ending point have significantly decreased autocorrelation for single event based cases a flood mostly has a long memory correlation due to the persistence of the process water level time series in flood conditions often consist of a rising and receding limb therefore holding a high positive autocorrelation of the floodplain water level time series in the first half of the event switching to a high negative autocorrelation in the second half bartl et al 2009 in order to make a best estimate of the decorrelation length we take into account a the first time when decorrelation temporarily occurs and b the stability of acquired perturbations the time decorrelation length was set at 3 days or 288 model time steps identical to what was obtained by garcía pintado et al 2013 the established error time series can be implemented either in a multiplicative or an additive way depending on the nature of the variable in the case of river flow the errors increase with increasing discharge rates while for water levels errors remain constant as such discharge errors are mostly expressed as percentage of flow while water level errors are expressed in metres for a discharge time series q the perturbed time series q is obtained as follows 6 q q 1 ϵ from this equation it is clear that ϵ projects the model uncertainty hereafter referred to as ϵ m recall from eq 4 that this error time series is dependent on the values of α and w that can be chosen such that the desired model error is obtained in this study multiplicative flow error values of q 10 20 30 and 40 or equivalently ϵ m equal to 0 1 0 2 0 3 and 0 4 are assumed for analysis and listed in table 1 in contrast for the water levels of ironbridge station errors are introduced in an additive way and are represented as standard deviations varying from 1 2 3 to 4 cm also listed in table 1 although many studies on river flow uncertainty mention flow error values of 10 20 madsen et al 2006 di baldassarre and montanari 2009 hartnack et al 2005 sikorska et al 2013 and equivalently accuracies of 1 2 cm for water levels lang et al 2010 pappenberger et al 2006 some authors report higher uncertainties up to a flow error of 80 and a water level error of 10 cm in unsteady conditions mcmillan et al 2012 coxon et al 2015 madsen et al 2003 we therefore take into account a range of plausible values the upper panels of fig 3 illustrate the inflow discharge and outflow water level time series black for all gauging stations together with the ensemble forecasts grey derived from the truth state with ϵ m equal to 0 3 the bottom panels display the distributed floodplain water levels at certain points in time 2 2 3 assessment of the synthetic observations only a few studies have examined the potential of a floodplain in situ observation network for flood monitoring neal et al 2007 werner et al 2005 cooper et al 2018 however none of them have formulated the optimal configuration of sparse observation locations in this study we explore the impact of different observation configurations on the da forecast skill to provide guidelines on the allocation of floodplain water level sensors since no sensors are yet located in the floodplain synthetic water level observations are derived from the model truth output by drawing samples from a gaussian distribution with its mean equal to the true state and a standard deviation ϵ o that is varied throughout analysis 1 5 10 15 20 25 up to 30 cm table 1 the authors believe that those values reflect a range of conditions from steady flood routing to unsteady water surface conditions that real case scenarios could deal with 2 3 data assimilation a data assimilation da procedure aims at combining different sources of information generally a prior model forecast and observations to obtain a new best estimate of a system state generally the prior knowledge is represented by a vector of m predicted states ψf that is merged with d a set of n observations the da procedure subsequently creates a posterior state vector ψa that is hoped to be a better representation of the truth ψ truth the ensemble kalman filter enkf uses a collection of n model realisations ensemble members to represent the prior model uncertainty denoted as the forecast state vector ψ f ψ 1 f ψ 2 f ψ n f t m n evensen 2003 burgers et al 1998 katzfuss et al 2016 likewise a vector of noisy observations d d 1 d 2 d n t n n is used to mimic the observational uncertainty burgers et al 1998 combining both uncertain sources of information constrains the prior forecast resulting in the analysis state vector ψ a ψ a 1 ψ a 2 ψ a n t m n the analysis scheme of the enkf for an ensemble size equal to n is represented in the state space as follows 7 ψ a ψ f k d h ψ f h n m is the forward operator that performs mapping from the model space to the observation space houtekamer and zhang 2016 if the state variables of interest are directly observed the elements of h can take either of two values 0 or 1 k m n is the kalman gain matrix that in itself is a function of both the background error covariance matrix p m m and the observation error covariance matrix r n n 8 k p h t h p h t r 1 9 p ψ f ψ f ψ f ψ f t 10 r ϵ o ϵ o t where ϵ o assigns errors to the measurements details on the background and observation error covariances are discussed in section 2 3 1 regardless of the simplicity and flexibility of the enkf its application requires the gaussian assumption for error statistics that cannot always be met however in our synthetic case errors are controlled and all assumptions are valid 2 3 1 on the kalman gain it was shown that uncertain flood forecasts can be constrained by the help of observations this study considers the observations to be sparsely distributed along the floodplain as a consequence various observation configurations may each influence the assimilation procedure differently in view of the continuous search for improved flood forecasting systems the main goal of this study is thus to understand how different observation configurations influence the forecasts and more importantly what an adequate configuration should look like following eq 7 the right hand sided element k d h ψ f is known as the increment and represents the discrepancy between the observed states and their corresponding model estimates weighted by k the forecast skill of the da setup relies on the appropriate definition of k in order to improve the model estimates therefore the elements of k are also referred to as weights or gain values furthermore the kalman gain matrix reflects the relative magnitudes of the measurement error r on the one hand and the model error p on the other hand eq 8 depending on the trust that is given to either the model or the measurements where the trust is inversely proportional to the uncertainty on the model states or the observations the forecast will be updated accordingly for example if the observational uncertainty is small compared to the model uncertainty values for k will tend to be large and updates will favour the observations how these values can be attributed to both the model and the measurement covariance matrices will be discussed in the next two paragraphs first the measurement covariance matrix is evaluated in the case of independent observation acquisitions such as for a sparse in situ network the observation errors are uncorrelated in both space and time if we assume an equal instrument error for all observations the covariance matrix r reduces to a diagonal matrix moreover if different observation configurations in this study are to be compared r will be kept constant in our analysis as such the impact of the background error covariance matrix p on the forecast skill can be independently investigated secondly the error covariance matrix p reflects how variables co vary in terms of an ensemble we refer to the ensemble error covariance matrix p from eq 9 it can be derived that the magnitude of the covariance between two ensemble states is determined by their individual ensemble variance while its sign is determined by their correlation the covariance matrix p thus consists of ensemble variances along its main diagonal and ensemble covariances between each pair of locations in the other matrix positions for example if two locations show a similarly large ensemble variance but their behaviour is inversely proportional the covariance will tend to be large and negative in short this covariance matrix identifies how pairs of state variables in the study domain mutually behave and is used to define the kalman gain matrix k in practice each of the columns of k consists of m elements or gains one of which points to the observed state as a result the magnitude of each element of k will determine to what degree a specific observation will contribute in the updating of forecasts thus k is composed of elements k i j that update a domain pixel i 1 2 m in accordance to an observed pixel j 1 2 n 11 k m n k 1 1 k 1 2 k 1 j k 1 n k 2 1 k 2 2 k 2 j k 2 n k i 1 k i 2 k i j k i n k m 1 k m 2 k m j k m n based upon the previous findings for each pixel i there exist n weights k i 1 k i 2 k i n that each contribute to the updating of the pixel or in other words the contribution of each observation j towards every domain pixel is evaluated with the weight vector k 1 j k 2 j k m j t in order to investigate which observation j contributes most to the model updates we have to rearrange and decompose eq 7 into element wise products 12 ψ i j a ψ i j f k i j d j if we now expand this equation for n ensemble members and we wish to assess for a single state i the mean absolute contribution z i j of the ensemble for a single observation j we may calculate 13 z i j 1 n e 1 n k i j d j e where d n n represents the innovation matrix for an entire ensemble of size n to avoid the cancellation of contributions due to positive and negative values of k absolute values are used in the equation finally if one wants to know which of the n observations contributes most in updating a particular state we search for the index j that provides the largest value for z i j 14 z i max j 1 n e 1 n k i j d j e the above elaboration validates for a single assimilation time step if we extend this reasoning for multiple time steps during the assimilation procedure the value of zi is acquired at different time steps this will allow us to obtain an overall view on the individual contribution of observations not only spatially but also in time bearing this in mind such analysis may identify the presence of complementary or redundant sensors in an observation network and provide guidance for real case scenarios 2 4 observation network design in view of the increasing threat of floods certain flood prone areas have been subjected to the assessment of a relatively dense stationary network of river gauging stations where water level and or discharge is recorded to our knowledge no directed planning on the allocation of floodplain sensors has yet been explored similarly to what is required for the allocation of river sensors floodplain sensors should in the first place be easily accessible and installed on a stable subsurface in addition locations should exclude among other things steep slopes and tall vegetation as those may hamper reliable sensor reading further a dispersed sensor placement increases the chance of capturing both small and large scale flood events during recent years awareness arose on the importance of strategic planning that not only involves the allocation of a sensor network as introduced above but also allows a deeper scientific understanding of flood processes marsh and hannaford 2008 since flood routing is mainly driven by the floodplain topography local topographic variabilities may cause certain sensor configurations to be suboptimal moreover no research has yet focused on the potential complementarity of nearby observation locations in da experiments hence this study will explore the use of different observation configurations based on not only geographical information but also flood dynamics as the objective of this study is to evaluate the opportunities of flood prediction by means of in situ floodplain observations different design techniques will allocate observation networks intended to cover a range of possible configurations one of the applied techniques is based on the cluster analysis of a distance matrix liu et al 2013 to this end a similarity measure subjects water level time series of the floodplain true state to a pairwise comparison if each pair of the time series is assigned a value of similarity all pairs can compose a distance matrix a squared matrix of size m m then a cluster analysis identifies structures in the distance matrix that can be segmented into groups or clusters if two state variables exhibit analogous similarities they are likely to be merged in the same cluster in contrast if two state variables show a different behaviour they are likely to end up in different clusters where each cluster may have the tendency to provide complementary information this study proposes three different similarity measures each highlighting a different side of the flood dynamics the similarity measures between water level time series represented by vectors x and y are listed below 1 euclidean distance 15 d e x y t x t y t 2 2 cosine distance 16 d c x y t x t y t t x t 2 t y t 2 3 mahalanobis distance 17 d m x y x y t p d 1 x y where p d is the covariance matrix of the full dataset and t represents the transpose of the matrix the euclidean distance is widely used for time series analysis because it is simple robust and hard to beat serrà and arcos 2014 rather than accounting for numerical values the cosine distance evaluates vector directions zhang and lu 2003 the mahalanobis distance is a data driven measure that takes into account the covariance matrix p d that contains the covariances of all state variables calculated from the true floodplain water level time series it should be noted that this covariance matrix is different from the one used by the enkf p in that the latter is an ensemble based matrix calculated for a single analysis time step whereas the mahalanobis covariance is based on the truth simulation and determined for the full time range this study employs hierarchical agglomerative clustering hac a bottom up approach that starts with assigning a separate cluster to each datapoint and repeatedly merges similar clusters until the desired number of n homogeneous clusters remains liao 2005 the merging procedure is determined by the ward s method that minimizes the clusters overall sum of squared distances in this case the commonly chosen euclidean distance murtagh and legendre 2014 note that this clustering distance is different from the time series distance mentioned above when cluster assigning is completed cluster representatives are determined as the states with the highest within cluster correlation those representatives then constitute the observation network further this study also employs clustering based on multilevel thresholding of the initial inundation time of each of the states computed by the otsu algorithm otsu 1979 as such the state variables that initially flood between two fixed time steps are brought together in a cluster cluster representatives are chosen in the same way as described above lastly a voronoi decomposition is applied to the modelled study domain this technique partitions the area into polygons that contain domain points related by a user defined proximity function in this study the squared euclidean distance as a result the centroids of the clusters define a dispersed observation network and thus do not need any further processing the voronoi clustering is based on a geometrical spacing rather than on water level dynamics such as the aforementioned techniques hence the obtained observation network provides observations that are distributed evenly in space bandyopadhyay and coyle 2003 kao et al 2008 guruprasad 2011 2 5 evaluation of the data assimilation setup the impact of the data assimilation algorithm on the flood forecasts can be evaluated using performance measures in terms of state variables the truth ψ truth validates the ensemble mean assimilation run ψ a and depending on the measure of use may compare it to the mean forecast without updating ψ f this study not only investigates the potential of in situ floodplain monitoring within a da framework but also tests different observation network configurations designed following section 2 4 therefore a comparison of all network designs by means of evaluation measures could establish a link between the forecast skill of a network and the configuration of its sensors previous studies suggest a combined use of different performance measures as an integrated approach for the evaluation of the da impact chai and draxler 2014 nguyen et al 2016 we will therefore use 2 performance measures that each highlight a different interpretation of the forecast skill the rmse root mean squared error giustarini et al 2011 king et al 2014 computes the average floodplain water level error thereby penalizing for large errors the second measure the csi critical success index is a classification measure that involves a binary approach to a flood and is mainly applied in studies where satellite data are involved landuyt et al 2018 stephens et al 2014 values may range from 1 perfect classification to 0 all pixels misclassified its maximum value of 1 corresponds to a perfect match between the assimilation run and the truth however if its value drops to 0 or less the assimilation run scores equally well or worse than the open loop run both measures are computed for the full time range and averaged over the model domain as such they represent the spatio temporal performance of the flood forecasts the formulation of the rmse is referred to in the accompanying references the csi is computed following eq 18 18 c s i a a b c where a is the number of correctly classified pixels hits b the number of overpredicted pixels false alarms and c the number of underpredictions misses based on the contingency table shown in table 2 3 results and discussion the data assimilation setup outlined in section 2 2 is evaluated by a total of 2520 simulations performed with the lisflood fp flood inundation model each simulation arises from a unique combination of the variables listed in table 1 and is validated by the truth by means of different evaluation measures section 2 5 as such it is possible to identify simulations that either favour the predictions or degrade the performance as compared to the open loop prediction more specifically this study focuses on the impact of observation configuration on the da forecast skill to this end this section covers four main aspects of our data assimilation setup first the spatial distribution of several observation configurations and their forecast skill is explored then for each type of configuration the time interval after assimilation during which improved predictions are obtained is investigated in order to gain in depth knowledge on the forecast skill a third analysis on the individual observation contributions within the network is conducted as a function of both time and space table 3 provides an overview of the da setups that each analysis accompanied with a figure deals with 3 1 observation network configurations the allocation methods introduced in section 2 4 are implemented for 3 different network sizes and provide observations for the assimilation procedure in total 5 similarity measures were used to allocate the observations eucl cos mah initt and vor table 1 the top panels of fig 4 illustrate for each measure the position of n 5 observations relative to the river and the maximum flood extent that is reached by the truth run recall that the allocation methods make use of clusters whose highest time series intracorrelations select the observing locations since the majority of the observations is located close to the lower river reach high correlations seem to occur close to the river a more detailed analysis of all domain pixels reveals that high correlations indeed roughly correspond to locations close to the river results not shown moreover these correlations diminish as we move further towards the flood edge in other words the correlations seem to follow the flood dynamics this phenomenon is explained by the downstream overtopping of the river dee where the inundation begins followed by a wavefront propagation that gradually stretches the water outwards onto the floodplain due to the floodplain roughness the floodwave slows down and becomes more distorted the further the water is routed onto the floodplain it is thus likely that similar water level time series occur rather close to the river than close to the flood edge in the synthetic study of cooper et al 2018 the authors advocate the use of upstream floodplain observations in their assimilation experiment for a simplified domain however they do note that for more complex topographies strong correlations could also occur further down the river reach the bottom panels of fig 4 show the true water level time series for the observation locations displayed in the corresponding top figure frames most of the pixels exhibit rather similar flood dynamics though with a clear water level variability that is explained by the variations in the dem the peak flows of each network more or less coincide with a maximum peak shift of around 5 h for pixel 5 of the vor configuration it should also be noted that some pixels hardly show any inundation cos pixel 1 and 2 initt pixel 5 making their observational contribution to be questioned the time series similarity measures introduced in section 2 4 discern clusters from one another it follows that the cluster representatives time series are intended to exhibit strong dissimilarities however this turns out to not always be the case for the eucl distance the variance of water level time series was expected to be higher as is the case for the mah distance the cos distance was included to incorporate pixels with distinctly different dynamics however since the flood dynamics are similar for the entire domain the cos configuration could be regarded unsuitable for clustering in contrast the time series of the initt and vor configurations do reflect their intended design the initt configuration discerns pixels from their time of inundation while vor discerns pixels from their location moreover vor is the only configuration that shows a distributed coverage of the observations for now our goal is not to provide an established method for observation allocation but rather to obtain different network designs whose assimilation results can be studied hence the configurations are implemented as such without relocations 3 2 overall performance of in situ floodplain monitoring our primary aim is to assess the impact of different observation configurations on the performance of the da procedure however other features than configuration may also affect the performance such as the assimilation frequency and the observation network size in order to examine the impact of a feature its values are varied while other variables are held constant for each value of the control feature simulations are carried out that vary the remaining features from table 1 and constitute the uncertainty of a prediction as such this experiment can determine both the average performance of a prediction as well as its uncertainty in fig 5 each boxplot belongs to a certain network configuration and consists of 84 simulations varying in both the observational and model uncertainty as well as the observation network size the assimilation frequency is fixed for a daily a and an hourly b monitoring the rmse and csi performance measures evaluate the assimilated forecasts spatially averaged over all floodplain domain pixels excluding the observed pixels to obtain a single score the average rmse performance of each configuration method reveals that eucl initt and vor provide the best results conversely the overall score of cos suggests that this configuration is suboptimal for our study in the case of hourly monitoring the average rmse remains below 3 cm as compared to 5 cm for daily monitoring while the open loop score equals 7 93 cm similar conclusions can be drawn for the csi where hourly assimilation shows a distinct advantage over daily monitoring with average values of around 0 79 and 0 73 respectively and an open loop score of 0 65 it should also be noted that the csi measure becomes insensitive to water level changes when the flood reaches its valley filling effect mentioned in section 2 1 this may cause csi values to be close to 1 during the peak flow in spite of much research effort on the assimilation of distributed srs observations reports on the floodplain forecast skill are virtually non existent many of those studies have predominantly focused on the forecast skill of the river observations thus whilst little reliable information is available from literature we believe the achieved scores are promising further it can be noted that from the rmse the spread or uncertainty of a prediction generally increases as the performance of a configuration method itself declines for example in case of the hourly assimilation the uncertainty for eucl initt and vor in fig 5b is similar while for cos and to a lesser extent for mah the uncertainty is noticeably larger this does not apply for the csi index which is a classification measure causing its value only to be influenced by pixel misclassifications rather than numerical error variations lastly the boxplots show outliers for both assimilation frequencies their simulations arise from a high model and or observational uncertainty subjecting the floodplain water levels to inconsistencies which in turn may cause the simulations to perform worse than the open loop forecast in the case of daily monitoring besides this test was repeated for multiple frequencies table 1 from which it followed that an hourly measuring frequency is required for all simulations to benefit from assimilation so far we correctly presumed that the forecast skill of a network could be attributed to the configuration of the observation network albeit no clear link between the da performance and the observation positions could yet be established from the previous section it was however noted that some state variables show a limited time of inundation in case of the cos configuration it can be emphasized that this limited inundation occurs for 2 observation locations 1 and 2 in fig 4 and so their added value to the joint network can be questioned this could possibly explain the lower score for the cos configuration rearranging the simulations from fig 5 as a function of network size we obtain the boxplots in fig 6 that each consist of a collection of 168 simulations since both performance measures of fig 5 draw similar conclusions the following results are only displayed for the rmse generally the average da performance improves the more observations are involved in assimilation this latter trend is the most significant for a network increase from 5 to 10 observations while the added value from 10 up to 20 observations is rather limited from this we deduce that adding observations benefits the predictions however in the case of the vor configuration little to no effect is observed for a network increase from 5 to 10 observations moreover extending the observation network implies practical and economic considerations such as operational costs and maintenance therefore we conclude that the network size depends on the available resources though the locations should in any case be carefully defined again the outliers from fig 6 can be attributed to high model and or observational uncertainty as well as low assimilation frequencies several studies encourage the use of a thinned set of srs observations mason et al 2012 ochotta et al 2005 2007 to minimize spatial autocorrelations and thus to avoid the assimilation of redundant information and the unnecessary use of computational power however in most operational cases autocorrelations from srs observations are still unfairly ignored in the off diagonal elements of r eq 8 waller et al 2016 these studies therefore propose the use of dispersed observation locations thus far we have not yet found evidence to adapt this allocation strategy for the case of in situ floodplain monitoring nonetheless from our analyses it is clear that a high measuring frequency yields a clear benefit compared to daily monitoring and the network size can remain limited as long as the locations are carefully selected it should further be noted that the average errors on forecasted water levels are rather small as compared to the open loop simulation i e up to a few centimetres which will allow flood managers to take targeted action in a worst case scenario a water level rise of 1 cm could result in a flood extent change of around 77600 m2 or equivalently 0 94 of the maximum true flood extent of this case study although we aim to reflect a realistic scenario these optimistic results may in part be due to the synthetic nature of this study application of the da setup in real case scenarios where less suitable data are available would therefore be indispensable to generalize our conclusions 3 3 impact of sensor placement and measuring frequency on the improvement persistency regardless of the numerical improvement assigned to a configuration it may be even more important to know how long this improvement persists and how it behaves in between two assimilation time steps ideally the improvement of a reliable configuration should persist up until the subsequent analysis time step and its behaviour should be predictable over time the hydrograph of a flood inundation event is characterized by a rising and a receding curve each of which has its specific properties gobeyn et al 2017 for this reason it is expected that the data assimilation process affects each of the curves differently in the following analysis both curves are therefore treated separately for each time step the spatially averaged rmse is determined for both the assimilation run rmseda and the open loop run rmseol and are subsequently merged into a single score the relative rmse reduction after madsen et al 2003 19 r m s e r e l 1 r m s e d a r m s e o l if we then extract all time windows between two consecutive assimilation steps and average the rmserel for corresponding time lags of those windows we obtain the average behaviour of the da forecast skill between two assimilation steps the relative rmse reduction can range from to 1 a value of 1 corresponds to a perfect match between the assimilation and the truth conversely values lower than 0 occur when the assimilation run no longer confers a benefit over the open loop prediction to eliminate the effect of observational and model uncertainties as well as varying network sizes the following analysis is performed for ϵ o 0 05 m eq 10 ϵ m 0 3 eq 4 and n 5 for this setup the results of the relative rmse for the rising and the recession curve are shown in respectively panel a and b of fig 7 as expected the behaviour of both curves is noticeably different the rising curve shows a monotonically decreasing rmse reduction whereas for the recession curve improvement increases up to 3 h after the analysis time step followed by a strong decrease to a minimum this phenomenon can be attributed to the mismatch between the ensemble spread and the observation variance during the rising of the storm the increase in floodplain water levels causes the water level uncertainty to raise unlike the more stagnant water level conditions during the recession characterized by a smaller uncertainty at the time of assimilation the imbalance between the model ensemble spread and the fixed observation variance may generate hydraulically inconsistent water levels which for this setup is most notable during the recession of the flood when the imbalance is largest subsequent flood forecasting redistributes inconsistent water levels over the model domain thereby increasing the performance temporarily moreover if the assimilation frequency is high the inconsistencies are unable to restore in time for that reason assimilation could be performed at a lower frequency if the imbalance between model and observational uncertainty is too large in general the persistency of the assimilation improvements is largest and most stable for the rising curve this result ties well with previous studies wherein pre flood peak srs observations are preferred for assimilation and calibration gobeyn et al 2017 garcía pintado et al 2013 even though the assimilation persistence is similar for all observation configurations some differences are yet to be noted as before the configurations of vor and eucl show the best results and remain quite stable in time in contrast cos performs worse but retains the observed persistence trend in other words the differences observed for the relative rmse are mainly due to the differences in performance as shown in fig 5 rather than the persistence of these performances from this analysis we can deduce that the allocation of observations should primarily be validated based on the overall data assimilation skill during the rising storm as the largest differences in forecast skill between configurations predominantly occur during this part of the flood event from the perspective of flood warning and mitigation modelling the upcoming of the storm event is in any case most crucial 3 4 performance of individual observations within their network from section 3 2 it was clear that the da performance between configurations can greatly differ and the same was encountered for their uncertainty in an attempt to explain these differences we investigated the persistence of their improvements but the results proved inconclusive in a following analysis the observation network is no longer viewed in a holistic manner instead each observation within a network is treated separately with the aim of finding a causal link between the individual performances and their joint performance these type of experiments are sometimes referred to as single observation impact experiments gustafsson et al 2014 gasperoni and wang 2015 in order to examine the individual observation impact on the forecast skill additional simulations were carried out that each assimilate an individual pixel from a network where previously a network of 5 observations would yield a single prediction this will now generate 5 separate predictions fig 8 shows the individual rmse performances dots relative to their combined performance areas for 3 cases that differ in their uncertainty and assimilation interval da1 ϵ o 0 05 m ϵ m 0 3 interval 1 h da2 ϵ o 0 05 m ϵ m 0 3 interval 24 h and da3 ϵ o 0 30 m ϵ m 0 3 interval 24 h respectively displayed in green blue and red and summarized in table 3 it is anticipated that the assimilation scenario da1 will likely perform better than da2 which in turn is expected to perform better than da3 if a dot is situated above the upper limit of its corresponding coloured area the joint assimilation outperforms the single observation assimilation conversely if a dot is located within or below the area of its colour assimilation of the individual observation is beneficial over the joint assimilation indicating that the network contains observations that worsen the joint performance the analysis was conducted for a network size n 5 and network configurations eucl vor and cos as those exhibited the largest performance differences the results of the rmse joint performances areas for each case in fig 8 are ranked as expected with da1 performing the best followed by da2 and da3 this ranking is hereafter referred to as behavioural however the same does not apply to the individual performances dots more specifically the da1 scenario seems to show a deviant behaviour for certain pixels whose rmse is significantly higher for da1 than for da2 and da3 this namely concerns pixel 5 from eucl pixels 1 2 and 3 from cos and pixels 2 and 3 from vor as can be seen from fig 4 inundation for these non behavioural pixels starts relatively late compared to the other domain pixels how this relates to the reduced performance of the non behavioural pixels can be explained as follows once the assimilation procedure starts observations are generated from the truth and subsequently assimilated however in the event that one or more ensemble members of a particular state prematurely show inundation at an analysis time step the assimilation procedure is initiated and observations have to be generated from a zero truth water level considering that on the one hand observations with a zero water level are also associated with uncertainty and on the other hand no negative water levels should be assimilated for a short period of time a positive bias is introduced onto the observations this can in turn cause other domain pixels to be inadequately adjusted and hence influence the overall model results moreover as soon as this temporary bias is present our enkf setup does not allow for flooded locations to reduce their water level back to zero this is caused by the complex interaction between domain pixels that continuously redistribute water across the floodplain and again by the uncertainty that is inherently associated with the observations the phenomenon described above is amplified in the case of scenario da1 due to its high assimilation frequency and its low observational uncertainty first the high assimilation frequency will soon detect premature flooding of the ensemble and subsequently cause the observational bias to be introduced at an early stage second the high trust given to the observations σ o 0 05 m for da1 as compared to σ o 0 30 m for da3 will substantially contribute to this initial bias since more weight is given to the observations of da1 as opposed to scenario da3 which will therefore encourage water to be erroneously retained in the floodplain to overcome these issues an assimilation framework should be developed to allow water volumes to be completely removed from the system and to avoid the problem of biased observations in case of zero water level observations to conclude the poor performance of certain networks can be attributed to late inundation times the allocation of observations should therefore be prioritized to the areas that are most prone to flooding usually where the elevation is low the majority of the dots in fig 8 are located above their corresponding shaded area however for the eucl and the cos configurations a few exceptions are made though almost exclusively for the da3 scenario it is striking that in some cases a single observation assimilation outperforms the assimilation of the full set that the single observation belongs to this discrepancy could be explained by the high observational uncertainty associated with da3 at time steps of low ensemble spread the high observational uncertainty could cause inconsistencies in the floodplain water levels just like mentioned for the receding curve in fig 7 this in turn leads to a degraded performance all other dots are located rather close to the upper limit of their respective shaded area in this respect it is important to investigate the added value of an observation within its set both spatially and in time these added values will be discussed in the next section 3 5 contributions of individual observations in time and space at this point the potential of in situ floodplain monitoring has been successfully demonstrated in addition some guidelines on the allocation of observations have been described from the foregoing analyses yet one key point is still missing namely how the forecast skill of an observation behaves in time and space this aspect is covered in this section previously it was pointed out that observation networks may consist of two types of observation locations behavioural and non behavioural ones depending on their performance rank of 3 da scenarios an empirical correlation between the number of non behavioural locations and the overall score of a particular configuration was established in order to respond to the challenges of observation allocation a thorough understanding of this relation should be gained additional experiments on the individual observation contributions are therefore conducted by means of scenario da2 considering that on the one hand its daily assimilation frequency ensures stable model results and on the other hand its observational uncertainty of 0 05 m reflects realistic in situ measurement errors in this final analysis the contribution of each observation within its network is compared according to the procedure described in section 2 3 1 for each analysis time step zi identifies the observation that induces the highest assimilation update for model state i in an attempt to identify the contribution of a particular observation to the entire model space zi is determined for all model states m the spatial contribution of an observation is then defined by the fraction of model states for which zi is represented by the considered observation as such this spatial fraction reflects the geographical influence of the observation if these calculations are repeated for every analysis time step the contribution of each observation is not only known in space but also in time for the case of da2 this implies that the contributions are assessed on a daily basis fig 9 demonstrates the contribution of each observation as a function of time and is expressed by the spatial fraction outlined above the colours assigned to the observations are in accordance with fig 4 at some time steps no fractions are displayed the reason is that neither the model ensemble nor the truth yet experience flooding at the observation locations considered no assimilation occurs in this situation fig 9 reveals a clear time dependence of the observation contributions for example observation 1 of eucl has a strong contribution during the rising of the storm which decreases over time the contrary is true for observation 4 whose contribution increases during the recession of the storm observations 3 and 5 show less a temporal trend while the added value of observation 2 is limited similar dynamics are encountered for the vor configuration although less pronounced in this case all 5 observations have a significant impact on the model state updates conversely the consequence of suboptimal observation allocation is clearly observed for the configuration of cos only 2 out of 5 observations have a profound impact on the updates averaging all spatial fractions over time allows to adequately compare different configurations and setups depicted on the right side of fig 9 from this it seems that configurations with a high forecast skill eucl vor are associated with an observation network in which most of the observations affect the water level estimates significantly whether that is in time or space further it can be noticed that for fixed time steps the vast majority of the bar plots consists of 1 to 3 observations that significantly influence the model results evidently the magnitude of the fractions is proportional to the spatial contribution of each observation moreover these contributions tend to occur clustered over the model domain results not shown the above findings suggest that when allocating observations the best practice is to select those that have a significant contribution in both time and space and ideally are complementary as a function of time the foregoing analysis was carried out for a network size of 5 observations previously assumed to be sufficient for this case study section 3 2 we can however extend this analysis to a variable network size in order to establish a link between the network size and the amount of observations that significantly contributes to the assimilation for each configuration technique observation networks of size 1 20 are designed and implemented with the same assimilation setup da2 as for fig 9 in case n 20 and all observations equally contribute each observation can reach a maximal fraction of 0 05 expressed as a spatial fraction therefore a fixed threshold of 5 is suggested for an observation to significantly contribute each dot in fig 10 represents a unique combination of configuration and network size due to the discrete nature of this exercise dots may overlap in that case the transparency of the dot is decreased the figure reveals a clear link between the amount of contributing observations and the network size even though the network size increases the number of contributing observations stagnates around a maximum of 5 7 contributing observations our assumption that a network size n 5 could be sufficient for this case study albeit on the condition that the observation locations are well chosen is hereby confirmed 4 conclusions this study explores the feasibility of using in situ floodplain water level observations as an alternative source of distributed information for flood monitoring our aim is to evaluate the potential of a sparse sensor network in a data assimilation framework and to provide general guidelines relating to the sensor positioning and assimilation setup the lisflood fp flood inundation model was used to simulate a real case flood event that is employed in the enkf data assimilation framework in order to investigate the impact of sensor positioning on the forecast skill multiple observation networks were designed through cluster analysis this involves the merging of floodplain water level time series by means of 3 similarity measures euclidean cosine and mahalanobis distance subsequently each cluster is assigned a representative state the observation in addition observation networks were also designed according to inundation time clustering as well as a voronoi decomposition in a first stage we assessed the overall performance of the study design for different network configurations and assimilation setups without updating the spatio temporal rmse of the forecasts equals 7 93 cm while in optimal assimilation conditions it drops below 2 cm it is obvious that low assimilation frequencies and high model and or observational uncertainties generally degrade the performance the results indicated that the da performance is also affected by the network configuration in an attempt to address this latter dependence the positive impact of each configuration after assimilation was studied although it was revealed that a positive impact is relatively more persistent during the storm rise than for the recession of the storm no clear differences among configurations were detected in a second stage the impact of individual observations on the forecast skill was investigated relative to their joint performance it was found that a joint network assimilation mostly outperforms the assimilation of an individual observation from that network a low performance of an individual observation was associated with late inundation times in a few cases however a single observation could outperform the joint network it was believed that this is caused by suboptimal assimilation conditions rather than configuration issues for example the assimilation of multiple highly uncertain observations could result in water level inconsistencies that decrease the performance in a final stage a more in depth analysis was carried out on the individual observation contributions in both space and time measured by the magnitude of the updates induced by each observation a good quality observation network consists of observations that contribute complementary in both space and time that is if an observation provides only useful information during storm rise it should be complemented with an observation that induces improvements during the recession furthermore a clear link between the network size and the amount of contributing observations was established from this it was clear that a network size of 5 observations would suffice for our flood case study only if the observation locations are carefully chosen that is choosing locations that are prone to early flooding and provide complementary information in both time and space in general our results show great potential for the use of an in situ floodplain sensor network although the information is only sparsely distributed it is a very cost effective way for flood monitoring practices it may provide data for ungauged areas or complement existing satellite sensor data therefore it is important to carefully design a sensor network and evaluate the assimilation setup the research presented in this paper makes it possible to follow some guidelines regarding the establishment of an observation network in the floodplain however it is important to keep in mind that this study is of a synthetic nature and the entire data assimilation setup was controlled firstly although several assimilation parameters have been throrougly studied not all sources of uncertainty were accounted for such as errors in the dem and bathymetry secondly the observations were derived from a synthetic truth which may have led to optimistic results lastly this truth also allowed us to examine the observation network dynamics in advance which would have been impossible in real case studies therefore in order to generalize our conclusions this study should be verified for real case flood events of different magnitudes to develop an optimal fixed observation network acknowledgements the authors are grateful to environment agency ea of england and wales for providing the lidar digital elevation data and the hydrometric data the computational resources stevin supercomputer infrastructure and services used in this work were provided by the vsc flemish supercomputer center funded by ghent university fwo research foundation flanders and the flemish government support was given by a phd fund of the bijzonder onderzoeksfonds bof of ghent university grant no 01j04015 who funded the work in all its stages lisa landuyt is a doctoral research fellow of the fwo and received funding through grant g 0179 16n 
616,reliable flood forecasting systems are the prerequisite for proper flood warning systems currently satellite remote sensing srs observations are widely used to improve model forecasts although they provide distributed information they are sometimes unable to satisfy flood modellers needs due to low overpass frequencies and high measuring uncertainties this paper assesses the potential of sparsely distributed in situ floodplain water level sensors to provide accurate near real time flood information as a means to enhance flood predictions a synthetic twin experiment evaluates the assimilation of different sensor network configurations designed through time series clustering and voronoi spacing with spatio temporal rmses reaching up to 1 cm the study demonstrates great potential adequate sensor placement proved crucial for improved performance in practice observation locations should be chosen such that they are located rather close to the river to increase the likelihood of early flooding and thus acquiring valuable information at an early stage of flooding furthermore high measuring frequencies benefit the simulations though one should be careful not to overcorrect water levels as these may result in inconsistencies lastly a network size of 5 to 7 observations yields good results while an increasing number of observations generally diminishes the importance of extra observations our findings could greatly contribute to future flood observing systems to either compensate for ungauged areas or complement current srs practices keywords flood monitoring data assimilation observation network 1 introduction changing climate conditions have undeniably affected the extremity of weather events of which floods are only one out of many gaume et al 2016 bryndal et al 2017 ummenhofer and meehl 2017 in a constant search for better flood warning systems much effort has been put into the development and improvement of flood inundation models hybrid models that combine 1d channel flow with 2d floodplain routing such as lisflood fp bates and de roo 2000 neal et al 2012 and mike flood patro et al 2009 have proven to be successful yet every model is only an approximation of nature and is therefore imperfect the mike flood model includes the boussinesq approximation to reproduce real flood dynamics which contrasts with the simpler inertial approximation used by the lisflood fp model each of the models inability of capturing the true dynamics should be accounted for by quantifying their uncertainties bates et al 2014 factors contributing to the total predictive uncertainty may include amongst others model equations and parameters errors in the digital elevation model dem parameterisation of sub grid physics and domain discretisation as this uncertainty might prevent flood managers from taking targeted action morss et al 2001 it is of paramount importance to reduce it this can be achieved by integrating observational information into the prediction hence constraining the prediction by means of both model and observations it should however be noted that observational information is also associated with uncertainty and this should be accounted for whether this includes systematic errors instrument error or random errors noise neppel et al 2010 di baldassarre et al 2012 this technique is known as data assimilation da and is widely used in environmental sciences for predictive analysis reichle 2008 carrassi et al 2018 in short da merges model states and observations thereby computing a weighted average of both states which is then considered as a new best estimate multiple studies have shown the potential of the joint use of a flood inundation model and observations with respect to calibration validation studies aronica et al 2002 horritt and bates 2002 schumann et al 2014 gobeyn et al 2017 during the last 20 years data assimilation has gained much attention within the field of flood inundation modelling in particular studies first focussed on the use of river observations measured or derived from in situ gauging stations madsen et al 2003 assimilated water levels of three river gauging stations thereby updating initial conditions at forecast time the flood forecast was subsequently improved for lead times up to 48 h other studies have focussed on further improving the assimilation procedure by either describing the spatial distribution of error statistics madsen and skotner 2005 or more recently defining the impact of sensor locations on the forecast skill mazzoleni et al 2015 given that the use of in situ river observations has always been of common practice barthélémy et al 2017 mazzoleni et al 2017 ricci et al 2011 in situ floodplain observations have only been used occasionally for instance madsen et al 2006 used a set of river and floodplain measurements as a validation set for a synthetic assimilation study although hartnack et al 2005 have suggested the potential use of in situ floodplain observations in assimilation studies neal et al 2007 were the first to effectively assimilate a combination of river and floodplain water levels into a hydraulic model for a small scale da experiment different sampling designs improved overall accuracy albeit with varying degrees of success furthermore the complementarity of river and floodplain sensors was investigated showing that river stage observations slightly outperform floodplain observations for updating the hydraulic model to the authors knowledge no study has yet investigated the optimal placement of sparse in situ floodplain sensors following the global decline of river gauging stations fekete and vörösmarty 2007 along with the need for distributed knowledge bates 2004 attention was shifted towards the use of satellite remote sensing srs observations to date many studies have demonstrated the use of synthetic aperture radar sar observations in the river andreadis and schumann 2014 giustarini et al 2011 matgen et al 2007a 2010 and floodplain hostache et al 2010a 2010b 2018 cooper et al 2018 into a da framework for flood monitoring along with this increasing amount of data ridler et al 2014 the efficient use of srs observations has been of growing interest a large number of studies have been devoted to the use of so called thinning strategies where redundant information is removed from the assimilation process for reasons of high computational cost ochotta et al 2005 increased water level uncertainty di baldassarre et al 2011 or the lack of reliable estimates of observation error covariances bormann and bauer 2010 in spite of varying degrees of success when assimilating srs observations it is important to realize their use suffers from drawbacks first considerable errors remain when water levels are extracted from sar imagery di baldassarre et al 2009 stephens et al 2012 schumann et al 2008 matgen et al 2010 2007a garcía pintado et al 2015 dasgupta et al 2018 second in some cases the impact of assimilation vanishes a few hours after acquisition although matgen et al 2007b point out that for large basins with a slowly changing rising or recession limb the impact is more persistent last the forecast skill seems to be proportional to the time interval between satellite overpasses andreadis et al 2007 showing the need for frequent measurements if continuous flood monitoring is desired as a result several authors emphasize that remotely sensed observations should not replace in situ measurements neal et al 2009 instead a complementary use of both ground data and remotely sensed observations is suggested matgen et al 2010 schumann et al 2009a in summary the evolution towards the use of srs observations in da studies is still ongoing however it is clear that the potential of in situ measurements should not be underestimated whereas previous work has mainly focused on in situ river observations the impact of in situ floodplain observations on the forecast skill remains unexplored to fill this gap this paper presents the assimilation of different floodplain observation configurations to improve or complement current flood prediction practices our key objective is to evaluate the overall da performance of several configurations and thereby offering guidance on the allocation and size of such an in situ network in addition we provide insight into the individual contribution of each observation within a network both in time and space hence we are able to identify which networks contain either mutually reinforcing observations or redundant information and as such to explain why some networks perform better than others lastly the assimilation frequency is varied in order to identify the impact of observation frequency on the persistency of the improvement for different networks the proof of concept is elaborated by means of a synthetic twin experiment which allows us to theoretically test the da setup without the need for real observations altogether the explorative nature of this research thus seeks to offer a basis for real case implementations to conclude the purpose of our investigation is not to advocate a replacement of current river gauging networks but rather to provide an insight and recommendation on the use of in situ floodplain networks eventually one could use an in situ floodplain network either alongside with srs observations as a compensation for poorly gauged areas mazzoleni et al 2017 2018 or as a single observation source in case river gauging stations no longer operate due to extreme flash floods aronica et al 2002 this paper is organised as follows section 2 outlines the design of the study and describes the flood inundation model the available data and the data assimilation algorithm the results are presented in section 3 and conclusions are drawn in section 4 2 methodology 2 1 study site and data in the snowdonia region of north wales the river dee originates and flows eastward through the steep wooded valleys of its upper reach serving as a natural boundary between england and wales the lower reach meanders in a flat topography before crossing the city of chester and flowing into the irish sea the overall length of the river dee is 110 km and its catchment area covers 1816 8 km2 the river dee was the first uk river to be classified as a water protection zone following a pollution incident in 1999 nowadays the river dee provides drinking water for over 3 million persons the modelled area of this study covers 40 km2 and includes the confluence of an 8 km reach of the river alyn with a 10 km section of the river dee s lower reach the average discharge rate of this lower reach is around 30 m3 s the river alyn has an average width of 12 m while the river dee is 30 m wide in average in contrast to the hilly upper reach the downstream floodplains have an average slope of only 0 005 grimaldi et al 2016 and are characterized by steep side slopes resulting in a valley filling event when flooded gobeyn et al 2017 owing to this flat topography the river floodplain can stretch up to more than 2 km and is mainly covered by arable land a lidar dem of 20 m was provided by the ea environment agency of england and wales and post processed in order to eliminate artificial artefacts of the area fig 1 although the river dee is well regulated it has experienced frequent flood events macdonald and sangster 2017 in december 2006 a 2 year flood event occurred of which the peak river discharge on december 8 caused an area of 8 3 km2 to be flooded this event has been of particular interest to many researchers because of the availability of satellite sar images acquired just after peak flood di baldassarre et al 2009 stephens et al 2012 gobeyn et al 2017 a visual delineation of this image s flood extent by schumann et al 2009b is also depicted in fig 1 the flood event of 2006 will also be used for our research purpose gauged water flow and stage data from november 18 until december 29 up and downstream of the study area are provided by the ea with a time resolution of 15 min in total three gauging stations cover the study area farndon dee ironbridge dee and pont y capel alyn named after a nearby city or construction 2 2 experimental design in this study the potential of in situ flood monitoring for improving model predictions through data assimilation experiments is tested from river forcings a set of roughness parameters and topographic information a flood inundation model is able to simulate floodplain water levels our state variables of interest these state variables will be synchronously updated by water level observations from a sparse in situ observation network apart from the current river gauging stations no additional stations yet exist in the floodplain area of interest as a result no validation data are available on the water levels in the floodplain since these are crucial in performance testing a synthetic case is used to examine the study object the experimental design of this study is based on an identical twin experiment masutani et al 2010 privé et al 2013 it offers a controlled setting for the evaluation of a data assimilation setup typically such experiment consists of 3 model runs a control run truth a free forecast run without observation integration open loop or background and an assimilation run analysis raicich and rampazzo 2003 other than for real case scenarios it is possible to design this experiment in such a way that a number of key complexities can be avoided first if all runs are simulated by the same model model structural errors can be neglected for comparative analysis second when both observations and forecasts are derived from the truth run through perturbed model initialisations this setup may also eliminate the presence of a possible model observation bias sørensen et al 2004 synthetic experiments are therefore very skillful for experimental control it is possible to not only identify the behaviour of da systems matgen et al 2010 andreadis and schumann 2014 but also to create different designs for the observing system privé et al 2013 if the synthetic da setup is successfully evaluated by proof of principle it should be further tested with real case data in an operational context within the area of flood inundation modelling many studies have demonstrated the use of a synthetic twin experiment for different research purposes sørensen et al 2004 hartnack et al 2005 madsen et al 2006 andreadis et al 2007 matgen et al 2010 garcía pintado et al 2013 andreadis and schumann 2014 gobeyn et al 2017 cooper et al 2018 in this study we investigate the potential of in situ floodplain observations in a da framework since the states of interest are unobserved they are synthetically generated by a flood inundation model that simulates floodplain f dynamics in terms of water level states ψ using river r flow boundary conditions q fig 2 consequently the aforementioned runs that constitute the twin experiment can be simulated with the use of river measurements from 3 gauging stations see section 2 1 first the truth run comprises an input qtruth and output ψ truth state linked by a dynamical system i e the model no uncertainty is associated with the truth secondly in order to obtain a model forecast an ensemble of inputs qens derived from the truth input using perturbations p is run through the same model yielding an ensemble of output states ψ f the ensemble average constitutes the open loop model prediction along with its uncertainty represented by the ensemble variance in this study we will use 32 ensemble members lastly for this prediction to improve observations are integrated into the da system in our design the observations ψ obs correspond to the truth output to which a gaussian error is added in order to represent the impact of observational uncertainty at a fixed frequency the data assimilation procedure combines the background model state and the synthetically generated observations taking into account not only their numerical values but also their respective uncertainties outlined in sections 2 2 2 and 2 2 3 this results in improved model states ψ a that will replace the old model states for subsequent forecasting until new observations become available for the next assimilation stage 2 2 1 flood inundation modelling the river dee flood event is simulated with the coupled 1d 2d lisflood fp flood inundation model it was first developed by bates et al 2010 with the aim of accurately reproducing flood events using a minimal representation of dynamical processes river flow is simulated by an approximation of the 1d de st venant equations 1 q x a t q 2 h x n 2 p 4 3 q 2 a 10 3 s 0 where q m3s 1 is the discharge a m2 the water cross section area x m the distance measured along the thalweg t s the time q m3s 1 additional flow from other sources to be set 0 for this case study s 0 the channel bed slope also approximated by the water surface slope n the manning friction coefficient p m the wetted perimeter for simplification the channel cross sections are assumed rectangular which is reasonable for rivers that are wide and shallow neal et al 2015 floodplain routing between two adjacent cells is numerically computed on a 2d raster grid by the manning formula horritt and bates 2002 3 q x i j h f l o w 5 3 n f p h i 1 j h i j δ x 1 2 δ y where h i j m is the water free surface height at node i j of the grid and hflow is defined as the difference between the highest water free surface of both cells and the highest bed elevation nfp is the manning floodplain coefficient and δx m and δy m are grid dimensions for the y dimension q y i j is defined in a similar way when the river water reaches bankful depth water flows into the adjacent floodplain cells along the river reach water is then routed through the floodplain by gravitational forcing using information from the dem each grid cell acts as a storage volume for which a mass balance is computed at every time step bates et al 2010 as a result the lisflood fp model computes water level time series for each grid in the study area using forcing variables parameters and topographic information both forcing variables as well as topographic information were provided by the ea the time resolution of the input forcing is 15 min whereas results were only retrieved on an hourly basis to reduce storage costs the spatial resolution of the model was restricted by the dem resolution and was set at 20 m following guidelines in chow et al 1988 and a former calibration study that used the lisflood fp model for the same case study true channel and floodplain manning coefficients were fixed at 0 06 and 0 12 respectively gobeyn et al 2017 wood et al 2016 according to hostache et al 2010a spatially distributed manning coefficients do not improve results significantly therefore both coefficients are considered to be constant in space and time the model was run in subgrid channel mode making it possible for subgrid scale river sections to be modelled neal et al 2012 2 2 2 generation of the ensemble model state assessment of the model state uncertainty usually requires the definition of many sources of uncertainty such as input state uncertainty parameter uncertainty and model structural error although errors in the dem and bathymetry could significantly affect model results these errors are disregarded in this synthetic study instead we focus on the river input and boundary state uncertainty as well as parameter uncertainty and assume a perfect model structure bathymetry and dem similar to gobeyn et al 2017 as such representation of the model state uncertainty requires model simulations with perturbed parameter sets and perturbed input states toth and kalnay 1993 the true parameter set adopted from section 2 2 1 is perturbed by gaussian noise with a standard deviation for the channel and floodplain manning coefficients of 0 01 respectively 0 02 perturbation of the true input states is performed using an autoregressive ar error model evensen 2003 4 ϵ j α ϵ j 1 1 α 2 w j where ϵ is the input error at any time j 1 given that the initial value of ϵ0 is known α is the temporal autocorrelation coefficient and w j n 0 σ w 2 is a white noise term like many error models this ar model consists of a deterministic component relating to the memory of the process red noise and a stochastic component white noise their accompanying weights determine how they relate both weights depend on the temporal autocorrelation coefficient α represented by an exponential decay garcía pintado et al 2013 5 α e δ t j τ where δtj s is the length of the time interval between two time steps j 1 and j and τ s is the decorrelation length of the forcing variable in this study it is assumed that the temporal autocorrelation of the flow error is equal to that of the flow for time series of river flow and river level data the decorrelation length is the length of a time frame where starting and ending point have significantly decreased autocorrelation for single event based cases a flood mostly has a long memory correlation due to the persistence of the process water level time series in flood conditions often consist of a rising and receding limb therefore holding a high positive autocorrelation of the floodplain water level time series in the first half of the event switching to a high negative autocorrelation in the second half bartl et al 2009 in order to make a best estimate of the decorrelation length we take into account a the first time when decorrelation temporarily occurs and b the stability of acquired perturbations the time decorrelation length was set at 3 days or 288 model time steps identical to what was obtained by garcía pintado et al 2013 the established error time series can be implemented either in a multiplicative or an additive way depending on the nature of the variable in the case of river flow the errors increase with increasing discharge rates while for water levels errors remain constant as such discharge errors are mostly expressed as percentage of flow while water level errors are expressed in metres for a discharge time series q the perturbed time series q is obtained as follows 6 q q 1 ϵ from this equation it is clear that ϵ projects the model uncertainty hereafter referred to as ϵ m recall from eq 4 that this error time series is dependent on the values of α and w that can be chosen such that the desired model error is obtained in this study multiplicative flow error values of q 10 20 30 and 40 or equivalently ϵ m equal to 0 1 0 2 0 3 and 0 4 are assumed for analysis and listed in table 1 in contrast for the water levels of ironbridge station errors are introduced in an additive way and are represented as standard deviations varying from 1 2 3 to 4 cm also listed in table 1 although many studies on river flow uncertainty mention flow error values of 10 20 madsen et al 2006 di baldassarre and montanari 2009 hartnack et al 2005 sikorska et al 2013 and equivalently accuracies of 1 2 cm for water levels lang et al 2010 pappenberger et al 2006 some authors report higher uncertainties up to a flow error of 80 and a water level error of 10 cm in unsteady conditions mcmillan et al 2012 coxon et al 2015 madsen et al 2003 we therefore take into account a range of plausible values the upper panels of fig 3 illustrate the inflow discharge and outflow water level time series black for all gauging stations together with the ensemble forecasts grey derived from the truth state with ϵ m equal to 0 3 the bottom panels display the distributed floodplain water levels at certain points in time 2 2 3 assessment of the synthetic observations only a few studies have examined the potential of a floodplain in situ observation network for flood monitoring neal et al 2007 werner et al 2005 cooper et al 2018 however none of them have formulated the optimal configuration of sparse observation locations in this study we explore the impact of different observation configurations on the da forecast skill to provide guidelines on the allocation of floodplain water level sensors since no sensors are yet located in the floodplain synthetic water level observations are derived from the model truth output by drawing samples from a gaussian distribution with its mean equal to the true state and a standard deviation ϵ o that is varied throughout analysis 1 5 10 15 20 25 up to 30 cm table 1 the authors believe that those values reflect a range of conditions from steady flood routing to unsteady water surface conditions that real case scenarios could deal with 2 3 data assimilation a data assimilation da procedure aims at combining different sources of information generally a prior model forecast and observations to obtain a new best estimate of a system state generally the prior knowledge is represented by a vector of m predicted states ψf that is merged with d a set of n observations the da procedure subsequently creates a posterior state vector ψa that is hoped to be a better representation of the truth ψ truth the ensemble kalman filter enkf uses a collection of n model realisations ensemble members to represent the prior model uncertainty denoted as the forecast state vector ψ f ψ 1 f ψ 2 f ψ n f t m n evensen 2003 burgers et al 1998 katzfuss et al 2016 likewise a vector of noisy observations d d 1 d 2 d n t n n is used to mimic the observational uncertainty burgers et al 1998 combining both uncertain sources of information constrains the prior forecast resulting in the analysis state vector ψ a ψ a 1 ψ a 2 ψ a n t m n the analysis scheme of the enkf for an ensemble size equal to n is represented in the state space as follows 7 ψ a ψ f k d h ψ f h n m is the forward operator that performs mapping from the model space to the observation space houtekamer and zhang 2016 if the state variables of interest are directly observed the elements of h can take either of two values 0 or 1 k m n is the kalman gain matrix that in itself is a function of both the background error covariance matrix p m m and the observation error covariance matrix r n n 8 k p h t h p h t r 1 9 p ψ f ψ f ψ f ψ f t 10 r ϵ o ϵ o t where ϵ o assigns errors to the measurements details on the background and observation error covariances are discussed in section 2 3 1 regardless of the simplicity and flexibility of the enkf its application requires the gaussian assumption for error statistics that cannot always be met however in our synthetic case errors are controlled and all assumptions are valid 2 3 1 on the kalman gain it was shown that uncertain flood forecasts can be constrained by the help of observations this study considers the observations to be sparsely distributed along the floodplain as a consequence various observation configurations may each influence the assimilation procedure differently in view of the continuous search for improved flood forecasting systems the main goal of this study is thus to understand how different observation configurations influence the forecasts and more importantly what an adequate configuration should look like following eq 7 the right hand sided element k d h ψ f is known as the increment and represents the discrepancy between the observed states and their corresponding model estimates weighted by k the forecast skill of the da setup relies on the appropriate definition of k in order to improve the model estimates therefore the elements of k are also referred to as weights or gain values furthermore the kalman gain matrix reflects the relative magnitudes of the measurement error r on the one hand and the model error p on the other hand eq 8 depending on the trust that is given to either the model or the measurements where the trust is inversely proportional to the uncertainty on the model states or the observations the forecast will be updated accordingly for example if the observational uncertainty is small compared to the model uncertainty values for k will tend to be large and updates will favour the observations how these values can be attributed to both the model and the measurement covariance matrices will be discussed in the next two paragraphs first the measurement covariance matrix is evaluated in the case of independent observation acquisitions such as for a sparse in situ network the observation errors are uncorrelated in both space and time if we assume an equal instrument error for all observations the covariance matrix r reduces to a diagonal matrix moreover if different observation configurations in this study are to be compared r will be kept constant in our analysis as such the impact of the background error covariance matrix p on the forecast skill can be independently investigated secondly the error covariance matrix p reflects how variables co vary in terms of an ensemble we refer to the ensemble error covariance matrix p from eq 9 it can be derived that the magnitude of the covariance between two ensemble states is determined by their individual ensemble variance while its sign is determined by their correlation the covariance matrix p thus consists of ensemble variances along its main diagonal and ensemble covariances between each pair of locations in the other matrix positions for example if two locations show a similarly large ensemble variance but their behaviour is inversely proportional the covariance will tend to be large and negative in short this covariance matrix identifies how pairs of state variables in the study domain mutually behave and is used to define the kalman gain matrix k in practice each of the columns of k consists of m elements or gains one of which points to the observed state as a result the magnitude of each element of k will determine to what degree a specific observation will contribute in the updating of forecasts thus k is composed of elements k i j that update a domain pixel i 1 2 m in accordance to an observed pixel j 1 2 n 11 k m n k 1 1 k 1 2 k 1 j k 1 n k 2 1 k 2 2 k 2 j k 2 n k i 1 k i 2 k i j k i n k m 1 k m 2 k m j k m n based upon the previous findings for each pixel i there exist n weights k i 1 k i 2 k i n that each contribute to the updating of the pixel or in other words the contribution of each observation j towards every domain pixel is evaluated with the weight vector k 1 j k 2 j k m j t in order to investigate which observation j contributes most to the model updates we have to rearrange and decompose eq 7 into element wise products 12 ψ i j a ψ i j f k i j d j if we now expand this equation for n ensemble members and we wish to assess for a single state i the mean absolute contribution z i j of the ensemble for a single observation j we may calculate 13 z i j 1 n e 1 n k i j d j e where d n n represents the innovation matrix for an entire ensemble of size n to avoid the cancellation of contributions due to positive and negative values of k absolute values are used in the equation finally if one wants to know which of the n observations contributes most in updating a particular state we search for the index j that provides the largest value for z i j 14 z i max j 1 n e 1 n k i j d j e the above elaboration validates for a single assimilation time step if we extend this reasoning for multiple time steps during the assimilation procedure the value of zi is acquired at different time steps this will allow us to obtain an overall view on the individual contribution of observations not only spatially but also in time bearing this in mind such analysis may identify the presence of complementary or redundant sensors in an observation network and provide guidance for real case scenarios 2 4 observation network design in view of the increasing threat of floods certain flood prone areas have been subjected to the assessment of a relatively dense stationary network of river gauging stations where water level and or discharge is recorded to our knowledge no directed planning on the allocation of floodplain sensors has yet been explored similarly to what is required for the allocation of river sensors floodplain sensors should in the first place be easily accessible and installed on a stable subsurface in addition locations should exclude among other things steep slopes and tall vegetation as those may hamper reliable sensor reading further a dispersed sensor placement increases the chance of capturing both small and large scale flood events during recent years awareness arose on the importance of strategic planning that not only involves the allocation of a sensor network as introduced above but also allows a deeper scientific understanding of flood processes marsh and hannaford 2008 since flood routing is mainly driven by the floodplain topography local topographic variabilities may cause certain sensor configurations to be suboptimal moreover no research has yet focused on the potential complementarity of nearby observation locations in da experiments hence this study will explore the use of different observation configurations based on not only geographical information but also flood dynamics as the objective of this study is to evaluate the opportunities of flood prediction by means of in situ floodplain observations different design techniques will allocate observation networks intended to cover a range of possible configurations one of the applied techniques is based on the cluster analysis of a distance matrix liu et al 2013 to this end a similarity measure subjects water level time series of the floodplain true state to a pairwise comparison if each pair of the time series is assigned a value of similarity all pairs can compose a distance matrix a squared matrix of size m m then a cluster analysis identifies structures in the distance matrix that can be segmented into groups or clusters if two state variables exhibit analogous similarities they are likely to be merged in the same cluster in contrast if two state variables show a different behaviour they are likely to end up in different clusters where each cluster may have the tendency to provide complementary information this study proposes three different similarity measures each highlighting a different side of the flood dynamics the similarity measures between water level time series represented by vectors x and y are listed below 1 euclidean distance 15 d e x y t x t y t 2 2 cosine distance 16 d c x y t x t y t t x t 2 t y t 2 3 mahalanobis distance 17 d m x y x y t p d 1 x y where p d is the covariance matrix of the full dataset and t represents the transpose of the matrix the euclidean distance is widely used for time series analysis because it is simple robust and hard to beat serrà and arcos 2014 rather than accounting for numerical values the cosine distance evaluates vector directions zhang and lu 2003 the mahalanobis distance is a data driven measure that takes into account the covariance matrix p d that contains the covariances of all state variables calculated from the true floodplain water level time series it should be noted that this covariance matrix is different from the one used by the enkf p in that the latter is an ensemble based matrix calculated for a single analysis time step whereas the mahalanobis covariance is based on the truth simulation and determined for the full time range this study employs hierarchical agglomerative clustering hac a bottom up approach that starts with assigning a separate cluster to each datapoint and repeatedly merges similar clusters until the desired number of n homogeneous clusters remains liao 2005 the merging procedure is determined by the ward s method that minimizes the clusters overall sum of squared distances in this case the commonly chosen euclidean distance murtagh and legendre 2014 note that this clustering distance is different from the time series distance mentioned above when cluster assigning is completed cluster representatives are determined as the states with the highest within cluster correlation those representatives then constitute the observation network further this study also employs clustering based on multilevel thresholding of the initial inundation time of each of the states computed by the otsu algorithm otsu 1979 as such the state variables that initially flood between two fixed time steps are brought together in a cluster cluster representatives are chosen in the same way as described above lastly a voronoi decomposition is applied to the modelled study domain this technique partitions the area into polygons that contain domain points related by a user defined proximity function in this study the squared euclidean distance as a result the centroids of the clusters define a dispersed observation network and thus do not need any further processing the voronoi clustering is based on a geometrical spacing rather than on water level dynamics such as the aforementioned techniques hence the obtained observation network provides observations that are distributed evenly in space bandyopadhyay and coyle 2003 kao et al 2008 guruprasad 2011 2 5 evaluation of the data assimilation setup the impact of the data assimilation algorithm on the flood forecasts can be evaluated using performance measures in terms of state variables the truth ψ truth validates the ensemble mean assimilation run ψ a and depending on the measure of use may compare it to the mean forecast without updating ψ f this study not only investigates the potential of in situ floodplain monitoring within a da framework but also tests different observation network configurations designed following section 2 4 therefore a comparison of all network designs by means of evaluation measures could establish a link between the forecast skill of a network and the configuration of its sensors previous studies suggest a combined use of different performance measures as an integrated approach for the evaluation of the da impact chai and draxler 2014 nguyen et al 2016 we will therefore use 2 performance measures that each highlight a different interpretation of the forecast skill the rmse root mean squared error giustarini et al 2011 king et al 2014 computes the average floodplain water level error thereby penalizing for large errors the second measure the csi critical success index is a classification measure that involves a binary approach to a flood and is mainly applied in studies where satellite data are involved landuyt et al 2018 stephens et al 2014 values may range from 1 perfect classification to 0 all pixels misclassified its maximum value of 1 corresponds to a perfect match between the assimilation run and the truth however if its value drops to 0 or less the assimilation run scores equally well or worse than the open loop run both measures are computed for the full time range and averaged over the model domain as such they represent the spatio temporal performance of the flood forecasts the formulation of the rmse is referred to in the accompanying references the csi is computed following eq 18 18 c s i a a b c where a is the number of correctly classified pixels hits b the number of overpredicted pixels false alarms and c the number of underpredictions misses based on the contingency table shown in table 2 3 results and discussion the data assimilation setup outlined in section 2 2 is evaluated by a total of 2520 simulations performed with the lisflood fp flood inundation model each simulation arises from a unique combination of the variables listed in table 1 and is validated by the truth by means of different evaluation measures section 2 5 as such it is possible to identify simulations that either favour the predictions or degrade the performance as compared to the open loop prediction more specifically this study focuses on the impact of observation configuration on the da forecast skill to this end this section covers four main aspects of our data assimilation setup first the spatial distribution of several observation configurations and their forecast skill is explored then for each type of configuration the time interval after assimilation during which improved predictions are obtained is investigated in order to gain in depth knowledge on the forecast skill a third analysis on the individual observation contributions within the network is conducted as a function of both time and space table 3 provides an overview of the da setups that each analysis accompanied with a figure deals with 3 1 observation network configurations the allocation methods introduced in section 2 4 are implemented for 3 different network sizes and provide observations for the assimilation procedure in total 5 similarity measures were used to allocate the observations eucl cos mah initt and vor table 1 the top panels of fig 4 illustrate for each measure the position of n 5 observations relative to the river and the maximum flood extent that is reached by the truth run recall that the allocation methods make use of clusters whose highest time series intracorrelations select the observing locations since the majority of the observations is located close to the lower river reach high correlations seem to occur close to the river a more detailed analysis of all domain pixels reveals that high correlations indeed roughly correspond to locations close to the river results not shown moreover these correlations diminish as we move further towards the flood edge in other words the correlations seem to follow the flood dynamics this phenomenon is explained by the downstream overtopping of the river dee where the inundation begins followed by a wavefront propagation that gradually stretches the water outwards onto the floodplain due to the floodplain roughness the floodwave slows down and becomes more distorted the further the water is routed onto the floodplain it is thus likely that similar water level time series occur rather close to the river than close to the flood edge in the synthetic study of cooper et al 2018 the authors advocate the use of upstream floodplain observations in their assimilation experiment for a simplified domain however they do note that for more complex topographies strong correlations could also occur further down the river reach the bottom panels of fig 4 show the true water level time series for the observation locations displayed in the corresponding top figure frames most of the pixels exhibit rather similar flood dynamics though with a clear water level variability that is explained by the variations in the dem the peak flows of each network more or less coincide with a maximum peak shift of around 5 h for pixel 5 of the vor configuration it should also be noted that some pixels hardly show any inundation cos pixel 1 and 2 initt pixel 5 making their observational contribution to be questioned the time series similarity measures introduced in section 2 4 discern clusters from one another it follows that the cluster representatives time series are intended to exhibit strong dissimilarities however this turns out to not always be the case for the eucl distance the variance of water level time series was expected to be higher as is the case for the mah distance the cos distance was included to incorporate pixels with distinctly different dynamics however since the flood dynamics are similar for the entire domain the cos configuration could be regarded unsuitable for clustering in contrast the time series of the initt and vor configurations do reflect their intended design the initt configuration discerns pixels from their time of inundation while vor discerns pixels from their location moreover vor is the only configuration that shows a distributed coverage of the observations for now our goal is not to provide an established method for observation allocation but rather to obtain different network designs whose assimilation results can be studied hence the configurations are implemented as such without relocations 3 2 overall performance of in situ floodplain monitoring our primary aim is to assess the impact of different observation configurations on the performance of the da procedure however other features than configuration may also affect the performance such as the assimilation frequency and the observation network size in order to examine the impact of a feature its values are varied while other variables are held constant for each value of the control feature simulations are carried out that vary the remaining features from table 1 and constitute the uncertainty of a prediction as such this experiment can determine both the average performance of a prediction as well as its uncertainty in fig 5 each boxplot belongs to a certain network configuration and consists of 84 simulations varying in both the observational and model uncertainty as well as the observation network size the assimilation frequency is fixed for a daily a and an hourly b monitoring the rmse and csi performance measures evaluate the assimilated forecasts spatially averaged over all floodplain domain pixels excluding the observed pixels to obtain a single score the average rmse performance of each configuration method reveals that eucl initt and vor provide the best results conversely the overall score of cos suggests that this configuration is suboptimal for our study in the case of hourly monitoring the average rmse remains below 3 cm as compared to 5 cm for daily monitoring while the open loop score equals 7 93 cm similar conclusions can be drawn for the csi where hourly assimilation shows a distinct advantage over daily monitoring with average values of around 0 79 and 0 73 respectively and an open loop score of 0 65 it should also be noted that the csi measure becomes insensitive to water level changes when the flood reaches its valley filling effect mentioned in section 2 1 this may cause csi values to be close to 1 during the peak flow in spite of much research effort on the assimilation of distributed srs observations reports on the floodplain forecast skill are virtually non existent many of those studies have predominantly focused on the forecast skill of the river observations thus whilst little reliable information is available from literature we believe the achieved scores are promising further it can be noted that from the rmse the spread or uncertainty of a prediction generally increases as the performance of a configuration method itself declines for example in case of the hourly assimilation the uncertainty for eucl initt and vor in fig 5b is similar while for cos and to a lesser extent for mah the uncertainty is noticeably larger this does not apply for the csi index which is a classification measure causing its value only to be influenced by pixel misclassifications rather than numerical error variations lastly the boxplots show outliers for both assimilation frequencies their simulations arise from a high model and or observational uncertainty subjecting the floodplain water levels to inconsistencies which in turn may cause the simulations to perform worse than the open loop forecast in the case of daily monitoring besides this test was repeated for multiple frequencies table 1 from which it followed that an hourly measuring frequency is required for all simulations to benefit from assimilation so far we correctly presumed that the forecast skill of a network could be attributed to the configuration of the observation network albeit no clear link between the da performance and the observation positions could yet be established from the previous section it was however noted that some state variables show a limited time of inundation in case of the cos configuration it can be emphasized that this limited inundation occurs for 2 observation locations 1 and 2 in fig 4 and so their added value to the joint network can be questioned this could possibly explain the lower score for the cos configuration rearranging the simulations from fig 5 as a function of network size we obtain the boxplots in fig 6 that each consist of a collection of 168 simulations since both performance measures of fig 5 draw similar conclusions the following results are only displayed for the rmse generally the average da performance improves the more observations are involved in assimilation this latter trend is the most significant for a network increase from 5 to 10 observations while the added value from 10 up to 20 observations is rather limited from this we deduce that adding observations benefits the predictions however in the case of the vor configuration little to no effect is observed for a network increase from 5 to 10 observations moreover extending the observation network implies practical and economic considerations such as operational costs and maintenance therefore we conclude that the network size depends on the available resources though the locations should in any case be carefully defined again the outliers from fig 6 can be attributed to high model and or observational uncertainty as well as low assimilation frequencies several studies encourage the use of a thinned set of srs observations mason et al 2012 ochotta et al 2005 2007 to minimize spatial autocorrelations and thus to avoid the assimilation of redundant information and the unnecessary use of computational power however in most operational cases autocorrelations from srs observations are still unfairly ignored in the off diagonal elements of r eq 8 waller et al 2016 these studies therefore propose the use of dispersed observation locations thus far we have not yet found evidence to adapt this allocation strategy for the case of in situ floodplain monitoring nonetheless from our analyses it is clear that a high measuring frequency yields a clear benefit compared to daily monitoring and the network size can remain limited as long as the locations are carefully selected it should further be noted that the average errors on forecasted water levels are rather small as compared to the open loop simulation i e up to a few centimetres which will allow flood managers to take targeted action in a worst case scenario a water level rise of 1 cm could result in a flood extent change of around 77600 m2 or equivalently 0 94 of the maximum true flood extent of this case study although we aim to reflect a realistic scenario these optimistic results may in part be due to the synthetic nature of this study application of the da setup in real case scenarios where less suitable data are available would therefore be indispensable to generalize our conclusions 3 3 impact of sensor placement and measuring frequency on the improvement persistency regardless of the numerical improvement assigned to a configuration it may be even more important to know how long this improvement persists and how it behaves in between two assimilation time steps ideally the improvement of a reliable configuration should persist up until the subsequent analysis time step and its behaviour should be predictable over time the hydrograph of a flood inundation event is characterized by a rising and a receding curve each of which has its specific properties gobeyn et al 2017 for this reason it is expected that the data assimilation process affects each of the curves differently in the following analysis both curves are therefore treated separately for each time step the spatially averaged rmse is determined for both the assimilation run rmseda and the open loop run rmseol and are subsequently merged into a single score the relative rmse reduction after madsen et al 2003 19 r m s e r e l 1 r m s e d a r m s e o l if we then extract all time windows between two consecutive assimilation steps and average the rmserel for corresponding time lags of those windows we obtain the average behaviour of the da forecast skill between two assimilation steps the relative rmse reduction can range from to 1 a value of 1 corresponds to a perfect match between the assimilation and the truth conversely values lower than 0 occur when the assimilation run no longer confers a benefit over the open loop prediction to eliminate the effect of observational and model uncertainties as well as varying network sizes the following analysis is performed for ϵ o 0 05 m eq 10 ϵ m 0 3 eq 4 and n 5 for this setup the results of the relative rmse for the rising and the recession curve are shown in respectively panel a and b of fig 7 as expected the behaviour of both curves is noticeably different the rising curve shows a monotonically decreasing rmse reduction whereas for the recession curve improvement increases up to 3 h after the analysis time step followed by a strong decrease to a minimum this phenomenon can be attributed to the mismatch between the ensemble spread and the observation variance during the rising of the storm the increase in floodplain water levels causes the water level uncertainty to raise unlike the more stagnant water level conditions during the recession characterized by a smaller uncertainty at the time of assimilation the imbalance between the model ensemble spread and the fixed observation variance may generate hydraulically inconsistent water levels which for this setup is most notable during the recession of the flood when the imbalance is largest subsequent flood forecasting redistributes inconsistent water levels over the model domain thereby increasing the performance temporarily moreover if the assimilation frequency is high the inconsistencies are unable to restore in time for that reason assimilation could be performed at a lower frequency if the imbalance between model and observational uncertainty is too large in general the persistency of the assimilation improvements is largest and most stable for the rising curve this result ties well with previous studies wherein pre flood peak srs observations are preferred for assimilation and calibration gobeyn et al 2017 garcía pintado et al 2013 even though the assimilation persistence is similar for all observation configurations some differences are yet to be noted as before the configurations of vor and eucl show the best results and remain quite stable in time in contrast cos performs worse but retains the observed persistence trend in other words the differences observed for the relative rmse are mainly due to the differences in performance as shown in fig 5 rather than the persistence of these performances from this analysis we can deduce that the allocation of observations should primarily be validated based on the overall data assimilation skill during the rising storm as the largest differences in forecast skill between configurations predominantly occur during this part of the flood event from the perspective of flood warning and mitigation modelling the upcoming of the storm event is in any case most crucial 3 4 performance of individual observations within their network from section 3 2 it was clear that the da performance between configurations can greatly differ and the same was encountered for their uncertainty in an attempt to explain these differences we investigated the persistence of their improvements but the results proved inconclusive in a following analysis the observation network is no longer viewed in a holistic manner instead each observation within a network is treated separately with the aim of finding a causal link between the individual performances and their joint performance these type of experiments are sometimes referred to as single observation impact experiments gustafsson et al 2014 gasperoni and wang 2015 in order to examine the individual observation impact on the forecast skill additional simulations were carried out that each assimilate an individual pixel from a network where previously a network of 5 observations would yield a single prediction this will now generate 5 separate predictions fig 8 shows the individual rmse performances dots relative to their combined performance areas for 3 cases that differ in their uncertainty and assimilation interval da1 ϵ o 0 05 m ϵ m 0 3 interval 1 h da2 ϵ o 0 05 m ϵ m 0 3 interval 24 h and da3 ϵ o 0 30 m ϵ m 0 3 interval 24 h respectively displayed in green blue and red and summarized in table 3 it is anticipated that the assimilation scenario da1 will likely perform better than da2 which in turn is expected to perform better than da3 if a dot is situated above the upper limit of its corresponding coloured area the joint assimilation outperforms the single observation assimilation conversely if a dot is located within or below the area of its colour assimilation of the individual observation is beneficial over the joint assimilation indicating that the network contains observations that worsen the joint performance the analysis was conducted for a network size n 5 and network configurations eucl vor and cos as those exhibited the largest performance differences the results of the rmse joint performances areas for each case in fig 8 are ranked as expected with da1 performing the best followed by da2 and da3 this ranking is hereafter referred to as behavioural however the same does not apply to the individual performances dots more specifically the da1 scenario seems to show a deviant behaviour for certain pixels whose rmse is significantly higher for da1 than for da2 and da3 this namely concerns pixel 5 from eucl pixels 1 2 and 3 from cos and pixels 2 and 3 from vor as can be seen from fig 4 inundation for these non behavioural pixels starts relatively late compared to the other domain pixels how this relates to the reduced performance of the non behavioural pixels can be explained as follows once the assimilation procedure starts observations are generated from the truth and subsequently assimilated however in the event that one or more ensemble members of a particular state prematurely show inundation at an analysis time step the assimilation procedure is initiated and observations have to be generated from a zero truth water level considering that on the one hand observations with a zero water level are also associated with uncertainty and on the other hand no negative water levels should be assimilated for a short period of time a positive bias is introduced onto the observations this can in turn cause other domain pixels to be inadequately adjusted and hence influence the overall model results moreover as soon as this temporary bias is present our enkf setup does not allow for flooded locations to reduce their water level back to zero this is caused by the complex interaction between domain pixels that continuously redistribute water across the floodplain and again by the uncertainty that is inherently associated with the observations the phenomenon described above is amplified in the case of scenario da1 due to its high assimilation frequency and its low observational uncertainty first the high assimilation frequency will soon detect premature flooding of the ensemble and subsequently cause the observational bias to be introduced at an early stage second the high trust given to the observations σ o 0 05 m for da1 as compared to σ o 0 30 m for da3 will substantially contribute to this initial bias since more weight is given to the observations of da1 as opposed to scenario da3 which will therefore encourage water to be erroneously retained in the floodplain to overcome these issues an assimilation framework should be developed to allow water volumes to be completely removed from the system and to avoid the problem of biased observations in case of zero water level observations to conclude the poor performance of certain networks can be attributed to late inundation times the allocation of observations should therefore be prioritized to the areas that are most prone to flooding usually where the elevation is low the majority of the dots in fig 8 are located above their corresponding shaded area however for the eucl and the cos configurations a few exceptions are made though almost exclusively for the da3 scenario it is striking that in some cases a single observation assimilation outperforms the assimilation of the full set that the single observation belongs to this discrepancy could be explained by the high observational uncertainty associated with da3 at time steps of low ensemble spread the high observational uncertainty could cause inconsistencies in the floodplain water levels just like mentioned for the receding curve in fig 7 this in turn leads to a degraded performance all other dots are located rather close to the upper limit of their respective shaded area in this respect it is important to investigate the added value of an observation within its set both spatially and in time these added values will be discussed in the next section 3 5 contributions of individual observations in time and space at this point the potential of in situ floodplain monitoring has been successfully demonstrated in addition some guidelines on the allocation of observations have been described from the foregoing analyses yet one key point is still missing namely how the forecast skill of an observation behaves in time and space this aspect is covered in this section previously it was pointed out that observation networks may consist of two types of observation locations behavioural and non behavioural ones depending on their performance rank of 3 da scenarios an empirical correlation between the number of non behavioural locations and the overall score of a particular configuration was established in order to respond to the challenges of observation allocation a thorough understanding of this relation should be gained additional experiments on the individual observation contributions are therefore conducted by means of scenario da2 considering that on the one hand its daily assimilation frequency ensures stable model results and on the other hand its observational uncertainty of 0 05 m reflects realistic in situ measurement errors in this final analysis the contribution of each observation within its network is compared according to the procedure described in section 2 3 1 for each analysis time step zi identifies the observation that induces the highest assimilation update for model state i in an attempt to identify the contribution of a particular observation to the entire model space zi is determined for all model states m the spatial contribution of an observation is then defined by the fraction of model states for which zi is represented by the considered observation as such this spatial fraction reflects the geographical influence of the observation if these calculations are repeated for every analysis time step the contribution of each observation is not only known in space but also in time for the case of da2 this implies that the contributions are assessed on a daily basis fig 9 demonstrates the contribution of each observation as a function of time and is expressed by the spatial fraction outlined above the colours assigned to the observations are in accordance with fig 4 at some time steps no fractions are displayed the reason is that neither the model ensemble nor the truth yet experience flooding at the observation locations considered no assimilation occurs in this situation fig 9 reveals a clear time dependence of the observation contributions for example observation 1 of eucl has a strong contribution during the rising of the storm which decreases over time the contrary is true for observation 4 whose contribution increases during the recession of the storm observations 3 and 5 show less a temporal trend while the added value of observation 2 is limited similar dynamics are encountered for the vor configuration although less pronounced in this case all 5 observations have a significant impact on the model state updates conversely the consequence of suboptimal observation allocation is clearly observed for the configuration of cos only 2 out of 5 observations have a profound impact on the updates averaging all spatial fractions over time allows to adequately compare different configurations and setups depicted on the right side of fig 9 from this it seems that configurations with a high forecast skill eucl vor are associated with an observation network in which most of the observations affect the water level estimates significantly whether that is in time or space further it can be noticed that for fixed time steps the vast majority of the bar plots consists of 1 to 3 observations that significantly influence the model results evidently the magnitude of the fractions is proportional to the spatial contribution of each observation moreover these contributions tend to occur clustered over the model domain results not shown the above findings suggest that when allocating observations the best practice is to select those that have a significant contribution in both time and space and ideally are complementary as a function of time the foregoing analysis was carried out for a network size of 5 observations previously assumed to be sufficient for this case study section 3 2 we can however extend this analysis to a variable network size in order to establish a link between the network size and the amount of observations that significantly contributes to the assimilation for each configuration technique observation networks of size 1 20 are designed and implemented with the same assimilation setup da2 as for fig 9 in case n 20 and all observations equally contribute each observation can reach a maximal fraction of 0 05 expressed as a spatial fraction therefore a fixed threshold of 5 is suggested for an observation to significantly contribute each dot in fig 10 represents a unique combination of configuration and network size due to the discrete nature of this exercise dots may overlap in that case the transparency of the dot is decreased the figure reveals a clear link between the amount of contributing observations and the network size even though the network size increases the number of contributing observations stagnates around a maximum of 5 7 contributing observations our assumption that a network size n 5 could be sufficient for this case study albeit on the condition that the observation locations are well chosen is hereby confirmed 4 conclusions this study explores the feasibility of using in situ floodplain water level observations as an alternative source of distributed information for flood monitoring our aim is to evaluate the potential of a sparse sensor network in a data assimilation framework and to provide general guidelines relating to the sensor positioning and assimilation setup the lisflood fp flood inundation model was used to simulate a real case flood event that is employed in the enkf data assimilation framework in order to investigate the impact of sensor positioning on the forecast skill multiple observation networks were designed through cluster analysis this involves the merging of floodplain water level time series by means of 3 similarity measures euclidean cosine and mahalanobis distance subsequently each cluster is assigned a representative state the observation in addition observation networks were also designed according to inundation time clustering as well as a voronoi decomposition in a first stage we assessed the overall performance of the study design for different network configurations and assimilation setups without updating the spatio temporal rmse of the forecasts equals 7 93 cm while in optimal assimilation conditions it drops below 2 cm it is obvious that low assimilation frequencies and high model and or observational uncertainties generally degrade the performance the results indicated that the da performance is also affected by the network configuration in an attempt to address this latter dependence the positive impact of each configuration after assimilation was studied although it was revealed that a positive impact is relatively more persistent during the storm rise than for the recession of the storm no clear differences among configurations were detected in a second stage the impact of individual observations on the forecast skill was investigated relative to their joint performance it was found that a joint network assimilation mostly outperforms the assimilation of an individual observation from that network a low performance of an individual observation was associated with late inundation times in a few cases however a single observation could outperform the joint network it was believed that this is caused by suboptimal assimilation conditions rather than configuration issues for example the assimilation of multiple highly uncertain observations could result in water level inconsistencies that decrease the performance in a final stage a more in depth analysis was carried out on the individual observation contributions in both space and time measured by the magnitude of the updates induced by each observation a good quality observation network consists of observations that contribute complementary in both space and time that is if an observation provides only useful information during storm rise it should be complemented with an observation that induces improvements during the recession furthermore a clear link between the network size and the amount of contributing observations was established from this it was clear that a network size of 5 observations would suffice for our flood case study only if the observation locations are carefully chosen that is choosing locations that are prone to early flooding and provide complementary information in both time and space in general our results show great potential for the use of an in situ floodplain sensor network although the information is only sparsely distributed it is a very cost effective way for flood monitoring practices it may provide data for ungauged areas or complement existing satellite sensor data therefore it is important to carefully design a sensor network and evaluate the assimilation setup the research presented in this paper makes it possible to follow some guidelines regarding the establishment of an observation network in the floodplain however it is important to keep in mind that this study is of a synthetic nature and the entire data assimilation setup was controlled firstly although several assimilation parameters have been throrougly studied not all sources of uncertainty were accounted for such as errors in the dem and bathymetry secondly the observations were derived from a synthetic truth which may have led to optimistic results lastly this truth also allowed us to examine the observation network dynamics in advance which would have been impossible in real case studies therefore in order to generalize our conclusions this study should be verified for real case flood events of different magnitudes to develop an optimal fixed observation network acknowledgements the authors are grateful to environment agency ea of england and wales for providing the lidar digital elevation data and the hydrometric data the computational resources stevin supercomputer infrastructure and services used in this work were provided by the vsc flemish supercomputer center funded by ghent university fwo research foundation flanders and the flemish government support was given by a phd fund of the bijzonder onderzoeksfonds bof of ghent university grant no 01j04015 who funded the work in all its stages lisa landuyt is a doctoral research fellow of the fwo and received funding through grant g 0179 16n 
617,while a large number of descriptive studies have delineated the interlinkages between water food and energy resources in the last decade there is still need for systematic conceptualization of resource nexus interconnections this paper proposes a theory of relational analysis of the nexus based on the analytical concept of nexus networks a taxonomy of nexus interconnections detailing sequential and hierarchical connections is characterized between and amongst the technosphere and biosphere we illustrate the use of a novel diagnostic tool with regard to its ability to integrate macro meso and microscale drivers of nexus problems we apply this framework to problems generated by intensive crop production for exportation in an arid landscape driven by external markets and sustainable management of water resources driven by public policies in a southern spanish region we elucidate interconnected causal mechanisms for groundwater overexploitation and profile different social ecological patterns on a spatially explicit basis the proposed approach is capable of accounting for the water energy food resource nexus in an integrated and multi level fashion addressing the tensions generated by both multi functionality and resource entanglement in complex social ecological systems graphical abstract image graphical abstract keywords nexus networks metabolic processor relational analysis water energy food nexus multi functionality 1 introduction mainstreamed by international calls for securing resources pressured by entangled global drivers the water energy food nexus notion has gained momentum at an increasing rate in sustainability research and policy agendas interconnections synergies and trade offs are common keywords within the nexus concept where the diversity of definitions has gathered into at least four perspectives i the nexus as governance approaches that seek coordination and harmonization nilsson et al 2012 weitz et al 2017 ii the nexus as the co occurrence of resource use in economic sectors and supply chains also referred to as the resource nexus font vivanco et al 2018 iii the nexus as the interconnection between different resource systems generated by specific activities or technologies d odorico et al 2018 and transversally iv the nexus as transdisciplinary and co production practices in sustainability research howarth and monasterolo 2017 scanlon et al 2017 nexus problems have been characterized as types of wicked or post normal problems underpinned by uncertainty ambiguity contested stakes and unpredictability harwood 2018 furthermore these nexus situations are argued to be the outcome of co existing macro meso and microscale processes cai et al 2018 which can be related to the different conceptualizations of the nexus identified above from a macro perspective the nexus preoccupation arises from the acknowledgement that economic sectors responsible for resource co occurrence are governed by different public policies and private markets with silo dynamics frequently expressing conflictive goals muranetto and witmer 2017 venghaus and hake 2018 therefore a need is claimed for closing the governance gap and improving policy coherence weitz et al 2017 whereas the macroscale is epitomized by this interface between policy and market constraints and economic sectors the mesoscale is represented by the interface between those sectors and the water food and energy resource management domains they connect resource management domains are associated with regional spatial scales since they are responsible for controlling the level of extraction of primary resources on the other hand economic sectors are networks of entities connecting different regions that metabolize extracted resources along production chains franz et al 2017 nexus difficulties at this level are encountered in cross sectoral management approaches when bringing together sectors and managing trade offs between them pahl wostl 2017 stein et al 2018 lastly at the microscale and somewhat paradoxically many localized nexus problems have emerged as the result of techno social innovations aimed at solving the conflict between policy goals cabello and madrid 2014 classical examples of nexus problems in this sense include desalination and irrigation efficiency innovations related to both of these problems add dependencies on energy resources such as electrical energy to drive water pumps to the water food nexus thereby increasing the structural complexity from a management perspective moreover as solutions for reducing pressures over fresh water resources emerge monitoring of social ecological interactions such as water withdrawal or pollution are demanded the nexus concept thus appears as an indicator of the need to improve our capacity to account for the interconnections between complex social and ecological processes tensioned by multi level and conflicting drivers recent empirical research commences to delineate interconnections as the different causal relations in processes involving more than one resource andrews speed et al 2012 bijl et al 2018 yet whereas relations are the core research object of the nexus concept little progress towards a categorization and formalization of interconnections has been made some consensus has gathered around the differentiation between biophysical connections coined as the resource nexus and other social and governance relations cai et al 2018 galaitsi et al 2018 within the resource nexus a first approximation to categorization is the font vivanco et al 2018 taxonomy of direct dependent and interdependent relations their model shows that the co occurrence of water and energy along supply chains can be mostly explained by direct extraction or one way dependencies while feedbacks appear minor their categories however account only for interactions amongst economic sectors at the national and global scales when moving to local and regional contexts interdependencies are likely to gain relevance following the consideration of social ecological interactions albrecht et al 2018 this paper furthers previous efforts in categorizing nexus interconnections by proposing an operational concept of nexus network for this purpose we draw on ideas from some branches of complexity theory namely relational analysis rosen 1958 2005 2012 hierarchy theory ahl and allen 1996 allen and starr 2017 and societal metabolism georgescu roegen 1971 1975 giampietro and mayumi 2000 we add to the nexus pathways concept of vivanco et al 2018 with a multi level conceptualization of nexus networks that includes interactions within and between the biosphere and technosphere this conceptual framework is applied to nexus problems derived from intensive crop production in arid and semiarid landscapes with over exploited water resources which often find solutions in energy intensive technologies the region of almeria in south eastern spain serves as illustrative case study the methodological framework multi scale integrated analysis of societal and ecosystem metabolism musiasem giampietro and mayumi 2000 is applied to quantitatively formalize the network addressing the following question why are groundwater resources over drafted in almería 2 relational analysis of nexus networks 2 1 theoretical ground relational analysis is a variety of complex system analysis that explores the identity of living systems by representing them formally in metabolic networks characterized by the four aristotelian causes material formal efficient and final rosen 2005 this understanding of relational analysis finds roots in theoretical biology and the seminal work of rashevsky 1954 and arbib et al 1973 furthered by rosen 2005 2012 and more recently by louie 2009 2013 2017 into a new conceptual framework to the modeling of living systems within the field of category theory following their discourse relational analysis addresses provocative questions of why by expounding the pluralism of causal relations defined over the components of a system 1 1 for illustrative example assume there exists a constituent component a i the material cause of a refers to the material out of which a is made ii the formal cause of a refers to the pattern form to which the material cause is required to assume in order to form a iii the efficient cause of a is in the majority of cases the agent which transforms the material cause into a adhering to the formal cause in abstract terms some function f iv the final cause reflects the for what of a the focus of relational analysis thus centers on the organizational unity of the analyzed system describing the various roles or functional behaviors of system components more specifically relational analysis views the fuzzy relations between function and structure observed in complex systems as compelling subjects of analysis kampis 1987a 1987b rosen 1970 bechtel and richardson 2010 in this sense described functions may be realized in an impredicative and epistemological manner by different structural elements within the field of hierarchy theory allen and starr 2017 pp 43 67 also elaborate on the impredicative duality of function and structure to be analyzed over a minimum of three analytical levels within a complex system together with relational analysis hierarchy theory helps define core characteristics of components in a complex metabolic network firstly system components are defined at the same time as a part e g a liver as organ of a human being and a whole e g a liver made up of cells secondly as mentioned previously system components handle the coupling between function and a structure e g the function professor must be realized by a particular material instance for example prof mcgonagall thirdly reflecting back on the aristotelian building blocks of relational analysis system components may be arranged sequentially material entailment and hierarchically functional entailment in the formation of a network to clarify a hierarchical disposition of system components represents a chain of definitions of function each functional definition of a component provides the final cause or purpose to the realizing structure s thereby its meaning each function may be realized by one or more structural components made of the material cause expressing their agency efficient cause when operating in an admissible environment and following certain patterns or codes the formal cause the final piece of this framework is the concept of societal metabolism a notion used to characterize the processes of energy and material transformation in a society that are necessary for its continued existence for an overview of its application to ecological economics see martínez alier and schlüpmann 1987 for an overview of its application in sociology see fischer kowalski 1998 the metabolic approach enables the application of relational analysis and hierarchy theory to nexus relations in social ecological systems giampietro 2018a defines such a system as a set of functional and structural components linked by a set of relations and operating within a given boundary to achieve a shared function a given final cause building upon this definition the musiasem accounting framework mayumi and giampietro 2000 has been proposed as one of the available integrative methods for quantitative analysis of the nexus giampietro et al 2014 keairns et al 2016 shannak et al 2018 in the following section we add a conceptualization of nexus networks to this framework 2 2 multi level nexus networks a nexus network can be defined as the set of processes governing the interdependency between water food and energy within and across a given boundary as any network nexus networks are represented with nodes and connecting edges nodes refer to specific processes consuming more than one resource and producing good s or service s in formal operational terms we represent these nodes using the concept of the metabolic processor rosen 2005 p 250 most typically metabolic processors hereafter processors perform work on a material substrate and transform it into something else performing a function in a complex network the elaboration of the processor concept into an analytical tool within societal metabolism theory is a remarkable contribution of the musiasem accounting framework ripa 2017 giampietro 2018b in this vein a processor fig 1 identifies an expected set of relations regarding i a structured process capable of producing a given output or expressing a given function n node in fig 1 ii a profile of inputs and outputs in both the technosphere and the biosphere required for that purpose e1 6 edge in fig 1 in this sense processors embody the impredicative duality between structure and function of a system component processors such as the canonical processor in fig 1 can be connected in different ways in the formation of a nexus network fig 2 presents this point using the same notation found in fig 1 in the analysis of social ecological systems a minimum of three connection domains should be identified i connections that remain within the socioeconomic realm the technosphere ii connections that remain in the ecological realm the biosphere and iii connections bridging socioeconomic and ecological processes the technosphere biosphere boundary in any given nexus problem the co occurrence and entanglement of resources implies causal connections within each of these three connection domains simultaneously in addition to these three connection domains and pursuing relational analysis a second dimension is added differentiating between sequential and hierarchical connections in fig 2 as already elaborated by vivanco et al 2018 sequential connections are material flows between specific activities or sectors therefore modelling expected outputs at a given level of observation in formal terms they are one one or one many allocations with one way or feedback direction determined by the material entailment between processors in contrast to sequential entailment hierarchical connections are determined by functional entailment that is the function to be expressed at one level has been defined as useful as having a proper final cause at the hierarchical level directly higher hierarchical connections are formalized as many to one or many to many mappings between system components expressed at different levels of observation for instance one can represent a network of connected food production processes following various stages of production and processing sequential relations within the technosphere this sequential pathway may then be scaled up through the various agricultural subsectors ultimately into the agricultural sector in a given region hierarchical relations within the technosphere alternatively the same sequential pathway may be scaled up through the different scales of river basins supplying water to the agricultural processors sequential relations across the technosphere biosphere boundary these two example scaling operations are non equivalent hierarchical mappings of the same set of sequential relations and they enable the answering of different questions about the system under analysis as in other system analyses the specification of a nexus network requires a research question or problem that enables the selection of analytical scales and the delimitation of system boundaries and system components with associated functions within the system from a societal metabolism perspective this is equivalent to identifying socioeconomic sectors and their specific activities intertwined with ecological processes within a defined social ecological boundary in the logic of analyzing final causes the functionally defined components at a given level must be further associated to patterns of structural components that are viable within system constraints at a lower level what is equivalent to specifying hierarchical relations following the previous example the agricultural sector would be split in subsectors for instance animal vs vegetal production and subsectors into supply chains that are viable in a given region we aim to design a diagnostic tool capable of analyzing the tensions generated by conflictive goals and multi level drivers using a functionally and structurally defined network these tensions can be described as a problem of multi functionality the fact that different system components coexist within a social ecological system and that their multiple functions need to be sustained while they adapt to external drivers creates many constraints on the type of structural elements that can fulfil those functions the spectrum of viable solutions to nexus problems is limited in the context of competing drivers and any implemented local solution will transform the network of relations by adding new structural constraints 2 3 formalization of nexus networks once the network of metabolic processors is defined in semantic terms the next analytical step is to instantiate them in formal terms a processor is a data array structure that contains information about the profile of inputs and outputs required in a given process the computational form related to this profile shares a number of similarities with that of life cycle assessment lca see heijungs and suh 2002 in relational terms an j indexed socioeconomic processor nj can be formalized as a column vector as shown in equation e1 the elements of this vector e 1 through ei represent for some processor n the set of expected relations defined in intensive terms against a relation or set of relations most typically the relations in a processor are described against just one element e g per unit of a processor output or otherwise immutable quantity such as human activity or land use processor inputs e1 n j e 1 e i further several rowwise partitions are erected in this column vector in the most general sense and following the canonical processor presented in fig 1 the relations of a processor e 1 through ei may be placed in one of four 2 2 relations within the system of interest are always within the technosphere an observation which explains how the six categories in fig 1 may be reduced to four at this stage of the formalization possible categories the combination of two variables each with two possible values creates these four categories the relation s domain technosphere or biosphere and its orientation input or output the set s in eq e2 lists these two variables and elaborates their possible values e2 s t e c h n o s p h e r e b i o s p h e r e i n p u t o u t p u t the last bit of information associated with each relation is its entailment dependency or dependencies i e in the case of a singular dependency whether a cause of one process is produced by or contributes to another process es in the case of a singular dependency we can say that the validity of the first process both depends on and entails the existence of another process es fig 2 depicted the various entailment dependencies possible in our relational analysis processors may therefore be represented as column vectors eq e1 partitioned along the elements of the set s eq e2 the concatenation of all processors described as sparse column vectors then provides the dataset describing the system of interest for the purposes of the analysis as shown in eq e3 it should furthermore be noted that the description made by eq e3 represents the most basic categorization possible an effective analysis necessarily includes additional subledgers dependant on the goals of the analysis for example in analyses of societal metabolism an additional subledger differentiating between fund flow and stock resource is typically regarded as an essential relation refinement georgescu roegen 1971 e3 n 1 n j e 1 1 e 1 j e i 1 e i j generally processors are stored in the dataset in intensive unitary terms to maintain validity care must be taken to relate intensive representations with their original external reference in an analysis of a system of interest the set of intensively described processors may be proportioned by the size of supply vector qs or by any other variable against which relations are defined to fulfil demand vector qd in the context of this work s case study supply vector qs would include mass quantities of e g almonds and vegetables demand vector qd would include various resource demands e g land area requirement and groundwater volume requirement in this vein processors arranged in a network may describe in intensive and or extensive terms the complete set of sequential and hierarchical relations within and between the technosphere and biosphere these two different sets of numerical representation provide two different key types of information about the system of interest king and carbajales dale 2016 eq e4 formalizes the relation between intensively defined processors supply and demand the formalism works for diagnostic analyses as well as anticipatory analysis in an anticipatory analysis changes in resource demand may be estimated from anticipated changes in i technical coefficients intensively defined processor data e g because of technological change and ii supply e g following demographic projections e4 n 1 n j q s q d 3 application to the nexus in arid land crop production in this section we illustrate the process of building a nexus network for analysing the tension between intensive export based crop production and sustainable management of water resources in the province of almería in south eastern spain 3 1 framing the problem almería is one of the driest regions in europe average annual rain 200 600 mm at the same time it represents one of the largest intra european suppliers of fresh vegetables exporting up to 64 of their overall production main importing countries are germany 30 france 16 the netherlands 12 uk 11 and italy 7 agencia tributaria 2013 other major land uses for crops are olive groves almonds and varieties of citrus agriculture sustains 8 of the employment in the region and 2 4 of its gdp junta de andalucía 2013 contrary to other intensive exploitation systems in spain land property is highly distributed with an average farm size of 2 to 3 hectares and where most farmers are associated to cooperatives for marketing purposes varela martínez et al 2016 the region is a renowned example of unsustainable exploitation of groundwater resources in spain with many aquifers classified as severely overexploited and or polluted according to the european water framework directive wfd assessment the european wfd also mandates that all european water bodies must recover to a good status a mandate expressed in terms of environmental goals defined by the year in which a good status shall be recovered following the implementation of new management measures fig 3 depicts these goals for the almería region framing the nexus problem a macroscale tension arises from the goals of water policy which require a reduction of the pressures over water resources and european food markets eager to absorb the increasing local production at the region level the problem translates into the question of how to re organize relations between agricultural and water management regimes agriculture is responsible for 80 of the water usage 62 5 of which is groundwater junta de andalucía 2015 the main solutions on the table so far have been the utilization of alternative water resources desalination and reclamation or external transfers as substitutes for groundwater and the improvement of irrigation efficiency a decade later overdraft rates are not reported to decrease and the contribution of these innovations to the stated purpose is unclear using the framework described above we build a nexus network to provide a relational analysis of the aquifer over exploitation phenomena as mentioned relational analysis focuses on multiple causality in complex problems by elucidating interconnected functions clearly addressing the question of why aquifers are overexploited in almería can be answered from different lenses for instance a governance and power relations reading would illuminate one sort of relevant responses regarding the key actors involved in the problem and their interconnections however this paper focuses on the metabolic reading of the nexus that is on the type of answers provided by sequential material and hierarchical functional entailments between metabolic processors we assume the view that the ultimate final cause for over drafted groundwater resources in present day almería is the maintenance of an agricultural economic model based on valuable crop exports understanding the other causal mechanisms for the overexploitation phenomena requires a threefold analysis first it is necessary to understand what farming systems are driving water demands second it is necessary to look at the role of groundwater within the overall pattern of water resources supplied to those farming systems by irrigation organizations lastly it is necessary to contextualize withdrawals in the hydrological balance of the different aquifers 3 2 building the network considering the whole region of almería as a social ecological system a multi level nexus network was defined connecting key components of food and water management domains identified above fig 4 at the top analytical level 1 these domains are responsible for organizing the implementation of public policies programs or strategic goals for those sectors posing top down constraints over the behavior of lower level system components as mentioned bottom up constraints can be uncovered by adding levels of functional structural metabolic processors to the network in this vein the agricultural sector has been disaggregated into irrigation areas as the organizational units of water management in agriculture and further into farming systems and even further into the different crops produced in each of those farming systems on the other hand sequential connections relate functions within the same level adding more constraints to the network in terms of multi functionality in our network sequential linkages are established within components of the water management system namely water bodies water suppliers and irrigation areas as water users 3 3 operationalizing the network as explained in section 2 the methodological approach proposed in this paper uses the numerical structure of processors to quantify nexus nodes and relations for this purpose one must identify physical instances matching the categories of system components quantify processor variables and profile representative types of processors out of them the most standard procedures are either data driven e g sample a population and cluster it in typologies or semantic driven approaches e g pre define typologies and sample representative instances for the almería case we followed a mixed approach within the agricultural domain we defined typologies of crop production processes and farming systems through a review of previous studies and techno agronomic reports garcía garcía et al 2016 varela martínez et al 2016 and the analyses of land use data for different crops petitioned from the andalusian agricultural administration on the other hand available georeferenced datasets for water system components in river basin management plans junta de andalucía 2015 gobierno de españa 2015 enabled the characterization of the full population of water bodies and irrigation areas crop data was only provided for the productive year 2012 2013 whereas data on water extraction and use is available as an annual average for the 2010 2015 management period therefore the analysis is bound to a single snapshot of the annual metabolism of the system without addressing its time evolution processors variables were populated with data gathered from different secondary sources and integrated in two databases describing the crop production and the water management domains variables were scaled from lower to upper levels in the network according to the land use patterns described across levels structured and documented datasets and r code for scaling operations are published in a zenodo 3 3 https zenodo org record 2539219 xdx9prbrcvs repository and the data management process from collection to visualization is thoroughly described in the appendix given the diversity of data sources and their production methods mostly based on estimations rather than accurate measurements the technical and methodological uncertainty associated to the datasets is notable for the purposes of this paper we restricted data analysis to the most trustful variables used by public institutions in their management plans further efforts in reliable data production for robust integrated assessments of nexus interconnections are needed in the future to address the proposed research question we provide a diagnostic analysis consisting of the interconnected visualization of some variables in food and water processors at different levels in the network namely withdrawal rates from aquifers by different irrigation areas level 2 patterns of water resources use in irrigation areas level 2 patterns of land uses in irrigation areas levels 2 3 4 and crop production factors in different farming systems levels 3 4 groundwater availability and the derived extraction index 4 4 under the water framework directive the extraction index is calculated as the ratio between annual withdrawals and available groundwater are used to check the sustainability of different water food nexus patterns against quantitative impacts on aquifers for analytical purposes the processors describing the 16 irrigation areas are clustered into 7 typologies based on their pattern of farming systems level 3 for this purpose profiles associated with quantitative criteria on land use were defined through a qualitative analysis of most representative patterns and the 16 irrigation areas were coded accordingly see table a3 in the appendix for further explanation and ouput clusterdata xlsx in the data folders for assignation of irrigation areas to each cluster processors variables including withdrawals from aquifers were aggregated for the obtained categories and selected indicators calculated this clustering enables profiling several typologies of social ecological patterns on a spatially explicit basis as presented in the next section 4 results and discussion 4 1 why are aquifers overdrafted in almería we are now in the position of providing a quantitative relational analyzes of our nexus problem fig 5 presents the geographical display of the clustered irrigation areas overlapping aquifers in the region classified by their extraction index fig 6 visualizes the above described variables of several connected processors within the food and water regimes according to the water districts management plans 12 out of 27 aquifers in almería show an extraction index over 0 8 meaning their exploitation rate is considered unsustainable the spatial distribution of this index reveals a certain predominance of high extraction rates along the coastline as compared to inner rural areas these rural irrigation areas are to a large extent dominated by low intensity almond and olive production irrigation area type ia4 farming systems f5 and f7 in fig 6 2 connected aquifers show either low or moderate extractions rates 0 8 1 1 in those areas with presence of vegetables or lettuce production in open fields ia2 and ia3 for those aquifers with extraction index over 1 1 fig 6 1 presents the annual volume of extraction split by type of irrigation area ia in absolute terms most groundwater over extraction in almería is incurred from three aquifers 011 012 and 013 by three irrigation areas clustered in ia1 these areas populated by greenhouse farming systems f1 in fig 6 2 grow a vast amount of fresh vegetables and elicit the highest rates of labor and gross value added generation as observed in fig 6 3 the ia1 cluster has introduced more alternative water resources than the other ias although they still represent a small share of the total water used in relative terms the aquifers showing the highest extraction indices are associated with other types of farming systems predominant in clusters ia5 and ia7 ia5 is represented by a single irrigation region occupied by olive groves in hyper intensive production mode f6 in fig 6 2 this novel system has rapidly extended due to its larger margin of benefits as compared to intensive olive production and spurred by public subsidies f5 whereas its contribution to annual overall crop production in the region is minor fig 6 4 it consumes almost exclusively groundwater from aquifers with very low annual availability 007 009 on the other hand cluster ia7 gathers two large irrigation areas representing the second most relevant contribution to regional water demand and crop production after ia1 fig 6 3 and 4 it combines three main types of farming systems producing mixed vegetables in open fields with lettuce and citrus f3 f4 and f8 in fig 6 2 these farming systems show middle intensities of labor and gross value added generation water resources in these areas blend groundwater from over drafted aquifers with desalinated water and an important share of external transfers even so current withdrawals contribute to surpass the very low availability rates in five aquifers 001 008 from the previous analyses we can conclude that the answer to the question of why aquifers are over drafted in almería is at least twofold on one hand the profitable greenhouse economy has very large demands of water so far fulfilled mostly with groundwater on the other hand we find a large area of less water demanding farming systems relying on aquifers with very limited water available implementing water policy objectives requires ratcheting withdrawals down below availability grey reference line in fig 6 1 the volume of these reductions might not look dramatic in absolute terms but it is an important fraction of water supplies for several irrigation areas as observed in fig 6 3 4 2 discussion in the absence of an effective harmonization between water and agricultural policies in the eu söderberg 2016 the tension between environmental policy objectives and market demands in relation to crop production in almería could be addressed by its i final causes e g what if the agricultural model in almería is transformed to meet sustainable aquifer yields ii material causes e g what if the available groundwater is increased iii formal causes e g what if the pattern of supplied water resources is transformed or iv efficient causes e g what if farming systems adapt to reduce their water demand these four categories of causal relations span both structural and functional considerations and may be combined in the search for robust solutions with coastal irrigation areas showing high irrigation efficiency rates and with the increment of external transfers blocked by social conflicts 5 5 the two external transfers in the province tajo segura and negratin almanzora are associated with noteworthy socio environmental conflicts in the origin basins hernández mora et al 2014 the main strategy implemented over the past decade in the analyzed irrigation areas lay in increasing the production of alternative water resources in order to replace groundwater i e a formal cause strategy however as observed in our analyses the penetration of these water supplies is still minor in most irrigation areas discussed reasons for this outcome include the combination of the high energy cost of related technologies and the context of increasing electricity prices while lowering crop prices 6 6 the average energy costs in the region range from 0 88 kwh m3 for groundwater pumping to 4 5 kwh m3 for desalination and to 1 66 kwh m3 for wastewater reclamation over the last years farmers in the region strongly championed for subsidies to desalinated water so far begetting little echo in public authorities march et al 2014 this burden is more significant for farming systems other than greenhouses given their more limited margin of benefits as a result and looking into the future either new strategies should be considered or the crop production function of almería is likely to be forcibly altered our diagnostic analysis shows three social ecological patterns involved in the overdraft phenomena namely i intensive greenhouses withdrawing very large quantities of water ii hyper intensive olives drawing from aquifers with extremely low availability and iii mix of open field fruits and vegetables drawing from aquifers with extremely low availability these clusters of wef interconnections face different economic and biophysical constraints that may require diverse adaptive pathways the multiple ways in which these changes may unfold open a space for simulation and scenario analysis in future research our nexus network can be expanded to appraise the viability of alternative solutions capable of maintaining socially desired system functions 5 conclusions nexus research has mobilized a myriad of existing methodological approaches and emphasized interconnections as its core research object this paper furthers these efforts by proposing a theory of relational analysis of nexus networks and a taxonomy of interconnections we argue for the need to attend to competing drivers of nexus problems at different scales and to social ecological interactions nexus networks are conceptualized in a multi level fashion that addresses cross scale feedbacks in terms of bottom up and top down constraints to multi functionality this is illustrated in an application to nexus problems generated by intensive export based crop production in conflict with sustainable management of water resources the scale where this tension is generated is european while regional solutions do not attend the diversity of water food management patterns and increase nexus interdependencies the analysis reveals several interconnected drivers of the aquifer overdraft phenomena at different analytical levels but also in different geographical locations proving the usefulness of the proposed diagnostic tool the research nonetheless is limited to a single snapshot and improved time series are needed for more conclusive results as signaled in many other nexus papers existing data sources face problems of methodological and technical uncertainty and further coordinated efforts are required if the nexus is to become a useful science for policy approach future advances shall be directed towards i embedding the tool in larger extended peer research processes where to implement quality assurance and uncertainty management protocols and ii anticipatory analysis in the assessment of the option space for accommodation of new top down functional constraints or bottom up structural transformations of wef interconnections acknowledgments the authors are grateful to project partners for their useful reviews and support to this research especially to dr carmela iorio for providing guidance on the clustering process this article reflects only the author s view none of the funding agencies are responsible for any use that may be made of the information it contains funding this research has received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 689669 corresponding to the project moving towards adaptive governance in complexity informing nexus security www magic nexus eu the spanish ministry of education culture and sport fpu15 03376 the spanish ministry of science innovation and universities through the maría de maeztu program for units of excellence mdm 2015 0552 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 06 014 appendix b supplementary materials image application 1 appendix data management a key contribution of this paper is the methodological development of a data modeling structure for relational analysis of a wef nexus network with entangled food and water supply a database per resource domain one for agriculture and another for water has been populated with processor data describing network components and their relations for the province of almería spain in fulfilment of the open access requirements of our funding body european union s horizon 2020 research and innovation programme data has been made available under the license creative commons attribution 4 0 international in zenodo repository https zenodo org record 2539219 xdx9prbrcvs as an integrated accounting method the proposed numerical structure of processors requires collecting and integrating data from multiple data sources this appendix describes the data management process from data collection to visualization a 1 sampling data collection and validation data was collected between march and june 2018 from various secondary data sources and direct petition to public administrations in general we found water datasets better documented and accessible than agricultural datasets in what follows we explain the processes of sampling definition of variables data collection and validation used to produce the two databases serving as inputs to the quantitative analysis a the agriculture database input cropdatabase xlsx survey sampling this database contains data on processors describing system components within the agricultural domain these components are hierarchically organized in our network fig 4 of the paper from level 4 l4 describing types of crop production processes to level 3 describing farming systems l3 and level 2 l2 describing irrigation areas level 1 l1 referring to the whole irrigated crop production system in almería is derived from previous levels through scaling operations in the absence of available data on the full population of farming systems we worked with representative typologies for characterizing processors at l4 and l3 besides the analysis of land use data data informing the decisions for which typologies should be included was obtained from a review of grey literature and techno agronomic reports describing local systems of production tolon becerra et al 2013 garcía garcía et al 2016 varela martínez et al 2016 starting with l4 on crop production processes data was petitioned from the andalusian agricultural administration by phone to its office in almería and followed up by email 7 7 for the province of almería data petitions contact is estadistica al capder juntadeandalucia es land use data on irrigated crops in the different agricultural management units to a large extent overlapping irrigation areas in the region was requested for the time series 2010 2015 out of those years only data for the production year 2012 2013 was provided for this reason and considering the absence of available time series for other variables in the crop production processors the analyzes was restricted to a single annual snapshot from all irrigated crops grown in the season 2012 2013 we selected the most representative ones based on two criteria those crops producing more than 10 000 tonne or occupying more than 5000 ha in the selected year in significantly different production systems in terms of yielded tonne ha year namely greenhouses with two cropping periods mc2 greenhouses without crop rotation mc1 open fields with intensive production and open fields with hyper intensive production the resulting list of 24 crop production processors is shown in table a1 crop categories correspond to those in the provided dataset and represent averages for several species for instance citrus includes different types of lemons oranges and mandarins defined with a similar mixed procedure of land use and literature analyses the sample for l3 includes 8 typologies of farming systems showing significantly different patterns of crop production processes on the other hand spatial data on irrigation areas l2 is publicly available in the management plans of the two river basins over which the region is hydrologically divided junta de andalucía 2015 gobierno de españa 2015 therefore the full population of 16 irrigation areas was sampled definition of variables for characterizing processors and their relations processors in this database contain variables characterizing the input output of agricultural production at the different analytical levels these variables and therefore the column vector structure of processors may vary with the level of observation for instance crop production processes are characterized through variables such as water and different nutrient requirements or crop yield while farming systems processors include other input variables such as the total use of fertilizers pesticides or plastics because of the lack of available data on yields at different analytical levels all variables are intensively defined per hectare of land used hierarchical connections between components are therefore operationalized in terms of land use patterns described across levels no sequential connections are considered for the crop production system data collection and validation input data for processor variables were collected from secondary data sources previous studies or reports data sources vary depending on the variable and are all indicated in the database see data source column in processors sheets and dataset key sheet for references most gathered data are estimations obtained through different methods as explained in the comments column in each dataset some variables were produced through simple modeling techniques as in the case of fertilizer use calculated using the guidelines set by the spanish ministry of agriculture and water footprints calculated in previous studies using standardized modeling procedures other data are statistical estimations developed by public institutions based on a number of assumptions such as production ratios water uses or labor requirements they were accessible in online data portals management plans and reports lastly measurements of real instances were only found in land use data obtained through gis methods the methodological and technical uncertainty associated to most of the above described data production methods is noteworthy the lack of a systematic accounting of environmental and social variables in agricultural statistics especially of water related variables hinders the potential of the proposed analytical tool and the capacity to produce meaningful assessments of the sustainability of nexus interconnections this is an open discussion within the nexus research community that needs to further involve public administrations and their statistical offices at all scales of resource governance a the water database input waterdatabase xlsx survey sampling this database contains data on processors describing system components within the water management domain namely irrigation areas as water users water providers and water bodies water suppliers are desalination and reclamation plants plus two external water transfers these components are sequentially connected in our network contrary to the agricultural domain water management districts have georeferenced datasets locating system components therefore we were able to include the whole population in the database with the exception of water suppliers that were characterised as typologies based on different technologies definition of variables for characterizing processors and their relations processors for irrigation areas within the water domain split their water requirements in the pattern of water resources fulfilling those requirements on the other hand processors describing water bodies include their inputs and outputs of water as expressed in their annual hydrological balance lastly water suppliers include the total supply capacity the annual water supply and the ratios of electricity consumption sequential connections in this database refer to the different water flows from different suppliers and or water bodies to the irrigation areas they are expressed in extensive terms hm3 year data collection and validation data was gathered from the websites of the two water districts in which the region of almería is divided the andalusian mediterranean basins water district junta de andalucía 2015 and the segura water district gobierno de españa 2015 georeferenced datasets were publicly available in their spatial data infrastructures however most data for characterizing processors variables had to be extracted from tables in pdf documents of their current management plans river basin districts in spain do not perform an annual accounting of actual water uses available data refer to estimated average annual water balances for the period 2010 2015 despite the obvious uncertainty associated to an estimated five year average we consider this data source more reliable than those of agricultural databases because it is the baseline information used by water districts for the management period 2015 2020 a 2 data organization and processing input database organization the two databases are organized in a similar structure that consist of the following datasets metadata including the description of workbook sheets acronyms list of acronyms used in all sheets datasource key references and links to data sources parameters parameters used for estimations or up scaling of variables instances list of research objects with external referent in the case study relations input data describing either sequential or hierarchical connections processors x series of sheets containing input data for processors describing network components each dataset includes interface data classified along the categories defined in section 2 of this paper four or more categorical variables plus the numerical continuity geographic and temporal scales data sources and comments documenting relevant information on estimation procedures or data sources data processing processor datasets in input databases contain only those variables that are newly defined for each component properties that cannot be derived from a hierarchical relation the rest of variables were aggregated from lower to upper levels through linear or non linear transformations the scaling was perform using r scripts for which the code may be found in the data folder output datasets collected in calibration xlsx contain full processors describing farming systems and irrigation areas with both extensive and intensive variables calibration of scaling operations was performed on only two variables first the overall production of different crops total annual tonnes contrasting model outputs at l1 the whole region of almeria with the annual statistics of the andalusian administration for the region junta de andalucía 2013 second the total water used by irrigation areas according to management plans was compared against the aggregated net water requirements from the model divided by a coefficient of irrigation efficiency to obtain gross water use comparison for these variables is shown in the following figs a1 and a2 fig a1 shows that the model underestimates production of fresh vegetables this means that either land use or yield data in the model is lower than real values on the other hand calculated citrus and olive productions were 20 and 37 larger than statistical estimations for that year respectively regarding water use estimations the mismatch is especially relevant in dm21 the largest irrigation area with intense greenhouse production in cluster ia1 in figs 5 and 6 of the paper this means that either total greenhouse land and or water requirements per hectare for the different crops are larger than reported in data sources or irrigation efficiency is significantly smaller sequential connections between instances of water bodies and users are not considered in water management plans therefore they were calculated through spatial analyses layers of irrigation areas water bodies and suppliers were overlapped and the most probable water sources for the different users located this information was contrasted with total withdrawals from those sources for agricultural purposes and adjusted when necessary in order to obtain an estimation of withdrawal by each irrigation area from each aquifer in almería clustering process the 16 irrigation areas l2 in the region were clustered into 7 typologies based on their land use pattern of farming systems l3 for this purpose profiles associated with quantitative criteria were defined through a qualitative analysis of most representative patterns table a2 and the 16 irrigation areas were coded accordingly see ouput clusterdata xlsx in the data folder for assignation of irrigation areas to each cluster processors variables including withdrawals from aquifers were aggregated for the obtained categories and selected indicators calculated as explained in the following section selection of indicators the following indicators shown in table a3 were extracted or calculated from the previously described datasets structured datasets of indicators can be found in visualization vizdata xlsx visualization the above described indicators were visualized in a canvas with four connected graphs fig 6 of the paper generated using the python programming language and organized in tableau the first graph f6 1 portrays the rate of annual withdrawal from over drafted aquifers in almería split by clustered irrigation areas the second graph f6 2 shows a voronoi tessellation of the pattern of water resources in the seven clustered irrigation areas in terms of relative annual water supply the third graph f6 3 displays three relevant farm production factors in relation to their crop production processes finally the fourth graph f6 4 shows the voronoi tessellation of the relative contribution of different crop production processes to the land use pattern of irrigation areas altogether the visualization enables a relational analysis of several causes for the aquifer overexploitation phenomenon interconnected at different levels in the described nexus network 
617,while a large number of descriptive studies have delineated the interlinkages between water food and energy resources in the last decade there is still need for systematic conceptualization of resource nexus interconnections this paper proposes a theory of relational analysis of the nexus based on the analytical concept of nexus networks a taxonomy of nexus interconnections detailing sequential and hierarchical connections is characterized between and amongst the technosphere and biosphere we illustrate the use of a novel diagnostic tool with regard to its ability to integrate macro meso and microscale drivers of nexus problems we apply this framework to problems generated by intensive crop production for exportation in an arid landscape driven by external markets and sustainable management of water resources driven by public policies in a southern spanish region we elucidate interconnected causal mechanisms for groundwater overexploitation and profile different social ecological patterns on a spatially explicit basis the proposed approach is capable of accounting for the water energy food resource nexus in an integrated and multi level fashion addressing the tensions generated by both multi functionality and resource entanglement in complex social ecological systems graphical abstract image graphical abstract keywords nexus networks metabolic processor relational analysis water energy food nexus multi functionality 1 introduction mainstreamed by international calls for securing resources pressured by entangled global drivers the water energy food nexus notion has gained momentum at an increasing rate in sustainability research and policy agendas interconnections synergies and trade offs are common keywords within the nexus concept where the diversity of definitions has gathered into at least four perspectives i the nexus as governance approaches that seek coordination and harmonization nilsson et al 2012 weitz et al 2017 ii the nexus as the co occurrence of resource use in economic sectors and supply chains also referred to as the resource nexus font vivanco et al 2018 iii the nexus as the interconnection between different resource systems generated by specific activities or technologies d odorico et al 2018 and transversally iv the nexus as transdisciplinary and co production practices in sustainability research howarth and monasterolo 2017 scanlon et al 2017 nexus problems have been characterized as types of wicked or post normal problems underpinned by uncertainty ambiguity contested stakes and unpredictability harwood 2018 furthermore these nexus situations are argued to be the outcome of co existing macro meso and microscale processes cai et al 2018 which can be related to the different conceptualizations of the nexus identified above from a macro perspective the nexus preoccupation arises from the acknowledgement that economic sectors responsible for resource co occurrence are governed by different public policies and private markets with silo dynamics frequently expressing conflictive goals muranetto and witmer 2017 venghaus and hake 2018 therefore a need is claimed for closing the governance gap and improving policy coherence weitz et al 2017 whereas the macroscale is epitomized by this interface between policy and market constraints and economic sectors the mesoscale is represented by the interface between those sectors and the water food and energy resource management domains they connect resource management domains are associated with regional spatial scales since they are responsible for controlling the level of extraction of primary resources on the other hand economic sectors are networks of entities connecting different regions that metabolize extracted resources along production chains franz et al 2017 nexus difficulties at this level are encountered in cross sectoral management approaches when bringing together sectors and managing trade offs between them pahl wostl 2017 stein et al 2018 lastly at the microscale and somewhat paradoxically many localized nexus problems have emerged as the result of techno social innovations aimed at solving the conflict between policy goals cabello and madrid 2014 classical examples of nexus problems in this sense include desalination and irrigation efficiency innovations related to both of these problems add dependencies on energy resources such as electrical energy to drive water pumps to the water food nexus thereby increasing the structural complexity from a management perspective moreover as solutions for reducing pressures over fresh water resources emerge monitoring of social ecological interactions such as water withdrawal or pollution are demanded the nexus concept thus appears as an indicator of the need to improve our capacity to account for the interconnections between complex social and ecological processes tensioned by multi level and conflicting drivers recent empirical research commences to delineate interconnections as the different causal relations in processes involving more than one resource andrews speed et al 2012 bijl et al 2018 yet whereas relations are the core research object of the nexus concept little progress towards a categorization and formalization of interconnections has been made some consensus has gathered around the differentiation between biophysical connections coined as the resource nexus and other social and governance relations cai et al 2018 galaitsi et al 2018 within the resource nexus a first approximation to categorization is the font vivanco et al 2018 taxonomy of direct dependent and interdependent relations their model shows that the co occurrence of water and energy along supply chains can be mostly explained by direct extraction or one way dependencies while feedbacks appear minor their categories however account only for interactions amongst economic sectors at the national and global scales when moving to local and regional contexts interdependencies are likely to gain relevance following the consideration of social ecological interactions albrecht et al 2018 this paper furthers previous efforts in categorizing nexus interconnections by proposing an operational concept of nexus network for this purpose we draw on ideas from some branches of complexity theory namely relational analysis rosen 1958 2005 2012 hierarchy theory ahl and allen 1996 allen and starr 2017 and societal metabolism georgescu roegen 1971 1975 giampietro and mayumi 2000 we add to the nexus pathways concept of vivanco et al 2018 with a multi level conceptualization of nexus networks that includes interactions within and between the biosphere and technosphere this conceptual framework is applied to nexus problems derived from intensive crop production in arid and semiarid landscapes with over exploited water resources which often find solutions in energy intensive technologies the region of almeria in south eastern spain serves as illustrative case study the methodological framework multi scale integrated analysis of societal and ecosystem metabolism musiasem giampietro and mayumi 2000 is applied to quantitatively formalize the network addressing the following question why are groundwater resources over drafted in almería 2 relational analysis of nexus networks 2 1 theoretical ground relational analysis is a variety of complex system analysis that explores the identity of living systems by representing them formally in metabolic networks characterized by the four aristotelian causes material formal efficient and final rosen 2005 this understanding of relational analysis finds roots in theoretical biology and the seminal work of rashevsky 1954 and arbib et al 1973 furthered by rosen 2005 2012 and more recently by louie 2009 2013 2017 into a new conceptual framework to the modeling of living systems within the field of category theory following their discourse relational analysis addresses provocative questions of why by expounding the pluralism of causal relations defined over the components of a system 1 1 for illustrative example assume there exists a constituent component a i the material cause of a refers to the material out of which a is made ii the formal cause of a refers to the pattern form to which the material cause is required to assume in order to form a iii the efficient cause of a is in the majority of cases the agent which transforms the material cause into a adhering to the formal cause in abstract terms some function f iv the final cause reflects the for what of a the focus of relational analysis thus centers on the organizational unity of the analyzed system describing the various roles or functional behaviors of system components more specifically relational analysis views the fuzzy relations between function and structure observed in complex systems as compelling subjects of analysis kampis 1987a 1987b rosen 1970 bechtel and richardson 2010 in this sense described functions may be realized in an impredicative and epistemological manner by different structural elements within the field of hierarchy theory allen and starr 2017 pp 43 67 also elaborate on the impredicative duality of function and structure to be analyzed over a minimum of three analytical levels within a complex system together with relational analysis hierarchy theory helps define core characteristics of components in a complex metabolic network firstly system components are defined at the same time as a part e g a liver as organ of a human being and a whole e g a liver made up of cells secondly as mentioned previously system components handle the coupling between function and a structure e g the function professor must be realized by a particular material instance for example prof mcgonagall thirdly reflecting back on the aristotelian building blocks of relational analysis system components may be arranged sequentially material entailment and hierarchically functional entailment in the formation of a network to clarify a hierarchical disposition of system components represents a chain of definitions of function each functional definition of a component provides the final cause or purpose to the realizing structure s thereby its meaning each function may be realized by one or more structural components made of the material cause expressing their agency efficient cause when operating in an admissible environment and following certain patterns or codes the formal cause the final piece of this framework is the concept of societal metabolism a notion used to characterize the processes of energy and material transformation in a society that are necessary for its continued existence for an overview of its application to ecological economics see martínez alier and schlüpmann 1987 for an overview of its application in sociology see fischer kowalski 1998 the metabolic approach enables the application of relational analysis and hierarchy theory to nexus relations in social ecological systems giampietro 2018a defines such a system as a set of functional and structural components linked by a set of relations and operating within a given boundary to achieve a shared function a given final cause building upon this definition the musiasem accounting framework mayumi and giampietro 2000 has been proposed as one of the available integrative methods for quantitative analysis of the nexus giampietro et al 2014 keairns et al 2016 shannak et al 2018 in the following section we add a conceptualization of nexus networks to this framework 2 2 multi level nexus networks a nexus network can be defined as the set of processes governing the interdependency between water food and energy within and across a given boundary as any network nexus networks are represented with nodes and connecting edges nodes refer to specific processes consuming more than one resource and producing good s or service s in formal operational terms we represent these nodes using the concept of the metabolic processor rosen 2005 p 250 most typically metabolic processors hereafter processors perform work on a material substrate and transform it into something else performing a function in a complex network the elaboration of the processor concept into an analytical tool within societal metabolism theory is a remarkable contribution of the musiasem accounting framework ripa 2017 giampietro 2018b in this vein a processor fig 1 identifies an expected set of relations regarding i a structured process capable of producing a given output or expressing a given function n node in fig 1 ii a profile of inputs and outputs in both the technosphere and the biosphere required for that purpose e1 6 edge in fig 1 in this sense processors embody the impredicative duality between structure and function of a system component processors such as the canonical processor in fig 1 can be connected in different ways in the formation of a nexus network fig 2 presents this point using the same notation found in fig 1 in the analysis of social ecological systems a minimum of three connection domains should be identified i connections that remain within the socioeconomic realm the technosphere ii connections that remain in the ecological realm the biosphere and iii connections bridging socioeconomic and ecological processes the technosphere biosphere boundary in any given nexus problem the co occurrence and entanglement of resources implies causal connections within each of these three connection domains simultaneously in addition to these three connection domains and pursuing relational analysis a second dimension is added differentiating between sequential and hierarchical connections in fig 2 as already elaborated by vivanco et al 2018 sequential connections are material flows between specific activities or sectors therefore modelling expected outputs at a given level of observation in formal terms they are one one or one many allocations with one way or feedback direction determined by the material entailment between processors in contrast to sequential entailment hierarchical connections are determined by functional entailment that is the function to be expressed at one level has been defined as useful as having a proper final cause at the hierarchical level directly higher hierarchical connections are formalized as many to one or many to many mappings between system components expressed at different levels of observation for instance one can represent a network of connected food production processes following various stages of production and processing sequential relations within the technosphere this sequential pathway may then be scaled up through the various agricultural subsectors ultimately into the agricultural sector in a given region hierarchical relations within the technosphere alternatively the same sequential pathway may be scaled up through the different scales of river basins supplying water to the agricultural processors sequential relations across the technosphere biosphere boundary these two example scaling operations are non equivalent hierarchical mappings of the same set of sequential relations and they enable the answering of different questions about the system under analysis as in other system analyses the specification of a nexus network requires a research question or problem that enables the selection of analytical scales and the delimitation of system boundaries and system components with associated functions within the system from a societal metabolism perspective this is equivalent to identifying socioeconomic sectors and their specific activities intertwined with ecological processes within a defined social ecological boundary in the logic of analyzing final causes the functionally defined components at a given level must be further associated to patterns of structural components that are viable within system constraints at a lower level what is equivalent to specifying hierarchical relations following the previous example the agricultural sector would be split in subsectors for instance animal vs vegetal production and subsectors into supply chains that are viable in a given region we aim to design a diagnostic tool capable of analyzing the tensions generated by conflictive goals and multi level drivers using a functionally and structurally defined network these tensions can be described as a problem of multi functionality the fact that different system components coexist within a social ecological system and that their multiple functions need to be sustained while they adapt to external drivers creates many constraints on the type of structural elements that can fulfil those functions the spectrum of viable solutions to nexus problems is limited in the context of competing drivers and any implemented local solution will transform the network of relations by adding new structural constraints 2 3 formalization of nexus networks once the network of metabolic processors is defined in semantic terms the next analytical step is to instantiate them in formal terms a processor is a data array structure that contains information about the profile of inputs and outputs required in a given process the computational form related to this profile shares a number of similarities with that of life cycle assessment lca see heijungs and suh 2002 in relational terms an j indexed socioeconomic processor nj can be formalized as a column vector as shown in equation e1 the elements of this vector e 1 through ei represent for some processor n the set of expected relations defined in intensive terms against a relation or set of relations most typically the relations in a processor are described against just one element e g per unit of a processor output or otherwise immutable quantity such as human activity or land use processor inputs e1 n j e 1 e i further several rowwise partitions are erected in this column vector in the most general sense and following the canonical processor presented in fig 1 the relations of a processor e 1 through ei may be placed in one of four 2 2 relations within the system of interest are always within the technosphere an observation which explains how the six categories in fig 1 may be reduced to four at this stage of the formalization possible categories the combination of two variables each with two possible values creates these four categories the relation s domain technosphere or biosphere and its orientation input or output the set s in eq e2 lists these two variables and elaborates their possible values e2 s t e c h n o s p h e r e b i o s p h e r e i n p u t o u t p u t the last bit of information associated with each relation is its entailment dependency or dependencies i e in the case of a singular dependency whether a cause of one process is produced by or contributes to another process es in the case of a singular dependency we can say that the validity of the first process both depends on and entails the existence of another process es fig 2 depicted the various entailment dependencies possible in our relational analysis processors may therefore be represented as column vectors eq e1 partitioned along the elements of the set s eq e2 the concatenation of all processors described as sparse column vectors then provides the dataset describing the system of interest for the purposes of the analysis as shown in eq e3 it should furthermore be noted that the description made by eq e3 represents the most basic categorization possible an effective analysis necessarily includes additional subledgers dependant on the goals of the analysis for example in analyses of societal metabolism an additional subledger differentiating between fund flow and stock resource is typically regarded as an essential relation refinement georgescu roegen 1971 e3 n 1 n j e 1 1 e 1 j e i 1 e i j generally processors are stored in the dataset in intensive unitary terms to maintain validity care must be taken to relate intensive representations with their original external reference in an analysis of a system of interest the set of intensively described processors may be proportioned by the size of supply vector qs or by any other variable against which relations are defined to fulfil demand vector qd in the context of this work s case study supply vector qs would include mass quantities of e g almonds and vegetables demand vector qd would include various resource demands e g land area requirement and groundwater volume requirement in this vein processors arranged in a network may describe in intensive and or extensive terms the complete set of sequential and hierarchical relations within and between the technosphere and biosphere these two different sets of numerical representation provide two different key types of information about the system of interest king and carbajales dale 2016 eq e4 formalizes the relation between intensively defined processors supply and demand the formalism works for diagnostic analyses as well as anticipatory analysis in an anticipatory analysis changes in resource demand may be estimated from anticipated changes in i technical coefficients intensively defined processor data e g because of technological change and ii supply e g following demographic projections e4 n 1 n j q s q d 3 application to the nexus in arid land crop production in this section we illustrate the process of building a nexus network for analysing the tension between intensive export based crop production and sustainable management of water resources in the province of almería in south eastern spain 3 1 framing the problem almería is one of the driest regions in europe average annual rain 200 600 mm at the same time it represents one of the largest intra european suppliers of fresh vegetables exporting up to 64 of their overall production main importing countries are germany 30 france 16 the netherlands 12 uk 11 and italy 7 agencia tributaria 2013 other major land uses for crops are olive groves almonds and varieties of citrus agriculture sustains 8 of the employment in the region and 2 4 of its gdp junta de andalucía 2013 contrary to other intensive exploitation systems in spain land property is highly distributed with an average farm size of 2 to 3 hectares and where most farmers are associated to cooperatives for marketing purposes varela martínez et al 2016 the region is a renowned example of unsustainable exploitation of groundwater resources in spain with many aquifers classified as severely overexploited and or polluted according to the european water framework directive wfd assessment the european wfd also mandates that all european water bodies must recover to a good status a mandate expressed in terms of environmental goals defined by the year in which a good status shall be recovered following the implementation of new management measures fig 3 depicts these goals for the almería region framing the nexus problem a macroscale tension arises from the goals of water policy which require a reduction of the pressures over water resources and european food markets eager to absorb the increasing local production at the region level the problem translates into the question of how to re organize relations between agricultural and water management regimes agriculture is responsible for 80 of the water usage 62 5 of which is groundwater junta de andalucía 2015 the main solutions on the table so far have been the utilization of alternative water resources desalination and reclamation or external transfers as substitutes for groundwater and the improvement of irrigation efficiency a decade later overdraft rates are not reported to decrease and the contribution of these innovations to the stated purpose is unclear using the framework described above we build a nexus network to provide a relational analysis of the aquifer over exploitation phenomena as mentioned relational analysis focuses on multiple causality in complex problems by elucidating interconnected functions clearly addressing the question of why aquifers are overexploited in almería can be answered from different lenses for instance a governance and power relations reading would illuminate one sort of relevant responses regarding the key actors involved in the problem and their interconnections however this paper focuses on the metabolic reading of the nexus that is on the type of answers provided by sequential material and hierarchical functional entailments between metabolic processors we assume the view that the ultimate final cause for over drafted groundwater resources in present day almería is the maintenance of an agricultural economic model based on valuable crop exports understanding the other causal mechanisms for the overexploitation phenomena requires a threefold analysis first it is necessary to understand what farming systems are driving water demands second it is necessary to look at the role of groundwater within the overall pattern of water resources supplied to those farming systems by irrigation organizations lastly it is necessary to contextualize withdrawals in the hydrological balance of the different aquifers 3 2 building the network considering the whole region of almería as a social ecological system a multi level nexus network was defined connecting key components of food and water management domains identified above fig 4 at the top analytical level 1 these domains are responsible for organizing the implementation of public policies programs or strategic goals for those sectors posing top down constraints over the behavior of lower level system components as mentioned bottom up constraints can be uncovered by adding levels of functional structural metabolic processors to the network in this vein the agricultural sector has been disaggregated into irrigation areas as the organizational units of water management in agriculture and further into farming systems and even further into the different crops produced in each of those farming systems on the other hand sequential connections relate functions within the same level adding more constraints to the network in terms of multi functionality in our network sequential linkages are established within components of the water management system namely water bodies water suppliers and irrigation areas as water users 3 3 operationalizing the network as explained in section 2 the methodological approach proposed in this paper uses the numerical structure of processors to quantify nexus nodes and relations for this purpose one must identify physical instances matching the categories of system components quantify processor variables and profile representative types of processors out of them the most standard procedures are either data driven e g sample a population and cluster it in typologies or semantic driven approaches e g pre define typologies and sample representative instances for the almería case we followed a mixed approach within the agricultural domain we defined typologies of crop production processes and farming systems through a review of previous studies and techno agronomic reports garcía garcía et al 2016 varela martínez et al 2016 and the analyses of land use data for different crops petitioned from the andalusian agricultural administration on the other hand available georeferenced datasets for water system components in river basin management plans junta de andalucía 2015 gobierno de españa 2015 enabled the characterization of the full population of water bodies and irrigation areas crop data was only provided for the productive year 2012 2013 whereas data on water extraction and use is available as an annual average for the 2010 2015 management period therefore the analysis is bound to a single snapshot of the annual metabolism of the system without addressing its time evolution processors variables were populated with data gathered from different secondary sources and integrated in two databases describing the crop production and the water management domains variables were scaled from lower to upper levels in the network according to the land use patterns described across levels structured and documented datasets and r code for scaling operations are published in a zenodo 3 3 https zenodo org record 2539219 xdx9prbrcvs repository and the data management process from collection to visualization is thoroughly described in the appendix given the diversity of data sources and their production methods mostly based on estimations rather than accurate measurements the technical and methodological uncertainty associated to the datasets is notable for the purposes of this paper we restricted data analysis to the most trustful variables used by public institutions in their management plans further efforts in reliable data production for robust integrated assessments of nexus interconnections are needed in the future to address the proposed research question we provide a diagnostic analysis consisting of the interconnected visualization of some variables in food and water processors at different levels in the network namely withdrawal rates from aquifers by different irrigation areas level 2 patterns of water resources use in irrigation areas level 2 patterns of land uses in irrigation areas levels 2 3 4 and crop production factors in different farming systems levels 3 4 groundwater availability and the derived extraction index 4 4 under the water framework directive the extraction index is calculated as the ratio between annual withdrawals and available groundwater are used to check the sustainability of different water food nexus patterns against quantitative impacts on aquifers for analytical purposes the processors describing the 16 irrigation areas are clustered into 7 typologies based on their pattern of farming systems level 3 for this purpose profiles associated with quantitative criteria on land use were defined through a qualitative analysis of most representative patterns and the 16 irrigation areas were coded accordingly see table a3 in the appendix for further explanation and ouput clusterdata xlsx in the data folders for assignation of irrigation areas to each cluster processors variables including withdrawals from aquifers were aggregated for the obtained categories and selected indicators calculated this clustering enables profiling several typologies of social ecological patterns on a spatially explicit basis as presented in the next section 4 results and discussion 4 1 why are aquifers overdrafted in almería we are now in the position of providing a quantitative relational analyzes of our nexus problem fig 5 presents the geographical display of the clustered irrigation areas overlapping aquifers in the region classified by their extraction index fig 6 visualizes the above described variables of several connected processors within the food and water regimes according to the water districts management plans 12 out of 27 aquifers in almería show an extraction index over 0 8 meaning their exploitation rate is considered unsustainable the spatial distribution of this index reveals a certain predominance of high extraction rates along the coastline as compared to inner rural areas these rural irrigation areas are to a large extent dominated by low intensity almond and olive production irrigation area type ia4 farming systems f5 and f7 in fig 6 2 connected aquifers show either low or moderate extractions rates 0 8 1 1 in those areas with presence of vegetables or lettuce production in open fields ia2 and ia3 for those aquifers with extraction index over 1 1 fig 6 1 presents the annual volume of extraction split by type of irrigation area ia in absolute terms most groundwater over extraction in almería is incurred from three aquifers 011 012 and 013 by three irrigation areas clustered in ia1 these areas populated by greenhouse farming systems f1 in fig 6 2 grow a vast amount of fresh vegetables and elicit the highest rates of labor and gross value added generation as observed in fig 6 3 the ia1 cluster has introduced more alternative water resources than the other ias although they still represent a small share of the total water used in relative terms the aquifers showing the highest extraction indices are associated with other types of farming systems predominant in clusters ia5 and ia7 ia5 is represented by a single irrigation region occupied by olive groves in hyper intensive production mode f6 in fig 6 2 this novel system has rapidly extended due to its larger margin of benefits as compared to intensive olive production and spurred by public subsidies f5 whereas its contribution to annual overall crop production in the region is minor fig 6 4 it consumes almost exclusively groundwater from aquifers with very low annual availability 007 009 on the other hand cluster ia7 gathers two large irrigation areas representing the second most relevant contribution to regional water demand and crop production after ia1 fig 6 3 and 4 it combines three main types of farming systems producing mixed vegetables in open fields with lettuce and citrus f3 f4 and f8 in fig 6 2 these farming systems show middle intensities of labor and gross value added generation water resources in these areas blend groundwater from over drafted aquifers with desalinated water and an important share of external transfers even so current withdrawals contribute to surpass the very low availability rates in five aquifers 001 008 from the previous analyses we can conclude that the answer to the question of why aquifers are over drafted in almería is at least twofold on one hand the profitable greenhouse economy has very large demands of water so far fulfilled mostly with groundwater on the other hand we find a large area of less water demanding farming systems relying on aquifers with very limited water available implementing water policy objectives requires ratcheting withdrawals down below availability grey reference line in fig 6 1 the volume of these reductions might not look dramatic in absolute terms but it is an important fraction of water supplies for several irrigation areas as observed in fig 6 3 4 2 discussion in the absence of an effective harmonization between water and agricultural policies in the eu söderberg 2016 the tension between environmental policy objectives and market demands in relation to crop production in almería could be addressed by its i final causes e g what if the agricultural model in almería is transformed to meet sustainable aquifer yields ii material causes e g what if the available groundwater is increased iii formal causes e g what if the pattern of supplied water resources is transformed or iv efficient causes e g what if farming systems adapt to reduce their water demand these four categories of causal relations span both structural and functional considerations and may be combined in the search for robust solutions with coastal irrigation areas showing high irrigation efficiency rates and with the increment of external transfers blocked by social conflicts 5 5 the two external transfers in the province tajo segura and negratin almanzora are associated with noteworthy socio environmental conflicts in the origin basins hernández mora et al 2014 the main strategy implemented over the past decade in the analyzed irrigation areas lay in increasing the production of alternative water resources in order to replace groundwater i e a formal cause strategy however as observed in our analyses the penetration of these water supplies is still minor in most irrigation areas discussed reasons for this outcome include the combination of the high energy cost of related technologies and the context of increasing electricity prices while lowering crop prices 6 6 the average energy costs in the region range from 0 88 kwh m3 for groundwater pumping to 4 5 kwh m3 for desalination and to 1 66 kwh m3 for wastewater reclamation over the last years farmers in the region strongly championed for subsidies to desalinated water so far begetting little echo in public authorities march et al 2014 this burden is more significant for farming systems other than greenhouses given their more limited margin of benefits as a result and looking into the future either new strategies should be considered or the crop production function of almería is likely to be forcibly altered our diagnostic analysis shows three social ecological patterns involved in the overdraft phenomena namely i intensive greenhouses withdrawing very large quantities of water ii hyper intensive olives drawing from aquifers with extremely low availability and iii mix of open field fruits and vegetables drawing from aquifers with extremely low availability these clusters of wef interconnections face different economic and biophysical constraints that may require diverse adaptive pathways the multiple ways in which these changes may unfold open a space for simulation and scenario analysis in future research our nexus network can be expanded to appraise the viability of alternative solutions capable of maintaining socially desired system functions 5 conclusions nexus research has mobilized a myriad of existing methodological approaches and emphasized interconnections as its core research object this paper furthers these efforts by proposing a theory of relational analysis of nexus networks and a taxonomy of interconnections we argue for the need to attend to competing drivers of nexus problems at different scales and to social ecological interactions nexus networks are conceptualized in a multi level fashion that addresses cross scale feedbacks in terms of bottom up and top down constraints to multi functionality this is illustrated in an application to nexus problems generated by intensive export based crop production in conflict with sustainable management of water resources the scale where this tension is generated is european while regional solutions do not attend the diversity of water food management patterns and increase nexus interdependencies the analysis reveals several interconnected drivers of the aquifer overdraft phenomena at different analytical levels but also in different geographical locations proving the usefulness of the proposed diagnostic tool the research nonetheless is limited to a single snapshot and improved time series are needed for more conclusive results as signaled in many other nexus papers existing data sources face problems of methodological and technical uncertainty and further coordinated efforts are required if the nexus is to become a useful science for policy approach future advances shall be directed towards i embedding the tool in larger extended peer research processes where to implement quality assurance and uncertainty management protocols and ii anticipatory analysis in the assessment of the option space for accommodation of new top down functional constraints or bottom up structural transformations of wef interconnections acknowledgments the authors are grateful to project partners for their useful reviews and support to this research especially to dr carmela iorio for providing guidance on the clustering process this article reflects only the author s view none of the funding agencies are responsible for any use that may be made of the information it contains funding this research has received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 689669 corresponding to the project moving towards adaptive governance in complexity informing nexus security www magic nexus eu the spanish ministry of education culture and sport fpu15 03376 the spanish ministry of science innovation and universities through the maría de maeztu program for units of excellence mdm 2015 0552 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 06 014 appendix b supplementary materials image application 1 appendix data management a key contribution of this paper is the methodological development of a data modeling structure for relational analysis of a wef nexus network with entangled food and water supply a database per resource domain one for agriculture and another for water has been populated with processor data describing network components and their relations for the province of almería spain in fulfilment of the open access requirements of our funding body european union s horizon 2020 research and innovation programme data has been made available under the license creative commons attribution 4 0 international in zenodo repository https zenodo org record 2539219 xdx9prbrcvs as an integrated accounting method the proposed numerical structure of processors requires collecting and integrating data from multiple data sources this appendix describes the data management process from data collection to visualization a 1 sampling data collection and validation data was collected between march and june 2018 from various secondary data sources and direct petition to public administrations in general we found water datasets better documented and accessible than agricultural datasets in what follows we explain the processes of sampling definition of variables data collection and validation used to produce the two databases serving as inputs to the quantitative analysis a the agriculture database input cropdatabase xlsx survey sampling this database contains data on processors describing system components within the agricultural domain these components are hierarchically organized in our network fig 4 of the paper from level 4 l4 describing types of crop production processes to level 3 describing farming systems l3 and level 2 l2 describing irrigation areas level 1 l1 referring to the whole irrigated crop production system in almería is derived from previous levels through scaling operations in the absence of available data on the full population of farming systems we worked with representative typologies for characterizing processors at l4 and l3 besides the analysis of land use data data informing the decisions for which typologies should be included was obtained from a review of grey literature and techno agronomic reports describing local systems of production tolon becerra et al 2013 garcía garcía et al 2016 varela martínez et al 2016 starting with l4 on crop production processes data was petitioned from the andalusian agricultural administration by phone to its office in almería and followed up by email 7 7 for the province of almería data petitions contact is estadistica al capder juntadeandalucia es land use data on irrigated crops in the different agricultural management units to a large extent overlapping irrigation areas in the region was requested for the time series 2010 2015 out of those years only data for the production year 2012 2013 was provided for this reason and considering the absence of available time series for other variables in the crop production processors the analyzes was restricted to a single annual snapshot from all irrigated crops grown in the season 2012 2013 we selected the most representative ones based on two criteria those crops producing more than 10 000 tonne or occupying more than 5000 ha in the selected year in significantly different production systems in terms of yielded tonne ha year namely greenhouses with two cropping periods mc2 greenhouses without crop rotation mc1 open fields with intensive production and open fields with hyper intensive production the resulting list of 24 crop production processors is shown in table a1 crop categories correspond to those in the provided dataset and represent averages for several species for instance citrus includes different types of lemons oranges and mandarins defined with a similar mixed procedure of land use and literature analyses the sample for l3 includes 8 typologies of farming systems showing significantly different patterns of crop production processes on the other hand spatial data on irrigation areas l2 is publicly available in the management plans of the two river basins over which the region is hydrologically divided junta de andalucía 2015 gobierno de españa 2015 therefore the full population of 16 irrigation areas was sampled definition of variables for characterizing processors and their relations processors in this database contain variables characterizing the input output of agricultural production at the different analytical levels these variables and therefore the column vector structure of processors may vary with the level of observation for instance crop production processes are characterized through variables such as water and different nutrient requirements or crop yield while farming systems processors include other input variables such as the total use of fertilizers pesticides or plastics because of the lack of available data on yields at different analytical levels all variables are intensively defined per hectare of land used hierarchical connections between components are therefore operationalized in terms of land use patterns described across levels no sequential connections are considered for the crop production system data collection and validation input data for processor variables were collected from secondary data sources previous studies or reports data sources vary depending on the variable and are all indicated in the database see data source column in processors sheets and dataset key sheet for references most gathered data are estimations obtained through different methods as explained in the comments column in each dataset some variables were produced through simple modeling techniques as in the case of fertilizer use calculated using the guidelines set by the spanish ministry of agriculture and water footprints calculated in previous studies using standardized modeling procedures other data are statistical estimations developed by public institutions based on a number of assumptions such as production ratios water uses or labor requirements they were accessible in online data portals management plans and reports lastly measurements of real instances were only found in land use data obtained through gis methods the methodological and technical uncertainty associated to most of the above described data production methods is noteworthy the lack of a systematic accounting of environmental and social variables in agricultural statistics especially of water related variables hinders the potential of the proposed analytical tool and the capacity to produce meaningful assessments of the sustainability of nexus interconnections this is an open discussion within the nexus research community that needs to further involve public administrations and their statistical offices at all scales of resource governance a the water database input waterdatabase xlsx survey sampling this database contains data on processors describing system components within the water management domain namely irrigation areas as water users water providers and water bodies water suppliers are desalination and reclamation plants plus two external water transfers these components are sequentially connected in our network contrary to the agricultural domain water management districts have georeferenced datasets locating system components therefore we were able to include the whole population in the database with the exception of water suppliers that were characterised as typologies based on different technologies definition of variables for characterizing processors and their relations processors for irrigation areas within the water domain split their water requirements in the pattern of water resources fulfilling those requirements on the other hand processors describing water bodies include their inputs and outputs of water as expressed in their annual hydrological balance lastly water suppliers include the total supply capacity the annual water supply and the ratios of electricity consumption sequential connections in this database refer to the different water flows from different suppliers and or water bodies to the irrigation areas they are expressed in extensive terms hm3 year data collection and validation data was gathered from the websites of the two water districts in which the region of almería is divided the andalusian mediterranean basins water district junta de andalucía 2015 and the segura water district gobierno de españa 2015 georeferenced datasets were publicly available in their spatial data infrastructures however most data for characterizing processors variables had to be extracted from tables in pdf documents of their current management plans river basin districts in spain do not perform an annual accounting of actual water uses available data refer to estimated average annual water balances for the period 2010 2015 despite the obvious uncertainty associated to an estimated five year average we consider this data source more reliable than those of agricultural databases because it is the baseline information used by water districts for the management period 2015 2020 a 2 data organization and processing input database organization the two databases are organized in a similar structure that consist of the following datasets metadata including the description of workbook sheets acronyms list of acronyms used in all sheets datasource key references and links to data sources parameters parameters used for estimations or up scaling of variables instances list of research objects with external referent in the case study relations input data describing either sequential or hierarchical connections processors x series of sheets containing input data for processors describing network components each dataset includes interface data classified along the categories defined in section 2 of this paper four or more categorical variables plus the numerical continuity geographic and temporal scales data sources and comments documenting relevant information on estimation procedures or data sources data processing processor datasets in input databases contain only those variables that are newly defined for each component properties that cannot be derived from a hierarchical relation the rest of variables were aggregated from lower to upper levels through linear or non linear transformations the scaling was perform using r scripts for which the code may be found in the data folder output datasets collected in calibration xlsx contain full processors describing farming systems and irrigation areas with both extensive and intensive variables calibration of scaling operations was performed on only two variables first the overall production of different crops total annual tonnes contrasting model outputs at l1 the whole region of almeria with the annual statistics of the andalusian administration for the region junta de andalucía 2013 second the total water used by irrigation areas according to management plans was compared against the aggregated net water requirements from the model divided by a coefficient of irrigation efficiency to obtain gross water use comparison for these variables is shown in the following figs a1 and a2 fig a1 shows that the model underestimates production of fresh vegetables this means that either land use or yield data in the model is lower than real values on the other hand calculated citrus and olive productions were 20 and 37 larger than statistical estimations for that year respectively regarding water use estimations the mismatch is especially relevant in dm21 the largest irrigation area with intense greenhouse production in cluster ia1 in figs 5 and 6 of the paper this means that either total greenhouse land and or water requirements per hectare for the different crops are larger than reported in data sources or irrigation efficiency is significantly smaller sequential connections between instances of water bodies and users are not considered in water management plans therefore they were calculated through spatial analyses layers of irrigation areas water bodies and suppliers were overlapped and the most probable water sources for the different users located this information was contrasted with total withdrawals from those sources for agricultural purposes and adjusted when necessary in order to obtain an estimation of withdrawal by each irrigation area from each aquifer in almería clustering process the 16 irrigation areas l2 in the region were clustered into 7 typologies based on their land use pattern of farming systems l3 for this purpose profiles associated with quantitative criteria were defined through a qualitative analysis of most representative patterns table a2 and the 16 irrigation areas were coded accordingly see ouput clusterdata xlsx in the data folder for assignation of irrigation areas to each cluster processors variables including withdrawals from aquifers were aggregated for the obtained categories and selected indicators calculated as explained in the following section selection of indicators the following indicators shown in table a3 were extracted or calculated from the previously described datasets structured datasets of indicators can be found in visualization vizdata xlsx visualization the above described indicators were visualized in a canvas with four connected graphs fig 6 of the paper generated using the python programming language and organized in tableau the first graph f6 1 portrays the rate of annual withdrawal from over drafted aquifers in almería split by clustered irrigation areas the second graph f6 2 shows a voronoi tessellation of the pattern of water resources in the seven clustered irrigation areas in terms of relative annual water supply the third graph f6 3 displays three relevant farm production factors in relation to their crop production processes finally the fourth graph f6 4 shows the voronoi tessellation of the relative contribution of different crop production processes to the land use pattern of irrigation areas altogether the visualization enables a relational analysis of several causes for the aquifer overexploitation phenomenon interconnected at different levels in the described nexus network 
618,evolving climate conditions and anthropogenic factors such as co2 emissions urbanization and population growth can cause changes in weather and climate extremes most current risk assessment models rely on the assumption of stationarity i e no temporal change in statistics of extremes most nonstationary modeling studies focus primarily on changes in extremes over time here we present process informed nonstationary extreme value analysis proneva as a generalized tool for incorporating different types of physical drivers i e underlying processes stationary and nonstationary concepts and extreme value analysis methods i e annual maxima peak over threshold proneva builds upon a newly developed hybrid evolution markov chain monte carlo mcmc approach for numerical parameters estimation and uncertainty assessment this offers more robust uncertainty estimates of return periods of climatic extremes under both stationary and nonstationary assumptions proneva is designed as a generalized tool allowing using different types of data and nonstationarity concepts physically based or purely statistical into account in this paper we show a wide range of applications describing changes in annual maxima river discharge in response to urbanization annual maxima sea levels over time annual maxima temperatures in response to co2 emissions in the atmosphere and precipitation with a peak over threshold approach proneva is freely available to the public and includes a user friendly graphical user interface gui to enhance its implementation keywords process informed nonstationary extreme value analysis physical based covariates drivers methods for nonstationary analysis 1 introduction natural hazards pose significant threats to public safety infrastructure integrity natural resources and economic development around the globe in recent years the frequency and impacts of extremes have increased substantially in many parts of the world e g melillo et al 2014 coumou and rahmstorf 2012 alexander et al 2006 mazdiyasni et al 2017 mallakpour and villarini 2017 hallegatte et al 2013 wahl et al 2015 vahedifard et al 2016 jongman et al 2014 aghakouchak et al 2014 for this reason there is a great deal of interest in understanding how extreme events will change in the future historical observations are the main source of information on extremes klemeš 1974 koutsoyiannis and montanari 2007 and statistical models are used to infer frequency and variability of extremes based on historical records e g katz et al 2002 statistical models used to study extremes can be broadly categorized into two groups stationary and nonstationary e g salas and pielke sr 2003 coles and pericchi 2003 griffis and stedinger 2007 obeysekera and salas 2013 serinaldi and kilsby 2015 madsen et al 2013 koutsoyiannis and montanari 2015 in a stationary model the observations are assumed to be drawn from a probability distribution function with constant parameters i e statistics of extremes do not change over time or with respect to another covariate in a nonstationary model however the parameters of the underlying probability distribution function change over time or in response to a given covariate sadegh et al 2015 water resources practices e g flood and precipitation frequency analysis have traditionally adopted stationary models however over the past decades increasing surface temperatures e g barnett et al 1999 villarini et al 2010 melillo et al 2014 diffenbaugh et al 2015 fischer and knutti 2015 mazdiyasni and aghakouchak 2015 more intense rainfall events e g zhang et al 2007 villarini et al 2010 min et al 2011 marvel and bonfils 2013 westra et al 2013 cheng et al 2014 fischer and knutti 2016 mallakpour and villarini 2017 changes in river discharge e g villarini et al 2009a 2009b hurkmans et al 2009 stahl et al 2010 and sea level rise e g holgate 2007 haigh et al 2010 wahl et al 2011 have been observed and to a great extent attributed to anthropogenic activities e g human caused climate change urbanization matalas 1997 argued that trend in hydrological records cannot firmly be established because of the variables intrinsic variability and limited length of observations in his reasoning the observed trend might only be part of a slow oscillation consequently matalas 1997 defined hydrological trends as real physical or perceived statistical even though using statistical trend analysis tools inevitably leads to detecting only statistical trends it is important to make a distinction between a trend which has a physical explanation e g increases in runoff in response to urbanization and a trend which cannot be fully explained by our understanding of the underlying processes regardless of the type of observed hydrologic trends i e in response to a physical process or only perceived statistical these trends challenge the stationary assumption milly et al 2008 several studies have promoted the idea of moving away from stationary models to ensure capturing the changing properties of extremes milly et al 2008 however some have criticized this viewpoint particularly because the assumption of nonstationarity implies adding a deterministic component in the stochastic process which must be justified by a well understood process koutsoyiannis 2011 matalas 2012 lins and cohn 2011 koutsoyiannis and montanari 2015 moreover limited observations could affect the exploratory diagnostics used to justify a nonstationary model serinaldi and kilsby 2015 this can potentially lead to higher uncertain in the results of extreme value analysis a nonstationary approach may also involve an assumption on the evolution of the relevant process variable in the future which would add to the overall uncertainty serinaldi and kilsby 2015 when it is not possible to determine a credible prediction of the future koutsoyiannis and montanari 2015 or make a reasonable assumption considering a stationary model may be a more appropriate solution luke et al 2017 concluded that for prediction of river discharge a stationary model should be preferred to avoid over extrapolation in the future however when information about alterations occurred within a watershed is known then an updated stationary model which accounts for the detected changes should be adopted luke et al 2017 in the debate around model assumptions montanari and koutsoyiannis 2014 noted that more efforts should focus on including relevant physical processes in stochastic models and suggested stochastic process based models as a way to bridge the gap between physically based models without statistics and statistical models without physics here we propose a generalized framework named process informed nonstationary extreme value analysis proneva in which the nonstationarity component is defined by a temporal or process based dependence of the observed extremes on an explanatory variable i e a physical driver here process informed refers to the process of incorporating a physical driver into a statistical analysis when there is evidence that the physical driver can alter the statistics of the extremes even though the approach proposed is purely data driven it encourages and facilitates the implementation of informed statistical analysis in light of external knowledge of processes especially for water resources management and risk assessment for example proneva can be used for analyzing changes in extreme temperatures as a function of co2 emissions it is widely recognized that higher amount of co2 in the atmosphere results in a warmer climate e g zwiers et al 2011 fischer and knutti 2015 barnett et al 1999 for this reason co2 emissions can be considered a physical covariate for explaining temperature extremes other examples include temperature or large scale climatic circulations as covariates for rainfall and co2 concentration or temperature as covariates for sea level rise 2 background and method 2 1 nonstationarity extreme value analysis extreme value theory evt provides the bases for estimating the magnitude and frequency of hazardous events including natural and non natural extreme events coles 2001 most applications utilize either the generalized extreme value distribution gev or the generalized pareto distribution gp for describing the behavior of extremes the former is applied to the annual maxima of a variable e g a time series consisting of the most extreme daily rainfall from each year of the record while the latter is used to describe extremes above a predefined threshold e g all independent river flow values above the flood stage both gev and gp allow incorporating nonstationarity through varying parameters several studies have investigated methodologies for testing the assumptions of stationarity and nonstationarity in hydrology climatology and earth system sciences e g katz et al 2002 sankarasubramanian and lall 2003 cooley et al 2007 mailhot et al 2007 huard et al 2009 villarini et al 2009a towler et al 2010 villarini et al 2010 vogel et al 2011 salas et al 2012 zhu et al 2012 willems et al 2012 katz 2013 obeysekera and salas 2013 salas and obeysekera 2014 rosner et al 2014 yilmaz and perera 2014 mirhosseini et al 2014 cheng and aghakouchak 2014 steinschneider and lall 2015 volpi et al 2015 krishnaswamy et al 2015 read and vogel 2015 sadegh et al 2015 mirhosseini et al 2015 mondal and mujumdar 2015 lima et al 2015 2016b 2016a sarhadi and soulis 2017 salas et al 2018 yan et al 2018 bracken et al 2018 ragno et al 2018 a number of packages and software tools are currently available for nonstationary extreme value analysis eva including the r package ismev gilleland et al 2013 gilleland and katz 2016 where nonstationarity is modeled as a linear regression function of generic covariates gilleland et al 2013 extremes offers eva capability and evaluates the underlying uncertainties with respect to parameters gilleland and katz 2016 extremes also allows tail dependence analysis and a declustering technique for peak over threshold analysis the package climextremes available also in python builds upon extremes and includes an estimate of the risk ratio for event attribution analyses r packages vgam and gamlss are available for modeling nonstationarity through generalized additive models see for example villarini et al 2009a the package gevcdn estimates the parameters of a nonstationary gev distribution using a conditional density method cannon 2010 cheng et al 2014 developed a bayesian based framework nonstationary extreme value analysis neva toolbox that estimates the parameters of gev and gp distributions and their associated uncertainty for time dependent extremes available in matlab in the nonstationary case the parameters are modeled as a linear function of time neva also includes return level curves based on the concept of expected waiting time wigley 2009 olsen et al 1998 salas and obeysekera 2014 and effective return level katz et al 2002 the package nonstationary flood frequency analysis estimates the parameters of the log pearson type iii distribution as a linear function of time based on bayesian inference approach luke et al 2017 the tseva toolbox implements the transformed stationary ts methodology described in mentaschi et al 2016 which comprises of first a transformation of a nonstationary time series into a stationary one so that the stationary eva theory can be applied and then a reverse transformation of the results to include the nonstationary components in the gev and the gp distributions despite significant advances a comprehensive framework which incorporates the widely used eva statistical models namely gp gev and lp3 under both stationary and nonstationary assumptions parameters as a function of time or a physical covariates is not available moreover the implementation of newly proposed approaches for return period estimation under the nonstationary assumption is still limited to address the above limitations we present proneva which builds upon neva package cheng et al 2014 but expands to a general nonstationary extreme value analysis indeed in addition to stationary eva proneva allows nonstationary analyses using user defined covariates which could be time or a physical variable fig 1 depicts the core structure of proneva the advantage of performing stationary analysis with physical related covariates resides in the possibility of imposing physical constraints to a statistical model even though such a statistical model nonstationary statistical model is purely data driven it can be constrained using physical information to avoid unrealistic extrapolation proneva offers parameter estimation uncertainty quantification and a comprehensive assessment of the goodness of fit the key features of proneva are described as follows a the model includes the most common distribution functions used for extreme value analysis including the gev gp and lp3 distributions b for nonstationary analysis the users can select both the covariate and the choice of function for describing change in parameters c the covariate can be any user defined physical covariate d the model also includes a default time covariate i e describing change over time without a physical covariate e the function describing change in parameters with respect to the covariate can be linear exponential or quadratic f the users can select the gp distribution threshold peak over threshold as a constant value or as a linear quantile regression function of the choice covariate g proneva estimates the distribution parameters based on a bayesian inference approach h the model allows using a wide range of priors for parameters including the uniform normal and gamma distributions i proneva samples the posterior distribution function of the parameters using a newly developed hybrid evolution markov chain monte carlo mcmc approach which is computationally more efficient than traditional mcmc algorithms searching rugged response surfaces and it provides a robust numerical parameter estimation and uncertainty quantification sadegh et al 2017 j different model diagnostics and model selection indices e g rmse aic bic are implemented to provide supporting information k proneva includes exploratory data analysis tools such as the mann kendall test for monotonic trends and the white test for homoscedasticity in time series l in addition to the source code a graphical user interface gui for proneva is also available for easier implementation see supplementary material finally m proneva is intended for a broad audience and hence it is structured such that users can easily customize and modify it based on their needs we acknowledge that there are other eva methods such as those in serago and vogel 2018 and gilleland and katz 2016 that we have not included in proneva in the reminder of the paper a detailed description of proneva is provided four different example applications are presented with different variables e g precipitation sea level temperature river discharge and different covariates time co2 emissions in the atmosphere urbanization proneva can be used for analyzing annual maxima also known as block maxima using the gev and lp3 distributions and peak over threshold pot or partial duration series using the gp distribution in the following we provide a brief overview of the extreme value models and their parameters 2 2 generalized extreme value gev the gev distribution function is used to model time series of block maxima the national oceanic and atmospheric administration noaa for example derives precipitation intensity duration frequency idf curves based on the gev distribution this distribution is also widely used in other fields including finance seismology and reliability assessment bridge performance assessment e g ming et al 2009 the gev cumulative distribution function is coles 2001 1 ψ g e v x e x p 1 ξ x μ σ 1 ξ for 1 ξ x μ σ 0 μ σ and ξ are the parameters of the distribution μ is the location parameter σ 0 is the scale parameter and ξ is the shape parameter which defines the tail behavior of the distribution the stationary gev model can be extended for dependent series by letting the parameters of the distribution be a function of a general covariate xc i e μ xc σ xc ξ xc coles 2001 hence the nonstationary form of eq 1 is described as 2 ψ g e v x x c e x p 1 ξ x c x μ x c σ x c 1 ξ x c in proneva for each of the three parameters the users can select a function to describe the change in the parameters with respect to the covariate xc table s1 supplementary material the function selected for each parameter does not constrain the functional relationship used for the other parameters to ensure the positivity of the scale parameter σ xc is modeled in the log scale coles 2001 katz 2013 consequently the exponential function is not available for σ xc moreover the shape parameter ξ xc is known to be a difficult parameter to precisely estimate even in the stationary case coles 2001 especially for short time series papalexiou and koutsoyiannis 2013 for this reason only the linear function is included for ξ xc 2 3 generalized pareto gp the gp distribution is used for modeling time series sampled based on the pot method the gp distribution has been applied to precipitation e g de michele and salvadori 2003 earthquake data e g pisarenko and sornette 2003 wind speed holmes and moriarty 1999 and economic data e g gençay and selçuk 2004 among others given a sequence y of independent and random variables for a large enough threshold u the cumulative distribution function of the excesses y e y u conditional on y u is approximated by the gp distribution function coles 2001 3 ψ g p y e 1 1 ξ y e σ 1 ξ in particular if block maxima of y follows a gev distribution then the threshold excesses ye have a gp distribution in which the parameter ξ is equal to the parameter ξ of the corresponding gev distribution coles 2001 in the nonstationary model of the gp distribution both the threshold value and the parameters of the distribution can be modeled as a function of the user covariate xc coles 2001 4 ψ g p y e x c 1 1 ξ x c y e x c σ x c 1 ξ x c where y e x c y u x c analogous to the gev case proneva allows incorporating different functional forms for describing change in parameters over time or with respect to a covariate table s2 the same considerations for the gev parameter functional forms are applied to gp distribution too in addition the users can specify the type of threshold u two quantile based options are available constant or linear in the case of a linear threshold a linear regression quantile model is adopted the α regression quantile function is koenker and bassett 1978 and kyselý et al 2010 5 y m u α r r where 0 α 1 is the quantile y is the column vector of n observations m x c i n with xc being the column vector of covariance and in the n identity vector u u 1 u 0 is the vector of the regression coefficients and r and r are respectively the positive and negative parts of the residuals then u α is calculated as the optimal solution to eq 6 koenker and bassett 1978 kyselý et al 2010 6 α i n r 1 α i n r m i n 2 4 log pearson type iii lp3 the lp3 distribution has been widely used in hydrology for flood frequency analysis particularly after the release of the usgs bulletin 17b u s water resources council 1982 however it has been applied to other studies such as design magnitude of earthquakes gupta and deshpande 1994 and evaluation of apple bud burst time and frost risk farajzadeh et al 2010 the lp3 distribution characterizes the random variable q exp x given that x follows a pearson type iii p3 distribution griffis et al 2007 hereafter the natural logarithm is used however any base can be implemented such as base 10 as in bulletin 17b griffis et al 2007 the p3 probability density function is 7 ψ p 3 x 1 β γ α x τ β α 1 exp x τ β defined for α 0 x τ β 0 and γ α being a complete gamma function griffis et al 2007 the parameters α β and τ are functions of the first three moments μx σx γx griffis et al 2007 8 α 4 γ x 2 9 β σ x γ x 2 10 τ μ x 2 σ x γ x in the case of nonstationary analysis the first three moments are modeled as a function of the user defined covariate xc table s3 the gev and gp considerations mentioned above hold for the functions to describe change in parameters 11 ψ p 3 x x c 1 β x c γ α x c x τ x c β x c α x c 1 exp x τ x c β x c 3 parameter estimation bayesian analysis and markov chain monte carlo sampling proneva estimates the parameters of the selected non stationary eva distribution using a bayesian approach which provides a robust characterization of the underlying uncertainty derived from both input errors and model selection bayesian analysis has been widely implemented for parameter inference and uncertainty quantification e g thiemann et al 2001 gupta et al 2008 cheng et al 2014 kwon and lall 2016 sarhadi et al 2016 sadegh et al 2017 luke et al 2017 sadegh et al 2018 let θ be the parameter of a given distribution and let y y 1 y n be the set of n observations following bayes theorem the probability of θ given y posterior is proportional to the product of the probability of θ prior and the probability of y given θ likelihood function assuming independence between the observations 12 p θ y i 1 n p θ p y i θ the prior brings a priori information which does not depend on the observed data into the parameter estimation process the choice of the prior distribution then is subjective and it is based on prior beliefs about the system of interest sadegh et al 2018 the available prior options in proneva include the uniform normal and gamma distributions providing a variety of possibilities proneva assumes independence of parameters and hence each parameter requires its own prior in the case of a nonstationary analysis the vector of parameters θ includes a higher number of elements than in the stationary case depending on the functional form selected for each of the distribution s parameters the posterior distribution is then delineated using a hybrid evolution mcmc approach proposed by sadegh et al 2017 the mcmc simulation searches for the region of interest with multiple chains running in parallel which share information on the fly moreover the hybrid evolution mcmc benefits from an intelligent starting point selection duan et al 1993 and employs adaptive metropolis am roberts and sahu 1997 haario et al 1999 2001 roberts and rosenthal 2009 differential evolution de storn and price 1997 ter braak and vrugt 2008 vrugt et al 2009 and snooker update gilks et al 1994 ter braak and vrugt 2008 sadegh and vrugt 2014 algorithms to search the feasible space the metropolis ratio is selected to accept reject the proposed sample and the gelman rubin r gelman and rubin 1992 is selected to monitor the convergence of the chains which should remain below the critical threshold of 1 2 for a more detailed description of the algorithm the reader is referred to sadegh et al 2017 4 model diagnostics and selection the purpose of fitting a statistical model whether it is stationary or nonstationary is to characterize the population from which the data was drawn for further analysis inference coles 2001 hence it is necessary to check the performance of the fitted model to the data coles 2001 we implemented different metrics in the proneva for goodness of fit gof assessment and model selection including quantile and probability plots for a graphical assessment see supplementary material two sample kolmogorov smirnov ks test akaike information criterion aic bayesian information criterion bic maximum likelihood ml root mean square error rmse and nash sutcliff efficiency nse coefficient the hybrid evolution mcmc approach sadegh et al 2017 within the bayesian framework provides an ensemble of solutions for the non stationary statistical model fitted to the data proneva uses the best set of parameters θ which maximizes the posterior distribution marginal posteriors will then provide uncertainty estimates of the estimated parameters 4 1 standard transformation when applied to nonstationary applications the lack of homogeneity in the distributional assumption requires an adjustment to the traditional gof techniques coles 2001 consequently proneva standardizes the observations based on the underlying distribution family such that the gof tests can be performed table s4 provides information on the transformation methods in proneva however it is worth noting that the choice of the reference distribution is arbitrary coles 2001 here we selected those transformations that are widely accepted in the literature coles 2001 koutrouvelis and canavos 1999 in the case of a lp3 distribution the transformation can only be applied when the parameter α is constant koutrouvelis and canavos 1999 based on eq 8 this implies that the transformation can be performed only in the case of constant skewness γx 4 2 kolmogorov smirnov test the two sample kolmogorov smirnov ks test is a non parametric hypothesis testing technique which compares two samples z 1 and z 2 to assess whether they belong to the same population massey 1951 being f z 1 z and f z 2 z the unknown statistical distributions of z 1 and z 2 respectively the null hypothesis h 0 is f z 1 z f z 2 z against alternatives the ks test statistic d is 13 d max z f z 1 z f z 2 z h 0 is rejected when the pvalue of the test is equal to or exceeds the selected α level of significance e g 5 we implemented the ks test in proneva as one of the methods to test the goodness of fit of the model specifically proneva generates 1000 random samples from the fitted statistical distribution or in the case of a nonstationary analysis from the reference distribution then the ks test is performed between the random samples and the input original or transformed data finally the rejection rate rr eq 14 is provided as a gof index 14 r r h 0 r e j e c t e d 1000 4 3 model selection based on model complexity a model showing desirable level of performance efficiency with the minimum number of parameters i e a parsimonious model serago and vogel 2018 is usually preferred over a model with similar performance but more parameters e g a nonstationary model with more parameters relative to a simpler stationary model serinaldi and kilsby 2015 luke et al 2017 consequently proneva evaluates different gof metrics i e aic bic which account for the number of parameters within the numerical model the akaike information criterion aic akaike 1974 1998 aho et al 2014 is formulated as follows 15 a i c 2 d l where d is the number of parameters of the statistical model and l is the log likelihood function evaluated at θ the model associated with a lower aic is considered a better fit the bayesian information criterion bic schwarz 1978 is defined as 16 b i c d l n n 2 l where n is the length of records similar to aic the model with lower bic results a better fit 4 4 model selection based on minimum residual root mean square error rmse and nash sutcliff efficiency nse coefficient are two metrics widely used in hydrology and climatology as gof measurements sadegh et al 2018 the focus of both is to minimize the residuals the vector of residual is defined as 17 res f 1 1 n 1 z 1 f 1 i n 1 z i f 1 n n 1 z n following the same notation used for defining the quantile plot hence 18 r m s e i 1 n r e s i 2 n 19 n s e 1 i 1 n r e s i 2 i 1 n z i m e a n z 2 a perfect fit is associated with rmse 0 and nse 1 given rmse 0 inf and nse inf 1 5 predictive distribution the primary objective of a statistical inference is to predict unobserved events renard et al 2013 eva for example provides the basis for estimating loads for infrastructure design and risk assessment of natural hazards e g floods extreme rainfall events considering a bayesian viewpoint the predictive distribution can be written as renard et al 2013 20 f z y f z θ y d θ f z θ f θ y d θ where y is the observed data z is a grid at which f z y will be evaluated θ is the vector of parameters f z θ is the probability density function pdf of the selected distribution i e gev gp lp3 and f θ y is the posterior distribution function the predictive distribution function relies on the fitted distribution function over the parameter space and uses the posterior distribution for uncertainty estimation renard et al 2013 in practice eq 20 often cannot be derived analytically therefore renard et al 2013 suggest to numerically evaluate it using the mcmc derived ensemble of solutions sampled from the posterior distribution the probability density of the kth element of the vector z is 21 f z k y 1 n s i m i 1 n s i m f z k θ i in the nonstationary case the predictive pdf is a function of the covariate since the distribution parameters depend on the covariates for this reason proneva provides the predictive pdf for a number of predefined values of the covariates 6 return level curves under nonstationarity given a time series of annual maxima the return level rl is defined as the quantile qi for which the probability of an annual maximum exceeding the selected quantile is qi cooley 2013 for example let s assume that annual maxima of precipitation intensities p p 1 p n have probability distribution fp the quantile qi is the value of precipitation intensity such that p r p q i 1 f p q i q i under the stationary assumption the characteristics of the statistical model are constant over time meaning that the probability qi of the quantile qi does not change on a yearly basis in this context the concept of return period rp of the quantile qi is defined as the inverse of its exceedance probability t i 1 q i in years referring back to the example of annual maxima of precipitation intensities p let s assume that qi is the precipitation intensity quantile such that the probability of being exceeded in each given year is p r p q i 1 f p q i 0 01 then the rp of qi or rl is t i 1 q i 1 0 01 100 in years under the stationary assumption there is a one to one relationship between rl and rp cooley 2013 therefore the rl curves are defined by the following points 22 t i q i t i 1 y r i 1 rl curves are traditionally used for defining extreme design loads for infrastructure design and risk assessment of natural hazards however in a nonstationary context both rp and rl terms become ambiguous cooley 2013 and numerous studies have attempted to address the issue for nonstationary analysis proneva integrates two different proposed concepts the expected waiting time salas and obeysekera 2014 for default time covariate only and the effective rl curves katz et al 2002 6 1 effective return level katz et al 2002 proposed the concept of effective design value or effective return level which is defined as q quantile q varying as a function of a given covariate i e time or physical therefore for a constant value of r p 1 q where q is the yearly exceedance probability the effective rl curves is defined by the points 23 x c q q x c q 0 1 where xc is the covariate and qq xc is the q quantile 6 2 expected waiting time wigley 2009 first introduced the concept of waiting time i e the expected waiting time until an event of magnitude qi is exceeded in which the probability of exceedance in each year qi changes over time olsen et al 1998 and later salas and obeysekera 2014 provided a comprehensive mathematical description of the suggested concept the event q q 0 is defined as the event with the exceedance probability at time t 0 equal to q 0 under nonstationary conditions at time t 1 the probability of exceedance of q q 0 will be q 1 at time t 2 it will be q 2 and so on given the selected statistical model fq with characteristics θt q t 1 f q q q 0 θ t hence the probability of the event to exceed q q 0 at time m is given by salas and obeysekera 2014 24 f m q m t 1 m 1 1 q t where f 1 q 1 the cumulative distribution function cdf of a geometrical distribution eq 24 is 25 f x x i 1 x f i i 1 x q i t 1 i 1 1 q t 1 t 1 x 1 q t where x is the time at which the event occurs x 1 x max f x 1 q 1 and f x x max 1 therefore the expected waiting time or rp in which for the first time the occurring event exceeds q q 0 can be derived as 26 t e x x 1 x max x f x x 1 x max x q x t 1 x 1 1 q t cooley 2013 simplifies eq 26 as 27 t e x 1 x 1 x m a x t 1 x 1 q t which gives the return period under nonstationary conditions and it is consistent with the definition of rp in the stationary case salas and obeysekera 2014 7 explanatory analysis mann kendall and white tests with the intention of providing explanatory data analysis proneva includes two different tests the mann kendall mk monotonic trend test and the white test wt for evaluating homoscedasticity in the records these tests can be used to decide whether to incorporate a trend function in one or more of the model parameters or not i e deciding whether to use a stationary or nonstationary model however these tests are optional and are not an integral part of proneva the selection of a stationary versus a nonstationary analysis is untied from the tests results but it is left to the users for more details about the mk and wt the readers is referred to the supplementary material and the references therein 8 proneva graphical user interface gui the framework here presented has also a graphical user interface gui fig 2 which we believe can promote and facilitate the application of proneva the user manual included in the package will provide the user with all the instructions needed 9 results as previously discussed the changes in extremes observed over the past years can stem from changes in different physical processes in order to account for the observed changes we need statistical tools that are able to incorporate those variables causing variability which can be represented as time covariate or a physical based covariate in the following we show example applications of proneva under both stationay and nonstationary assumptions including modeling changes induced by different types of covariates both temporal and process based changes it is important to point out that for statistical analyses under both the stationary and nonstationary assumptions the quality of information i e length of record representativeness of observations is fundamental generally the more information is available the more confident we can be about our inferences and also whether or not a model is representative for the application in hand however often observations of extremes are limited the issue of data quality and availability of covariates is also as important for nonstationary analysis for all application representativeness of the choice of model should be rigorously tested using different goodness of test methods in the first application we analyze discharge data from ferson creek st charles il which has experienced intense urban development over the years urbanization has a direct effect on the amount of water discharged at the catchment outlet since it increases impervious surfaces for this reason we use a process informed nonstationary lp3 model for fitting discharge data in which the covariate is represented by percent of urbanized catchment area the second application involves temperature maxima data averaged over the contiguous united states many studies have shown that the amount of co2 in the atmosphere causes temperatures to increase for this reason we fit a nonstationary gev model to temperature data in which the covariate is represented by co2 emissions in the atmosphere to include the underlying physical relationship in the third application we investigate sea level annual maxima in the city of trieste italy which has increased over the years in this case we adopted a temporal nonstationary gev model the last application involves precipitation data for new orleans louisiana in which we fit a stationary gp model given that there is no evidence of change in statistics of extremes 9 1 application 1 modeling discharge with urbanization as the physical driver since 1980 ferson creek st charles il basin has experienced land use land cover changes due to urbanization the percent of urban areas within the catchment has increased from 20 of the total basin s area in 1980 to almost 65 in 2010 river discharge highly depends on the land use and land cover of the basin as it determines the ratio of infiltration to direct runoff fig 3 here urbanization can be considered as a known physical process that has altered the runoff in the basin to incorporate the known physical process we investigate annual maxima discharge of the ferson creek station usgs 05551200 using a process informed nonstationary lp3 model in which the covariate xc is the percent of urbanized area lp3 is widely used for modeling discharge data bulletin 17b u s water resources council 1982 we select a nonstationary model in which the parameter μ is an exponential function of the covariate xc we adopt normal priors for the lp3 parameters fig 4 b shows the results of the process informed nonstationary analysis for an arbitrary value of urbanized area here 37 for the sake of comparison fig 4a displays the results when a stationary model is implemented it is worth noting that the nonstationary model fig 4b fits extreme discharge values high values of return period better than the stationary model fig 4a while based on the aic and bic diagnostic tests the stationary model and the nonstationary model perform rather similarly the rmse of the nonstationary model 25 06 m3 s is considerably lower than that of the stationary model 77 58 m3 s urbanization alters the runoff in the basin by reducing the amount of water that infiltrates and increasing the amount of direct runoff fig 4c shows the ability of the statistical model to incorporate this physical process as anticipated the expected ensemble median nonstationary return level curve associated with a 62 of urbanized area returns higher values of discharge than the one associated with a 37 of urbanized area for example under the nonstationary assumption the magnitude of a 50 year event is 62 47 m3 s for 37 of urbanized area similar to the stationary case however the magnitude of the 50 year event increases to 78 11 m3 s 25 more for 62 of urbanized area on the contrary the stationary analysis estimates a 50 year event as an event with magnitude 63 74 m3 s independent of the level of urbanization of the catchment the result demonstrates that a combination between statistical concepts and physical processes is required for correctly estimating the expected magnitude of an event fig 4d displays the effective return level curves katz et al 2002 which summarize the impact of urbanization on discharge by describing return levels as functions of the selected covariate x axis 9 2 application 2 modeling temperature with co2 as the physical covariate over the past decades many studies have reported increasing surface temperature e g zhang et al 2006 stott et al 2010 melillo et al 2014 zwiers et al 2011 mainly due to anthropogenic activities as a consequence of increase in greenhouse gasses concentration in the atmosphere therefore we investigate annual maxima surface temperature for the contiguous united states available from noaa ncdc archive https www ncdc noaa gov cag national time series using a process informed nonstationary gev model in which the user covariate is represented by co2 emissions over the us fig 5 a territorial fossil fuel co2 emissions data are available on global carbon atlas http www globalcarbonatlas org en co2 emissions boden et al 2017 bp 2017 unfccc 2017 to incorporate the observed relationship between temperature and co2 in the statistical model fig 5b we select a model in which the location and the scale parameters of the gev distribution are linear functions of the covariate while the shape parameter is constant we assume normal priors fig 6 b shows the results of the nonstationary model for a value of co2 equal to 4 9 gtco2 for comparison we also plot the results when a stationary model is selected in fig 6a one can see that the nonstationary model better captures the observed extreme events particularly events associated with higher values of co2 moreover the diagnostics tests confirm that the nonstationary model is a better fit for the nonstationary model the aic and the bic are 93 91 and 104 13 respectively when the stationary model is considered both the aic and bic increase to 104 98 and 111 11 respectively lower values of aic and bic indicate a superior model performance the advantage of the aic and bic for model selection is their ability to account for the number of model parameters models with more parameters are penalized figure s1 shows the effective return level as a function of co2 emissions the results show how temperature extremes change in response to the increasing co2 emissions here the physical covariate for example looking at the expected magnitude of a 50 year event the temperature increases of about 4 from 18 79 c to 19 5 c when the co2 emissions increase from 4 49 gtco2 to 5 51 gtco2 the results are consistent with the expectation that higher co2 leads to a warmer climate indicating that the statistical nonstationary model is able to model the observed physical relationship between temperature and co2 9 3 application 3 modeling sea level rise with time as the covariate the coastal city of trieste italy has been experiencing increasing sea level height over the years fig s2 given the observed trend we investigate annual maxima sea level data from the permanent service for mean sea level psmsl station id 154 by adopting a temporal nonstationary gev model the purpose of this example is to show that proneva can also be used for temporal nonstationary analysis the location and scale parameters of the gev distribution are modeled as linear functions of the time covarite the shape parameter is kept constant and we use normal priors for parameter estimation fig 7 b shows the return level curves for a fixed value of the time covariate equal to 45 years from the first observation i e 45 years into the future from the beginning of the data the nonstationary analysis in fig 7b provides better performance that the stationary model in fig 7a both the aic and the bic values confirm that the nonstationary model is the best choice to represent sea level observations in a changing climate the aic for the nonstationary model is 976 69 while it is 992 74 for the stationary model similarly the bic for the temporal nonstationary model is 989 08 while it is 1000 for the stationary model lower values for aic and bic indicates a superior model the value of the temporal covariate should be regarded as the time at which we estimate expected values of as in this specific case sea level the expected ensemble median nonstationary return level curves in fig 7c refer to three different time at which we evaluate sea level 45 85 and 133 years from the first observation here 133 years from the first observation is beyond the period of observations 88 years meaning that we project into the future the observed trend and we infer from there the observed increasing trend in the sea level records results in increasing values of sea level for higher value of the temporal covariate fig 7c for example a 50 year event is equal to 7296 3 mm for time equal to 45 years from the first observation 7349 3 mm for 85 years and 7410 4 mm for 133 years we register about 2 increase in sea level when the time of the first observation changes from 45 to 133 years confirming the ability of the nonstationary model to reproduce the increasing trend in observations on the contrary the stationary analysis returns a 50 year sea level equal to 7314 3 mm regardless of the first observation fig 7d shows the effective return level curves which capture the variability over time here the covariate in the observed data in the case of a nonstationary model with a temporal covariate it is possible to evaluate the expected waiting time wigley 2009 olsen et al 1998 salas and obeysekera 2014 which incorporates the observed changes in the sea level over time in the estimation of return periods fig s3 shows that the current return periods lower x axis will change considering the observed nonstationarity upper x asis for example the 100 year sea level estimated at t 0 beginning of the simulation turns into a 40 year event when the observed trend over time in sea level values is taken into account 9 4 application 4 modeling precipitation under a stationary assumption this application focuses on the generalized pareto gp distribution for peak over threshold extreme value analysis we investigate a time series of precipitation from new orleans lousiana that does not exhibit changes in statistics of extremes we obtain daily precipitation from the national climatic data center ncdc archive https www ncdc noaa gov cdo web for the city of new orleans station ghcnd usw00012930 given that we are interested in heavy precipitation events we use a gp distribution to focus on values above a high threshold i e avoid including non extreme values we extract precipitation excesses considering a constant threshold of the 98th percentile of daily precipitation values fig s4 for this application we select a stationary gp model given that we do not have physical evidence to justify a more complex model however for the sake of comparison we perform a nonstationary analysis considering the scale parameter as a linear function of time fig 8 a represents the return level curves based on a stationary model while fig 8b depicts return level curves for a value of the covariate here time equal to half of the period of observation from a comparison between the two models the stationary model performs better the stationary model returns values of the aic and bic equal to 713 3 and 721 14 respectively for the nonstationary model the values of the aic and bic are slightly higher 715 02 and 726 79 respectively the results of this example application suggests that when no evidence of changes due to a physical process can be identified proneva favors the simplest form of model that represents the historical observations 10 conclusion the ability to reliably estimate the expected magnitude and frequency of extreme events is fundamental for improving design concepts and risk assessment methods this is particularly important for extreme events that have significant impacts on society infrastructure and human lives such as extreme precipitation events causing flooding and landslides the observed increase in extreme events and their impacts reported from around the world have motivated moving away from the so called stationary approach to ensure capturing the changing properties of extremes milly et al 2008 however there are opposing opinions and perspective on the need and also form of suitable nonstationary models for extreme value analysis most of the existing tools for implementing extreme value analysis under the nonstationary assumption have a number of limitations including lack of a generalized framework for incorporating physically based covariates and estimating parameters which depend on a generic physical covariate to address these limitations we propose a generalized framework entitled process informed nonstationary extreme value analysis proneva in which the nonstationarity component is defined by a temporal or physical based dependence of the observed extremes on a physical driver e g change in runoff in response to urbanization or change in extreme temperatures in response to co2 emissions proneva offers stationary and temporal and process informed nonstationary extreme value analysis parameter estimation uncertainty quantification and a comprehensive assessment of the goodness of fit here we applied proneva to four different types of applications describing change in extreme river discharge in response to urbanization extreme sea levels over time extreme temperatures in response to co2 emissions in the atmosphere we have also demonstrated a peak over threshold approach using precipitation data the results indicate that proneva offers reliable estimates when considering a physical process or time as a covriate the source code of proneva is freely available to the scientific community a graphical user inter face gui version of the model fig 2 is also available to facilitate its applications see supporting information we hope that proneva motivates more process informed nonstationary analysis of extreme events acknowlgedgments this study was partially supported by national science foundation nsf grant cmmi 1635797 national aeronautics and space administration nasa grant nnx16ao56g national oceanic and atmospheric administration noaa grant na14oar4310222 and california energy commission grant 500 15 005 the data used for the four applications of the methodology proposed are freely available online links to the data are provided in the dedicated section we would like to acknowledge the comments of the anonymous reviewers and the editor which substantially improved the quality of this paper supplementary material supplementary material associated with this article can be found in the online version at doi https doi org 10 1016 j advwatres 2019 06 007 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
618,evolving climate conditions and anthropogenic factors such as co2 emissions urbanization and population growth can cause changes in weather and climate extremes most current risk assessment models rely on the assumption of stationarity i e no temporal change in statistics of extremes most nonstationary modeling studies focus primarily on changes in extremes over time here we present process informed nonstationary extreme value analysis proneva as a generalized tool for incorporating different types of physical drivers i e underlying processes stationary and nonstationary concepts and extreme value analysis methods i e annual maxima peak over threshold proneva builds upon a newly developed hybrid evolution markov chain monte carlo mcmc approach for numerical parameters estimation and uncertainty assessment this offers more robust uncertainty estimates of return periods of climatic extremes under both stationary and nonstationary assumptions proneva is designed as a generalized tool allowing using different types of data and nonstationarity concepts physically based or purely statistical into account in this paper we show a wide range of applications describing changes in annual maxima river discharge in response to urbanization annual maxima sea levels over time annual maxima temperatures in response to co2 emissions in the atmosphere and precipitation with a peak over threshold approach proneva is freely available to the public and includes a user friendly graphical user interface gui to enhance its implementation keywords process informed nonstationary extreme value analysis physical based covariates drivers methods for nonstationary analysis 1 introduction natural hazards pose significant threats to public safety infrastructure integrity natural resources and economic development around the globe in recent years the frequency and impacts of extremes have increased substantially in many parts of the world e g melillo et al 2014 coumou and rahmstorf 2012 alexander et al 2006 mazdiyasni et al 2017 mallakpour and villarini 2017 hallegatte et al 2013 wahl et al 2015 vahedifard et al 2016 jongman et al 2014 aghakouchak et al 2014 for this reason there is a great deal of interest in understanding how extreme events will change in the future historical observations are the main source of information on extremes klemeš 1974 koutsoyiannis and montanari 2007 and statistical models are used to infer frequency and variability of extremes based on historical records e g katz et al 2002 statistical models used to study extremes can be broadly categorized into two groups stationary and nonstationary e g salas and pielke sr 2003 coles and pericchi 2003 griffis and stedinger 2007 obeysekera and salas 2013 serinaldi and kilsby 2015 madsen et al 2013 koutsoyiannis and montanari 2015 in a stationary model the observations are assumed to be drawn from a probability distribution function with constant parameters i e statistics of extremes do not change over time or with respect to another covariate in a nonstationary model however the parameters of the underlying probability distribution function change over time or in response to a given covariate sadegh et al 2015 water resources practices e g flood and precipitation frequency analysis have traditionally adopted stationary models however over the past decades increasing surface temperatures e g barnett et al 1999 villarini et al 2010 melillo et al 2014 diffenbaugh et al 2015 fischer and knutti 2015 mazdiyasni and aghakouchak 2015 more intense rainfall events e g zhang et al 2007 villarini et al 2010 min et al 2011 marvel and bonfils 2013 westra et al 2013 cheng et al 2014 fischer and knutti 2016 mallakpour and villarini 2017 changes in river discharge e g villarini et al 2009a 2009b hurkmans et al 2009 stahl et al 2010 and sea level rise e g holgate 2007 haigh et al 2010 wahl et al 2011 have been observed and to a great extent attributed to anthropogenic activities e g human caused climate change urbanization matalas 1997 argued that trend in hydrological records cannot firmly be established because of the variables intrinsic variability and limited length of observations in his reasoning the observed trend might only be part of a slow oscillation consequently matalas 1997 defined hydrological trends as real physical or perceived statistical even though using statistical trend analysis tools inevitably leads to detecting only statistical trends it is important to make a distinction between a trend which has a physical explanation e g increases in runoff in response to urbanization and a trend which cannot be fully explained by our understanding of the underlying processes regardless of the type of observed hydrologic trends i e in response to a physical process or only perceived statistical these trends challenge the stationary assumption milly et al 2008 several studies have promoted the idea of moving away from stationary models to ensure capturing the changing properties of extremes milly et al 2008 however some have criticized this viewpoint particularly because the assumption of nonstationarity implies adding a deterministic component in the stochastic process which must be justified by a well understood process koutsoyiannis 2011 matalas 2012 lins and cohn 2011 koutsoyiannis and montanari 2015 moreover limited observations could affect the exploratory diagnostics used to justify a nonstationary model serinaldi and kilsby 2015 this can potentially lead to higher uncertain in the results of extreme value analysis a nonstationary approach may also involve an assumption on the evolution of the relevant process variable in the future which would add to the overall uncertainty serinaldi and kilsby 2015 when it is not possible to determine a credible prediction of the future koutsoyiannis and montanari 2015 or make a reasonable assumption considering a stationary model may be a more appropriate solution luke et al 2017 concluded that for prediction of river discharge a stationary model should be preferred to avoid over extrapolation in the future however when information about alterations occurred within a watershed is known then an updated stationary model which accounts for the detected changes should be adopted luke et al 2017 in the debate around model assumptions montanari and koutsoyiannis 2014 noted that more efforts should focus on including relevant physical processes in stochastic models and suggested stochastic process based models as a way to bridge the gap between physically based models without statistics and statistical models without physics here we propose a generalized framework named process informed nonstationary extreme value analysis proneva in which the nonstationarity component is defined by a temporal or process based dependence of the observed extremes on an explanatory variable i e a physical driver here process informed refers to the process of incorporating a physical driver into a statistical analysis when there is evidence that the physical driver can alter the statistics of the extremes even though the approach proposed is purely data driven it encourages and facilitates the implementation of informed statistical analysis in light of external knowledge of processes especially for water resources management and risk assessment for example proneva can be used for analyzing changes in extreme temperatures as a function of co2 emissions it is widely recognized that higher amount of co2 in the atmosphere results in a warmer climate e g zwiers et al 2011 fischer and knutti 2015 barnett et al 1999 for this reason co2 emissions can be considered a physical covariate for explaining temperature extremes other examples include temperature or large scale climatic circulations as covariates for rainfall and co2 concentration or temperature as covariates for sea level rise 2 background and method 2 1 nonstationarity extreme value analysis extreme value theory evt provides the bases for estimating the magnitude and frequency of hazardous events including natural and non natural extreme events coles 2001 most applications utilize either the generalized extreme value distribution gev or the generalized pareto distribution gp for describing the behavior of extremes the former is applied to the annual maxima of a variable e g a time series consisting of the most extreme daily rainfall from each year of the record while the latter is used to describe extremes above a predefined threshold e g all independent river flow values above the flood stage both gev and gp allow incorporating nonstationarity through varying parameters several studies have investigated methodologies for testing the assumptions of stationarity and nonstationarity in hydrology climatology and earth system sciences e g katz et al 2002 sankarasubramanian and lall 2003 cooley et al 2007 mailhot et al 2007 huard et al 2009 villarini et al 2009a towler et al 2010 villarini et al 2010 vogel et al 2011 salas et al 2012 zhu et al 2012 willems et al 2012 katz 2013 obeysekera and salas 2013 salas and obeysekera 2014 rosner et al 2014 yilmaz and perera 2014 mirhosseini et al 2014 cheng and aghakouchak 2014 steinschneider and lall 2015 volpi et al 2015 krishnaswamy et al 2015 read and vogel 2015 sadegh et al 2015 mirhosseini et al 2015 mondal and mujumdar 2015 lima et al 2015 2016b 2016a sarhadi and soulis 2017 salas et al 2018 yan et al 2018 bracken et al 2018 ragno et al 2018 a number of packages and software tools are currently available for nonstationary extreme value analysis eva including the r package ismev gilleland et al 2013 gilleland and katz 2016 where nonstationarity is modeled as a linear regression function of generic covariates gilleland et al 2013 extremes offers eva capability and evaluates the underlying uncertainties with respect to parameters gilleland and katz 2016 extremes also allows tail dependence analysis and a declustering technique for peak over threshold analysis the package climextremes available also in python builds upon extremes and includes an estimate of the risk ratio for event attribution analyses r packages vgam and gamlss are available for modeling nonstationarity through generalized additive models see for example villarini et al 2009a the package gevcdn estimates the parameters of a nonstationary gev distribution using a conditional density method cannon 2010 cheng et al 2014 developed a bayesian based framework nonstationary extreme value analysis neva toolbox that estimates the parameters of gev and gp distributions and their associated uncertainty for time dependent extremes available in matlab in the nonstationary case the parameters are modeled as a linear function of time neva also includes return level curves based on the concept of expected waiting time wigley 2009 olsen et al 1998 salas and obeysekera 2014 and effective return level katz et al 2002 the package nonstationary flood frequency analysis estimates the parameters of the log pearson type iii distribution as a linear function of time based on bayesian inference approach luke et al 2017 the tseva toolbox implements the transformed stationary ts methodology described in mentaschi et al 2016 which comprises of first a transformation of a nonstationary time series into a stationary one so that the stationary eva theory can be applied and then a reverse transformation of the results to include the nonstationary components in the gev and the gp distributions despite significant advances a comprehensive framework which incorporates the widely used eva statistical models namely gp gev and lp3 under both stationary and nonstationary assumptions parameters as a function of time or a physical covariates is not available moreover the implementation of newly proposed approaches for return period estimation under the nonstationary assumption is still limited to address the above limitations we present proneva which builds upon neva package cheng et al 2014 but expands to a general nonstationary extreme value analysis indeed in addition to stationary eva proneva allows nonstationary analyses using user defined covariates which could be time or a physical variable fig 1 depicts the core structure of proneva the advantage of performing stationary analysis with physical related covariates resides in the possibility of imposing physical constraints to a statistical model even though such a statistical model nonstationary statistical model is purely data driven it can be constrained using physical information to avoid unrealistic extrapolation proneva offers parameter estimation uncertainty quantification and a comprehensive assessment of the goodness of fit the key features of proneva are described as follows a the model includes the most common distribution functions used for extreme value analysis including the gev gp and lp3 distributions b for nonstationary analysis the users can select both the covariate and the choice of function for describing change in parameters c the covariate can be any user defined physical covariate d the model also includes a default time covariate i e describing change over time without a physical covariate e the function describing change in parameters with respect to the covariate can be linear exponential or quadratic f the users can select the gp distribution threshold peak over threshold as a constant value or as a linear quantile regression function of the choice covariate g proneva estimates the distribution parameters based on a bayesian inference approach h the model allows using a wide range of priors for parameters including the uniform normal and gamma distributions i proneva samples the posterior distribution function of the parameters using a newly developed hybrid evolution markov chain monte carlo mcmc approach which is computationally more efficient than traditional mcmc algorithms searching rugged response surfaces and it provides a robust numerical parameter estimation and uncertainty quantification sadegh et al 2017 j different model diagnostics and model selection indices e g rmse aic bic are implemented to provide supporting information k proneva includes exploratory data analysis tools such as the mann kendall test for monotonic trends and the white test for homoscedasticity in time series l in addition to the source code a graphical user interface gui for proneva is also available for easier implementation see supplementary material finally m proneva is intended for a broad audience and hence it is structured such that users can easily customize and modify it based on their needs we acknowledge that there are other eva methods such as those in serago and vogel 2018 and gilleland and katz 2016 that we have not included in proneva in the reminder of the paper a detailed description of proneva is provided four different example applications are presented with different variables e g precipitation sea level temperature river discharge and different covariates time co2 emissions in the atmosphere urbanization proneva can be used for analyzing annual maxima also known as block maxima using the gev and lp3 distributions and peak over threshold pot or partial duration series using the gp distribution in the following we provide a brief overview of the extreme value models and their parameters 2 2 generalized extreme value gev the gev distribution function is used to model time series of block maxima the national oceanic and atmospheric administration noaa for example derives precipitation intensity duration frequency idf curves based on the gev distribution this distribution is also widely used in other fields including finance seismology and reliability assessment bridge performance assessment e g ming et al 2009 the gev cumulative distribution function is coles 2001 1 ψ g e v x e x p 1 ξ x μ σ 1 ξ for 1 ξ x μ σ 0 μ σ and ξ are the parameters of the distribution μ is the location parameter σ 0 is the scale parameter and ξ is the shape parameter which defines the tail behavior of the distribution the stationary gev model can be extended for dependent series by letting the parameters of the distribution be a function of a general covariate xc i e μ xc σ xc ξ xc coles 2001 hence the nonstationary form of eq 1 is described as 2 ψ g e v x x c e x p 1 ξ x c x μ x c σ x c 1 ξ x c in proneva for each of the three parameters the users can select a function to describe the change in the parameters with respect to the covariate xc table s1 supplementary material the function selected for each parameter does not constrain the functional relationship used for the other parameters to ensure the positivity of the scale parameter σ xc is modeled in the log scale coles 2001 katz 2013 consequently the exponential function is not available for σ xc moreover the shape parameter ξ xc is known to be a difficult parameter to precisely estimate even in the stationary case coles 2001 especially for short time series papalexiou and koutsoyiannis 2013 for this reason only the linear function is included for ξ xc 2 3 generalized pareto gp the gp distribution is used for modeling time series sampled based on the pot method the gp distribution has been applied to precipitation e g de michele and salvadori 2003 earthquake data e g pisarenko and sornette 2003 wind speed holmes and moriarty 1999 and economic data e g gençay and selçuk 2004 among others given a sequence y of independent and random variables for a large enough threshold u the cumulative distribution function of the excesses y e y u conditional on y u is approximated by the gp distribution function coles 2001 3 ψ g p y e 1 1 ξ y e σ 1 ξ in particular if block maxima of y follows a gev distribution then the threshold excesses ye have a gp distribution in which the parameter ξ is equal to the parameter ξ of the corresponding gev distribution coles 2001 in the nonstationary model of the gp distribution both the threshold value and the parameters of the distribution can be modeled as a function of the user covariate xc coles 2001 4 ψ g p y e x c 1 1 ξ x c y e x c σ x c 1 ξ x c where y e x c y u x c analogous to the gev case proneva allows incorporating different functional forms for describing change in parameters over time or with respect to a covariate table s2 the same considerations for the gev parameter functional forms are applied to gp distribution too in addition the users can specify the type of threshold u two quantile based options are available constant or linear in the case of a linear threshold a linear regression quantile model is adopted the α regression quantile function is koenker and bassett 1978 and kyselý et al 2010 5 y m u α r r where 0 α 1 is the quantile y is the column vector of n observations m x c i n with xc being the column vector of covariance and in the n identity vector u u 1 u 0 is the vector of the regression coefficients and r and r are respectively the positive and negative parts of the residuals then u α is calculated as the optimal solution to eq 6 koenker and bassett 1978 kyselý et al 2010 6 α i n r 1 α i n r m i n 2 4 log pearson type iii lp3 the lp3 distribution has been widely used in hydrology for flood frequency analysis particularly after the release of the usgs bulletin 17b u s water resources council 1982 however it has been applied to other studies such as design magnitude of earthquakes gupta and deshpande 1994 and evaluation of apple bud burst time and frost risk farajzadeh et al 2010 the lp3 distribution characterizes the random variable q exp x given that x follows a pearson type iii p3 distribution griffis et al 2007 hereafter the natural logarithm is used however any base can be implemented such as base 10 as in bulletin 17b griffis et al 2007 the p3 probability density function is 7 ψ p 3 x 1 β γ α x τ β α 1 exp x τ β defined for α 0 x τ β 0 and γ α being a complete gamma function griffis et al 2007 the parameters α β and τ are functions of the first three moments μx σx γx griffis et al 2007 8 α 4 γ x 2 9 β σ x γ x 2 10 τ μ x 2 σ x γ x in the case of nonstationary analysis the first three moments are modeled as a function of the user defined covariate xc table s3 the gev and gp considerations mentioned above hold for the functions to describe change in parameters 11 ψ p 3 x x c 1 β x c γ α x c x τ x c β x c α x c 1 exp x τ x c β x c 3 parameter estimation bayesian analysis and markov chain monte carlo sampling proneva estimates the parameters of the selected non stationary eva distribution using a bayesian approach which provides a robust characterization of the underlying uncertainty derived from both input errors and model selection bayesian analysis has been widely implemented for parameter inference and uncertainty quantification e g thiemann et al 2001 gupta et al 2008 cheng et al 2014 kwon and lall 2016 sarhadi et al 2016 sadegh et al 2017 luke et al 2017 sadegh et al 2018 let θ be the parameter of a given distribution and let y y 1 y n be the set of n observations following bayes theorem the probability of θ given y posterior is proportional to the product of the probability of θ prior and the probability of y given θ likelihood function assuming independence between the observations 12 p θ y i 1 n p θ p y i θ the prior brings a priori information which does not depend on the observed data into the parameter estimation process the choice of the prior distribution then is subjective and it is based on prior beliefs about the system of interest sadegh et al 2018 the available prior options in proneva include the uniform normal and gamma distributions providing a variety of possibilities proneva assumes independence of parameters and hence each parameter requires its own prior in the case of a nonstationary analysis the vector of parameters θ includes a higher number of elements than in the stationary case depending on the functional form selected for each of the distribution s parameters the posterior distribution is then delineated using a hybrid evolution mcmc approach proposed by sadegh et al 2017 the mcmc simulation searches for the region of interest with multiple chains running in parallel which share information on the fly moreover the hybrid evolution mcmc benefits from an intelligent starting point selection duan et al 1993 and employs adaptive metropolis am roberts and sahu 1997 haario et al 1999 2001 roberts and rosenthal 2009 differential evolution de storn and price 1997 ter braak and vrugt 2008 vrugt et al 2009 and snooker update gilks et al 1994 ter braak and vrugt 2008 sadegh and vrugt 2014 algorithms to search the feasible space the metropolis ratio is selected to accept reject the proposed sample and the gelman rubin r gelman and rubin 1992 is selected to monitor the convergence of the chains which should remain below the critical threshold of 1 2 for a more detailed description of the algorithm the reader is referred to sadegh et al 2017 4 model diagnostics and selection the purpose of fitting a statistical model whether it is stationary or nonstationary is to characterize the population from which the data was drawn for further analysis inference coles 2001 hence it is necessary to check the performance of the fitted model to the data coles 2001 we implemented different metrics in the proneva for goodness of fit gof assessment and model selection including quantile and probability plots for a graphical assessment see supplementary material two sample kolmogorov smirnov ks test akaike information criterion aic bayesian information criterion bic maximum likelihood ml root mean square error rmse and nash sutcliff efficiency nse coefficient the hybrid evolution mcmc approach sadegh et al 2017 within the bayesian framework provides an ensemble of solutions for the non stationary statistical model fitted to the data proneva uses the best set of parameters θ which maximizes the posterior distribution marginal posteriors will then provide uncertainty estimates of the estimated parameters 4 1 standard transformation when applied to nonstationary applications the lack of homogeneity in the distributional assumption requires an adjustment to the traditional gof techniques coles 2001 consequently proneva standardizes the observations based on the underlying distribution family such that the gof tests can be performed table s4 provides information on the transformation methods in proneva however it is worth noting that the choice of the reference distribution is arbitrary coles 2001 here we selected those transformations that are widely accepted in the literature coles 2001 koutrouvelis and canavos 1999 in the case of a lp3 distribution the transformation can only be applied when the parameter α is constant koutrouvelis and canavos 1999 based on eq 8 this implies that the transformation can be performed only in the case of constant skewness γx 4 2 kolmogorov smirnov test the two sample kolmogorov smirnov ks test is a non parametric hypothesis testing technique which compares two samples z 1 and z 2 to assess whether they belong to the same population massey 1951 being f z 1 z and f z 2 z the unknown statistical distributions of z 1 and z 2 respectively the null hypothesis h 0 is f z 1 z f z 2 z against alternatives the ks test statistic d is 13 d max z f z 1 z f z 2 z h 0 is rejected when the pvalue of the test is equal to or exceeds the selected α level of significance e g 5 we implemented the ks test in proneva as one of the methods to test the goodness of fit of the model specifically proneva generates 1000 random samples from the fitted statistical distribution or in the case of a nonstationary analysis from the reference distribution then the ks test is performed between the random samples and the input original or transformed data finally the rejection rate rr eq 14 is provided as a gof index 14 r r h 0 r e j e c t e d 1000 4 3 model selection based on model complexity a model showing desirable level of performance efficiency with the minimum number of parameters i e a parsimonious model serago and vogel 2018 is usually preferred over a model with similar performance but more parameters e g a nonstationary model with more parameters relative to a simpler stationary model serinaldi and kilsby 2015 luke et al 2017 consequently proneva evaluates different gof metrics i e aic bic which account for the number of parameters within the numerical model the akaike information criterion aic akaike 1974 1998 aho et al 2014 is formulated as follows 15 a i c 2 d l where d is the number of parameters of the statistical model and l is the log likelihood function evaluated at θ the model associated with a lower aic is considered a better fit the bayesian information criterion bic schwarz 1978 is defined as 16 b i c d l n n 2 l where n is the length of records similar to aic the model with lower bic results a better fit 4 4 model selection based on minimum residual root mean square error rmse and nash sutcliff efficiency nse coefficient are two metrics widely used in hydrology and climatology as gof measurements sadegh et al 2018 the focus of both is to minimize the residuals the vector of residual is defined as 17 res f 1 1 n 1 z 1 f 1 i n 1 z i f 1 n n 1 z n following the same notation used for defining the quantile plot hence 18 r m s e i 1 n r e s i 2 n 19 n s e 1 i 1 n r e s i 2 i 1 n z i m e a n z 2 a perfect fit is associated with rmse 0 and nse 1 given rmse 0 inf and nse inf 1 5 predictive distribution the primary objective of a statistical inference is to predict unobserved events renard et al 2013 eva for example provides the basis for estimating loads for infrastructure design and risk assessment of natural hazards e g floods extreme rainfall events considering a bayesian viewpoint the predictive distribution can be written as renard et al 2013 20 f z y f z θ y d θ f z θ f θ y d θ where y is the observed data z is a grid at which f z y will be evaluated θ is the vector of parameters f z θ is the probability density function pdf of the selected distribution i e gev gp lp3 and f θ y is the posterior distribution function the predictive distribution function relies on the fitted distribution function over the parameter space and uses the posterior distribution for uncertainty estimation renard et al 2013 in practice eq 20 often cannot be derived analytically therefore renard et al 2013 suggest to numerically evaluate it using the mcmc derived ensemble of solutions sampled from the posterior distribution the probability density of the kth element of the vector z is 21 f z k y 1 n s i m i 1 n s i m f z k θ i in the nonstationary case the predictive pdf is a function of the covariate since the distribution parameters depend on the covariates for this reason proneva provides the predictive pdf for a number of predefined values of the covariates 6 return level curves under nonstationarity given a time series of annual maxima the return level rl is defined as the quantile qi for which the probability of an annual maximum exceeding the selected quantile is qi cooley 2013 for example let s assume that annual maxima of precipitation intensities p p 1 p n have probability distribution fp the quantile qi is the value of precipitation intensity such that p r p q i 1 f p q i q i under the stationary assumption the characteristics of the statistical model are constant over time meaning that the probability qi of the quantile qi does not change on a yearly basis in this context the concept of return period rp of the quantile qi is defined as the inverse of its exceedance probability t i 1 q i in years referring back to the example of annual maxima of precipitation intensities p let s assume that qi is the precipitation intensity quantile such that the probability of being exceeded in each given year is p r p q i 1 f p q i 0 01 then the rp of qi or rl is t i 1 q i 1 0 01 100 in years under the stationary assumption there is a one to one relationship between rl and rp cooley 2013 therefore the rl curves are defined by the following points 22 t i q i t i 1 y r i 1 rl curves are traditionally used for defining extreme design loads for infrastructure design and risk assessment of natural hazards however in a nonstationary context both rp and rl terms become ambiguous cooley 2013 and numerous studies have attempted to address the issue for nonstationary analysis proneva integrates two different proposed concepts the expected waiting time salas and obeysekera 2014 for default time covariate only and the effective rl curves katz et al 2002 6 1 effective return level katz et al 2002 proposed the concept of effective design value or effective return level which is defined as q quantile q varying as a function of a given covariate i e time or physical therefore for a constant value of r p 1 q where q is the yearly exceedance probability the effective rl curves is defined by the points 23 x c q q x c q 0 1 where xc is the covariate and qq xc is the q quantile 6 2 expected waiting time wigley 2009 first introduced the concept of waiting time i e the expected waiting time until an event of magnitude qi is exceeded in which the probability of exceedance in each year qi changes over time olsen et al 1998 and later salas and obeysekera 2014 provided a comprehensive mathematical description of the suggested concept the event q q 0 is defined as the event with the exceedance probability at time t 0 equal to q 0 under nonstationary conditions at time t 1 the probability of exceedance of q q 0 will be q 1 at time t 2 it will be q 2 and so on given the selected statistical model fq with characteristics θt q t 1 f q q q 0 θ t hence the probability of the event to exceed q q 0 at time m is given by salas and obeysekera 2014 24 f m q m t 1 m 1 1 q t where f 1 q 1 the cumulative distribution function cdf of a geometrical distribution eq 24 is 25 f x x i 1 x f i i 1 x q i t 1 i 1 1 q t 1 t 1 x 1 q t where x is the time at which the event occurs x 1 x max f x 1 q 1 and f x x max 1 therefore the expected waiting time or rp in which for the first time the occurring event exceeds q q 0 can be derived as 26 t e x x 1 x max x f x x 1 x max x q x t 1 x 1 1 q t cooley 2013 simplifies eq 26 as 27 t e x 1 x 1 x m a x t 1 x 1 q t which gives the return period under nonstationary conditions and it is consistent with the definition of rp in the stationary case salas and obeysekera 2014 7 explanatory analysis mann kendall and white tests with the intention of providing explanatory data analysis proneva includes two different tests the mann kendall mk monotonic trend test and the white test wt for evaluating homoscedasticity in the records these tests can be used to decide whether to incorporate a trend function in one or more of the model parameters or not i e deciding whether to use a stationary or nonstationary model however these tests are optional and are not an integral part of proneva the selection of a stationary versus a nonstationary analysis is untied from the tests results but it is left to the users for more details about the mk and wt the readers is referred to the supplementary material and the references therein 8 proneva graphical user interface gui the framework here presented has also a graphical user interface gui fig 2 which we believe can promote and facilitate the application of proneva the user manual included in the package will provide the user with all the instructions needed 9 results as previously discussed the changes in extremes observed over the past years can stem from changes in different physical processes in order to account for the observed changes we need statistical tools that are able to incorporate those variables causing variability which can be represented as time covariate or a physical based covariate in the following we show example applications of proneva under both stationay and nonstationary assumptions including modeling changes induced by different types of covariates both temporal and process based changes it is important to point out that for statistical analyses under both the stationary and nonstationary assumptions the quality of information i e length of record representativeness of observations is fundamental generally the more information is available the more confident we can be about our inferences and also whether or not a model is representative for the application in hand however often observations of extremes are limited the issue of data quality and availability of covariates is also as important for nonstationary analysis for all application representativeness of the choice of model should be rigorously tested using different goodness of test methods in the first application we analyze discharge data from ferson creek st charles il which has experienced intense urban development over the years urbanization has a direct effect on the amount of water discharged at the catchment outlet since it increases impervious surfaces for this reason we use a process informed nonstationary lp3 model for fitting discharge data in which the covariate is represented by percent of urbanized catchment area the second application involves temperature maxima data averaged over the contiguous united states many studies have shown that the amount of co2 in the atmosphere causes temperatures to increase for this reason we fit a nonstationary gev model to temperature data in which the covariate is represented by co2 emissions in the atmosphere to include the underlying physical relationship in the third application we investigate sea level annual maxima in the city of trieste italy which has increased over the years in this case we adopted a temporal nonstationary gev model the last application involves precipitation data for new orleans louisiana in which we fit a stationary gp model given that there is no evidence of change in statistics of extremes 9 1 application 1 modeling discharge with urbanization as the physical driver since 1980 ferson creek st charles il basin has experienced land use land cover changes due to urbanization the percent of urban areas within the catchment has increased from 20 of the total basin s area in 1980 to almost 65 in 2010 river discharge highly depends on the land use and land cover of the basin as it determines the ratio of infiltration to direct runoff fig 3 here urbanization can be considered as a known physical process that has altered the runoff in the basin to incorporate the known physical process we investigate annual maxima discharge of the ferson creek station usgs 05551200 using a process informed nonstationary lp3 model in which the covariate xc is the percent of urbanized area lp3 is widely used for modeling discharge data bulletin 17b u s water resources council 1982 we select a nonstationary model in which the parameter μ is an exponential function of the covariate xc we adopt normal priors for the lp3 parameters fig 4 b shows the results of the process informed nonstationary analysis for an arbitrary value of urbanized area here 37 for the sake of comparison fig 4a displays the results when a stationary model is implemented it is worth noting that the nonstationary model fig 4b fits extreme discharge values high values of return period better than the stationary model fig 4a while based on the aic and bic diagnostic tests the stationary model and the nonstationary model perform rather similarly the rmse of the nonstationary model 25 06 m3 s is considerably lower than that of the stationary model 77 58 m3 s urbanization alters the runoff in the basin by reducing the amount of water that infiltrates and increasing the amount of direct runoff fig 4c shows the ability of the statistical model to incorporate this physical process as anticipated the expected ensemble median nonstationary return level curve associated with a 62 of urbanized area returns higher values of discharge than the one associated with a 37 of urbanized area for example under the nonstationary assumption the magnitude of a 50 year event is 62 47 m3 s for 37 of urbanized area similar to the stationary case however the magnitude of the 50 year event increases to 78 11 m3 s 25 more for 62 of urbanized area on the contrary the stationary analysis estimates a 50 year event as an event with magnitude 63 74 m3 s independent of the level of urbanization of the catchment the result demonstrates that a combination between statistical concepts and physical processes is required for correctly estimating the expected magnitude of an event fig 4d displays the effective return level curves katz et al 2002 which summarize the impact of urbanization on discharge by describing return levels as functions of the selected covariate x axis 9 2 application 2 modeling temperature with co2 as the physical covariate over the past decades many studies have reported increasing surface temperature e g zhang et al 2006 stott et al 2010 melillo et al 2014 zwiers et al 2011 mainly due to anthropogenic activities as a consequence of increase in greenhouse gasses concentration in the atmosphere therefore we investigate annual maxima surface temperature for the contiguous united states available from noaa ncdc archive https www ncdc noaa gov cag national time series using a process informed nonstationary gev model in which the user covariate is represented by co2 emissions over the us fig 5 a territorial fossil fuel co2 emissions data are available on global carbon atlas http www globalcarbonatlas org en co2 emissions boden et al 2017 bp 2017 unfccc 2017 to incorporate the observed relationship between temperature and co2 in the statistical model fig 5b we select a model in which the location and the scale parameters of the gev distribution are linear functions of the covariate while the shape parameter is constant we assume normal priors fig 6 b shows the results of the nonstationary model for a value of co2 equal to 4 9 gtco2 for comparison we also plot the results when a stationary model is selected in fig 6a one can see that the nonstationary model better captures the observed extreme events particularly events associated with higher values of co2 moreover the diagnostics tests confirm that the nonstationary model is a better fit for the nonstationary model the aic and the bic are 93 91 and 104 13 respectively when the stationary model is considered both the aic and bic increase to 104 98 and 111 11 respectively lower values of aic and bic indicate a superior model performance the advantage of the aic and bic for model selection is their ability to account for the number of model parameters models with more parameters are penalized figure s1 shows the effective return level as a function of co2 emissions the results show how temperature extremes change in response to the increasing co2 emissions here the physical covariate for example looking at the expected magnitude of a 50 year event the temperature increases of about 4 from 18 79 c to 19 5 c when the co2 emissions increase from 4 49 gtco2 to 5 51 gtco2 the results are consistent with the expectation that higher co2 leads to a warmer climate indicating that the statistical nonstationary model is able to model the observed physical relationship between temperature and co2 9 3 application 3 modeling sea level rise with time as the covariate the coastal city of trieste italy has been experiencing increasing sea level height over the years fig s2 given the observed trend we investigate annual maxima sea level data from the permanent service for mean sea level psmsl station id 154 by adopting a temporal nonstationary gev model the purpose of this example is to show that proneva can also be used for temporal nonstationary analysis the location and scale parameters of the gev distribution are modeled as linear functions of the time covarite the shape parameter is kept constant and we use normal priors for parameter estimation fig 7 b shows the return level curves for a fixed value of the time covariate equal to 45 years from the first observation i e 45 years into the future from the beginning of the data the nonstationary analysis in fig 7b provides better performance that the stationary model in fig 7a both the aic and the bic values confirm that the nonstationary model is the best choice to represent sea level observations in a changing climate the aic for the nonstationary model is 976 69 while it is 992 74 for the stationary model similarly the bic for the temporal nonstationary model is 989 08 while it is 1000 for the stationary model lower values for aic and bic indicates a superior model the value of the temporal covariate should be regarded as the time at which we estimate expected values of as in this specific case sea level the expected ensemble median nonstationary return level curves in fig 7c refer to three different time at which we evaluate sea level 45 85 and 133 years from the first observation here 133 years from the first observation is beyond the period of observations 88 years meaning that we project into the future the observed trend and we infer from there the observed increasing trend in the sea level records results in increasing values of sea level for higher value of the temporal covariate fig 7c for example a 50 year event is equal to 7296 3 mm for time equal to 45 years from the first observation 7349 3 mm for 85 years and 7410 4 mm for 133 years we register about 2 increase in sea level when the time of the first observation changes from 45 to 133 years confirming the ability of the nonstationary model to reproduce the increasing trend in observations on the contrary the stationary analysis returns a 50 year sea level equal to 7314 3 mm regardless of the first observation fig 7d shows the effective return level curves which capture the variability over time here the covariate in the observed data in the case of a nonstationary model with a temporal covariate it is possible to evaluate the expected waiting time wigley 2009 olsen et al 1998 salas and obeysekera 2014 which incorporates the observed changes in the sea level over time in the estimation of return periods fig s3 shows that the current return periods lower x axis will change considering the observed nonstationarity upper x asis for example the 100 year sea level estimated at t 0 beginning of the simulation turns into a 40 year event when the observed trend over time in sea level values is taken into account 9 4 application 4 modeling precipitation under a stationary assumption this application focuses on the generalized pareto gp distribution for peak over threshold extreme value analysis we investigate a time series of precipitation from new orleans lousiana that does not exhibit changes in statistics of extremes we obtain daily precipitation from the national climatic data center ncdc archive https www ncdc noaa gov cdo web for the city of new orleans station ghcnd usw00012930 given that we are interested in heavy precipitation events we use a gp distribution to focus on values above a high threshold i e avoid including non extreme values we extract precipitation excesses considering a constant threshold of the 98th percentile of daily precipitation values fig s4 for this application we select a stationary gp model given that we do not have physical evidence to justify a more complex model however for the sake of comparison we perform a nonstationary analysis considering the scale parameter as a linear function of time fig 8 a represents the return level curves based on a stationary model while fig 8b depicts return level curves for a value of the covariate here time equal to half of the period of observation from a comparison between the two models the stationary model performs better the stationary model returns values of the aic and bic equal to 713 3 and 721 14 respectively for the nonstationary model the values of the aic and bic are slightly higher 715 02 and 726 79 respectively the results of this example application suggests that when no evidence of changes due to a physical process can be identified proneva favors the simplest form of model that represents the historical observations 10 conclusion the ability to reliably estimate the expected magnitude and frequency of extreme events is fundamental for improving design concepts and risk assessment methods this is particularly important for extreme events that have significant impacts on society infrastructure and human lives such as extreme precipitation events causing flooding and landslides the observed increase in extreme events and their impacts reported from around the world have motivated moving away from the so called stationary approach to ensure capturing the changing properties of extremes milly et al 2008 however there are opposing opinions and perspective on the need and also form of suitable nonstationary models for extreme value analysis most of the existing tools for implementing extreme value analysis under the nonstationary assumption have a number of limitations including lack of a generalized framework for incorporating physically based covariates and estimating parameters which depend on a generic physical covariate to address these limitations we propose a generalized framework entitled process informed nonstationary extreme value analysis proneva in which the nonstationarity component is defined by a temporal or physical based dependence of the observed extremes on a physical driver e g change in runoff in response to urbanization or change in extreme temperatures in response to co2 emissions proneva offers stationary and temporal and process informed nonstationary extreme value analysis parameter estimation uncertainty quantification and a comprehensive assessment of the goodness of fit here we applied proneva to four different types of applications describing change in extreme river discharge in response to urbanization extreme sea levels over time extreme temperatures in response to co2 emissions in the atmosphere we have also demonstrated a peak over threshold approach using precipitation data the results indicate that proneva offers reliable estimates when considering a physical process or time as a covriate the source code of proneva is freely available to the scientific community a graphical user inter face gui version of the model fig 2 is also available to facilitate its applications see supporting information we hope that proneva motivates more process informed nonstationary analysis of extreme events acknowlgedgments this study was partially supported by national science foundation nsf grant cmmi 1635797 national aeronautics and space administration nasa grant nnx16ao56g national oceanic and atmospheric administration noaa grant na14oar4310222 and california energy commission grant 500 15 005 the data used for the four applications of the methodology proposed are freely available online links to the data are provided in the dedicated section we would like to acknowledge the comments of the anonymous reviewers and the editor which substantially improved the quality of this paper supplementary material supplementary material associated with this article can be found in the online version at doi https doi org 10 1016 j advwatres 2019 06 007 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
619,a novel numerical framework is presented for simulation of compositional compressible gas water phase flow in fractured porous media based on a fully implicit cell centred finite volume method on unstructured grids we employ a discrete fracture model where fractures are modelled lower dimensionally within a rock matrix a mass balance equation is solved over the rock matrix while a lower dimensional flow equation is solved over the lower dimensional fractures with coupling terms to account for the flux transfer from the surrounding rock matrix the discretisation of the darcy fluxes is based on the control volume distributed multi point flux approximation cvd mpfa coupled with a lower dimensional fracture model we solve a non linear system for the primary variables that are the phase pressure and the component molar densities while the secondary variables are updated depending on the updated values of the primary variables at each iteration of the non linear solver we use the cubic type equation of state eos to model the physical properties of gas components e g co2 the water phase is multi component as we allow solubility of gas components in the water phase solubility of h2o in the gaseous phase is also included in the developed method we compare compositional simulation results using unstructured grids obtained via the lower dimensional fracture model and the results obtained by the equi dimensional fracture model for a case of co2 injection into a water saturated reservoir with intersecting fractures the results show good agreement between the two models while the lower dimensional fracture model is also computationally cost effective results are also presented for the evolution of injected co2 under gravity through a water saturated reservoir with and without fractures that demonstrate the profound effects of discrete fractures that can lead to co2 leakage the presented method is also applied to a 3d simulation of injection of a multi component gas with co2 and ch4 into a saline water saturated reservoir with a surface 2d fracture network keywords finite volume cvd mpfa compositional two phase unstructured grid co2 1 introduction the water h2o and the carbon dioxide co2 molecules in the air are responsible of absorbing thermal energy to cause greenhouse effect other greenhouse gases are methane ch4 nitrous oxide n2o and synthetic halocarbons cfcs but co2 is the most abundant in the atmosphere rising level of co2 in the atmosphere is one of the main contributions to the increase in global average temperature the geological storage of co2 is regarded as one of the most effective techniques to contain the level of co2 in the atmosphere metz et al 2005 the depleted oil and gas reservoirs deep saline aquifers water saturated rocks and coal beds are the potential sites for co2 storage the saline aquifers are the most effective storage sites because of their large storage capacity and broad distribution around the world zhao et al 2010 fluid migration during the storage of co2 involves complexities because of geological heterogeneities fractures networks multi component multi phase flow co2 dissolution pore space trapping mineralization metz et al 2005 ren et al 2017 efficient numerical modelling and simulation is a tool that is vital to understanding managing and predicting such complex processes of greenhouse gases injection into potential geological sites the phase behaviour of co2 brine and other gas components is essential to understanding of the fluid migration in subsurface conditions the pioneer work started from 1980s on the thermodynamic modelling of co2 carbon monoxide co and hydrocarbons li and nghiem 1986 harvey and prausnitz 1989 zuo and guo 1991 these models show a good behaviour at low salinities low temperatures and low pressures but these are not suitable for applications in subsurface conditions søreide and whitson 1992 proposed a cubic model based on the modification of peng robinson model peng and robinson 1976 to calculate the mutual solubilities of light hydrocarbons i e co2 nitrogen n2 hydrogen sulphide h2s and h2o with salt nacl dissolved the model can reproduce mutual solubility measurements when molality moles in 1 kg of h2o molal for short hereafter of nacl is lower than 2 molal at various temperatures and pressures li et al 2015a made modifications based on the søreide whitson model and the model can calculate mutual solubilities of co2 ch4 h2s brine system to high temperatures pressures and salinities all the mentioned models use the same form of the equation of state eos for both the gaseous phase and the water phase these are called fugacity fugacity models springer et al 2012 li et al 2014 another type of a model is the fugacity activity model duan and others worked on modelling of a series of gas solubility in water and brine based on comprehensive review of solubility measurements of co2 ch4 n2 h2s and oxygen o2 duan and sun 2003 duan and mao 2006 li and duan 2011 geng and duan 2010 mao et al 2013 they used the virial type eos duan et al 1992 to calculate gas fugacities and the pitzer model pitzer 1973 to calculate activities of aqueous species their models can accurately calculate single component gas solubility but are hard to extend to multi gas components li et al 2015b established the mutual solubility model of light hydrocarbon and brine systems the model uses the peng robinson model peng and robinson 1976 for the gas fugacities and the pitzer model for the water activities li et al 2014 compared the two types of model of gas water mutual solubilities in terms of model accuracy and computation efficiency and concluded that the fugacity activity model is superior to the fugacity fugacity model here we use and implement the fugacity activity type model of li et al 2015b for the simulation of co2 injection into fractured water saturated reservoirs where the co2 dissolution is also incorporated fractures are a system of rock discontinuities e g faults joints and fissures that occur in porous media with apertures having widths ranging over scales from microns to centimetres reiss 1980 flow in any rock is affected by a few large fractures by a dense network of small fractures or by a combination of fractures of varying length scales ranging from microns to hundreds of kilometres erhel et al 2009 berkowitz 2002 the well connected high permeable fracture networks provide high permeability to the reservoir that leads to requirement of less number of injection wells in recent years increasing effort is being devoted to the development of efficient and accurate numerical methods based on discrete fracture method to simulate the fluid flow through fractured porous media in the discrete fracture method dfm see e g noorishad and mehran 1982 baca et al 1984 lough et al 1998 lee et al 2001 karimi fard and firoozabadi 2003 karimi fard et al 2004 martin et al 2005 reichenberger et al 2006 matthäi et al 2007 hoteit and firoozabadi 2008a paluszny et al 2007 hajibeygi et al 2011 girault et al 2015 actual orientation and location of the fractures are honoured in the domain unlike the conventional model called the dual porosity method barenblatt et al 1960 warren and root 1963 kazemi et al 1976 gilman 2003 the effect of individual fractures on fluid flow can be determined and the fluid transfer between the fracture and the matrix is more straightforward and consistent generally the fractures are modelled by n 1 dimensional elements in an n dimensional domain for example in 2d fractures are represented by lines which coincide with the edges of polygonal matrix elements berre et al 2018 an unstructured grid is used to honour the explicit fracture geometry where grid cells can be boundary aligned with fractures see mallison et al 2010 mustapha et al 2011 equi dimensional representation of fractures hægland et al 2009 are not popular because of complexity and computational cost contributed by thin cells different techniques of flow approximation in a discrete fracture matrix system have been presented finite element and extended finite element xfem methods for embedded fractures into non conforming meshes are presented in foster and nejad 2013 d angelo and scotti 2012 and berrone et al 2014 mixed hybridized finite element mhfe and discontinous galerkin dg methods hoteit and firoozabadi 2008a are presented for multi component compressible flow in moortgat and firoozabadi 2013 zidane and firoozabadi 2014 zidane and firoozabadi 2015 and moortgat et al 2016 other recent methods for discrete fracture model simulations include the mimetic finite difference method al hinai et al 2015 and the vertex approximate gradient vag method masson et al 2014 brenner et al 2014 a finite volume method based on the two point flux approximation tpfa has been presented for discrete fracture matrix simulations in karimi fard et al 2004 where fractures are n 1 dimensional in the physical mesh but are expanded to n dimensional in the computational domain the method has been extended to control volume distributed multi point flux approximation cvd mpfa o method default quadrature q 1 in sandve et al 2012 for the single phase flow a recently developed cell centred finite volume cvd mpfa method coupled with a lower dimensional fracture model has been presented in ahmed et al 2015a b for single phase flow simulations the method models fractures by n 1 dimensional entities within n dimensional rock both in the physical mesh as well as in the computational domain very recently the method has been applied to two phase flow problems in gläser et al 2017 and xie and edwards 2019 without any compositional behaviour in this work we extend the cvd mpfa method coupled with a lower dimensional fracture model to isothermal compositional two phase water and gas phase flow through fractured porous media with a focus on co2 injection into saline aquifers the method has been implemented into our newly developed reservoir simulator named as dmflow that also includes the advance fugacity activity type model of mutual solubilities as discussed above we validate our numerical framework by a comparison with tough2 numerical simulator for the compositional two phase flow problem of co2 injection into a water saturated reservoir without fractures moreover we present a comparison of the compositional results obtained by the equi dimensional fracture model and the lower dimensional fracture model for the intersecting fractures using unstructured grid the results show that the lower dimensional fracture model produces results comparable to the equi dimensional fracture model for the compositional two phase flow problems moreover the numerical simulations presented for complex fracture networks within the water saturated reservoirs show the robustness of the presented numerical framework the paper is outlined as follows we describe the flow equations of isothermal compositional two phase water and gas phase fluid flow through porous media in section 2 the thermodynamic model of the compositional two phase water and gas phase fluid is briefly discussed in section 3 the discretisation of the flow equations for the fractured porous media is described in section 4 we present a comparison of a numerical simulation with the tough2 numerical simulator in section 5 we then present numerical simulation results based on the presented method for fractured media in section 6 before concluding the work in section 7 2 flow equations 2 1 mass conservation and the darcy s law in this work we focus on discretisation of the equations of the mass conservation of all the components in a compressible two phase water and gas phase flow of an nc component mixture which is given as follows 1 φ m i t x i w ξ w v w x i g ξ g v g q i i 1 n c where mi is the overall moles of component i per pore volume kmol m 3 and by mass conservation m i m i w m i g total moles of all components per unit pore volume is written as m t i 1 n c m i the porosity is denoted by φ qi denotes the injecting molar flow rates of the ith component and ξw and ξg denote the molar densities kmol m 3 of the water and gas phases respectively xiw and xig are the mole fractions of component i in the water and gas phases respectively which are defined by x i α m i α j 1 n c m j α α w g the mole fractions of the components in each phase α are constraint by i 1 n c x i α 1 in eq 1 v w and v g are the velocities m day of the water and gas phases these velocities are given by darcy s law 2 v α k α μ α p α ρ α g z α w g where the pressure of phase α is denoted by pα bar and z denotes the vertical height m which is positive in downward direction in the above equation note that we assume that molecular diffusion and hydraulic dispersion are negligible the magnitude of the gravitational acceleration is denoted by g here k α k r α k where k is the absolute permeability md tensor of the porous medium which is known from geological data in general k is a symmetric tensor with possibly non zero off diagonal terms k k 11 k 12 k 12 k 22 in 2d and k 11 k 12 k 13 k 12 k 22 k 23 k 13 k 23 k 33 in 3d krα is the relative permeability of phase α in the porous medium and is assumed to be known in terms of the phase saturation the phase viscosity μα cp molar density ξα and mass density ρα kg m 3 are the functions of the respective phase pressures and compositions α w g the phase pressures pα are related by the capillary pressure p c p g p w the capillary pressure is a known function of phase saturation only which is the standard model the more advanced models of capillary pressure that involve inter facial areas along with the saturation e g see niessner and hassanizadeh 2008 will be included in future work using eq 2 to replace phase velocities and the capillary pressure definition p c p g p w to replace pg the conservation eq 1 is written as 3 φ m i t α x i α ξ α k α μ α p w x i α ξ α k α μ α ρ α g z x i g ξ g k g μ g p c q i i 1 n c 2 2 volume balance constraint an additional equation is needed to completely define a compositional multiphase problem this equation is usually referred to as a volume balance constraint chien et al 1985 schlumberger 2012 it is the constraint that fluid volume vf should be equal to the pore volume vp it can be written as a saturation constraint equation 4 s w s g 1 0 where the phase saturation sα is defined as s α m α ξ α the overall moles of a phase per unit pore volume mα is given by m α l α m t where lα is the phase mole fraction having a constraint of α l α 1 0 3 thermodynamic model when we solve the mass balance eq 3 mole fractions xiw and xig of each component in each phase are the secondary variables these are also input parameters for density and viscosity calculations mole fractions are calculated by the phase partitioning models we assume that the water and gas phases are always in a phase equilibrium state the distribution of each hydrocarbon into the two phases is subject to the condition of stable thermodynamic equilibrium which is given by minimizing the gibbs free energy of the compositional system chen et al 2006 schlumberger 2012 5 ψ i w p w x 1 w x 2 w x n c w ψ i g p g x 1 g x 2 g x n c g where ψiw and ψig are the chemical potential of water and gas phases respectively the chemical potential of solutes in each phase can be expressed in terms of fugacity or activity ψ i w t p g x i g ψ i g 0 t r t ln f i t p g x i g ψ i g 0 t r t ln x i g p g r t ln ϕ i t p g x i g and ψ i w t p w m s o l ψ i w 0 t p w r t ln a i t p w m s o l ψ i w 0 t p w r t ln m h 2 o x i w r t ln γ i t p w m s o l where ψig denotes the chemical potential of component i in gas phase and ψiw denotes the chemical potential of component i in water phase t is the temperature in kelvins k r is the gas constant which is 8 31446 j k mol ϕi is the fugacity coefficient of component i in gas phase m s o l is the molality of salt in water phase m h 2 o is the mole number of 1 kg h2o which is equal to 55 508 moles ψ i g 0 is the standard chemical potential of component i in gas phase which is the ideal gas chemical potential at the pressure of 1 bar and ψ i w 0 is the standard chemical potential of component i in ideal water solution with a hypothetical unit molality duan and sun 2003 li and duan 2011 the equilibrium constant of component i can be defined as li and duan 2011 6 ln k i ψ i w 0 ψ i g 0 r t ln f i a i since ψ i w ψ i g under equilibrium condition we have x i g ϕ i p m h 2 o k i x i w γ i where ki is a function of temperature and pressure and it can be regressed from the experimental data of gas solubility in pure water ϕi is calculated from peng robinson model peng and robinson 1976 γi can be calculated from pitzer model pitzer 1973 phase mole fractions lw and lg are calculated by solving rachford rice equation michelsen and mollerup 2007 p 259 unknowns xiw and xig are solved for iteratively for details we refer to li et al 2015b gas phase density and viscosity are calculated by the cubic type equation of state peng and robinson 1976 for water phase density we consider the influences of dissolved salt and gas components we denote salt component in brine by nacl and number of gas components by n c g in aqueous solution with 1 kg h2o we calculate the water phase density following duan and mao 2006 as follows 7 ρ w 1000 m nacl m nacl i 1 n c g m g i m g i 1000 m nacl m nacl i 1 n c g m g i m g i ρ h 2 o nacl i 1 n c g m g i v g i where m nacl is the molality of nacl salinity dissolved in a solution m nacl is the molar weight of nacl ρ h 2 o nacl is the density of solution with m nacl salt dissolved and free of any gas component m g i is the molality of gas component i m g i is the molar weight of gas component i and v g i denotes the partial molar volume of gas component i ρ h 2 o nacl is calculated following the method presented in spivey et al 2004 for the calculation of water phase viscosity we follow the method of spivey et al 2004 where the gas effect is not considered 4 numerical model we use the finite volume cvd mpfa method to discretise the flow eq 3 in space and implicit euler scheme for time stepping the flow equation is integrated over each grid cell control volume using the gauss divergence theorem to obtain 8 ω l φ m i t d v ω l ω l α x i α ξ α k α μ α p w x i α ξ α k α μ α ρ α g z x i g ξ g k g μ g p c n d a ω l q i d v ω l i 1 n c 4 1 matrix flow model the discrete approximation of the eq 8 for the matrix cells is written in terms of the out ward fluxes on all the sub interfaces half edges in 2d of the grid cells 9 φ m i t v ω l k n s ω l α x i α ξ α k r α μ α f α k q i v ω l i 1 n c where n s ω l denotes the total number of sub interfaces of the grid cell ω l f α is the matrix matrix and matrix fracture total flux outward normal to the matrix cell corresponding to the phase α the total fluxes are written as f w f w ρ w g f z and f g f w ρ g g f z f c where f w k p w n k f z k z n k and f c k p c n k denote the outward normal fluxes on sub interface k corresponding to the water phase pressure the vertical height and the capillary pressure respectively in this work the capillary pressure is always continuous across the matrix matrix and matrix fracture interfaces as the focus is on the compositional behaviour with the co2 dissolution in the water phase in fractured reservoirs we will include the discontinuous capillarity ahmed et al 2019 in the future work applying the first order upwind scheme the terms x i α ξ α k r α μ α α w g at the interfaces are computed according to the upstream direction of the corresponding total fluxes f α α w g similar to the approach as discussed in zhou et al 2011 without fractures the phase mass densities ρα used in the above equation at a sub interface are computed by averaging the phase densities related to the two cells sharing the sub interface 4 1 1 cvd mpfa flux approximation the fluxes are approximated by the cvd mpfa method edwards and rogers 1998 friis et al 2008 coupled with lower dimensional fractures ahmed et al 2015a 2015b here we use the triangular pressure support tps scheme of the cvd mpfa method the full pressure support fps scheme of the cvd mpfa method has been presented in ahmed et al 2017 for discrete fracture matrix flow simulations we outline the approximation of the matrix matrix and matrix fracture flux f k p n k corresponding to a generic pressure p that is applicable to fluxes fw fz fc corresponding to the water phase pressure the vertical height and the capillary pressure we work on a cluster of cells common to a given vertex and approximate fluxes on all the sub interfaces cell half edges in 2d consider a cluster of cells attached to the vertex v as shown in fig 1 the lower dimensional fracture cells are modelled by the interfaces edges between the matrix cells pressures on all the centroids of the matrix and the fracture cells are also shown in fig 1 following the tps scheme we introduce auxiliary interface pressures on all the sub interfaces as shown in the fig 1 to build the pressure variations in all the sub cells the auxiliary interface pressures are continuous across the sub interfaces that are not fracture cells following the discontinuous pressure fracture model ahmed et al 2015a we introduce two more auxiliary pressures on both sides of the sub interfaces that are fractures e g p f 1 and p f 1 on the right and the left sides of the interface i f 1 respectively that is the fracture cell with centroidal pressure p f1 the points of flux continuities and the auxiliary interface pressures are defined by the parametric variation of quadrature points 0 q 1 along the sub interface for the symmetric positive definite spd scheme q 2 3 friis et al 2008 the points of auxiliary interface pressures on the sub interfaces that are fracture cells are always at the centroids that correspond to q 1 in each of the matrix sub cells of a cluster pressure has a piecewise linear variation e g for the sub cell of the cell with the centroid m 1 fig 1 pressure variation is written in terms of barycentric coordinates ξ η where p 1 ξ η p m 1 ξ p f 1 η p a a darcy velocity vector is determined in each sub cell from the piecewise constant pressure gradient vector from the pressure field the darcy velocity is resolved along the two outward sub interface normals of the sub cell the normal flux at the left hand side of sub interface i f 1 is given by velocity resolution along the normal vector n k 0 5 y v y v f 1 x v x v f 1 t r outward normal to the sub cell of the cell with centroid m 1 10 f i f 1 1 v n k t 11 1 p ξ t 12 1 p η a 1 where v k m p t t q is a discrete approximation of the general piola tensor in physical space and define the coefficients of ϕξ ϕη tr further details of the general tensor approximation are given in friis et al 2008 similarly fluxes are determined on both sides of all the sub interfaces as well fluxes are in terms of the matrix centroid pressures p m p m 1 p m 2 p m 3 t r and the interface pressures p i p a p f 1 p f 1 p f 2 p f 2 p f 3 p f 3 t r we impose continuity of the fluxes on the sub interfaces that are not fracture cells and the continuous interface pressures e g pa in this case are eliminated the fluxes on all the sub interfaces that are fracture cells are discontinuous because these are the flux transfer to a fracture cell from the matrix cells on both the sides of the fracture cell the discontinuous pressures on the sub interfaces that are fracture cells are eliminated by using two transmission conditions robin type conditions martin et al 2005 ahmed et al 2015a on each of the sub interfaces that are fracture cells for the sub interface i f 1 as shown in fig 2 the two transmission conditions are written as 11 ζ f i f 1 1 2 k f n a f p f 1 1 ζ f i f 1 2 2 k f n a f p f 1 12 ζ f i f 1 2 2 k f n a f p f 1 1 ζ f i f 1 1 2 k f n a f p f 1 where p f 1 is the unknown pressure associated with the involved fracture cell f 1 and is specified at the centroid and f i f 1 1 and f i f 1 2 are the respective outward normal matrix fracture fluxes on the two sides of the lower dimensional fracture k f n and af are the normal permeability and the given aperture of the fracture ζ is a positive parameter such that ζ 1 2 1 martin et al 2005 ζ 3 4 corresponds to the second order pressure approximation across the fracture and ζ 1 corresponds to simple finite volume scheme of the fracture cell the effects of the values of the ζ parameter on the numerical results for the low permeable fractures have been discussed in our previous work ahmed et al 2015a 2015b and ζ 2 3 has been found to be an optimal choice discontinuity in pressure across the fracture is important whenever normal permeability k f n is lower than the matrix permeability and the fracture acts as a barrier for a high normal permeability a f 2 k f n 0 and the transmission conditions approach continuity of pressure across the fracture cell i e p f 1 p f 1 p f 1 with continuous pressure across fracture cell in the limit using the flux continuity conditions and the transmission conditions we obtain the matrix matrix and matrix fracture fluxes in terms of the matrix and fracture centroid pressures i e f a 7 4 p m b 7 3 p f where p f p f 1 p f 2 p f 3 t r 4 2 fracture flow model the lower dimensional discrete flow equation in the lower dimensional 1d fracture cells is written as 13 φ f m i t a f a f k 1 2 α x i α ξ α k r α μ α f α k α x i α ξ α k r α μ α f α 1 α x i α ξ α k r α μ α f α 2 q i a f a f i 1 n c where f α is the total fracture fracture flux outward to lower dimensional fracture cell corresponding to phase α the fluxes are f w f w ρ w g f z and f g f w ρ g g f z f c where f w k f t p f w n k f z k f t z f n k and f c k f t p f c n k denote the outward normal fracture fracture fluxes on sub interface k end point of a 1d fracture cell corresponding to the water phase pressure p f w the vertical height zf and the capillary pressure p f c respectively of the lower dimensional fracture cells t denotes the longitudinal tangential gradient operator considering a cluster of fracture cells with the common vertex v shown in fig 3 the outgoing fracture flux f k f t p f n k of the 1d fracture cell with centroid mid point f 1 is simply determined as f 1 k f t p f 1 p v a f l f ahmed et al 2015a where p f 1 denotes the pressure at the centroid of the fracture cell pv denotes the pressure at the common vertex of the fracture cells lf is the half of the length of the fracture cell k f t denotes the longitudinal tangential fracture permeability and af denotes the given aperture of the fracture similarly outgoing fracture fluxes are determined for all the fracture cells in a cluster the pressure at the common vertex is eliminated from flux expressions by imposing the condition of mass conservation at the vertex for the cluster of fracture cells shown in fig 3 the system of fracture fracture fluxes is then expressed as f a f 3 3 p f where p f p f 1 p f 2 p f 3 t r the quantity x i α ξ α k r α μ α at the fracture fracture intersection point is computed by assuming no accumulation of mass at the intersection point following the procedure discussed in hoteit and firoozabadi 2008b sandve et al 2012 and ahmed et al 2015a if there are p intersecting fractures meeting at a point v and there are l fluxes going into the intersection point out of the fracture cells then we can compute x i α ξ α k r α μ α at the intersection point v by the following condition k 1 l f α x i α ξ α k r α μ α k x i α ξ α k r α μ α v k 1 p l f α k the phase mass densities ρα at a intersection point are computed by averaging the phase densities related to the fractures cells connected to the intersection point the approximation of matrix fracture fluxes f α outward to matrix cells on two sides of a fracture cell has been discussed above the quantity x i α ξ α k r α μ α for the matrix fracture interaction are computed according to the upstream direction of corresponding matrix fracture flux f α 4 3 numerical solution we solve the nc flow eqs 9 and 13 and one volume balance eq 4 over all the matrix and fracture cells by the fully implicit time discretisation scheme for the primary unknown variables that are pw m i i 1 n c the non linear system of equations is solved by a non linear solver newton raphson method at each time step we update the secondary unknowns sα and xiα and the phase molar densities ξα by the thermodynamic models section 3 using the updated values of the primary variables at each iteration of the non linear solver 5 a numerical comparison with the tough2 numerical simulator tough2 is a numerical simulator for multi component and multiphase flow in porous media mainly used for nuclear waste disposal and hydrology problems pruess 1991 we compare a numerical simulation of co2 injection into a water saturated reservoir obtained by using the tough2 numerical simulator with eco2n module pan et al 2014 and our newly implemented framework of dmflow a water saturated rectangular reservoir with the initial pressure of 8 e6 pa 80 bar and the temperature of 25 c is shown in fig 4 co2 is injected at the bottom left corner cell at the rate of 20 kg sec and the cell at the diagonally opposite corner is imposed with the pressure of 8 e6 pa the relative permeabilities are the cubic functions of the phase saturations with zero residual and irreducible saturations the capillary pressure is defined by the van genuchten function p c s w 1 λ 1 1 λ pa with λ 0 457 the pressure solution contours and the gas phase saturation contours at 900 days obtained via the tough2 and the dmflow are shown in fig 5 the variations of pressure and gas phase saturation at the injector cell with respect to time are shown in fig 6 the pressure results of dmflow are in the units of bar there is a general agreement between the results of the pressure and the phase saturation obtained by the tough2 and the dmflow note that there is a slight difference in the shown results and the compositions of each phase because of the difference in phase property models and the mutual solubility model compared to our model discussed in section 3 tough2 models the mutual solubility following the spycher and pruess 2010 the details of phase properties models used by tough2 are given in pan et al 2014 and battistelli et al 1997 6 numerical results with fractures now we present numerical simulation results in this section to demonstrate applicability of the presented model for co2 injection into fractured water saturated reservoirs first we present a comparison of results obtained by equi dimensional 2d fracture model and the presented lower dimensional 1d fracture model for a reservoir with intersecting fractures then we present a simulation to show the gravity driven gas flow in a fractured reservoir with a heterogeneous and anisotropic full tensor permeability field we also present a 3d simulation of multi component co2 ch4 gas injection into a fractured saline reservoir for all the simulations we use ζ 2 3 for the transmission conditions 11 and 12 and q 1 for triangular pressure supports tps of the cvd mpfa method we use the quadratic saturation functions for the relative permeabilities of the water and gas phases in the whole reservoirs the capillary pressure is computed by p c φ k ln s w 6 1 comparison of the equi dimensional and the lower dimensional fracture models we simulate a test case of co2 injection into a fractured reservoir of size 120 m 120 m and saturated with water initially we compare the solutions computed by the equi dimensional 2d fracture model and the presented lower dimensional 1d fracture model the unstructured triangular meshes used by both the models are shown in fig 7 the fractures are modelled by the rectangular cells for the 2d fracture model while 1d fractures are modelled by lines edges between the triangular matrix cells there are small square cells at the intersection of the 2d fractures we inject co2 at the bottom left corner cell at the rate of 20 m 3 day for 1100 days and the pressure of 100 bar is imposed on the right top corner cell the temperature of the flow is 315 k the properties of the matrix and the fractures are given in table 1 first we discuss the results for the case when fractures have the permeability of k f 10 3 i md the pressure fields and the gas saturation obtained via the 2d fracture model and the 1d fracture model at 200 days and 700 days are shown in figs 8 and 9 respectively the results produced by the 1d fracture model are in agreement with those produced by the 2d fracture model we also present the co2 composition over a line that passes through the two of the intersections of fractures as shown in fig 7 at 200 days and 700 days in fig 10 there are depressions in the plots at the fracture intersections which are well produced by the 1d fracture model similarly the gas saturation over a diagonal line is shown in fig 11 we have a dynamic time stepping employed in our simulator the time step is doubled if the number of iterations of the newton s method is equal to or less than a specified number the time step is scaled by half if the convergence is not achieved within the allowed maximum number of iterations the average of the time steps used during the full simulation of the 2d fracture model is 3 1216 days and that of the 1d fracture model is 3 2802 days the average of the number of iterations of the non linear solver used during the simulation by 2d fracture model is 6 1790 and the average number of iterations of 5 8743 has been observed for the 1d fracture model that demonstrate the computational efficiency of the 1d fracture model we simulate the same test case but assigning the fractures a high permeability of k f 10 4 i md the pressure fields and the gas saturation obtained via the 2d fracture model and the 1d fracture model at 200 days and 700 days are shown in figs 12 and 13 respectively the co2 composition over a diagonal line at 200 days and 700 days is shown in fig 14 there is a very small discrepancy between the compositions obtained by the two models at and away from the intersection of small fractures at 700 days similarly the saturation over a line is shown in fig 15 the average of the time steps used by the 2d fracture model is 1 2921 days and the average time step of the 1d fracture model is 3 2638 days the average of the number of iterations used by the 2d fracture model is 5 8331 and the average number of iterations of 6 4926 has been observed for the 1d fracture model the lower requirement of the times step of the 1d fracture model for the higher permeable fractures demonstrates the computational efficiency of the 1d fracture model which avoids the explicit representation of the small intersection cell that is included in the 2d fracture model 6 2 gravity driven flow in a heterogeneous and anisotropic fractured reservoir we now present a gravity driven compositional two phase flow in a 2d vertical fractured reservoir the reservoir with 8 fractures has been discretised into a triangular mesh where triangular cells are aligned with the 1d fractures as shown in fig 16 the reservoir is initially saturated with water with a hydrostatic pressure as shown in fig 16 and the temperature of 315 k the properties of the matrix and the fractures are given in table 2 co2 is injected from a injector at the rate of 20 m 3 day for 500 days and the pressure of 100 bars is imposed for 730 days 2 years at top right corner cell constant pressure producer we simulate the case for 30 years to present the gravity effects on the injected co2 into the fractured reservoir as shown in figs 17 and 18 co2 flows rapidly into the fractures because of phase pressure and gravity at initial period of time during injection and production when the injection and the production is stopped the buoyancy effects are profound and co2 flows upwards under gravity co2 flows close to the top driven by the gravity because of the conductive fractures compared to the case without fractures the effects of anisotropic permeability field can also be noticed in fig 17 and fig 18 where the gas phase plume spreads diagonally under gravity the simulation shows that the fractures provide the preferential pathways for the injected co2 in an aquifer the fractures favour injected co2 to quickly migrate to the top that can lead to co2 leakage in the practical co2 storage projects in our experience of this test case the computational cost of the simulation with fractures is 3 1 times the computational cost of the simulation without fractures moreover considering the capillary pressure and its contrast between the matrix and the fractures significantly increases the computational cost of the simulation and influences the flow behaviour the computational cost of the simulation of fractured reservoir is 1 8 times the case when we ignore the capillary pressure in the whole fractured reservoir 6 3 3d multi component gas injection into a fractured saline aquifer a 3d simulation is presented in this section to demonstrate the applicability of the cvd mpfa coupled with the lower dimensional fracture model to a compositional compressible gas water phase flow in 3d with a 2d surface fracture network ahmed et al 2015b a 3d reservoir with a surface 2d fracture network is shown in fig 19 the injector location is also shown the surface fractures are discretised into a surface triangular mesh while the matrix is dicretised into an unstructured delaunay tetrahedral mesh that conforms to the triangular fracture cells as shown in fig 20 the delaunay tetrahedral mesh has been generated using tetgen si 2013 mesh generator the properties of the matrix and the fractures are given in table 3 the reservoir is initially saturated with brine at the temperature of 315 k with salinity of 0 5 molal and the hydrostatic pressure of 191 502 199 5 bar the right yz boundary is imposed with a fixed pressure equal to the initial hydrostatic pressure for the whole simulation the multi component gas with 85 co2 and 15 ch4 methane is injected from a injector at the rate of 3698 6301 m 3 day for 2000 days the salinity m nacl is updated explicitly by solving the tracer convection equation by the finite volume method and the first order upwind scheme using the known values of phase fluxes calculated by the known phase pressure field at each iteration of the non linear solver of the primary equations the tracer equation for salinity m nacl is as follows 14 φ m nacl t v w m nacl q nacl gas saturation fields at 600 days 1200 days and 2000 days obtained by the presented lower dimensional fracture model are shown in fig 21 the fractures provide the conductive pathways to the gas that flows from the injector towards the boundary with a specified low pressure the mole fraction fields of ch4 in gas phase are shown in fig 22 it is evident from the results that there is a high ch4 composition at the front methane bank of gas phase plume in the reservoir which is consistent with the previous work li and li 2015 the presented simulation demonstrates the capability of our presented model and the simulator dmflow to the multi component gas flow in a saline fractured aquifer 7 conclusions we have presented a numerical framework based on the cell centred finite volume cvd mpfa coupled with a lower dimensional fracture model to simulate the compositional compressible gas water phase fluid flow in fractured porous media we use the fugacity activity type model for the solubility of gas components in the water phase the h2o component is also allowed to dissolve into the gas phase we have validated the general behaviour of gas water flow of our method by a comparison with tough2 numerical simulator we have presented a comparison of the equi dimensional fracture model and the lower dimensional fracture model for the co2 gas injection into a water saturated reservoir with intersecting fractures using unstructured grids the results obtained via the two models are in agreement with each other while the lower dimensional fracture model is cost effective compared to the equi dimensional fracture model that includes the small intersection cells we also present a gas injection simulation into a water saturated reservoir with an anisotropic and heterogeneous full tensor permeability field where co2 gas flow is driven by gravity and a highly permeable fracture network that leads to co2 migration to the top of the reservoir the simulation demonstrates the effects of gravity and the highly permeable fractures on the injected co2 in an aquifer where co2 can leak because of the conductive fractures moreover the presented method is then applied to a realistic 3d simulation of a multi component co2 ch4 gas injection into a saline water saturated reservoir where fractures are modelled by the surface triangular cells between the tetrahedral matrix cells the 3d simulation also shows the robustness of the method to produce a methane bank in the multi component gas phase plume in a fractured reservoir acknowledgments we thank the anonymous reviewers for their helpful comments for the improvement of this manuscript supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2019 06 008 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
619,a novel numerical framework is presented for simulation of compositional compressible gas water phase flow in fractured porous media based on a fully implicit cell centred finite volume method on unstructured grids we employ a discrete fracture model where fractures are modelled lower dimensionally within a rock matrix a mass balance equation is solved over the rock matrix while a lower dimensional flow equation is solved over the lower dimensional fractures with coupling terms to account for the flux transfer from the surrounding rock matrix the discretisation of the darcy fluxes is based on the control volume distributed multi point flux approximation cvd mpfa coupled with a lower dimensional fracture model we solve a non linear system for the primary variables that are the phase pressure and the component molar densities while the secondary variables are updated depending on the updated values of the primary variables at each iteration of the non linear solver we use the cubic type equation of state eos to model the physical properties of gas components e g co2 the water phase is multi component as we allow solubility of gas components in the water phase solubility of h2o in the gaseous phase is also included in the developed method we compare compositional simulation results using unstructured grids obtained via the lower dimensional fracture model and the results obtained by the equi dimensional fracture model for a case of co2 injection into a water saturated reservoir with intersecting fractures the results show good agreement between the two models while the lower dimensional fracture model is also computationally cost effective results are also presented for the evolution of injected co2 under gravity through a water saturated reservoir with and without fractures that demonstrate the profound effects of discrete fractures that can lead to co2 leakage the presented method is also applied to a 3d simulation of injection of a multi component gas with co2 and ch4 into a saline water saturated reservoir with a surface 2d fracture network keywords finite volume cvd mpfa compositional two phase unstructured grid co2 1 introduction the water h2o and the carbon dioxide co2 molecules in the air are responsible of absorbing thermal energy to cause greenhouse effect other greenhouse gases are methane ch4 nitrous oxide n2o and synthetic halocarbons cfcs but co2 is the most abundant in the atmosphere rising level of co2 in the atmosphere is one of the main contributions to the increase in global average temperature the geological storage of co2 is regarded as one of the most effective techniques to contain the level of co2 in the atmosphere metz et al 2005 the depleted oil and gas reservoirs deep saline aquifers water saturated rocks and coal beds are the potential sites for co2 storage the saline aquifers are the most effective storage sites because of their large storage capacity and broad distribution around the world zhao et al 2010 fluid migration during the storage of co2 involves complexities because of geological heterogeneities fractures networks multi component multi phase flow co2 dissolution pore space trapping mineralization metz et al 2005 ren et al 2017 efficient numerical modelling and simulation is a tool that is vital to understanding managing and predicting such complex processes of greenhouse gases injection into potential geological sites the phase behaviour of co2 brine and other gas components is essential to understanding of the fluid migration in subsurface conditions the pioneer work started from 1980s on the thermodynamic modelling of co2 carbon monoxide co and hydrocarbons li and nghiem 1986 harvey and prausnitz 1989 zuo and guo 1991 these models show a good behaviour at low salinities low temperatures and low pressures but these are not suitable for applications in subsurface conditions søreide and whitson 1992 proposed a cubic model based on the modification of peng robinson model peng and robinson 1976 to calculate the mutual solubilities of light hydrocarbons i e co2 nitrogen n2 hydrogen sulphide h2s and h2o with salt nacl dissolved the model can reproduce mutual solubility measurements when molality moles in 1 kg of h2o molal for short hereafter of nacl is lower than 2 molal at various temperatures and pressures li et al 2015a made modifications based on the søreide whitson model and the model can calculate mutual solubilities of co2 ch4 h2s brine system to high temperatures pressures and salinities all the mentioned models use the same form of the equation of state eos for both the gaseous phase and the water phase these are called fugacity fugacity models springer et al 2012 li et al 2014 another type of a model is the fugacity activity model duan and others worked on modelling of a series of gas solubility in water and brine based on comprehensive review of solubility measurements of co2 ch4 n2 h2s and oxygen o2 duan and sun 2003 duan and mao 2006 li and duan 2011 geng and duan 2010 mao et al 2013 they used the virial type eos duan et al 1992 to calculate gas fugacities and the pitzer model pitzer 1973 to calculate activities of aqueous species their models can accurately calculate single component gas solubility but are hard to extend to multi gas components li et al 2015b established the mutual solubility model of light hydrocarbon and brine systems the model uses the peng robinson model peng and robinson 1976 for the gas fugacities and the pitzer model for the water activities li et al 2014 compared the two types of model of gas water mutual solubilities in terms of model accuracy and computation efficiency and concluded that the fugacity activity model is superior to the fugacity fugacity model here we use and implement the fugacity activity type model of li et al 2015b for the simulation of co2 injection into fractured water saturated reservoirs where the co2 dissolution is also incorporated fractures are a system of rock discontinuities e g faults joints and fissures that occur in porous media with apertures having widths ranging over scales from microns to centimetres reiss 1980 flow in any rock is affected by a few large fractures by a dense network of small fractures or by a combination of fractures of varying length scales ranging from microns to hundreds of kilometres erhel et al 2009 berkowitz 2002 the well connected high permeable fracture networks provide high permeability to the reservoir that leads to requirement of less number of injection wells in recent years increasing effort is being devoted to the development of efficient and accurate numerical methods based on discrete fracture method to simulate the fluid flow through fractured porous media in the discrete fracture method dfm see e g noorishad and mehran 1982 baca et al 1984 lough et al 1998 lee et al 2001 karimi fard and firoozabadi 2003 karimi fard et al 2004 martin et al 2005 reichenberger et al 2006 matthäi et al 2007 hoteit and firoozabadi 2008a paluszny et al 2007 hajibeygi et al 2011 girault et al 2015 actual orientation and location of the fractures are honoured in the domain unlike the conventional model called the dual porosity method barenblatt et al 1960 warren and root 1963 kazemi et al 1976 gilman 2003 the effect of individual fractures on fluid flow can be determined and the fluid transfer between the fracture and the matrix is more straightforward and consistent generally the fractures are modelled by n 1 dimensional elements in an n dimensional domain for example in 2d fractures are represented by lines which coincide with the edges of polygonal matrix elements berre et al 2018 an unstructured grid is used to honour the explicit fracture geometry where grid cells can be boundary aligned with fractures see mallison et al 2010 mustapha et al 2011 equi dimensional representation of fractures hægland et al 2009 are not popular because of complexity and computational cost contributed by thin cells different techniques of flow approximation in a discrete fracture matrix system have been presented finite element and extended finite element xfem methods for embedded fractures into non conforming meshes are presented in foster and nejad 2013 d angelo and scotti 2012 and berrone et al 2014 mixed hybridized finite element mhfe and discontinous galerkin dg methods hoteit and firoozabadi 2008a are presented for multi component compressible flow in moortgat and firoozabadi 2013 zidane and firoozabadi 2014 zidane and firoozabadi 2015 and moortgat et al 2016 other recent methods for discrete fracture model simulations include the mimetic finite difference method al hinai et al 2015 and the vertex approximate gradient vag method masson et al 2014 brenner et al 2014 a finite volume method based on the two point flux approximation tpfa has been presented for discrete fracture matrix simulations in karimi fard et al 2004 where fractures are n 1 dimensional in the physical mesh but are expanded to n dimensional in the computational domain the method has been extended to control volume distributed multi point flux approximation cvd mpfa o method default quadrature q 1 in sandve et al 2012 for the single phase flow a recently developed cell centred finite volume cvd mpfa method coupled with a lower dimensional fracture model has been presented in ahmed et al 2015a b for single phase flow simulations the method models fractures by n 1 dimensional entities within n dimensional rock both in the physical mesh as well as in the computational domain very recently the method has been applied to two phase flow problems in gläser et al 2017 and xie and edwards 2019 without any compositional behaviour in this work we extend the cvd mpfa method coupled with a lower dimensional fracture model to isothermal compositional two phase water and gas phase flow through fractured porous media with a focus on co2 injection into saline aquifers the method has been implemented into our newly developed reservoir simulator named as dmflow that also includes the advance fugacity activity type model of mutual solubilities as discussed above we validate our numerical framework by a comparison with tough2 numerical simulator for the compositional two phase flow problem of co2 injection into a water saturated reservoir without fractures moreover we present a comparison of the compositional results obtained by the equi dimensional fracture model and the lower dimensional fracture model for the intersecting fractures using unstructured grid the results show that the lower dimensional fracture model produces results comparable to the equi dimensional fracture model for the compositional two phase flow problems moreover the numerical simulations presented for complex fracture networks within the water saturated reservoirs show the robustness of the presented numerical framework the paper is outlined as follows we describe the flow equations of isothermal compositional two phase water and gas phase fluid flow through porous media in section 2 the thermodynamic model of the compositional two phase water and gas phase fluid is briefly discussed in section 3 the discretisation of the flow equations for the fractured porous media is described in section 4 we present a comparison of a numerical simulation with the tough2 numerical simulator in section 5 we then present numerical simulation results based on the presented method for fractured media in section 6 before concluding the work in section 7 2 flow equations 2 1 mass conservation and the darcy s law in this work we focus on discretisation of the equations of the mass conservation of all the components in a compressible two phase water and gas phase flow of an nc component mixture which is given as follows 1 φ m i t x i w ξ w v w x i g ξ g v g q i i 1 n c where mi is the overall moles of component i per pore volume kmol m 3 and by mass conservation m i m i w m i g total moles of all components per unit pore volume is written as m t i 1 n c m i the porosity is denoted by φ qi denotes the injecting molar flow rates of the ith component and ξw and ξg denote the molar densities kmol m 3 of the water and gas phases respectively xiw and xig are the mole fractions of component i in the water and gas phases respectively which are defined by x i α m i α j 1 n c m j α α w g the mole fractions of the components in each phase α are constraint by i 1 n c x i α 1 in eq 1 v w and v g are the velocities m day of the water and gas phases these velocities are given by darcy s law 2 v α k α μ α p α ρ α g z α w g where the pressure of phase α is denoted by pα bar and z denotes the vertical height m which is positive in downward direction in the above equation note that we assume that molecular diffusion and hydraulic dispersion are negligible the magnitude of the gravitational acceleration is denoted by g here k α k r α k where k is the absolute permeability md tensor of the porous medium which is known from geological data in general k is a symmetric tensor with possibly non zero off diagonal terms k k 11 k 12 k 12 k 22 in 2d and k 11 k 12 k 13 k 12 k 22 k 23 k 13 k 23 k 33 in 3d krα is the relative permeability of phase α in the porous medium and is assumed to be known in terms of the phase saturation the phase viscosity μα cp molar density ξα and mass density ρα kg m 3 are the functions of the respective phase pressures and compositions α w g the phase pressures pα are related by the capillary pressure p c p g p w the capillary pressure is a known function of phase saturation only which is the standard model the more advanced models of capillary pressure that involve inter facial areas along with the saturation e g see niessner and hassanizadeh 2008 will be included in future work using eq 2 to replace phase velocities and the capillary pressure definition p c p g p w to replace pg the conservation eq 1 is written as 3 φ m i t α x i α ξ α k α μ α p w x i α ξ α k α μ α ρ α g z x i g ξ g k g μ g p c q i i 1 n c 2 2 volume balance constraint an additional equation is needed to completely define a compositional multiphase problem this equation is usually referred to as a volume balance constraint chien et al 1985 schlumberger 2012 it is the constraint that fluid volume vf should be equal to the pore volume vp it can be written as a saturation constraint equation 4 s w s g 1 0 where the phase saturation sα is defined as s α m α ξ α the overall moles of a phase per unit pore volume mα is given by m α l α m t where lα is the phase mole fraction having a constraint of α l α 1 0 3 thermodynamic model when we solve the mass balance eq 3 mole fractions xiw and xig of each component in each phase are the secondary variables these are also input parameters for density and viscosity calculations mole fractions are calculated by the phase partitioning models we assume that the water and gas phases are always in a phase equilibrium state the distribution of each hydrocarbon into the two phases is subject to the condition of stable thermodynamic equilibrium which is given by minimizing the gibbs free energy of the compositional system chen et al 2006 schlumberger 2012 5 ψ i w p w x 1 w x 2 w x n c w ψ i g p g x 1 g x 2 g x n c g where ψiw and ψig are the chemical potential of water and gas phases respectively the chemical potential of solutes in each phase can be expressed in terms of fugacity or activity ψ i w t p g x i g ψ i g 0 t r t ln f i t p g x i g ψ i g 0 t r t ln x i g p g r t ln ϕ i t p g x i g and ψ i w t p w m s o l ψ i w 0 t p w r t ln a i t p w m s o l ψ i w 0 t p w r t ln m h 2 o x i w r t ln γ i t p w m s o l where ψig denotes the chemical potential of component i in gas phase and ψiw denotes the chemical potential of component i in water phase t is the temperature in kelvins k r is the gas constant which is 8 31446 j k mol ϕi is the fugacity coefficient of component i in gas phase m s o l is the molality of salt in water phase m h 2 o is the mole number of 1 kg h2o which is equal to 55 508 moles ψ i g 0 is the standard chemical potential of component i in gas phase which is the ideal gas chemical potential at the pressure of 1 bar and ψ i w 0 is the standard chemical potential of component i in ideal water solution with a hypothetical unit molality duan and sun 2003 li and duan 2011 the equilibrium constant of component i can be defined as li and duan 2011 6 ln k i ψ i w 0 ψ i g 0 r t ln f i a i since ψ i w ψ i g under equilibrium condition we have x i g ϕ i p m h 2 o k i x i w γ i where ki is a function of temperature and pressure and it can be regressed from the experimental data of gas solubility in pure water ϕi is calculated from peng robinson model peng and robinson 1976 γi can be calculated from pitzer model pitzer 1973 phase mole fractions lw and lg are calculated by solving rachford rice equation michelsen and mollerup 2007 p 259 unknowns xiw and xig are solved for iteratively for details we refer to li et al 2015b gas phase density and viscosity are calculated by the cubic type equation of state peng and robinson 1976 for water phase density we consider the influences of dissolved salt and gas components we denote salt component in brine by nacl and number of gas components by n c g in aqueous solution with 1 kg h2o we calculate the water phase density following duan and mao 2006 as follows 7 ρ w 1000 m nacl m nacl i 1 n c g m g i m g i 1000 m nacl m nacl i 1 n c g m g i m g i ρ h 2 o nacl i 1 n c g m g i v g i where m nacl is the molality of nacl salinity dissolved in a solution m nacl is the molar weight of nacl ρ h 2 o nacl is the density of solution with m nacl salt dissolved and free of any gas component m g i is the molality of gas component i m g i is the molar weight of gas component i and v g i denotes the partial molar volume of gas component i ρ h 2 o nacl is calculated following the method presented in spivey et al 2004 for the calculation of water phase viscosity we follow the method of spivey et al 2004 where the gas effect is not considered 4 numerical model we use the finite volume cvd mpfa method to discretise the flow eq 3 in space and implicit euler scheme for time stepping the flow equation is integrated over each grid cell control volume using the gauss divergence theorem to obtain 8 ω l φ m i t d v ω l ω l α x i α ξ α k α μ α p w x i α ξ α k α μ α ρ α g z x i g ξ g k g μ g p c n d a ω l q i d v ω l i 1 n c 4 1 matrix flow model the discrete approximation of the eq 8 for the matrix cells is written in terms of the out ward fluxes on all the sub interfaces half edges in 2d of the grid cells 9 φ m i t v ω l k n s ω l α x i α ξ α k r α μ α f α k q i v ω l i 1 n c where n s ω l denotes the total number of sub interfaces of the grid cell ω l f α is the matrix matrix and matrix fracture total flux outward normal to the matrix cell corresponding to the phase α the total fluxes are written as f w f w ρ w g f z and f g f w ρ g g f z f c where f w k p w n k f z k z n k and f c k p c n k denote the outward normal fluxes on sub interface k corresponding to the water phase pressure the vertical height and the capillary pressure respectively in this work the capillary pressure is always continuous across the matrix matrix and matrix fracture interfaces as the focus is on the compositional behaviour with the co2 dissolution in the water phase in fractured reservoirs we will include the discontinuous capillarity ahmed et al 2019 in the future work applying the first order upwind scheme the terms x i α ξ α k r α μ α α w g at the interfaces are computed according to the upstream direction of the corresponding total fluxes f α α w g similar to the approach as discussed in zhou et al 2011 without fractures the phase mass densities ρα used in the above equation at a sub interface are computed by averaging the phase densities related to the two cells sharing the sub interface 4 1 1 cvd mpfa flux approximation the fluxes are approximated by the cvd mpfa method edwards and rogers 1998 friis et al 2008 coupled with lower dimensional fractures ahmed et al 2015a 2015b here we use the triangular pressure support tps scheme of the cvd mpfa method the full pressure support fps scheme of the cvd mpfa method has been presented in ahmed et al 2017 for discrete fracture matrix flow simulations we outline the approximation of the matrix matrix and matrix fracture flux f k p n k corresponding to a generic pressure p that is applicable to fluxes fw fz fc corresponding to the water phase pressure the vertical height and the capillary pressure we work on a cluster of cells common to a given vertex and approximate fluxes on all the sub interfaces cell half edges in 2d consider a cluster of cells attached to the vertex v as shown in fig 1 the lower dimensional fracture cells are modelled by the interfaces edges between the matrix cells pressures on all the centroids of the matrix and the fracture cells are also shown in fig 1 following the tps scheme we introduce auxiliary interface pressures on all the sub interfaces as shown in the fig 1 to build the pressure variations in all the sub cells the auxiliary interface pressures are continuous across the sub interfaces that are not fracture cells following the discontinuous pressure fracture model ahmed et al 2015a we introduce two more auxiliary pressures on both sides of the sub interfaces that are fractures e g p f 1 and p f 1 on the right and the left sides of the interface i f 1 respectively that is the fracture cell with centroidal pressure p f1 the points of flux continuities and the auxiliary interface pressures are defined by the parametric variation of quadrature points 0 q 1 along the sub interface for the symmetric positive definite spd scheme q 2 3 friis et al 2008 the points of auxiliary interface pressures on the sub interfaces that are fracture cells are always at the centroids that correspond to q 1 in each of the matrix sub cells of a cluster pressure has a piecewise linear variation e g for the sub cell of the cell with the centroid m 1 fig 1 pressure variation is written in terms of barycentric coordinates ξ η where p 1 ξ η p m 1 ξ p f 1 η p a a darcy velocity vector is determined in each sub cell from the piecewise constant pressure gradient vector from the pressure field the darcy velocity is resolved along the two outward sub interface normals of the sub cell the normal flux at the left hand side of sub interface i f 1 is given by velocity resolution along the normal vector n k 0 5 y v y v f 1 x v x v f 1 t r outward normal to the sub cell of the cell with centroid m 1 10 f i f 1 1 v n k t 11 1 p ξ t 12 1 p η a 1 where v k m p t t q is a discrete approximation of the general piola tensor in physical space and define the coefficients of ϕξ ϕη tr further details of the general tensor approximation are given in friis et al 2008 similarly fluxes are determined on both sides of all the sub interfaces as well fluxes are in terms of the matrix centroid pressures p m p m 1 p m 2 p m 3 t r and the interface pressures p i p a p f 1 p f 1 p f 2 p f 2 p f 3 p f 3 t r we impose continuity of the fluxes on the sub interfaces that are not fracture cells and the continuous interface pressures e g pa in this case are eliminated the fluxes on all the sub interfaces that are fracture cells are discontinuous because these are the flux transfer to a fracture cell from the matrix cells on both the sides of the fracture cell the discontinuous pressures on the sub interfaces that are fracture cells are eliminated by using two transmission conditions robin type conditions martin et al 2005 ahmed et al 2015a on each of the sub interfaces that are fracture cells for the sub interface i f 1 as shown in fig 2 the two transmission conditions are written as 11 ζ f i f 1 1 2 k f n a f p f 1 1 ζ f i f 1 2 2 k f n a f p f 1 12 ζ f i f 1 2 2 k f n a f p f 1 1 ζ f i f 1 1 2 k f n a f p f 1 where p f 1 is the unknown pressure associated with the involved fracture cell f 1 and is specified at the centroid and f i f 1 1 and f i f 1 2 are the respective outward normal matrix fracture fluxes on the two sides of the lower dimensional fracture k f n and af are the normal permeability and the given aperture of the fracture ζ is a positive parameter such that ζ 1 2 1 martin et al 2005 ζ 3 4 corresponds to the second order pressure approximation across the fracture and ζ 1 corresponds to simple finite volume scheme of the fracture cell the effects of the values of the ζ parameter on the numerical results for the low permeable fractures have been discussed in our previous work ahmed et al 2015a 2015b and ζ 2 3 has been found to be an optimal choice discontinuity in pressure across the fracture is important whenever normal permeability k f n is lower than the matrix permeability and the fracture acts as a barrier for a high normal permeability a f 2 k f n 0 and the transmission conditions approach continuity of pressure across the fracture cell i e p f 1 p f 1 p f 1 with continuous pressure across fracture cell in the limit using the flux continuity conditions and the transmission conditions we obtain the matrix matrix and matrix fracture fluxes in terms of the matrix and fracture centroid pressures i e f a 7 4 p m b 7 3 p f where p f p f 1 p f 2 p f 3 t r 4 2 fracture flow model the lower dimensional discrete flow equation in the lower dimensional 1d fracture cells is written as 13 φ f m i t a f a f k 1 2 α x i α ξ α k r α μ α f α k α x i α ξ α k r α μ α f α 1 α x i α ξ α k r α μ α f α 2 q i a f a f i 1 n c where f α is the total fracture fracture flux outward to lower dimensional fracture cell corresponding to phase α the fluxes are f w f w ρ w g f z and f g f w ρ g g f z f c where f w k f t p f w n k f z k f t z f n k and f c k f t p f c n k denote the outward normal fracture fracture fluxes on sub interface k end point of a 1d fracture cell corresponding to the water phase pressure p f w the vertical height zf and the capillary pressure p f c respectively of the lower dimensional fracture cells t denotes the longitudinal tangential gradient operator considering a cluster of fracture cells with the common vertex v shown in fig 3 the outgoing fracture flux f k f t p f n k of the 1d fracture cell with centroid mid point f 1 is simply determined as f 1 k f t p f 1 p v a f l f ahmed et al 2015a where p f 1 denotes the pressure at the centroid of the fracture cell pv denotes the pressure at the common vertex of the fracture cells lf is the half of the length of the fracture cell k f t denotes the longitudinal tangential fracture permeability and af denotes the given aperture of the fracture similarly outgoing fracture fluxes are determined for all the fracture cells in a cluster the pressure at the common vertex is eliminated from flux expressions by imposing the condition of mass conservation at the vertex for the cluster of fracture cells shown in fig 3 the system of fracture fracture fluxes is then expressed as f a f 3 3 p f where p f p f 1 p f 2 p f 3 t r the quantity x i α ξ α k r α μ α at the fracture fracture intersection point is computed by assuming no accumulation of mass at the intersection point following the procedure discussed in hoteit and firoozabadi 2008b sandve et al 2012 and ahmed et al 2015a if there are p intersecting fractures meeting at a point v and there are l fluxes going into the intersection point out of the fracture cells then we can compute x i α ξ α k r α μ α at the intersection point v by the following condition k 1 l f α x i α ξ α k r α μ α k x i α ξ α k r α μ α v k 1 p l f α k the phase mass densities ρα at a intersection point are computed by averaging the phase densities related to the fractures cells connected to the intersection point the approximation of matrix fracture fluxes f α outward to matrix cells on two sides of a fracture cell has been discussed above the quantity x i α ξ α k r α μ α for the matrix fracture interaction are computed according to the upstream direction of corresponding matrix fracture flux f α 4 3 numerical solution we solve the nc flow eqs 9 and 13 and one volume balance eq 4 over all the matrix and fracture cells by the fully implicit time discretisation scheme for the primary unknown variables that are pw m i i 1 n c the non linear system of equations is solved by a non linear solver newton raphson method at each time step we update the secondary unknowns sα and xiα and the phase molar densities ξα by the thermodynamic models section 3 using the updated values of the primary variables at each iteration of the non linear solver 5 a numerical comparison with the tough2 numerical simulator tough2 is a numerical simulator for multi component and multiphase flow in porous media mainly used for nuclear waste disposal and hydrology problems pruess 1991 we compare a numerical simulation of co2 injection into a water saturated reservoir obtained by using the tough2 numerical simulator with eco2n module pan et al 2014 and our newly implemented framework of dmflow a water saturated rectangular reservoir with the initial pressure of 8 e6 pa 80 bar and the temperature of 25 c is shown in fig 4 co2 is injected at the bottom left corner cell at the rate of 20 kg sec and the cell at the diagonally opposite corner is imposed with the pressure of 8 e6 pa the relative permeabilities are the cubic functions of the phase saturations with zero residual and irreducible saturations the capillary pressure is defined by the van genuchten function p c s w 1 λ 1 1 λ pa with λ 0 457 the pressure solution contours and the gas phase saturation contours at 900 days obtained via the tough2 and the dmflow are shown in fig 5 the variations of pressure and gas phase saturation at the injector cell with respect to time are shown in fig 6 the pressure results of dmflow are in the units of bar there is a general agreement between the results of the pressure and the phase saturation obtained by the tough2 and the dmflow note that there is a slight difference in the shown results and the compositions of each phase because of the difference in phase property models and the mutual solubility model compared to our model discussed in section 3 tough2 models the mutual solubility following the spycher and pruess 2010 the details of phase properties models used by tough2 are given in pan et al 2014 and battistelli et al 1997 6 numerical results with fractures now we present numerical simulation results in this section to demonstrate applicability of the presented model for co2 injection into fractured water saturated reservoirs first we present a comparison of results obtained by equi dimensional 2d fracture model and the presented lower dimensional 1d fracture model for a reservoir with intersecting fractures then we present a simulation to show the gravity driven gas flow in a fractured reservoir with a heterogeneous and anisotropic full tensor permeability field we also present a 3d simulation of multi component co2 ch4 gas injection into a fractured saline reservoir for all the simulations we use ζ 2 3 for the transmission conditions 11 and 12 and q 1 for triangular pressure supports tps of the cvd mpfa method we use the quadratic saturation functions for the relative permeabilities of the water and gas phases in the whole reservoirs the capillary pressure is computed by p c φ k ln s w 6 1 comparison of the equi dimensional and the lower dimensional fracture models we simulate a test case of co2 injection into a fractured reservoir of size 120 m 120 m and saturated with water initially we compare the solutions computed by the equi dimensional 2d fracture model and the presented lower dimensional 1d fracture model the unstructured triangular meshes used by both the models are shown in fig 7 the fractures are modelled by the rectangular cells for the 2d fracture model while 1d fractures are modelled by lines edges between the triangular matrix cells there are small square cells at the intersection of the 2d fractures we inject co2 at the bottom left corner cell at the rate of 20 m 3 day for 1100 days and the pressure of 100 bar is imposed on the right top corner cell the temperature of the flow is 315 k the properties of the matrix and the fractures are given in table 1 first we discuss the results for the case when fractures have the permeability of k f 10 3 i md the pressure fields and the gas saturation obtained via the 2d fracture model and the 1d fracture model at 200 days and 700 days are shown in figs 8 and 9 respectively the results produced by the 1d fracture model are in agreement with those produced by the 2d fracture model we also present the co2 composition over a line that passes through the two of the intersections of fractures as shown in fig 7 at 200 days and 700 days in fig 10 there are depressions in the plots at the fracture intersections which are well produced by the 1d fracture model similarly the gas saturation over a diagonal line is shown in fig 11 we have a dynamic time stepping employed in our simulator the time step is doubled if the number of iterations of the newton s method is equal to or less than a specified number the time step is scaled by half if the convergence is not achieved within the allowed maximum number of iterations the average of the time steps used during the full simulation of the 2d fracture model is 3 1216 days and that of the 1d fracture model is 3 2802 days the average of the number of iterations of the non linear solver used during the simulation by 2d fracture model is 6 1790 and the average number of iterations of 5 8743 has been observed for the 1d fracture model that demonstrate the computational efficiency of the 1d fracture model we simulate the same test case but assigning the fractures a high permeability of k f 10 4 i md the pressure fields and the gas saturation obtained via the 2d fracture model and the 1d fracture model at 200 days and 700 days are shown in figs 12 and 13 respectively the co2 composition over a diagonal line at 200 days and 700 days is shown in fig 14 there is a very small discrepancy between the compositions obtained by the two models at and away from the intersection of small fractures at 700 days similarly the saturation over a line is shown in fig 15 the average of the time steps used by the 2d fracture model is 1 2921 days and the average time step of the 1d fracture model is 3 2638 days the average of the number of iterations used by the 2d fracture model is 5 8331 and the average number of iterations of 6 4926 has been observed for the 1d fracture model the lower requirement of the times step of the 1d fracture model for the higher permeable fractures demonstrates the computational efficiency of the 1d fracture model which avoids the explicit representation of the small intersection cell that is included in the 2d fracture model 6 2 gravity driven flow in a heterogeneous and anisotropic fractured reservoir we now present a gravity driven compositional two phase flow in a 2d vertical fractured reservoir the reservoir with 8 fractures has been discretised into a triangular mesh where triangular cells are aligned with the 1d fractures as shown in fig 16 the reservoir is initially saturated with water with a hydrostatic pressure as shown in fig 16 and the temperature of 315 k the properties of the matrix and the fractures are given in table 2 co2 is injected from a injector at the rate of 20 m 3 day for 500 days and the pressure of 100 bars is imposed for 730 days 2 years at top right corner cell constant pressure producer we simulate the case for 30 years to present the gravity effects on the injected co2 into the fractured reservoir as shown in figs 17 and 18 co2 flows rapidly into the fractures because of phase pressure and gravity at initial period of time during injection and production when the injection and the production is stopped the buoyancy effects are profound and co2 flows upwards under gravity co2 flows close to the top driven by the gravity because of the conductive fractures compared to the case without fractures the effects of anisotropic permeability field can also be noticed in fig 17 and fig 18 where the gas phase plume spreads diagonally under gravity the simulation shows that the fractures provide the preferential pathways for the injected co2 in an aquifer the fractures favour injected co2 to quickly migrate to the top that can lead to co2 leakage in the practical co2 storage projects in our experience of this test case the computational cost of the simulation with fractures is 3 1 times the computational cost of the simulation without fractures moreover considering the capillary pressure and its contrast between the matrix and the fractures significantly increases the computational cost of the simulation and influences the flow behaviour the computational cost of the simulation of fractured reservoir is 1 8 times the case when we ignore the capillary pressure in the whole fractured reservoir 6 3 3d multi component gas injection into a fractured saline aquifer a 3d simulation is presented in this section to demonstrate the applicability of the cvd mpfa coupled with the lower dimensional fracture model to a compositional compressible gas water phase flow in 3d with a 2d surface fracture network ahmed et al 2015b a 3d reservoir with a surface 2d fracture network is shown in fig 19 the injector location is also shown the surface fractures are discretised into a surface triangular mesh while the matrix is dicretised into an unstructured delaunay tetrahedral mesh that conforms to the triangular fracture cells as shown in fig 20 the delaunay tetrahedral mesh has been generated using tetgen si 2013 mesh generator the properties of the matrix and the fractures are given in table 3 the reservoir is initially saturated with brine at the temperature of 315 k with salinity of 0 5 molal and the hydrostatic pressure of 191 502 199 5 bar the right yz boundary is imposed with a fixed pressure equal to the initial hydrostatic pressure for the whole simulation the multi component gas with 85 co2 and 15 ch4 methane is injected from a injector at the rate of 3698 6301 m 3 day for 2000 days the salinity m nacl is updated explicitly by solving the tracer convection equation by the finite volume method and the first order upwind scheme using the known values of phase fluxes calculated by the known phase pressure field at each iteration of the non linear solver of the primary equations the tracer equation for salinity m nacl is as follows 14 φ m nacl t v w m nacl q nacl gas saturation fields at 600 days 1200 days and 2000 days obtained by the presented lower dimensional fracture model are shown in fig 21 the fractures provide the conductive pathways to the gas that flows from the injector towards the boundary with a specified low pressure the mole fraction fields of ch4 in gas phase are shown in fig 22 it is evident from the results that there is a high ch4 composition at the front methane bank of gas phase plume in the reservoir which is consistent with the previous work li and li 2015 the presented simulation demonstrates the capability of our presented model and the simulator dmflow to the multi component gas flow in a saline fractured aquifer 7 conclusions we have presented a numerical framework based on the cell centred finite volume cvd mpfa coupled with a lower dimensional fracture model to simulate the compositional compressible gas water phase fluid flow in fractured porous media we use the fugacity activity type model for the solubility of gas components in the water phase the h2o component is also allowed to dissolve into the gas phase we have validated the general behaviour of gas water flow of our method by a comparison with tough2 numerical simulator we have presented a comparison of the equi dimensional fracture model and the lower dimensional fracture model for the co2 gas injection into a water saturated reservoir with intersecting fractures using unstructured grids the results obtained via the two models are in agreement with each other while the lower dimensional fracture model is cost effective compared to the equi dimensional fracture model that includes the small intersection cells we also present a gas injection simulation into a water saturated reservoir with an anisotropic and heterogeneous full tensor permeability field where co2 gas flow is driven by gravity and a highly permeable fracture network that leads to co2 migration to the top of the reservoir the simulation demonstrates the effects of gravity and the highly permeable fractures on the injected co2 in an aquifer where co2 can leak because of the conductive fractures moreover the presented method is then applied to a realistic 3d simulation of a multi component co2 ch4 gas injection into a saline water saturated reservoir where fractures are modelled by the surface triangular cells between the tetrahedral matrix cells the 3d simulation also shows the robustness of the method to produce a methane bank in the multi component gas phase plume in a fractured reservoir acknowledgments we thank the anonymous reviewers for their helpful comments for the improvement of this manuscript supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2019 06 008 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
