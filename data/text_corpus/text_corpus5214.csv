index,text
26070,the agricultural water use ag package was developed for simulating demand driven and supply constrained agricultural water use in modflow and gsflow models the ag package uses pre existing hydrologic simulation provided by modflow and gsflow three options are available for simulating water use for agriculture 1 user specified demands 2 demands determined by a user specified irrigation trigger value that is compared to the ratio of the simulated actual to potential evapotranspiration et and 3 demands determined by minimizing the difference between potential and actual et the latter two approaches use energy and soil water balance to determine crop water demands irrigation withdrawals are diverted into canals and routed to fields using the modflow sfr package or irrigation water is provided supplemented by groundwater combined with modflow or gsflow the ag package can simulate dynamic water use by agriculture in developed basins while providing flexibility to represent a range of irrigation practices keywords integrated hydrologic modeling agricultural water use gsflow modflow drought irrigation withdrawals water resources conjunctive use surface water and groundwater interactions software and or data availability section software and data used for this work model input files for each example problem and ancillary data are available through the github repository https github com rniswon mfnwt tree agoptions the agricultural ag water use package was developed by richard niswonger rniswon usgs gov and was released february 2020 and runs on windows unix and macintosh operating systems and requires no specific hardware or software to run gsflow and its components are written in fortran and the program files are less than 10 mbytes the ag package including source code compiled binary files for windows and unix example problems and jupyter notebooks for plotting results are provided through the mfnwt github repository 1 introduction agriculture is a major water consumer in many basins around the world and estimating irrigation withdrawals in hydrologic models is important for water resources planning and management wang et al 1996 jones et al 2017 water management decision support software is paramount in many river basins in the western united states and other parts of the world for adapting to climate change and population growth and for evaluating new water management strategies tian et al 2015 hydrologic models that incorporate surface water and groundwater can provide valuable information about water resources sustainability in conjunctive use systems this is especially true for agricultural regions susceptible to climate change and population growth that stress water supplies faunt 2009 elliott et al 2014 gorelick and zheng 2015 hydrologic software such as modflow simulates 3 dimensional groundwater flow and includes many add on capabilities such as representation of surface water features and other hydrologic processes harbaugh 2005 langevin et al 2017 gsflow is the integration of modflow and prms and can simulate all major hydrologic processes in watersheds including distributed energy and water consumption by plants markstrom et al 2008 2015 gsflow can simulate partitioning of precipitation into snowpack runoff evapotranspiration et and groundwater flow using energy and water balance approaches markstrom et al 2008 modflow and gsflow have been used for simulating regional scale agricultural systems hu et al 2010 morway et al 2013 bailey et al 2016 wu et al 2016 guzman et al 2015 woolfenden and nishikawa 2014 essaid and caldwell 2017 an add on to modflow called the farm process was developed to represent agricultural systems supplied by surface water and groundwater schmid et al 2006 hanson et al 2010 2014 a common approach for simulating agricultural systems in regional integrated models is to estimate irrigation demands as a pre processing step where a separate soil water balance model is used to calculate demands irrigation demands are subsequently specified to a regional integrated model that does not simulate field soil water balance dogrul et al 2011 the farm process for modflow 2005 assumes the irrigation demand is independent of a farm s soil water content and that precipitation can be subtracted from reference et to account for rain fed crop consumption hanson et al 2010 2014 another approach presented herein is to include simulation of soil water balance within the hydrologic simulator to better represent soil water in irrigated lands the advantage of this approach is that simulated soil water conditions can be used to estimate the rain fed component of crop consumption for the estimation of irrigation withdrawals e g allen et al 2007 huntington et al 2017 here an agricultural ag water use package is presented for modflow and gsflow for regional river basin scale simulations the ag package also can simulate conjunctive use of surface water and groundwater by automatically pumping groundwater when surface water availability is less than demand schmid et al 2006 because irrigation demand irrigation efficiency and crop consumption can be simulated using daily climatic conditions the model can be used to simulate impacts of climate change on water supply the ag package can represent changes in land use including changes in crop type expansion or contraction of farmlands or changes in irrigation technology through existing features in gsflow and recent enhancements regan and lafontaine 2017 changes in land use can be simulated using the dynamic parameters capability in gsflow to represent changes in vegetation cover type crop coefficients and other input parameters that vary with changes in land use regan and lafontaine 2017 climate variability can cause regional shifts in agricultural demand due to systematic changes in soil moisture and irrigated lands and indirectly as reductions in return flows fischer et al 2007 interactions such as these occur over time periods that span irrigation events or irrigation seasons or they can span much longer time periods due to multi year shifts in climate and groundwater supply evolving supply and demand conditions such as these support simulating demand using energy and soil water balance within integrated hydrologic models rather than estimating demands as a pre processing step or independent of soil moisture the ag package for modflow and gsflow provides a wholistic approach for representing dynamic irrigation withdrawals and can be used for planning and assessing impacts of agriculture on other water use sectors and for evaluating long term sustainability the ag package also provides necessary capabilities for integration of gsflow with river reservoir operations models such as modsim for simulating impacts of water use priorities on agricultural systems labadie 2010 morway et al 2016 niswonger et al 2017 kitlasten et al 2020 two example problems are presented for representing agriculture in modflow and gsflow and these examples are run using different options to demonstrate application of the new package and its capabilities for simulating agricultural water use for different hydrographic settings and irrigation practices example problem 1 demonstrates the new package in a modflow simulation and represents an agricultural basin in northwest nevada prudic et al 2004 the second example demonstrates the package in a gsflow simulation and represents an undeveloped basin in northeast california including hypothetical irrigated lands previously published work provides theory and application of modflow and gsflow and only new theoretical and implementation details for the ag package are provided herein readers can refer to these published works for simulation capabilities related to modflow and gsflow including energy and water balance calculations for hydrologic simulations that are used by the ag package harbaugh 2005 markstrom et al 2008 niswonger et al 2011 general descriptions of the components in an agricultural system are provided here to set the context for the theoretical explanation of these components this is followed by descriptions of the integration between the agricultural system and the regional hydrologic system details of the algorithms and model code developed for the ag package are provided as wells as explanation of the various options that can be used to simulate agricultural water use two different example models are described to illustrate the implementation of the ag package using both the modflow and gsflow hydrologic modeling frameworks results of these models and their discussion are provided to highlight appropriate use of different model options and implications of these options in the model results 2 methods 2 1 irrigation water delivery in practice irrigation is withdrawn from one location and it is routed through reservoirs streams canals pipes and furrows to its place of use fig 1 a place of use is an agricultural field where plant roots uptake water from shallow soils as water is delivered to fields part of it is lost along the way due to et leaky pipes and canals misdirected surface flows and seepage irrigation water also can increase during delivery if the irrigation system gains from other sources not all the water applied to fields is used by the crop and instead there are field losses due to surface runoff seepage below the plant roots and soil evaporation field losses depend on field conditions and the irrigation practices that vary with irrigation approach such as flood sprinkler and drip irrigation conveyance and other system gains and losses cause irrigation withdrawals to be different than crop consumption this difference is referred to as the system efficiency allen et al 1998 the ag package was developed to represent these processes explicitly using hydrologic simulation capabilities in modflow and gsflow or implicitly by specifying efficiency factors to represent all or a portion of the system gains and losses 2 1 1 surface water irrigation surface water delivery for irrigation is simulated by the modflow streamflow routing sfr package including open channel flow in streams and canals or non pressurized flow through pipes prudic et al 2004 niswonger and prudic 2005 surface water demands for diverting irrigation water and applying it to fields can be set by user specified values or they can be calculated by the model using field based crop water demands sfr routes steady or kinematic flow by coupling continuity and manning s equation and user defined relationships between flow area and depth to represent a variety of flow geometries sfr neglects diffusion and other acceleration terms in the shallow water and pipe flow equations however as times steps are typically 1 day or longer this simplification is generally applicable for regional agricultural systems diversion segments are used to deliver irrigation water and are initialized in the sfr input file diversion segments can be designated as irrigation diversions in the ag input file to apply diverted surface water to fields sfr diversion flow rates are constrained by the amount of water flowing in the upstream segment and 1 of 4 water use priority options prudic et al 2004 surface reservoirs are simulated by the modflow lake lak package merritt and konikow 2000 for modflow simulations and or open detention storage reservoirs for gsflow simulations regan and lafontaine 2017 sfr routes channel flows into and out of reservoirs represented by lak and open detention storage reservoirs diversion segments and reservoirs represented by sfr and lak are integrated with the groundwater flow equation to simulate surface water and groundwater interactions however open detention reservoirs do not interact with groundwater 2 1 2 groundwater irrigation groundwater irrigation is provided by wells that can pump water from a groundwater cell wells are defined and maximum pumping rates are specified within the ag package input file irrigation wells are assumed to have a screened interval that spans the model cell thickness and smoothing functions are used to reduce the pumping rate to zero as the water table drops below the cell bottom niswonger et al 2011 non irrigation wells such as public supply or thermoelectric wells are handled outside of the ag package using one of the other modflow well packages harbaugh 2005 ag wells must have negative pumping rates to represent flow out of an aquifer groundwater wells are designated in ag as irrigation wells to apply pumped groundwater to fields pumping rates for irrigation wells can be set by user specified values or they can be calculated by the model using groundwater irrigation demands pumping rates also can be calculated by the model to supplement surface water rights such that all or a portion of the shortage is pumped from groundwater however this version of the ag package cannot be used to represent the use of surface water to supplement a groundwater right 2 1 3 mapping point of diversion to place of use irrigation provided by diversion segments and groundwater wells is applied to designated cells or hydrologic response units hrus with a user specified mapping between numerically identified sfr segments ag wells and cells hrus modflow simulations require that ag features irrigation diversions or wells be associated with modflow cells because surface spatial units in modflow are cells however for gsflow simulations surface spatial units are hrus and ag package features must be associated with hrus the point of diversion is located at the upstream end of a diversion segment or well used for irrigation and the place of use is the area of fields irrigated by the diversion modflow cells are identified by their row and column hrus are identified by their hru id diversion segments are identified by their sfr segment number and wells are identified by their ag well number mapping identifiers are input to the ag input file and they can change during a simulation to represent changes in withdrawal locations or irrigated lands sfr diversion segments can consist of 1 or more reaches where reaches are the length of stream or canal that spans a single model cell a segment can span many model cells to represent great distances between a withdrawal point and irrigated field and diversion segments can divert from other diversion segments irrigation cannot be applied to a partial area of a cell in modflow which could be a limitation in models with cells that are larger than fields however irrigation can be applied to a fraction of an hru using the impervious fraction parameter and non irrigated areas within an hru are assumed to be impervious a diversion and or well can provide water for multiple cells hrus or multiple diversions and or wells can provide water for a single cell hru additionally a well can supplement several diversions or several wells can supplement one or more diversions if multiple sfr diversions supply irrigation to a single cell hru then the order that water is diverted occurs in the same order that the irrigation segments are specified however if multiple wells supply a single cell hru then the demand is split evenly among the wells 2 2 simulating crop consumption et can be simulated using soil water balance over any time step length for modflow simulations or et can be simulated using daily energy and water balance for gsflow simulations markstrom et al 2008 niswonger et al 2011 actual crop et e t a can be calculated by uzf as a function of the depth dependent soil water contents using a kinematic wave formulation niswonger et al 2006 or e t a can be calculated by prms as a function of volume averaged soil saturation using a nonlinear soil water reservoir approach markstrom et al 2008 2015 crop specific et demand is calculated by multiplying the crop coefficient k c by the reference et e t o allen et al 1998 a single crop coefficient approach is used by the ag package and k c represents crop specific information including growth patterns and soil evaporation seasonal k c values for common crops are available in the literature allen et al 1998 if using uzf to represent agricultural fields then the product k c e t o is input for the uzf variable pet if using prms to represent agricultural fields e t o is calculated using one of six options available in prms including jensen haise hargraeves semani penman monteith priestly taylor hamon and pan potential et modules markstrom et al 2015 example problem 2 below demonstrates how k c is incorporated into gsflow simulations using the prms jensen haise formulation other than including k c into the calculation of e t o all other prms input does not change due to the ag package sub irrigation is a process in which plants use shallow groundwater to meet crop water demands growers apply less irrigation water where there is shallow groundwater beneath their crops thus this process is important for estimating irrigation demand sub irrigation is simulated by uzf assuming a linear capillary rise as a function of groundwater head sub irrigation is simulated in gsflow by groundwater discharge to the prms soil zone due to linear capillary rise or saturated discharge conditions niswonger et al 2006 markstrom et al 2008 total crop consumption for a cell e t a is calculated in uzf by summing the unsaturated zone and groundwater e t a where groundwater e t a is a linear function of the water table elevation above the et extinction depth and is zero when the water table is below the et extinction depth uzf input variable extdp additional to the previously available approach for simulating e t a in uzf a new option was added to simulate crop consumption using a pressure gradient approach this approach is recommended for the ag package and etdemand option and a description is provided here because it was not included in the original uzf or gsflow documents for this case the capillary pressures are calculated in the crop root zone using the brooks corey retention function and 3 new uzf input variables including the root activity function air entry pressure and root pressure lappala et al 1987 e t a is calculated using 1 e t a d t k θ r t ψ θ h r o o t where d t is the thickness of the root zone or et extinction depth that can change during the growing season l k θ is unsaturated hydraulic conductivity as a function of water content lt 1 r t is the root activity function that can change during the growing season l 2 ψ θ is capillary pressure head as a function of water content l and h r o o t is the negative root pressure head l variables in equation 1 are calculated using brooks and corey 1966 unsaturated hydraulic conductivity and capillary pressure functions this option also is documented by langevin et al 2017 2 3 simulating irrigation return flows irrigation return flow is water that returns to a surface water body or seeps to groundwater rather than entering the atmosphere due to et it is considered return flow because the water becomes available to other growers or for other uses in the system return flow can occur anywhere between a withdrawal and a field including the area where irrigation is applied gains and losses in the irrigation conveyance system are represented by the integration of surface water and groundwater in sfr and lak for the channel and surface reservoir domains canal or pond and gains and losses are simulated by uzf or prms for the overland flow domain field return flow also occurs between the overland flow domain and the channel and reservoir domains groundwater return flow occurs as irrigation percolates beneath the uzf et extinction depth or through the base of the soil zone defined in prms there is no explicit representation of irrigation for salt leaching however specified amounts of irrigation can be applied to cells hrus to represent salt leaching effects of salt stress on e t a are neglected exchanges between surface water and groundwater are simulated using implicit coupling of the surface water and groundwater equations or to the kinematic wave equation for unsaturated flow where streams are separated from groundwater by an unsaturated zone niswonger and prudic 2005 pipe networks represented by sfr segments can be made semi pervious to represent leaky pipes there is no explicit representation of irrigation technology in the ag package such as sprinkler and drip equipment however differences in how irrigation is applied can be emulated using irrigation scheduling and application rates accordingly water can be applied to fields at a greater rate to represent flood irrigation and at a lower rate to represent sprinkler irrigation for example depending on the application rate and duration a portion of this water will runoff and flow laterally toward another surface water body runoff is routed in uzf using the irunbnd procedure for modflow simulations and by the prms cascade routing procedure for gsflow simulations niswonger et al 2006 markstrom et al 2008 henson et al 2013 additionally applied irrigation water can pass through the root zone beneath a field and deep percolate to the water table the amount of deep percolation also is dependent on irrigation technology scheduling and field hydraulic properties that can vary for each cell hru representing fields in the model alternatively irrigation return flow can be set using irrigation efficiency factors or a combination of explicitly represented infrastructure and efficiency factors 2 4 irrigation demand and scheduling irrigation demand can be specified directly by the user or demand can be calculated by the model using the et deficit equal to the reference et times the crop coefficient for well watered conditions minus the simulated actual et three options are provided in order to support applications to systems with differing amounts of data and differing agricultural practices for example if irrigation diversions and or groundwater withdrawals are accurately known then option 1 described below is suitable if irrigation withdrawals are uncertain and crop consumption rates are more certain then options 2 or 3 depending on irrigation practices are suitable only one of the options can be used in a single simulation 2 4 1 option 1 user specified irrigation demand and schedule using surface water diversions and or groundwater wells option 1 is the default approach fig 2 a and irrigation demand is set using time varying surface water diversions specified in sfr tabfiles or time varying pumping rates specified in ag alternatively the user can have the model calculate irrigation demand using options 2 or 3 in which case the time varying surface water diversions specified in sfr tabfiles or time varying pumping rates specified in ag represent maximum irrigation withdrawals for option 1 irrigation water is applied to modflow cells or prms hrus et and groundwater and surface water return flow is simulated using explicit representation of irrigation delivery infrastructure or infrastructure can be represented implicitly using efficiency factors and the difference between irrigation water delivery and crop consumption is applied as groundwater return flow and surface water return flow is assumed to be zero in many agricultural regions irrigation is provided by surface water and groundwater is used to supplement surface water during drought or seasonally low flow periods irrigation wells can be designated as supplementary wells and rather than specifying pumping rates pumping rates are calculated as the difference between the irrigation demand and the actual diverted surface water rate referred to as the surface water shortfall s f i r r l3t 1 2 s f i r r f a c m a x q d e m a n d q d i v e r s i o n where q d e m a n d is the volumetric demand rate for the irrigation period required for crop growth in 1 or more cells hrus supplied by a diversion l3t 1 q d i v e r s i o n is the volumetric diversion rate that can be less than or equal to q d e m a n d if surface water supplies limit the diversion rate l3t 1 f a c m a x is the maximum percentage of q d e m a n d that will be supplemented by groundwater the volumetric rate of supplementary groundwater irrigation for a diversion that can be supplied by 1 or more wells is calculated as q s u p l3t 1 3 q s u p f a c s u p s f i r r where f a c s u p is the fraction of s f i r r that will be supplemented by groundwater when using efficiency factors to simulate crop consumption and if water is supplied by surface water and supplemented by groundwater e t a for each cell hru is calculated as 4 e t a n n 1 n c e l l n h r u f f n e f g w q s u p e f s w q d i v e r s i o n a n and for groundwater only irrigation e t a for each cell hru is calculated as 5 e t a n n 1 n c e l l n h r u f f n e f g w q g w a n where e t a n is the actual et for cell hru n lt 1 ncell and nhru are the total number of modflow cells or prms hrus irrigated by a diversion or groundwater well f f n is the user specified fraction of the diverted irrigation water that will be applied to cell n q g w is the groundwater irrigation delivered to one or more cells hrus l3t 1 e f g w is the groundwater irrigation efficiency factor e f s w is the user specified surface water irrigation efficiency and a n is the area for cell hru n l2 groundwater return flow for a diversion and or groundwater or supplemental well q r e t u r n l3t 1 1 is calculated as 6 q r e t u r n 1 e f s w q s w 1 e f g w q g w the amount of groundwater return flow applied to each cell hru r f n lt 1 is 7 r f n n 1 n c e l l n h r u f f n q r e t u r n a n if efficiency factors are used to represent crop consumption e f s w and e f g w 0 then the uzf input variable pet and prms input parameter jh coef for the jensen haise formulation should be set to zero for cells hrus that contain fields note that efficiency factors partition water that is applied to fields into e t a and r f n however system gains losses that occur between the point of diversion and the place of use not including field gains losses must be simulated using pervious sfr segments or combined with field gains losses using efficiency factors surface water return flows that occur due to irrigation rates applied in excess of the vertical hydraulic conductivity of the field are not represented using approach 1 2 4 2 option 2 triggered irrigation events option 2 is activated when the character input variable trigger is specified in the ag input file fig 2b once the irrigation event is triggered the user specified diversion or pumped amount is delivered and applied to fields for the user specified irrigation period diversions are specified using sfr tabfiles and pumping rates are specified in ag supplementary groundwater pumping can be used to satisfy a surface water demand after an irrigation event is triggered as described in option 1 irrigation events can be triggered consecutively if the et ratio remains below the specified threshold irrigation automatically starts when the et ratio summed over all cells hrus supplied by a diversion or ag well decreases below a user specified threshold during the growing season irrigation is turned on when 8 q e t a c t u a l q s u m f c t t r i g g e r 9 q e t a c t u a l n 1 n c e l l s n h r u s a n e t a n and 10 q s u m n 1 n c e l l s n h r u s a n k c n e t o n where f c t t r i g g e r is the user specified et deficit threshold that triggers an irrigation event and is a value between zero and 1 q e t a c t u a l is the sum of actual et for all cells hrus irrigated by a diversion or well l3t 1 q s u m is the sum of crop et for well watered conditions for all cells uzf input variable pet multiplied by the cell area or hrus prms calculated value pet times pervious hru area irrigated by a diversion or well l3t 1 k c n is the crop coefficient for cell hru n and e t o n is the reference et for cell hru n an irrigation event for a diversion or well continues until 11 t i r r t p e r i o d where t i r r and t p e r i o d t are the elapsed and specified irrigation time respectively conditions for starting a new irrigation period are evaluated at the end of each period 2 4 3 option 3 optimal net irrigation water requirement option 3 is activated when the character input variable etdemand is specified in the ag input file fig 2c net irrigation withdrawal niw l is the total annual irrigation withdrawal required for plant growth divided by the irrigated area niw is calculated by the model according to 12 n i w g i w i r r l g where i r r l g l is the quantity of irrigation water loss or gain that occurs between the point of diversion up to and including the place of use divided by the irrigated area and g i w is the annual gross irrigation withdrawal defined as the irrigation withdrawal required for plant growth divided by the irrigated area including gains and losses that occur during delivery and on the field l supplementary groundwater pumping can be used to supply the giw as described in option 1 surface water and groundwater return flows can occur during delivery and on farms giw is calculated by the model as the amount of water that must be diverted and or pumped such that the difference between the simulated e t a and k c e t o is minimized for modflow simulations the product k c e t o under well watered conditions e t w w is specified as variable pet in uzf for gsflow e t w w is calculated as 13 e t w w k c p e t h r u the volumetric rate of water consumed by a crop for well water conditions q e t w w is 14 q e t w w n 1 n c e l l n h r u e t w w n a n where p e t h r u is calculated using the previously described approaches and is multiplied by k c internally for gsflow simulations the diversion and or pumped amount is calculated by minimizing min the et deficit e t d e f lt 1 as 15 m i n e t d e f e t w w e t a subject to the amount of surface water that can be diverted and or groundwater that can be pumped as with option 2 e t a and e t w w are summed over all fields irrigated by a diversion and or a well in addition to simulated water supply constraints values specified for diversions using sfr tabfiles and pumping rates specified in ag can be used to constrain irrigation timing and maximum amounts a solution to equation 15 is accomplished by determining the minimum amount of water required to be diverted or pumped to meet the crop water demand the volumetric rate of crop consumption for a time step can be written as a function of the irrigation demand as 16 q e t i 1 q e t i q e t i q t o t i δ q t o t i 1 and after substituting δ q t o t i 1 q t o t i 1 q t o t i and re arranging terms equation 16 becomes 17 q t o t i 1 q t o t i q e t i 1 q e t i q e t i q t o t i where i is an iteration counter for solving nonlinearities between irrigation demand and crop consumption q t o t i 1 and q t o t i are total irrigation water diverted and or pumped for iterations i 1 and i respectively l3t 1 q e t i 1 and q e t i are the crop consumption for iterations i 1 and i respectively l3t 1 note that i also is the modflow or gsflow outer iteration counter markstrom et al 2008 niswonger et al 2011 the amount of water that is applied to each cell hru n i r r n lt 1 that is irrigated by a diversion well is 18 i r r n f f s w n q s w f f g w n q g w a n where f f s w n and f f g w n are the fractions of the total irrigation water delivery from surface water and groundwater applied to each cell hru n respectively 2 4 4 constraining surface water diversions and groundwater pumping rates diversion and pumping rates are automatically constrained by the supply of surface water at the upstream diversion point and by the water table elevation in the cell that contains the well pumping rates specified in the ag input file are used to set the groundwater irrigation rate for option 1 or they can be used to set the maximum irrigation pumping rate for options 2 and 3 additional constraints can be applied to surface water diversions for all 3 options using diversion rates specified in sfr tabfiles and 1 of 4 diversion priority options in sfr prudic et al 2004 1 demand is greater than the flow available in the upstream segment and the diversion is reduced to the amount available 2 demand is greater than flow available in the upstream segment and no water is diverted from the stream 3 demand is greater than a specified fraction of the flow in the upstream segment and the diversion is reduced to the fraction of flow 4 diversion is set equal to demand only if the remaining streamflow in the upstream segment exceeds the value specified in the sfr tabfile otherwise no water is diverted 3 description of example problems two example problems are presented to illustrate the capabilities of the ag package for simulating water use by agriculture example problem 1 is a modflow simulation that was modified from test 1 presented previously by prudic and others 2004 test problem 2 is a gsflow simulation that was modified from the sagehen creek watershed gsflow example problem markstrom et al 2008 although there is no agriculture in the sagehen creek watershed the ag package was added for this example to simulate irrigation from surface water and supplementary wells to several hrus in the lower part of the watershed that represent hypothetical agricultural fields both example problems retain the units used in their original presentations and thus example problem 1 uses english units and example problem 2 uses metric units 3 1 example problem 1 modflow with conjunctive use of surface water sw and groundwater gw etdemand option this model represents an alluvial river basin in a semi arid region the basin receives most of its precipitation in the surrounding mountains and intermittent streams drain the mountains and flow into a perennial river that crosses the southern portion of the valley fig 3 the valley aquifer consists of alluvium dominated by sand and gravel and the mountains consist of bedrock that has much lower hydraulic conductivity than the valley alluvium recharge in the basin primarily occurs as seepage loss from the intermittent stream channels and to a lesser extent as groundwater flowing to the valley from the mountain block and diffuse recharge through valley sediment prudic and others 2004 present additional details describing this test problem including representation of the stream network and distribution of recharge and et parameters used within the model niswonger and others 2006 describe modifications made to this example to replace the et and recharge packages with the uzf package excess applied infiltration and rejected infiltration spring discharge is routed to streams the model domain extends to a maximum of 520 feet below land surface in the valley bottom and extends laterally 14 miles in the north south direction and 9 5 miles in the east west direction fig 3 the model is discretized into 1 layer 15 rows and 10 columns and only model cells coincident with the basin fill are active consolidated rocks are not included layer 1 ranges in thickness between 130 feet and 520 feet model cells have a constant dimension of 5000 feet in the row and column directions a total of 3440 acres 6 model cells are irrigated for agriculture in the central part of the basin irrigation water is diverted from the green river fig 3 and pumped from the shallow aquifer beneath the fields two tributary streams that enter the model from the northwest and northeast join the mainstem in the southern part of the model fig 3 simulations included an initial steady state stress period followed by forty eight transient stress periods each stress period represents a calendar month that is divided into daily time steps the simulation begins on january 1 results are presented for the final 2 years of the simulation as the steady state stress period and first 2 years of the simulations are used to establish initial conditions hydraulic conductivity and specific yield of the water table aquifer increase in the valley bottoms that comprise of floodplains or new alluvium of the tributary streams and river monthly e t w w was specified as uzf input variable pet using annual estimates disaggregated into monthly values using average monthly temperatures prudic and herman 1996 users are referred to the input files for this problem that accompany this work for additional details two versions of example problem 1 are presented example problem 1a ep1a simulates irrigation water provided by surface water and supplementary groundwater and example problem 1b ep1b that simulates irrigation water provided solely by groundwater both models simulate irrigation demands using the etdemand approach that minimizes the et deficit using equation 17 fig 3 shows the cells designated as agricultural fields that receive irrigation sfr diversion segment number 9 was used to divert water from the green river and route it to the fields fig 3 demand is satisfied solely by groundwater in ep1b 3 2 example problem 2 gsflow conjunctive use of sw and gw etdemand verses trigger options example problem 2 was developed by modifying the gsflow sagehen example problem markstrom et al 2008 to include agricultural fields in the lower part of the basin fig 4 sagehen creek drains a 27 km2 watershed on the east slope of the northern sierra nevada geology of the sagehen creek watershed consists of granodiorite bedrock overlain by andesitic tertiary volcanic material which are overlain by till and alluvium composed of granodiorite and andesite clasts and some quaternary gravels burnett and jennings 1965 the principal aquifer model layer 2 was assumed to consist of volcanic material with thickness ranging between 50 and 300 m a veneer of alluvium covers the volcanic material that is thicker along channels in the lower section of the watershed burnett and jennings 1965 alluvium model layer 1 was assumed to range in thickness between 0 and 10 m the model domain extends laterally 6 4 km in the north south direction and 7 1 km in the east west direction fig 4 the model is discretized into 90 90 m cells using 2 layers 71 rows and 79 columns eighteen years are simulated each year is divided into 12 stress periods each period represents a calendar month and is divided into daily time steps the transient simulation begins on october 1 two versions of example problem 2 are presented example problem 2a ep2a and example problem 2b ep2b simulate demand using the etdemand and trigger options respectively fig 4 shows the cells designated as agricultural fields that receive irrigation including 34 cells irrigated by 2 segments that divert water from sagehen creek segment 18 supplies water for 14 cells and segment 19 supplies water for 20 cells fig 4 all 34 irrigated cells sum to an area to 27 5 ha irrigation can be nonzero during the growing season june 1 august 30 and zero outside the growing season these constraints on the surface water diversions for irrigation were specified using sfr tabfiles that define maximum diversion amounts for segment numbers 18 and 19 wells were placed in each agricultural cell for supplementary pumping to meet irrigation requirements 4 results 4 1 example problem 1a impacts of sw supply on supplementary gw pumping ep1a was run with high and low inflow hydrographs fig 5 representing average and drought years respectively to evaluate how differences in surface water supply impact the relative proportions of surface water and supplemental groundwater used for irrigation fig 6 specified surface water inflow enters the model through segment 1 over the northeast corner of the model boundary fig 3 maximum surface water diversions rates were set in a sfr tabfile for segment 9 with an irrigation period from april 1 to september 30 and a maximum rate of 100 ft3 s which is greater than the maximum irrigation demand and thus only the amount of flow in segment upstream of the irrigation segment will constrain irrigation option 1 soil and crop properties for ep1a are those of the fine textured soil shown in table 1 fig 6 shows the proportions of surface water and supplementary groundwater used for irrigation for the average and drought conditions for this example irrigation demand is nearly equal to crop consumption due to the values of et extinction depth saturated water content and natural rainfall supplementary groundwater makes up a greater proportion of the irrigation water supply during the low flow hydrograph 53 relative to the high flow hydrograph 42 due to surface water supply constraints fig 5 average annual irrigation water requirements were the same for both simulations 2 58 feet and slightly less than the annual average crop consumption 2 6 feet due to small amounts of precipitation in the valley supplementary pumping rates increase abruptly right as the flow at the diversion point decreases and then re equilibrate as the crop demand eta decreases similarly pumping rates decrease abruptly when the demand decreases abruptly fig 6 4 2 example problem 1b impacts of soil properties on gw irrigation demand this example problem is run for 2 different agricultural field soil types including fine and coarse soil textures table 1 the coarse soil requires greater amounts of irrigation earlier during the growing season relative to the fine soil because of lower antecedent soil moisture at the onset of the growing season fig 7 faster drainage increases the average annual irrigation demand for the coarse soil 3 1 feet relative to the fine soil 2 6 feet due to greater amounts of groundwater return flow return flow is greater for coarse soils because of lower saturated water content porosity and greater deep percolation as there is no constraint on irrigation supply average annual e t a equals the e t w w of 2 7 acre feet per acre irrigation demand is slightly less than crop consumption for fine soil due to rain fed irrigation and the larger soil storage relative to coarse soil 4 3 example problem 2a effects of crop coefficient on irrigation demands ep2a illustrates the effects of the crop coefficient k c on irrigation demand and crop consumption using the etdemand option for irrigation fig 8 k c is incorporated into the et demand by multiplying the prms input parameter jh coef by the monthly k c values note that jh ceof can be specified for each hydrologic response unit in prms and for each of the 12 calendar months markstrom et al 2015 as this option represents optimal irrigation scheduling to minimize the et deficit these results reflect optimal water use to meet crop water demand annual average giwr for the period 1991 1993 is 1 1 ha meter per hectare for high k c and 0 70 ha meter per hectare for low k c annual average crop consumption is equal to 1 06 ha meters per hectare for high k c and 0 81 ha meter per hectare for low k c actual et equals e t w w in this example because the etdemand option is used and because constraints on the irrigation amounts set in the sfr tabfile and ag pumping rates did not limit irrigation irrigation demand is less than crop consumption for 1992 and 1994 because of water supplied by precipitation and sub irrigation however 1993 was a drought year and demand is greater than consumption ep2a also demonstrates the influence that early growing season antecedent soil water conditions have on crop water demand total annual precipitation amounts measured at the independence lake climate station for water years 1991 1992 and 1993 was 83 cm 71 cm and 149 cm respectively while demand was 79 158 and 70 percent of the crop water consumption during these years for the case of high k c and 66 129 and 62 percent of the crop water consumption during these years for the case of low k c fig 9 real world irrigation practices likely cannot exactly mimic this optimal irrigation schedule for practical and logistical reasons nonetheless these model results are useful for providing guidance on irrigation schedules setting lower bounds on irrigation demand and for providing a base model for evaluating factors affecting demand and consumption irrigation constraints can be superimposed onto the etdemand option using sfr tabfiles and ag pumping rates to mimic real world conditions as will be shown in ep2b additional flexibility in simulating irrigation practices is provided by the trigger option 4 4 example problem 2b using irrigation triggers to estimate demand example problem 2b is identical to ep2a except that the trigger option is used and seasonal k c were set using the high k c curve fig 8 ep2b illustrates the influence that different irrigation trigger values have on the surface water diversions and groundwater pumping rates an irrigation event starts when the et ratio e t a e t w w becomes less than the specified trigger threshold results are shown for a high 0 85 and low 0 35 trigger value representing well watered and deficit irrigation conditions respectively note that the length of an irrigation period is specified as 3 days for both high and low trigger values however if the et ratio is below the trigger value at the end of a period then a new irrigation period will begin immediately irrigation delivery is directly proportional to the trigger value where higher trigger values result in greater surface water diversions pumping and crop water consumption fig 10 irrigation demand for the period 1991 1993 is 1 4 ha meters per hectare for a high trigger value and 0 7 ha meters per hectare for a low trigger value annual average crop consumption is the same as ep2a 1 06 ha meter per hectare except for the low trigger value simulation results in a crop consumption of 0 83 ha meter per hectare a low trigger value causes the model to delay an irrigation event because the simulated et will reduce to a lower fraction of the reference et before an irrigation event is triggered accordingly lower trigger values allow the soils to drain longer between irrigation events and lower trigger values result in less actual et as compared to higher trigger values fig 10b generally the trigger option results in significantly more surface water and groundwater irrigation withdrawal relative to the etdemand option this is because the irrigation rate is specified for the trigger option and may not be optimal for an agricultural field whereas the irrigation rate is calculated as a function of the et deficit for the demand option and reflects the optimal irrigation rate 5 discussion a new package for modflow and gsflow is presented that provides capabilities for simulating agricultural water use in regional scale hydrologic models the ag package can be used to estimate agricultural water use for systems where information about irrigation supply and demand are not available or it can be used to simulate irrigation withdrawals and their impacts on water supply the latter application is important in regions where there are competing needs for water and climate change population growth and land use change are causing unknown impacts design of the ag package includes flexibility for representing systems with varying amounts of data different grower irrigation practices and feedbacks between water supply and water use by agriculture water demands rely on energy and soil water balance calculations and regionally specific conditions can be represented such as spatial variations in temperature solar radiation and plant type specific attributes of a region can be considered including soil hydraulic properties depth to groundwater canal or pipe properties and antecedent soil moisture and precipitation water consumption relies on explicit simulation of irrigation infrastructure soil water budgets and surface water and groundwater availability these design features provide flexibility for evaluating water use in a wide variety of agricultural systems and for developing optimal irrigation schedules unique to a region however the effects of salinity stress on crops and crop water use are not represented in the ag package existing software used to simulate agricultural water use in regional hydrologic models do not provide capabilities of the ag package the modflow based package called the farm process requires monthly time steps and it does not simulate soil water balance for the estimation of irrigation withdrawals and e t a hanson et al 2014 the ag package simulates daily soil water dynamics that play an important role in determining irrigation schedules and amounts soil water balance is important for representing the rain fed component of crop consumption required for estimating irrigation withdrawals senay et al 2014 landsat derived e t a can be integrated through soil water balance into hydrology models that represent both agricultural systems and the broader regional to national hydrologic system a variety of options are provided for mimicking different irrigation practices specifically with regards to the timing and amounts of irrigation examples are presented that illustrate impacts of surface water supply on groundwater pumping ep1a irrigation supplied solely by groundwater ep1b irrigation estimated for optimal water use conditions that minimizes the et deficit ep2a and irrigation that is triggered when the et deficit drops below a specified threshold all these approaches are provided as options to best represent regionally specific conditions because irrigation water is explicitly routed and applied to individual fields the model can be used to evaluate irrigation return flows and changes in land use practical applications of integrated hydrologic models that represent agricultural water use must rely on data that characterize a broad range climactic and hydrogeologic conditions additionally representation of agriculture requires characterization of water governance and irrigation practices complete data sets are rarely available and integrated models provide a means of maximizing information with partial data sets by combining data with physical process equations and generalized frameworks for representing human impacts on water distribution and consumption the ag package for modflow and gsflow provides a powerful decision support tool that can maximize understanding of water resources in agricultural basins and provide hindcast information about historical water budgets and system response as well as future projections of sustainability and management change 6 conclusions hydrologic simulation of developed basins is difficult or impossible without representing agricultural water use integrated hydrologic models are useful decision support tools for developing regional water budgets and evaluating water management strategies and sustainability for human populations and ecosystem services despite significant data gaps in water use at regional scales hydrologic models can complement incomplete datasets and provide a more complete picture of water resources process understanding and theoretical representation of agricultural water use are well established however limited software is available that explicitly represents agricultural water use in regional scale integrated hydrologic models the ag package for modflow and gsflow provides a wholistic representation of agricultural water use in the context of the natural hydrologic system and other water use sectors through a series of simple but realistic example problems this paper demonstrates the software s applicability for a variety of approaches for simulating irrigation practices and associated effects on water distribution and supply in regional scale systems disclosure any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government declaration of competing interest there are no potential conflicts of interest for the work presented in the manuscript titled an agricultural water use package for modflow and gsflow acknowledgements support for the author was provided by the usgs water availability and use program additionally support was provided by the national science foundation grant number 1360506 and the u s department of agriculture national institute of food and agriculture grant number 1360507 the author is thankful for the thoughtful reviews provided by hedeff essaid usgs and 2 anonymous reviewers the author also thanks r steven regan usgs for his input on code implementation 
26070,the agricultural water use ag package was developed for simulating demand driven and supply constrained agricultural water use in modflow and gsflow models the ag package uses pre existing hydrologic simulation provided by modflow and gsflow three options are available for simulating water use for agriculture 1 user specified demands 2 demands determined by a user specified irrigation trigger value that is compared to the ratio of the simulated actual to potential evapotranspiration et and 3 demands determined by minimizing the difference between potential and actual et the latter two approaches use energy and soil water balance to determine crop water demands irrigation withdrawals are diverted into canals and routed to fields using the modflow sfr package or irrigation water is provided supplemented by groundwater combined with modflow or gsflow the ag package can simulate dynamic water use by agriculture in developed basins while providing flexibility to represent a range of irrigation practices keywords integrated hydrologic modeling agricultural water use gsflow modflow drought irrigation withdrawals water resources conjunctive use surface water and groundwater interactions software and or data availability section software and data used for this work model input files for each example problem and ancillary data are available through the github repository https github com rniswon mfnwt tree agoptions the agricultural ag water use package was developed by richard niswonger rniswon usgs gov and was released february 2020 and runs on windows unix and macintosh operating systems and requires no specific hardware or software to run gsflow and its components are written in fortran and the program files are less than 10 mbytes the ag package including source code compiled binary files for windows and unix example problems and jupyter notebooks for plotting results are provided through the mfnwt github repository 1 introduction agriculture is a major water consumer in many basins around the world and estimating irrigation withdrawals in hydrologic models is important for water resources planning and management wang et al 1996 jones et al 2017 water management decision support software is paramount in many river basins in the western united states and other parts of the world for adapting to climate change and population growth and for evaluating new water management strategies tian et al 2015 hydrologic models that incorporate surface water and groundwater can provide valuable information about water resources sustainability in conjunctive use systems this is especially true for agricultural regions susceptible to climate change and population growth that stress water supplies faunt 2009 elliott et al 2014 gorelick and zheng 2015 hydrologic software such as modflow simulates 3 dimensional groundwater flow and includes many add on capabilities such as representation of surface water features and other hydrologic processes harbaugh 2005 langevin et al 2017 gsflow is the integration of modflow and prms and can simulate all major hydrologic processes in watersheds including distributed energy and water consumption by plants markstrom et al 2008 2015 gsflow can simulate partitioning of precipitation into snowpack runoff evapotranspiration et and groundwater flow using energy and water balance approaches markstrom et al 2008 modflow and gsflow have been used for simulating regional scale agricultural systems hu et al 2010 morway et al 2013 bailey et al 2016 wu et al 2016 guzman et al 2015 woolfenden and nishikawa 2014 essaid and caldwell 2017 an add on to modflow called the farm process was developed to represent agricultural systems supplied by surface water and groundwater schmid et al 2006 hanson et al 2010 2014 a common approach for simulating agricultural systems in regional integrated models is to estimate irrigation demands as a pre processing step where a separate soil water balance model is used to calculate demands irrigation demands are subsequently specified to a regional integrated model that does not simulate field soil water balance dogrul et al 2011 the farm process for modflow 2005 assumes the irrigation demand is independent of a farm s soil water content and that precipitation can be subtracted from reference et to account for rain fed crop consumption hanson et al 2010 2014 another approach presented herein is to include simulation of soil water balance within the hydrologic simulator to better represent soil water in irrigated lands the advantage of this approach is that simulated soil water conditions can be used to estimate the rain fed component of crop consumption for the estimation of irrigation withdrawals e g allen et al 2007 huntington et al 2017 here an agricultural ag water use package is presented for modflow and gsflow for regional river basin scale simulations the ag package also can simulate conjunctive use of surface water and groundwater by automatically pumping groundwater when surface water availability is less than demand schmid et al 2006 because irrigation demand irrigation efficiency and crop consumption can be simulated using daily climatic conditions the model can be used to simulate impacts of climate change on water supply the ag package can represent changes in land use including changes in crop type expansion or contraction of farmlands or changes in irrigation technology through existing features in gsflow and recent enhancements regan and lafontaine 2017 changes in land use can be simulated using the dynamic parameters capability in gsflow to represent changes in vegetation cover type crop coefficients and other input parameters that vary with changes in land use regan and lafontaine 2017 climate variability can cause regional shifts in agricultural demand due to systematic changes in soil moisture and irrigated lands and indirectly as reductions in return flows fischer et al 2007 interactions such as these occur over time periods that span irrigation events or irrigation seasons or they can span much longer time periods due to multi year shifts in climate and groundwater supply evolving supply and demand conditions such as these support simulating demand using energy and soil water balance within integrated hydrologic models rather than estimating demands as a pre processing step or independent of soil moisture the ag package for modflow and gsflow provides a wholistic approach for representing dynamic irrigation withdrawals and can be used for planning and assessing impacts of agriculture on other water use sectors and for evaluating long term sustainability the ag package also provides necessary capabilities for integration of gsflow with river reservoir operations models such as modsim for simulating impacts of water use priorities on agricultural systems labadie 2010 morway et al 2016 niswonger et al 2017 kitlasten et al 2020 two example problems are presented for representing agriculture in modflow and gsflow and these examples are run using different options to demonstrate application of the new package and its capabilities for simulating agricultural water use for different hydrographic settings and irrigation practices example problem 1 demonstrates the new package in a modflow simulation and represents an agricultural basin in northwest nevada prudic et al 2004 the second example demonstrates the package in a gsflow simulation and represents an undeveloped basin in northeast california including hypothetical irrigated lands previously published work provides theory and application of modflow and gsflow and only new theoretical and implementation details for the ag package are provided herein readers can refer to these published works for simulation capabilities related to modflow and gsflow including energy and water balance calculations for hydrologic simulations that are used by the ag package harbaugh 2005 markstrom et al 2008 niswonger et al 2011 general descriptions of the components in an agricultural system are provided here to set the context for the theoretical explanation of these components this is followed by descriptions of the integration between the agricultural system and the regional hydrologic system details of the algorithms and model code developed for the ag package are provided as wells as explanation of the various options that can be used to simulate agricultural water use two different example models are described to illustrate the implementation of the ag package using both the modflow and gsflow hydrologic modeling frameworks results of these models and their discussion are provided to highlight appropriate use of different model options and implications of these options in the model results 2 methods 2 1 irrigation water delivery in practice irrigation is withdrawn from one location and it is routed through reservoirs streams canals pipes and furrows to its place of use fig 1 a place of use is an agricultural field where plant roots uptake water from shallow soils as water is delivered to fields part of it is lost along the way due to et leaky pipes and canals misdirected surface flows and seepage irrigation water also can increase during delivery if the irrigation system gains from other sources not all the water applied to fields is used by the crop and instead there are field losses due to surface runoff seepage below the plant roots and soil evaporation field losses depend on field conditions and the irrigation practices that vary with irrigation approach such as flood sprinkler and drip irrigation conveyance and other system gains and losses cause irrigation withdrawals to be different than crop consumption this difference is referred to as the system efficiency allen et al 1998 the ag package was developed to represent these processes explicitly using hydrologic simulation capabilities in modflow and gsflow or implicitly by specifying efficiency factors to represent all or a portion of the system gains and losses 2 1 1 surface water irrigation surface water delivery for irrigation is simulated by the modflow streamflow routing sfr package including open channel flow in streams and canals or non pressurized flow through pipes prudic et al 2004 niswonger and prudic 2005 surface water demands for diverting irrigation water and applying it to fields can be set by user specified values or they can be calculated by the model using field based crop water demands sfr routes steady or kinematic flow by coupling continuity and manning s equation and user defined relationships between flow area and depth to represent a variety of flow geometries sfr neglects diffusion and other acceleration terms in the shallow water and pipe flow equations however as times steps are typically 1 day or longer this simplification is generally applicable for regional agricultural systems diversion segments are used to deliver irrigation water and are initialized in the sfr input file diversion segments can be designated as irrigation diversions in the ag input file to apply diverted surface water to fields sfr diversion flow rates are constrained by the amount of water flowing in the upstream segment and 1 of 4 water use priority options prudic et al 2004 surface reservoirs are simulated by the modflow lake lak package merritt and konikow 2000 for modflow simulations and or open detention storage reservoirs for gsflow simulations regan and lafontaine 2017 sfr routes channel flows into and out of reservoirs represented by lak and open detention storage reservoirs diversion segments and reservoirs represented by sfr and lak are integrated with the groundwater flow equation to simulate surface water and groundwater interactions however open detention reservoirs do not interact with groundwater 2 1 2 groundwater irrigation groundwater irrigation is provided by wells that can pump water from a groundwater cell wells are defined and maximum pumping rates are specified within the ag package input file irrigation wells are assumed to have a screened interval that spans the model cell thickness and smoothing functions are used to reduce the pumping rate to zero as the water table drops below the cell bottom niswonger et al 2011 non irrigation wells such as public supply or thermoelectric wells are handled outside of the ag package using one of the other modflow well packages harbaugh 2005 ag wells must have negative pumping rates to represent flow out of an aquifer groundwater wells are designated in ag as irrigation wells to apply pumped groundwater to fields pumping rates for irrigation wells can be set by user specified values or they can be calculated by the model using groundwater irrigation demands pumping rates also can be calculated by the model to supplement surface water rights such that all or a portion of the shortage is pumped from groundwater however this version of the ag package cannot be used to represent the use of surface water to supplement a groundwater right 2 1 3 mapping point of diversion to place of use irrigation provided by diversion segments and groundwater wells is applied to designated cells or hydrologic response units hrus with a user specified mapping between numerically identified sfr segments ag wells and cells hrus modflow simulations require that ag features irrigation diversions or wells be associated with modflow cells because surface spatial units in modflow are cells however for gsflow simulations surface spatial units are hrus and ag package features must be associated with hrus the point of diversion is located at the upstream end of a diversion segment or well used for irrigation and the place of use is the area of fields irrigated by the diversion modflow cells are identified by their row and column hrus are identified by their hru id diversion segments are identified by their sfr segment number and wells are identified by their ag well number mapping identifiers are input to the ag input file and they can change during a simulation to represent changes in withdrawal locations or irrigated lands sfr diversion segments can consist of 1 or more reaches where reaches are the length of stream or canal that spans a single model cell a segment can span many model cells to represent great distances between a withdrawal point and irrigated field and diversion segments can divert from other diversion segments irrigation cannot be applied to a partial area of a cell in modflow which could be a limitation in models with cells that are larger than fields however irrigation can be applied to a fraction of an hru using the impervious fraction parameter and non irrigated areas within an hru are assumed to be impervious a diversion and or well can provide water for multiple cells hrus or multiple diversions and or wells can provide water for a single cell hru additionally a well can supplement several diversions or several wells can supplement one or more diversions if multiple sfr diversions supply irrigation to a single cell hru then the order that water is diverted occurs in the same order that the irrigation segments are specified however if multiple wells supply a single cell hru then the demand is split evenly among the wells 2 2 simulating crop consumption et can be simulated using soil water balance over any time step length for modflow simulations or et can be simulated using daily energy and water balance for gsflow simulations markstrom et al 2008 niswonger et al 2011 actual crop et e t a can be calculated by uzf as a function of the depth dependent soil water contents using a kinematic wave formulation niswonger et al 2006 or e t a can be calculated by prms as a function of volume averaged soil saturation using a nonlinear soil water reservoir approach markstrom et al 2008 2015 crop specific et demand is calculated by multiplying the crop coefficient k c by the reference et e t o allen et al 1998 a single crop coefficient approach is used by the ag package and k c represents crop specific information including growth patterns and soil evaporation seasonal k c values for common crops are available in the literature allen et al 1998 if using uzf to represent agricultural fields then the product k c e t o is input for the uzf variable pet if using prms to represent agricultural fields e t o is calculated using one of six options available in prms including jensen haise hargraeves semani penman monteith priestly taylor hamon and pan potential et modules markstrom et al 2015 example problem 2 below demonstrates how k c is incorporated into gsflow simulations using the prms jensen haise formulation other than including k c into the calculation of e t o all other prms input does not change due to the ag package sub irrigation is a process in which plants use shallow groundwater to meet crop water demands growers apply less irrigation water where there is shallow groundwater beneath their crops thus this process is important for estimating irrigation demand sub irrigation is simulated by uzf assuming a linear capillary rise as a function of groundwater head sub irrigation is simulated in gsflow by groundwater discharge to the prms soil zone due to linear capillary rise or saturated discharge conditions niswonger et al 2006 markstrom et al 2008 total crop consumption for a cell e t a is calculated in uzf by summing the unsaturated zone and groundwater e t a where groundwater e t a is a linear function of the water table elevation above the et extinction depth and is zero when the water table is below the et extinction depth uzf input variable extdp additional to the previously available approach for simulating e t a in uzf a new option was added to simulate crop consumption using a pressure gradient approach this approach is recommended for the ag package and etdemand option and a description is provided here because it was not included in the original uzf or gsflow documents for this case the capillary pressures are calculated in the crop root zone using the brooks corey retention function and 3 new uzf input variables including the root activity function air entry pressure and root pressure lappala et al 1987 e t a is calculated using 1 e t a d t k θ r t ψ θ h r o o t where d t is the thickness of the root zone or et extinction depth that can change during the growing season l k θ is unsaturated hydraulic conductivity as a function of water content lt 1 r t is the root activity function that can change during the growing season l 2 ψ θ is capillary pressure head as a function of water content l and h r o o t is the negative root pressure head l variables in equation 1 are calculated using brooks and corey 1966 unsaturated hydraulic conductivity and capillary pressure functions this option also is documented by langevin et al 2017 2 3 simulating irrigation return flows irrigation return flow is water that returns to a surface water body or seeps to groundwater rather than entering the atmosphere due to et it is considered return flow because the water becomes available to other growers or for other uses in the system return flow can occur anywhere between a withdrawal and a field including the area where irrigation is applied gains and losses in the irrigation conveyance system are represented by the integration of surface water and groundwater in sfr and lak for the channel and surface reservoir domains canal or pond and gains and losses are simulated by uzf or prms for the overland flow domain field return flow also occurs between the overland flow domain and the channel and reservoir domains groundwater return flow occurs as irrigation percolates beneath the uzf et extinction depth or through the base of the soil zone defined in prms there is no explicit representation of irrigation for salt leaching however specified amounts of irrigation can be applied to cells hrus to represent salt leaching effects of salt stress on e t a are neglected exchanges between surface water and groundwater are simulated using implicit coupling of the surface water and groundwater equations or to the kinematic wave equation for unsaturated flow where streams are separated from groundwater by an unsaturated zone niswonger and prudic 2005 pipe networks represented by sfr segments can be made semi pervious to represent leaky pipes there is no explicit representation of irrigation technology in the ag package such as sprinkler and drip equipment however differences in how irrigation is applied can be emulated using irrigation scheduling and application rates accordingly water can be applied to fields at a greater rate to represent flood irrigation and at a lower rate to represent sprinkler irrigation for example depending on the application rate and duration a portion of this water will runoff and flow laterally toward another surface water body runoff is routed in uzf using the irunbnd procedure for modflow simulations and by the prms cascade routing procedure for gsflow simulations niswonger et al 2006 markstrom et al 2008 henson et al 2013 additionally applied irrigation water can pass through the root zone beneath a field and deep percolate to the water table the amount of deep percolation also is dependent on irrigation technology scheduling and field hydraulic properties that can vary for each cell hru representing fields in the model alternatively irrigation return flow can be set using irrigation efficiency factors or a combination of explicitly represented infrastructure and efficiency factors 2 4 irrigation demand and scheduling irrigation demand can be specified directly by the user or demand can be calculated by the model using the et deficit equal to the reference et times the crop coefficient for well watered conditions minus the simulated actual et three options are provided in order to support applications to systems with differing amounts of data and differing agricultural practices for example if irrigation diversions and or groundwater withdrawals are accurately known then option 1 described below is suitable if irrigation withdrawals are uncertain and crop consumption rates are more certain then options 2 or 3 depending on irrigation practices are suitable only one of the options can be used in a single simulation 2 4 1 option 1 user specified irrigation demand and schedule using surface water diversions and or groundwater wells option 1 is the default approach fig 2 a and irrigation demand is set using time varying surface water diversions specified in sfr tabfiles or time varying pumping rates specified in ag alternatively the user can have the model calculate irrigation demand using options 2 or 3 in which case the time varying surface water diversions specified in sfr tabfiles or time varying pumping rates specified in ag represent maximum irrigation withdrawals for option 1 irrigation water is applied to modflow cells or prms hrus et and groundwater and surface water return flow is simulated using explicit representation of irrigation delivery infrastructure or infrastructure can be represented implicitly using efficiency factors and the difference between irrigation water delivery and crop consumption is applied as groundwater return flow and surface water return flow is assumed to be zero in many agricultural regions irrigation is provided by surface water and groundwater is used to supplement surface water during drought or seasonally low flow periods irrigation wells can be designated as supplementary wells and rather than specifying pumping rates pumping rates are calculated as the difference between the irrigation demand and the actual diverted surface water rate referred to as the surface water shortfall s f i r r l3t 1 2 s f i r r f a c m a x q d e m a n d q d i v e r s i o n where q d e m a n d is the volumetric demand rate for the irrigation period required for crop growth in 1 or more cells hrus supplied by a diversion l3t 1 q d i v e r s i o n is the volumetric diversion rate that can be less than or equal to q d e m a n d if surface water supplies limit the diversion rate l3t 1 f a c m a x is the maximum percentage of q d e m a n d that will be supplemented by groundwater the volumetric rate of supplementary groundwater irrigation for a diversion that can be supplied by 1 or more wells is calculated as q s u p l3t 1 3 q s u p f a c s u p s f i r r where f a c s u p is the fraction of s f i r r that will be supplemented by groundwater when using efficiency factors to simulate crop consumption and if water is supplied by surface water and supplemented by groundwater e t a for each cell hru is calculated as 4 e t a n n 1 n c e l l n h r u f f n e f g w q s u p e f s w q d i v e r s i o n a n and for groundwater only irrigation e t a for each cell hru is calculated as 5 e t a n n 1 n c e l l n h r u f f n e f g w q g w a n where e t a n is the actual et for cell hru n lt 1 ncell and nhru are the total number of modflow cells or prms hrus irrigated by a diversion or groundwater well f f n is the user specified fraction of the diverted irrigation water that will be applied to cell n q g w is the groundwater irrigation delivered to one or more cells hrus l3t 1 e f g w is the groundwater irrigation efficiency factor e f s w is the user specified surface water irrigation efficiency and a n is the area for cell hru n l2 groundwater return flow for a diversion and or groundwater or supplemental well q r e t u r n l3t 1 1 is calculated as 6 q r e t u r n 1 e f s w q s w 1 e f g w q g w the amount of groundwater return flow applied to each cell hru r f n lt 1 is 7 r f n n 1 n c e l l n h r u f f n q r e t u r n a n if efficiency factors are used to represent crop consumption e f s w and e f g w 0 then the uzf input variable pet and prms input parameter jh coef for the jensen haise formulation should be set to zero for cells hrus that contain fields note that efficiency factors partition water that is applied to fields into e t a and r f n however system gains losses that occur between the point of diversion and the place of use not including field gains losses must be simulated using pervious sfr segments or combined with field gains losses using efficiency factors surface water return flows that occur due to irrigation rates applied in excess of the vertical hydraulic conductivity of the field are not represented using approach 1 2 4 2 option 2 triggered irrigation events option 2 is activated when the character input variable trigger is specified in the ag input file fig 2b once the irrigation event is triggered the user specified diversion or pumped amount is delivered and applied to fields for the user specified irrigation period diversions are specified using sfr tabfiles and pumping rates are specified in ag supplementary groundwater pumping can be used to satisfy a surface water demand after an irrigation event is triggered as described in option 1 irrigation events can be triggered consecutively if the et ratio remains below the specified threshold irrigation automatically starts when the et ratio summed over all cells hrus supplied by a diversion or ag well decreases below a user specified threshold during the growing season irrigation is turned on when 8 q e t a c t u a l q s u m f c t t r i g g e r 9 q e t a c t u a l n 1 n c e l l s n h r u s a n e t a n and 10 q s u m n 1 n c e l l s n h r u s a n k c n e t o n where f c t t r i g g e r is the user specified et deficit threshold that triggers an irrigation event and is a value between zero and 1 q e t a c t u a l is the sum of actual et for all cells hrus irrigated by a diversion or well l3t 1 q s u m is the sum of crop et for well watered conditions for all cells uzf input variable pet multiplied by the cell area or hrus prms calculated value pet times pervious hru area irrigated by a diversion or well l3t 1 k c n is the crop coefficient for cell hru n and e t o n is the reference et for cell hru n an irrigation event for a diversion or well continues until 11 t i r r t p e r i o d where t i r r and t p e r i o d t are the elapsed and specified irrigation time respectively conditions for starting a new irrigation period are evaluated at the end of each period 2 4 3 option 3 optimal net irrigation water requirement option 3 is activated when the character input variable etdemand is specified in the ag input file fig 2c net irrigation withdrawal niw l is the total annual irrigation withdrawal required for plant growth divided by the irrigated area niw is calculated by the model according to 12 n i w g i w i r r l g where i r r l g l is the quantity of irrigation water loss or gain that occurs between the point of diversion up to and including the place of use divided by the irrigated area and g i w is the annual gross irrigation withdrawal defined as the irrigation withdrawal required for plant growth divided by the irrigated area including gains and losses that occur during delivery and on the field l supplementary groundwater pumping can be used to supply the giw as described in option 1 surface water and groundwater return flows can occur during delivery and on farms giw is calculated by the model as the amount of water that must be diverted and or pumped such that the difference between the simulated e t a and k c e t o is minimized for modflow simulations the product k c e t o under well watered conditions e t w w is specified as variable pet in uzf for gsflow e t w w is calculated as 13 e t w w k c p e t h r u the volumetric rate of water consumed by a crop for well water conditions q e t w w is 14 q e t w w n 1 n c e l l n h r u e t w w n a n where p e t h r u is calculated using the previously described approaches and is multiplied by k c internally for gsflow simulations the diversion and or pumped amount is calculated by minimizing min the et deficit e t d e f lt 1 as 15 m i n e t d e f e t w w e t a subject to the amount of surface water that can be diverted and or groundwater that can be pumped as with option 2 e t a and e t w w are summed over all fields irrigated by a diversion and or a well in addition to simulated water supply constraints values specified for diversions using sfr tabfiles and pumping rates specified in ag can be used to constrain irrigation timing and maximum amounts a solution to equation 15 is accomplished by determining the minimum amount of water required to be diverted or pumped to meet the crop water demand the volumetric rate of crop consumption for a time step can be written as a function of the irrigation demand as 16 q e t i 1 q e t i q e t i q t o t i δ q t o t i 1 and after substituting δ q t o t i 1 q t o t i 1 q t o t i and re arranging terms equation 16 becomes 17 q t o t i 1 q t o t i q e t i 1 q e t i q e t i q t o t i where i is an iteration counter for solving nonlinearities between irrigation demand and crop consumption q t o t i 1 and q t o t i are total irrigation water diverted and or pumped for iterations i 1 and i respectively l3t 1 q e t i 1 and q e t i are the crop consumption for iterations i 1 and i respectively l3t 1 note that i also is the modflow or gsflow outer iteration counter markstrom et al 2008 niswonger et al 2011 the amount of water that is applied to each cell hru n i r r n lt 1 that is irrigated by a diversion well is 18 i r r n f f s w n q s w f f g w n q g w a n where f f s w n and f f g w n are the fractions of the total irrigation water delivery from surface water and groundwater applied to each cell hru n respectively 2 4 4 constraining surface water diversions and groundwater pumping rates diversion and pumping rates are automatically constrained by the supply of surface water at the upstream diversion point and by the water table elevation in the cell that contains the well pumping rates specified in the ag input file are used to set the groundwater irrigation rate for option 1 or they can be used to set the maximum irrigation pumping rate for options 2 and 3 additional constraints can be applied to surface water diversions for all 3 options using diversion rates specified in sfr tabfiles and 1 of 4 diversion priority options in sfr prudic et al 2004 1 demand is greater than the flow available in the upstream segment and the diversion is reduced to the amount available 2 demand is greater than flow available in the upstream segment and no water is diverted from the stream 3 demand is greater than a specified fraction of the flow in the upstream segment and the diversion is reduced to the fraction of flow 4 diversion is set equal to demand only if the remaining streamflow in the upstream segment exceeds the value specified in the sfr tabfile otherwise no water is diverted 3 description of example problems two example problems are presented to illustrate the capabilities of the ag package for simulating water use by agriculture example problem 1 is a modflow simulation that was modified from test 1 presented previously by prudic and others 2004 test problem 2 is a gsflow simulation that was modified from the sagehen creek watershed gsflow example problem markstrom et al 2008 although there is no agriculture in the sagehen creek watershed the ag package was added for this example to simulate irrigation from surface water and supplementary wells to several hrus in the lower part of the watershed that represent hypothetical agricultural fields both example problems retain the units used in their original presentations and thus example problem 1 uses english units and example problem 2 uses metric units 3 1 example problem 1 modflow with conjunctive use of surface water sw and groundwater gw etdemand option this model represents an alluvial river basin in a semi arid region the basin receives most of its precipitation in the surrounding mountains and intermittent streams drain the mountains and flow into a perennial river that crosses the southern portion of the valley fig 3 the valley aquifer consists of alluvium dominated by sand and gravel and the mountains consist of bedrock that has much lower hydraulic conductivity than the valley alluvium recharge in the basin primarily occurs as seepage loss from the intermittent stream channels and to a lesser extent as groundwater flowing to the valley from the mountain block and diffuse recharge through valley sediment prudic and others 2004 present additional details describing this test problem including representation of the stream network and distribution of recharge and et parameters used within the model niswonger and others 2006 describe modifications made to this example to replace the et and recharge packages with the uzf package excess applied infiltration and rejected infiltration spring discharge is routed to streams the model domain extends to a maximum of 520 feet below land surface in the valley bottom and extends laterally 14 miles in the north south direction and 9 5 miles in the east west direction fig 3 the model is discretized into 1 layer 15 rows and 10 columns and only model cells coincident with the basin fill are active consolidated rocks are not included layer 1 ranges in thickness between 130 feet and 520 feet model cells have a constant dimension of 5000 feet in the row and column directions a total of 3440 acres 6 model cells are irrigated for agriculture in the central part of the basin irrigation water is diverted from the green river fig 3 and pumped from the shallow aquifer beneath the fields two tributary streams that enter the model from the northwest and northeast join the mainstem in the southern part of the model fig 3 simulations included an initial steady state stress period followed by forty eight transient stress periods each stress period represents a calendar month that is divided into daily time steps the simulation begins on january 1 results are presented for the final 2 years of the simulation as the steady state stress period and first 2 years of the simulations are used to establish initial conditions hydraulic conductivity and specific yield of the water table aquifer increase in the valley bottoms that comprise of floodplains or new alluvium of the tributary streams and river monthly e t w w was specified as uzf input variable pet using annual estimates disaggregated into monthly values using average monthly temperatures prudic and herman 1996 users are referred to the input files for this problem that accompany this work for additional details two versions of example problem 1 are presented example problem 1a ep1a simulates irrigation water provided by surface water and supplementary groundwater and example problem 1b ep1b that simulates irrigation water provided solely by groundwater both models simulate irrigation demands using the etdemand approach that minimizes the et deficit using equation 17 fig 3 shows the cells designated as agricultural fields that receive irrigation sfr diversion segment number 9 was used to divert water from the green river and route it to the fields fig 3 demand is satisfied solely by groundwater in ep1b 3 2 example problem 2 gsflow conjunctive use of sw and gw etdemand verses trigger options example problem 2 was developed by modifying the gsflow sagehen example problem markstrom et al 2008 to include agricultural fields in the lower part of the basin fig 4 sagehen creek drains a 27 km2 watershed on the east slope of the northern sierra nevada geology of the sagehen creek watershed consists of granodiorite bedrock overlain by andesitic tertiary volcanic material which are overlain by till and alluvium composed of granodiorite and andesite clasts and some quaternary gravels burnett and jennings 1965 the principal aquifer model layer 2 was assumed to consist of volcanic material with thickness ranging between 50 and 300 m a veneer of alluvium covers the volcanic material that is thicker along channels in the lower section of the watershed burnett and jennings 1965 alluvium model layer 1 was assumed to range in thickness between 0 and 10 m the model domain extends laterally 6 4 km in the north south direction and 7 1 km in the east west direction fig 4 the model is discretized into 90 90 m cells using 2 layers 71 rows and 79 columns eighteen years are simulated each year is divided into 12 stress periods each period represents a calendar month and is divided into daily time steps the transient simulation begins on october 1 two versions of example problem 2 are presented example problem 2a ep2a and example problem 2b ep2b simulate demand using the etdemand and trigger options respectively fig 4 shows the cells designated as agricultural fields that receive irrigation including 34 cells irrigated by 2 segments that divert water from sagehen creek segment 18 supplies water for 14 cells and segment 19 supplies water for 20 cells fig 4 all 34 irrigated cells sum to an area to 27 5 ha irrigation can be nonzero during the growing season june 1 august 30 and zero outside the growing season these constraints on the surface water diversions for irrigation were specified using sfr tabfiles that define maximum diversion amounts for segment numbers 18 and 19 wells were placed in each agricultural cell for supplementary pumping to meet irrigation requirements 4 results 4 1 example problem 1a impacts of sw supply on supplementary gw pumping ep1a was run with high and low inflow hydrographs fig 5 representing average and drought years respectively to evaluate how differences in surface water supply impact the relative proportions of surface water and supplemental groundwater used for irrigation fig 6 specified surface water inflow enters the model through segment 1 over the northeast corner of the model boundary fig 3 maximum surface water diversions rates were set in a sfr tabfile for segment 9 with an irrigation period from april 1 to september 30 and a maximum rate of 100 ft3 s which is greater than the maximum irrigation demand and thus only the amount of flow in segment upstream of the irrigation segment will constrain irrigation option 1 soil and crop properties for ep1a are those of the fine textured soil shown in table 1 fig 6 shows the proportions of surface water and supplementary groundwater used for irrigation for the average and drought conditions for this example irrigation demand is nearly equal to crop consumption due to the values of et extinction depth saturated water content and natural rainfall supplementary groundwater makes up a greater proportion of the irrigation water supply during the low flow hydrograph 53 relative to the high flow hydrograph 42 due to surface water supply constraints fig 5 average annual irrigation water requirements were the same for both simulations 2 58 feet and slightly less than the annual average crop consumption 2 6 feet due to small amounts of precipitation in the valley supplementary pumping rates increase abruptly right as the flow at the diversion point decreases and then re equilibrate as the crop demand eta decreases similarly pumping rates decrease abruptly when the demand decreases abruptly fig 6 4 2 example problem 1b impacts of soil properties on gw irrigation demand this example problem is run for 2 different agricultural field soil types including fine and coarse soil textures table 1 the coarse soil requires greater amounts of irrigation earlier during the growing season relative to the fine soil because of lower antecedent soil moisture at the onset of the growing season fig 7 faster drainage increases the average annual irrigation demand for the coarse soil 3 1 feet relative to the fine soil 2 6 feet due to greater amounts of groundwater return flow return flow is greater for coarse soils because of lower saturated water content porosity and greater deep percolation as there is no constraint on irrigation supply average annual e t a equals the e t w w of 2 7 acre feet per acre irrigation demand is slightly less than crop consumption for fine soil due to rain fed irrigation and the larger soil storage relative to coarse soil 4 3 example problem 2a effects of crop coefficient on irrigation demands ep2a illustrates the effects of the crop coefficient k c on irrigation demand and crop consumption using the etdemand option for irrigation fig 8 k c is incorporated into the et demand by multiplying the prms input parameter jh coef by the monthly k c values note that jh ceof can be specified for each hydrologic response unit in prms and for each of the 12 calendar months markstrom et al 2015 as this option represents optimal irrigation scheduling to minimize the et deficit these results reflect optimal water use to meet crop water demand annual average giwr for the period 1991 1993 is 1 1 ha meter per hectare for high k c and 0 70 ha meter per hectare for low k c annual average crop consumption is equal to 1 06 ha meters per hectare for high k c and 0 81 ha meter per hectare for low k c actual et equals e t w w in this example because the etdemand option is used and because constraints on the irrigation amounts set in the sfr tabfile and ag pumping rates did not limit irrigation irrigation demand is less than crop consumption for 1992 and 1994 because of water supplied by precipitation and sub irrigation however 1993 was a drought year and demand is greater than consumption ep2a also demonstrates the influence that early growing season antecedent soil water conditions have on crop water demand total annual precipitation amounts measured at the independence lake climate station for water years 1991 1992 and 1993 was 83 cm 71 cm and 149 cm respectively while demand was 79 158 and 70 percent of the crop water consumption during these years for the case of high k c and 66 129 and 62 percent of the crop water consumption during these years for the case of low k c fig 9 real world irrigation practices likely cannot exactly mimic this optimal irrigation schedule for practical and logistical reasons nonetheless these model results are useful for providing guidance on irrigation schedules setting lower bounds on irrigation demand and for providing a base model for evaluating factors affecting demand and consumption irrigation constraints can be superimposed onto the etdemand option using sfr tabfiles and ag pumping rates to mimic real world conditions as will be shown in ep2b additional flexibility in simulating irrigation practices is provided by the trigger option 4 4 example problem 2b using irrigation triggers to estimate demand example problem 2b is identical to ep2a except that the trigger option is used and seasonal k c were set using the high k c curve fig 8 ep2b illustrates the influence that different irrigation trigger values have on the surface water diversions and groundwater pumping rates an irrigation event starts when the et ratio e t a e t w w becomes less than the specified trigger threshold results are shown for a high 0 85 and low 0 35 trigger value representing well watered and deficit irrigation conditions respectively note that the length of an irrigation period is specified as 3 days for both high and low trigger values however if the et ratio is below the trigger value at the end of a period then a new irrigation period will begin immediately irrigation delivery is directly proportional to the trigger value where higher trigger values result in greater surface water diversions pumping and crop water consumption fig 10 irrigation demand for the period 1991 1993 is 1 4 ha meters per hectare for a high trigger value and 0 7 ha meters per hectare for a low trigger value annual average crop consumption is the same as ep2a 1 06 ha meter per hectare except for the low trigger value simulation results in a crop consumption of 0 83 ha meter per hectare a low trigger value causes the model to delay an irrigation event because the simulated et will reduce to a lower fraction of the reference et before an irrigation event is triggered accordingly lower trigger values allow the soils to drain longer between irrigation events and lower trigger values result in less actual et as compared to higher trigger values fig 10b generally the trigger option results in significantly more surface water and groundwater irrigation withdrawal relative to the etdemand option this is because the irrigation rate is specified for the trigger option and may not be optimal for an agricultural field whereas the irrigation rate is calculated as a function of the et deficit for the demand option and reflects the optimal irrigation rate 5 discussion a new package for modflow and gsflow is presented that provides capabilities for simulating agricultural water use in regional scale hydrologic models the ag package can be used to estimate agricultural water use for systems where information about irrigation supply and demand are not available or it can be used to simulate irrigation withdrawals and their impacts on water supply the latter application is important in regions where there are competing needs for water and climate change population growth and land use change are causing unknown impacts design of the ag package includes flexibility for representing systems with varying amounts of data different grower irrigation practices and feedbacks between water supply and water use by agriculture water demands rely on energy and soil water balance calculations and regionally specific conditions can be represented such as spatial variations in temperature solar radiation and plant type specific attributes of a region can be considered including soil hydraulic properties depth to groundwater canal or pipe properties and antecedent soil moisture and precipitation water consumption relies on explicit simulation of irrigation infrastructure soil water budgets and surface water and groundwater availability these design features provide flexibility for evaluating water use in a wide variety of agricultural systems and for developing optimal irrigation schedules unique to a region however the effects of salinity stress on crops and crop water use are not represented in the ag package existing software used to simulate agricultural water use in regional hydrologic models do not provide capabilities of the ag package the modflow based package called the farm process requires monthly time steps and it does not simulate soil water balance for the estimation of irrigation withdrawals and e t a hanson et al 2014 the ag package simulates daily soil water dynamics that play an important role in determining irrigation schedules and amounts soil water balance is important for representing the rain fed component of crop consumption required for estimating irrigation withdrawals senay et al 2014 landsat derived e t a can be integrated through soil water balance into hydrology models that represent both agricultural systems and the broader regional to national hydrologic system a variety of options are provided for mimicking different irrigation practices specifically with regards to the timing and amounts of irrigation examples are presented that illustrate impacts of surface water supply on groundwater pumping ep1a irrigation supplied solely by groundwater ep1b irrigation estimated for optimal water use conditions that minimizes the et deficit ep2a and irrigation that is triggered when the et deficit drops below a specified threshold all these approaches are provided as options to best represent regionally specific conditions because irrigation water is explicitly routed and applied to individual fields the model can be used to evaluate irrigation return flows and changes in land use practical applications of integrated hydrologic models that represent agricultural water use must rely on data that characterize a broad range climactic and hydrogeologic conditions additionally representation of agriculture requires characterization of water governance and irrigation practices complete data sets are rarely available and integrated models provide a means of maximizing information with partial data sets by combining data with physical process equations and generalized frameworks for representing human impacts on water distribution and consumption the ag package for modflow and gsflow provides a powerful decision support tool that can maximize understanding of water resources in agricultural basins and provide hindcast information about historical water budgets and system response as well as future projections of sustainability and management change 6 conclusions hydrologic simulation of developed basins is difficult or impossible without representing agricultural water use integrated hydrologic models are useful decision support tools for developing regional water budgets and evaluating water management strategies and sustainability for human populations and ecosystem services despite significant data gaps in water use at regional scales hydrologic models can complement incomplete datasets and provide a more complete picture of water resources process understanding and theoretical representation of agricultural water use are well established however limited software is available that explicitly represents agricultural water use in regional scale integrated hydrologic models the ag package for modflow and gsflow provides a wholistic representation of agricultural water use in the context of the natural hydrologic system and other water use sectors through a series of simple but realistic example problems this paper demonstrates the software s applicability for a variety of approaches for simulating irrigation practices and associated effects on water distribution and supply in regional scale systems disclosure any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government declaration of competing interest there are no potential conflicts of interest for the work presented in the manuscript titled an agricultural water use package for modflow and gsflow acknowledgements support for the author was provided by the usgs water availability and use program additionally support was provided by the national science foundation grant number 1360506 and the u s department of agriculture national institute of food and agriculture grant number 1360507 the author is thankful for the thoughtful reviews provided by hedeff essaid usgs and 2 anonymous reviewers the author also thanks r steven regan usgs for his input on code implementation 
26071,as the resolution of dems is becoming higher both the efficiency and accuracy of channel head recognition are important for drainage network extraction in this paper a d8 compatible high efficient channel head recognition method is proposed for each potential channel this method first calculates a geomorphologic parameter series along the flow path and then determines the channel head by detecting the change point in the series instead of directly using one threshold value for a whole region that is still commonly used in d8 based methods the proposed method recognizes channel heads by their local geomorphology one by one the proposed method is applied with high resolution dems for different terrains and the identified channel heads and extracted drainage networks show good agreement with observations in comparison with a popular software with state of the art channel head recognition method the proposed one shows similar accuracy but much higher computational efficiency keywords channel head recognition change point detection drainage network extraction geomorphologic parameter d8 flow direction 1 introduction rivers are the largest watercourses through which energy and materials are transported drainage networks and associated geomorphologic parameters are the basic elements of geomorphologic analyses environmental assessments and hydrologic simulations moore et al 1991 giannoni et al 2005 gandolfi and bischetti 2015 pilotti et al 2019 qiao et al 2019 therefore the accuracy of drainage networks is crucial to the reliability and validity of corresponding research band 1986 clarke et al 2008 as a key factor of drainage networks precise channel head recognition is of great importance for drainage network extraction first physically based hydrologic and soil erosion models must distinguish between overland runoff on hillslopes and water flow in channels and use accurate channel heads to depict real scale hillslope channel units and corresponding parameters for model simulation julien et al 1995 li et al 2009 2011 shi et al 2014 lin et al 2018 second the location of channel heads has a high impact on the morphometric and scaling properties of a drainage network bertolo 2000 gangodagamage et al 2011 bennett and liu 2016 such as drainage density length of drainage paths and horton s ratios with the increasing resolution and precision of digital elevation model dem data or digital terrain model dtm drainage networks and channel features extracted from them are becoming more and more popular in representing river system zheng et al 2019 wu et al 2019 but it brings two challenges for drainage network extraction one is the increasing computation cost requires more efficient extraction algorithm the other is the correct reflection of detailed geographical characteristics contained in dems among the state of the art drainage network extraction algorithms and softwares the recognition of channel heads has been emphasized and improved e g the geonet passalacqua et al 2010 sangireddy et al 2016 however effective and efficient channel head recognition is still a challenging problem in drainage network extraction where more attention is needed the existing drainage network extraction approaches can be categorized into two broad groups the first group mainly uses comprehensive topographic and geomorphologic features to recognize river valleys and ridge lines yoeli 1984 band 1986 passalacqua et al 2010 2012 matsuura and aniya 2012 pelletier 2013 koenders et al 2014 xiong et al 2014 sangireddy et al 2016 the second group mainly relies on flow direction calculations on dem cells to simulate flow convergence and to determine drainage networks accordingly o callaghan and mark 1984 tarboton et al 1991 tarboton 1997 martz and garbrecht 1999 turcotte et al 2001 nobre et al 2011 choi 2012 bai et al 2015 in both of these approaches channel head recognition is one of the most challenging aspects and has important effects on the accuracy of the extracted drainage networks an enormous amount of effort has been devoted to utilizing the topographic and geomorphologic features over a river basin to determine channel heads tarboton et al 1988 montgomery and dietrich 1988 1992 dietrich et al 1993 passalacqua et al 2010 petroff et al 2011 julian et al 2012 liu et al 2014 sangireddy et al 2016 the techniques relevant to channel head recognition can also be classified into two categories 1 geometric techniques e g passalacqua et al 2010 sangireddy et al 2016 and 2 process based techniques that search the characteristic point along the longitudinal profile of each flow path from hillslope to channel e g montgomery and dietrich 1988 1992 geonet passalacqua et al 2010 sangireddy et al 2016 is a well developed and representative open source software using geometric techniques it extracts channel heads and channel network based on the geodesic minimization principles possible channelized pixels are identified by computing a new cost function passalacqua et al 2012 at each location of the landscape and channel heads are detected by scanning the skeleton of the drainage network connected possible channelized pixels without any breaks with a search box sangireddy et al 2016 proposed setting this search box size equal to the median hillslope length computed on the input dem a dynamic method instead of a user defined parameter a fixed threshold previously passalacqua and foufoula georgiou 2015 and obtained improved river network output that is more in line with the actual density because of its complex geometric based channel head recognition algorithm and the loosely coupled program design the extraction efficiency is not high the process based techniques use a threshold called critical source area csa the slope area relationship giannoni et al 2005 hancock and evans 2006 or the elevation chi relationship perron and royden 2012 clubb et al 2014 to distinguish channels from hillslopes among the process based techniques the csa method still dominates in large scale drainage network extraction from dem data because of its simple form and low computational cost the esri arc hydro tools has been developed for more than 20 years and is well known and widely used by scholars and engineers in the hydrological field but it still uses the csa method to detect the location of the channel heads djokic et al 2011 in the csa method a single csa value is used to determine each channel s origin over a whole river basin o callaghan and mark 1984 jenson and domingue 1988 the csa method performs well when a river basin s geomorphologic and erosional characteristics are homogeneous or when only lower resolution dems are used however a single csa value cannot reflect different source areas of real channel heads that caused by non homogeneity in a river basin and consequently the csa method is unable to utilize detailed information supplied from high resolution dem data to identify the real and non uniformly distributed channel heads to overcome the limitation of the csa method the slope area relationship method was proposed montgomery and dietrich 1992 in this method the local slope s was introduced into the csa method to supplement the source area a based on the threshold criterion observed by montgomery and dietrich 1992 as c as 2 a more general threshold calculated as c as k or its equivalent forms where k varies is used to determine channel heads giannoni et al 2005 hancock and evans 2006 jefferson and mcgee 2013 based on this criterion channel heads are recognized when c exceeds a threshold value however the difficulty encountered in the determination of the threshold c and the value of k as it is not as intuitive as a csa value hampers the application of this method meanwhile tarboton and ames 2001 pointed out that this algorithm may result in feathering of the drainage network in steeper areas while omitting drainage networks in less steep valleys clubb et al 2014 proposed the drainage extraction by identifying channel heads dreich method using the elevation chi relationship and reported its better performance over geometric techniques however the dreich method introduces a new parameter that is sensitive to drainage density results and the procedure for calculation is also more complicated than the above mentioned threshold methods with the development of remote sensing technology the global high resolution dems become easy to get we only had 30 m resolution dems like srtm farr et al 2007 and aster gdem nasa meti aist japan spacesystems and u s japan aster science team 2009 ten years ago and now we have 12 m resolution worlddem riegler et al 2015 and 5 m resolution aw3d global dsm data takaku et al 2016 as dems with higher and higher resolutions are available both the efficiency and accuracy of channel head recognition become more and more important in river network extraction algorithms and softwares the popular esri arc hydro tools still uses the csa method to decide channel heads because of its high efficiency many scholars have proposed new accurate algorithms such as the geometric based method proposed by the geonet team but their computational efficiency is not compatible with the simple threshold method like csa inherit the efficiency of the d8 algorithm tarboton 1997 we propose a d8 compatible high efficient channel head recognition method attempting to meet the challenge in both of the two aspects in drainage network extraction the objective of this paper is to develop a dynamic channel head recognition method that detects geomorphic change points without a direct threshold like csa and also promises low computational cost of the detection this proposed method is then incorporated into a d8 based efficient and comprehensive model for drainage network extraction from dem with billions of pixels bai et al 2015 the effectiveness of the method is tested through its application to two river basins with significantly different landscape properties 1 the maple run river basin in the eastern valley in maryland the united states and 2 a watershed in the huangfuchuan basin a sub basin of the yellow river basin located on the loess plateau in china the remainder of the paper is organized as follows section 2 introduces the framework of the d8 compatible channel head recognition method two main procedures of the proposed method to recognize each channel head and the detection strategy to traverse in a drainage network section 3 introduces the two study areas the maple run river basin and the watershed in huangfuchuan river basin section 4 presents the results a detailed discussion on the sensitivity of model parameters of this method is made in section 5 finally section 6 draws some conclusions 2 methods 2 1 general framework of the channel head recognition method raster cell is the basic unit in the extraction of drainage network from dem data the size of the regularly spaced cells is defined as the resolution of dem data as fig 1 shows such a cell p has several geographic attributes a p is the upstream source area of cell p s p is the local slope of cell p calculated by the 3rd finite difference method sharpnack and akin 1969 and l p is the accumulative length of the flow path from the watershed divide to cell p there is always a flow path passingthrough cell p if the position of channel head is located at cell p then one can get a channel beginning from this cell p a detection window centered at cell p is also shown in fig 1 which will be discussed in section 2 3 2 the csa method relies on the source area of a cell to determine whether the cell belongs to a channel or not o callaghan and mark 1984 jenson and domingue 1988 the area slope method considers the local slope and the source area of a cell to calculate the c value to distinguish channels from hillslopes montgomery and dietrich 1992 giannoni et al 2005 such methods however evaluate for each time the critical thresholds of channels on a single cell only to overcome the limitation of binary classification that is judged on each single dem cell a new recognition method is proposed in this study the method uses the change point detection technique in a sub window of the data series of some geographical characteristic function values which are calculated cell by cell in dem along flow paths a cell p becomes the channel head when it is at the corresponding dem cell of the change point position of the detection as an improvement this channel head detection method determines each channel head according to the change point of a data series along the related flow path by which the geomorphologic characteristics along each flow path can be explored the flow paths used here are identified during the whole drainage network extraction procedure when the dem cells have been queued by elevation and flow direction there are mainly two key points for the proposed geomorphologic change point method for channel head recognition one is the selection of a proper geomorphologic characteristic function the other is the strategy to implement change point detection the inverse relationship between local slope s and catchment area a presented as as k c where c was found to be in a compact range following a number of measured channel head points e g montgomery and dietrich 1992 moreover the value of as k varies along the flow path crossing a channel head it can be used as a geomorphologic characteristic function for channel head recognition however if we observe the as k value along a flow path from the ridge through hillslope across the channel head and then into channels it keeps a growing trend montgomery and dietrich 1992 hancock and evans 2006 which is found unsuitable for change point detection in many situations after our trials the new geomorphologic characteristic function that we proposed and tested will be introduced in section 2 3 1 the implementation framework of the proposed method is shown in fig 2 the key steps will be presented throughout the remaining part of this section listed as below 1 the obtained flow paths are first screened by the planar shape of the contributing area to select potential channels see flow path screening in fig 2 as introduced in section 2 2 2 the change point detection process to identify each single channel head including dynamic sub series and change point detection in fig 2 will be introduced in detail in section 2 3 3 the ergodic strategy of iterative channel head detection to traverse the treelike structure of a whole river basin see the loop before all flow paths detected will be illustrated in section 2 4 2 2 flow path screening there are some common rules governing a natural river basin s form for example the basin area is generally proportional to the square of the main stem length which yields the width length ratio of river basins and catchments at different scales falling within a certain range this phenomenon reflects the planar shape of the source areas of channels and its quantitative relationship has been intensively studied montgomery and dietrich 1992 concluded the relationship between channel length l and the corresponding catchment area a as l 3a 0 5 which indicates the width length ratio a l 2 1 3 although this relationship about the planar shape cannot be used for channel head recognition it provides a useful foundation to screen out flow paths on hillslopes from potential channels because the processes on hillslopes generally produce parallel flow paths the planar shape of the contributing area of those flow paths is much narrower than that of channels which means a l 2 1 3 this is the most primary rule for flow path screening to reduce computational cost in the following steps and it is proposed as 1 a l 2 α where α means the critical value of the width length ratio a and l are calculated by the kernel algorithm of drainage network extraction bai et al 2015 following the result by montgomery and dietrich 1992 α should be less than 0 33 to retain most potential channels the value of α is suggested to be smaller to avoid excessive computational consumption and possible misjudgment in the subsequent steps the value of α should not be too small our analysis suggests that changing the value of α from 0 1 to 0 3 does not significantly influence the outcome see the discussion section and fig 13 therefore we believe a value of 0 2 would be more appropriate but for steep and erosive basins where catchment a l 2 tends to be small a smaller α value can reduce the omissions of channels it is worth noting that because of the square shape of the dem cells several cells near ridges will always be misjudged by equation 1 for example the a l 2 value of the first cell of a flow path is always 1 and for a one cell width straight flow path that value of the fifth cell equals 0 2 therefore the first 1 α cells from the beginning of each flow path will not be examined by equation 1 when a normal source area exists upstream of a dem cell i e this cell satisfies equation 1 then the whole flow path passing through this cell will be reserved for the next steps otherwise when any normal source area cannot be found before a flow path converges with a higher order one e g with a larger source area then the dem cells along this flow path should be on a hillslope and will be screened out fig 3 shows the effect of this screening the thick lines are the reserved flow paths where a potential channel head exists in each of them the light lines represent flow paths on hillslopes and the corresponding dem cells are screened out in this step parameter α controls the efficiency of the flow path screening process in fig 3 the dem resolution is 1 m and the value of α equals to 0 2 it can be observed that a large portion of flow paths has been screened out most of them representing parallel overland flow the reserved flow paths are still far more than the real channels and will be distinguished in the flowing steps 2 3 change point detection a channel generally develops from a gully where overland flow concentrates to cause sufficient erosion to form and maintain the channel upstream of the channel head first the slope is very mild on the top of a hillslope and theoretically equals zero on the ridge as additional water gathers further down a ridge the main part of the hillslope has medium slopes generally slope increases with drainage area when hillslope processes are dominant roering et al 2007 when channel head appears near the bottom of the hillslope the profile curvature reaches its maximum and commonly there is a sudden appearance of steeper slope which would be represented by high resolution dem data in the direction downstream of the channel channel slope will be milder following the power law function of s k s a θ flint 1974 and the appearance of abrupt steep slopes would be less notable in other words slope is a key geomorphologic parameter in the detection of channel head like the slope area relationship method slope must also be incorporated in channel head detection in our proposed method however the form of the geomorphologic characteristic function used for change point detection should be carefully studied below we explain how our methodology addresses the mechanism 2 3 1 geomorphologic characteristic function the geomorphologic characteristic function f of a flow path should be able to distinguish a normal channel from a non channel flow path following up on the existing studies e g montgomery and dietrich 1988 1989 1992 as 2 is our first consideration using the field observed channel head positions julian et al 2012 as an example we plot the as 2 values over the normalized distance from watershed divides defined as zero and passing through the channel heads defined as one there are 73 data series shown together as a scatter plot in fig 4 a and a box plot in fig 4 b as 2 keeps a growing trend along the flow path from the ridge through hillslope into channels which is not suitable for change point detection the geographical characteristic function used in this paper is selected by the principle that it suits for change point detection along each flow path after examining different forms of the combination of s and a the characteristic function f is proposed as 2 f s ln a through fig 4 c and d we can see that the s ln a value has a steady stage upstream of the channel head and decreases rapidly downstream of the channel head this indicates that there are change points in the flow paths and these change points are the channel heads we are looking for for change point detection here we use the method proposed by pettitt 1979 the position and length of the detected data series are important factors in determining whether and where a change point exists therefore in section 2 3 2 we purpose a dynamic sub series window method to select the detected series 2 3 2 dynamic sub series window a geomorphic characteristic function works reasonably well in many situations e g the as 2 by montgomery and dietrich 1992 however it must also be noted that the flow paths where different channel heads are located vary dramatically according to nearby terrain and can have different function values for instance as shown in fig 5 the flow path length of channel head rv76 is longer than the length of channel head rv75 from the ridge to downstream confluence point i e a b and c d moreover more than one change point may exist on a flow path as a result the change point detection of a geomorphic characteristic function must be based on an appropriate flow path sub series to guarantee the validity and accuracy of the change points detected the following dynamic sub series window method will help us to select the right detection sub series the data series of the geographical characteristic function f along different flow paths is comprised of data points on each dem cell and the data series length is controlled by local topography and can vary dramatically from channel to channel as shown in fig 5 as a result the length of the detection sub series cannot be defined as a constant following the relationship as 2 c montgomery and dietrich 1992 adopted in the area slope threshold methods we use such a relationship to control the sub series for change point detection instead of the threshold c we use a parameter β to locate the center position most possible change point of the change point detection series as 3 as 2 β where β is the detection window control parameter in m2 the role of β is to guarantee that the potential channel head is contained in the sub series for change point detection rather than to determine the channel head directly in this paper we recommend the value of β between 400 m2 and 900 m2 which is close to the lower limit suggested by montgomery and dietrich 1992 the sensitivity analysis of parameter β is carried out in the discussion section through the condition of equation 3 we can find the possible positions of the channel heads and set them as the midpoints of their s ln a sub series i e the detection windows as stated in section 2 2 the first 1 α pixels will not be examined by equation 1 similarly we cut off the first 1 4 of the data series from the ridge to the potential point found by the as 2 value to avoid including the first peak caused by small a values into the data series then we extend 3 4 length of the data series downstream the potential point to make it center and obtain the full detection series the strategy is shown in fig 6 two different detection cases of rv75 and rv76 are shown in fig 7 including their as 2 and s ln a series the field observed channel heads the detection windows and the detected change points in fig 7 c and d the value of β equals to 400 m2 that is when the as 2 value exceeds 400 m2 for the first time the data point is selected as the midpoint shown by the red dot dashed lines of the detection window shown by the light red background color for the two channel heads respectively the red vertical dashed lines are the detected change points the p values of pettitt s approach to the change points are far less than 0 01 6 24e 07 and 2 49e 05 respectively which means statistical significance in fig 7 e and f the β value of the as 2 condition equals 900 m2 similarly the midpoints shown by the blue dot dashed lines the detection windows shown by the light blue background color and the detected change points shown by the blue vertical dashed lines are obtained for these two cases the change of β value is found to affect the detection windows however within the β range considered here the detected change points are not affected and the change points agree well with the field observed channel heads this phenomenon clearly indicates the robustness of the proposed method relative to the as 2 threshold method 2 4 detection strategy there are three important assumptions that govern the detection process to search through a river basin first for a certain subbasin branch the cell with the largest drainage area is its outlet and the main stem can be found by searching for more dominant cells upstream second if no channel head is detected along a main stem then no other channel head is considered in this branch third all the downstream cells of a detected channel head are considered as channel cells after flow path screening firstly the main stem of a whole river basin is identified fig 8 a secondly the channel head of the main stem is determined by change point detection fig 8 b thirdly the above third assumption is used to mark the downstream cells as channel until it meets another potential channel branch flowing into the main stem fig 8 c then this confluence cell is considered as the outlet of the branch and the channel head detection process is reinitiated for this branch fig 8 d these operations are recursively repeated until all potential channel heads are identified and all cells in the channel have been defined when the basin outlet is reached fig 8 e 3 study areas and data the proposed methodology is applied to two different watersheds 1 the maple run river in the united states and 2 the huangfuchuan river in china some information about these study areas and the data used are presented here the maple run watershed is located in the eastern valley in maryland united states fig 9 a it is a tributary of the town creek river which locates between the mountainous regions and the relatively flat coastal plain this region is characterized by irregular plains and moderate relief with greater amounts of pine and mixed forests and has thick clay rich soils underlain by deeply weathered bedrock and relatively few solid outcrops julian et al 2012 the mean annual precipitation in this basin is 1084 1159 mm for the period 1971 2000 according to the prism dataset daly et al 1997 and the 30 min rainfall intensity with a recurrence interval of 2 years is 29 30 mm according to noaa pfds julian et al 2012 the general river network data for this basin shown in fig 9 a was obtained from the nhdplus high resolution beta dataset available online at https viewer nationalmap gov basic bare earth dem data with 1 9 arc second approximately 3 m resolution was obtained from the national elevation dataset available online at https viewer nationalmap gov basic the 73 field observed channel heads fig 9 b were obtained from julian et al 2012 and the related region covers an area of about 9 14 km2 which means the spatial density of channel heads is about 8 km 2 the huangfuchuan river is a tributary of the yellow river in china at the northern edge of the loess plateau the mean annual precipitation in huangfuchuan river basin is 363 388 mm for the period 1956 2002 according to the local climate stations and about 80 of the total precipitation falls between june and september sui et al 2008 because of dry weather only sparse vegetation grows in this region the loess rich flat high lands here are severely incised by gullies and low relief channel valleys fig 9 d because of intensive erosions in this study an upstream watershed in the huangfuchuan river basin is selected for analysis this watershed is located in jungar banner inner mongolia fig 9 c and covers an area of 2 46 km2 dem data with 1 m resolution was obtained from aerial geodetic survey carried out by the yellow river conservancy commission because of the large number of channel heads a careful field survey in the huangfuchuan watershed is costly we visually recognized the channel heads on remote sensing images 0 4 m spatial resolution from geoeye accessed from google earth and 294 channel heads were found in the upstream huangfuchuan watershed see the green and red dots in fig 12 then the spatial density of channel heads is about 120 km 2 much larger than that in the maple run moreover the channel head density varies intensively in spatial in this watershed the geographic and climatic conditions in the maple run river basin and the huangfuchuan river basin are quite different from each other and the dem resolutions are also different 3 m and 1 m therefore they can be used to test the compatibility of the proposed method with different geomorphology and data what s more we acknowledge that even 1 m resolution may not be sufficient for channel head identification but for river network extraction from dem this is a common high resolution and the results can meet the needs of hydrological modelling and water management 4 results for the two study areas in this study the procedure for the implementation of the method is essentially the same the proposed methods in this paper have been integrated into the drainage network extraction program first developed by bai et al 2015 which was coded by the c language besides in order to evaluate the effectiveness and efficiency of our proposed method we compared the results with the geonet software passalacqua et al 2010 sangireddy et al 2016 the geonet software was coded by the python language and the source code can be downloaded from https github com passah2o geonet for the maple run river the extracted channel heads are compared with the surveyed ones to calculate the reliability and sensitivity indices and also compared with the geonet results moreover the proposed β values from 400 m2 to 2500 m2 with intervals of 100 m2 are also considered to carry out a sensitivity test for the huangfuchuan river the extracted channel heads are only compared with those visually identified on high resolution remote sensing images 4 1 maple run river because of the available field observed channel head data in this case the evaluation of extracted channel heads is rigorously carried out here the results of channel head locations can be divided into three classes orlandini et al 2011 true positive tp false positive fp and false negative fn fig 10 a which is implemented following the method proposed by clubb et al 2014 a tp means an observed channel head is successfully found through the channel head recognition method an fp means the method recognizes a non existent channel head and an fn means the method fails to recognize an observed channel head for the accuracy criteria a tp is defined when an observed channel head can be found in the corresponding extracted first horton strahler order contributing area according to the point assessment results two quantitative indices can be calculated and evaluated orlandini et al 2011 the reliability of the result can be defined as r σtp σtp σfp where σtp and σfp are the total numbers of true and false positives respectively and the sum of them means the total number of the recognized channel heads the sensitivity of the result can be defined as s σtp σtp σfn where σtp and σfn are the total numbers of true positives and false negatives respectively and the sum of them means the total number of observed channel heads fig 10 shows the extracted drainage network and evaluation indices obtained using the proposed method the results presented in fig 10 a correspond to the parameter β 600 m2 in this map the sensitivity index r equals 0 904 which means that 90 4 of the field observed channel heads have been successfully recognized the reliability index equals 0 452 which is a bit low this is partially caused by the stricter standard of the channel head field investigation relative to drainage network extraction from dem data fig 10 b shows the frequency of position errors between predicted and field observed channel heads for true positives in fig 10 a the mean and standard deviation of the horizontal distance between predicted and field observed channel heads are found to be 56 m less than 20 dem cells and 104 m respectively fig 10 c shows the response of evaluation indices r and s to parameter β with an increase in parameter β sensitivity s decreases and reliability r increases generally when parameter β varies from 400 m2 to 900 m2 sensitivity s varies from 0 918 to 0 781 which is very high for the selection of optimal parameters better sensitivity generally comes with worsening reliability and vice versa the trade off between sensitivity and reliability can be determined depending on the purpose of the application of extracted drainage networks for hydrological simulation purposes a smaller β will lead to more channel heads and then higher sensitivity and will produce higher but still adequate resolution drainage network to evaluate the overall performance of channel head recognition methods the sum of sensitivity and reliability indices would be useful the sum of s and r of the proposed method for the maple run river is found to be in the range of 1 180 1 356 when β varies from 400 m2 to 2500 m2 in a similar work done by clubb et al 2014 the sum of s and r for three river basins using three different methods varied from 1 06 to 1 32 from this perspective it can be inferred that the method proposed in the present study performs slightly better than this existing method furthermore the model performance is less sensitive with its parameter in a large range because the parameter β only controls the position and length of the change point detection window instead of determines channel heads directly which is one of the key innovations of this paper for comparison we applied the geonet software for the same study area and we altered its single parameter flowthresholdforskeleton short for tfs below from 10 to 1000 then we evaluated the reliability r sensitivity s and the sum of them for the results from geonet software and the calculation times for both the methods the results are compared in table 1 from table 1 we can find out that the geonet software recognized fewer channel heads and the extracted network was sparser as shown in fig 11 with tfs 200 for the largest r s when the parameter tfs varied from 10 to 1000 the reliability r increased from 0 449 to 0 778 sensitivity s decreased from 0 548 to 0 288 and r s varied in the range from 0 997 to 1 066 in contrast when the reliability r obtained by the proposed method varied in a similar range from 0 396 to 0 769 the maximum seneitivity s reached higher than 0 9 and r s varied in the range from 1 180 to 1 356 which proved that the proposed method was more effective moreover the calculation time of the proposed method was less than 10s obviously shorter than that of the geonet software this is partly because geonet builds a loosely coupled python program that separates dem preprocessing flow direction judgment channel head recognition channel extraction and vectorization of the river network independently and each part generates intermediate files for coupling which results in frequent file reading and writing while the proposed program combines all the above steps using a d8 based unified data structure and is implemented by using the c language however when those methods are being used for much larger river basins the d8 compatibility of the proposed method as well as the ideal o nlogn computing efficiency of d8 algorithm will provide higher scalability 4 2 huangfuchuan river despite different dem resolutions and landscapes in the huangfuchuan basin and the maple run river basin the same model parameters of β 600 m2 and α 0 2 were adopted for the huangfuchuan basin as well to investigate the stability of model performance fig 12 shows the extracted streams blue lines and the corresponding image on google earth from the image of this watershed we can see that because of the highly erosive loess there are many new channel heads under development the challenge here would be achieving high sensitivity to recognize those developing channel heads a total of 402 channel heads were extracted for the study area with model parameters of β 600 m2 and α 0 2 by comparing the extracted streams with the channel heads that visually recognized observed on remotely sensed images we can find out that 207 extracted channel heads tp see the green dots in fig 12 fit well with those observed with position deviations less than 20 m and 87 observed channel heads fn see the red dots in fig 12 are missed in the extraction this means that 51 5 207 in 402 of the identified channel heads are necessary and accurate i e reliability r equals 0 515 and 70 4 207 in 294 of the observed channel heads have been extracted i e sensitivity s equals 0 704 the sum of s and r equals 1 219 also suggests a promising precision level 4 3 remarks the results presented above for the two river basins indicate that the use of the same parameters yields different reliability and sensitivity of model s results a higher reliability r but a lower sensitivity s i e relatively lower channel head density than observed was obtained in the huangfuchuan watershed compared with those in the maple run river basin from table 1 we can see that similar r and s results can be obtained in the maple run river basin with larger β between 600 and 1600 m2 the main reason for this is most likely the difference in landscape those newly developed channel heads along the main channles in the upstream huangfuchuan watershed see fig 12 are too dense to be fully recognized though the dem used here has higher resolution therefore for different terrain and dem resolution we can adjust the β value within a certain range to obtain different results a preference between reliability and sensitivity while maintaining the overall accuracy r s at a reasonable level 5 discussion in the drainage network extraction for the maple run river basin fig 10 the value of α was chosen as 0 2 and the best value of β was found as 600 m2 when r s reached the maximum the results demonstrate that channel head accuracy is satisfiable with such proper parameter values moreover we also changed the value of parameter α to examine if the detection results are sensitive to it fig 13 the parameter α means the critical value of the width length ratio and is used to screen out flow paths on hillslopes from potential channels when α is bigger than a certain value some true channels would be screened out and the corresponding channel heads will be missed leading to a lower sensitivity however it won t influence the location of channel heads that can be recognized on the other hand when α is smaller than a certain value fewer flow paths on hillslopes are screened out then more change point calculations are needed and some fake channel heads would possibly be detected on those hillslope flow paths which will lead to a lower reliability value from fig 13 we can observe that this speculated phenomenon happened with very slight performance difference when α varied between 0 1 and 0 3 the most important parameter put forward in this paper is β which determines the position and length of the change point detection window for the geomorphological characteristics function we applied different values for the parameter β between 400 m2 and 2500 m2 in the extraction of the drainage network in the maple run river basin and evaluated model performance using sensitivity s and reliability r as well as their sum see fig 10 c through the presentation of the r versus s plots varying with parameter β one can observe the reliability and sensitivity characteristics of the model the fact that the reliability index r increases stepwise and smoothly with the increase in parameter β suggests that one can easily control the ratio of false positives fps an important issue in channel network extraction further the observation that the sensitivity index s decreases quickly in the beginning and then slowly as the parameter β increases suggest that one can get a satisfactory result with a small ratio of false negatives fns when parameter β is controlled within an upper limit finally the intersection of r and s means a balance of the parameter β and a higher intersection point means a better recognition method in other words higher and stable values of the sum of r and s under changing model parameters can prove the robustness of a channel head recognition method as dems with higher and higher resolutions are available the proposed method makes full use of the geomorphologic features brought by the high resolution dem data the present results suggest that the geomorphic change point detection method can effectively and reliably recognize the locations of channel heads moreover the method is robust enough that the results remain stable when the parameters change within a certain range of values on the other hand the proposed algorithm has significant computational efficiency advantages as shown in table 1 compared with geonet software the extraction program in this paper only uses less than 10s when geonet uses more than 100s for the same basin and compared with the traditional threshold method such as csa this algorithm does not increase too much calculation time the kernel part of the proposed method is a rank based change point detection algorithm in the detection window with a limited size so its computational complexity will not exceed o n and the computational complexity of the whole d8 based extraction process is o nlogn bai et al 2015 thanks to the efficiency of the d8 based algorithm little time will be spent on detecting channel heads among the total computational cost a dem with about 100 million pixels was used to test the computational cost the total calculation time is 3 min 40 s among which only 8 s less than 4 was taken up by channel head recognition 6 conclusions this study has proposed a d8 compatible high efficient channel head recognition method based on change point detection of a geographical characteristic function in place of a designated threshold value or complex calculations non uniformly distributed channel heads and then more accurate river networks can be extracted from high resolution dems using this method the results also demonstrate the response of performance to the method s parameters which shows a large acceptable parameter range and suggests the trade off between sensitivity and reliability the proposed method can be used to obtain more accurate drainage networks than using the widely used esri arc hydro tools moreover it is more efficient when compared to popular geonet software with similar channel head accuracy besides the advantages of the proposed method there are still some limitations in its application first the resolution of dem data must be high enough to ensure accurate channel head results second application in this paper doesn t consider errors caused by imperfections in dem data so the recognized results are just responsible for the dem data used in this regard more dem data with higher resolution and accuracy are likely to be available worldwide in the future as new satellite remote sensing techniques are being developed therefore the proposed method will become even more appropriate and computational feasible when compared to those complex geomorphological methods in recognizing channel heads in addition the proposed algorithm and main parameters are only tested in two small cases the maple run river basin with gentle slope and the loess erosion controlled huangfuchuan river basin and are only compared with the geonet software we cannot prove that for other terrains especially for mountainous and steep slope regions similar results can be obtained and we may ignore other effective and efficient algorithms that could provide reliable results we have put our source code on github everyone is welcomed to test and compare this method for more cases and communicate with us and we will continue to improve our work moreover the proposed method only considers the accuracy of channel heads but a complete drainage network extraction procedure includes multiple steps that affect the accuracy of the whole network in this regard a high efficient flow enforcement method was also proposed by us to improve the accuracy of extracted main streams using mapped streamlines or remote sensed water surfaces wu et al 2019 which is also d8 compatible therefore a comprehensive d8 compatible high efficient and high accuracy drainage network extraction program can be developed by combining these two methods in the future software availability the channel head recognition method using geomorphologic change point detection has been integrated into a d8 based drainage network extraction program the source code is freely available on github at https github com thuleef dne declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the national key research and development program of china 2016yfe0201900 2017yfc0403600 the national natural science foundation of china 91547204 51579131 51569026 and the state key laboratory of hydroscience and engineering of china 2017 ky 04 any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the authors or funders 
26071,as the resolution of dems is becoming higher both the efficiency and accuracy of channel head recognition are important for drainage network extraction in this paper a d8 compatible high efficient channel head recognition method is proposed for each potential channel this method first calculates a geomorphologic parameter series along the flow path and then determines the channel head by detecting the change point in the series instead of directly using one threshold value for a whole region that is still commonly used in d8 based methods the proposed method recognizes channel heads by their local geomorphology one by one the proposed method is applied with high resolution dems for different terrains and the identified channel heads and extracted drainage networks show good agreement with observations in comparison with a popular software with state of the art channel head recognition method the proposed one shows similar accuracy but much higher computational efficiency keywords channel head recognition change point detection drainage network extraction geomorphologic parameter d8 flow direction 1 introduction rivers are the largest watercourses through which energy and materials are transported drainage networks and associated geomorphologic parameters are the basic elements of geomorphologic analyses environmental assessments and hydrologic simulations moore et al 1991 giannoni et al 2005 gandolfi and bischetti 2015 pilotti et al 2019 qiao et al 2019 therefore the accuracy of drainage networks is crucial to the reliability and validity of corresponding research band 1986 clarke et al 2008 as a key factor of drainage networks precise channel head recognition is of great importance for drainage network extraction first physically based hydrologic and soil erosion models must distinguish between overland runoff on hillslopes and water flow in channels and use accurate channel heads to depict real scale hillslope channel units and corresponding parameters for model simulation julien et al 1995 li et al 2009 2011 shi et al 2014 lin et al 2018 second the location of channel heads has a high impact on the morphometric and scaling properties of a drainage network bertolo 2000 gangodagamage et al 2011 bennett and liu 2016 such as drainage density length of drainage paths and horton s ratios with the increasing resolution and precision of digital elevation model dem data or digital terrain model dtm drainage networks and channel features extracted from them are becoming more and more popular in representing river system zheng et al 2019 wu et al 2019 but it brings two challenges for drainage network extraction one is the increasing computation cost requires more efficient extraction algorithm the other is the correct reflection of detailed geographical characteristics contained in dems among the state of the art drainage network extraction algorithms and softwares the recognition of channel heads has been emphasized and improved e g the geonet passalacqua et al 2010 sangireddy et al 2016 however effective and efficient channel head recognition is still a challenging problem in drainage network extraction where more attention is needed the existing drainage network extraction approaches can be categorized into two broad groups the first group mainly uses comprehensive topographic and geomorphologic features to recognize river valleys and ridge lines yoeli 1984 band 1986 passalacqua et al 2010 2012 matsuura and aniya 2012 pelletier 2013 koenders et al 2014 xiong et al 2014 sangireddy et al 2016 the second group mainly relies on flow direction calculations on dem cells to simulate flow convergence and to determine drainage networks accordingly o callaghan and mark 1984 tarboton et al 1991 tarboton 1997 martz and garbrecht 1999 turcotte et al 2001 nobre et al 2011 choi 2012 bai et al 2015 in both of these approaches channel head recognition is one of the most challenging aspects and has important effects on the accuracy of the extracted drainage networks an enormous amount of effort has been devoted to utilizing the topographic and geomorphologic features over a river basin to determine channel heads tarboton et al 1988 montgomery and dietrich 1988 1992 dietrich et al 1993 passalacqua et al 2010 petroff et al 2011 julian et al 2012 liu et al 2014 sangireddy et al 2016 the techniques relevant to channel head recognition can also be classified into two categories 1 geometric techniques e g passalacqua et al 2010 sangireddy et al 2016 and 2 process based techniques that search the characteristic point along the longitudinal profile of each flow path from hillslope to channel e g montgomery and dietrich 1988 1992 geonet passalacqua et al 2010 sangireddy et al 2016 is a well developed and representative open source software using geometric techniques it extracts channel heads and channel network based on the geodesic minimization principles possible channelized pixels are identified by computing a new cost function passalacqua et al 2012 at each location of the landscape and channel heads are detected by scanning the skeleton of the drainage network connected possible channelized pixels without any breaks with a search box sangireddy et al 2016 proposed setting this search box size equal to the median hillslope length computed on the input dem a dynamic method instead of a user defined parameter a fixed threshold previously passalacqua and foufoula georgiou 2015 and obtained improved river network output that is more in line with the actual density because of its complex geometric based channel head recognition algorithm and the loosely coupled program design the extraction efficiency is not high the process based techniques use a threshold called critical source area csa the slope area relationship giannoni et al 2005 hancock and evans 2006 or the elevation chi relationship perron and royden 2012 clubb et al 2014 to distinguish channels from hillslopes among the process based techniques the csa method still dominates in large scale drainage network extraction from dem data because of its simple form and low computational cost the esri arc hydro tools has been developed for more than 20 years and is well known and widely used by scholars and engineers in the hydrological field but it still uses the csa method to detect the location of the channel heads djokic et al 2011 in the csa method a single csa value is used to determine each channel s origin over a whole river basin o callaghan and mark 1984 jenson and domingue 1988 the csa method performs well when a river basin s geomorphologic and erosional characteristics are homogeneous or when only lower resolution dems are used however a single csa value cannot reflect different source areas of real channel heads that caused by non homogeneity in a river basin and consequently the csa method is unable to utilize detailed information supplied from high resolution dem data to identify the real and non uniformly distributed channel heads to overcome the limitation of the csa method the slope area relationship method was proposed montgomery and dietrich 1992 in this method the local slope s was introduced into the csa method to supplement the source area a based on the threshold criterion observed by montgomery and dietrich 1992 as c as 2 a more general threshold calculated as c as k or its equivalent forms where k varies is used to determine channel heads giannoni et al 2005 hancock and evans 2006 jefferson and mcgee 2013 based on this criterion channel heads are recognized when c exceeds a threshold value however the difficulty encountered in the determination of the threshold c and the value of k as it is not as intuitive as a csa value hampers the application of this method meanwhile tarboton and ames 2001 pointed out that this algorithm may result in feathering of the drainage network in steeper areas while omitting drainage networks in less steep valleys clubb et al 2014 proposed the drainage extraction by identifying channel heads dreich method using the elevation chi relationship and reported its better performance over geometric techniques however the dreich method introduces a new parameter that is sensitive to drainage density results and the procedure for calculation is also more complicated than the above mentioned threshold methods with the development of remote sensing technology the global high resolution dems become easy to get we only had 30 m resolution dems like srtm farr et al 2007 and aster gdem nasa meti aist japan spacesystems and u s japan aster science team 2009 ten years ago and now we have 12 m resolution worlddem riegler et al 2015 and 5 m resolution aw3d global dsm data takaku et al 2016 as dems with higher and higher resolutions are available both the efficiency and accuracy of channel head recognition become more and more important in river network extraction algorithms and softwares the popular esri arc hydro tools still uses the csa method to decide channel heads because of its high efficiency many scholars have proposed new accurate algorithms such as the geometric based method proposed by the geonet team but their computational efficiency is not compatible with the simple threshold method like csa inherit the efficiency of the d8 algorithm tarboton 1997 we propose a d8 compatible high efficient channel head recognition method attempting to meet the challenge in both of the two aspects in drainage network extraction the objective of this paper is to develop a dynamic channel head recognition method that detects geomorphic change points without a direct threshold like csa and also promises low computational cost of the detection this proposed method is then incorporated into a d8 based efficient and comprehensive model for drainage network extraction from dem with billions of pixels bai et al 2015 the effectiveness of the method is tested through its application to two river basins with significantly different landscape properties 1 the maple run river basin in the eastern valley in maryland the united states and 2 a watershed in the huangfuchuan basin a sub basin of the yellow river basin located on the loess plateau in china the remainder of the paper is organized as follows section 2 introduces the framework of the d8 compatible channel head recognition method two main procedures of the proposed method to recognize each channel head and the detection strategy to traverse in a drainage network section 3 introduces the two study areas the maple run river basin and the watershed in huangfuchuan river basin section 4 presents the results a detailed discussion on the sensitivity of model parameters of this method is made in section 5 finally section 6 draws some conclusions 2 methods 2 1 general framework of the channel head recognition method raster cell is the basic unit in the extraction of drainage network from dem data the size of the regularly spaced cells is defined as the resolution of dem data as fig 1 shows such a cell p has several geographic attributes a p is the upstream source area of cell p s p is the local slope of cell p calculated by the 3rd finite difference method sharpnack and akin 1969 and l p is the accumulative length of the flow path from the watershed divide to cell p there is always a flow path passingthrough cell p if the position of channel head is located at cell p then one can get a channel beginning from this cell p a detection window centered at cell p is also shown in fig 1 which will be discussed in section 2 3 2 the csa method relies on the source area of a cell to determine whether the cell belongs to a channel or not o callaghan and mark 1984 jenson and domingue 1988 the area slope method considers the local slope and the source area of a cell to calculate the c value to distinguish channels from hillslopes montgomery and dietrich 1992 giannoni et al 2005 such methods however evaluate for each time the critical thresholds of channels on a single cell only to overcome the limitation of binary classification that is judged on each single dem cell a new recognition method is proposed in this study the method uses the change point detection technique in a sub window of the data series of some geographical characteristic function values which are calculated cell by cell in dem along flow paths a cell p becomes the channel head when it is at the corresponding dem cell of the change point position of the detection as an improvement this channel head detection method determines each channel head according to the change point of a data series along the related flow path by which the geomorphologic characteristics along each flow path can be explored the flow paths used here are identified during the whole drainage network extraction procedure when the dem cells have been queued by elevation and flow direction there are mainly two key points for the proposed geomorphologic change point method for channel head recognition one is the selection of a proper geomorphologic characteristic function the other is the strategy to implement change point detection the inverse relationship between local slope s and catchment area a presented as as k c where c was found to be in a compact range following a number of measured channel head points e g montgomery and dietrich 1992 moreover the value of as k varies along the flow path crossing a channel head it can be used as a geomorphologic characteristic function for channel head recognition however if we observe the as k value along a flow path from the ridge through hillslope across the channel head and then into channels it keeps a growing trend montgomery and dietrich 1992 hancock and evans 2006 which is found unsuitable for change point detection in many situations after our trials the new geomorphologic characteristic function that we proposed and tested will be introduced in section 2 3 1 the implementation framework of the proposed method is shown in fig 2 the key steps will be presented throughout the remaining part of this section listed as below 1 the obtained flow paths are first screened by the planar shape of the contributing area to select potential channels see flow path screening in fig 2 as introduced in section 2 2 2 the change point detection process to identify each single channel head including dynamic sub series and change point detection in fig 2 will be introduced in detail in section 2 3 3 the ergodic strategy of iterative channel head detection to traverse the treelike structure of a whole river basin see the loop before all flow paths detected will be illustrated in section 2 4 2 2 flow path screening there are some common rules governing a natural river basin s form for example the basin area is generally proportional to the square of the main stem length which yields the width length ratio of river basins and catchments at different scales falling within a certain range this phenomenon reflects the planar shape of the source areas of channels and its quantitative relationship has been intensively studied montgomery and dietrich 1992 concluded the relationship between channel length l and the corresponding catchment area a as l 3a 0 5 which indicates the width length ratio a l 2 1 3 although this relationship about the planar shape cannot be used for channel head recognition it provides a useful foundation to screen out flow paths on hillslopes from potential channels because the processes on hillslopes generally produce parallel flow paths the planar shape of the contributing area of those flow paths is much narrower than that of channels which means a l 2 1 3 this is the most primary rule for flow path screening to reduce computational cost in the following steps and it is proposed as 1 a l 2 α where α means the critical value of the width length ratio a and l are calculated by the kernel algorithm of drainage network extraction bai et al 2015 following the result by montgomery and dietrich 1992 α should be less than 0 33 to retain most potential channels the value of α is suggested to be smaller to avoid excessive computational consumption and possible misjudgment in the subsequent steps the value of α should not be too small our analysis suggests that changing the value of α from 0 1 to 0 3 does not significantly influence the outcome see the discussion section and fig 13 therefore we believe a value of 0 2 would be more appropriate but for steep and erosive basins where catchment a l 2 tends to be small a smaller α value can reduce the omissions of channels it is worth noting that because of the square shape of the dem cells several cells near ridges will always be misjudged by equation 1 for example the a l 2 value of the first cell of a flow path is always 1 and for a one cell width straight flow path that value of the fifth cell equals 0 2 therefore the first 1 α cells from the beginning of each flow path will not be examined by equation 1 when a normal source area exists upstream of a dem cell i e this cell satisfies equation 1 then the whole flow path passing through this cell will be reserved for the next steps otherwise when any normal source area cannot be found before a flow path converges with a higher order one e g with a larger source area then the dem cells along this flow path should be on a hillslope and will be screened out fig 3 shows the effect of this screening the thick lines are the reserved flow paths where a potential channel head exists in each of them the light lines represent flow paths on hillslopes and the corresponding dem cells are screened out in this step parameter α controls the efficiency of the flow path screening process in fig 3 the dem resolution is 1 m and the value of α equals to 0 2 it can be observed that a large portion of flow paths has been screened out most of them representing parallel overland flow the reserved flow paths are still far more than the real channels and will be distinguished in the flowing steps 2 3 change point detection a channel generally develops from a gully where overland flow concentrates to cause sufficient erosion to form and maintain the channel upstream of the channel head first the slope is very mild on the top of a hillslope and theoretically equals zero on the ridge as additional water gathers further down a ridge the main part of the hillslope has medium slopes generally slope increases with drainage area when hillslope processes are dominant roering et al 2007 when channel head appears near the bottom of the hillslope the profile curvature reaches its maximum and commonly there is a sudden appearance of steeper slope which would be represented by high resolution dem data in the direction downstream of the channel channel slope will be milder following the power law function of s k s a θ flint 1974 and the appearance of abrupt steep slopes would be less notable in other words slope is a key geomorphologic parameter in the detection of channel head like the slope area relationship method slope must also be incorporated in channel head detection in our proposed method however the form of the geomorphologic characteristic function used for change point detection should be carefully studied below we explain how our methodology addresses the mechanism 2 3 1 geomorphologic characteristic function the geomorphologic characteristic function f of a flow path should be able to distinguish a normal channel from a non channel flow path following up on the existing studies e g montgomery and dietrich 1988 1989 1992 as 2 is our first consideration using the field observed channel head positions julian et al 2012 as an example we plot the as 2 values over the normalized distance from watershed divides defined as zero and passing through the channel heads defined as one there are 73 data series shown together as a scatter plot in fig 4 a and a box plot in fig 4 b as 2 keeps a growing trend along the flow path from the ridge through hillslope into channels which is not suitable for change point detection the geographical characteristic function used in this paper is selected by the principle that it suits for change point detection along each flow path after examining different forms of the combination of s and a the characteristic function f is proposed as 2 f s ln a through fig 4 c and d we can see that the s ln a value has a steady stage upstream of the channel head and decreases rapidly downstream of the channel head this indicates that there are change points in the flow paths and these change points are the channel heads we are looking for for change point detection here we use the method proposed by pettitt 1979 the position and length of the detected data series are important factors in determining whether and where a change point exists therefore in section 2 3 2 we purpose a dynamic sub series window method to select the detected series 2 3 2 dynamic sub series window a geomorphic characteristic function works reasonably well in many situations e g the as 2 by montgomery and dietrich 1992 however it must also be noted that the flow paths where different channel heads are located vary dramatically according to nearby terrain and can have different function values for instance as shown in fig 5 the flow path length of channel head rv76 is longer than the length of channel head rv75 from the ridge to downstream confluence point i e a b and c d moreover more than one change point may exist on a flow path as a result the change point detection of a geomorphic characteristic function must be based on an appropriate flow path sub series to guarantee the validity and accuracy of the change points detected the following dynamic sub series window method will help us to select the right detection sub series the data series of the geographical characteristic function f along different flow paths is comprised of data points on each dem cell and the data series length is controlled by local topography and can vary dramatically from channel to channel as shown in fig 5 as a result the length of the detection sub series cannot be defined as a constant following the relationship as 2 c montgomery and dietrich 1992 adopted in the area slope threshold methods we use such a relationship to control the sub series for change point detection instead of the threshold c we use a parameter β to locate the center position most possible change point of the change point detection series as 3 as 2 β where β is the detection window control parameter in m2 the role of β is to guarantee that the potential channel head is contained in the sub series for change point detection rather than to determine the channel head directly in this paper we recommend the value of β between 400 m2 and 900 m2 which is close to the lower limit suggested by montgomery and dietrich 1992 the sensitivity analysis of parameter β is carried out in the discussion section through the condition of equation 3 we can find the possible positions of the channel heads and set them as the midpoints of their s ln a sub series i e the detection windows as stated in section 2 2 the first 1 α pixels will not be examined by equation 1 similarly we cut off the first 1 4 of the data series from the ridge to the potential point found by the as 2 value to avoid including the first peak caused by small a values into the data series then we extend 3 4 length of the data series downstream the potential point to make it center and obtain the full detection series the strategy is shown in fig 6 two different detection cases of rv75 and rv76 are shown in fig 7 including their as 2 and s ln a series the field observed channel heads the detection windows and the detected change points in fig 7 c and d the value of β equals to 400 m2 that is when the as 2 value exceeds 400 m2 for the first time the data point is selected as the midpoint shown by the red dot dashed lines of the detection window shown by the light red background color for the two channel heads respectively the red vertical dashed lines are the detected change points the p values of pettitt s approach to the change points are far less than 0 01 6 24e 07 and 2 49e 05 respectively which means statistical significance in fig 7 e and f the β value of the as 2 condition equals 900 m2 similarly the midpoints shown by the blue dot dashed lines the detection windows shown by the light blue background color and the detected change points shown by the blue vertical dashed lines are obtained for these two cases the change of β value is found to affect the detection windows however within the β range considered here the detected change points are not affected and the change points agree well with the field observed channel heads this phenomenon clearly indicates the robustness of the proposed method relative to the as 2 threshold method 2 4 detection strategy there are three important assumptions that govern the detection process to search through a river basin first for a certain subbasin branch the cell with the largest drainage area is its outlet and the main stem can be found by searching for more dominant cells upstream second if no channel head is detected along a main stem then no other channel head is considered in this branch third all the downstream cells of a detected channel head are considered as channel cells after flow path screening firstly the main stem of a whole river basin is identified fig 8 a secondly the channel head of the main stem is determined by change point detection fig 8 b thirdly the above third assumption is used to mark the downstream cells as channel until it meets another potential channel branch flowing into the main stem fig 8 c then this confluence cell is considered as the outlet of the branch and the channel head detection process is reinitiated for this branch fig 8 d these operations are recursively repeated until all potential channel heads are identified and all cells in the channel have been defined when the basin outlet is reached fig 8 e 3 study areas and data the proposed methodology is applied to two different watersheds 1 the maple run river in the united states and 2 the huangfuchuan river in china some information about these study areas and the data used are presented here the maple run watershed is located in the eastern valley in maryland united states fig 9 a it is a tributary of the town creek river which locates between the mountainous regions and the relatively flat coastal plain this region is characterized by irregular plains and moderate relief with greater amounts of pine and mixed forests and has thick clay rich soils underlain by deeply weathered bedrock and relatively few solid outcrops julian et al 2012 the mean annual precipitation in this basin is 1084 1159 mm for the period 1971 2000 according to the prism dataset daly et al 1997 and the 30 min rainfall intensity with a recurrence interval of 2 years is 29 30 mm according to noaa pfds julian et al 2012 the general river network data for this basin shown in fig 9 a was obtained from the nhdplus high resolution beta dataset available online at https viewer nationalmap gov basic bare earth dem data with 1 9 arc second approximately 3 m resolution was obtained from the national elevation dataset available online at https viewer nationalmap gov basic the 73 field observed channel heads fig 9 b were obtained from julian et al 2012 and the related region covers an area of about 9 14 km2 which means the spatial density of channel heads is about 8 km 2 the huangfuchuan river is a tributary of the yellow river in china at the northern edge of the loess plateau the mean annual precipitation in huangfuchuan river basin is 363 388 mm for the period 1956 2002 according to the local climate stations and about 80 of the total precipitation falls between june and september sui et al 2008 because of dry weather only sparse vegetation grows in this region the loess rich flat high lands here are severely incised by gullies and low relief channel valleys fig 9 d because of intensive erosions in this study an upstream watershed in the huangfuchuan river basin is selected for analysis this watershed is located in jungar banner inner mongolia fig 9 c and covers an area of 2 46 km2 dem data with 1 m resolution was obtained from aerial geodetic survey carried out by the yellow river conservancy commission because of the large number of channel heads a careful field survey in the huangfuchuan watershed is costly we visually recognized the channel heads on remote sensing images 0 4 m spatial resolution from geoeye accessed from google earth and 294 channel heads were found in the upstream huangfuchuan watershed see the green and red dots in fig 12 then the spatial density of channel heads is about 120 km 2 much larger than that in the maple run moreover the channel head density varies intensively in spatial in this watershed the geographic and climatic conditions in the maple run river basin and the huangfuchuan river basin are quite different from each other and the dem resolutions are also different 3 m and 1 m therefore they can be used to test the compatibility of the proposed method with different geomorphology and data what s more we acknowledge that even 1 m resolution may not be sufficient for channel head identification but for river network extraction from dem this is a common high resolution and the results can meet the needs of hydrological modelling and water management 4 results for the two study areas in this study the procedure for the implementation of the method is essentially the same the proposed methods in this paper have been integrated into the drainage network extraction program first developed by bai et al 2015 which was coded by the c language besides in order to evaluate the effectiveness and efficiency of our proposed method we compared the results with the geonet software passalacqua et al 2010 sangireddy et al 2016 the geonet software was coded by the python language and the source code can be downloaded from https github com passah2o geonet for the maple run river the extracted channel heads are compared with the surveyed ones to calculate the reliability and sensitivity indices and also compared with the geonet results moreover the proposed β values from 400 m2 to 2500 m2 with intervals of 100 m2 are also considered to carry out a sensitivity test for the huangfuchuan river the extracted channel heads are only compared with those visually identified on high resolution remote sensing images 4 1 maple run river because of the available field observed channel head data in this case the evaluation of extracted channel heads is rigorously carried out here the results of channel head locations can be divided into three classes orlandini et al 2011 true positive tp false positive fp and false negative fn fig 10 a which is implemented following the method proposed by clubb et al 2014 a tp means an observed channel head is successfully found through the channel head recognition method an fp means the method recognizes a non existent channel head and an fn means the method fails to recognize an observed channel head for the accuracy criteria a tp is defined when an observed channel head can be found in the corresponding extracted first horton strahler order contributing area according to the point assessment results two quantitative indices can be calculated and evaluated orlandini et al 2011 the reliability of the result can be defined as r σtp σtp σfp where σtp and σfp are the total numbers of true and false positives respectively and the sum of them means the total number of the recognized channel heads the sensitivity of the result can be defined as s σtp σtp σfn where σtp and σfn are the total numbers of true positives and false negatives respectively and the sum of them means the total number of observed channel heads fig 10 shows the extracted drainage network and evaluation indices obtained using the proposed method the results presented in fig 10 a correspond to the parameter β 600 m2 in this map the sensitivity index r equals 0 904 which means that 90 4 of the field observed channel heads have been successfully recognized the reliability index equals 0 452 which is a bit low this is partially caused by the stricter standard of the channel head field investigation relative to drainage network extraction from dem data fig 10 b shows the frequency of position errors between predicted and field observed channel heads for true positives in fig 10 a the mean and standard deviation of the horizontal distance between predicted and field observed channel heads are found to be 56 m less than 20 dem cells and 104 m respectively fig 10 c shows the response of evaluation indices r and s to parameter β with an increase in parameter β sensitivity s decreases and reliability r increases generally when parameter β varies from 400 m2 to 900 m2 sensitivity s varies from 0 918 to 0 781 which is very high for the selection of optimal parameters better sensitivity generally comes with worsening reliability and vice versa the trade off between sensitivity and reliability can be determined depending on the purpose of the application of extracted drainage networks for hydrological simulation purposes a smaller β will lead to more channel heads and then higher sensitivity and will produce higher but still adequate resolution drainage network to evaluate the overall performance of channel head recognition methods the sum of sensitivity and reliability indices would be useful the sum of s and r of the proposed method for the maple run river is found to be in the range of 1 180 1 356 when β varies from 400 m2 to 2500 m2 in a similar work done by clubb et al 2014 the sum of s and r for three river basins using three different methods varied from 1 06 to 1 32 from this perspective it can be inferred that the method proposed in the present study performs slightly better than this existing method furthermore the model performance is less sensitive with its parameter in a large range because the parameter β only controls the position and length of the change point detection window instead of determines channel heads directly which is one of the key innovations of this paper for comparison we applied the geonet software for the same study area and we altered its single parameter flowthresholdforskeleton short for tfs below from 10 to 1000 then we evaluated the reliability r sensitivity s and the sum of them for the results from geonet software and the calculation times for both the methods the results are compared in table 1 from table 1 we can find out that the geonet software recognized fewer channel heads and the extracted network was sparser as shown in fig 11 with tfs 200 for the largest r s when the parameter tfs varied from 10 to 1000 the reliability r increased from 0 449 to 0 778 sensitivity s decreased from 0 548 to 0 288 and r s varied in the range from 0 997 to 1 066 in contrast when the reliability r obtained by the proposed method varied in a similar range from 0 396 to 0 769 the maximum seneitivity s reached higher than 0 9 and r s varied in the range from 1 180 to 1 356 which proved that the proposed method was more effective moreover the calculation time of the proposed method was less than 10s obviously shorter than that of the geonet software this is partly because geonet builds a loosely coupled python program that separates dem preprocessing flow direction judgment channel head recognition channel extraction and vectorization of the river network independently and each part generates intermediate files for coupling which results in frequent file reading and writing while the proposed program combines all the above steps using a d8 based unified data structure and is implemented by using the c language however when those methods are being used for much larger river basins the d8 compatibility of the proposed method as well as the ideal o nlogn computing efficiency of d8 algorithm will provide higher scalability 4 2 huangfuchuan river despite different dem resolutions and landscapes in the huangfuchuan basin and the maple run river basin the same model parameters of β 600 m2 and α 0 2 were adopted for the huangfuchuan basin as well to investigate the stability of model performance fig 12 shows the extracted streams blue lines and the corresponding image on google earth from the image of this watershed we can see that because of the highly erosive loess there are many new channel heads under development the challenge here would be achieving high sensitivity to recognize those developing channel heads a total of 402 channel heads were extracted for the study area with model parameters of β 600 m2 and α 0 2 by comparing the extracted streams with the channel heads that visually recognized observed on remotely sensed images we can find out that 207 extracted channel heads tp see the green dots in fig 12 fit well with those observed with position deviations less than 20 m and 87 observed channel heads fn see the red dots in fig 12 are missed in the extraction this means that 51 5 207 in 402 of the identified channel heads are necessary and accurate i e reliability r equals 0 515 and 70 4 207 in 294 of the observed channel heads have been extracted i e sensitivity s equals 0 704 the sum of s and r equals 1 219 also suggests a promising precision level 4 3 remarks the results presented above for the two river basins indicate that the use of the same parameters yields different reliability and sensitivity of model s results a higher reliability r but a lower sensitivity s i e relatively lower channel head density than observed was obtained in the huangfuchuan watershed compared with those in the maple run river basin from table 1 we can see that similar r and s results can be obtained in the maple run river basin with larger β between 600 and 1600 m2 the main reason for this is most likely the difference in landscape those newly developed channel heads along the main channles in the upstream huangfuchuan watershed see fig 12 are too dense to be fully recognized though the dem used here has higher resolution therefore for different terrain and dem resolution we can adjust the β value within a certain range to obtain different results a preference between reliability and sensitivity while maintaining the overall accuracy r s at a reasonable level 5 discussion in the drainage network extraction for the maple run river basin fig 10 the value of α was chosen as 0 2 and the best value of β was found as 600 m2 when r s reached the maximum the results demonstrate that channel head accuracy is satisfiable with such proper parameter values moreover we also changed the value of parameter α to examine if the detection results are sensitive to it fig 13 the parameter α means the critical value of the width length ratio and is used to screen out flow paths on hillslopes from potential channels when α is bigger than a certain value some true channels would be screened out and the corresponding channel heads will be missed leading to a lower sensitivity however it won t influence the location of channel heads that can be recognized on the other hand when α is smaller than a certain value fewer flow paths on hillslopes are screened out then more change point calculations are needed and some fake channel heads would possibly be detected on those hillslope flow paths which will lead to a lower reliability value from fig 13 we can observe that this speculated phenomenon happened with very slight performance difference when α varied between 0 1 and 0 3 the most important parameter put forward in this paper is β which determines the position and length of the change point detection window for the geomorphological characteristics function we applied different values for the parameter β between 400 m2 and 2500 m2 in the extraction of the drainage network in the maple run river basin and evaluated model performance using sensitivity s and reliability r as well as their sum see fig 10 c through the presentation of the r versus s plots varying with parameter β one can observe the reliability and sensitivity characteristics of the model the fact that the reliability index r increases stepwise and smoothly with the increase in parameter β suggests that one can easily control the ratio of false positives fps an important issue in channel network extraction further the observation that the sensitivity index s decreases quickly in the beginning and then slowly as the parameter β increases suggest that one can get a satisfactory result with a small ratio of false negatives fns when parameter β is controlled within an upper limit finally the intersection of r and s means a balance of the parameter β and a higher intersection point means a better recognition method in other words higher and stable values of the sum of r and s under changing model parameters can prove the robustness of a channel head recognition method as dems with higher and higher resolutions are available the proposed method makes full use of the geomorphologic features brought by the high resolution dem data the present results suggest that the geomorphic change point detection method can effectively and reliably recognize the locations of channel heads moreover the method is robust enough that the results remain stable when the parameters change within a certain range of values on the other hand the proposed algorithm has significant computational efficiency advantages as shown in table 1 compared with geonet software the extraction program in this paper only uses less than 10s when geonet uses more than 100s for the same basin and compared with the traditional threshold method such as csa this algorithm does not increase too much calculation time the kernel part of the proposed method is a rank based change point detection algorithm in the detection window with a limited size so its computational complexity will not exceed o n and the computational complexity of the whole d8 based extraction process is o nlogn bai et al 2015 thanks to the efficiency of the d8 based algorithm little time will be spent on detecting channel heads among the total computational cost a dem with about 100 million pixels was used to test the computational cost the total calculation time is 3 min 40 s among which only 8 s less than 4 was taken up by channel head recognition 6 conclusions this study has proposed a d8 compatible high efficient channel head recognition method based on change point detection of a geographical characteristic function in place of a designated threshold value or complex calculations non uniformly distributed channel heads and then more accurate river networks can be extracted from high resolution dems using this method the results also demonstrate the response of performance to the method s parameters which shows a large acceptable parameter range and suggests the trade off between sensitivity and reliability the proposed method can be used to obtain more accurate drainage networks than using the widely used esri arc hydro tools moreover it is more efficient when compared to popular geonet software with similar channel head accuracy besides the advantages of the proposed method there are still some limitations in its application first the resolution of dem data must be high enough to ensure accurate channel head results second application in this paper doesn t consider errors caused by imperfections in dem data so the recognized results are just responsible for the dem data used in this regard more dem data with higher resolution and accuracy are likely to be available worldwide in the future as new satellite remote sensing techniques are being developed therefore the proposed method will become even more appropriate and computational feasible when compared to those complex geomorphological methods in recognizing channel heads in addition the proposed algorithm and main parameters are only tested in two small cases the maple run river basin with gentle slope and the loess erosion controlled huangfuchuan river basin and are only compared with the geonet software we cannot prove that for other terrains especially for mountainous and steep slope regions similar results can be obtained and we may ignore other effective and efficient algorithms that could provide reliable results we have put our source code on github everyone is welcomed to test and compare this method for more cases and communicate with us and we will continue to improve our work moreover the proposed method only considers the accuracy of channel heads but a complete drainage network extraction procedure includes multiple steps that affect the accuracy of the whole network in this regard a high efficient flow enforcement method was also proposed by us to improve the accuracy of extracted main streams using mapped streamlines or remote sensed water surfaces wu et al 2019 which is also d8 compatible therefore a comprehensive d8 compatible high efficient and high accuracy drainage network extraction program can be developed by combining these two methods in the future software availability the channel head recognition method using geomorphologic change point detection has been integrated into a d8 based drainage network extraction program the source code is freely available on github at https github com thuleef dne declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the national key research and development program of china 2016yfe0201900 2017yfc0403600 the national natural science foundation of china 91547204 51579131 51569026 and the state key laboratory of hydroscience and engineering of china 2017 ky 04 any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the authors or funders 
26072,this paper presents the release of ltrans zlev which is a new version of the off line lagrangian ocean particle tracking model ltrans v 2b that is compatible with a z coordinate constant depth layers discretization of the hydrodynamic equations the model capitalizes on and massively extends the capabilities of the original code ltrans which is already quite popular but can be used only adopting a sigma coordinate terrain following layers discretization among the additional features included in ltrans zlev there are the backward in time particle tracking algorithm and some new customizable larval behaviour options the new version also includes the oiltrans module for oil spill simulations the new implementations were validated by using the output of the z coordinate massachusetts institute of technology general circulation model mitgcm for an idealized case study describing a cyclonic gyre in a mid latitude closed basin another test case in which larval dispersal is modelled in the northern adriatic sea illustrates some of the new features of ltrans zlev graphical abstract image 1 keywords ltrans mitgcm lagrangian modelling larva oil spill 1 introduction lagrangian particle tracking algorithms provide a way to simulate analyse and describe oceanographic transport processes that can be of particular interest in a number of studies from larval transport cowen and sponaugle 2009 werner et al 2007 to pollution dispersion and water quality assessment james 2002 to oil spill modelling canu et al 2015 north et al 2011 in the lagrangian description pope 1985 taylor 1922 the transport processes are tracked by following fluid particles in order to identify the trajectories starting from or arriving at a given point the transport equations are discretized along a moving frame of reference and particle positions and velocities are considered also in between the eulerian grid points therefore lagrangian approaches provide a finer description of the transport processes compared with the one traditionally obtained in purely eulerian transport models in which the dynamics of the flow is described by computing the hydrodynamic properties at every grid element of a fixed geo reference grid however the lagrangian approach still requires the input of advection velocity fields which are used to update the particle position and must be computed in advance by an eulerian hydrodynamic model in an eulerian model the vertical discretization of the computational domain can be based mainly on three kinds of coordinate systems constant depth levels z terrain following σ layers or isopycnal ρ layers in the z case each layer is characterized by a constant depth over the whole domain and therefore the computational domain has more vertical layers in the deeper areas than in the shallower ones examples of such a kind of models are mom griffies et al 2000 2004 nemo madec and the nemo team 2016 and mitgcm adcroft et al 2017 marshall et al 1997a 1997b conversely in the terrain following σ models such as pom blumberg and mellor 1987 mellor 2003 and roms shchepetkin and mcwilliams 2009 the number of layers is the same all over the domain so that in deeper areas layers are thicker isopycnal models such as micom bleck et al 2002 use a vertical discretization based on the potential density ρ referenced to a given pressure value each discretization technique z σ or ρ has both advantages and disadvantages chassignet et al 1996 griffies et al 2000 and the choice between these approaches is not straightforward shapiro et al 2013 it depends on several factors such as the basin topography the relative importance of the various processes in a given study site and the numerical schemes implemented in the model shchepetkin and mcwilliams 2009 the z level discretization is usually more accurate in representing the surface mixed layer and the diabatic processes it includes while the σ coordinates allows a better representation of the bottom boundary layer isopycnal models are recommended for tracer advection and diffusion along sloping density surfaces in the ocean isopycnal models are less efficient in representing the bottom boundary layer and the surface mixing layer which are mostly unstratified therefore z or σ models may be preferred when simulating water basins whose dynamics are strongly influenced by the processes involved in these layers for basins showing remarkable variations of topography a z grid model is computationally more expensive than a σ model as it requires a higher number of vertical levels than a σ model to solve both the surface and bottom boundary layer dynamics blumberg and mellor 1987 conversely using σ coordinates may cause inconsistencies in the areas of sloping topography where numerical truncation errors in the computation of the pressure gradient force may become important and where the condition of hydrostatic consistency is more critical haney 1991 to handle such cases the vertical resolution of the σ grid must be increased mellor et al 1994 at the cost of reducing model performance most eulerian geophysical dynamics models already have their own native on line lagrangian tracer tracking modules however for studies involving multiple experiments large or high resolution time or space intervals or ensemble simulations the computational cost of using on line tracking modules can be very high and often not affordable since the whole hydrodynamic flow fields have to be recomputed for every realization on the contrary the use of off line lagrangian particle transport models allows to perform multiple transport modelling studies using a single set of stored flow fields with very affordable computational costs it also permits backtracking applications off line models can be used as long as it is assumed that the transported materials have no effect on the geophysical flow passive tracers and as long as the hydrodynamic stored fields have a sufficiently refined temporal resolution to approach high frequency phenomena the use of lagrangian models is indeed growing and recent reviews and books illustrating the wide use and popularity of the lagrangian approach in oceanography can be found in recent literature lynch et al 2014 prants et al 2017 van sebille et al 2018 among the principal off line lagrangian models we can cite ichthyop lett et al 2008 which was developed for σ coordinates systems parcels lange and sebille 2017 is instead compatible with both σ and z coordinates but does not yet include ready to use larval behaviour and oil spill weathering modules while cms paris et al 2013 offers these possibilities but it is compatible only with z grid models the lagrangian transport model ltrans ltrans v 2b is one of the most popular off line three dimensional particle tracking modules north et al 2013 schlag and north 2012 it has been developed to simulate the movement of passive tracers particles with sinking or floating behaviour like sediment or oil droplets and planktonic organisms like oyster larvae the source code is written in fortran 90 it includes the popular 4th order runge kutta scheme for particle advection and a random displacement model to account for sub grid scale turbulent diffusion ltrans v 2b estimates two dimensional water properties such as sea surface height or bottom depth at the particle location using bi linear interpolation instead three dimensional quantities fluid velocity temperature etc are estimated by see subsection 2 3 a interpolating them bi linearly at the particle horizontal location for every vertical level and b creating a water column profile using a tension spline curve north et al 2006 sticking and reflective boundary conditions on solid walls specific particle behaviour routines and settlement algorithms are also included the transport model can be used together with an oil spill weathering module named oiltrans and developed within ltrans v 2 berry et al 2012 oiltrans considers mechanical spreading advection and diffusion of the oil particles by winds and currents as well as evaporation emulsification and vertical dispersion since hydrodynamic fields are usually stored with a too low frequency to drive particle tracking algorithms ltrans considers the time interval between two fields e g hours or days as an external forcing time step the particle motion time scale is instead based on a much shorter internal time step e g minutes satisfying the cfl condition uδt δx 1 allowing particles to move over smaller space and time intervals in order to ensure a good accuracy of the solution at every internal time step the advection velocities at the particle position are interpolated in time and space from the eulerian fields and boundary condition algorithms ensure that the particles remain within the volume composed by the water cells of the domain as an output ltrans provides netcdf files containing the coordinates of the trajectories followed by the particles and if requested the flow field properties temperature salinity etc sampled along these trajectories the ltrans v 2b model was originally developed to run adopting the stored hydrodynamic fields produced by the regional ocean modeling system roms which is based on a terrain following σ layer discretization due to this reason ltrans v 2b employs grids bathymetry and velocity fields based on a σ coordinate system in this paper we present ltrans zlev which is a new version of ltrans that handles the grids and hydrodynamic flow fields of geophysical models based on a z coordinate discretization with staggered arakawa c grid the modification required an extensive editing of the code but it significantly improved and widened its use since many oceanographic models are indeed based on z levels furthermore several new features and options have been implemented and or included in the code the backward in time particle tracking capacity the capability of simulating coastal stranding in selected areas and at prescribed distances from the coast new larval behaviour options and customizable larval scenarios including near bottom release diel vertical migration followed by seabed transport the capacity to handle multiple independent i e not connected water basins and the oiltrans module for oil spill simulations the zlev version can handle z coordinates as well by adjusting the height of every level along the vertical coordinate according to the sea surface elevation changes furthermore ltrans zlev still offers the possibility to use the vertical layer discretization based on σ instead of z vertical coordinates the new version of ltrans described in this paper has been tested with the hydrodynamic flow fields provided by a set of mitgcm massachusetts institute of technology general circulation model simulations the code provided in the repository includes a short user manual a test case ready to be used and some pre processing tools to build the grids using the mitgcm outputs the details of the main developments implemented in ltrans zlev in order to handle the z levels of the mitgcm model instead of the σ discretization of roms are described in section 2 together with the new features available in the zlev version section 3 1 gives technical and computational information regarding ltrans zlev while section 3 2 presents a test case performed on an idealized cyclonic gyre in a mid latitude closed basin in which we compared particle trajectories computed by ltrans zlev off line coupled to the 3d eulerian flow fields provided by the mitgcm versus particle trajectories computed by the online native tracking module of the mitgcm then section 3 3 presents a realistic application considering particles with larval behaviour in which particles are released close to the seabed then they rise towards the surface perform diel vertical migration for a few days after which they sink back to the bottom where they are transported along the seabed searching for the environmental conditions suitable for settlement conclusions and future perspectives are finally drawn in section 4 2 model description 2 1 overview the original ltrans v 2b model comprises an initialization phase which builds the grids and boundaries of the water domain and a running phase loading the hydrodynamic flow fields at every external time step and then updating the particles positions at every internal time step in ltrans zlev the global structure of the ltrans v 2b code is unchanged but most parts of code handling the grids hydrodynamic flow fields and boundary conditions have been deeply rewritten besides new functions have been implemented to calculate for example the depth of the deepest grid nodes which define the z level bathymetry and to interpolate the bottom depth at the particle position leading to the new version of ltrans represented in the diagram of fig 1 in addition to the source code an external pre processing python tool included in the ltrans zlev package creates the z grid file required by ltrans zlev starting from the output file provided by the off line z level hydrodynamic model mitgcm in our case this external tool rewrites the grid coordinates and water land masks at every node of the arakawa c grid in a unique file in netcdf format as requested by the initial version of ltrans the native mitgcm binary output files or the merged in time ones are instead read directly by ltrans zlev at step 7 in fig 1 2 2 boundary conditions while tracking the movement of lagrangian particles particular care has to be given to the boundary conditions in fact it might happen that the numerical integration of a particle path i e the computation of where the interpolated velocity field projects a particle after a time step moves a particle outside the water volume of the computational domain for this reason it is fundamental to have well defined boundaries whose position and typology define what happens along the borders of the water volume 2 2 1 z level arakawa c grid most geophysical models such as roms or mitgcm use arakawa c grids where the scalar properties e g water density ρ and the u v and w components of velocity are defined on staggered nodes in the arakawa c staggered grid the ρ nodes are at the centre of cubic land or water cells while the u v and w nodes lie on the cell edges in the x y and z directions respectively fig 2 a b illustrate the vertical levels and arakawa c staggering respectively for a σ level grid and a z level grid without partial bottom cells in both cases every level along the vertical direction is made by a planar layer whose grid is defined by ρ u v and w nodes the horizontal position of the nodes does not vary with the vertical level meaning that the nodes of two different vertical levels have the same horizontal x y location but a different vertical z depth in every grid cell they store different values of the three dimensional flow fields at every vertical level the sea surface elevation and the bottom depth are stored at ρ node positions but they are two dimensional quantities depending only on the horizontal x y position as shown in fig 2 a b the water volume of the computational domain is enclosed by two kinds of boundaries the upper and lower boundaries made of horizontal planes delimiting the extremities of the water column i e sea surface and bottom and the lateral boundaries made of vertical planes delimiting the longitudinal and latitudinal extension of the basin 2 2 2 lateral boundaries σ level and z level grids represent lateral boundaries in a different way in σ coordinates the number of water cells at any vertical horizontal section remains constant but the height of each grid cell varies according to the local bottom depth the planar layers can tilt vertically at all depths to follow the bathymetry and the lateral land water boundaries are the same for all layers as shown in red in fig 2 a in contrast z grids layer boundaries change with depth fig 2 b and therefore different vertical transects might have a different number of wet i e non land cells according to the horizontal position of the transect and horizontal sections at different depths might have a different number of wet cells from the eulerian grids shown in fig 2 a b ltrans elaborates the lateral boundaries as illustrated in fig 3 a b for σ and z grids respectively by linking together at every level the u and v nodes at the interface between land and water cells to form a sequence of vertical planes that the particles cannot cross the nodes positioned at the centre of every element made by 4 adjacent ρ nodes are called e nodes their projection on the bottom is represented by the magenta circles in fig 3 ltrans v 2b considers the lateral borders of an ocean basin on a σ level grid as a series of vertical walls whose horizontal x y coordinates are identical at every vertical level fig 3 a shows that the water volume is enclosed by the same vertical red planes for any vertical level conversely for a z level grid the position of the lateral solid boundaries is instead dependent on the vertical level fig 3 b as a consequence on z level grids there might be multiple isolated sub basins at depth as well as water basins nested inside island areas due to the presence of rifts and trenches at the bottom of the sea ltrans zlev is designed to handle such kinds of water sub basins particles are often projected outside the water domain across a lateral boundary during the numerical integration of their position in such a case the particles will stop along the boundary to simulate for example coastal stranding of a pollutant or bottom contamination alternatively to simulate larval dispersion one might assume that the particles bounce against the boundary and they are reflected back into the water the two assumptions are labelled as sticking and reflective conditions respectively 2 2 3 upper and lower boundaries the sea surface height and the bottom depth at a particle location are estimated at every time step in order to maintain the particle within the upper and lower boundaries of the water column on a σ level grid ltrans v 2b interpolates these quantities by running bi linear interpolations of the sea surface height and the bottom depth represented by planar crosses and circles in fig 2 a respectively which are both two dimensional quantities stored at the 4 ρ nodes surrounding horizontally a particle by doing so when a particle is moved laterally in an area shallower than its previous position it is also risen up towards the surface so that it cannot by construction cross the bottom boundary i e flow below the deepest layer therefore in ltrans v 2b on a σ level grid the lateral boundary reflection conditions are applied only along the lateral vertical walls that is when the particles reach a lateral coastal land or island boundary represented in red in fig 2 a using a z grid with depth dependent lateral boundaries a direct linear interpolation of the bottom depth from the surrounding ρ nodes planar white circles in figs 2 b 3 b is no longer possible in fact such a seabed surface would pass across the deeper boundary walls and particles moving downward from one vertical level to another could find themselves trapped behind one of the vertical walls composing the lateral boundaries we implemented two possible configurations the first one uses full cells with a bathymetry made by horizontal planes and vertical walls while the second one is characterized by sloping surfaces and takes into account the partial cells in the first approach no partial cells the ltrans zlev bathymetry has to be compliant with the red boundary walls positioned along the u and v nodes represented in fig 3 b to do so a grid refinement is applied to the bottom boundary depth matrix originally defined at the position of the ρ nodes using the position of the ρ u v and e nodes as new grid points the depth of the bottom under the u v and e nodes is defined once for the whole simulation at step 4 of fig 1 it depends on the shape of the lateral boundaries at the various vertical levels the sharp bathymetry profile associated to the non interpolated z grid boundaries shown in fig 3 b causes multiple reflections along lateral walls slowing down the particles that run into bathymetry elevations this can be advantageous when running simulations for basins characterized by steep slopes in the bathymetry such as submarine canyons or when studying the interaction of marine particles with any kind of vertical man built underwater structure for example dikes or oil and gas extraction terminals unfortunately this approach cannot accommodate a partial bottom cells discretization of the eulerian grid a second alternative configuration of ltrans zlev allows to avoid this limitation by implementing partial cells and considering the seabed as made by sloping surfaces in this second case the partial cells discretization of an eulerian z grid like the one shown in fig 4 a is represented in the lagrangian simulation by a smoother interpolated bathymetry presenting tilted planes along the bottom surface such as the one illustrated in fig 4 b in this configuration the local depth is evaluated by a linear interpolation of the three nodes composing the triangle containing the particle with this approach the particles will rise whenever they encounter bathymetry elevations blue sloping surfaces in fig 4 b conversely they will experience reflections along the lateral boundaries red walls in fig 4 b only in the upper layers when moving across the continent or island borders this configuration may be preferred for simulations in most oceanic basins characterized by gentle slopes or flat grounds unlikely to retain particles when using the interpolated bathymetry the depth of the bottom under some of the ρ u v and e nodes is subject to a re computation at step 4 of fig 1 to reduce depth discontinuities ltrans zlev offers the possibility to use either the first non interpolated configuration represented in fig 3 b or the second interpolated configuration represented in fig 4 b if the user adopts the first non interpolated approach on a z grid containing partial bottom cells these partial cells are either converted into full cells or cancelled masked as land points by the model depending on the relative height of the partial cell 2 2 4 application of the boundary conditions the position of every particle is updated at every internal time step by computing the velocity associated to its former position and the related displacement within the integration time step step 16 of fig 1 the movement related to waves stokes drift wind step 17 of fig 1 and turbulence is added as well at step 19 of fig 1 the particle is reflected horizontally whenever the trajectory between the previous position and the new position crosses any lateral boundary of the vertical level associated to the earlier particle position 2 3 flow interpolation methods in ltrans v 2b the interpolation routines of the flow field used for example at step 16 are based on the identification of the 4 surrounding ρ nodes u v w nodes for the velocity fields respectively forming a squared element containing the particle e g the element formed by the white ρ nodes of coordinates i j i 1j i j and 2 2 in fig 2 a at every internal time step optimized algorithms searching in neighbouring elements determine the 2 d planar element in which the particle is located then among the 4 nodes composing the element itself the 3 nodes that are closest to the particle are selected defining a triangle containing the particle the interpolation coefficients are computed depending on the position of the particle relatively to these 3 nodes in a first phase these coefficients are used to interpolate horizontally the water properties at every vertical level to create a vertical water column profile at the particle horizontal position in a second phase the water properties are interpolated at the exact particle depth using 4 vertical levels 2 levels above 2 levels below the particle with the tspack tension spline curve fitting package renka 1993 since the tension spline curve is computed by considering the interpolated values in each layer for a z level mesh the different nodes to be considered for interpolations may have a different number of water layers in areas with uneven topography moreover some areas might have less than 4 vertical water levels above the bottom in the v 2b version 4 vertical levels are systematically used for the vertical profile while in the zlev version z level mesh the tension spline is computed only if at least 3 vertical levels are present at the particle position using at least 3 levels in case of only 2 vertical levels a linear interpolation is performed otherwise if the particle is above the last ρ cell centre or in case there is only one vertical level above the bottom the water properties are assumed to be those of the upper level instead of being linearly interpolated besides in the zlev version the user can choose to deactivate the tension spline curve fitting and use instead a simple linear interpolation to compute the water properties at the exact depth of the particle on z grids in areas of varied topography the fact that the different nodes to be considered for interpolations have a different number of water layers can affect also the particles transported near the bottom in fact such situations might lead to erroneous interpolation of the water properties near the bottom where the zero values stored in land nodes might be used to interpolate properties such as current velocities temperature and salinity the problem is solved by fictitiously assigning to each land node the water properties of its closest water nodes giving the properties of the upper node to the first land node under the bottom or averaging the properties of the water horizontal neighbours for deeper land nodes this procedure is applied every time a new hydrodynamic field is loaded to the quantities stored at the ρ nodes and to the horizontal current velocities stored along every lateral boundary in a second step to account for the boundary layer viscous effects as described in section 2 4 6 the model offers the possibility to parameterize the decrease of the speed of the particles which are slowed down as they approach the bottom 2 4 additional features 2 4 1 oil spill weathering and effects of wind and stokes drifts ltrans zlev integrates also the oiltrans oil spill weathering module developed within ltrans berry et al 2012 together with the effects of wind forcing on oil slicks the wind forcing used to calculate the drift of the particles is two dimensional varying on each grid element and interpolated in space for every particle in the zlev version the wind intensity used to compute the oil weathering processes of the oiltrans module is calculated at every time step by averaging over all the particles of the simulation the value of the wind intensity interpolated at every particle position at every time step the average over the particles of the water temperature and water column height are calculated in the same way and used to compute the weathering effect the user can choose which percentage of the instantaneous in time but spatially averaged over all the particles wind intensity will be used for the weathering processes as the weathering is highly responsive to wind this parameterization allows a better calibration of its intensity which is useful if we want to take into account and evaluate the effects of the temporal resolution of the forcing fields in fact wind forcing with high or low temporal resolution might show peaks of wind intensity or a relatively smooth variability respectively in this way the user can adapt the wind intensity term if the calibration of the various processes involved in the weathering has been done for wind fields sampled at a different temporal resolution furthermore in the zlev version wind forcing can be enabled even when the oil spill module is turned off this feature was not included in the original ltrans but it is quite useful to model wind effects on the dispersion of inertial floating material for objects with large portions floating above the sea level and consequently directly exposed to wind drag such as macroplastics additionally the amplitude of the wind and stokes drift as well as the deviation angle of the wind drift to the right hand side of the wind vector can now be customized by the user 2 4 2 tracking particles backward in time backward in time computations negative time step are possible in ltrans zlev by reading the hydrodynamic files in reverse order that is from the last one to the first one and using a negative time step to reverse the direction of the advection fields this feature can be used when the goal of the simulation is the identification of the source of the transported particles in order to estimate for example the release position of a pollutant or the coordinates of an accident when people or objects are found lost at the sea 2 4 3 stranding and settlement in addition to the possibility of simulating particle settlement by defining settlement polygons already included in ltrans v 2b ltrans zlev parameterizes also particle stranding by defining the distance from the coast within which a particle must be considered as stranded the user can also choose a preferential depth range for stranding or settlement these options increase the versatility of the model and allow to simulate a wider range of larval species which follow different settling scenarios 2 4 4 particle release from the bottom layer another new possibility offered by ltrans zlev is the release of particles at a given distance which can be customized above the seabed within this option the user must specify only the horizontal positions where the particles should be released and ltrans will compute for every particle the exact depth of its release point following the shape of the bathymetry this feature is useful for example when simulating the dispersal of larvae released by organisms living close to the bottom 2 4 5 larval behaviour with diel vertical migration and transport along the seabed in ltrans zlev an option allows the user to entirely customize the parameters of a new larval scenario representative of the behaviour of the larvae of several species the particles are released close to the seabed then rise towards the surface and perform diel vertical migration after which they sink to the bottom to be transported along the seabed by selecting this new behaviour the user can setup diel vertical migration dvm by linking in the input file the name of a data file containing time varying spatially average short wave downward swd radiation data for the computational domain and setting threshold values for the upward downward swimming phases of the larvae in other words to compute the temporal variability of the target depth range of the particles ltrans zlev reads the space averaged swd radiation file extracted from the eulerian model and calculates the instantaneous swd radiation intensity then the model checks if the value is higher than the sunrise threshold or lower than the sunset threshold depending on the current swd radiation ltrans zlev determines whether the target layer of the particles is the daily one or the nightly one both threshold values for sunrise and sunset as well as the vertical swimming speeds and the depth of the day night target layers can be entirely set by the user together with the length of the dvm period when the simulated time exceeds the dvm period the particles sink to the bottom and are transported along the seabed remaining within a target layer the maximum height above the bottom of this target layer can be set by the user at any moment the particles that are located outside their target layer range due to the advection by the currents or when going from one target layer to another are pushed back towards it using a vertical component of swimming speed which is superimposed to the hydrodynamic transport the development of this new behaviour option widens the applicability of the new version of ltrans the possibility to customize its parameters makes it suitable for modelling different species in the same environmental conditions the zlev version still includes the larval scenarios already implemented in the ltrans v 2b which were specifically parameterized to model c virginica and c ariakensis oysters for other species near surface or near bottom transport and diel vertical migration based on time since midnight instead of short wave downward radiation fields can still be modelled as single behaviours maintained for the whole simulation as it was already implemented in ltrans v 2b 2 4 6 advection velocity in the boundary layer the height of the bottom boundary layer and the advection velocity of any particle entering it can be set by the user within the selected boundary layer the advection of particles transported near the bottom is reduced by applying either the logarithmic law of the wall already implemented in the v 2b version or a percentage of the ambient flow velocity 2 4 7 diagnostic outputs along particle paths if set by the user when printing the output files containing particle paths ltrans can extract instantaneous flow field properties which are critical for larval behaviour such as temperature and salinity at every particle position the zlev version offers also the possibility to extract the minimum or maximum values besides the instantaneous values also available in ltrans v 2b encountered by the particles since the last output of the lagrangian model moreover in ltrans zlev a sediment grain size map defined on the same grid as the hydrodynamic model can be loaded then the instantaneous minimum or maximum values of the grain size encountered by the particle since the last extraction can be written together with the temperature and salinity values finally the presence of a particle within a polygon can be tracked independently from any settlement behaviour by storing at every instant the identification number of the polygon containing the particle all these options are useful to run using a single ltrans simulation multiple post processing scenarios in which the death or settlement of particles representing larvae depend on the encountered grain size or water properties when passing through the settlement polygons the option of running multiple post processing scenarios is of particular importance when dealing with the sensitivity analysis of the model parameterization which is always needed and recommended when modelling biologically driven processes gibson and spitz 2011 such as in larval connectivity studies a short user manual accompanies the code in the repository and presents the flags that the user can set to activate the various options described in this section 3 results and discussion 3 1 code optimization parallelization and other computational aspects the ltrans zlev code is written in fortran 90 the main time demanding loop of the present version has been parallelized with open mp directives see fig 1 the parallelization of other sections of the code like the i o sections or the oil module is not implemented yet but may be included in future versions the choice of open mp for a first parallel implementation was guided by the fact that this approach parallelizes the instructions while keeping the memory shared among the parallel threads this allows a balanced distribution of the workload among the threads while limiting the communication costs this kind of parallelization could be further enhanced by an additional mpi parallelization that could allow the increase of the memory available for the simulation by using multiple nodes but besides the obvious implementation cost this might lead to a sub optimal use of the resources in fact by using the domain decomposition the workload might become unequally divided among the processes when the distribution of the particles is not uniform over the domain and tends instead to concentrate in specific areas the option to parallelize with mpi using other memory partition schemes is not straightforward andwould in any case require a complex implementation to limit the time consuming communications among the processes since every particle needs to access different portions of the hydrodynamic and grid matrixes at every single time step of the simulation unlike the v 2b version ltrans zlev can read the input parameters from files the name of which can be selected by the user that are provided as an input argument to the software executable similarly the output files generated by the model are named and identified by means of a prefix which must be specified by the user in the input parameter file these upgrades allow to simplify the simultaneous run of multiple independent simulations giving the possibility to run multiple executions from the same directory in this way the user can change some parameters of the simulations for example release time or position random seed larval behaviour options oil spill weathering properties etc to perform sensitivity studies or to compute ensemble statistics to reduce the time needed to initialize the model the user can choose to store the matrixes of adjacent elements during the first run on a new domain and then re use this file instead of re computing the adjacent elements at every new run on the same domain the simulations performed in the benchmark presented in table 1 used these features to run multiple independent simulations launched simultaneously by means of distinct job submissions every job occupied part or the total number of cores of the node nevertheless the whole node had been reserved for the simulation table 1 presents the computational resources required by a simulation running on a computational node composed by two x86 64 intel r xeon r cpus with a total of 20 cores per node namely the cpu socket model is e5 2680 v2 2 80 ghz turbo speed 3 60 ghz with 10 cores for each socket 25 mb cache and 62 gb of ram available we used gfortran gcc version 4 8 2 to compile ltrans zlev together with the hdf5 and netcdf libraries five groups of simulations are presented in table 1 they are identified by the letters a b c d e and are characterized by different lagrangian parameters the particles in the lagrangian simulations do not interact with each other hence the modelling of large amounts of particles can be split into several simultaneous simulations for various sub groups of particles within each group table 1 compares the computational cost for several configurations of simulations including multiple simultaneous independent sequential simulations 1 5 or 20 simultaneous runs identified by the seq label a single open mp simulation using 20 threads omp 1 and multiple 5 7 or 10 simultaneous independent runs simultaneous open mp simulations using 4 or 2 threads each the parameters differentiating the groups b c d and e from the reference case a are identified in blue in table 1 the parameters of the lagrangian model highlighted in blue are those whose values have been increased to analyse their respective influence on the memory and the time requirements of the simulations two distinct grid sizes about 4 106 and 11 5 106 elements were tested and each set of simulations considered a total of either 2000 or 20000 particles tracked for 5 or 30 days group a is characterized by light simulations using the small grid 4 106 elements and running for only 5 days and 2000 particles as highlighted in blue in table 1 the simulations of group b and c differ from those of group a by respectively a higher number of particles and a larger number of grid elements the simulations of group d present both a high number of particles and a large number of grid elements while the simulations of the group e are the most demanding ones differing from those of group d by the length of the simulated time period 30 days instead of 5 the results of the comparative analysis between simulations characterized by various hardware or parallel configurations and lagrangian parameterizations is presented in the last three columns of table 1 the memory and time consumption of the simulations are highlighted by a colour scale varying from red to green to identify low to high efficiency of the simulations in terms of memory and runtime in particular we found that for the test cases presented in table 1 the memory required by the model is directly proportional to the size of the eulerian grid multiplied by the number of simultaneous runs and it is not impacted by the number of particles to be simulated for a higher number of particles the required memory would necessarily increase it is estimated that a maximum of 500 bytes of memory space is required per particle so that 2 million particles would need 1 gb of memory for large grids the memory cost limits the number of simulations that can be run simultaneously we choose for example a maximum of 7 simultaneous runs for the 11 5 106 elements domain the open mp parallelization allows a reduction of the computational time without any increase of the memory usage for example omp e3 has the same memory use as the seq e3 simulation either in sequential or in parallel mode unless a very few particles are released it is preferable to run a few simultaneous simulations for heavy simulations as much as it can be allowed by the available memory and reduce the number of particles per run then the use of the omp parallelization with an accurate number of threads reduces the time requested by the simulations the time required for the simulation real time can be minimized by choosing the optimal balance between the number of simultaneous simulations and the number of parallel threads in fact table 1 shows that given the particular computational node configuration and lagrangian parameters selected in this study the most efficient solution is to run 5 omp simultaneous simulations using 4 threads each simulations omp 2 conversely running either multiple sequential simulations seq or a single simulation with 20 parallel threads omp 1 or running 7 or 10 simultaneous simulations with 2 threads omp 3 is always more time consuming modelling the whole group of particles in a unique sequential simulation seq 1 is by far the most demanding solution and must be avoided 3 2 validation of the ltrans zlev implementation we verified the absence of setup and coding errors in ltrans zlev by running a comparison between the trajectories computed by the native online virtual float tracking module included in the mitgcm model and the particle trajectories computed by ltrans zlev driven by the same velocity fields provided by mitgcm mitgcm is a general circulation model that can solve either the hydrostatic or non hydrostatic navier stokes equations under the boussinesq approximation for an incompressible flow the non hydrostatic capability allows mitgcm to simulate oceanic processes over a wide range of scales the model is based on a finite volume discretization on a curvilinear horizontal grid with vertical z coordinate levels and partial cells for the bottom layers our test considered idealized flow conditions obtained by simulating a cyclonic gyre in a mid latitude closed basin cossarini et al 2017 the circulation structure is controlled by the shape of the bathymetry circular pit in a squared domain fig 5 and the flow is in geostrophic equilibrium the horizontal resolution of the hydrodynamic simulation is 1 128 for 256 256 grid points so that the domain size is approximately 170 220 km in the zonal and meridional direction respectively there are 60 vertical layers whose height varies from 1 5 m in the surface layer to about 60 m in the deepest region in the centre of the basin the model is forced by steady winds and a seasonal cycle of surface heat downward long wave and short wave radiation and mass precipitation fluxes the horizontal shear in the surface wind field maintains a permanent cyclonic gyre whereas the surface heat fluxes act on the thermohaline properties of the water column inducing a yearly cycle summer stratification winter mixing this simulation has been run for several years to reach steady state conditions perpetual year simulation in this study the bathymetry used by ltrans zlev considers the non interpolated depth in order to have as similar as possible boundary conditions as the mitgcm particle tracking flt i e float package both the mitgcm hydrodynamic simulation and float module as well as the ltrans implementation used a 100 seconds internal time discretization the simulation tracked for a 5 days period the paths of 1302 particles released at 0 75 m depth on a uniform horizontal grid to present a statistically relevant group of trajectories the set of 1302 particles was released at a daily frequency for 30 consecutive days and every set was followed over a 5 days long time interval the abbreviation mitflt refers to the simulation of particle trajectories computed by the mitgcm online virtual float tracking module the particle trajectories computed by ltrans zlev were driven by the velocity fields provided by the mitgcm with an hourly frequency in order to evaluate the influence of the hydrodynamic fields driving the ltrans particles the ltrans zlev simulations were performed six times using different sets of mitgcm hydrodynamic fields initially we tested whether it would be more accurate to use time averaged or instantaneous variables the first ltrans zlev case used a set of hourly averaged hydrodynamic fields identified as ltr hrav while the second ltrans zlev case was performed using a set of instantaneous states snapshots of the hydrodynamic field identified as ltr hrsn still with hourly frequency we also performed a third fourth and fifth experiment using hydrodynamic fields delayed by one time step 1 hour and 24 hours identified as ltr hrav dt ltr hrav 1hr ltr hrav 1day respectively in order to analyse the differences between the trajectories of particles released in the same point but at different moments and therefore advected by different hydrodynamic fields we finally performed a last test using the output velocity fields dumped at every time step in order to drive ltrans with exactly the same velocity fields used by the mitgcm online flt module fig 5 represents the computational domain the black arrows show the direction and intensity of the time averaged surface currents while yellow and green lines represent for the ltr hrav and mitflt simulations respectively the paths during the first two days of advection of a selection of particles released at the same moment to evaluate the intensity of the transport the average distance travelled by n particles is calculated through the following expression where x n t y n t z n t is the position of the particle n at time t a d 3 d t 1 n n 1 n t t 0 t x n t x n t 1 2 y n t y n t 1 2 z n t z n t 1 2 the difference between the particle trajectories of the various groups of the two simulations was quantified by computing for every group the three dimensional separation distance sd3d which is the average over all of the n particles distance between the two trajectories originated at the same time and starting from the same position s d 3 d t c a s e a c a s e b 1 n n 1 n x a n t x b n t 2 y a n t y b n t 2 z a n t z b n t 2 the three dimensional advection distance is represented by the thick grey dashed line in fig 6 b and 6 c for the mitflt particles only but it was almost identical for each of the ltrans cases i e advection distances are very similar fig 6b shows that the advection distance increases almost linearly in time reaching about 50 km after two days and 110 km after 5 days as the domain dimensions span about 170 and 220 km in the zonal and meridional direction respectively the size of the gyre allows at least a few days of free transport before the trajectory of the particles feel the presence of the solid boundaries being diverted by them the evolution in time of the separation distances between the various sets of particles in the different experiments after one and five days are represented by dashed coloured lines in fig 6b and c respectively fig 6 a represents with orange continuous lines the distance between the particle position computed by ltrans zlev ltr hrav simulation and the corresponding particle position computed by the mitgcm virtual float module at the end of each of the 5 days of advection the orange dashed lines in fig 6 b and c shows the evolution of the separation distance in time which increases almost exponentially reaching about 15 km after 5 days of simulation o 10 of the total advection distance we tested the sensitivity of ltrans to the different hydrodynamic fields by comparing the trajectories computed by using hourly averaged advection fields delayed by one time step 1 hour and one day cyan light blue and dark blue lines in fig 6 respectively a delay of only one time step i e 100 s gives the non negligible separation distance cyan lines in fig 6 of almost 2 km in 5 days 4320 timesteps highlighting the turbulent and chaotic features of the system a 1 hour delay gives similar results as the comparison between ltrans hourly averages and mitflt trajectories a delay of one day produces completely different and strongly divergent trajectories reaching a separation distance of 50 km in 5 days also the use of instantaneous ltr hrsn or averaged ltr hrav hourly advection fields give rise to a non negligible separation distance between ltrans trajectories red dashed lines in fig 6 b and c 3 km after 5 days the outcomes of the last test performed by using the mitgcm output velocity fields dumped at every time step purple dashed line in fig 6 b show that the results are comparable with the hourly averaged test case ltr hrav suggesting that the separation distance between ltrans zlev and mitflt particles is independent from the time averaging of the mitgcm velocity fields used to drive the ltrans simulations at least up to 1 hour this test was performed for only one day due to the large dataset made up by the mitgcm hydrodynamic output files dumped at every time step in light of these considerations and given that both tracer transport modules use the same 4th order runge kutta scheme for particle advection the difference between ltrans zlev and mitflt trajectories must be ascribed to the different interpolation methods of the velocity fields in our specific case and differently from ltrans zlev see subsection 2 3 the 3d advection of passive particles in mitgcm is obtained by interpolating the velocity fields using a trilinear scheme adcroft et al 2017 overall the results prove the effectiveness of the new implementation and the satisfactory accuracy of the advection algorithm 3 3 a realistic test case in order to illustrate and check the implementation of some additional features of this new release of the transport model we performed a simulation focused on the northern adriatic sea reproducing specific larval vertical behaviours and using the interpolated bathymetry configuration of ltrans zlev the new larval behaviours were implemented in ltrans zlev in the framework of the mantis project a complete description of the experiments is presented in m canu et al nephrops norvegicus connectivity model to support the design of efh in the adriatic sea in preparation where nephrops norvegicus larval dynamics and connectivity in the whole adriatic are studied in our test case the grids daily hydrodynamic fields and hourly space averaged short wave downward swd radiation data were provided by the mitgcm implementation characterized by a 1 64 horizontal resolution fig 7 shows the ltrans zlev interpolated bathymetry together with a representation of the paths of a few particles characterized by specific larval vertical behaviour the particles are initially released close to the seabed then an upward movement is superimposed to the hydrodynamic transport so that particles reach the upper layers where they start a diel vertical migration for 8 days before sinking back to the bottom afterwards they are transported along the seabed for further 12 days during the dvm phase the depth range of the target layers of the particles is 0 5 meters during the night and 25 30 meters during the day the values of the swd radiation thresholds were chosen analysing the time evolution of the swd radiation for the selected time period which depends on the cloud coverage as well as dawn and sunset timetables after the end of the dvm phase the particles sink to the bottom and are transported within a 1 m thick layer above the seabed with a velocity reduced by 90 to account for the bottom viscous boundary effects in fact given the absence of precise information regarding the height of the boundary layer or the height above the bottom at which larvae are transported it may be preferable to set the advection velocity of the particles to a percentage 10 in this case of the velocity of the currents calculated with the vertical tension spline described in section 2 3 as expected the global transport pattern shown in fig 7 highlights the typical dynamics of the adriatic sea governed by a basin scale cyclonic circulation it appears that during the dvm phase the particles perform longer advection distances when floating close to the surface than in the deeper target layer the reason lies in the higher intensity of the surface currents with respect to the deeper ones the effect of the wind is also included in the hydrodynamic flow fields provided by the mitgcm even though this ltrans zlev simulation was not parametrized to include additional wind forcing on the particles this simulation which includes biologically determined vertical behaviour would not have been possible in the framework of the built in mitgcm lagrangian modules nor in the standard ltrans v 2b version that does not allow the user to customize all the described parameters or to set light thresholds using the shortwave downward radiation fields 4 conclusions this paper presents ltrans zlev a lagrangian model working off line by reading bathymetry and flow fields of either σ or z coordinate hydrodynamic models ltrans zlev tracks marine particles dispersed by oceanographic currents and winds and can model oil spill transport and weathering through the oiltrans module furthermore ltrans zlev incudes additional features that were not present in ltrans v 2b among which the possibility to customize larval behaviour and environmental dependent behaviour of any transported agent such as vertical swimming speed temperature dependent survival and particle growth time dependent age diel vertical migration duration and light related thresholds the lagrangian tracking capabilities have been validated by a comparison with the results from the internal mitgcm float module and model features have been tested and exemplified in both an idealized case study and a more realistic setup the model runs in parallel on platforms using openmp also in sequential mode on standard workstations but it requires a computational power that is proportional to the domain size the possibility to work with either σ or z coordinates increases the versatility of this new version of ltrans as the choice of the vertical discretization of the hydrodynamic equations depends on many factors such as the topography of the basin or the kind of processes to be investigated moreover ltrans zlev allows the use of either a smooth interpolated bathymetry favourable to particle horizontal advection or a sharp bathymetry made by vertical cliff edges which may be preferred for simulations in basins presenting sub marine canyons or vertical underwater structures such as dikes or oil and gas extraction terminals in comparison with on line tracking tools ltrans zlev allows a remarkable reduction of the computational costs when performing multiple experiments or ensemble simulations or when adopting high resolution discretizations in time or space 5 software availability the ltrans zlev model is written in fortran 90 and parallelized with open mp directives the code is available on the repository http github com inogs ltrans zlev since 2019 the software needs as an input the hydrodynamic fields and grid files of an eulerian model based on arakawa c grids e g mitgcm roms etc ltrans zlev does not require particular hardware configurations its memory usage is proportional to the size of the mesh of the eulerian input files the software can be used either in sequential or parallel mode and requires the hdf5 netcdf and open mp libraries as the original v 2b version the ltrans zlev is freely available under an mit x licence and includes the mersenne twister random number generator and the tension spline curve fitting package tspack which have their own licenses and are freely available for non commercial uses declaration of competing interest this work is original and has not been published nor is it currently under consideration for publication elsewhere we also have no conflicts of interest to disclose after publication the code will be made public author of the original version of ltrans have been already informed about our extension and already sent us a positive feedback acknowledgments author contributions cl designed and developed the z discretization and parallelization of the code with contributions from cs and sq dmc defined the new behaviour added to the larval model and its parameterization and cl implemented them together with the updates of the oiltrans module cl performed the lagrangian simulations using ltrans zlev and sq performed the eulerian simulations with the mitgcm and the flt package particle tracking cl and cs wrote the manuscript with contributions from the other authors funding this work was partially funded by the project eu mantis dg mare 2014 41 under the theme marine protected areas network s for enhancement of sustainable fisheries in eu mediterranean waters by mise italian ministry of economic development in the frame of the program agreement on research on natural and anthropogenic risks for security and protection of offshore activities modeling marine hydrocarbon dispersal and by the hpc tres appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104621 
26072,this paper presents the release of ltrans zlev which is a new version of the off line lagrangian ocean particle tracking model ltrans v 2b that is compatible with a z coordinate constant depth layers discretization of the hydrodynamic equations the model capitalizes on and massively extends the capabilities of the original code ltrans which is already quite popular but can be used only adopting a sigma coordinate terrain following layers discretization among the additional features included in ltrans zlev there are the backward in time particle tracking algorithm and some new customizable larval behaviour options the new version also includes the oiltrans module for oil spill simulations the new implementations were validated by using the output of the z coordinate massachusetts institute of technology general circulation model mitgcm for an idealized case study describing a cyclonic gyre in a mid latitude closed basin another test case in which larval dispersal is modelled in the northern adriatic sea illustrates some of the new features of ltrans zlev graphical abstract image 1 keywords ltrans mitgcm lagrangian modelling larva oil spill 1 introduction lagrangian particle tracking algorithms provide a way to simulate analyse and describe oceanographic transport processes that can be of particular interest in a number of studies from larval transport cowen and sponaugle 2009 werner et al 2007 to pollution dispersion and water quality assessment james 2002 to oil spill modelling canu et al 2015 north et al 2011 in the lagrangian description pope 1985 taylor 1922 the transport processes are tracked by following fluid particles in order to identify the trajectories starting from or arriving at a given point the transport equations are discretized along a moving frame of reference and particle positions and velocities are considered also in between the eulerian grid points therefore lagrangian approaches provide a finer description of the transport processes compared with the one traditionally obtained in purely eulerian transport models in which the dynamics of the flow is described by computing the hydrodynamic properties at every grid element of a fixed geo reference grid however the lagrangian approach still requires the input of advection velocity fields which are used to update the particle position and must be computed in advance by an eulerian hydrodynamic model in an eulerian model the vertical discretization of the computational domain can be based mainly on three kinds of coordinate systems constant depth levels z terrain following σ layers or isopycnal ρ layers in the z case each layer is characterized by a constant depth over the whole domain and therefore the computational domain has more vertical layers in the deeper areas than in the shallower ones examples of such a kind of models are mom griffies et al 2000 2004 nemo madec and the nemo team 2016 and mitgcm adcroft et al 2017 marshall et al 1997a 1997b conversely in the terrain following σ models such as pom blumberg and mellor 1987 mellor 2003 and roms shchepetkin and mcwilliams 2009 the number of layers is the same all over the domain so that in deeper areas layers are thicker isopycnal models such as micom bleck et al 2002 use a vertical discretization based on the potential density ρ referenced to a given pressure value each discretization technique z σ or ρ has both advantages and disadvantages chassignet et al 1996 griffies et al 2000 and the choice between these approaches is not straightforward shapiro et al 2013 it depends on several factors such as the basin topography the relative importance of the various processes in a given study site and the numerical schemes implemented in the model shchepetkin and mcwilliams 2009 the z level discretization is usually more accurate in representing the surface mixed layer and the diabatic processes it includes while the σ coordinates allows a better representation of the bottom boundary layer isopycnal models are recommended for tracer advection and diffusion along sloping density surfaces in the ocean isopycnal models are less efficient in representing the bottom boundary layer and the surface mixing layer which are mostly unstratified therefore z or σ models may be preferred when simulating water basins whose dynamics are strongly influenced by the processes involved in these layers for basins showing remarkable variations of topography a z grid model is computationally more expensive than a σ model as it requires a higher number of vertical levels than a σ model to solve both the surface and bottom boundary layer dynamics blumberg and mellor 1987 conversely using σ coordinates may cause inconsistencies in the areas of sloping topography where numerical truncation errors in the computation of the pressure gradient force may become important and where the condition of hydrostatic consistency is more critical haney 1991 to handle such cases the vertical resolution of the σ grid must be increased mellor et al 1994 at the cost of reducing model performance most eulerian geophysical dynamics models already have their own native on line lagrangian tracer tracking modules however for studies involving multiple experiments large or high resolution time or space intervals or ensemble simulations the computational cost of using on line tracking modules can be very high and often not affordable since the whole hydrodynamic flow fields have to be recomputed for every realization on the contrary the use of off line lagrangian particle transport models allows to perform multiple transport modelling studies using a single set of stored flow fields with very affordable computational costs it also permits backtracking applications off line models can be used as long as it is assumed that the transported materials have no effect on the geophysical flow passive tracers and as long as the hydrodynamic stored fields have a sufficiently refined temporal resolution to approach high frequency phenomena the use of lagrangian models is indeed growing and recent reviews and books illustrating the wide use and popularity of the lagrangian approach in oceanography can be found in recent literature lynch et al 2014 prants et al 2017 van sebille et al 2018 among the principal off line lagrangian models we can cite ichthyop lett et al 2008 which was developed for σ coordinates systems parcels lange and sebille 2017 is instead compatible with both σ and z coordinates but does not yet include ready to use larval behaviour and oil spill weathering modules while cms paris et al 2013 offers these possibilities but it is compatible only with z grid models the lagrangian transport model ltrans ltrans v 2b is one of the most popular off line three dimensional particle tracking modules north et al 2013 schlag and north 2012 it has been developed to simulate the movement of passive tracers particles with sinking or floating behaviour like sediment or oil droplets and planktonic organisms like oyster larvae the source code is written in fortran 90 it includes the popular 4th order runge kutta scheme for particle advection and a random displacement model to account for sub grid scale turbulent diffusion ltrans v 2b estimates two dimensional water properties such as sea surface height or bottom depth at the particle location using bi linear interpolation instead three dimensional quantities fluid velocity temperature etc are estimated by see subsection 2 3 a interpolating them bi linearly at the particle horizontal location for every vertical level and b creating a water column profile using a tension spline curve north et al 2006 sticking and reflective boundary conditions on solid walls specific particle behaviour routines and settlement algorithms are also included the transport model can be used together with an oil spill weathering module named oiltrans and developed within ltrans v 2 berry et al 2012 oiltrans considers mechanical spreading advection and diffusion of the oil particles by winds and currents as well as evaporation emulsification and vertical dispersion since hydrodynamic fields are usually stored with a too low frequency to drive particle tracking algorithms ltrans considers the time interval between two fields e g hours or days as an external forcing time step the particle motion time scale is instead based on a much shorter internal time step e g minutes satisfying the cfl condition uδt δx 1 allowing particles to move over smaller space and time intervals in order to ensure a good accuracy of the solution at every internal time step the advection velocities at the particle position are interpolated in time and space from the eulerian fields and boundary condition algorithms ensure that the particles remain within the volume composed by the water cells of the domain as an output ltrans provides netcdf files containing the coordinates of the trajectories followed by the particles and if requested the flow field properties temperature salinity etc sampled along these trajectories the ltrans v 2b model was originally developed to run adopting the stored hydrodynamic fields produced by the regional ocean modeling system roms which is based on a terrain following σ layer discretization due to this reason ltrans v 2b employs grids bathymetry and velocity fields based on a σ coordinate system in this paper we present ltrans zlev which is a new version of ltrans that handles the grids and hydrodynamic flow fields of geophysical models based on a z coordinate discretization with staggered arakawa c grid the modification required an extensive editing of the code but it significantly improved and widened its use since many oceanographic models are indeed based on z levels furthermore several new features and options have been implemented and or included in the code the backward in time particle tracking capacity the capability of simulating coastal stranding in selected areas and at prescribed distances from the coast new larval behaviour options and customizable larval scenarios including near bottom release diel vertical migration followed by seabed transport the capacity to handle multiple independent i e not connected water basins and the oiltrans module for oil spill simulations the zlev version can handle z coordinates as well by adjusting the height of every level along the vertical coordinate according to the sea surface elevation changes furthermore ltrans zlev still offers the possibility to use the vertical layer discretization based on σ instead of z vertical coordinates the new version of ltrans described in this paper has been tested with the hydrodynamic flow fields provided by a set of mitgcm massachusetts institute of technology general circulation model simulations the code provided in the repository includes a short user manual a test case ready to be used and some pre processing tools to build the grids using the mitgcm outputs the details of the main developments implemented in ltrans zlev in order to handle the z levels of the mitgcm model instead of the σ discretization of roms are described in section 2 together with the new features available in the zlev version section 3 1 gives technical and computational information regarding ltrans zlev while section 3 2 presents a test case performed on an idealized cyclonic gyre in a mid latitude closed basin in which we compared particle trajectories computed by ltrans zlev off line coupled to the 3d eulerian flow fields provided by the mitgcm versus particle trajectories computed by the online native tracking module of the mitgcm then section 3 3 presents a realistic application considering particles with larval behaviour in which particles are released close to the seabed then they rise towards the surface perform diel vertical migration for a few days after which they sink back to the bottom where they are transported along the seabed searching for the environmental conditions suitable for settlement conclusions and future perspectives are finally drawn in section 4 2 model description 2 1 overview the original ltrans v 2b model comprises an initialization phase which builds the grids and boundaries of the water domain and a running phase loading the hydrodynamic flow fields at every external time step and then updating the particles positions at every internal time step in ltrans zlev the global structure of the ltrans v 2b code is unchanged but most parts of code handling the grids hydrodynamic flow fields and boundary conditions have been deeply rewritten besides new functions have been implemented to calculate for example the depth of the deepest grid nodes which define the z level bathymetry and to interpolate the bottom depth at the particle position leading to the new version of ltrans represented in the diagram of fig 1 in addition to the source code an external pre processing python tool included in the ltrans zlev package creates the z grid file required by ltrans zlev starting from the output file provided by the off line z level hydrodynamic model mitgcm in our case this external tool rewrites the grid coordinates and water land masks at every node of the arakawa c grid in a unique file in netcdf format as requested by the initial version of ltrans the native mitgcm binary output files or the merged in time ones are instead read directly by ltrans zlev at step 7 in fig 1 2 2 boundary conditions while tracking the movement of lagrangian particles particular care has to be given to the boundary conditions in fact it might happen that the numerical integration of a particle path i e the computation of where the interpolated velocity field projects a particle after a time step moves a particle outside the water volume of the computational domain for this reason it is fundamental to have well defined boundaries whose position and typology define what happens along the borders of the water volume 2 2 1 z level arakawa c grid most geophysical models such as roms or mitgcm use arakawa c grids where the scalar properties e g water density ρ and the u v and w components of velocity are defined on staggered nodes in the arakawa c staggered grid the ρ nodes are at the centre of cubic land or water cells while the u v and w nodes lie on the cell edges in the x y and z directions respectively fig 2 a b illustrate the vertical levels and arakawa c staggering respectively for a σ level grid and a z level grid without partial bottom cells in both cases every level along the vertical direction is made by a planar layer whose grid is defined by ρ u v and w nodes the horizontal position of the nodes does not vary with the vertical level meaning that the nodes of two different vertical levels have the same horizontal x y location but a different vertical z depth in every grid cell they store different values of the three dimensional flow fields at every vertical level the sea surface elevation and the bottom depth are stored at ρ node positions but they are two dimensional quantities depending only on the horizontal x y position as shown in fig 2 a b the water volume of the computational domain is enclosed by two kinds of boundaries the upper and lower boundaries made of horizontal planes delimiting the extremities of the water column i e sea surface and bottom and the lateral boundaries made of vertical planes delimiting the longitudinal and latitudinal extension of the basin 2 2 2 lateral boundaries σ level and z level grids represent lateral boundaries in a different way in σ coordinates the number of water cells at any vertical horizontal section remains constant but the height of each grid cell varies according to the local bottom depth the planar layers can tilt vertically at all depths to follow the bathymetry and the lateral land water boundaries are the same for all layers as shown in red in fig 2 a in contrast z grids layer boundaries change with depth fig 2 b and therefore different vertical transects might have a different number of wet i e non land cells according to the horizontal position of the transect and horizontal sections at different depths might have a different number of wet cells from the eulerian grids shown in fig 2 a b ltrans elaborates the lateral boundaries as illustrated in fig 3 a b for σ and z grids respectively by linking together at every level the u and v nodes at the interface between land and water cells to form a sequence of vertical planes that the particles cannot cross the nodes positioned at the centre of every element made by 4 adjacent ρ nodes are called e nodes their projection on the bottom is represented by the magenta circles in fig 3 ltrans v 2b considers the lateral borders of an ocean basin on a σ level grid as a series of vertical walls whose horizontal x y coordinates are identical at every vertical level fig 3 a shows that the water volume is enclosed by the same vertical red planes for any vertical level conversely for a z level grid the position of the lateral solid boundaries is instead dependent on the vertical level fig 3 b as a consequence on z level grids there might be multiple isolated sub basins at depth as well as water basins nested inside island areas due to the presence of rifts and trenches at the bottom of the sea ltrans zlev is designed to handle such kinds of water sub basins particles are often projected outside the water domain across a lateral boundary during the numerical integration of their position in such a case the particles will stop along the boundary to simulate for example coastal stranding of a pollutant or bottom contamination alternatively to simulate larval dispersion one might assume that the particles bounce against the boundary and they are reflected back into the water the two assumptions are labelled as sticking and reflective conditions respectively 2 2 3 upper and lower boundaries the sea surface height and the bottom depth at a particle location are estimated at every time step in order to maintain the particle within the upper and lower boundaries of the water column on a σ level grid ltrans v 2b interpolates these quantities by running bi linear interpolations of the sea surface height and the bottom depth represented by planar crosses and circles in fig 2 a respectively which are both two dimensional quantities stored at the 4 ρ nodes surrounding horizontally a particle by doing so when a particle is moved laterally in an area shallower than its previous position it is also risen up towards the surface so that it cannot by construction cross the bottom boundary i e flow below the deepest layer therefore in ltrans v 2b on a σ level grid the lateral boundary reflection conditions are applied only along the lateral vertical walls that is when the particles reach a lateral coastal land or island boundary represented in red in fig 2 a using a z grid with depth dependent lateral boundaries a direct linear interpolation of the bottom depth from the surrounding ρ nodes planar white circles in figs 2 b 3 b is no longer possible in fact such a seabed surface would pass across the deeper boundary walls and particles moving downward from one vertical level to another could find themselves trapped behind one of the vertical walls composing the lateral boundaries we implemented two possible configurations the first one uses full cells with a bathymetry made by horizontal planes and vertical walls while the second one is characterized by sloping surfaces and takes into account the partial cells in the first approach no partial cells the ltrans zlev bathymetry has to be compliant with the red boundary walls positioned along the u and v nodes represented in fig 3 b to do so a grid refinement is applied to the bottom boundary depth matrix originally defined at the position of the ρ nodes using the position of the ρ u v and e nodes as new grid points the depth of the bottom under the u v and e nodes is defined once for the whole simulation at step 4 of fig 1 it depends on the shape of the lateral boundaries at the various vertical levels the sharp bathymetry profile associated to the non interpolated z grid boundaries shown in fig 3 b causes multiple reflections along lateral walls slowing down the particles that run into bathymetry elevations this can be advantageous when running simulations for basins characterized by steep slopes in the bathymetry such as submarine canyons or when studying the interaction of marine particles with any kind of vertical man built underwater structure for example dikes or oil and gas extraction terminals unfortunately this approach cannot accommodate a partial bottom cells discretization of the eulerian grid a second alternative configuration of ltrans zlev allows to avoid this limitation by implementing partial cells and considering the seabed as made by sloping surfaces in this second case the partial cells discretization of an eulerian z grid like the one shown in fig 4 a is represented in the lagrangian simulation by a smoother interpolated bathymetry presenting tilted planes along the bottom surface such as the one illustrated in fig 4 b in this configuration the local depth is evaluated by a linear interpolation of the three nodes composing the triangle containing the particle with this approach the particles will rise whenever they encounter bathymetry elevations blue sloping surfaces in fig 4 b conversely they will experience reflections along the lateral boundaries red walls in fig 4 b only in the upper layers when moving across the continent or island borders this configuration may be preferred for simulations in most oceanic basins characterized by gentle slopes or flat grounds unlikely to retain particles when using the interpolated bathymetry the depth of the bottom under some of the ρ u v and e nodes is subject to a re computation at step 4 of fig 1 to reduce depth discontinuities ltrans zlev offers the possibility to use either the first non interpolated configuration represented in fig 3 b or the second interpolated configuration represented in fig 4 b if the user adopts the first non interpolated approach on a z grid containing partial bottom cells these partial cells are either converted into full cells or cancelled masked as land points by the model depending on the relative height of the partial cell 2 2 4 application of the boundary conditions the position of every particle is updated at every internal time step by computing the velocity associated to its former position and the related displacement within the integration time step step 16 of fig 1 the movement related to waves stokes drift wind step 17 of fig 1 and turbulence is added as well at step 19 of fig 1 the particle is reflected horizontally whenever the trajectory between the previous position and the new position crosses any lateral boundary of the vertical level associated to the earlier particle position 2 3 flow interpolation methods in ltrans v 2b the interpolation routines of the flow field used for example at step 16 are based on the identification of the 4 surrounding ρ nodes u v w nodes for the velocity fields respectively forming a squared element containing the particle e g the element formed by the white ρ nodes of coordinates i j i 1j i j and 2 2 in fig 2 a at every internal time step optimized algorithms searching in neighbouring elements determine the 2 d planar element in which the particle is located then among the 4 nodes composing the element itself the 3 nodes that are closest to the particle are selected defining a triangle containing the particle the interpolation coefficients are computed depending on the position of the particle relatively to these 3 nodes in a first phase these coefficients are used to interpolate horizontally the water properties at every vertical level to create a vertical water column profile at the particle horizontal position in a second phase the water properties are interpolated at the exact particle depth using 4 vertical levels 2 levels above 2 levels below the particle with the tspack tension spline curve fitting package renka 1993 since the tension spline curve is computed by considering the interpolated values in each layer for a z level mesh the different nodes to be considered for interpolations may have a different number of water layers in areas with uneven topography moreover some areas might have less than 4 vertical water levels above the bottom in the v 2b version 4 vertical levels are systematically used for the vertical profile while in the zlev version z level mesh the tension spline is computed only if at least 3 vertical levels are present at the particle position using at least 3 levels in case of only 2 vertical levels a linear interpolation is performed otherwise if the particle is above the last ρ cell centre or in case there is only one vertical level above the bottom the water properties are assumed to be those of the upper level instead of being linearly interpolated besides in the zlev version the user can choose to deactivate the tension spline curve fitting and use instead a simple linear interpolation to compute the water properties at the exact depth of the particle on z grids in areas of varied topography the fact that the different nodes to be considered for interpolations have a different number of water layers can affect also the particles transported near the bottom in fact such situations might lead to erroneous interpolation of the water properties near the bottom where the zero values stored in land nodes might be used to interpolate properties such as current velocities temperature and salinity the problem is solved by fictitiously assigning to each land node the water properties of its closest water nodes giving the properties of the upper node to the first land node under the bottom or averaging the properties of the water horizontal neighbours for deeper land nodes this procedure is applied every time a new hydrodynamic field is loaded to the quantities stored at the ρ nodes and to the horizontal current velocities stored along every lateral boundary in a second step to account for the boundary layer viscous effects as described in section 2 4 6 the model offers the possibility to parameterize the decrease of the speed of the particles which are slowed down as they approach the bottom 2 4 additional features 2 4 1 oil spill weathering and effects of wind and stokes drifts ltrans zlev integrates also the oiltrans oil spill weathering module developed within ltrans berry et al 2012 together with the effects of wind forcing on oil slicks the wind forcing used to calculate the drift of the particles is two dimensional varying on each grid element and interpolated in space for every particle in the zlev version the wind intensity used to compute the oil weathering processes of the oiltrans module is calculated at every time step by averaging over all the particles of the simulation the value of the wind intensity interpolated at every particle position at every time step the average over the particles of the water temperature and water column height are calculated in the same way and used to compute the weathering effect the user can choose which percentage of the instantaneous in time but spatially averaged over all the particles wind intensity will be used for the weathering processes as the weathering is highly responsive to wind this parameterization allows a better calibration of its intensity which is useful if we want to take into account and evaluate the effects of the temporal resolution of the forcing fields in fact wind forcing with high or low temporal resolution might show peaks of wind intensity or a relatively smooth variability respectively in this way the user can adapt the wind intensity term if the calibration of the various processes involved in the weathering has been done for wind fields sampled at a different temporal resolution furthermore in the zlev version wind forcing can be enabled even when the oil spill module is turned off this feature was not included in the original ltrans but it is quite useful to model wind effects on the dispersion of inertial floating material for objects with large portions floating above the sea level and consequently directly exposed to wind drag such as macroplastics additionally the amplitude of the wind and stokes drift as well as the deviation angle of the wind drift to the right hand side of the wind vector can now be customized by the user 2 4 2 tracking particles backward in time backward in time computations negative time step are possible in ltrans zlev by reading the hydrodynamic files in reverse order that is from the last one to the first one and using a negative time step to reverse the direction of the advection fields this feature can be used when the goal of the simulation is the identification of the source of the transported particles in order to estimate for example the release position of a pollutant or the coordinates of an accident when people or objects are found lost at the sea 2 4 3 stranding and settlement in addition to the possibility of simulating particle settlement by defining settlement polygons already included in ltrans v 2b ltrans zlev parameterizes also particle stranding by defining the distance from the coast within which a particle must be considered as stranded the user can also choose a preferential depth range for stranding or settlement these options increase the versatility of the model and allow to simulate a wider range of larval species which follow different settling scenarios 2 4 4 particle release from the bottom layer another new possibility offered by ltrans zlev is the release of particles at a given distance which can be customized above the seabed within this option the user must specify only the horizontal positions where the particles should be released and ltrans will compute for every particle the exact depth of its release point following the shape of the bathymetry this feature is useful for example when simulating the dispersal of larvae released by organisms living close to the bottom 2 4 5 larval behaviour with diel vertical migration and transport along the seabed in ltrans zlev an option allows the user to entirely customize the parameters of a new larval scenario representative of the behaviour of the larvae of several species the particles are released close to the seabed then rise towards the surface and perform diel vertical migration after which they sink to the bottom to be transported along the seabed by selecting this new behaviour the user can setup diel vertical migration dvm by linking in the input file the name of a data file containing time varying spatially average short wave downward swd radiation data for the computational domain and setting threshold values for the upward downward swimming phases of the larvae in other words to compute the temporal variability of the target depth range of the particles ltrans zlev reads the space averaged swd radiation file extracted from the eulerian model and calculates the instantaneous swd radiation intensity then the model checks if the value is higher than the sunrise threshold or lower than the sunset threshold depending on the current swd radiation ltrans zlev determines whether the target layer of the particles is the daily one or the nightly one both threshold values for sunrise and sunset as well as the vertical swimming speeds and the depth of the day night target layers can be entirely set by the user together with the length of the dvm period when the simulated time exceeds the dvm period the particles sink to the bottom and are transported along the seabed remaining within a target layer the maximum height above the bottom of this target layer can be set by the user at any moment the particles that are located outside their target layer range due to the advection by the currents or when going from one target layer to another are pushed back towards it using a vertical component of swimming speed which is superimposed to the hydrodynamic transport the development of this new behaviour option widens the applicability of the new version of ltrans the possibility to customize its parameters makes it suitable for modelling different species in the same environmental conditions the zlev version still includes the larval scenarios already implemented in the ltrans v 2b which were specifically parameterized to model c virginica and c ariakensis oysters for other species near surface or near bottom transport and diel vertical migration based on time since midnight instead of short wave downward radiation fields can still be modelled as single behaviours maintained for the whole simulation as it was already implemented in ltrans v 2b 2 4 6 advection velocity in the boundary layer the height of the bottom boundary layer and the advection velocity of any particle entering it can be set by the user within the selected boundary layer the advection of particles transported near the bottom is reduced by applying either the logarithmic law of the wall already implemented in the v 2b version or a percentage of the ambient flow velocity 2 4 7 diagnostic outputs along particle paths if set by the user when printing the output files containing particle paths ltrans can extract instantaneous flow field properties which are critical for larval behaviour such as temperature and salinity at every particle position the zlev version offers also the possibility to extract the minimum or maximum values besides the instantaneous values also available in ltrans v 2b encountered by the particles since the last output of the lagrangian model moreover in ltrans zlev a sediment grain size map defined on the same grid as the hydrodynamic model can be loaded then the instantaneous minimum or maximum values of the grain size encountered by the particle since the last extraction can be written together with the temperature and salinity values finally the presence of a particle within a polygon can be tracked independently from any settlement behaviour by storing at every instant the identification number of the polygon containing the particle all these options are useful to run using a single ltrans simulation multiple post processing scenarios in which the death or settlement of particles representing larvae depend on the encountered grain size or water properties when passing through the settlement polygons the option of running multiple post processing scenarios is of particular importance when dealing with the sensitivity analysis of the model parameterization which is always needed and recommended when modelling biologically driven processes gibson and spitz 2011 such as in larval connectivity studies a short user manual accompanies the code in the repository and presents the flags that the user can set to activate the various options described in this section 3 results and discussion 3 1 code optimization parallelization and other computational aspects the ltrans zlev code is written in fortran 90 the main time demanding loop of the present version has been parallelized with open mp directives see fig 1 the parallelization of other sections of the code like the i o sections or the oil module is not implemented yet but may be included in future versions the choice of open mp for a first parallel implementation was guided by the fact that this approach parallelizes the instructions while keeping the memory shared among the parallel threads this allows a balanced distribution of the workload among the threads while limiting the communication costs this kind of parallelization could be further enhanced by an additional mpi parallelization that could allow the increase of the memory available for the simulation by using multiple nodes but besides the obvious implementation cost this might lead to a sub optimal use of the resources in fact by using the domain decomposition the workload might become unequally divided among the processes when the distribution of the particles is not uniform over the domain and tends instead to concentrate in specific areas the option to parallelize with mpi using other memory partition schemes is not straightforward andwould in any case require a complex implementation to limit the time consuming communications among the processes since every particle needs to access different portions of the hydrodynamic and grid matrixes at every single time step of the simulation unlike the v 2b version ltrans zlev can read the input parameters from files the name of which can be selected by the user that are provided as an input argument to the software executable similarly the output files generated by the model are named and identified by means of a prefix which must be specified by the user in the input parameter file these upgrades allow to simplify the simultaneous run of multiple independent simulations giving the possibility to run multiple executions from the same directory in this way the user can change some parameters of the simulations for example release time or position random seed larval behaviour options oil spill weathering properties etc to perform sensitivity studies or to compute ensemble statistics to reduce the time needed to initialize the model the user can choose to store the matrixes of adjacent elements during the first run on a new domain and then re use this file instead of re computing the adjacent elements at every new run on the same domain the simulations performed in the benchmark presented in table 1 used these features to run multiple independent simulations launched simultaneously by means of distinct job submissions every job occupied part or the total number of cores of the node nevertheless the whole node had been reserved for the simulation table 1 presents the computational resources required by a simulation running on a computational node composed by two x86 64 intel r xeon r cpus with a total of 20 cores per node namely the cpu socket model is e5 2680 v2 2 80 ghz turbo speed 3 60 ghz with 10 cores for each socket 25 mb cache and 62 gb of ram available we used gfortran gcc version 4 8 2 to compile ltrans zlev together with the hdf5 and netcdf libraries five groups of simulations are presented in table 1 they are identified by the letters a b c d e and are characterized by different lagrangian parameters the particles in the lagrangian simulations do not interact with each other hence the modelling of large amounts of particles can be split into several simultaneous simulations for various sub groups of particles within each group table 1 compares the computational cost for several configurations of simulations including multiple simultaneous independent sequential simulations 1 5 or 20 simultaneous runs identified by the seq label a single open mp simulation using 20 threads omp 1 and multiple 5 7 or 10 simultaneous independent runs simultaneous open mp simulations using 4 or 2 threads each the parameters differentiating the groups b c d and e from the reference case a are identified in blue in table 1 the parameters of the lagrangian model highlighted in blue are those whose values have been increased to analyse their respective influence on the memory and the time requirements of the simulations two distinct grid sizes about 4 106 and 11 5 106 elements were tested and each set of simulations considered a total of either 2000 or 20000 particles tracked for 5 or 30 days group a is characterized by light simulations using the small grid 4 106 elements and running for only 5 days and 2000 particles as highlighted in blue in table 1 the simulations of group b and c differ from those of group a by respectively a higher number of particles and a larger number of grid elements the simulations of group d present both a high number of particles and a large number of grid elements while the simulations of the group e are the most demanding ones differing from those of group d by the length of the simulated time period 30 days instead of 5 the results of the comparative analysis between simulations characterized by various hardware or parallel configurations and lagrangian parameterizations is presented in the last three columns of table 1 the memory and time consumption of the simulations are highlighted by a colour scale varying from red to green to identify low to high efficiency of the simulations in terms of memory and runtime in particular we found that for the test cases presented in table 1 the memory required by the model is directly proportional to the size of the eulerian grid multiplied by the number of simultaneous runs and it is not impacted by the number of particles to be simulated for a higher number of particles the required memory would necessarily increase it is estimated that a maximum of 500 bytes of memory space is required per particle so that 2 million particles would need 1 gb of memory for large grids the memory cost limits the number of simulations that can be run simultaneously we choose for example a maximum of 7 simultaneous runs for the 11 5 106 elements domain the open mp parallelization allows a reduction of the computational time without any increase of the memory usage for example omp e3 has the same memory use as the seq e3 simulation either in sequential or in parallel mode unless a very few particles are released it is preferable to run a few simultaneous simulations for heavy simulations as much as it can be allowed by the available memory and reduce the number of particles per run then the use of the omp parallelization with an accurate number of threads reduces the time requested by the simulations the time required for the simulation real time can be minimized by choosing the optimal balance between the number of simultaneous simulations and the number of parallel threads in fact table 1 shows that given the particular computational node configuration and lagrangian parameters selected in this study the most efficient solution is to run 5 omp simultaneous simulations using 4 threads each simulations omp 2 conversely running either multiple sequential simulations seq or a single simulation with 20 parallel threads omp 1 or running 7 or 10 simultaneous simulations with 2 threads omp 3 is always more time consuming modelling the whole group of particles in a unique sequential simulation seq 1 is by far the most demanding solution and must be avoided 3 2 validation of the ltrans zlev implementation we verified the absence of setup and coding errors in ltrans zlev by running a comparison between the trajectories computed by the native online virtual float tracking module included in the mitgcm model and the particle trajectories computed by ltrans zlev driven by the same velocity fields provided by mitgcm mitgcm is a general circulation model that can solve either the hydrostatic or non hydrostatic navier stokes equations under the boussinesq approximation for an incompressible flow the non hydrostatic capability allows mitgcm to simulate oceanic processes over a wide range of scales the model is based on a finite volume discretization on a curvilinear horizontal grid with vertical z coordinate levels and partial cells for the bottom layers our test considered idealized flow conditions obtained by simulating a cyclonic gyre in a mid latitude closed basin cossarini et al 2017 the circulation structure is controlled by the shape of the bathymetry circular pit in a squared domain fig 5 and the flow is in geostrophic equilibrium the horizontal resolution of the hydrodynamic simulation is 1 128 for 256 256 grid points so that the domain size is approximately 170 220 km in the zonal and meridional direction respectively there are 60 vertical layers whose height varies from 1 5 m in the surface layer to about 60 m in the deepest region in the centre of the basin the model is forced by steady winds and a seasonal cycle of surface heat downward long wave and short wave radiation and mass precipitation fluxes the horizontal shear in the surface wind field maintains a permanent cyclonic gyre whereas the surface heat fluxes act on the thermohaline properties of the water column inducing a yearly cycle summer stratification winter mixing this simulation has been run for several years to reach steady state conditions perpetual year simulation in this study the bathymetry used by ltrans zlev considers the non interpolated depth in order to have as similar as possible boundary conditions as the mitgcm particle tracking flt i e float package both the mitgcm hydrodynamic simulation and float module as well as the ltrans implementation used a 100 seconds internal time discretization the simulation tracked for a 5 days period the paths of 1302 particles released at 0 75 m depth on a uniform horizontal grid to present a statistically relevant group of trajectories the set of 1302 particles was released at a daily frequency for 30 consecutive days and every set was followed over a 5 days long time interval the abbreviation mitflt refers to the simulation of particle trajectories computed by the mitgcm online virtual float tracking module the particle trajectories computed by ltrans zlev were driven by the velocity fields provided by the mitgcm with an hourly frequency in order to evaluate the influence of the hydrodynamic fields driving the ltrans particles the ltrans zlev simulations were performed six times using different sets of mitgcm hydrodynamic fields initially we tested whether it would be more accurate to use time averaged or instantaneous variables the first ltrans zlev case used a set of hourly averaged hydrodynamic fields identified as ltr hrav while the second ltrans zlev case was performed using a set of instantaneous states snapshots of the hydrodynamic field identified as ltr hrsn still with hourly frequency we also performed a third fourth and fifth experiment using hydrodynamic fields delayed by one time step 1 hour and 24 hours identified as ltr hrav dt ltr hrav 1hr ltr hrav 1day respectively in order to analyse the differences between the trajectories of particles released in the same point but at different moments and therefore advected by different hydrodynamic fields we finally performed a last test using the output velocity fields dumped at every time step in order to drive ltrans with exactly the same velocity fields used by the mitgcm online flt module fig 5 represents the computational domain the black arrows show the direction and intensity of the time averaged surface currents while yellow and green lines represent for the ltr hrav and mitflt simulations respectively the paths during the first two days of advection of a selection of particles released at the same moment to evaluate the intensity of the transport the average distance travelled by n particles is calculated through the following expression where x n t y n t z n t is the position of the particle n at time t a d 3 d t 1 n n 1 n t t 0 t x n t x n t 1 2 y n t y n t 1 2 z n t z n t 1 2 the difference between the particle trajectories of the various groups of the two simulations was quantified by computing for every group the three dimensional separation distance sd3d which is the average over all of the n particles distance between the two trajectories originated at the same time and starting from the same position s d 3 d t c a s e a c a s e b 1 n n 1 n x a n t x b n t 2 y a n t y b n t 2 z a n t z b n t 2 the three dimensional advection distance is represented by the thick grey dashed line in fig 6 b and 6 c for the mitflt particles only but it was almost identical for each of the ltrans cases i e advection distances are very similar fig 6b shows that the advection distance increases almost linearly in time reaching about 50 km after two days and 110 km after 5 days as the domain dimensions span about 170 and 220 km in the zonal and meridional direction respectively the size of the gyre allows at least a few days of free transport before the trajectory of the particles feel the presence of the solid boundaries being diverted by them the evolution in time of the separation distances between the various sets of particles in the different experiments after one and five days are represented by dashed coloured lines in fig 6b and c respectively fig 6 a represents with orange continuous lines the distance between the particle position computed by ltrans zlev ltr hrav simulation and the corresponding particle position computed by the mitgcm virtual float module at the end of each of the 5 days of advection the orange dashed lines in fig 6 b and c shows the evolution of the separation distance in time which increases almost exponentially reaching about 15 km after 5 days of simulation o 10 of the total advection distance we tested the sensitivity of ltrans to the different hydrodynamic fields by comparing the trajectories computed by using hourly averaged advection fields delayed by one time step 1 hour and one day cyan light blue and dark blue lines in fig 6 respectively a delay of only one time step i e 100 s gives the non negligible separation distance cyan lines in fig 6 of almost 2 km in 5 days 4320 timesteps highlighting the turbulent and chaotic features of the system a 1 hour delay gives similar results as the comparison between ltrans hourly averages and mitflt trajectories a delay of one day produces completely different and strongly divergent trajectories reaching a separation distance of 50 km in 5 days also the use of instantaneous ltr hrsn or averaged ltr hrav hourly advection fields give rise to a non negligible separation distance between ltrans trajectories red dashed lines in fig 6 b and c 3 km after 5 days the outcomes of the last test performed by using the mitgcm output velocity fields dumped at every time step purple dashed line in fig 6 b show that the results are comparable with the hourly averaged test case ltr hrav suggesting that the separation distance between ltrans zlev and mitflt particles is independent from the time averaging of the mitgcm velocity fields used to drive the ltrans simulations at least up to 1 hour this test was performed for only one day due to the large dataset made up by the mitgcm hydrodynamic output files dumped at every time step in light of these considerations and given that both tracer transport modules use the same 4th order runge kutta scheme for particle advection the difference between ltrans zlev and mitflt trajectories must be ascribed to the different interpolation methods of the velocity fields in our specific case and differently from ltrans zlev see subsection 2 3 the 3d advection of passive particles in mitgcm is obtained by interpolating the velocity fields using a trilinear scheme adcroft et al 2017 overall the results prove the effectiveness of the new implementation and the satisfactory accuracy of the advection algorithm 3 3 a realistic test case in order to illustrate and check the implementation of some additional features of this new release of the transport model we performed a simulation focused on the northern adriatic sea reproducing specific larval vertical behaviours and using the interpolated bathymetry configuration of ltrans zlev the new larval behaviours were implemented in ltrans zlev in the framework of the mantis project a complete description of the experiments is presented in m canu et al nephrops norvegicus connectivity model to support the design of efh in the adriatic sea in preparation where nephrops norvegicus larval dynamics and connectivity in the whole adriatic are studied in our test case the grids daily hydrodynamic fields and hourly space averaged short wave downward swd radiation data were provided by the mitgcm implementation characterized by a 1 64 horizontal resolution fig 7 shows the ltrans zlev interpolated bathymetry together with a representation of the paths of a few particles characterized by specific larval vertical behaviour the particles are initially released close to the seabed then an upward movement is superimposed to the hydrodynamic transport so that particles reach the upper layers where they start a diel vertical migration for 8 days before sinking back to the bottom afterwards they are transported along the seabed for further 12 days during the dvm phase the depth range of the target layers of the particles is 0 5 meters during the night and 25 30 meters during the day the values of the swd radiation thresholds were chosen analysing the time evolution of the swd radiation for the selected time period which depends on the cloud coverage as well as dawn and sunset timetables after the end of the dvm phase the particles sink to the bottom and are transported within a 1 m thick layer above the seabed with a velocity reduced by 90 to account for the bottom viscous boundary effects in fact given the absence of precise information regarding the height of the boundary layer or the height above the bottom at which larvae are transported it may be preferable to set the advection velocity of the particles to a percentage 10 in this case of the velocity of the currents calculated with the vertical tension spline described in section 2 3 as expected the global transport pattern shown in fig 7 highlights the typical dynamics of the adriatic sea governed by a basin scale cyclonic circulation it appears that during the dvm phase the particles perform longer advection distances when floating close to the surface than in the deeper target layer the reason lies in the higher intensity of the surface currents with respect to the deeper ones the effect of the wind is also included in the hydrodynamic flow fields provided by the mitgcm even though this ltrans zlev simulation was not parametrized to include additional wind forcing on the particles this simulation which includes biologically determined vertical behaviour would not have been possible in the framework of the built in mitgcm lagrangian modules nor in the standard ltrans v 2b version that does not allow the user to customize all the described parameters or to set light thresholds using the shortwave downward radiation fields 4 conclusions this paper presents ltrans zlev a lagrangian model working off line by reading bathymetry and flow fields of either σ or z coordinate hydrodynamic models ltrans zlev tracks marine particles dispersed by oceanographic currents and winds and can model oil spill transport and weathering through the oiltrans module furthermore ltrans zlev incudes additional features that were not present in ltrans v 2b among which the possibility to customize larval behaviour and environmental dependent behaviour of any transported agent such as vertical swimming speed temperature dependent survival and particle growth time dependent age diel vertical migration duration and light related thresholds the lagrangian tracking capabilities have been validated by a comparison with the results from the internal mitgcm float module and model features have been tested and exemplified in both an idealized case study and a more realistic setup the model runs in parallel on platforms using openmp also in sequential mode on standard workstations but it requires a computational power that is proportional to the domain size the possibility to work with either σ or z coordinates increases the versatility of this new version of ltrans as the choice of the vertical discretization of the hydrodynamic equations depends on many factors such as the topography of the basin or the kind of processes to be investigated moreover ltrans zlev allows the use of either a smooth interpolated bathymetry favourable to particle horizontal advection or a sharp bathymetry made by vertical cliff edges which may be preferred for simulations in basins presenting sub marine canyons or vertical underwater structures such as dikes or oil and gas extraction terminals in comparison with on line tracking tools ltrans zlev allows a remarkable reduction of the computational costs when performing multiple experiments or ensemble simulations or when adopting high resolution discretizations in time or space 5 software availability the ltrans zlev model is written in fortran 90 and parallelized with open mp directives the code is available on the repository http github com inogs ltrans zlev since 2019 the software needs as an input the hydrodynamic fields and grid files of an eulerian model based on arakawa c grids e g mitgcm roms etc ltrans zlev does not require particular hardware configurations its memory usage is proportional to the size of the mesh of the eulerian input files the software can be used either in sequential or parallel mode and requires the hdf5 netcdf and open mp libraries as the original v 2b version the ltrans zlev is freely available under an mit x licence and includes the mersenne twister random number generator and the tension spline curve fitting package tspack which have their own licenses and are freely available for non commercial uses declaration of competing interest this work is original and has not been published nor is it currently under consideration for publication elsewhere we also have no conflicts of interest to disclose after publication the code will be made public author of the original version of ltrans have been already informed about our extension and already sent us a positive feedback acknowledgments author contributions cl designed and developed the z discretization and parallelization of the code with contributions from cs and sq dmc defined the new behaviour added to the larval model and its parameterization and cl implemented them together with the updates of the oiltrans module cl performed the lagrangian simulations using ltrans zlev and sq performed the eulerian simulations with the mitgcm and the flt package particle tracking cl and cs wrote the manuscript with contributions from the other authors funding this work was partially funded by the project eu mantis dg mare 2014 41 under the theme marine protected areas network s for enhancement of sustainable fisheries in eu mediterranean waters by mise italian ministry of economic development in the frame of the program agreement on research on natural and anthropogenic risks for security and protection of offshore activities modeling marine hydrocarbon dispersal and by the hpc tres appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104621 
26073,multi spectral spaceborne sensors with different spatial resolutions produce earth observation eo time series ts with global coverage the interactive visualization and interpretation of ts is essential to better understand changes in land use and land cover and to extract reference information for model calibration and validation however available software tools are often limited to specific sensors or optimized for application specific visualizations to overcome these limitations we developed the eo time series viewer a free and open source qgis plugin for user friendly visualization interpretation and labeling of multi sensor ts data the eo time series viewer i combines advantages of spatial spectral and temporal data visualization concepts that are so far not available in a single tool ii provides maximum flexibility in terms of supported data formats iii minimizes the user interactions required to load and visualize multi sensor ts data and iv speeds up labeling of ts data based on enhanced gis vector tools and formats keywords change detection training validation open source qgis plugin eo time series viewer software availability name of software eo time series viewer developer benjamin jakimow earth observation lab humboldt universität zu berlin germany contact benjamin jakimow geo hu berlin de source language python program language english program size 5 mbyte including example data license gnu gpl 3 required software qgis geoinformation system https qgis org required hardware any hardware that allows to run qgis year first available 2018 as official qgis plugin source code public git repository with issue tracker https bitbucket org jakimowb eo time series viewer documentation https eo time series viewer readthedocs io installation via qgis plugin manager 1 introduction an increasing number of sensors on spaceborne satellite platforms are collecting earth observation eo data with global coverage and at varying temporal resolutions belward and skøien 2015 e g the sentinel 2a b s2 multispectral instrument msi drusch et al 2012 gascon et al 2017 and landsat 8 l8 operational land imager oli roy et al 2014 wulder et al 2019 that provide raster data with 10 30 m spatial resolution and 16 l8 respectively 5 days s2 temporal resolution such earth observation eo time series ts data allow to map land cover and land use lclu and related changes gómez et al 2016 joshi et al 2016 which may occur at different spatial and temporal scales these processes include e g urban growth and dynamics liu et al 2018 schug et al 2018 zhou et al 2018 crop cycles and agricultural intensification belgiu and csillik 2018 griffiths et al 2019 roy and yan 2018 wildfires and burning hawbaker et al 2017 roy et al 2019 villarreal et al 2016 deforestation and forest management griffiths et al 2018 hansen et al 2013 or trends and changes in plant phenology jönsson et al 2018 open data policies the ongoing consolidation of historical data archives and the provision of analysis ready data ard ceos 2019 egorov et al 2018 frantz 2019 with already preprocessed and atmospherically corrected imagery improved the accessibility and usability of recent and historical ts claverie et al 2018 zhu et al 2019 at the same time advances in computational technologies allow to process even very large volumes of raster data efficiently this pushed ts data analysis towards a large area and dense time series monitoring of long lasting land cover and land use changes griffiths et al 2012 hansen et al 2013 and into the domain of big data analysis gorelick et al 2017 lewis et al 2017 liu et al 2018 methods from the field of machine learning like image classification and quantitative process analysis cracknell and reading 2014 lary et al 2016 zhu et al 2017 zhu 2017 can analyze the high informational content of ts efficiently and on local or cloud based computer infrastructures nevertheless the visual interpretation of ts data to better understand lclu change and to create reference data remains an important yet complex and mostly time consuming step for such analyses experienced image interpreters evaluate and contextualize the spectral spatial and temporal characteristics of the ts data to identify or exclude lclu changes they incorporate auxiliary data e g very high resolution vhr imagery topographic maps statistical surveys or data from field studies finally the results of the visual interpretation are used to create new reference data sets e g by tagging points or areas of interest i e single pixels or groups of neighbored pixels with their respective attributes such attributes in the following labels can be a category of land cover or land cover change a time stamp or more detailed information in writing on what the visual interpretation revealed reference labels are required to calibrate or parameterize supervised classification and change detection approaches as well as regression approaches that allow quantifying extracted information high quality reference data is also mandatory for a statistically sound assessment of map accuracy olofsson et al 2013 2014 and of the temporal accuracy of automated change detection algorithms dara et al 2018 especially if the timing of changes has to be included in the reference data with utmost precision many eo raster images need to be browsed and visualized with the highest possible temporal resolution this demands for specialized software that provides efficient and easy to use visualization and labeling of ts raster data which as we will show in the next section rarely exists 1 1 existing software software that is used in the eo science community for visualizing and partly for labeling eo data might be differentiated by their development history into i geographical information systems gis with a strength in the analysis of vector data ii digital image processing dip software with a focus on the analysis of raster based remote sensing data and iii software specifically developed for eo time series data analysis tsa existing software offer different capabilities if evaluated with specific regard to the loading of ts data i e the opening of many raster images an interactive visualization of its spatial spectral and temporal dimensions and the software availability e g license model and costs table 1 gis like arcgis and qgis are well suited to visualize heterogeneous spatial data from local sources or web services they traditionally organize the rendering of spatial data in a layer stack and offer a very flexible control for data visualization typically gis support on the fly re projections which eases a visualization of geo data from different projection systems dip like erdas imagine the environment for visualizing images envi or the sentinel application platform snap have a long development history and provide a rich set of features to visualize the spatial and spectral dimensions of eo data they visualize raster images in multiple maps mm side by side or as an image stack which are geographically linked and allow comparing spatial patterns they show spectral profiles sp for single pixel positions to compare the spectral responses of different surfaces e g vegetation vs artificial materials and save profiles in spectral libraries specialized on remote sensing raster formats and workflows dip value sensor specific meta data e g band wavelength information to identify the raster image bands for a true color band combination gis usually provide a rich toolset for labeling eo data based on vector geometries points lines or polygons spatial objects can flexibly be described as geometries and each geometry can be annotated with various attributes i e fields with numeric values or text this information can be saved to vector formats e g geopackage open geospatial consortium 2018 or the quasi standard esri shapefile esri 1998 to interchange label information with other software in contrary dip often use individual formats to label eo data e g domain specific xml and binary formats that need to be transformed into other formats to be shared with other software furthermore virtual raster images vi e g the geospatial data abstraction library gdal virtual format vrt gdal ogr contributors 2019 are emerging which allow to reorganize data without the need to reorganize image data physically for example single band raster datasets can be virtually combined into a multi multiband dataset with unified spatial resolution band names and additional metadata information for visualization in a gis without copying of actual image data working with ts data creates special needs for data visualization and management in addition to the capabilities brought by standard dip and gis software users of ts data are often confronted with a multitude of physical raster images and software specific to visualize ts preferable accounts for that by recognizing observation dates from file names or image metadata the eo science community therefore developed tsa tools like timesync cohen et al 2010 timestats udelhoven 2011 timesat eklundh and jönsson 2016 tstools holden 2016 or collect earth bey et al 2016 tsa tools have an explicit concept of raster time series they are usually strong with regard to a certain field of application or the characteristics of a single sensor e g timesync for landsat time series and timesat for deriving vegetation phenology from modis time series or more recently also for landsat and sentinel 2 data jönsson et al 2018 collect earth on the other hand focusses on the labeling of pre sampled plots for which extensive dialogs can be customized to enter label attributes collect earth is based on the google earth https www google com earth interface and makes use of eo data stored in or uploaded to the google earth engine gee gorelick et al 2017 archives uploading data to cloud services might be constrained by legal limits of data usage agreements e g in case of commercial vhr data sources and is problematic for larger e g terabyte scale data sources furthermore web based tools cannot be used offline of e g during field work however the focus of existing tsa software often comes with trade offs in terms of broad applicability e g a need for unified file formats file naming and map projection in general there is a lack of applicability to heterogeneous multi sensor ts e g to simultaneously inspect two or more ts from sensors with different spatial spectral and temporal resolutions moreover existing tools for labeling ts rarely adequately value the temporal context e g the need to save time stamps and file paths related to the observations on which a labeling decision is based 1 2 eo time series viewer software that combines the advantages of different concepts from the worlds of dip gis and tsa to visualize and label ts data in a data agnostic way i e the support of a direct reading of different raster formats and in particular data from multiple sensors is so far not available therefore we summarized the strengths and weaknesses of existing software table 1 plus demands that arose during our own work to compile a list of desired software capabilities and requirements table 2 based on that list we conceptually laid out the eo time series viewer eotsv fig 1 the main goal of the eotsv is to enable and simplify the visualization and labeling of ts as well as to accelerate these often time consuming steps in scientific analyses in particular our aims were 1 to combine different visualization concepts that are well known from established dip gis and tsa to visualize the spatial spectral and temporal domains of multi sensor ts 2 to provide maximum flexibility in terms of supported sensors data sources and image properties e g different data projections and file formats 3 to minimize the number of user interactions mouse clicks that is required to load and visualize ts data 4 to ease the labeling and extraction of information from ts based on standard gis vector and dip spectral library formats respectively for good interoperability with other software and 5 to allow for an offline visualization of ts e g during fieldwork and in regions with poor internet access in the following we describe the features of the eotsv i e how ts data is provided and loaded section 2 how spatial temporal and spectral data information content is visualized section 3 and how the eotsv eases the labeling of ts data section 4 we then provide an overview on case studies where the eotsv was used to assess ts data section 5 show a benchmark section 6 and discuss the usability and potential future developments of the eotsv section 7 2 data model software for visualizing ts often demands the adaptation of raster data to specific structural properties e g file formats naming conventions or a shared map projection this often requires additional preprocessing of the input data and its metadata often at the expense of the original spatial or temporal resolution of the data the eotsv mitigates such preprocessing by supporting a wide range of data formats and evaluating the relevant metadata for required information retrieval the eotsv therefore provides a flexibility that to our best knowledge is not offered by any other comparable tool valid raster inputs for the eotsv multi sensor time series model fig 2 are geolocated raster images that can be read with the gdal and define a date time group dtg 2 1 geolocated raster images based on gdal and the qgis api the eotsv currently reads more than 140 different raster formats gdal ogr contributors 2019 e g file formats like tiff and jpeg2000 as well as web sources with web coverage wcs and web map services wms source images are visualized independently from differences in coordinate reference systems spatial extents or number of bands this avoids that large amounts of ts data need first to be preprocessed into a unified raster format raster dimensions or projection system if necessary vrt images can be used for re organizing input raster files and its metadata without changing the original data sources this is useful e g for stacking single band data sources into virtual multi band images for mosaicking tiled raster data or for adding missing metadata like the dtg and band wavelength information temporal information can either be stored image or band wise i e one image per observation date or as a raster stack with each raster band relating to a different observation date respectively 2 2 date time group the dtg is required to locate an image on the temporal axis of the time series it is searched in order of the following source image locations 1 image internal metadata key value pairs like acquisition date 2014 04 23 if a metadata key matches the case insensitive regular expression acquisition date datetime it will be checked for a valid dtg definition 2 the image base name like in image2014 04 23 bsq or 3 the image directory like in c data 2014 04 23 image bsq dtgs can be specified according to iso 8601 e g like in 2014 04 23 or 20140423t123112234 as date without hyphens yyyymmdd e g 20140423 in year day of year yyyydoy notation e g 2014113 or as year only yyyy 2 3 multi sensor time series if a source image is readable and defines a dtg the eotsv describes its image product specification hereafter referred as sensor the eotsv reads the ground sampling distance gsd colloquially pixel size number of bands data type and if defined in the source image metadata band wavelength and sensor name source images are treated as coming from different sensors if they differ in either gsd band count or data type in addition sensors can be differentiated by wavelength default and sensor name it is for example possible to distinguish images between sentinel 2a and 2b or to regard both as from the same sensor source images from the same sensor with identical dtg are considered to be tiles from an image mosaic and are grouped in the same image group each image group represents an entity on the temporal axis of the multi sensor time series ordered by dtg and sensor name in some cases it is useful that source images with different dtgs are binned to a unified dtg and get linked to the same image group for example the acquisition time of rapideye level 3a images is specified with millisecond precision it is useful to group these images by the day of observation i e all images from the same overpass therefore the eotsv matches source images from the same sensor with a dtg precision of one day by default fig 1 b yet may be of higher or lower temporal resolution the sensor panel fig 1 a gives an overview on derived sensors and the total number of source image sources linked to each sensor the panel allows to modify the sensor name which if not defined in the source image metadata is initialized with a short description like 6bands 30m the current state of the multi sensor time series model i e all source images image groups and sensors is shown in the time series panel fig 1 b it allows to add or to remove source images to modify the visibility of single image groups and it highlights which of them are currently visible as spatial maps the source image paths can be saved to a text file that may be used to reload the time series or to pass the file path information to other algorithms e g processing scripts that demand for a visually filtered ts 3 data visualization 3 1 spatial data showing a ts as a set of spatial maps is a well known concept and used in other ts viewer software e g timesync cohen et al 2010 however we are only aware of static solutions where adjustments to the map render settings require several user interactions e g to quickly enhance image contrasts change visualized band combinations or shift a geographic map extent the eotsv overcomes these drawbacks and applies established gis handling fig 1 e to all images e g to change map extents and adjust the raster layer render settings for a consistent and interactive representation of spectral and temporal relations between source images the eotsv uses a standardized grid arrangement of individual gis maps 3 1 1 the mapping panel fig 1 c controls the map size and content and the number of maps n j x k with j the number of image groups or maps in a row 3 1 2 and k the number of map views or maps in a column 3 1 2 shown at the same time 3 1 1 maps all maps are linked and thus share the same projection and spatial extent which can be modified using standard qgis tools e g to zoom and pan fig 1 d each map relates to one image group i e one sensor and dtg and visualizes all of its source images in the same style e g raster tiles of a larger satellite data take additional raster and vector layers can be added e g locations of visualized temporal 3 2 and spectral profiles 3 3 or layers that have been opened in qgis e g an open street map layer vector layers can utilize map specific variables like the dtg doy sensor name or map view e g to render vector features only if they match the map s dtg fig 3 map view 1 point feature 1 the map context menu fig 3 a provides access to the layer settings of raster and vector layers fig 3 d e which spatially intersect with the map location for which the context menu was called this for example eases the identification of individual source images in long ts to e g identify individual raster tiles with erroneous data 3 1 2 image groups the time slider and navigation buttons fig 1 g control the temporal subset that is visualized i e which image groups are shown as columns of maps the image group visibility can be changed individually this allows to focus on image groups that contain interpretable data e g by hiding image groups with clouded observations map and image group specific information like the dtg the sensor name or a bitmap of the map can be copied to the clipboard to be used in other software fig 3 g 3 1 3 map views maps in the same map view which are represented as maps in the same row share a similar spectral representation over time regardless of the sensor used each map view specifies a layer tree that lists i a raster layer for each sensor of the ts data and ii other raster or vector layers fig 1 c the visualization of sensor related raster layers can be optimized based on the current map extent fig 3 d f such changes will be applied automatically to all other raster layers in the map view that are linked to the same sensor raster and vector layer styles can be shared copy paste style with other layers and between map layers in the eotsv or qgis multiple map views are stacked vertically i e in rows and allow to define multiple layer trees these allow to specify different data visualization at the same time e g to visualize both true color bands and near infrared bands they can be used to show different sets of vector layers too which otherwise would visually overload the map if shown together in the same map view 3 2 temporal profiles a visualization of temporal profiles is usually not foreseen in gis environments and can be cumbersome with many dip tools ts viewers often only focus on visualizing single bands or index values from single locations over time the eotsv overcomes such limitations as it flexibly allows comparing temporal profiles from separate locations band values or on the fly derived spectral indices and various sensors simultaneously temporal profiles are extracted by selecting their locations in a map or loading them from a vector data source the eotsv loads image band values for i all pixels that intersect the profile s geolocation and ii which are not masked by image internal no data values fig 4 the visualized profiles can be specified individually to compare different locations sensors or bands in the same plot window fig 4 the plotted values are specified with an individual expression like b1 to plot values of the first raster band table 3 this can be used to account for sensor specific scaling differences and to compare spectral indices like the normalized difference vegetation index ndvi fig 4 between different sensors style and scale settings can be changed interactively and using the plot context menu the map visualizations can be moved to an identified date of interest temporal profile coordinates can be saved as vector data set and band values in a csv text file 3 3 spectral profiles spectral profiles are stored in spectral libraries together with metadata like location and time of measurement or attributes to describe the measured surfaces spectral libraries are widely used in eo e g to parameterize land cover and land use classification models or to derive fractions of land cover dudley et al 2015 priem et al 2019 schug et al 2018 viscarra rossel et al 2016 spectral profiles can be extracted for each image pixel and are collected and visualized in the spectral library viewer fig 5 the plot window allows comparing profiles from different sensors based on their different band wavelength information collected spectral profiles can be tagged and labeled and are internally stored in an in memory spectral library which can then be saved e g as csv text file envi spectral library or copied to the clipboard to be used in table calculation software 4 data labeling labeling areas or points of interest on single raster images but in particular for ts is often adapted to the requirements of specific workflows this complicates the labeling process for more complex labeling scenarios e g when multiple attributes need to be described the eotsv speeds up labeling sessions by shortcuts to quickly label temporal and categorical information this reduces the number of user interactions required to edit informational content the eotsv labeling panel fig 6 can be used to edit features of a vector layer i e to label ts data like in qgis vector layer manipulations are performed in an attribute table that allows to edit feature attributes while new feature geometries i e point lines or polygons can be draw directly in each map modifications to the vector layer exists in a transaction model and need to be confirmed or rejected before leaving the edit mode the eotsv quick labels functionality enhances existing qgis vector manipulation functions by saving the acquisition date sensor specific or categorical information further it allows to quickly label multiple attributes e g a land cover category and the date and path of the image source they have been observed if specified in the vector layers property dialog fig 7 the map context menu allows sending image group date and classification scheme specific values to the fields of selected vector features table 4 together with the labeling panel s next feature and previous feature buttons this allows for a workflow that quickly visualizes the spatial environment of selected vector features and labels them with the requested information fig 6 the vector layer can be edited in qgis at the same time allowing to use qgis and other qgis plugin vector functionalities as well e g a straightforward renaming of attribute values based on field calculations or to export the labeled vector data into formats understood by other applications 5 case studies the eotsv was initially developed to support the analysis described in jakimow et al 2018 where a 3 year time series of 80 landsat images was analyzed for mapping burning and tillage on pastures and areas with secondary regrowth in the region of novo progresso brazilian amazon the region s frequent cloud cover and a quick sequence of management processes e g the subsequent ploughing of burnt pasture fig 8 inspired the development of the clear observation sequence cos approach at pixel level a cos consists of an unclouded unshaded non masked and therefore clear observation at reference date t it s next clear predecessor and next clear successor in case of clouded shaded or otherwise masked pixels the entire cos is masked with a no data value cos were generated for each landsat observation and used to extract a band stack of 42 spectral and temporal features the feature stacks were classified with a random forest breiman 2001 to obtain class probabilities for burned pasture tilled pasture and burned secondary regrowth for each landsat observation the class probabilities were aggregated into an annual score for each class from which final maps of burned pasture burned secondary regrowth tilled pasture and burned and tilled pastures were derived for 2013 2015 the area adjusted accuracy assessment olofsson et al 2014 revealed overall accuracies oaa 88 and user accuracies ua 86 for burned areas and 74 for tilled areas the eotsv helped to synergistically visualize the landsat ts together with auxiliary spatial data e g 42 rapideye observations prodes inpe 2019a deforestation and terraclass inpe 2019b landcover maps gps tracks and geo located photographs made in field because reference data for burned and tilled areas were not available at landsat spatial resolution with exact date of management the eotsv was used to digitize 196 reference polygons and to label them with identified classes of land use or land management and the corresponding dates of the landsat observation e g when a tilled pasture became visible the first time these reference polygons were input to a python script that extracted the related cos feature stacks from the landsat time series and calibrated the random forest the eotsv ability for simultaneous visualization of ts data in a true color map view and a near infrared map view provided several advantages and generally greatly facilitated the visual recognition of the temporal course of e g burning and tillage measures as compared to previously available software packages or the stock qgis burned areas for example were better visible in the near shortwave infrared because of an increased contrast of vital vegetation in false color visualization as compared to true color visualization fig 8 b c the simultaneous true color visualization better highlighted soil condition in dried and tilled areas fig 8 d e taking into account the temporal development as e g described in temporal profiles and adjacent maps of a map view allowed separating natural drying from short termed tilling in order to determine the map accuracy of the three land management maps according to olofsson et al 2014 a total of 969 validation points was created by stratified random sampling due to the sometimes short visibility of burning or ploughing all landsat and rapideye observation of the respective year had to be inspected for the validation of each point the eotsv allowed to quickly pan to the next validation point and to scroll through the landsat observation dates which substantially shortened the work that would have been much more time consuming with a conventional gis or dip another case study that used the eotsv is described in yin et al 2018 the authors mapped agricultural land abandonment in parts of russia and georgia using a landsat ts with 301 images from 1985 to 2015 they used a spatial segmentation to obtain agricultural land objects and calculated a time series of annual agricultural land probabilities which then was segmented with the landsat based detection of trends in disturbance and recovery landtrendr kennedy et al 2010 algorithm the segments were classified to yield annual maps of stable agricultural land fallow land and re cultivation of agricultural land the authors used the eotsv to label 450 randomly sampled validation points based on the landsat ts plus 51 aster images high resolution images from google earth and bing maps and temporal profiles from a modis ndvi ts mod13q1 the accuracy assessment according to olofsson et al 2014 achieved an area adjusted overall accuracy of 97 1 and average user and producer accuracies of 66 and 69 for fallow and re cultivation class rufin et al 2019 used 2403 landsat observations to map five cropping practices across entire turkey for 2015 they used eotsv to inspect the landsat surface reflectance time series and derived products like stacks of tasseled cap component time series which were generated with the framework for operational radiometric correction for environmental monitoring force frantz 2019 according to personal communication the authors also used the eotsv to validate annual landsat based irrigated area products rufin et al 2019b in prep and to compare the results of a landsat sentinel co registration approach rufin et al 2019a under review 6 benchmark 6 1 setup the time required to interpret and label a ts is influenced by various aspects e g data quality the respective research question and related goals or knowledge and experience of the image interpreter a prerequisite for an efficient interactive interpretation and labeling is that data loading operations occur fast we therefore benchmarked how long it takes to change the map visualization and to load temporal and spectral profiles from different ts table 5 an optical ts of tiled landsat and sentinel 2 surface reflectance data was generated with force and used along with rapideye level 3a and pléiades images as delivered by its data providers a sentinel 1 radar ts coherence amplitude and interferograms was generated with the open source sar investigation system osaris loibl et al 2019 as osaris outputs are single band images we virtually stacked coherence amplitude and interferogram bands for each date using the gdal vrt format sentinel 1 vrt and visualized this ts in a multiband color visualization fig 9 to assess if loading benefits from overview images i e image pyramids adelson et al 1984 we duplicated the virtual sentinel 1 time series and generated overview images for zoom levels 2 to 64 sentinel 1 vrt ovr to simulate different user interactions and their realization times we measured the following durations starting with a randomly selected source image a we simulated navigation on the time axis by showing the source image with its full extent in the center image group of the map visualization this forced the eotsv to load the image data for previous and following observation dates into the two image groups left and right of the center image group b we performed a zoom operation by focusing the visible map extent to the 200 200 pixels around the source image center image subset 200 200 c we extracted a temporal profile of all band 1 b1 values for the center pixel of the source image and finally d loaded the spectral profile from the source image center we repeated the measurements a d for a total of 200 randomly selected source images per ts with 14 maps 7 image groups x 2 map views of 200 200 pixels this way the benchmark simulates different user interactions that require to change the temporal and spatial scope of the map visualization in order to accelerate the visualization speed the eotsv buffers raster layers when e g the map visualization scrolls to the next image group the eotsv uses all raster layers which the current and new visualized temporal selection have in common raster layers that are excluded from the visualized temporal selection are deleted to free memory resources we therefore repeated tests a b using a single map 1 image group x 1 map view to measure loading times without layer buffering to identify differences between a direct reading of source images and reading through virtual images we measured the loading times for the sentinel 1 coherence time series and the vrt stack s coherence band sentinel 1 vrt coherence both using a single band grey color render style the sentinel 1 vrt and sentinel 1 vrt ovr ts were visualized in a multiband color rendering that required to load the map data from all three source bands all tests were run on a desktop windows pc intel core i7 2600 3 4 ghz 16 gb ram with qgis 3 10 and all source images were stored in the same external network area storage nas 6 2 results on average it required less than 5 s to refresh all 14 maps respectively less than 0 3 s for a single map table 6 a b loading map data for the full image extent required for all ts more time than for the subsets of 200 200 pixels since the number of different observation dates nt table 5 corresponds to that of the image groups the visualization of the pléiades ts benefits greatly from the layer buffering in the 7 2 maps visualization otherwise the 1 1 single map visualization shows that due to the larger image size pléiades images required about twice as much time to be loaded in single cases the loading required up to 18 s e g for the sentinel 1 vrt stack the log files and repetitions of the benchmark showed no clear pattern so it is likely that this slowdown is due to a different use of the nas by other users accessing the sentinel 1 coherence band directly improved the access rate by a factor of 1 5 as compared to loading it via the vrt stack consequently the visualization in a multiband rgb style which required loading data from 3 different bands took longest in case of the sentinel 1 vrt stack without overview images compared to all other ts the temporal difference between a direct data access and an indirect vrt data access became smaller when visualizing the image data for a 200 200 pixel subset instead of the full image extent using overview images required 25 more disk space but increased the map loading speed by a factor of up to 1 5 full image extent loading a temporal profile table 6 c needed up to 5 s landsat 7 and on average less than 2 s for all tested ts table 5 the extraction of a single spectral profile was considerably faster and required less than 0 5 s in all cases 7 discussion and outlook by systematically developing the eotsv along the demands listed in table 2 we achieved our aims to implement a qgis plugin for an integrated and interactive visualization and labeling of multi sensor ts which provides the flexibility to be used in different data and application scenarios one has to consider how many steps are required in a conventional gis to switch between different views on the ts data for example to display a certain observation date and the previous and next observations to optimize the image contrast and to synchronize it along all image sources from the same sensor based on gdal the eotsv is able to read more than 140 different raster image formats gdal ogr contributors 2019 its data agnostic implementation allowed us to visualize optical landsat sentinel 2 rapideye and pléiades data as well as sar sentinel 1 products however per definition the eotsv data model demands for single or multiband source images with unified band size and ground sampling distance multi dimensional raster images like the hierarchical data format hdf that is commonly used for modis products cannot be directly loaded by opening a file path we first need to define which of the internally available datasets are to be used as shown with the sentinel 1 vrt ts 6 1 this can be achieved with virtual raster files that map desired source bands into a virtual multi band image understood by the eotsv eotsv users can prepare vrt files e g using the gdal gdalbuildvrt command line application or tools like the vrt raster builder https plugins qgis org plugins vrtbuilderplugin however they should be aware that in using vrt might increase the time required to map image data even that our benchmarking showed that time loss is negligible being a desktop software the eotsv can be used offline and for field work eotsv s performance is hence depending on the respective specifications of local hardware specifications not on internet connectivity as e g web client based solutions our tests showed that the eotsv easily handles ts with 700 images fig 1 a in principle the eotsv can help to inspect even terabyte sized collections of ts data on standard desktop computers the increased amount of data affects the eotsv loading times in different ways the time to i setup up the multi sensor ts model increases proportionally with the total number of source images because more files are inspected for its dtg and sensor specification using virtual mosaics to group images with same dtg and sensor properties reduces the total number of source images and therefore speeds up the initial loading the time to ii realize map visualizations increases with the spatial resolution of the source images or by visualizing a larger map extent as more pixels have to be read for the same map extent as shown in the benchmark the loading times can be reduced by focusing on smaller spatial map extents and by generating overview images on suitable scales in this way the eotsv benefits from the same means that improve the performance of other desktop gis and dip software as well otherwise requirements for a fluid interactive map visualization have to be balanced with that for redundancy free data storage the map visualization time is not affected by an increased temporal length of the ts or increased number of bands per source image because the total number of maps i e map views x number of maps per map view and the number of source image bands loaded per map is fixed this is different for the visualization of temporal and spectral profiles which requires more time with using longer ts respectively more bands per source image however in both cases the band values are loaded for single pixels only therefore loading times should be acceptable even for longer ts with more temporal observation and hyperspectral ts data with more than 200 bands per source image bands like observed by the hyperspectral precursor and application mission prisma or the upcoming environmental mapping and analysis program enmap transon et al 2018 some features of the eotsv e g the loading of spectral and temporal profiles are planned to be optimized by future revisions and optimizations of the source code a 3d visualization of temporal profiles or spectral temporal surfaces would enhance the ability to interpret changes that occur over time and space instead of single points or pixel locations only spectral and temporal profiles could be displayed and saved or visualized as average values for areas of interest which may reduce spectral or temporal noise our free and open source qgis plugin takes only a few mouse clicks to be installed with the qgis plugin manager and to be run on linux windows and macos systems we envisage that the open source character of the eotsv encourages others to support its development i e by providing feature requests bug reports or own source code being open source and supporting standard image and vector data formats the eotsv can be easily used with other tools and algorithms from the qgis ecosystem and beyond for example scientific workflows could use validation points that were labeled with the eotsv as input to the accuracy assessment of thematic maps acatama llano 2019 plugin to calculate the area adjusted accuracy metrics recommended by olofsson et al 2014 in future eotsv versions we plan to further simplify the loading of multi dimensional raster images e g through an automated creation of vrts to access bands in hdf data sets we will also improve the integration between spectral and temporal profile visualization and speed up the loading of profile data a session setting system will restore the sensor and map view specific visualization settings bug fixing integrating user recommendations adopting changes in the specification of prominent eo products and valuing improvements in qgis and gdal libraries are inherent to our software development strategy 8 conclusion the eo time series viewer is a free and open source qgis plugin to visualize understand and describe changes on earth e g in the generation of reference data regarding changes in land cover and land use it enhances established qgis functionality with concepts well known from digital image processing software some of its most innovative features are a data agnostic support of image sources from multiple sensors an integrated visualization of spatial maps spectral and temporal profiles to compare different observation dates spectral regions and sensors and its support to speed up the labeling of time series data this way the eo time series viewer provides a toolset that bridges between the gis world and analyses of raster time series supports mandatory workflows for many users of eo data and helps to better put in value long term eo data archives and free data sharing policies acknowledgements earlier versions of the eo time series viewer were partly developed during the sensecarbon project funded by the german aerospace center dlr and granted by the federal ministry of education and research bmbf grant no 50ee1254 since 2017 development of the software is supported by the german research centre for geosciences gfz as part of the enmap core science team activities funded by dlr and granted by the federal ministry of economic affairs and energy bmwi grant no 50ee1529 and the land use monitoring system lumos project funded by the belgian science policy office as part of the stereo iii research program grant no sr 01 349 we further thank the global forest observation initiative for providing access to pléiades data and the developers behind software utilized in this study in particular python www python org the geospatial data abstraction library www gdal org qgis www qgis org and pyqtgraph http pyqtgraph org this publication also supports activities of the landsat science team 2018 2023 https www usgs gov land resources nli landsat 2018 2023 landsat science team appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104631 
26073,multi spectral spaceborne sensors with different spatial resolutions produce earth observation eo time series ts with global coverage the interactive visualization and interpretation of ts is essential to better understand changes in land use and land cover and to extract reference information for model calibration and validation however available software tools are often limited to specific sensors or optimized for application specific visualizations to overcome these limitations we developed the eo time series viewer a free and open source qgis plugin for user friendly visualization interpretation and labeling of multi sensor ts data the eo time series viewer i combines advantages of spatial spectral and temporal data visualization concepts that are so far not available in a single tool ii provides maximum flexibility in terms of supported data formats iii minimizes the user interactions required to load and visualize multi sensor ts data and iv speeds up labeling of ts data based on enhanced gis vector tools and formats keywords change detection training validation open source qgis plugin eo time series viewer software availability name of software eo time series viewer developer benjamin jakimow earth observation lab humboldt universität zu berlin germany contact benjamin jakimow geo hu berlin de source language python program language english program size 5 mbyte including example data license gnu gpl 3 required software qgis geoinformation system https qgis org required hardware any hardware that allows to run qgis year first available 2018 as official qgis plugin source code public git repository with issue tracker https bitbucket org jakimowb eo time series viewer documentation https eo time series viewer readthedocs io installation via qgis plugin manager 1 introduction an increasing number of sensors on spaceborne satellite platforms are collecting earth observation eo data with global coverage and at varying temporal resolutions belward and skøien 2015 e g the sentinel 2a b s2 multispectral instrument msi drusch et al 2012 gascon et al 2017 and landsat 8 l8 operational land imager oli roy et al 2014 wulder et al 2019 that provide raster data with 10 30 m spatial resolution and 16 l8 respectively 5 days s2 temporal resolution such earth observation eo time series ts data allow to map land cover and land use lclu and related changes gómez et al 2016 joshi et al 2016 which may occur at different spatial and temporal scales these processes include e g urban growth and dynamics liu et al 2018 schug et al 2018 zhou et al 2018 crop cycles and agricultural intensification belgiu and csillik 2018 griffiths et al 2019 roy and yan 2018 wildfires and burning hawbaker et al 2017 roy et al 2019 villarreal et al 2016 deforestation and forest management griffiths et al 2018 hansen et al 2013 or trends and changes in plant phenology jönsson et al 2018 open data policies the ongoing consolidation of historical data archives and the provision of analysis ready data ard ceos 2019 egorov et al 2018 frantz 2019 with already preprocessed and atmospherically corrected imagery improved the accessibility and usability of recent and historical ts claverie et al 2018 zhu et al 2019 at the same time advances in computational technologies allow to process even very large volumes of raster data efficiently this pushed ts data analysis towards a large area and dense time series monitoring of long lasting land cover and land use changes griffiths et al 2012 hansen et al 2013 and into the domain of big data analysis gorelick et al 2017 lewis et al 2017 liu et al 2018 methods from the field of machine learning like image classification and quantitative process analysis cracknell and reading 2014 lary et al 2016 zhu et al 2017 zhu 2017 can analyze the high informational content of ts efficiently and on local or cloud based computer infrastructures nevertheless the visual interpretation of ts data to better understand lclu change and to create reference data remains an important yet complex and mostly time consuming step for such analyses experienced image interpreters evaluate and contextualize the spectral spatial and temporal characteristics of the ts data to identify or exclude lclu changes they incorporate auxiliary data e g very high resolution vhr imagery topographic maps statistical surveys or data from field studies finally the results of the visual interpretation are used to create new reference data sets e g by tagging points or areas of interest i e single pixels or groups of neighbored pixels with their respective attributes such attributes in the following labels can be a category of land cover or land cover change a time stamp or more detailed information in writing on what the visual interpretation revealed reference labels are required to calibrate or parameterize supervised classification and change detection approaches as well as regression approaches that allow quantifying extracted information high quality reference data is also mandatory for a statistically sound assessment of map accuracy olofsson et al 2013 2014 and of the temporal accuracy of automated change detection algorithms dara et al 2018 especially if the timing of changes has to be included in the reference data with utmost precision many eo raster images need to be browsed and visualized with the highest possible temporal resolution this demands for specialized software that provides efficient and easy to use visualization and labeling of ts raster data which as we will show in the next section rarely exists 1 1 existing software software that is used in the eo science community for visualizing and partly for labeling eo data might be differentiated by their development history into i geographical information systems gis with a strength in the analysis of vector data ii digital image processing dip software with a focus on the analysis of raster based remote sensing data and iii software specifically developed for eo time series data analysis tsa existing software offer different capabilities if evaluated with specific regard to the loading of ts data i e the opening of many raster images an interactive visualization of its spatial spectral and temporal dimensions and the software availability e g license model and costs table 1 gis like arcgis and qgis are well suited to visualize heterogeneous spatial data from local sources or web services they traditionally organize the rendering of spatial data in a layer stack and offer a very flexible control for data visualization typically gis support on the fly re projections which eases a visualization of geo data from different projection systems dip like erdas imagine the environment for visualizing images envi or the sentinel application platform snap have a long development history and provide a rich set of features to visualize the spatial and spectral dimensions of eo data they visualize raster images in multiple maps mm side by side or as an image stack which are geographically linked and allow comparing spatial patterns they show spectral profiles sp for single pixel positions to compare the spectral responses of different surfaces e g vegetation vs artificial materials and save profiles in spectral libraries specialized on remote sensing raster formats and workflows dip value sensor specific meta data e g band wavelength information to identify the raster image bands for a true color band combination gis usually provide a rich toolset for labeling eo data based on vector geometries points lines or polygons spatial objects can flexibly be described as geometries and each geometry can be annotated with various attributes i e fields with numeric values or text this information can be saved to vector formats e g geopackage open geospatial consortium 2018 or the quasi standard esri shapefile esri 1998 to interchange label information with other software in contrary dip often use individual formats to label eo data e g domain specific xml and binary formats that need to be transformed into other formats to be shared with other software furthermore virtual raster images vi e g the geospatial data abstraction library gdal virtual format vrt gdal ogr contributors 2019 are emerging which allow to reorganize data without the need to reorganize image data physically for example single band raster datasets can be virtually combined into a multi multiband dataset with unified spatial resolution band names and additional metadata information for visualization in a gis without copying of actual image data working with ts data creates special needs for data visualization and management in addition to the capabilities brought by standard dip and gis software users of ts data are often confronted with a multitude of physical raster images and software specific to visualize ts preferable accounts for that by recognizing observation dates from file names or image metadata the eo science community therefore developed tsa tools like timesync cohen et al 2010 timestats udelhoven 2011 timesat eklundh and jönsson 2016 tstools holden 2016 or collect earth bey et al 2016 tsa tools have an explicit concept of raster time series they are usually strong with regard to a certain field of application or the characteristics of a single sensor e g timesync for landsat time series and timesat for deriving vegetation phenology from modis time series or more recently also for landsat and sentinel 2 data jönsson et al 2018 collect earth on the other hand focusses on the labeling of pre sampled plots for which extensive dialogs can be customized to enter label attributes collect earth is based on the google earth https www google com earth interface and makes use of eo data stored in or uploaded to the google earth engine gee gorelick et al 2017 archives uploading data to cloud services might be constrained by legal limits of data usage agreements e g in case of commercial vhr data sources and is problematic for larger e g terabyte scale data sources furthermore web based tools cannot be used offline of e g during field work however the focus of existing tsa software often comes with trade offs in terms of broad applicability e g a need for unified file formats file naming and map projection in general there is a lack of applicability to heterogeneous multi sensor ts e g to simultaneously inspect two or more ts from sensors with different spatial spectral and temporal resolutions moreover existing tools for labeling ts rarely adequately value the temporal context e g the need to save time stamps and file paths related to the observations on which a labeling decision is based 1 2 eo time series viewer software that combines the advantages of different concepts from the worlds of dip gis and tsa to visualize and label ts data in a data agnostic way i e the support of a direct reading of different raster formats and in particular data from multiple sensors is so far not available therefore we summarized the strengths and weaknesses of existing software table 1 plus demands that arose during our own work to compile a list of desired software capabilities and requirements table 2 based on that list we conceptually laid out the eo time series viewer eotsv fig 1 the main goal of the eotsv is to enable and simplify the visualization and labeling of ts as well as to accelerate these often time consuming steps in scientific analyses in particular our aims were 1 to combine different visualization concepts that are well known from established dip gis and tsa to visualize the spatial spectral and temporal domains of multi sensor ts 2 to provide maximum flexibility in terms of supported sensors data sources and image properties e g different data projections and file formats 3 to minimize the number of user interactions mouse clicks that is required to load and visualize ts data 4 to ease the labeling and extraction of information from ts based on standard gis vector and dip spectral library formats respectively for good interoperability with other software and 5 to allow for an offline visualization of ts e g during fieldwork and in regions with poor internet access in the following we describe the features of the eotsv i e how ts data is provided and loaded section 2 how spatial temporal and spectral data information content is visualized section 3 and how the eotsv eases the labeling of ts data section 4 we then provide an overview on case studies where the eotsv was used to assess ts data section 5 show a benchmark section 6 and discuss the usability and potential future developments of the eotsv section 7 2 data model software for visualizing ts often demands the adaptation of raster data to specific structural properties e g file formats naming conventions or a shared map projection this often requires additional preprocessing of the input data and its metadata often at the expense of the original spatial or temporal resolution of the data the eotsv mitigates such preprocessing by supporting a wide range of data formats and evaluating the relevant metadata for required information retrieval the eotsv therefore provides a flexibility that to our best knowledge is not offered by any other comparable tool valid raster inputs for the eotsv multi sensor time series model fig 2 are geolocated raster images that can be read with the gdal and define a date time group dtg 2 1 geolocated raster images based on gdal and the qgis api the eotsv currently reads more than 140 different raster formats gdal ogr contributors 2019 e g file formats like tiff and jpeg2000 as well as web sources with web coverage wcs and web map services wms source images are visualized independently from differences in coordinate reference systems spatial extents or number of bands this avoids that large amounts of ts data need first to be preprocessed into a unified raster format raster dimensions or projection system if necessary vrt images can be used for re organizing input raster files and its metadata without changing the original data sources this is useful e g for stacking single band data sources into virtual multi band images for mosaicking tiled raster data or for adding missing metadata like the dtg and band wavelength information temporal information can either be stored image or band wise i e one image per observation date or as a raster stack with each raster band relating to a different observation date respectively 2 2 date time group the dtg is required to locate an image on the temporal axis of the time series it is searched in order of the following source image locations 1 image internal metadata key value pairs like acquisition date 2014 04 23 if a metadata key matches the case insensitive regular expression acquisition date datetime it will be checked for a valid dtg definition 2 the image base name like in image2014 04 23 bsq or 3 the image directory like in c data 2014 04 23 image bsq dtgs can be specified according to iso 8601 e g like in 2014 04 23 or 20140423t123112234 as date without hyphens yyyymmdd e g 20140423 in year day of year yyyydoy notation e g 2014113 or as year only yyyy 2 3 multi sensor time series if a source image is readable and defines a dtg the eotsv describes its image product specification hereafter referred as sensor the eotsv reads the ground sampling distance gsd colloquially pixel size number of bands data type and if defined in the source image metadata band wavelength and sensor name source images are treated as coming from different sensors if they differ in either gsd band count or data type in addition sensors can be differentiated by wavelength default and sensor name it is for example possible to distinguish images between sentinel 2a and 2b or to regard both as from the same sensor source images from the same sensor with identical dtg are considered to be tiles from an image mosaic and are grouped in the same image group each image group represents an entity on the temporal axis of the multi sensor time series ordered by dtg and sensor name in some cases it is useful that source images with different dtgs are binned to a unified dtg and get linked to the same image group for example the acquisition time of rapideye level 3a images is specified with millisecond precision it is useful to group these images by the day of observation i e all images from the same overpass therefore the eotsv matches source images from the same sensor with a dtg precision of one day by default fig 1 b yet may be of higher or lower temporal resolution the sensor panel fig 1 a gives an overview on derived sensors and the total number of source image sources linked to each sensor the panel allows to modify the sensor name which if not defined in the source image metadata is initialized with a short description like 6bands 30m the current state of the multi sensor time series model i e all source images image groups and sensors is shown in the time series panel fig 1 b it allows to add or to remove source images to modify the visibility of single image groups and it highlights which of them are currently visible as spatial maps the source image paths can be saved to a text file that may be used to reload the time series or to pass the file path information to other algorithms e g processing scripts that demand for a visually filtered ts 3 data visualization 3 1 spatial data showing a ts as a set of spatial maps is a well known concept and used in other ts viewer software e g timesync cohen et al 2010 however we are only aware of static solutions where adjustments to the map render settings require several user interactions e g to quickly enhance image contrasts change visualized band combinations or shift a geographic map extent the eotsv overcomes these drawbacks and applies established gis handling fig 1 e to all images e g to change map extents and adjust the raster layer render settings for a consistent and interactive representation of spectral and temporal relations between source images the eotsv uses a standardized grid arrangement of individual gis maps 3 1 1 the mapping panel fig 1 c controls the map size and content and the number of maps n j x k with j the number of image groups or maps in a row 3 1 2 and k the number of map views or maps in a column 3 1 2 shown at the same time 3 1 1 maps all maps are linked and thus share the same projection and spatial extent which can be modified using standard qgis tools e g to zoom and pan fig 1 d each map relates to one image group i e one sensor and dtg and visualizes all of its source images in the same style e g raster tiles of a larger satellite data take additional raster and vector layers can be added e g locations of visualized temporal 3 2 and spectral profiles 3 3 or layers that have been opened in qgis e g an open street map layer vector layers can utilize map specific variables like the dtg doy sensor name or map view e g to render vector features only if they match the map s dtg fig 3 map view 1 point feature 1 the map context menu fig 3 a provides access to the layer settings of raster and vector layers fig 3 d e which spatially intersect with the map location for which the context menu was called this for example eases the identification of individual source images in long ts to e g identify individual raster tiles with erroneous data 3 1 2 image groups the time slider and navigation buttons fig 1 g control the temporal subset that is visualized i e which image groups are shown as columns of maps the image group visibility can be changed individually this allows to focus on image groups that contain interpretable data e g by hiding image groups with clouded observations map and image group specific information like the dtg the sensor name or a bitmap of the map can be copied to the clipboard to be used in other software fig 3 g 3 1 3 map views maps in the same map view which are represented as maps in the same row share a similar spectral representation over time regardless of the sensor used each map view specifies a layer tree that lists i a raster layer for each sensor of the ts data and ii other raster or vector layers fig 1 c the visualization of sensor related raster layers can be optimized based on the current map extent fig 3 d f such changes will be applied automatically to all other raster layers in the map view that are linked to the same sensor raster and vector layer styles can be shared copy paste style with other layers and between map layers in the eotsv or qgis multiple map views are stacked vertically i e in rows and allow to define multiple layer trees these allow to specify different data visualization at the same time e g to visualize both true color bands and near infrared bands they can be used to show different sets of vector layers too which otherwise would visually overload the map if shown together in the same map view 3 2 temporal profiles a visualization of temporal profiles is usually not foreseen in gis environments and can be cumbersome with many dip tools ts viewers often only focus on visualizing single bands or index values from single locations over time the eotsv overcomes such limitations as it flexibly allows comparing temporal profiles from separate locations band values or on the fly derived spectral indices and various sensors simultaneously temporal profiles are extracted by selecting their locations in a map or loading them from a vector data source the eotsv loads image band values for i all pixels that intersect the profile s geolocation and ii which are not masked by image internal no data values fig 4 the visualized profiles can be specified individually to compare different locations sensors or bands in the same plot window fig 4 the plotted values are specified with an individual expression like b1 to plot values of the first raster band table 3 this can be used to account for sensor specific scaling differences and to compare spectral indices like the normalized difference vegetation index ndvi fig 4 between different sensors style and scale settings can be changed interactively and using the plot context menu the map visualizations can be moved to an identified date of interest temporal profile coordinates can be saved as vector data set and band values in a csv text file 3 3 spectral profiles spectral profiles are stored in spectral libraries together with metadata like location and time of measurement or attributes to describe the measured surfaces spectral libraries are widely used in eo e g to parameterize land cover and land use classification models or to derive fractions of land cover dudley et al 2015 priem et al 2019 schug et al 2018 viscarra rossel et al 2016 spectral profiles can be extracted for each image pixel and are collected and visualized in the spectral library viewer fig 5 the plot window allows comparing profiles from different sensors based on their different band wavelength information collected spectral profiles can be tagged and labeled and are internally stored in an in memory spectral library which can then be saved e g as csv text file envi spectral library or copied to the clipboard to be used in table calculation software 4 data labeling labeling areas or points of interest on single raster images but in particular for ts is often adapted to the requirements of specific workflows this complicates the labeling process for more complex labeling scenarios e g when multiple attributes need to be described the eotsv speeds up labeling sessions by shortcuts to quickly label temporal and categorical information this reduces the number of user interactions required to edit informational content the eotsv labeling panel fig 6 can be used to edit features of a vector layer i e to label ts data like in qgis vector layer manipulations are performed in an attribute table that allows to edit feature attributes while new feature geometries i e point lines or polygons can be draw directly in each map modifications to the vector layer exists in a transaction model and need to be confirmed or rejected before leaving the edit mode the eotsv quick labels functionality enhances existing qgis vector manipulation functions by saving the acquisition date sensor specific or categorical information further it allows to quickly label multiple attributes e g a land cover category and the date and path of the image source they have been observed if specified in the vector layers property dialog fig 7 the map context menu allows sending image group date and classification scheme specific values to the fields of selected vector features table 4 together with the labeling panel s next feature and previous feature buttons this allows for a workflow that quickly visualizes the spatial environment of selected vector features and labels them with the requested information fig 6 the vector layer can be edited in qgis at the same time allowing to use qgis and other qgis plugin vector functionalities as well e g a straightforward renaming of attribute values based on field calculations or to export the labeled vector data into formats understood by other applications 5 case studies the eotsv was initially developed to support the analysis described in jakimow et al 2018 where a 3 year time series of 80 landsat images was analyzed for mapping burning and tillage on pastures and areas with secondary regrowth in the region of novo progresso brazilian amazon the region s frequent cloud cover and a quick sequence of management processes e g the subsequent ploughing of burnt pasture fig 8 inspired the development of the clear observation sequence cos approach at pixel level a cos consists of an unclouded unshaded non masked and therefore clear observation at reference date t it s next clear predecessor and next clear successor in case of clouded shaded or otherwise masked pixels the entire cos is masked with a no data value cos were generated for each landsat observation and used to extract a band stack of 42 spectral and temporal features the feature stacks were classified with a random forest breiman 2001 to obtain class probabilities for burned pasture tilled pasture and burned secondary regrowth for each landsat observation the class probabilities were aggregated into an annual score for each class from which final maps of burned pasture burned secondary regrowth tilled pasture and burned and tilled pastures were derived for 2013 2015 the area adjusted accuracy assessment olofsson et al 2014 revealed overall accuracies oaa 88 and user accuracies ua 86 for burned areas and 74 for tilled areas the eotsv helped to synergistically visualize the landsat ts together with auxiliary spatial data e g 42 rapideye observations prodes inpe 2019a deforestation and terraclass inpe 2019b landcover maps gps tracks and geo located photographs made in field because reference data for burned and tilled areas were not available at landsat spatial resolution with exact date of management the eotsv was used to digitize 196 reference polygons and to label them with identified classes of land use or land management and the corresponding dates of the landsat observation e g when a tilled pasture became visible the first time these reference polygons were input to a python script that extracted the related cos feature stacks from the landsat time series and calibrated the random forest the eotsv ability for simultaneous visualization of ts data in a true color map view and a near infrared map view provided several advantages and generally greatly facilitated the visual recognition of the temporal course of e g burning and tillage measures as compared to previously available software packages or the stock qgis burned areas for example were better visible in the near shortwave infrared because of an increased contrast of vital vegetation in false color visualization as compared to true color visualization fig 8 b c the simultaneous true color visualization better highlighted soil condition in dried and tilled areas fig 8 d e taking into account the temporal development as e g described in temporal profiles and adjacent maps of a map view allowed separating natural drying from short termed tilling in order to determine the map accuracy of the three land management maps according to olofsson et al 2014 a total of 969 validation points was created by stratified random sampling due to the sometimes short visibility of burning or ploughing all landsat and rapideye observation of the respective year had to be inspected for the validation of each point the eotsv allowed to quickly pan to the next validation point and to scroll through the landsat observation dates which substantially shortened the work that would have been much more time consuming with a conventional gis or dip another case study that used the eotsv is described in yin et al 2018 the authors mapped agricultural land abandonment in parts of russia and georgia using a landsat ts with 301 images from 1985 to 2015 they used a spatial segmentation to obtain agricultural land objects and calculated a time series of annual agricultural land probabilities which then was segmented with the landsat based detection of trends in disturbance and recovery landtrendr kennedy et al 2010 algorithm the segments were classified to yield annual maps of stable agricultural land fallow land and re cultivation of agricultural land the authors used the eotsv to label 450 randomly sampled validation points based on the landsat ts plus 51 aster images high resolution images from google earth and bing maps and temporal profiles from a modis ndvi ts mod13q1 the accuracy assessment according to olofsson et al 2014 achieved an area adjusted overall accuracy of 97 1 and average user and producer accuracies of 66 and 69 for fallow and re cultivation class rufin et al 2019 used 2403 landsat observations to map five cropping practices across entire turkey for 2015 they used eotsv to inspect the landsat surface reflectance time series and derived products like stacks of tasseled cap component time series which were generated with the framework for operational radiometric correction for environmental monitoring force frantz 2019 according to personal communication the authors also used the eotsv to validate annual landsat based irrigated area products rufin et al 2019b in prep and to compare the results of a landsat sentinel co registration approach rufin et al 2019a under review 6 benchmark 6 1 setup the time required to interpret and label a ts is influenced by various aspects e g data quality the respective research question and related goals or knowledge and experience of the image interpreter a prerequisite for an efficient interactive interpretation and labeling is that data loading operations occur fast we therefore benchmarked how long it takes to change the map visualization and to load temporal and spectral profiles from different ts table 5 an optical ts of tiled landsat and sentinel 2 surface reflectance data was generated with force and used along with rapideye level 3a and pléiades images as delivered by its data providers a sentinel 1 radar ts coherence amplitude and interferograms was generated with the open source sar investigation system osaris loibl et al 2019 as osaris outputs are single band images we virtually stacked coherence amplitude and interferogram bands for each date using the gdal vrt format sentinel 1 vrt and visualized this ts in a multiband color visualization fig 9 to assess if loading benefits from overview images i e image pyramids adelson et al 1984 we duplicated the virtual sentinel 1 time series and generated overview images for zoom levels 2 to 64 sentinel 1 vrt ovr to simulate different user interactions and their realization times we measured the following durations starting with a randomly selected source image a we simulated navigation on the time axis by showing the source image with its full extent in the center image group of the map visualization this forced the eotsv to load the image data for previous and following observation dates into the two image groups left and right of the center image group b we performed a zoom operation by focusing the visible map extent to the 200 200 pixels around the source image center image subset 200 200 c we extracted a temporal profile of all band 1 b1 values for the center pixel of the source image and finally d loaded the spectral profile from the source image center we repeated the measurements a d for a total of 200 randomly selected source images per ts with 14 maps 7 image groups x 2 map views of 200 200 pixels this way the benchmark simulates different user interactions that require to change the temporal and spatial scope of the map visualization in order to accelerate the visualization speed the eotsv buffers raster layers when e g the map visualization scrolls to the next image group the eotsv uses all raster layers which the current and new visualized temporal selection have in common raster layers that are excluded from the visualized temporal selection are deleted to free memory resources we therefore repeated tests a b using a single map 1 image group x 1 map view to measure loading times without layer buffering to identify differences between a direct reading of source images and reading through virtual images we measured the loading times for the sentinel 1 coherence time series and the vrt stack s coherence band sentinel 1 vrt coherence both using a single band grey color render style the sentinel 1 vrt and sentinel 1 vrt ovr ts were visualized in a multiband color rendering that required to load the map data from all three source bands all tests were run on a desktop windows pc intel core i7 2600 3 4 ghz 16 gb ram with qgis 3 10 and all source images were stored in the same external network area storage nas 6 2 results on average it required less than 5 s to refresh all 14 maps respectively less than 0 3 s for a single map table 6 a b loading map data for the full image extent required for all ts more time than for the subsets of 200 200 pixels since the number of different observation dates nt table 5 corresponds to that of the image groups the visualization of the pléiades ts benefits greatly from the layer buffering in the 7 2 maps visualization otherwise the 1 1 single map visualization shows that due to the larger image size pléiades images required about twice as much time to be loaded in single cases the loading required up to 18 s e g for the sentinel 1 vrt stack the log files and repetitions of the benchmark showed no clear pattern so it is likely that this slowdown is due to a different use of the nas by other users accessing the sentinel 1 coherence band directly improved the access rate by a factor of 1 5 as compared to loading it via the vrt stack consequently the visualization in a multiband rgb style which required loading data from 3 different bands took longest in case of the sentinel 1 vrt stack without overview images compared to all other ts the temporal difference between a direct data access and an indirect vrt data access became smaller when visualizing the image data for a 200 200 pixel subset instead of the full image extent using overview images required 25 more disk space but increased the map loading speed by a factor of up to 1 5 full image extent loading a temporal profile table 6 c needed up to 5 s landsat 7 and on average less than 2 s for all tested ts table 5 the extraction of a single spectral profile was considerably faster and required less than 0 5 s in all cases 7 discussion and outlook by systematically developing the eotsv along the demands listed in table 2 we achieved our aims to implement a qgis plugin for an integrated and interactive visualization and labeling of multi sensor ts which provides the flexibility to be used in different data and application scenarios one has to consider how many steps are required in a conventional gis to switch between different views on the ts data for example to display a certain observation date and the previous and next observations to optimize the image contrast and to synchronize it along all image sources from the same sensor based on gdal the eotsv is able to read more than 140 different raster image formats gdal ogr contributors 2019 its data agnostic implementation allowed us to visualize optical landsat sentinel 2 rapideye and pléiades data as well as sar sentinel 1 products however per definition the eotsv data model demands for single or multiband source images with unified band size and ground sampling distance multi dimensional raster images like the hierarchical data format hdf that is commonly used for modis products cannot be directly loaded by opening a file path we first need to define which of the internally available datasets are to be used as shown with the sentinel 1 vrt ts 6 1 this can be achieved with virtual raster files that map desired source bands into a virtual multi band image understood by the eotsv eotsv users can prepare vrt files e g using the gdal gdalbuildvrt command line application or tools like the vrt raster builder https plugins qgis org plugins vrtbuilderplugin however they should be aware that in using vrt might increase the time required to map image data even that our benchmarking showed that time loss is negligible being a desktop software the eotsv can be used offline and for field work eotsv s performance is hence depending on the respective specifications of local hardware specifications not on internet connectivity as e g web client based solutions our tests showed that the eotsv easily handles ts with 700 images fig 1 a in principle the eotsv can help to inspect even terabyte sized collections of ts data on standard desktop computers the increased amount of data affects the eotsv loading times in different ways the time to i setup up the multi sensor ts model increases proportionally with the total number of source images because more files are inspected for its dtg and sensor specification using virtual mosaics to group images with same dtg and sensor properties reduces the total number of source images and therefore speeds up the initial loading the time to ii realize map visualizations increases with the spatial resolution of the source images or by visualizing a larger map extent as more pixels have to be read for the same map extent as shown in the benchmark the loading times can be reduced by focusing on smaller spatial map extents and by generating overview images on suitable scales in this way the eotsv benefits from the same means that improve the performance of other desktop gis and dip software as well otherwise requirements for a fluid interactive map visualization have to be balanced with that for redundancy free data storage the map visualization time is not affected by an increased temporal length of the ts or increased number of bands per source image because the total number of maps i e map views x number of maps per map view and the number of source image bands loaded per map is fixed this is different for the visualization of temporal and spectral profiles which requires more time with using longer ts respectively more bands per source image however in both cases the band values are loaded for single pixels only therefore loading times should be acceptable even for longer ts with more temporal observation and hyperspectral ts data with more than 200 bands per source image bands like observed by the hyperspectral precursor and application mission prisma or the upcoming environmental mapping and analysis program enmap transon et al 2018 some features of the eotsv e g the loading of spectral and temporal profiles are planned to be optimized by future revisions and optimizations of the source code a 3d visualization of temporal profiles or spectral temporal surfaces would enhance the ability to interpret changes that occur over time and space instead of single points or pixel locations only spectral and temporal profiles could be displayed and saved or visualized as average values for areas of interest which may reduce spectral or temporal noise our free and open source qgis plugin takes only a few mouse clicks to be installed with the qgis plugin manager and to be run on linux windows and macos systems we envisage that the open source character of the eotsv encourages others to support its development i e by providing feature requests bug reports or own source code being open source and supporting standard image and vector data formats the eotsv can be easily used with other tools and algorithms from the qgis ecosystem and beyond for example scientific workflows could use validation points that were labeled with the eotsv as input to the accuracy assessment of thematic maps acatama llano 2019 plugin to calculate the area adjusted accuracy metrics recommended by olofsson et al 2014 in future eotsv versions we plan to further simplify the loading of multi dimensional raster images e g through an automated creation of vrts to access bands in hdf data sets we will also improve the integration between spectral and temporal profile visualization and speed up the loading of profile data a session setting system will restore the sensor and map view specific visualization settings bug fixing integrating user recommendations adopting changes in the specification of prominent eo products and valuing improvements in qgis and gdal libraries are inherent to our software development strategy 8 conclusion the eo time series viewer is a free and open source qgis plugin to visualize understand and describe changes on earth e g in the generation of reference data regarding changes in land cover and land use it enhances established qgis functionality with concepts well known from digital image processing software some of its most innovative features are a data agnostic support of image sources from multiple sensors an integrated visualization of spatial maps spectral and temporal profiles to compare different observation dates spectral regions and sensors and its support to speed up the labeling of time series data this way the eo time series viewer provides a toolset that bridges between the gis world and analyses of raster time series supports mandatory workflows for many users of eo data and helps to better put in value long term eo data archives and free data sharing policies acknowledgements earlier versions of the eo time series viewer were partly developed during the sensecarbon project funded by the german aerospace center dlr and granted by the federal ministry of education and research bmbf grant no 50ee1254 since 2017 development of the software is supported by the german research centre for geosciences gfz as part of the enmap core science team activities funded by dlr and granted by the federal ministry of economic affairs and energy bmwi grant no 50ee1529 and the land use monitoring system lumos project funded by the belgian science policy office as part of the stereo iii research program grant no sr 01 349 we further thank the global forest observation initiative for providing access to pléiades data and the developers behind software utilized in this study in particular python www python org the geospatial data abstraction library www gdal org qgis www qgis org and pyqtgraph http pyqtgraph org this publication also supports activities of the landsat science team 2018 2023 https www usgs gov land resources nli landsat 2018 2023 landsat science team appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104631 
26074,smart meters installed at the user level provide a new data source for managing water infrastructure this research explores the use of machine learning methods including random forests rfs artificial neural networks anns and support vector regression svr to forecast hourly water demand at 90 accounts using smart metered data demands are predicted using lagged demand seasonality weather and household characteristics time series clustering is applied to delineate data based on the time of day and day of the week which improves model performance two modeling approaches are compared individual models are developed separately for each meter and a group model is trained using a data set of multiple meters individual models predict demands at meters in the original data set with lower error than group models while the group model predicts demands at new meters with lower error than individual models results demonstrate that rf and ann perform better than svr across all scenarios keywords smart water meters ami forecasting model hourly water demand user level data water demand management machine learning urban water systems 1 introduction designing and operating water distribution systems rely on models and forecasts of water consumption water managers use operational or short term water demand forecasts ranging from one day to a few weeks to efficiently manage devices such as pumps and valves short term forecasting models are based on data collected at the account level using data that have historically been collected at monthly or quarterly intervals corresponding to billing cycles recently municipalities and utilities have deployed smart water meters in the context of the smart cities paradigm berglund et al 2020 providing new data about account level demands at hourly or sub hourly frequencies some utilities use a time resolution for reporting demands in the range of 15 s to 30 min battery life and transmitting issues however limit the frequency of data collection and other utilities collect water demand at an hourly frequency beal and flynn 2015 the use of smart meter data for developing models for forecasting demands at sub hourly or hourly frequencies is limited to date unlike the energy sector in which smart meters have been extensively deployed to forecast consumption as part of advanced metering infrastructure ami projects kavousian et al 2013 the water sector has not benefited from the development and use of models that forecast water demand at the account or user level with high temporal resolution ami can be deployed by water utilities to gain insight into water consumption at high spatial and temporal resolutions and to implement advanced capabilities for water management march et al 2017 stewart et al 2018 ami provides the technology to collect big data about water consumption and to communicate unusual water consumption to consumers for identifying post meter water leaks giurco et al 2010 luciani et al 2019 smart meter data have been used to support the development of water demand management policies cominola et al 2015 near real time water distribution system models arandia perez et al 2014 gurung et al 2017 and enhanced hydraulic and water quality models gurung et al 2014 creaco et al 2017b ami data was also used to develop descriptive water demand models that were applied to identify appliance level end uses cardell oliver 2013 nguyen et al 2014 gurung et al 2015 to determine demand patterns for daily consumption profiles and hourly peak values beal and stewart 2014 cominola et al 2018b and to group households with similar consumption behaviors cardell oliver et al 2014 forecasting models were developed to predict water consumption at the next time step using data from smart meters that were placed at district metered areas dmas or at main pipes with sub minute reporting frequencies brentan et al 2018b donkor et al 2014 these models were developed using lagged demands or past consumption as predictor variables romano and kapelan 2014 chen and boccelli 2018 in addition to exogenous variables such as weather variables and social characteristics sebri 2016 hussien et al 2016 data reported at monthly and annual frequencies at the account level have been analyzed to explore the effects of pricing on water consumption arbues et al 2003 and the effects of rebate programs on the adoption of low flow appliances price et al 2014 account level data collected at high sub hourly resolution allows researchers to parameterize residential water demand models alvisi et al 2014 gargano et al 2016 kofinas et al 2018 creaco et al 2016 model water quality blokker et al 2008 analyze water end uses blokker et al 2009 buchberger and wells 1996 creaco et al 2017a mostafavi et al 2018 develop models to describe demand gurung et al 2017 develop conservation policies maas et al 2017 and evaluate feedback strategies to customers about their water consumption sonderlund et al 2016 account level data collected at medium resolution can be used to develop forecasting models forecasting models can be used within a portfolio of management tools to identify leaks explore water restriction policies during water supply interruptions and design demand management strategies to reduce peak demands monks et al 2019 the high spatial resolution at the user level and temporal resolution of smart meter data increase variability and the presence of zero valued data points in the data set these characteristics lead to difficulties in forecasting water consumption cominola et al 2018a variability emerges in the data due to factors including diverse end uses seasonality and socio economic conditions boyle et al 2013 as a result a limited number of studies have used account level consumption data collected at a medium temporal resolution to develop models to forecast or classify water demand aksela and aksela 2011 walker et al 2015 mckenna et al 2014 candelieri 2017 in this research new forecasting models are developed using machine learning ml methods for hourly smart water data ml has shown promising results for building predictive models for high resolution demands savic et al 2014 unlike mechanistic regression models ml techniques do not require the definition of an explicit relationship between water consumption and independent variables ml methods have been applied to model and forecast water demand using traditionally available water demand data such as billing records of water consumption at monthly or quarterly time steps jain and ormsbee 2002 adamowski 2008 odan et al 2012 duerr et al 2018 support vector regression svr artificial neural networks anns and random forests rfs have been applied to model water demands at aggregate levels such as the system and the dma levels mouatadid and adamowski 2017 gagliardi et al 2017 antunes et al 2018 anns have performed better than traditional regression models in forecasting short term water demands bougadis et al 2005 herrera et al 2010 compared the performance of different models to forecast hourly water demand collected at a dma in spain and found that svr anns and rfs performed similarly well to predict water demand the research presented here tests the application of three ml models including rfs anns and svr to forecast hourly water demand based on the information provided by smart meters data were retrieved from a set of 90 smart meters located in cary north carolina that reported hourly consumption in increments of 10 gallons per hour gph 1 gallon 3 78 l for a 12 month period in 2017 time series clustering is explored to improve the accuracy of forecasts by creating separate models for distinct hours of the day models are explored with observed demands the inclusion of weather and social variables as predictor variables results demonstrate that rf and ann models perform better than svr in accurately predicting water demands time series clustering improves model predictability two modeling approaches are compared in the first approach ml methods are developed for each smart meter separately to explore the level of predictability in the second approach the entire data set of smart meters is used to train a model to forecast water demand at any meter results demonstrate that the performance of the two modeling approaches is relatively similar however the individual models show a slightly lower error for existing meters and the group model predicts demands at new meters with lower error than individual models the models developed through this research provide new tools for water management by providing demand forecasts at both existing and new accounts 2 background four research studies explored the development of forecasting and classification models using smart water meter data these studies analyze hourly data collected at the account level using gaussian mixture models aksela and aksela 2011 mckenna et al 2014 a coupled evolutionary algorithm and ann approach walker et al 2015 and a support vector machine model candelieri 2017 these studies applied clustering to reduce the variability in data sets by grouping vectors of data based on consumption aksela and aksela 2011 mckenna et al 2014 or time of day candelieri 2017 to further reduce variability mckenna et al 2014 excluded weekends from the data set the four data sets described by these studies varied in size data were collected at 81 meters over a three month period aksela and aksela 2011 85 meters over a six month period mckenna et al 2014 nine meters over a two month period walker et al 2015 and 26 meters over a four month period candelieri 2017 these studies tested the time of day lagged demand and average consumption in a range of forms as demand predictors aksela and aksela 2011 used average weekly consumption to forecast water demand a week ahead whereas mckenna et al 2014 classified daily demand patterns walker et al 2015 explored model inputs including the reported water demand at the previous hour the average consumption of the prior seven days and the time of the day to improve model performance candelieri 2017 developed models to use the first six hours of consumption as predictors for the remaining 18 h of a day all the models showed the importance of clustering water demand and showed the use of some form of lagged demands previous demands in developing predictive models related research also explored how alternative predictors affect hourly or sub hourly demands research demonstrated that aggregate e g system level water consumption is significantly correlated with weather data praskievicz and chang 2009 house peters et al 2010 and property characteristics aitken et al 1994 whereas the models described in the paragraph above used only lagged water demands to forecast future demands recent work tested the correlation of weather and property characteristics parameters with account level hourly water demand reported by smart meters xenochristou et al 2019 more than 1500 smart meters reporting consumption for a 20 month period were analyzed to test predictors including the building area number of occupants household income and maximum daily temperature for correlation with hourly water demand results demonstrated that water consumption is correlated to air temperature especially during working days in the spring and summer seasons further research explored additional weather variables for correlation and demonstrated that precipitation did not influence demands xenochristou et al 2018 3 methods and materials this section describes the procedure applied to forecast water demand one hour ahead at the user level using hourly data reported by smart meters the first subsection describes smart meter data and exogenous predictor variables including weather data and characteristics of the households the second subsection describes the time series clustering approach that is applied to group data based on time of day and day of the week and the third subsection describes the ml methods that are applied to forecast water demand 3 1 data 3 1 1 water consumption data the data used in this study are collected from a set of 100 smart meters that report hourly consumption at the account level the data set represents a small sample of an ongoing project which has already installed more than 60 000 smart meters in cary north carolina usa town of cary nc the accounts represented in this data set are located throughout the city and include residential and non residential users water consumption was captured hourly during a 12 month period starting on january 1st of 2017 an iperl meter produced by sensus was installed at each house with a smart point transmitter and flexnet data transmission technology meters transmit the hourly data using a data logger to the water utility after a four hour period to preserve battery life the meters report data only when the hourly water consumption of the building is at least 10 gallons the hourly water demand of the set of smart meters is characterized by high variability the median of the data set is zero and the meters periodically report hourly consumption values greater than 10 gallons an empirical cumulative distribution function is used to represent the water consumption across all meters fig 1a as shown in fig 1a the majority of data points are zero for all meters which creates difficulties in predicting hourly demands the variability of the data set also contributes to challenging issues in modeling water demand preliminary analysis was performed to preserve data continuity the maximum number of data points reported by each meter is 8760 number of hours in 2017 and a meter was excluded from the analysis if it reported zero consumption for more than 720 consecutive hours approximately a month based on the assumption that those meters correspond to an empty building or a malfunctioning data logger ten smart meters were discarded as a result of this screening process of the 90 remaining meters ten meters are associated with accounts that are not single family residential accounts data from 85 meters are used in training validating and testing the models the data from the remaining five meters which are all associated with residential accounts are used in testing the models for performance at new meters outliers were identified using two criteria during one week of the observed period most of the data loggers stopped transmitting information due to a wide scale power outage when meters resumed working the data loggers reported cumulative values of water consumption generating misleading peaks these outliers were removed from the data set the second criterion applied a threshold consumption of 500 gph based on the experience of the utility the threshold value was exceeded by a minor number of data points less than 5 and no meters were discarded due to this criterion 3 1 2 weather data weather data were retrieved from the nearest meteorological station located at raleigh durham international airport rdu which is approximately seven miles 12 5 km away from the location of the households weather data that are included in this research are the last 24 h of air temperature f 32 f 0 c dew point f relative humidity the maximum hourly temperature of the last 24 h f hourly precipitation of the last 24 h in 1 in 25 4 mm and the occurrence of precipitation during the last 24 h as a binary variable hourly precipitation during 2017 in cary reports mostly zero values with some hours reporting up to one inch of rain the time sampling resolution of the weather variables is hourly fig 1b 3 1 3 household characteristics a set of data was collected describing household characteristics including the building area ft2 1 m2 10 76 ft2 the lot area acres 1 acre 4047 m2 the building age years and the property value u s data were retrieved from a gis database made available through the town of cary town of cary 2013 variation in household characteristics represented by the data set is shown in fig 1c 3 2 time series clustering similar to the work presented by candelieri 2017 this study uses a clustering algorithm to group the average hourly water demand of the households into clusters that are based on the time of day the k means clustering algorithm lloyd 1982 was applied to the average consumption at each hour of the day where the average is calculated across all meters the number of clusters was explored for its effects on the model performance and silhouette analysis was used to quantitatively assess the most efficient number of clusters arbelaitz et al 2013 the k means algorithm david and vassilvitskii 2007 was used to initialize cluster centers which improves the running time of the k means algorithm and a random seed was used to initialize random clusters for a set of simulations clusters were generated using the sum of absolute differences as the distance metric to account for variability due to the random generation of cluster centers the clustering subroutine was run multiple times and the most commonly repeated clusters were selected 3 3 machine learning methods three ml methods were applied to forecast short term water demand rfs anns and svr were implemented using methods available through matlab 2019a mathworks com due to the range of the magnitude of the variables the input data were normalized for the ann models min max scaling was applied to normalize the data and convert the data into a range of 1 and 1 values for the svr models data were standardized by centering and scaling each column of the predictor set using the mean and the standard deviation of each predictor respectively for the rf model input data were used directly 3 3 1 random forests rfs are represented by ensembles of decision trees which are expanding structures of nodes with the application of binary splits breiman et al 1984 each node represents a predictor variable the initial value of each node is the average of the response variable over all the observations of that variable splits are formed by using the inequality condition and the performance of a split is evaluated through the gini index which measures how diverse the data are until a terminal node is reached the tree size is determined based on the number of nodes which is used as input to minimize the variance of each split rfs are created through a bootstrap aggregation bagging process breiman 1996 and data are re sampled randomly with replacement the use of ensemble modeling can improve the overall performance of the model though overfitting and complexity issues may emerge breiman 2001 algorithmic settings for rf models include the number of trees in an ensemble and the leaf size which is the minimum number of observations per terminal node herrera et al 2010 villarin and rodriguez galiano 2019 rfs are modeled by joining several individual decision trees which has proved to provide better results in terms of accuracy an ensemble of decision trees creates a model that can be considered a gray box where understanding which parameters lead to a good performance is more difficult compared to models that are built using a single decision tree the leaf size refers to the number of observations evaluated at each node where a low number of observations will generate deep trees that may overfit data on the other hand a high number of observations may lead to poor model performance these settings affect model accuracy and the computing time 3 3 2 artificial neural networks anns are widely used in water systems applications adamowski 2008 herrera et al 2010 romano and kapelan 2014 anns are modeled after the human brain to simulate the mechanisms of human neurons to collect analyze and transmit information through different layers haykin 2008 this study uses a feed forward neural network with input hidden and output layers the neurons of the input layer correspond to each of the predictor variables the output layer represents the response variable which is the forecasted water demand and the hidden layer nodes apply the activation function bias component and weights to transform the input data mathematically the process is described as 1 y i f j 1 m w i j x j b i where y i is the response variable i is the corresponding data point f represents the activation function which typically corresponds to an s shaped function m is the number of inputs w i j is the weight applied to the j th input signal x j is the j th input signal predictor value and b i represents the bias applied to the data at the i data point a back propagation algorithm is used to iteratively adjust the connection among neurons bias and weights and improve the value of the mean squared error mse that is calculated based on the modeled and the observed values in the training process ann settings that should be identified are the number of hidden layers and the number of neurons within each layer hidden layers separate nonlinear data to improve the predictive capabilities of the model each hidden layer uses eq 1 in the learning process and the weights and bias terms are stored in the neurons of each hidden layer the number of neurons in the hidden layer generally corresponds to the double of the number of predictors cutore et al 2008 more predictors increase the number of neurons and the complexity of an ann model 3 3 3 support vector regression svr applies a transformation or kernel function to map a non linear data set into a linear function in a high dimensional feature space haykin 2008 gaussian linear and polynomial functions can be used within svr to transform data the mathematical formulation of svr is represented as smola and scholkopf 2004 2 f x w ϕ x b where f x is the response value that should fall within the bandwidth defined by an allowable margin ϵ for all modeled data the support vectors define the feature space from ϵ to ϵ ϕ is the mapping function to transform non linear relations into linear functions in eq 2 w ϕ x represents the dot product of the weight vector w and the transformed input data set ϕ x and b is the bias applied to the function approximation a convex optimization problem is solved to identify the smallest value of the error between modeled and observed data a cost or box constraint controls with a positive numeric value the penalty for outputs that lie outside of the allowable margin ϵ and helps to prevent overfitting to apply svr the user should select the type of transformation or kernel function e g linear polynomial or gaussian function the value of the cost or box constraint the bandwidth margin ϵ and the kernel scale scaling the kernel function plays an important role on the performance of svr as explained by williams et al 2005 in this research the kernel scale is included as a setting to evaluate model performance 4 time clustering results the k means clustering algorithm was applied to cluster the average hourly water demand based on the time of day the number of clusters was selected based on two criteria first a quantitative approach with different numbers of clusters 2 3 and 4 was applied and evaluated using the silhouette analysis as described in section 3 2 two clusters reported the highest silhouette value 0 72 similar to results reported by candelieri 2017 who also used two clusters to group water demand data the use of two clusters also agrees with the emergent characteristics of daily water demands that is water consumption data typically follows a diurnal pattern in which two peak values occur one peak in the morning around 7 am and another peak in the early evening around 6 pm adamowski 2008 buchberger et al 2017 the smart meter data set also demonstrates these peaks in the average daily demand in the morning and evening during the weekdays the first cluster starts at 3 am and continues through the 9 am hour 7 h the peak consumption as shown in fig 2a occurs at 7 am with an average consumption of up to 15 gph between 7 am and 8 am on wednesdays for weekdays the evening peak occurs at 8 pm except for thursday and friday where the peak consumption is at 5 pm the second cluster begins at 10 am and continues to 2 am 17 h on weekend days the clusters and the peaks change the morning peak consumption occurs at 10 am cluster 1 begins at 2 am and continues to 10 am 9 h and the duration of cluster 2 is 15 h similarly the night peak consumption during the weekends as shown in fig 2b occurs at 9 pm with a pronounced difference between saturday and sunday 5 modeling results 5 1 experimental design the predictive model relies on several predictors to forecast water demand one hour ahead at the user level a set of experiments was conducted to test the importance of these predictor variables data clusters and size of data sets on model performance three sets of predictor variables are created to forecast water demand grouped as demand and seasonality ds variables weather w variables and property characteristics variables ch table 1 four input data sets are created with data grouped in alternative sets of clusters table 2 the first input all data includes all data points from a data set without the use of any clustering the second data set wd we clusters data into two clusters based on weekdays and weekend days for the third and fourth data sets hour and wd we hour clusters were used that are based on the time of day and identified through the use of the k means clustering algorithm as shown in section 4 finally two different data set sizes are explored for individual data sets individual one model is trained for each smart meter where each individual model has 8760 data points corresponding to the number of hours in 2017 for the group data set group one model is trained using the entire data set of 85 smart meters where the group model has 744 600 data points corresponding to the product of the number of hours in a year and the number of meters this study evaluates the effects of smart metered data as the main predictor to forecast water demand ds predictor the inclusion of additional sets of predictors ds w ds ch and ds w ch different clusters based on time of day and day of the week all data wd we hour wd we hour and different size of the data sets individual group are explored as they affect the capabilities of the forecasting model experiments for each combination of predictor set cluster and data set size are conducted table 3 for example one model is developed to forecast water demand using only previously recorded consumption and seasonality data clustered by hour of the day during all days of the week for a single household this combination corresponds to the settings ds hour individual table 3 based on the combination of predictors clusters and data set size a total of 24 experiments are created and for each experiment the three ml methods are applied and evaluated similar to the work presented by walker et al 2015 to initialize each model the first week of water consumption is stored to use as predictors pwsh in table 1 of the forecasting model the root mean squared error rmse is used as a metric of performance to evaluate the models for individual models rmse is calculated for each of the 85 meters and for each experiment as shown in eq 3 3 r m s e j i 1 n j d p r e d i d o b s i 2 n j where rmse j is the rmse for the j th meter for the training validation or test data set and there are n j data points associated with the j th meter d p r e d i is the i th demand predicted using a forecasting model and d o b s i is the i th demand observed note that different data points d p r e d i may be predicted using different models at one meter if data were clustered each model was trained 30 times for one meter to account for stochasticity of the ml methods and the average rmse associated with each meter is reported as the average across the 30 trials in gph to aggregate the rmse value for comparison among the combination of model settings we report the median across the meters of the average rmse values for group models rmse is evaluated across the entire data set as shown in eq 4 4 r m s e i 1 n d p r e d i d o b s i 2 n where rmse is calculated for the training validation or test data set of n data points across multiple meters again different data points may be predicted using different models if data were clustered for group models rmse is the average performance across 30 trials the spearman s rank order correlation r s is used as a second metric to define the strength of a monotonic relationship between observed and predicted water demand the spearman s rank correlation is used instead of the pearson s correlation r because a linear relationship is not apparent in this data which is a zero inflated date set myers and sirois 2004 the spearman s rank correlation was not aggregated and is reported in the results section for each meter for each of the 24 experiments table 3 data are divided into training validation and test sets for model developing training data are used for the model to learn from the data the validation data set is used to identify the model parameters to best fit the modeled outputs with the observed data while reducing overfitting finally the model is applied to a test data set to evaluate its performance james et al 2013 a random sampling without replacement algorithm was used to divide the data set ensuring that no overlapping occurs based on previously conducted studies mouatadid and adamowski 2017 guo et al 2018 80 of data were used for training 10 for validation and 10 for testing because of the noisiness in the water demand data large training sets are needed to guide the learning procedure that is used to develop the model only a few settings are determined based on the performance for validation data and a small data set is sufficient to make those selections to evaluate the effects of random sampling the data partition was re initialized for each of the 30 trials the approach leaves 10 of data out for each run thus applying a hold out cross validation technique to generalize the model 5 2 machine learning settings analysis preliminary analysis was explored to set algorithmic parameters for each ml approach a set of 10 smart meters was randomly selected from the set of 85 meters for this analysis the predictor variable was set as ds no clustering was applied cluster all data and individual data sets were used to train one individual model separately for each meter a selection of settings were evaluated for each ml method based on previously conducted work herrera et al 2010 antunes et al 2018 each ml method was run 30 times and the average and standard deviation of the rmse values for each meter were calculated the settings that produced the lowest average rmse across the 30 trials were used for the remainder of the study described in this manuscript because the differences in the mean of rmse values reported in tables 4 and 5 are small for each combination of settings the computational complexity e g computational time of executing each method was also considered when selecting settings 5 2 1 random forest settings the rf model was developed using the bootstrap aggregation technique breiman 1996 and algorithmic settings include the number of trees and the minimum number of observations per tree leaf three values were evaluated for both parameters 50 100 and 200 the range for evaluating the number of trees was based on research showing that 50 or fewer trees lead to accurate predictions antunes et al 2018 another study demonstrates that using more than 200 trees increases the execution time beyond practical limits villarin and rodriguez galiano 2019 within this range the best performance was found for 100 trees and a leaf size of 50 table 4 5 2 2 artificial neural network settings an ann was applied to explore settings for the number of hidden layers and the number of neurons per layer based on previous applications of ml methods for water demand herrera et al 2010 mouatadid and adamowski 2017 antunes et al 2018 potential settings for both parameters were identified as 1 20 and 30 and a total of nine combinations were analyzed the best settings are selected based on both model performance and computing time as one hidden layer and 20 neurons table 4 previous predictive models have also used relatively low numbers of neurons as a characteristic of shallow neural networks cutore et al 2008 herrera et al 2010 antunes et al 2018 5 2 3 support vector regression settings the box constraint b c and bandwidth ϵ were evaluated based on procedures shown by mouatadid and adamowski 2017 and fan et al 2005 the effects of the kernel scale k s were also evaluated based on previous work williams et al 2005 the setting for the box constraint is based on the value of the hourly water demand which ranges from 0 to 500 gph the box constraint was tested at settings of 50 500 and 1000 similarly the allowable margin ϵ that defines the feature space was evaluated using values in the same order of magnitude as the reported hourly water demand at 10 50 and 100 the kernel scale was evaluated at values of 10 50 and 100 this analysis includes a total of 27 combinations the settings of b c 50 k s 10 and ϵ 10 generate the lowest rmse value table 5 many of the rmse values reported by svr were higher than those reported by rf and ann a bayesian optimization algorithm gelbart et al 2014 is available in the matlab toolbox and was applied to improve the performance of the svr models the bayesian optimization subroutine is constrained by the size of the data set and could only be applied to train individual models the performance of the svr that was found using the optimization procedure is approximately the same as the best values shown in boldface reported in table 5 5 3 models for individual meters the performance of the models used for the individual data sets is reported in this section each of the 85 smart meters is included in the analysis and predictor variable sets ds and ds w and alternative settings for clustering are evaluated the settings defined in section 5 2 are applied to train the individual models ann with one hidden layer and 20 neurons rf with 100 trees and at least 50 observations per leaf and svr with box constraint equal to 50 kernel scale equal to 10 and ϵ equal to 10 the median of the average rmse for each meter is reported in table 6 all results are reported for test data for each of the ml methods the median of the rmse values decreases when data are clustered the lowest error is reported as 9 5 gph by the rf model using the ds predictor set and clustering by days and hours wd we hour for these settings the average rmse reported for 85 meters ranges from 4 3 gph to 80 1 gph out of the 85 meters 28 meters report an average rmse value less than 10 gph which is the resolution of the data five meters report an average rmse greater that 40 gph for these five meters the demand pattern is erratic with multiple changes between low e g 10 gph and high e g 100 500 gph demands during a 24 h period three of these five meters correspond to other meters that are not associated with single family residential accounts which may explain a lack of pattern in the demand data the individual models however do not perform poorly overall in simulating demands at other meters based on rmse values the distributions of rmse values for single family residential meters and other meters are similar the ann models with the same settings ds predictor set and wd we hour cluster reported similar rmse values with a median of the averages of 9 5 gph the highest error is reported by svr the lowest rmse value reported by the svr models is 8 higher than the rmse found using the rf and ann models and corresponds to the ds predictor set with wd we hour cluster the longest computational time for training a model for one individual meter was approximately 30 s using the all data cluster and the ds w predictor set and the time required was similar for each of the ml methods a pc with an i7 processor and 16 0 gb of ram was used for the experiments the spearman s rank correlation r s is also shown for individual models using the ds predictor set alone fig 3 because this set performed best based on rmse values as shown above table 6 when the entire data set is used without clustering all data the interquartile range of r s for rf spans from 0 36 to 0 44 and meters reporting an r s greater than 0 58 are considered as outliers similar to the results reported for rmse values rf reports the highest median of r s 0 42 for the ds predictor set and the all data cluster based on r s values the individual models perform similarly for single family residential meters and other meters models based on data that are clustered using the hours cluster shows less variability than other clusters shown in the bottom row of fig 3 while clustering improved the rmse value associated with models clustering does not similarly increase the value of r s a similar set of subplots was generated for the ds w predictor variables and the results are similar to those reported in this section 5 4 models for group data set the results of the ml models applied to the group data set are presented in this section these models use the entire data set of water demand reported by the 85 smart meters during 2017 and the same set of clusters were tested for developing models table 7 the lowest error is reported by rf corresponding to the predictor variable set ds w ch and the wd we hour cluster using the all data cluster the rf models outperform ann and svr models across the different set of predictors ann models perform similar to rf models for the days of week cluster reporting an rmse of 17 8 gph with the predictors ds ch and ds w respectively using the hour cluster the lowest error is found using rf and the ds w ch predictor set with an rmse of 16 7 gph 4 above the lowest value unlike the models trained using the individual data set the models trained using the group data set do not show performance that monotonically improves with clustering the longest computational time for training the group model varied among the ml methods ann training took thirty minutes rf required four hours and svr required six hours using the all data cluster and the ds w ch predictor set for each method the running time did not vary with the type of predictor set but it did vary with the type of cluster where clusters reporting large data sets e g all data took longer than small data sets e g wd we hour similar to the results reported for the individual models the results presented in tables 7 and 8 correspond to the test data set for the group models the highest average value of the spearman s rank correlation is found when using the hour cluster table 8 the comparison between ml methods shows that rf report the highest r s values across the predictor sets and the clusters the predictor set that produces the highest r s corresponded to ds similar to the individual models rf and svr report the highest and lowest values of r s respectively when the entire data set is used all data r s ranges from 0 37 to 0 52 in this case clustering decreases the value of the spearman s rank correlation when the size of the data set is smallest wd we hour r s is reported at the lowest value 0 15 including other variables in the predictor set does not improve r s 5 5 feature importance analysis analysis of feature importance was conducted for the rf models the predictor sets were ds w and ds w ch for the individual and group models respectively and the all data cluster was used the importance of each predictor was found using the tree based iterative input selection algorithm galelli and castelletti 2013 fig 4a shows the most repeated results of feature importance among the 85 individual models hour of the day t i m e the average consumption of the previous 24 h a v 24 h r and day of the week d a y are the three most important predictors fig 4b shows the feature importance results of the group model the most important features in the group model are the consumption of the last week at the same hour p w s h followed by the average consumption of the previous 24 h a v 24 h r and the hour of the day h o u r new individual and group models were generated using only the three most important predictors for each model type and the performance of the models in terms of rmse and r s showed negligible improvement 5 6 comparing the performance of individual and group models the analysis conducted above explores the best settings to obtain individual and group models in this section the individual and group models are compared to provide recommendations about developing predictive models for application in the field individual models are trained to match data from a specific meter and it is expected that an individual model could precisely model the behavior at that meter group models on the other hand are developed using a larger data set which may improve the performance over all meters in addition group models should perform better for new meters that have not been used to train the model to compare the two approaches we select the best individual and group models based on the experiments which reported the highest r s values in sections 5 3 and 5 4 respectively the experiments reporting the best performance for the individual and group models correspond to rf the ds predictor set and the all data cluster first the performance of the best individual and best group model for one smart meter are shown as the observed and modeled water demand of hourly consumption fig 5 to provide a more detailed visualization of the time series fig 6 shows the same data during may 2017 the meter was selected randomly from the set of 85 meters both models capture the trends in water demands over the 8760 h but the peak values are not accurately identified the rmse value reported for the individual model calculated over the 8760 h is 30 gph and the rmse for the group model is 38 gph the spearman s rank correlation r s is 0 87 and 0 75 for the individual and group models respectively the best individual and best group models were then compared based on their capability to predict test data for each smart meter the test data set of each meter includes 876 values of water demand values the test data set is defined as 10 of the data the cumulative distribution plot fig 7 shows that for the individual and group models approximately 30 of the meters report rmse values below 12 gph at the upper limit of the distribution of errors 10 of the meters report rmse values above 25 gph a two sample kolmogrov smirnov hypothesis test marsaglia et al 2003 is applied to the rmse of the models and the test does not reject the null hypothesis that the results from the individual and group models are from populations with the same distribution at the 5 significance level as shown in fig 7 therefore the results obtained from the best individual and group models in terms of rmse are not significantly different the r s coefficient is also calculated to compare the values of the individual and group models for the test data fig 8 shows that the group model reports a stronger monotonic relationship between observed and modeled data than the individual model the two sample kolmogrov smirnov hypothesis test confirms that the group model generates higher r s values than the individual model at the 5 significance level five meters from the original set of 90 meters were used to test the ability of the best individual and group models to predict demands for new data sets similar to the 85 original meters each of the five meters reports 8760 hourly water demand values during 2017 the rmse value generated by both models ranges from 4 73 to 28 gph the group model shows slightly lower errors than the individual model fig 9 this is because the best group model was trained using a larger data set than the best individual model and the predictive capability of the best group model for new data sets is higher the two sample kolmogrov smirnov was applied to test the null hypothesis that rmse from the best individual and group models comes from population with the same distribution and the result indicates that the test does not reject the null hypothesis at the 5 significance level for the r s coefficient the median value reported by the individual and group models was 0 23 and 0 45 respectively the two sample test does not reject the null hypothesis that the r s values come from populations with the same distributions these results demonstrate that there is no statistical difference in performance of the best individual and group models based on rmse or r s at the 5 significance level for five new meters 6 discussion this research tests the application of three ml methods for forecasting water demands on an hourly basis at individual accounts forecasting demands at individual accounts one hour ahead can enable a utility to identify abnormal consumption when comparing modeled and observed values with errors of 10 gph forecasting water demand with one hour resolution data is a challenging modeling task and the precision of forecasting models may be improved through further research however this margin of error may be sufficient to identify a running toilet for example which can account for a loss of up to 140 gph us epa water sense to compare the performance of this work with previously conducted research reported by walker et al 2015 we calculate the pearson s correlation coefficient r for each of the 85 meters using the best group model our results show a range of r values from 0 25 to 0 80 which is a value higher than the range obtained by walker et al 2015 they report r values ranging from to 0 30 to 0 65 for predicting hourly water demands at nine account level meters we focus on the use of r s in our analysis above to evaluate the monotonic relationship between observed and modeled data instead of a linear relationship which is represented by the calculation of r due to the volumetric resolution of the smart meters 10 gallons the observed data set includes many zeros and the mape cannot be calculated to compare these models with results reported by candelieri 2017 previous research that explored the use of svr used only the time of day as a predictor candelieri et al 2015 while this research explores alternative representations of previous water consumption weather variables and property characteristics as predictors the methodology proposed here includes weekends in the analysis and explores the effect of demands that change during the weekend previous work in water demand using smart meters removed weekends from the data set to reduce noisiness in the data mckenna et al 2014 mckenna et al 2014 do not report performance of their models for comparison purposes the approach presented here does not rely on information about the type of user account i e residential vs non residential which gives the process general applicability the models perform similarly well for single family residential meters and other meters the data set is limited in representing types of other meters and further research can explore the effects of user types on model performance through additional data clustering the data set to differentiate weekdays and weekends was in general effective in finding models with reduced errors the performance of the best individual model for weekdays and weekends differs by only 1 8 demonstrating that the model can perform similarly well for both weekdays and weekends with slightly better outcomes for weekdays in terms of rmse the models that were developed in this research had limited capabilities to accurately predict demand peaks fig 5 because most peaks do not follow a periodic or predictable pattern the hourly volumetric resolution of the meters 10 gallons produces a zero inflated time series which creates difficulties in developing forecasting models the number of meters allows us to test the capabilities of the models on new data sets using only five smart meters future work can use larger sets of smart meters to develop models training models for big data is computationally expensive and optimization techniques were not applied when training the group models due to the impracticability of the required computational time ann is one of the most used techniques in the urban water demand field as shown in previous works adamowski 2008 romano and kapelan 2014 walker et al 2015 this research found that rf performs similar to or better than ann as suggested by herrera et al 2010 this study did not explore the effects of different types of normalization to train a neural network and only applied the min max scaling future work may explore the outcome of applying different normalization techniques the application of rf presents advantages for a better understanding of the model in terms of feature importance as shown by villarin and rodriguez galiano 2019 computing time is also an important criterion in the selection of models rf ann and svr take around 30 s to train an individual model using a pc with an i7 processor and 16 0 gb of ram when working with a group model ann takes thirty minutes to train whereas rf and svr take four and six hours respectively while rf generates models with higher correlations the time required for training models is much longer than the time required for anns deployment strategies vary among utilities in the temporal resolution at which data are collected which affects not only the cost of maintaining an ami system but also the type of analysis and modeling that can be conducted mckenna et al 2012 hourly data which may be considered as the upper limit of high temporal resolution is too coarse to identify end uses or support water quality modeling but the data can be analyzed to detect anomalies in post meter water consumption britton et al 2013 and to forecast water demands candelieri et al 2015 the models developed here can be applied to forecast anomalies and send alerts to consumers in the data set explored in this research water consumption is reported in increments of 10 gph this level of resolution can affect the performance of the ml methods explored here which report continuous values as model output classifier approaches may provide a better performance for application to this data set the precision that can be achieved using a classifier approach will affect the time required to train the classifier and exploration of classifier methods was outside of the scope of the work presented here the ml methods described here are broadly applicable across smart meter data sets of varying resolution and frequency as technology for power or battery life and communication improves data sets collected at smart meters may continue to increase in resolution and frequency which may increase the variability of demands trade offs between precision and predictive capabilities for increasingly high resolution data sets should be explored in future research 7 conclusions this research develops a set of models to forecast water demand using data reported by smart meters installed at the user level the models are developed to forecast water demand at the subsequent time step the input data used as predictors consist of lagged or previously observed water demand weather variables and characteristics of the households models were trained for individual meters and for the data set as a whole individual and group models were compared using test data which was randomly selected from the time series each individual model was trained specifically for a meter and was able to continue to predict demands at that meter only marginally better than the group model for all the predictor variables and clusters analyzed when comparing the best individual model with the best group model using the test data set of each smart meter the results in terms of rmse are not different at the 5 significance level however for the group model the correlation between modeled and observed data as measured by the spearman s rank correlation r s is higher than the r s of the individual model at the 5 significance level a third comparison was performed to test the best individual model and the best group model for a new set of five meters based on that comparison the group model performed marginally better than the individual model the group model used a much larger data set in training and was able to predict demands at new meters better than an individual model which was trained using a limited data set however when evaluating the results using a two sample kolmogrov smirnov test the rmse values reported by individual and group models are not different at the 5 significance level three ml methods were applied to forecast water demand based on regression rfs anns and svr despite fine tuning the methods with similar combination of settings reported by previous works herrera et al 2010 mouatadid and adamowski 2017 antunes et al 2018 rf and ann models outperformed svr in all the applications for the individual models a bayesian optimization of the hyperparameters was applied to svr and the rmse values remained higher than those reported by rf and ann models this optimization technique was feasible only for the individual models that have around 8700 data points for the group models around 740 000 data points the svr settings were fixed and the error followed the same pattern as the individual models this conclusion agrees with those reported by brentan et al 2018a who applied svr for water demand data the inclusion of exogenous variables such as weather and property characteristics only marginally improves the model performance effectively most of the information from these ancillary variables is already captured in the consumption data for individual models the performance of demand driven models is not affected by the inclusion of weather variables the results are in accordance with previous works that did not find a significant correlation between short term hourly water demand and weather variables group models showed a slight improvement in performance due to the inclusion of weather variables and characteristics of the households building area lot area building age and property value in the array of predictors while an improvement was observed with the inclusion of these exogenous variables obtaining this information may be impractical characteristics of the households are obtained from census data or surveys and this data may not be public to protect the privacy of constituents seasonality was analyzed by clustering data for weekdays and weekends and based on the time of the day the performance of both individual and group models improved by clustering data where no clustering resulted in the highest errors and the highest level of clustering resulted in the lowest errors differentiating weekdays from weekends and clustering for the time of the day resulted in the lowest error for the individual models whereas only clustering for weekdays and weekends reported the lowest error for the group model in summary clustering for seasonality improved individual models more than group models the median rmse value reported by the models varies from 9 5 to 16 gph of water giving some level of confidence for using models to alert customers of high water use anomalies that indicate potential post meter leaks when comparing actual and predicted consumption the meter resolution fundamentally affects the performance of forecasting models this variability is expected when working with noisy data which is a characteristic of individual accounts at hourly temporal resolution a higher resolution may lead to a better performance of forecasting models and trade offs may emerge in precision and variability in the data set which is in accordance with recent works cominola et al 2018a future work can explore the use of classifiers for forecasting data reported in discrete intervals real time forecasting methods have a critical role in smart water management and provide a tool for identifying leaks encouraging conservation and shaving peak demands we anticipate that further research will demonstrate the utility of these models in enhancing the performance of water distribution infrastructure declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported in part by the unc inter institutional planning grant program usa and the ecuadorian government through the secretaría nacional de educación superior ciencia tecnología e innovación senescyt the authors express their thanks to the town of cary for sharing data and collaborating in this research 
26074,smart meters installed at the user level provide a new data source for managing water infrastructure this research explores the use of machine learning methods including random forests rfs artificial neural networks anns and support vector regression svr to forecast hourly water demand at 90 accounts using smart metered data demands are predicted using lagged demand seasonality weather and household characteristics time series clustering is applied to delineate data based on the time of day and day of the week which improves model performance two modeling approaches are compared individual models are developed separately for each meter and a group model is trained using a data set of multiple meters individual models predict demands at meters in the original data set with lower error than group models while the group model predicts demands at new meters with lower error than individual models results demonstrate that rf and ann perform better than svr across all scenarios keywords smart water meters ami forecasting model hourly water demand user level data water demand management machine learning urban water systems 1 introduction designing and operating water distribution systems rely on models and forecasts of water consumption water managers use operational or short term water demand forecasts ranging from one day to a few weeks to efficiently manage devices such as pumps and valves short term forecasting models are based on data collected at the account level using data that have historically been collected at monthly or quarterly intervals corresponding to billing cycles recently municipalities and utilities have deployed smart water meters in the context of the smart cities paradigm berglund et al 2020 providing new data about account level demands at hourly or sub hourly frequencies some utilities use a time resolution for reporting demands in the range of 15 s to 30 min battery life and transmitting issues however limit the frequency of data collection and other utilities collect water demand at an hourly frequency beal and flynn 2015 the use of smart meter data for developing models for forecasting demands at sub hourly or hourly frequencies is limited to date unlike the energy sector in which smart meters have been extensively deployed to forecast consumption as part of advanced metering infrastructure ami projects kavousian et al 2013 the water sector has not benefited from the development and use of models that forecast water demand at the account or user level with high temporal resolution ami can be deployed by water utilities to gain insight into water consumption at high spatial and temporal resolutions and to implement advanced capabilities for water management march et al 2017 stewart et al 2018 ami provides the technology to collect big data about water consumption and to communicate unusual water consumption to consumers for identifying post meter water leaks giurco et al 2010 luciani et al 2019 smart meter data have been used to support the development of water demand management policies cominola et al 2015 near real time water distribution system models arandia perez et al 2014 gurung et al 2017 and enhanced hydraulic and water quality models gurung et al 2014 creaco et al 2017b ami data was also used to develop descriptive water demand models that were applied to identify appliance level end uses cardell oliver 2013 nguyen et al 2014 gurung et al 2015 to determine demand patterns for daily consumption profiles and hourly peak values beal and stewart 2014 cominola et al 2018b and to group households with similar consumption behaviors cardell oliver et al 2014 forecasting models were developed to predict water consumption at the next time step using data from smart meters that were placed at district metered areas dmas or at main pipes with sub minute reporting frequencies brentan et al 2018b donkor et al 2014 these models were developed using lagged demands or past consumption as predictor variables romano and kapelan 2014 chen and boccelli 2018 in addition to exogenous variables such as weather variables and social characteristics sebri 2016 hussien et al 2016 data reported at monthly and annual frequencies at the account level have been analyzed to explore the effects of pricing on water consumption arbues et al 2003 and the effects of rebate programs on the adoption of low flow appliances price et al 2014 account level data collected at high sub hourly resolution allows researchers to parameterize residential water demand models alvisi et al 2014 gargano et al 2016 kofinas et al 2018 creaco et al 2016 model water quality blokker et al 2008 analyze water end uses blokker et al 2009 buchberger and wells 1996 creaco et al 2017a mostafavi et al 2018 develop models to describe demand gurung et al 2017 develop conservation policies maas et al 2017 and evaluate feedback strategies to customers about their water consumption sonderlund et al 2016 account level data collected at medium resolution can be used to develop forecasting models forecasting models can be used within a portfolio of management tools to identify leaks explore water restriction policies during water supply interruptions and design demand management strategies to reduce peak demands monks et al 2019 the high spatial resolution at the user level and temporal resolution of smart meter data increase variability and the presence of zero valued data points in the data set these characteristics lead to difficulties in forecasting water consumption cominola et al 2018a variability emerges in the data due to factors including diverse end uses seasonality and socio economic conditions boyle et al 2013 as a result a limited number of studies have used account level consumption data collected at a medium temporal resolution to develop models to forecast or classify water demand aksela and aksela 2011 walker et al 2015 mckenna et al 2014 candelieri 2017 in this research new forecasting models are developed using machine learning ml methods for hourly smart water data ml has shown promising results for building predictive models for high resolution demands savic et al 2014 unlike mechanistic regression models ml techniques do not require the definition of an explicit relationship between water consumption and independent variables ml methods have been applied to model and forecast water demand using traditionally available water demand data such as billing records of water consumption at monthly or quarterly time steps jain and ormsbee 2002 adamowski 2008 odan et al 2012 duerr et al 2018 support vector regression svr artificial neural networks anns and random forests rfs have been applied to model water demands at aggregate levels such as the system and the dma levels mouatadid and adamowski 2017 gagliardi et al 2017 antunes et al 2018 anns have performed better than traditional regression models in forecasting short term water demands bougadis et al 2005 herrera et al 2010 compared the performance of different models to forecast hourly water demand collected at a dma in spain and found that svr anns and rfs performed similarly well to predict water demand the research presented here tests the application of three ml models including rfs anns and svr to forecast hourly water demand based on the information provided by smart meters data were retrieved from a set of 90 smart meters located in cary north carolina that reported hourly consumption in increments of 10 gallons per hour gph 1 gallon 3 78 l for a 12 month period in 2017 time series clustering is explored to improve the accuracy of forecasts by creating separate models for distinct hours of the day models are explored with observed demands the inclusion of weather and social variables as predictor variables results demonstrate that rf and ann models perform better than svr in accurately predicting water demands time series clustering improves model predictability two modeling approaches are compared in the first approach ml methods are developed for each smart meter separately to explore the level of predictability in the second approach the entire data set of smart meters is used to train a model to forecast water demand at any meter results demonstrate that the performance of the two modeling approaches is relatively similar however the individual models show a slightly lower error for existing meters and the group model predicts demands at new meters with lower error than individual models the models developed through this research provide new tools for water management by providing demand forecasts at both existing and new accounts 2 background four research studies explored the development of forecasting and classification models using smart water meter data these studies analyze hourly data collected at the account level using gaussian mixture models aksela and aksela 2011 mckenna et al 2014 a coupled evolutionary algorithm and ann approach walker et al 2015 and a support vector machine model candelieri 2017 these studies applied clustering to reduce the variability in data sets by grouping vectors of data based on consumption aksela and aksela 2011 mckenna et al 2014 or time of day candelieri 2017 to further reduce variability mckenna et al 2014 excluded weekends from the data set the four data sets described by these studies varied in size data were collected at 81 meters over a three month period aksela and aksela 2011 85 meters over a six month period mckenna et al 2014 nine meters over a two month period walker et al 2015 and 26 meters over a four month period candelieri 2017 these studies tested the time of day lagged demand and average consumption in a range of forms as demand predictors aksela and aksela 2011 used average weekly consumption to forecast water demand a week ahead whereas mckenna et al 2014 classified daily demand patterns walker et al 2015 explored model inputs including the reported water demand at the previous hour the average consumption of the prior seven days and the time of the day to improve model performance candelieri 2017 developed models to use the first six hours of consumption as predictors for the remaining 18 h of a day all the models showed the importance of clustering water demand and showed the use of some form of lagged demands previous demands in developing predictive models related research also explored how alternative predictors affect hourly or sub hourly demands research demonstrated that aggregate e g system level water consumption is significantly correlated with weather data praskievicz and chang 2009 house peters et al 2010 and property characteristics aitken et al 1994 whereas the models described in the paragraph above used only lagged water demands to forecast future demands recent work tested the correlation of weather and property characteristics parameters with account level hourly water demand reported by smart meters xenochristou et al 2019 more than 1500 smart meters reporting consumption for a 20 month period were analyzed to test predictors including the building area number of occupants household income and maximum daily temperature for correlation with hourly water demand results demonstrated that water consumption is correlated to air temperature especially during working days in the spring and summer seasons further research explored additional weather variables for correlation and demonstrated that precipitation did not influence demands xenochristou et al 2018 3 methods and materials this section describes the procedure applied to forecast water demand one hour ahead at the user level using hourly data reported by smart meters the first subsection describes smart meter data and exogenous predictor variables including weather data and characteristics of the households the second subsection describes the time series clustering approach that is applied to group data based on time of day and day of the week and the third subsection describes the ml methods that are applied to forecast water demand 3 1 data 3 1 1 water consumption data the data used in this study are collected from a set of 100 smart meters that report hourly consumption at the account level the data set represents a small sample of an ongoing project which has already installed more than 60 000 smart meters in cary north carolina usa town of cary nc the accounts represented in this data set are located throughout the city and include residential and non residential users water consumption was captured hourly during a 12 month period starting on january 1st of 2017 an iperl meter produced by sensus was installed at each house with a smart point transmitter and flexnet data transmission technology meters transmit the hourly data using a data logger to the water utility after a four hour period to preserve battery life the meters report data only when the hourly water consumption of the building is at least 10 gallons the hourly water demand of the set of smart meters is characterized by high variability the median of the data set is zero and the meters periodically report hourly consumption values greater than 10 gallons an empirical cumulative distribution function is used to represent the water consumption across all meters fig 1a as shown in fig 1a the majority of data points are zero for all meters which creates difficulties in predicting hourly demands the variability of the data set also contributes to challenging issues in modeling water demand preliminary analysis was performed to preserve data continuity the maximum number of data points reported by each meter is 8760 number of hours in 2017 and a meter was excluded from the analysis if it reported zero consumption for more than 720 consecutive hours approximately a month based on the assumption that those meters correspond to an empty building or a malfunctioning data logger ten smart meters were discarded as a result of this screening process of the 90 remaining meters ten meters are associated with accounts that are not single family residential accounts data from 85 meters are used in training validating and testing the models the data from the remaining five meters which are all associated with residential accounts are used in testing the models for performance at new meters outliers were identified using two criteria during one week of the observed period most of the data loggers stopped transmitting information due to a wide scale power outage when meters resumed working the data loggers reported cumulative values of water consumption generating misleading peaks these outliers were removed from the data set the second criterion applied a threshold consumption of 500 gph based on the experience of the utility the threshold value was exceeded by a minor number of data points less than 5 and no meters were discarded due to this criterion 3 1 2 weather data weather data were retrieved from the nearest meteorological station located at raleigh durham international airport rdu which is approximately seven miles 12 5 km away from the location of the households weather data that are included in this research are the last 24 h of air temperature f 32 f 0 c dew point f relative humidity the maximum hourly temperature of the last 24 h f hourly precipitation of the last 24 h in 1 in 25 4 mm and the occurrence of precipitation during the last 24 h as a binary variable hourly precipitation during 2017 in cary reports mostly zero values with some hours reporting up to one inch of rain the time sampling resolution of the weather variables is hourly fig 1b 3 1 3 household characteristics a set of data was collected describing household characteristics including the building area ft2 1 m2 10 76 ft2 the lot area acres 1 acre 4047 m2 the building age years and the property value u s data were retrieved from a gis database made available through the town of cary town of cary 2013 variation in household characteristics represented by the data set is shown in fig 1c 3 2 time series clustering similar to the work presented by candelieri 2017 this study uses a clustering algorithm to group the average hourly water demand of the households into clusters that are based on the time of day the k means clustering algorithm lloyd 1982 was applied to the average consumption at each hour of the day where the average is calculated across all meters the number of clusters was explored for its effects on the model performance and silhouette analysis was used to quantitatively assess the most efficient number of clusters arbelaitz et al 2013 the k means algorithm david and vassilvitskii 2007 was used to initialize cluster centers which improves the running time of the k means algorithm and a random seed was used to initialize random clusters for a set of simulations clusters were generated using the sum of absolute differences as the distance metric to account for variability due to the random generation of cluster centers the clustering subroutine was run multiple times and the most commonly repeated clusters were selected 3 3 machine learning methods three ml methods were applied to forecast short term water demand rfs anns and svr were implemented using methods available through matlab 2019a mathworks com due to the range of the magnitude of the variables the input data were normalized for the ann models min max scaling was applied to normalize the data and convert the data into a range of 1 and 1 values for the svr models data were standardized by centering and scaling each column of the predictor set using the mean and the standard deviation of each predictor respectively for the rf model input data were used directly 3 3 1 random forests rfs are represented by ensembles of decision trees which are expanding structures of nodes with the application of binary splits breiman et al 1984 each node represents a predictor variable the initial value of each node is the average of the response variable over all the observations of that variable splits are formed by using the inequality condition and the performance of a split is evaluated through the gini index which measures how diverse the data are until a terminal node is reached the tree size is determined based on the number of nodes which is used as input to minimize the variance of each split rfs are created through a bootstrap aggregation bagging process breiman 1996 and data are re sampled randomly with replacement the use of ensemble modeling can improve the overall performance of the model though overfitting and complexity issues may emerge breiman 2001 algorithmic settings for rf models include the number of trees in an ensemble and the leaf size which is the minimum number of observations per terminal node herrera et al 2010 villarin and rodriguez galiano 2019 rfs are modeled by joining several individual decision trees which has proved to provide better results in terms of accuracy an ensemble of decision trees creates a model that can be considered a gray box where understanding which parameters lead to a good performance is more difficult compared to models that are built using a single decision tree the leaf size refers to the number of observations evaluated at each node where a low number of observations will generate deep trees that may overfit data on the other hand a high number of observations may lead to poor model performance these settings affect model accuracy and the computing time 3 3 2 artificial neural networks anns are widely used in water systems applications adamowski 2008 herrera et al 2010 romano and kapelan 2014 anns are modeled after the human brain to simulate the mechanisms of human neurons to collect analyze and transmit information through different layers haykin 2008 this study uses a feed forward neural network with input hidden and output layers the neurons of the input layer correspond to each of the predictor variables the output layer represents the response variable which is the forecasted water demand and the hidden layer nodes apply the activation function bias component and weights to transform the input data mathematically the process is described as 1 y i f j 1 m w i j x j b i where y i is the response variable i is the corresponding data point f represents the activation function which typically corresponds to an s shaped function m is the number of inputs w i j is the weight applied to the j th input signal x j is the j th input signal predictor value and b i represents the bias applied to the data at the i data point a back propagation algorithm is used to iteratively adjust the connection among neurons bias and weights and improve the value of the mean squared error mse that is calculated based on the modeled and the observed values in the training process ann settings that should be identified are the number of hidden layers and the number of neurons within each layer hidden layers separate nonlinear data to improve the predictive capabilities of the model each hidden layer uses eq 1 in the learning process and the weights and bias terms are stored in the neurons of each hidden layer the number of neurons in the hidden layer generally corresponds to the double of the number of predictors cutore et al 2008 more predictors increase the number of neurons and the complexity of an ann model 3 3 3 support vector regression svr applies a transformation or kernel function to map a non linear data set into a linear function in a high dimensional feature space haykin 2008 gaussian linear and polynomial functions can be used within svr to transform data the mathematical formulation of svr is represented as smola and scholkopf 2004 2 f x w ϕ x b where f x is the response value that should fall within the bandwidth defined by an allowable margin ϵ for all modeled data the support vectors define the feature space from ϵ to ϵ ϕ is the mapping function to transform non linear relations into linear functions in eq 2 w ϕ x represents the dot product of the weight vector w and the transformed input data set ϕ x and b is the bias applied to the function approximation a convex optimization problem is solved to identify the smallest value of the error between modeled and observed data a cost or box constraint controls with a positive numeric value the penalty for outputs that lie outside of the allowable margin ϵ and helps to prevent overfitting to apply svr the user should select the type of transformation or kernel function e g linear polynomial or gaussian function the value of the cost or box constraint the bandwidth margin ϵ and the kernel scale scaling the kernel function plays an important role on the performance of svr as explained by williams et al 2005 in this research the kernel scale is included as a setting to evaluate model performance 4 time clustering results the k means clustering algorithm was applied to cluster the average hourly water demand based on the time of day the number of clusters was selected based on two criteria first a quantitative approach with different numbers of clusters 2 3 and 4 was applied and evaluated using the silhouette analysis as described in section 3 2 two clusters reported the highest silhouette value 0 72 similar to results reported by candelieri 2017 who also used two clusters to group water demand data the use of two clusters also agrees with the emergent characteristics of daily water demands that is water consumption data typically follows a diurnal pattern in which two peak values occur one peak in the morning around 7 am and another peak in the early evening around 6 pm adamowski 2008 buchberger et al 2017 the smart meter data set also demonstrates these peaks in the average daily demand in the morning and evening during the weekdays the first cluster starts at 3 am and continues through the 9 am hour 7 h the peak consumption as shown in fig 2a occurs at 7 am with an average consumption of up to 15 gph between 7 am and 8 am on wednesdays for weekdays the evening peak occurs at 8 pm except for thursday and friday where the peak consumption is at 5 pm the second cluster begins at 10 am and continues to 2 am 17 h on weekend days the clusters and the peaks change the morning peak consumption occurs at 10 am cluster 1 begins at 2 am and continues to 10 am 9 h and the duration of cluster 2 is 15 h similarly the night peak consumption during the weekends as shown in fig 2b occurs at 9 pm with a pronounced difference between saturday and sunday 5 modeling results 5 1 experimental design the predictive model relies on several predictors to forecast water demand one hour ahead at the user level a set of experiments was conducted to test the importance of these predictor variables data clusters and size of data sets on model performance three sets of predictor variables are created to forecast water demand grouped as demand and seasonality ds variables weather w variables and property characteristics variables ch table 1 four input data sets are created with data grouped in alternative sets of clusters table 2 the first input all data includes all data points from a data set without the use of any clustering the second data set wd we clusters data into two clusters based on weekdays and weekend days for the third and fourth data sets hour and wd we hour clusters were used that are based on the time of day and identified through the use of the k means clustering algorithm as shown in section 4 finally two different data set sizes are explored for individual data sets individual one model is trained for each smart meter where each individual model has 8760 data points corresponding to the number of hours in 2017 for the group data set group one model is trained using the entire data set of 85 smart meters where the group model has 744 600 data points corresponding to the product of the number of hours in a year and the number of meters this study evaluates the effects of smart metered data as the main predictor to forecast water demand ds predictor the inclusion of additional sets of predictors ds w ds ch and ds w ch different clusters based on time of day and day of the week all data wd we hour wd we hour and different size of the data sets individual group are explored as they affect the capabilities of the forecasting model experiments for each combination of predictor set cluster and data set size are conducted table 3 for example one model is developed to forecast water demand using only previously recorded consumption and seasonality data clustered by hour of the day during all days of the week for a single household this combination corresponds to the settings ds hour individual table 3 based on the combination of predictors clusters and data set size a total of 24 experiments are created and for each experiment the three ml methods are applied and evaluated similar to the work presented by walker et al 2015 to initialize each model the first week of water consumption is stored to use as predictors pwsh in table 1 of the forecasting model the root mean squared error rmse is used as a metric of performance to evaluate the models for individual models rmse is calculated for each of the 85 meters and for each experiment as shown in eq 3 3 r m s e j i 1 n j d p r e d i d o b s i 2 n j where rmse j is the rmse for the j th meter for the training validation or test data set and there are n j data points associated with the j th meter d p r e d i is the i th demand predicted using a forecasting model and d o b s i is the i th demand observed note that different data points d p r e d i may be predicted using different models at one meter if data were clustered each model was trained 30 times for one meter to account for stochasticity of the ml methods and the average rmse associated with each meter is reported as the average across the 30 trials in gph to aggregate the rmse value for comparison among the combination of model settings we report the median across the meters of the average rmse values for group models rmse is evaluated across the entire data set as shown in eq 4 4 r m s e i 1 n d p r e d i d o b s i 2 n where rmse is calculated for the training validation or test data set of n data points across multiple meters again different data points may be predicted using different models if data were clustered for group models rmse is the average performance across 30 trials the spearman s rank order correlation r s is used as a second metric to define the strength of a monotonic relationship between observed and predicted water demand the spearman s rank correlation is used instead of the pearson s correlation r because a linear relationship is not apparent in this data which is a zero inflated date set myers and sirois 2004 the spearman s rank correlation was not aggregated and is reported in the results section for each meter for each of the 24 experiments table 3 data are divided into training validation and test sets for model developing training data are used for the model to learn from the data the validation data set is used to identify the model parameters to best fit the modeled outputs with the observed data while reducing overfitting finally the model is applied to a test data set to evaluate its performance james et al 2013 a random sampling without replacement algorithm was used to divide the data set ensuring that no overlapping occurs based on previously conducted studies mouatadid and adamowski 2017 guo et al 2018 80 of data were used for training 10 for validation and 10 for testing because of the noisiness in the water demand data large training sets are needed to guide the learning procedure that is used to develop the model only a few settings are determined based on the performance for validation data and a small data set is sufficient to make those selections to evaluate the effects of random sampling the data partition was re initialized for each of the 30 trials the approach leaves 10 of data out for each run thus applying a hold out cross validation technique to generalize the model 5 2 machine learning settings analysis preliminary analysis was explored to set algorithmic parameters for each ml approach a set of 10 smart meters was randomly selected from the set of 85 meters for this analysis the predictor variable was set as ds no clustering was applied cluster all data and individual data sets were used to train one individual model separately for each meter a selection of settings were evaluated for each ml method based on previously conducted work herrera et al 2010 antunes et al 2018 each ml method was run 30 times and the average and standard deviation of the rmse values for each meter were calculated the settings that produced the lowest average rmse across the 30 trials were used for the remainder of the study described in this manuscript because the differences in the mean of rmse values reported in tables 4 and 5 are small for each combination of settings the computational complexity e g computational time of executing each method was also considered when selecting settings 5 2 1 random forest settings the rf model was developed using the bootstrap aggregation technique breiman 1996 and algorithmic settings include the number of trees and the minimum number of observations per tree leaf three values were evaluated for both parameters 50 100 and 200 the range for evaluating the number of trees was based on research showing that 50 or fewer trees lead to accurate predictions antunes et al 2018 another study demonstrates that using more than 200 trees increases the execution time beyond practical limits villarin and rodriguez galiano 2019 within this range the best performance was found for 100 trees and a leaf size of 50 table 4 5 2 2 artificial neural network settings an ann was applied to explore settings for the number of hidden layers and the number of neurons per layer based on previous applications of ml methods for water demand herrera et al 2010 mouatadid and adamowski 2017 antunes et al 2018 potential settings for both parameters were identified as 1 20 and 30 and a total of nine combinations were analyzed the best settings are selected based on both model performance and computing time as one hidden layer and 20 neurons table 4 previous predictive models have also used relatively low numbers of neurons as a characteristic of shallow neural networks cutore et al 2008 herrera et al 2010 antunes et al 2018 5 2 3 support vector regression settings the box constraint b c and bandwidth ϵ were evaluated based on procedures shown by mouatadid and adamowski 2017 and fan et al 2005 the effects of the kernel scale k s were also evaluated based on previous work williams et al 2005 the setting for the box constraint is based on the value of the hourly water demand which ranges from 0 to 500 gph the box constraint was tested at settings of 50 500 and 1000 similarly the allowable margin ϵ that defines the feature space was evaluated using values in the same order of magnitude as the reported hourly water demand at 10 50 and 100 the kernel scale was evaluated at values of 10 50 and 100 this analysis includes a total of 27 combinations the settings of b c 50 k s 10 and ϵ 10 generate the lowest rmse value table 5 many of the rmse values reported by svr were higher than those reported by rf and ann a bayesian optimization algorithm gelbart et al 2014 is available in the matlab toolbox and was applied to improve the performance of the svr models the bayesian optimization subroutine is constrained by the size of the data set and could only be applied to train individual models the performance of the svr that was found using the optimization procedure is approximately the same as the best values shown in boldface reported in table 5 5 3 models for individual meters the performance of the models used for the individual data sets is reported in this section each of the 85 smart meters is included in the analysis and predictor variable sets ds and ds w and alternative settings for clustering are evaluated the settings defined in section 5 2 are applied to train the individual models ann with one hidden layer and 20 neurons rf with 100 trees and at least 50 observations per leaf and svr with box constraint equal to 50 kernel scale equal to 10 and ϵ equal to 10 the median of the average rmse for each meter is reported in table 6 all results are reported for test data for each of the ml methods the median of the rmse values decreases when data are clustered the lowest error is reported as 9 5 gph by the rf model using the ds predictor set and clustering by days and hours wd we hour for these settings the average rmse reported for 85 meters ranges from 4 3 gph to 80 1 gph out of the 85 meters 28 meters report an average rmse value less than 10 gph which is the resolution of the data five meters report an average rmse greater that 40 gph for these five meters the demand pattern is erratic with multiple changes between low e g 10 gph and high e g 100 500 gph demands during a 24 h period three of these five meters correspond to other meters that are not associated with single family residential accounts which may explain a lack of pattern in the demand data the individual models however do not perform poorly overall in simulating demands at other meters based on rmse values the distributions of rmse values for single family residential meters and other meters are similar the ann models with the same settings ds predictor set and wd we hour cluster reported similar rmse values with a median of the averages of 9 5 gph the highest error is reported by svr the lowest rmse value reported by the svr models is 8 higher than the rmse found using the rf and ann models and corresponds to the ds predictor set with wd we hour cluster the longest computational time for training a model for one individual meter was approximately 30 s using the all data cluster and the ds w predictor set and the time required was similar for each of the ml methods a pc with an i7 processor and 16 0 gb of ram was used for the experiments the spearman s rank correlation r s is also shown for individual models using the ds predictor set alone fig 3 because this set performed best based on rmse values as shown above table 6 when the entire data set is used without clustering all data the interquartile range of r s for rf spans from 0 36 to 0 44 and meters reporting an r s greater than 0 58 are considered as outliers similar to the results reported for rmse values rf reports the highest median of r s 0 42 for the ds predictor set and the all data cluster based on r s values the individual models perform similarly for single family residential meters and other meters models based on data that are clustered using the hours cluster shows less variability than other clusters shown in the bottom row of fig 3 while clustering improved the rmse value associated with models clustering does not similarly increase the value of r s a similar set of subplots was generated for the ds w predictor variables and the results are similar to those reported in this section 5 4 models for group data set the results of the ml models applied to the group data set are presented in this section these models use the entire data set of water demand reported by the 85 smart meters during 2017 and the same set of clusters were tested for developing models table 7 the lowest error is reported by rf corresponding to the predictor variable set ds w ch and the wd we hour cluster using the all data cluster the rf models outperform ann and svr models across the different set of predictors ann models perform similar to rf models for the days of week cluster reporting an rmse of 17 8 gph with the predictors ds ch and ds w respectively using the hour cluster the lowest error is found using rf and the ds w ch predictor set with an rmse of 16 7 gph 4 above the lowest value unlike the models trained using the individual data set the models trained using the group data set do not show performance that monotonically improves with clustering the longest computational time for training the group model varied among the ml methods ann training took thirty minutes rf required four hours and svr required six hours using the all data cluster and the ds w ch predictor set for each method the running time did not vary with the type of predictor set but it did vary with the type of cluster where clusters reporting large data sets e g all data took longer than small data sets e g wd we hour similar to the results reported for the individual models the results presented in tables 7 and 8 correspond to the test data set for the group models the highest average value of the spearman s rank correlation is found when using the hour cluster table 8 the comparison between ml methods shows that rf report the highest r s values across the predictor sets and the clusters the predictor set that produces the highest r s corresponded to ds similar to the individual models rf and svr report the highest and lowest values of r s respectively when the entire data set is used all data r s ranges from 0 37 to 0 52 in this case clustering decreases the value of the spearman s rank correlation when the size of the data set is smallest wd we hour r s is reported at the lowest value 0 15 including other variables in the predictor set does not improve r s 5 5 feature importance analysis analysis of feature importance was conducted for the rf models the predictor sets were ds w and ds w ch for the individual and group models respectively and the all data cluster was used the importance of each predictor was found using the tree based iterative input selection algorithm galelli and castelletti 2013 fig 4a shows the most repeated results of feature importance among the 85 individual models hour of the day t i m e the average consumption of the previous 24 h a v 24 h r and day of the week d a y are the three most important predictors fig 4b shows the feature importance results of the group model the most important features in the group model are the consumption of the last week at the same hour p w s h followed by the average consumption of the previous 24 h a v 24 h r and the hour of the day h o u r new individual and group models were generated using only the three most important predictors for each model type and the performance of the models in terms of rmse and r s showed negligible improvement 5 6 comparing the performance of individual and group models the analysis conducted above explores the best settings to obtain individual and group models in this section the individual and group models are compared to provide recommendations about developing predictive models for application in the field individual models are trained to match data from a specific meter and it is expected that an individual model could precisely model the behavior at that meter group models on the other hand are developed using a larger data set which may improve the performance over all meters in addition group models should perform better for new meters that have not been used to train the model to compare the two approaches we select the best individual and group models based on the experiments which reported the highest r s values in sections 5 3 and 5 4 respectively the experiments reporting the best performance for the individual and group models correspond to rf the ds predictor set and the all data cluster first the performance of the best individual and best group model for one smart meter are shown as the observed and modeled water demand of hourly consumption fig 5 to provide a more detailed visualization of the time series fig 6 shows the same data during may 2017 the meter was selected randomly from the set of 85 meters both models capture the trends in water demands over the 8760 h but the peak values are not accurately identified the rmse value reported for the individual model calculated over the 8760 h is 30 gph and the rmse for the group model is 38 gph the spearman s rank correlation r s is 0 87 and 0 75 for the individual and group models respectively the best individual and best group models were then compared based on their capability to predict test data for each smart meter the test data set of each meter includes 876 values of water demand values the test data set is defined as 10 of the data the cumulative distribution plot fig 7 shows that for the individual and group models approximately 30 of the meters report rmse values below 12 gph at the upper limit of the distribution of errors 10 of the meters report rmse values above 25 gph a two sample kolmogrov smirnov hypothesis test marsaglia et al 2003 is applied to the rmse of the models and the test does not reject the null hypothesis that the results from the individual and group models are from populations with the same distribution at the 5 significance level as shown in fig 7 therefore the results obtained from the best individual and group models in terms of rmse are not significantly different the r s coefficient is also calculated to compare the values of the individual and group models for the test data fig 8 shows that the group model reports a stronger monotonic relationship between observed and modeled data than the individual model the two sample kolmogrov smirnov hypothesis test confirms that the group model generates higher r s values than the individual model at the 5 significance level five meters from the original set of 90 meters were used to test the ability of the best individual and group models to predict demands for new data sets similar to the 85 original meters each of the five meters reports 8760 hourly water demand values during 2017 the rmse value generated by both models ranges from 4 73 to 28 gph the group model shows slightly lower errors than the individual model fig 9 this is because the best group model was trained using a larger data set than the best individual model and the predictive capability of the best group model for new data sets is higher the two sample kolmogrov smirnov was applied to test the null hypothesis that rmse from the best individual and group models comes from population with the same distribution and the result indicates that the test does not reject the null hypothesis at the 5 significance level for the r s coefficient the median value reported by the individual and group models was 0 23 and 0 45 respectively the two sample test does not reject the null hypothesis that the r s values come from populations with the same distributions these results demonstrate that there is no statistical difference in performance of the best individual and group models based on rmse or r s at the 5 significance level for five new meters 6 discussion this research tests the application of three ml methods for forecasting water demands on an hourly basis at individual accounts forecasting demands at individual accounts one hour ahead can enable a utility to identify abnormal consumption when comparing modeled and observed values with errors of 10 gph forecasting water demand with one hour resolution data is a challenging modeling task and the precision of forecasting models may be improved through further research however this margin of error may be sufficient to identify a running toilet for example which can account for a loss of up to 140 gph us epa water sense to compare the performance of this work with previously conducted research reported by walker et al 2015 we calculate the pearson s correlation coefficient r for each of the 85 meters using the best group model our results show a range of r values from 0 25 to 0 80 which is a value higher than the range obtained by walker et al 2015 they report r values ranging from to 0 30 to 0 65 for predicting hourly water demands at nine account level meters we focus on the use of r s in our analysis above to evaluate the monotonic relationship between observed and modeled data instead of a linear relationship which is represented by the calculation of r due to the volumetric resolution of the smart meters 10 gallons the observed data set includes many zeros and the mape cannot be calculated to compare these models with results reported by candelieri 2017 previous research that explored the use of svr used only the time of day as a predictor candelieri et al 2015 while this research explores alternative representations of previous water consumption weather variables and property characteristics as predictors the methodology proposed here includes weekends in the analysis and explores the effect of demands that change during the weekend previous work in water demand using smart meters removed weekends from the data set to reduce noisiness in the data mckenna et al 2014 mckenna et al 2014 do not report performance of their models for comparison purposes the approach presented here does not rely on information about the type of user account i e residential vs non residential which gives the process general applicability the models perform similarly well for single family residential meters and other meters the data set is limited in representing types of other meters and further research can explore the effects of user types on model performance through additional data clustering the data set to differentiate weekdays and weekends was in general effective in finding models with reduced errors the performance of the best individual model for weekdays and weekends differs by only 1 8 demonstrating that the model can perform similarly well for both weekdays and weekends with slightly better outcomes for weekdays in terms of rmse the models that were developed in this research had limited capabilities to accurately predict demand peaks fig 5 because most peaks do not follow a periodic or predictable pattern the hourly volumetric resolution of the meters 10 gallons produces a zero inflated time series which creates difficulties in developing forecasting models the number of meters allows us to test the capabilities of the models on new data sets using only five smart meters future work can use larger sets of smart meters to develop models training models for big data is computationally expensive and optimization techniques were not applied when training the group models due to the impracticability of the required computational time ann is one of the most used techniques in the urban water demand field as shown in previous works adamowski 2008 romano and kapelan 2014 walker et al 2015 this research found that rf performs similar to or better than ann as suggested by herrera et al 2010 this study did not explore the effects of different types of normalization to train a neural network and only applied the min max scaling future work may explore the outcome of applying different normalization techniques the application of rf presents advantages for a better understanding of the model in terms of feature importance as shown by villarin and rodriguez galiano 2019 computing time is also an important criterion in the selection of models rf ann and svr take around 30 s to train an individual model using a pc with an i7 processor and 16 0 gb of ram when working with a group model ann takes thirty minutes to train whereas rf and svr take four and six hours respectively while rf generates models with higher correlations the time required for training models is much longer than the time required for anns deployment strategies vary among utilities in the temporal resolution at which data are collected which affects not only the cost of maintaining an ami system but also the type of analysis and modeling that can be conducted mckenna et al 2012 hourly data which may be considered as the upper limit of high temporal resolution is too coarse to identify end uses or support water quality modeling but the data can be analyzed to detect anomalies in post meter water consumption britton et al 2013 and to forecast water demands candelieri et al 2015 the models developed here can be applied to forecast anomalies and send alerts to consumers in the data set explored in this research water consumption is reported in increments of 10 gph this level of resolution can affect the performance of the ml methods explored here which report continuous values as model output classifier approaches may provide a better performance for application to this data set the precision that can be achieved using a classifier approach will affect the time required to train the classifier and exploration of classifier methods was outside of the scope of the work presented here the ml methods described here are broadly applicable across smart meter data sets of varying resolution and frequency as technology for power or battery life and communication improves data sets collected at smart meters may continue to increase in resolution and frequency which may increase the variability of demands trade offs between precision and predictive capabilities for increasingly high resolution data sets should be explored in future research 7 conclusions this research develops a set of models to forecast water demand using data reported by smart meters installed at the user level the models are developed to forecast water demand at the subsequent time step the input data used as predictors consist of lagged or previously observed water demand weather variables and characteristics of the households models were trained for individual meters and for the data set as a whole individual and group models were compared using test data which was randomly selected from the time series each individual model was trained specifically for a meter and was able to continue to predict demands at that meter only marginally better than the group model for all the predictor variables and clusters analyzed when comparing the best individual model with the best group model using the test data set of each smart meter the results in terms of rmse are not different at the 5 significance level however for the group model the correlation between modeled and observed data as measured by the spearman s rank correlation r s is higher than the r s of the individual model at the 5 significance level a third comparison was performed to test the best individual model and the best group model for a new set of five meters based on that comparison the group model performed marginally better than the individual model the group model used a much larger data set in training and was able to predict demands at new meters better than an individual model which was trained using a limited data set however when evaluating the results using a two sample kolmogrov smirnov test the rmse values reported by individual and group models are not different at the 5 significance level three ml methods were applied to forecast water demand based on regression rfs anns and svr despite fine tuning the methods with similar combination of settings reported by previous works herrera et al 2010 mouatadid and adamowski 2017 antunes et al 2018 rf and ann models outperformed svr in all the applications for the individual models a bayesian optimization of the hyperparameters was applied to svr and the rmse values remained higher than those reported by rf and ann models this optimization technique was feasible only for the individual models that have around 8700 data points for the group models around 740 000 data points the svr settings were fixed and the error followed the same pattern as the individual models this conclusion agrees with those reported by brentan et al 2018a who applied svr for water demand data the inclusion of exogenous variables such as weather and property characteristics only marginally improves the model performance effectively most of the information from these ancillary variables is already captured in the consumption data for individual models the performance of demand driven models is not affected by the inclusion of weather variables the results are in accordance with previous works that did not find a significant correlation between short term hourly water demand and weather variables group models showed a slight improvement in performance due to the inclusion of weather variables and characteristics of the households building area lot area building age and property value in the array of predictors while an improvement was observed with the inclusion of these exogenous variables obtaining this information may be impractical characteristics of the households are obtained from census data or surveys and this data may not be public to protect the privacy of constituents seasonality was analyzed by clustering data for weekdays and weekends and based on the time of the day the performance of both individual and group models improved by clustering data where no clustering resulted in the highest errors and the highest level of clustering resulted in the lowest errors differentiating weekdays from weekends and clustering for the time of the day resulted in the lowest error for the individual models whereas only clustering for weekdays and weekends reported the lowest error for the group model in summary clustering for seasonality improved individual models more than group models the median rmse value reported by the models varies from 9 5 to 16 gph of water giving some level of confidence for using models to alert customers of high water use anomalies that indicate potential post meter leaks when comparing actual and predicted consumption the meter resolution fundamentally affects the performance of forecasting models this variability is expected when working with noisy data which is a characteristic of individual accounts at hourly temporal resolution a higher resolution may lead to a better performance of forecasting models and trade offs may emerge in precision and variability in the data set which is in accordance with recent works cominola et al 2018a future work can explore the use of classifiers for forecasting data reported in discrete intervals real time forecasting methods have a critical role in smart water management and provide a tool for identifying leaks encouraging conservation and shaving peak demands we anticipate that further research will demonstrate the utility of these models in enhancing the performance of water distribution infrastructure declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported in part by the unc inter institutional planning grant program usa and the ecuadorian government through the secretaría nacional de educación superior ciencia tecnología e innovación senescyt the authors express their thanks to the town of cary for sharing data and collaborating in this research 
