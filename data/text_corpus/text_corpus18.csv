index,text
90,the fletcher ponnambalam fp method is an explicit stochastic optimization method for design and operations management of real world storage systems including surface water reservoir and groundwater management problems the fp method faces no curse of dimensionality and no need for scenario generation the paper introduces a novel implementation for the fp method named fp 2022 here for clarity by removing the need for nonlinear constraints and by decreasing the number of decision variables to just one third of its original value significantly reducing solving time 27 times faster than the original formulation additionally new expressions derived for the first and second moments of both reservoir release deficit and surplus variables and the already derived expression for the second moments of reservoir storages are incorporated into the fp 2022 formulation enabling the method to reach an improved optimality for a nonlinear objective function the enhanced procedure is applied to solving a reservoir operation optimization problem for a major dam in brazil the result comparisons made with other methods along with a thorough analysis of release operation policies prove the optimality of this highly numerically efficient and convenient to use fp 2022 method finally a multi reservoir application of the model is also tested with corrective simulations for improved estimates of some additional variables of interest a specific constraint handling approach regarding reservoir release lower and upper bounds is also presented satisfactory results are obtained for solving the parambikulam aliyar reservoir system a real world five reservoir operation optimization problem from india keywords multireservoir operations optimization two stage stochastic programming stochastic dynamic programming fletcher ponnambalam method monte carlo simulation data availability i have shared in the manuscript a link to data code 1 introduction most complex natural phenomena modeled by means of the systems concept are affected by the presence of unpredictable variables this is the case of storage systems governed by the mass balance equation in which the input output are stochastic processes storage systems analysis is encountered in many areas today e g warehouse management energy management atm cash machines etc water resource systems planning and management in view of uncertain hydrology and changing climate is another mature field dealing with these problems for the past several decades which this paper is about explicit esp and implicit isp stochastic programming optimization techniques are recognized to be efficient tools for identifying optimal planning and operating strategies for multipurpose multireservoir systems under uncertainty alizadeh et al 2018 archibald and marshall 2018 pan et al 2015 fayaed et al 2013 celeste and billib 2009 labadie 2004 esp incorporates probabilistic inflow models directly into the optimization formulation in the practice of reservoir systems operation optimization under uncertainty stochastic dynamic programming sdp based models are typically the optimization approach of choice sdp finds steady state operating policies by means of a discretization scheme of reservoir inflows and storages loucks and van beek 2017 the need for discretization in multiple state variables results in the so called curse of dimensionality in this context several modifications and enhancements of the traditional sdp formulation have been introduced ponnambalam and adams 1987 turgeon 1981 adams and ponnambalam 1994 ponnambalam and adams 1996 mousavi et al 2004 saadat and asghari 2017 the isp method on the other hand applies perfect forecast deterministic optimization to operate the reservoir for several equally possible inflow scenarios and then examines the set of optimal results in order to define release policies the main inconvenience of isp especially for use in multireservoir systems is the need for many inflow scenarios and deterministic optimization problems to be solved which may turn to be laborious it also requires proper post processing methods to infer general operation rules from the optimization results labadie 2004 mousavi et al 2007 2014 cai and rosegrant 2004 compared the results of a two stage model and an isp model and demonstrated the possible bias with the isp model for when the number of scenarios are limited fletcher and ponnambalam fp fletcher and ponnambalam 1996 introduced a discretization free explicit stochastic optimization method that incorporates indicator functions into the reservoir mass balance equation in order to deal with storage bounds and to find statistical moments of storage together with probabilities of other variables of interest the fp method requires neither discretization of state variables nor generation of inflow scenarios to deal with uncertainty making it fast to easily address multireservoir problems without facing the curse of dimensionality the most recent version fletcher and ponnambalam 2008 of the fp method using s type linear decision rules rather than the original fletcher and ponnambalam 1996 open loop constant release policy has been applied successfully to single and multireservoir systems mahootchi and ponnambalam 2013 ganji and jowkarshorijeh 2012 mahootchi et al 2010 it has been also adapted to groundwater management problems joodavi et al 2017 and other storage systems such as energy ponnambalam et al 2010 and warehouse systems mahootchi et al 2012 although the fp method is well established there is still room to improve its performance in both its formulation and its computer implementation this paper introduces a novel implementation that formulates the fp method into an entirely unconstrained optimization problem with a drastic reduction in the number of decision variables that is easily implemented in a vectorized form facilitating its coding in numerical matrix computing environments such as matlab or octave additionally newly derived equations for reservoir release deficit and surplus variables as well as information already derived from the fp method second moments of storage will be fully used for the improvement of the model formulation in this regard most applications of the fp method so far have adopted zeroth order taylor series expansion of the expected value of the objective function thus in spite of expressions estimating the first and second moments of reservoir storage have been available only the first moments have been used in the objective function formulation in most applications one exception being mahootchi et al 2010 where a risk part using second moments of storage was included and the second moments have been left just for comparison purposes with the sample second moments calculated by simulating the policies derived by the fp optimization model that has been one reason for a gap between the simulated objective function values and those estimated by the fp optimization analyzing this gap we show that for a nonlinear quadratic objective function it is important to include the second moments of storage in the objective function to accurately estimate the expected value of the objective function moreover in the fp model applications so far there are expressions derived for the probabilities of containment deficit and surplus but not for the moments of deficit and surplus those derived probabilities are also not utilized in the objective function and are left just for comparison purposes with simulation results this study presents fp 2022 new explicit expressions for the moments of deficit and surplus that are incorporated in the objective function resulting in a much more accurate evaluation of the objective function compared to when they are not included the significance of the proposed enhancements and the performance of fp 2022 implementation is assessed by applying it to the operation of a real world system in brazil the very large sobradinho reservoir moments and variances of storage deficit and surplus variables as well as probabilities of containment deficit and surplus terms found by a single run of the vectorized fp 2022 method are compared with those obtained by monte carlo simulations of monthly reservoir operations for many different scenarios including for a scenario of over 1 000 years a comprehensive analysis of the derived optimal policies is also made by comparing the results of the enhanced fp model with those of sdp two stage stochastic programming tsp and isp models for different types of operating policies and both non gaussian correlated and independent gaussian inflows the enhanced very fast running method was also applied to the parambikulam aliyar multireservoir system india where a new way of estimating cross correlations among variables of interest and handling release lower and upper bound constraints was also presented using corrective simulations 2 models and methods this section presents the basic fp 2022 method new modifications to a quadratic objective function that provide a better accuracy the directions for vectorized implementation of the method time complexity and extensions to other nonlinear objective functions and multireservoir systems in order to compare the results of the fp 2022 method we present briefly the two stage stochastic programming method which also allowed us to test the linear decsion rules ldr policies used in different versions of the fp method with other more general policies and inflow scenarios readers are referred to other literature for descriptions of sdp which is also used to compare the results 2 1 the fp method the main function of a water supply reservoir is to accumulate water in periods of high flows in order to regulate streamflows and to meet demands to the greatest extent possible during dry seasons a general equation that describes the mass balance of a water supply reservoir may be written as follows 1 s t s t 1 i t u t s p t δ t where st and s t 1 represent the reservoir storage at the end of time periods t and t 1 respectively it is the net natural inflow into the reservoir during the time period t and ut is the proposed total release from the reservoir in time t spt represents the surplus when the reservoir is full while δ t is defined as the storage deficit when the reservoir storage goes below the minimum active storage with the proposed release ut in the actual operation ut is usually reduced to a level to make the storage stay within the minimum storage bound according to fig 1 we assume the storage st to be bounded by lower s t m i n and upper s t m a x limits let s t s t 1 i t u t denote the projected storage volume i e the storage at the end of time t if the proposed ut is released and the final storage is contained or remains within the bounds i e s t m i n s t s t m a x so s t s t when the projected storage is not contained there will be either but not simultaneously surplus spt or deficit δ t if releasing ut causes the projected storage to violate the upper bound i e s t s t m a x then the excess water must be released from the reservoir and the actual release is rt ut spt in this case the surplus variable spt denotes the volume of surplus so that the final storage becomes s t s t m a x the amount of surplus will be s p t s t s t m a x alternatively when s t s t m i n then ut cannot be fully met and there will be a release deficit δ t a situation that requires an alternative release rt ut so that the final storage becomes at least s t m i n in this case the amount of deficit will be δ t s t m i n s t and the actual release will be rt ut δ t note that both spt and δ t are nonnegative quantities consequently the actual total outflow rt from the system is 2 r t u t 0 0 u t i f c o n t a i n m e n t u t δ t 0 u t δ t r t i f d e f i c i t u t 0 s p t u t s p t i f s p i l l in the fp method the dynamics of a reservoir system taking all the above situations into account is written as follows 3 s t s t 1 i t η t u t 1 s t m i n s t m a x s t s t m i n 1 s t m i n s t s t m a x 1 s t m a x s t where the inflow is now split into i t i t η t the mean inflow i t plus a zero mean random component η t with variance var η t the notation 1 s t denotes the indicator characteristic function with the following properties 4 1 s t min s t max s t 1 for s t min s t s t max cont ainm ent 0 othe rwise 5 1 s t m i n s t 1 f o r s t s t m i n d e f i c i t 0 o t h e r w i s e 6 1 s t m a x s t 1 f o r s t s t m a x s p i l l 0 o t h e r w i s e therefore at a given time only one of three indicator functions can have a value of 1 and others must be zero continuity eq 3 can be simplified if we write the projected reservoir release in the form of an s type linear decision rule ldr revelle et al 1969 hence the proposed release is a function of current storage unlike in fletcher and ponnambalam 1996 1998 7 u t s t 1 k t eq 7 is an important assumption in the proposed fp model therefore we present in section 4 1 the results of a thorough analysis conducted for assessing how this simple ldr performs compared to other methods benefiting from more sophisticated release policies and a policy free isp method with a large number of scenarios which are the same used in simulation for comparison purposes by applying the above ldr the projected storage volume becomes 8 s t i t η t k t and substituting ut from eq 7 into eq 3 in order to eliminate s t 1 yields 9 s t i t η t k t 1 s t m i n s t m a x s t s t m i n 1 s t m i n s t s t m a x 1 s t m a x s t if we square the above equation the terms containing the products of different indicator functions will disappear resulting in the final expression below 10 s t 2 i t η t k t 2 1 s t m i n s t m a x s t s t m i n 2 1 s t m i n s t s t m a x 2 1 s t m a x s t where the indicator functions squared yield only binary outcomes and hence for simplicity not shown as squared taking expectation of eqs 9 and 10 enables the derivation of expressions for the storage first and second statistical moments respectively for the assumption of gaussian statistically independent random inflows such expressions are presented below see fletcher and ponnambalam 2008 for their detailed derivations see mahootchi et al 2010 for removing the gaussian assumption to arbitrary non gaussian inflows modelled by the kumaraswamy distribution 11 e s t i t k t 2 e r f u b e r f l b v a r η t 2 π exp u b 2 exp l b 2 s t m i n 2 1 e r f l b s t m a x 2 1 e r f u b 12 e s t 2 i t k t 2 2 erf ub erf lb 2 i t k t var η t 2 π exp u b 2 exp l b 2 var η t 2 π s t max i t k t exp u b 2 s t min i t k t exp l b 2 var η t 2 erf ub erf lb s t min 2 2 1 erf lb s t max 2 2 1 erf ub where e denotes the expectation operator η t is a zero mean random variable following a gaussian distribution of the form n 0 var η t l b s t m i n i t k t 2 v a r η t u b s t m a x i t k t 2 v a r η t and erf is the error function see appendix b for more details it is important to note that the right hand side terms of eqs 11 and 12 are only a function of kt the decision variable and other known values and hence are easily evaluated and are not considered in the constraints as in the original formulation of the fp method 2 2 implementation of the fp 2022 method since the implementation of the fp 2022 method explicitly accounts for the role of deficit and surplus variables to model a quadratic objective function exactly we first derive the expressions for these terms 2 2 1 new expressions for the moments of deficit and surplus as defined previously the actual total reservoir outflow rt ut spt δ t is the proposed release ut accounting for either deficit or surplus we determine the deficit and surplus terms as follows 13 δ t s t m i n s t 1 s t m i n s t s t m i n i t k t η t 1 s t m i n s t 14 s p t s t s t m a x 1 s t m a x s t i t k t s t m a x η t 1 s t m a x s t squaring the above expressions yields 15 δ t 2 s t m i n i t k t 2 1 s t m i n s t 2 η t s t m i n i t k t 1 s t m i n s t η t 2 1 s t m i n s t 16 s p t 2 i t k t s t m a x 2 1 s t m a x s t 2 η t i t k t s t m a x 1 s t m a x s t η t 2 1 s t m a x s t taking expectation of all above equations enables the derivation of expressions for the first and second statistical moments of deficit and surplus such expressions are presented below for the first time and their detailed derivation is given in appendix a 17 e δ t s t m i n i t k t s t m i n i t k t f η t η t d η t s t m i n i t k t η t f η t η t d η t 18 e δ t 2 s t min i t k t 2 s t min i t k t f η t η t d η t 2 s t min i t k t s t min i t k t η t f η t η t d η t s t min i t k t η t 2 f η t η t d η t 19 e s p t i t k t s t m a x s t m a x i t k t f η t η t d η t s t m a x i t k t η t f η t η t d η t 20 e s p t 2 i t k t s t m a x 2 s t m a x i t k t f η t η t d η t 2 i t k t s t m a x s t m a x i t k t η t f η t η t d η t s t m a x i t k t η t 2 f η t η t d η t where f η t η t is the probability density function of inflow random component η t based on eqs 19 and 20 we have derived and presented analytical expressions for the first and second moments of deficit and surplus in appendix b for gaussian inflows an important aspect to notice is that all expressions become a function of the known inflow moments system storage bounds s t m a x and s t m i n and the ldr parameters k t 1 nv which are the only decision variables to optimize note that the number of decision variables nv 12 for a single reservoir case 2 2 2 optimization problem formulation a monthly stochastic reservoir operation optimization problem may be formulated with the objective of minimizing the expected value of the sum of squared deviations between releases and demands 21 m i n i m i z e z e t 1 12 r t d t 2 e t 1 12 u t s p t δ t d t 2 where dt is the demand target release for month t adding terms for storage targets and minimizing the sum of deviations in 21 is possible and has been dealt with in fletcher and ponnambalam 1998 also see section 2 2 4 later for other general nonlinear objective functions the inclusion of deficit and surplus terms in the objective function means that both water supply and flood control are important for the operation if the only objective is water supply then rt may be used instead of rt and the objective function becomes z e t 1 12 r t d t 2 e t 1 12 u t δ t d t 2 currently release bound constraints are not considered during optimization because the objective function penalizes both surpluses and deficits however we account for release bounds and spillway through flow explicitly later in the multireservoir application the assumed ldr is ut s t 1 kt therefore the objective function becomes 22 z e t 1 t 12 s t 1 s p t δ t k t d t 2 which can be developed to 23 z t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 e δ t 2 2 e s t 1 δ t 2 k t d t e δ t e s p t 2 2 e s t 1 s p t 2 e s p t δ t 2 k t d t e s p t since for any time period t either spt or δ t is zero then e s p t δ t 0 assuming s t 1 to be independent of both δ t and spt the objective function finally becomes 24 z t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 e δ t 2 2 e s t 1 e δ t 2 k t d t e δ t e s p t 2 2 e s t 1 e s p 2 k t d t e s p t the assumption on independence of s t 1 and both δ t and spt was indeed verified using simulation results and it was found that the corresponding spearman correlation coefficients that measure nonlinear dependence better were very low for the case studied in the multireservoir section we use corrective simulations that not only relax this simplistic assumption but also account for all other necessary covariance terms in the model formulation because the optimization procedure and single simulation are very fast requiring only a few corrective optimization simulation runs the method remains efficient in optimizing long term operations of multi reservoir systems as all the first and second moments in the above expression are dependent only on kt so is the objective function z consequently the vector of decision variables of the final optimization problem is k k 1 k 12 i e one value of kt for each month of the year t 1 january through t 12 december the symbol represents the vector transpose operator the value of kt in ut s t 1 kt may be negative storage has enough water to meet proposed release or positive storage needs additional water to meet proposed release thus the decision vector k is free at sign and is bounded for a bounded zero mean inflow random variable η t as st is bounded see eq 8 since the only decision variables are the elements of vector k we face an unconstrained nonlinear optimization formulation only having box constraints regarding lower and upper bounds of the decision variables this formulation can be easily vectorized as detailed in appendix c once the optimal values of the ldr parameters k 1 k 12 are found the monthly values of the first and second moments variances of storage deficit and surplus variables can be calculated by the derived expressions presented earlier furthermore the probabilities of containment p t con deficit p t def and surplus p t sp for the projected storage s t t are simply the expected values of the three indicator functions in eq 5 as presented in the fp method fletcher and ponnambalam 2008 expressions of which are also known see also appendices a and b the assumed ldr can be used as a guide to operate the reservoir for a given initial storage s t 1 and inflow it a release of ut s t 1 kt is proposed first the actual total outflow for that month rt ut spt δ t can then be decided by checking the mass balance to identify whether surplus or deficit should be triggered note that in previous applications of the fp method a zeroth order taylor series expansion of the objective function has been used where neither second moments of storage nor deficit and surplus terms have been used leading to the following approximation 25 z 1 e t 1 12 u t d t 2 t 1 12 e u t d t 2 t 1 12 e s t 1 k t d t 2 t 1 12 e s t 1 k t d t 2 t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 in the above equation only the first moment of storage is needed e s t 1 has already been estimated by eq 11 considering storage bounds using indicator functions however a more exact objective function estimate from eq 24 if surplus and deficit terms are omitted is 26 z 2 t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 function z 2 requires the second moment of storage e s t 1 2 given in eq 12 the difference between eqs 26 and 25 objective functions z 2 and z 1 is simply equal to e s t 1 2 e s t 1 2 v a r s t 1 i e the variance of the initial storage therefore as expected the larger the variance of the monthly storage the greater the error in the zeroth order taylor approximation will be 2 2 3 reduction in computing time it was noted in section 2 2 2 that at any given time t all necessary expressions can be calculated as simply as a function of decision vector k leading to vectorization possibilities shown in appendix c the speed up one gets is a function of the software that we use but matlab allows for such vectorized calculations and executes much faster than nonvectorized equivalents however by removing the constraints the time to solve significantly decreases even in a linear programming case the time complexity is o nv 3 where nv is the number of variables to be optimized as defined in section 2 2 1 here i we remove the o nv constraints that was used to define the first and second moments in the original fp formulation fletcher and ponnambalam 2008 and ii because of i the number of variables becomes 1 3 of the original number of variables as the expressions for the moments are simply calculated and defined only by the decision vector k the time to solve now reduces to o 1 3 n v 3 therefore the speed up of this current formulation compared to the original formulation is at least 27 times 2 2 4 other nonlinear objective functions the derivation for the quadratic objective function in eq 21 produced eq 24 which required only the first and second order moments of any decision and storage variables that all of them are available in explicit analytical forms in the fp method on the other hand for other functions that are nonlinear but not quadratic it is possible to use the first order second moment taylor series methods to approximate such objective functions as they only need the first and second order moments of the required variables such as release and water level say hydraulic head for hydropower operations that is related to storage nonlinearly and are available one can also use the approximations suggested in loucks and van beek loucks and van beek 2017 page 504 which is simpler as it linearizes the equations around the mean values of variables and uses only the zeroth order taylor series terms the advantage of these approaches is that the point at which linearization is done is at the mean values of the storage and release variables that are available and are continuously updated as the optimization proceeds the problems we solve are non convex see analytical expressions derived so there is no global optimality guarantee but the use of monte carlo simulations help us validate the accuracy of the estimates between the fp method and the corresponding simulation results apart from the possibility explained above we can easily use an extended form of the objective function in which deviations from both target releases water demands and target storages starg are included as e t 1 12 r t d t 2 s t 1 s t a r g t 2 this objective function accounts for both release and storage dependent purposes such as navigation recreation and hydropower operations in many practical real world problems note that all we need in the above function are the newly derived first and second moments of release including surplus and deficit and the already derived moments of storage variables in the fp model additionally we have probabilities of surpluses and deficits that can be utilized in the model formulation for risk based operation or other specific planned purposes to the best of our knowledge there is no other explicit optimization model available where such terms and information are available with high accuracies as shown in section 4 i e results and discussion 2 3 two stage programing tsp the open loop constant release policy no direct dependence on the storage state and the s type linear decision rule dependent on the storage state are the policies used in previous fletcher and ponnambalam 1996 and fletcher and ponnambalam 2008 and current fp models respectively these decision rules make the model formulation tractable so as to derive analytical expressions for different variables of interest inflows were also assumed to be normally distributed and statistically independent ignoring serial persistence correlations to assess how these simplistic assumptions impact the performance of the proposed fp model we compare it with other methods including sdp in which a more general state dependent policy is available and the tsp as the implicit stochastic optimization counterpart of the fp and isp the tsp method can also be used to easily account for a variety of operating policies and to consider non gaussian serially correlated inflow time series as tsp can be implemented under any inflow scenarios such comparisons have been made in mahootchi et al 2010 2012 presenting good comparable results for all the three methods however the original objective function was linear where the current extension to quadratic objective function was not needed as well as deficits and surpluses and comparisons with the general sq type policy where release is a function of both storage and inflow and the policy free model were not considered following is the tsp model s formulation to compare with the fp method description above the formulation implemented here for random inflow scenarios uses the fan type as against the tree type scenarios although both fan and tree types give good results séguin et al 2017 fan type scenario generation is more commonly used in water resources as monte carlo simulations 27 min z m i n 1 n i 1 n t 1 t 12 u t d t δ t i s p t i 2 subject to the constraint set for each scenario 28 s t s t 1 u t δ t 1 s p t 1 i t 1 f o r t 1 t s t s t 1 u t δ t 2 s p t 2 i t 2 f o r t 1 t s t s t 1 u t δ t n s p t n i t n f o r t 1 t s 0 s t where n is the number of scenarios years other variables are already defined in the fp method description in this original tsp model that we have named it later model tsp1 storage variables st and the proposed release variables ut are first stage variables and do not change with the scenario number year however since for each scenario or sample one year with 12 months inflow to the reservoir in a season month is different the second stage variables of surplus s p t i and deficit δ t i are added to the balance equations of each scenario i to keep the model feasible we also examine other versions of tsp depending on the variability of storage variables over different scenarios and the release operating policies adopted we can set a specific type of release operating policy in the tsp model by replacing ut with the form or equation of that policy for example if equation ut s t 1 k t is added to the above formulation as an additional constraint we will have a tsp model equipped by the s type release operation policy the same policy employed in the fp model another well known stochastic optimization method we use to compare the fp model with is the sdp which works based on the bellman s principle of optimality and solves a recursive form of the objective function for different discrete values of the state variables vector within the state decision space we do not present here the sdp model formulation of the problem as sdp is a well documented approach see for example loucks and van beek 2017 vedula and mujumdar 2005 note that sdp faces the curse of dimensionality in multireservoir problems due to discretization of the state variables 2 4 extension to multireservoir systems the derivation of the means and variances of storage states of multireservoir systems using the model of fletcher and ponnambalam 2008 has been already presented in mahootchi et al 2010 which solved a five reservoir problem for both gaussian and non gaussian inflows however for the same objective function that used in this study the second moments of surpluses and deficits were not included in water balance equations extending the fp method and the vectorized implementations presented in this study to multireservoir systems or extending the previous multireservoir systems method to consider other objective functions is now possible expressions for the first and second moments of storages for multireservoir operations optimization using s type release policy have been presented in mahootchi et al mahootchi et al 2010 we have used the same storage equations herein additionally expressions for the first and second moments of surpluses and deficits presented for the single reservoir case in sections 2 2 1 and 2 2 2 have been extended to multireservoir operations straightforwardly in all cases the key terms are the expressions for the first and second moments of inflows to a downstream reservoir such as reservoir j receiving releases from upstream reservoir i i e ri plus the interbasin natural inflows intj as follows 29 s t j s t 1 j i t j u t j s p t j δ t j where the total inflow to the reservoir is i t j int t j r t i in which release from the upstream reservoir is r t i u t i sp t i δ t i therefore 30 e i t j e i n t t j e r t i 31 v a r i t j v a r i n t t j v a r r t i 2 c o v i n t t j r t i where 32 e r t i e u t i e s p t i e δ t i 33 v a r r t i e r t i 2 e 2 r t i 34 e r t i 2 e u t i 2 e s p t i 2 e δ t i 2 2 e u t i e s p t i 2 e u t i e δ t i 2 c o v u t i s p t i 2 c o v u t i δ t i 2 e s p t i δ t i note that the last term in above equation is zero additionally the expected values and the variances of releases from upstream reservoir i are known from previous calculations for the associated upstream reservoirs and those of interbasin flows i e e i n t t j and var int t j are estimated from historical records available another novel consideration addressed in this study is about how to estimate all the covariance terms appearing in above equations or in the formulation of any multireseroir applications in this regard we approximate all the unknown covariance terms such as cov int t j r t i using corrective simulations in other words after performing the fp optimization model the covariance terms are calculated by simulating the optimal policies found just a few iterations usually about six or less over successive optimization and simulation runs were found to be enough for the convergence of optimization and simulation objective function values above expressions 32 to 34 are used in 30 and 31 to estimate the moments of the total inflow to a downstream reservoir j receiving both upstream releases and interbasin flows having determined the moments of the total inflows to reservoir j one can use them in deriving the first and second moments of storages surpluses and deficits for reservoir j just like the single reservoir case therefore similar to the single reservoir case the use of the ldr removes the dependence of releases on the storage volumes as they become only a function of linear decision parameters k so the multireservoir expressions are much simpler than in fletcher and ponnambalam 1998 that solved the operations optimization problem of the great lakes system considering five of the lakes using an open loop constant release operating policy 3 case studies and data 3 1 single reservoir case one of the most important hydropower plants in brazil s grid is the one located at the sobradinho dam within the são francisco river basin sfrb in the southeast and northeast regions of the country fig 2 the reservoir holds approximately the same volume of water as the three gorges dam in china which of course has a much higher flow capacity due to being built on the yangtze river the sfrb covers an area of approximately 640 000 km² 7 5 of the brazilian national territory extending over six brazilian states fifty eight percent of the sfrb includes part of the so called polygon of droughts semiarid region characterized by critical periods of prolonged droughts as a result of low rainfall and high evapotranspiration agência nacional de águas 2015 the sobradinho reservoir provides multi year regulation of the são francisco river with a minimum flow of 2 060 m³ month allowing the full utilization of other hydroelectric plants located downstream the 34 billion cubic meter capacity of the sobradinho reservoir floods an area of 4 241 km² with a length of 327 5 km at the 393 5 meter water level design flood forming the largest artificial lake in latin america dantas 2005 hydropower production plays an important role in the sfrb we however assume the sobradinho reservoir is to be used only for water supply to test the fp 2022 model and hence the assumption of constant demand note that this is not a limitation of the method as shown in eq 26 and it only facilitates focusing on the main purpose of this study as explained in section 2 2 4 terms penalizing deviations from target storage levels can easily be added to the objective function of the proposed fp 2022 model to consider other storage related purposes such as recreation and hydropower operations in order to satisfy the real world system requirement of constant releases see above a constant demand equal to 80 of the mean annual flow was specified as dt first the fp method was performed in order to find the best ldr parameters together with the estimation of first and second moments as well as probabilities of containment deficit and surplus later a 1 000 year monthly reservoir operation implementing the derived ldr operating policy was carried out as explained in section 2 2 2 from this monte carlo simulation values of moments and frequencies were calculated to be compared with those already found by the fp method the 1 000 year monthly inflow scenario was synthetically generated from historical records provided by government established national electric system operator ons operador nacional do sistema elétrico for the period 1931 2015 85 years 3 2 multireservoir case the parambikulam aliyar project pap from india is taken as a demonstration example since it has already been studied using ad dp and fp methods adams and ponnambalam 1994 mahootchi and ponnambalam 2013 mahootchi et al 2010 we provide only the minimum details that are relevant to this study detailed introduction of the pap multi reservoir system including the data and characteristics can be obtained other works and references such as those mentioned above the pap system is shown in fig 3 the system comprises of two series of reservoirs in which the west side reservoirs have remarkably higher inflows and demands than the reservoir on the east side the main purpose of this project is to deliver water from the western slopes of anamalai mountains to irrigate the eastern arid area in two states tamilnadu and kerala the main physical and operating constraints agree upon the inter stage agreement including minimum water transfers through inter reservoir canals and canal discharge capacities the objective function here and in previous studies is as follows 35 m a x z m a x i 1 n 5 j 1 n 5 t 1 t 12 b i t r i j t where b i t is the benefit per unit release which is given in table 1 note that the above simple linear objective function has been chosen to be exactly the same as the objective function used in the literature for other methods with which we compare our proposed model however having determined the first and second moments of all the involved variables including new equations derived for the moments of surplus and deficit variables we can handle more complicated nonlinear functions including the first and second moments of controlled release deficit surplus and covariance values among the variables of interest it is worth noting that 20 loss should be considered for the release from the second reservoir parambikulam to the fourth reservoir tirumurthy because of the long tunnel connecting the two reservoirs the lower release bounds are set to zero for all periods table 2 lists the upper bounds for all different connections in the pap as canal discharge capacities i e the release from reservoir i to reservoir j the diagonal elements in this table indicate the total maximum releases for each of the five reservoirs all data used are available to the reader in ponnambalam 2020 see also acknowledgments 4 results and discussion single reservoir case in the original fp model in the objective function neither the second moment of storage nor the first and second moments of surplus and deficit have been incorporated into the model s formulation as done here in eq 26 and a zeroth order taylor series expansion approximation has been used in the objective function for example in fletcher and ponnambalam 1996 for solving the great lakes storage and release target operations optimization in the application to the single reservoir case study the role played separately by each additional element is presented separately in appendices d and e here we discuss the results of full eq 26 or what is called model 4 in appendix e initially we assume that monthly inflows to the sobradinho reservoir follow a gaussian distribution that means that the 1 000 year monthly inflow scenario is generated from normally distributed random numbers with same monthly means and standard deviations as historical records to simulate the reservoir operation this simulation is conducted using the optimal policies obtained by the fp method optimal kt values to assess how the optimal solutions obtained perform under different conditions and assumptions for example inclusion or not inclusion of the first or higher moments of storage deficit and surplus variables in the fp model s formulation we present results corresponding to actual historical inflow data from sobradinho later in section 4 4 4 1 analysis of release operating policies simple release policies of s type ut s t 1 kt and open loop ut kt have been used in the current and previously developed fp models respectively one can argue that such simple operation policies may not be efficient enough therefore the question to be assessed here is whether these simple policies affect the fp model s performance drastically compared to other stochastic optimization models such as sdp employing more sophisticated nonlinear state dependent policies this comparative analysis of various operating policies with fp results is new we compare in this section the proposed fp model with sdp tsp and policy free isp approaches the reason is that we can easily develop different versions of tsp or isp accounting for different operation policies from the simplest constant release policy to a policy free model therefore comparison of tsp and isp models as the implicit stochastic counterpart of the fp model with the fp model when their difference is only in their release operating polices can quantify what impact using those simple linear policies will have on the performance of the fp model to do so the following five alternative models are tested 1 the original tsp tsp1 open tsp1 s type in which a constant release open loop s type policy is considered respectively for a typical year with 12 seasons months the number of release decision variables in tsp1 is 12 with additional 12 1 13 storage variables these are called the first stage variables and do not vary from one scenario to another to these numbers 2 12 n additional surplus and deficit decision variables are added where n is the number of scenarios years 2 tsp2 open s type considers reservoir storage volumes to vary over both seasons and scenarios years it means that in addition to 12 constant release decision variables which are now the only first stage variables 12 n 1 storage volumes are also decision variables to be optimized so now these are second stage variables this allows for storage variances to be non zero like the fp method when the second moments of storages are accounted for in other words the fp model is the explicit stochastic equivalent version of the tsp2 model 3 tsp3 is similar to tsp2 in which a more general complete release rule called general sq type rt s t 1 kkt it kt is employed traditional sq type policy where kkt 1 for all months has already been used in chance constrained programming mousavi et al 2014 therefore here the variables kkt and kt are the first stage variables 4 the last one the implicit stochastic programming isp allows all release variables to vary both over different seasons and scenarios years it is a policy free model in terms of release rules mousavi et al 2014 in which time series of releases are among unknown decision variables in this model deficits or surpluses are part of total releases so no need to define and consider them as separate variables the importance of this model is because it provides the best possible objective function value that can ever be reached as it does not impose any additional constraint release policies on the tsp optimization model and it benefits from having perfect foresight on future inflows therefore any other model utilizing even a very sophisticated nonlinear state dependent release policy cannot perform better than this model and its global optimum objective function value will be the upper bound of the best possible objective function value therefore comparison of the tsp1 tsp2 tsp3 and fp models with such a policy free isp model will show what impact the release operation policies used in each of them can have on the optimality of their solutions the number of variables of each method and sample cpu times for the fp sdp and tsp2 methods are presented in tables 3 and 5 respectively it is clear that when the number of reservoirs increases the number of variables in the fp method increases linearly see also mahootchi et al 2010 for solving a five reservoir problem with fp method while other methods face the curse of dimensionality and cannot be solved one important point for tsp1 tsp2 and tsp3 models is that if we do not make them forced to activate surplus deficit variables second stage variables only if the end of month storage volume reaches the upper lower bound of the reservoir storage volume then they will be exactly the same as isp because of the freedom of surplus and deficit variables to take any arbitrary values in the balance equations additionally in each period simultaneously surplus and deficit terms cannot be nonzero to account for these requirements three additional penalty terms were added to the objective function of the tsp models as follows where z is the same as in eq 27 36 m i n i m i z e z z w 1 i 1 n t 1 12 s p t i s t m a x s t i w 2 i 1 n t 1 12 δ t i s t i s t m i n w 3 i 1 n t 1 12 s p t i δ t i the second and the third terms in the above formula ensure surplus deficit variable i e s p t i δ t i is not triggered until s t i s t m a x s t m i n and the last term guarantees the surplus and deficit terms do not take positive values concurrently our experiments showed that w 1 w 2 w 3 1 worked well table 3 presents the results in terms of objective function values both in simulation and optimization for all the models to be fair and focus only on the role of operations policies we have calculated sdp transition probabilities using a 125 year synthetic gaussian inflow series this is because other models results being reported are also for gaussian inflows later in the next section we present the sdp model results for correlated non gaussian historical inflows note that cpu time reported for the sdp method corresponds to ni 7 inflow classes resulted in the best obj function in optimization simulation ns 30 discrete storages and niter 10 cycles to reach steady state conditions from above results one can see that as expected the best objective function value is that of the policy free isp model 26 63 and the differences among the models solutions both in optimization and the simulation are between 1 and 15 and the worst is tsp1 open ignoring the unfinished tsp2 open after 200 000 iterations additionally the tsp2 stype s and tsp2 open s objective function value is 8 worse than the best possible result that can ever be achieved which is that of isp these results clearly indicate that simple open loop or s type release policies employed in the original or current fp 2022 models perform quite well the difference in the long term simulation with the best isp policy is 2 6 and close to the best possible state dependent more sophisticated release policies of sdp the fp s open loop policy is almost the same as fp s s type policy but it leads to more complicated expressions especially for mutireservoir systems fletcher and ponnambalam 1996 and is not clear that it is worth losing simplicity in practice therefore the concern about using simple optimal release rules in the proposed extended fp model is not really important at least for the problem approached which is a long term optimal reservoir operation planning problem on the other hand fp can solve multireservoir problems very fast while most of the other methods have to use other approximations to solve multireservoir problems the approximations are either in modeling the system e g in the aggregation method of turgeon 1981 and ponnambalam and adams 1987 ponnambalam and adams 1996 as explicit stochastic programming esp methods or in using a reduced number of scenarios in tsp isp which also produces suboptimal solutions 4 2 performance assessment for correlated inflows in this section we show the application of the proposed formulation and implementation of the proposed fp model to the sobradinho reservoir system without assuming that the synthetic inflows used in simulation follow a gaussian distribution as in section 4 1 this is because another concern with the proposed fp model is that of assuming serially independent gaussian inflows of course fp model is not restricted to only gaussian inflows and can easily applied by other distributions such as kumaraswamy distribution mahootchi et al 2010 however it is yet to be extended to cases considering serial and cross correlations therefore in this section we want to assess how significant the role of such simplification would be when compared to models accounting for inflows persistence such as sdp now the 1 000 year monthly inflow scenario for simulation is synthetically generated by the method of fragments svanidze 1980 trying to preserve the actual inflow structure of the historical records the final equations for storage deficit and surplus moments as well as those for their probabilities were still derived assuming normality of inflows for each month of the year january december therefore tests for normality lilliefors 1967 were performed for each month in the inflow records and results indicated that normality was reasonable only for january october november and december inflow data for all other eight months were rejected to follow a gaussian distribution after running the vectorized fp model optimization with input data from the sobradinho reservoir the following information were obtained for every month of the year t 1 12 and are presented below ldr parameters kt first e s t and second moments e s t 2 as well as variance var st of storage probabilities of containment p t c o n deficit p t d e f and surplus p t s p first e δ t and second moments e δ t 2 as well as variance var δ t of deficit first e s p t and second moments e s p t 2 as well as variance var spt of surplus next same statistics m1 and m2 stand for the first and second moments respectively were calculated using the optimal values of kt by performing a simulation model under the generated 1 000 year inflow scenario therefore fp model results were validated if they were close to those obtained by the long period simulation in terms of the objective function value and the storage deficit and surplus moments as well as the probabilities of containment deficit and surplus fig 4 presents the comparison of the fp model optimization and simulation results when the fp optimal policies derived under the gaussian inflow assumption are simulated against a 1 000 year independent non gaussian inflow series the agreement is very good and the difference between optimization and simulation objective function values is just 0 32 the only major issue was an underestimation of the moment of surplus for march optimization provided e s p t 0 0081 against the simulated m 1 spt 0 0401 therefore normality assumption has not been a restriction in the fp model for the case studied moreover the fp model provides accurate estimations of random variables up to second moments and also accurate estimations of probabilities of important storage states however to further investigate the issue and to quantify the impact of both normality and independence of inflow assumptions on the optimal polices derived by the fp model we subsequently compare the results of the fp sdp and tsp2 methods against different inflow scenarios these scenarios include non normal serially correlated historical inflow time series having lag 1 serial correlation coefficients as reported in table 4 among different tsp models tsp2 is used here because it is the implicit stochastic optimization counterpart model of the fp model as both of them consider non zero second moments of storages and employ the s type operation policy different optimization and simulation experiments are conducted including 1 simulating the derived by fp policies against a a 85 year historical inflow time series where inflows are neither gaussian nor independent b a 85 year gaussian independent synthetic inflow time series and c a 1 000 year gaussian independent synthetic inflow time series 2 running the tsp2 model using the 85 year non gaussian correlated historical inflows then simulating the resulting policies against the three inflow scenarios a c and 3 running the tsp2 model using the 85 year gaussian independent synthetic inflow series then simulating the resulting policies against the three mentioned a c inflow scenarios additionally sdp transitional probabilities are determined from the 85 year historical series scenarios a with nclass 7 and the derived policies are simulated against scenarios a c table 5 presents the results obtained tsp2 hist uses the 85 year historical monthly inflows whereas tsp2 gauss works with gaussian independent synthetic inflow time series having the same length and same first and second moments as those of the historical time series therefore in above results 28 43 is about simulating optimal policies obtained from correlated historical inflows 85 years against independent gaussian inflows of the same size 85 years and 27 77 is about simulating the policies obtained from 85 year gaussian inflow series against 85 year correlated historical inflows additionally 27 50 is for simulation the policies obtained from 85 year historical inflows against a 1000 year gaussian independent series whereas 27 12 is about simulating the optimal policies obtained from 85 year gaussian independent flows against a 1 000 year gaussian independent inflow time series we also mentioned that sdp policies have been derived using serially correlated non gaussian historical inflows and they are then simulated against three different scenarios of correlated and non correlated inflows note that the cpu time taken for solving fp sdp and tsp2 hist or gauss models were 1 44 4 01 and 6000 s respectively using matlab in a windows 10 intel r core tm i7 8850u cpu 1 80 ghz processor laptop the results presented in table 5 demonstrate that the assumption of normality and independence for inflows do not have significant impacts on the optimal policies derived by the proposed fp and sdp models as the objective function values resulted from optimization and simulation under the examined scenarios are close and their differences are between 1 and 4 even if we cannot generalize such an outcome to all other case studies we believe the same situation would be the case for long term reservoir operation problems according to previous experiences such as zhang and ponnambalam 2006 a same analysis and examination can be carried out for a multireservoir system with respect to the impact of cross correlations of inflows where the fp model has a significant advantage over other techniques such as sdp in dealing with the curse of dimensionality problem 4 3 results for the pap multireservoir case application of the proposed approach to stochastic optimization of pap multiresevoir operations is presented in this section the application of this extended approach to multireservoir systems still has the linear time complexity thus avoiding any curse of dimensionalities as mentioned earlier results for the pap problem are associated with a linear objective function as follows 37 m a x b e n e f i t m a x i 1 n t 1 t 12 b t i u t i benefit terms b t i are given in table 1 this is the objective function used in previous applications in which controlled release u have been used however here we note that u is neither total actual release r u sp δ nor the actual total release subtracted by flow passing through spillway spil that does not go through canals towards irrigation areas in other words in the pap releases passing through spillways are lost from the system and do not reach the associated downstream reservoirs except for the spilled release made from reservoir 1 tamilnadu shokayar to reservoir 3 kerala sholayar therefore it would be more meaningful to replace u with r spil as the spilled releases do not pass through canals so they do not contribute to irrigation benefits however the moments of spillway through releases are unknowns and no expressions are available for them simplistically even if we assume that spillway flow is the same as surplus sp we should then replace u with sp u δ something that has not been considered previously this approach is closer to real world situation than in other previous works such as hooshyar et al 2020 additionally a 20 loss should be considered for canal 2 4 s flow transferring water from reservoir 2 parambikulam to reservoir 4 tirumurthy table 6 presents optimization and simulation objective function values over six successive iterations where all covariance terms are set to zero at the first iteration of the fp optimization model after which they are estimated at the end of any simulation run which are used then in the next run of optimization the objective function values in simulation are comparable and better than those obtained previously using other optimization approaches applied to the same system for example the best objective function values obtained by aggregation decomposition reinforcement learning ad rl and multilevel ad dynamic programming approaches are equal to 1354 and 1432 respectively hooshyar et al 2020 improved covariance estimates from simulation did not change the objective function values significantly a reasonable match between optimization and simulation objective function values are observed the difference between the two values is less than 10 for all iterations and about 8 for the last iteration the reasons for differences are explained further below figs 5 and 6 compare the results of optimization and simulation for the mean and standard deviations of storages obtained respectively a reasonable match between the mentioned storage moments has also been resulted that shows how appropriately the proposed fp model accounts for dynamics of the pap operations the results for standard deviations in terms of the match between simulation and optimization are much better than that reported in mahootchi et al 2010 the authors found that the main reason for differences in the objective function values and storage moments of optimization and simulation are due to the canal discharge capacities and how they are accounted for in the fp model in other words an almost perfect match were resulted when canal capacities u t max i were considered unbounded for example the optimization and simulated obj function values at iteration 6 for the case of unbounded canal discharge capacities were equal to 2131 and 2104 respectively almost perfect matches for the first and the second moments of all variables were also resulted for this unbounded release case not shown here because of lack of space therefore we have made a significant effort to deal with this important issue for the current application of the proposed fp model to the pap multireservoir problem as explained below the following release bound constraints have been considered in previous applications of the fp model 38 u t m i n i u t i u t m a x i where u t min i is the minimum release required in month t and u t max i is the maximum possible release that can be made from reservoir i in month t which is in fact the discharge capacity of the canal transferring water from reservoir i to other downstream reservoirs in the first applications of the fp model fletcher and ponnambalam 1996 and fletcher and ponnambalam 1998 these constraints were deterministic as u t i were deterministic unknown variables to be optimized according to open loop constant release policy however in other applications such as mahootchi et al 2010 u t i has been a function of s t 1 i so it has been a random variable implying that the release bound constraints are chance constraints mahootchi et al 2010 proposed a method by which only the first moments of these constraints were met so other moments especially the second moments of the constraints left unsatisfied additionally u t i is just an approximation for actual release passing through the canal as it is neither total release r and u t max i is the capacity given in table 2 as the canal discharge capacity therefore the correct precise form of the above release upper bound constraint must be as follows 39 r t i s p i l t i u t m a x i s o u t i s p t i δ t i s p i l t i u t m a x i 40 r t i u t m i n i s o u t i s p t i δ t i u t m i n i where spil t i is the spillway through flow not passing through the associated reservoir interconnecting canal note that spillway flow spil is not equal to surplus sp as surplus is the excess release which is made if the end of month storage exceeds the storage capacity this excess release must first go through the canal and the spillway flow is activated only when the canal capacity is reached i e u t i sp t i u t max i in such a situation 41 s p i l t i u t i s p t i u t m a x i s p t i therefore we need to deal with how to account for inequalities 39 and 40 in our proposed unconstrained optimization model to do so these inequalities can be converted to equality constraints as follows 42 r t i s p i l t i a l p h a t i u t m a x i 43 r t i b e t a t i u t m i n i the slack variable alpha t i is a random variable having an atom mass of probability at zero value probability of the canal discharge to become full the first and second moments of alpha t i are determined below 44 e a l p h a t i u t m a x i e r t i e s p i l t i 45 e a l p h a t i 2 u t m a x i 2 e r t i 2 e s p i l t i 2 2 u t m a x i e r t i 2 u t m a x i e s p i l t i 2 e r t i e s p i l t i 2 c o v r t i s p i l t i 46 var alph a t i e alph a t i t i 2 e 2 alph a t i std alph a t i sqrt var alph a t i although we have already derived expressions for the first and second moments of surplus sp there are no expressions derived for the first and second moments of the spillway through flow spil and the associated variance covariance terms appeared in above equations in this regard one can approximate moments of spillway variables by those of surplus variables however since spill and surplus variables are not identical see 41 such approximation would impose a certain level of error on the model this error increases when the constraint on the canal discharge capacity becomes binding more frequently therefore the authors have proposed evaluation of all these unknown terms by using corrective simulations in other words all the mentioned terms are set to zero in the first run of the fp model and then they are estimated at the end of any simulation run where the obtained fp policies are simulated and all variables including spillway through flows are determined afterwards the first and second moments of slack variable alpha t i can be determined from above equations although this iterative approach has also some degree of approximation the exact detailed forms of the release upper and lower bound constraints are modeled exactly in the simulation model so simulating the derived by fp policies and comparing the simulation results with those obtained by fp optimization would evaluate how well any approximations made may perform we found that just a few iterations of optimization and simulation are enough while estimating the unknown covariance terms and the moments of spillway through flow variables through the proposed successive simulations after which there are no significant differences in the results obtained in next subsequent iterations on the other hand the following conditions for the slack variable alpha t i must be met 47 0 a l p h a t i u t m a x i although the probability distribution of alpha t i is unknown above conditions for this random variable imply that the following constraints are met using the first and second moments of alpha t i determined by 44 and 47 48 e a l p h a t i m 1 s t d a l p h a t i 0 49 e a l p h a t i m 2 s t d a l p h a t i u t m a x i where m1 and m2 are two parameters to be estimated empirically for instance m1 m2 3 if alpha t i would be normally distributed consequently meeting the release canal upper bound constraint 42 is equivalent to meeting the two above constraints 48 and 49 using the first and second moments of slack variable alpha t i to keep the proposed fp model formulation unconstrained we have considered constraints 48 and 49 by adding the associated quadratic penalty terms to the objective function when they are violated the same procedure has been used while handling the constraint on minimum release 43 by deriving expressions for the first and second moments of random variable beta t i 5 remarks and conclusions this paper proposed novel extensions to the fp explicit stochastic optimization method fp 2022 applied to reservoir systems operations the significance of the proposed modifications was investigated through the application of the fp 2022 to the monthly operation of the sobradinho single reservoir and the pap multireservoir case studies the main remarks conclusions and contributions are as follows 1 when the fp approach was introduced by fletcher and ponnambalam 17 the final optimization model had to include also the moments as decision variables these typically led to an optimization problem with 36 decision variables 12 equality constraints 12 inequality constraints and 24 bound constraints for single reservoir applications of the fp method the fp 2022 method needed only the linear decision rule ldr parameters as decision variables and considerably simplified the original highly constrained nonlinear optimization problem to a completely unconstrained vectorized 12 variable much faster at least a 27 times speedup optimization model in this regard each iteration of the proposed model when applied to the studied five reservoir system took only 55 s on a normal pc system so the total cpu time needed to solve the pap multireservoir application was less than 2 4 min considering the fact that the computational load would increase almost linearly with the number of reservoirs the proposed approach is especially valuable to the design and operation of multireservoir systems with an extremely large number of reservoirs 2 new expressions were proposed for the first and second moments of deficit and surplus these expressions together with already derived second moments of storage were then incorporated in the fp 2022 model s nonlinear objective function and provided useful information that considerably improved the model s ability to estimate the true objective function value we also elaborated on how the proposed fp 2022 method can deal with other objective functions accounting for storage dependent purposes such as recreation and hydropower operations the proposed fp 2022 model results revealed good agreements with those obtained by simulating the reservoir operation over a long period using the derived by fp 2022 release policies if necessary the various probabilities can also be calculated using the equations given in appendix b we also conducted detailed analyses to assess the role of simple linear decision rules ldr and the assumption of gaussian independent inflows employed in the fp method the fp 2022 method s results revealed that the derived by fp 2022 policies based on ldr performed quite satisfactorily compared to sdp tsp and isp methods benefiting from more sophisticated operation policies even when the derived policies were simulated against non gaussian correlated inflows more investigation is required regarding the simple ldr assumption for large reservoir systems that carry storage crossing years under different inflow and demand variabilities and correlation conditions but the authors think the reason the simple linear policies work well like in this case study where there are inter annual storage happens is that all future statistics are used when deriving the parameters kt to implement the proposed approach to the pap multireservoir system we proposed a new approach based on corrective simulations to estimate cross correlations covariance terms among random variables of interest additionally specific consideration was given to how to model canal discharge capacity constraints and the moments of spillway through flow variables rather than surplus variables the presented approach deals with the release lower upper bounds more realistically than previous versions of the fp model together with the non requirement for discretization of storage and inflow state variables the above mentioned characteristics and results of the newly proposed fp 2022 model can be of great advantage when compared to other methods such as sdp as a summary the fp 2022 model a accounts for stochasticity of independent gaussian and non gaussian inflows explicitly b has no dimensionality problem and c can handle the nonlinear objective functions that use only up to a second order approximation as both the first and second moments of all variables of interest are now available these advantages are important as there is still no explicit stochastic optimization method capable of addressing perfectly all these challenging aspects i e nonlinearity stochasticity and dimensionality at such a fast speed as this method jamshid mousavi fp 2022 tsp sdp and isp methodology software validation testing writing original draft of fp 2022 and multireservoir section kumaraswamy ponnambalam methodology of corrective simulations validations of unconstrained vectorized fp fp 2022 tsp and software testing and all drafts review and editing alcigeimes celeste methodology of unconstrained and vectorized fp method and software for single reservoir writing of original fp method and single reservoir case study and review of drafts ximing cai design of experiments draft corrections and review declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments and data the author sjm thanks amirkabir university of technology for partial support during his sabbatical research period at the university of waterloo the author kp thanks the natural sciences and engineering research council of canada nserc for their discovery and crd grants he acknowledges also the ontario power generation opg grant on dam safety we also thank ifeanyi e okwuchi for his help in tsp1 model implementation many thanks to reviewers who helped improve the presentation of this paper the data and and the programs associated with this study are available in ponnambalam 2020 a derivation of the expressions for the moments of deficit and surplus a 1 first moment of deficit taking expectation of eq 13 gives a1 e δ t e s t min i t k t η t 1 s t min s t s t min i t k t e 1 s t min s t e η t 1 s t min s t the expected value of the indicator function of a random variable over any region is the probability of that random variable occurring within that same region thus the first expectation in eq a1 represents the probability of deficit p t d e f i e projected storage below the minimum and can be calculated as a2 e l s t min s t p t def pr s t s t min pr i t η t k t s t min pr η t s t min i t k t s t min i t k t f η t η t d η t in which pr denotes probability the second term on the right hand side of eq a1 represents the expectation of a function g η t η t 1 s t m i n s t m a x s t of the random variable η t given the expectation property e g x g x f x d x in which x is a random variable and f x is its probability density function then a3 e η t 1 s t min s t η t 1 s t min s t f η t η t d η t this integral can be separated into two parts corresponding to intervals s t min i t k t and s t m i n i t k t and finally be expressed only for the limits where the indicator function is the unity first interval as a4 e η t 1 s t m i n s t s t m i n i t k t η t f η t η t d η t thus eq a1 for the first moment of deficit finally becomes a5 e δ t s t m i n i t k t s t m i n i t k t f η t η t d η t s t m i n i t k t η t f η t η t d η t a 2 second moment of deficit taking expectation of eq 15 gives a6 e δ t 2 s t m i n i t k t 2 e l s t m i n s t 2 s t m i n i t k t e η t l s t m i n s t e η t 2 l s t m i n s t using the same principle applied in eq a3 for the second and third terms and substituting eq a2 yields the expression for the second moment of deficit a7 e δ t 2 s t min i t k t 2 s t min i t k t f η t η t d η t 2 s t min i t k t s t min i t k t η t f η t η t d η t s t min i t k t η t 2 f η t η t d η t a 3 first moment of surplus taking expectation of eq 14 gives a8 e s p t e i t k t s t max η t l s t max s t i t k t s t max e l s t max s t e η t l s t max s t the first expectation in eq a8 represents the probability of surplus p t sp i e projected storage above maximum and can be calculated as a9 e l s t max s t p t sp pr s t s t max pr i t η t k t s t max pr η t s t max i t k t s t max i t k t f η t η t d η t using the same principle applied in eq a3 for the second expectation in a8 and substituting eq a9 yields the expression for the first moment of surplus a10 e s p t i t k t s t m a x s t m a x i t k t f η t η t d η t s t m a x i t k t η t f η t η t d η t a 4 second moment of surplus taking expectation of eq 16 gives a11 e s p t 2 i t k t s t max 2 e l s t max s t 2 i t k t s t max e η t l s t max s t e η t 2 l s t max s t using the same principle applied in eq a3 for the second and third terms and substituting eq a9 yields the expression for the second moment of surplus a12 e s p t 2 i t k t s t max 2 s t max i t k t f η t η t d η t 2 i t k t s t max s t max i t k t η t f η t η t d η t s t max i t k t η t 2 f η t η t d η t similar to eqs a2 and a9 the probability of containment p t con can be expressed as a13 e l s t min s t max s t p t c o n pr s t min s t s t max pr s t min i t η t k t s t max pr s t min i t k t η t s t max i t k t s t min i t k t s t max i t k t f η t η t d η t b expressions assuming gaussian inflows the probability density function of a zero mean random variable η following a gaussian distribution of the form n 0 var η t is given by b1 f η t η 1 2 π v a r η t exp η 2 2 v a r η t its correspondent cumulative distribution function cdf is b2 f η t η pr η t η η f η t t dt 1 2 1 erf η 2 var η t in which erf is the error function formulated as b3 e r f x 2 π 0 x e t 2 d t with these the solutions for the three types of integrals appearing in the expressions of moments of storage fletcher and ponnambalam 2008 deficit eqs 17 and 18 and surplus eqs 19 and 20 as well as in the expressions for probabilities eqs a2 a9 and a13 are given as below assuming generic lower l and upper u limits of integration b4 u l f η t η d η t f η t u f η t l 1 2 erf u 2 var η t erf l 2 var η t b5 u l η f η t η d η t 1 2 π var η t u l η exp η 2 2 var η t d η t var η t 2 π exp u 2 2 var η t exp l 2 2 var η t b6 u l η 2 f η t η d η t 1 2 π var η t u l η 2 exp η 2 2 var η t d η t var η t 2 π u exp u 2 2 var η t l exp l 2 2 var η t var η t 2 erf u 2 var η t erf l 2 var η t the limits of integration l and u can be changed accordingly in order to derive the final expressions the expressions for the storage moments were already shown in eqs 11 and 12 the final expressions for probabilities and moments of deficit and surplus are displayed below using lb and ub defined in section 2 1 probability of containment b7 p t c o n 1 2 e r f u b e r f l b probability of deficit b8 p t d e f 1 2 1 e r f l b probability of surplus b9 p t s p 1 2 1 e r f u b first moment of deficit b10 e δ t s t m i n i t k t p t d e f v a r η t 2 π exp l b 2 second moment of deficit b11 e δ t 2 s t min i t k t 2 p t d e f 2 s t min i t k t v a r η t 2 π exp l b 2 v a r η t 2 π s t min i t k t exp l b 2 v a r η t 2 1 e r f l b first moment of surplus b12 e s p t i t k t s t m a x p t s p v a r η t 2 π exp u b 2 second moment of surplus b13 e s p t 2 i t k t s t max 2 p t s p 2 i t k t s t max v a r η t 2 π exp u b 2 v a r η t 2 π s t max i t k t exp u b 2 v a r η t 2 1 e r f u b c vectorization let k k 1 k 12 be the vector formed by the twelve unknown ldr parameters similarly we can define vectors for minimum and maximum storages as well as for monthly mean inflow and inflow variances respectively c1 s m i n s 1 m i n s 12 m i n c2 s m a x s 1 m a x s 12 m a x c3 i i 1 i 12 c4 v a r η v a r η 1 v a r η 12 corresponding vectorized versions of lb and ub may be written as c5 l b s m i n i k 2 v a r η c6 u b s m a x i k 2 v a r η which in turn provide a means to write the vectorized expression for the first moment of storage eq 11 c7 e 1 i k 2 erf ub erf lb var η 2 π exp u b 2 exp l b 2 s min 2 1 erf lb s max 2 1 erf ub where e 1 e s 12 e s 1 e s 11 t and all operations are conducted element wise alternative vector expressions can be easily derived for second storage moment e2 and moments of deficit e1δ e2δ and surplus e1sp e2sp defining two other vectors c8 e 1 0 e s 12 e s 1 e s 11 t c9 e 2 0 e s 12 2 e s 1 2 e s 11 2 t the vectorized version of the objective function 24 may be written as c10 z sum e 2 0 2 k d e 1 0 k d 2 e 2 δ 2 e 1 0 e 1 δ 2 k d e 1 δ e 2 sp 2 e 1 0 e 1 sp 2 k d e 1 sp for demand vector d d 1 d 12 and operator sum representing the sum of array elements all these vectorized expressions are straightforwardly implemented in matrix programming environments such as matlab or octave d evaluating the role of the second moment of storage omitting the deficit and surplus terms at this stage by comparing the results of the fp method in which z 1 eq 25 and z 2 eq 26 are used as the objective function and simulating their policies we can evaluate how important the role of incorporating the second moments of storage is for convenience the implementations using z 1 and z 2 were named model 1 and model 2 respectively fig d1 model 1 of loucks and van beek 2017 and fig d2 model 2 show comparison of statistics obtained by optimization and simulation for both models note that in both optimization and simulation modes the values of variables inflow storage release surplus deficit in units of volume were scaled by the volume equivalent to the mean annual flow as mentioned before the simulation of the reservoir operation employed the ldr guided policies derived from optimization optimal kt values for 1000 years from which the simulated first and second sample moments of storage were calculated for every month of the year from the figures acceptable match between simulated and optimization based first and second moments are seen however to be more precise the sums of squared errors between optimized and simulated first sse 1 and second moments sse 2 for both models were used for comparison sse 1 values were 0 00094 0 00065 whereas sse 2 was 0 060 0 049 for model 1 model 2 therefore the match between optimization and simulation based raw moments of storage for model 2 with exact objective function was better than that for model 1 with zeroth order taylor approximation of the objective function it is also important to evaluate the performance of these models in terms of the most important optimality criterion i e the objective function value for model 1 while the objective function value of the optimization model was almost zero z 1 o p t 2 75 10 7 the simulated objective function value was quite different z 1 s i m 0 79 however for model 2 not only the simulated objective function z 2 s i m 0 61 was about 23 better than that of model 1 it also better matched the optimization objective function z 2 o p t 0 70 e evaluating the role of deficit and surplus variables looking carefully at the most important set of equations 3 representing the dynamics of a nonlinear bounded system one can notice that ut is not the total release from the reservoir but part of the release that makes the end of period storage volume contained in all applications of the fp model so far only ut has been used in the objective function meaning that the role of deficit and surplus terms have not been included in the objective function evaluation of any candidate solution however we show here that consideration of deficit and surplus terms is quite important when a nonlinear objective function like the one used in this study is being considered the importance of the issue is because penalizing the objective function due to deficit or surplus occurrences is all what the model s objective is about to account for these terms we derived new expressions for the first and second moments of deficit and surplus and used them in the expected value of the objective function we first analyze the role of incorporating the deficit term typically spillway capacity is very large so in cases where the downstream river s safe discharge is also large enough we may not care about surplus volumes to be penalized in the objective function e 1 role of deficit variables to evaluate how important the incorporation of the deficit term in the objective function is two other fp formulations were compared one that uses only ut in the objective function with consideration of the second moment of storage model 2b and another using the deficit term and new expressions for its first e δ t and second e δ t 2 moments added to the optimization model formulation model 3 however in both cases the release made in the simulation model is the actual total release including ut and δ t therefore the difference between simulated objective functions in model 2b and model 3 will be due to the impact of how the deficit term has been considered in the optimization model s formulation note that model 2b is the same as model 2 introduced in the previous section in optimization mode and their difference is just in simulation mode the deficit term is included in simulated releases in model 2b whereas they are not in model 2 for model 3 the objective function is e1 z 3 t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 e δ t 2 2 e s t 1 e δ t 2 k t d t e δ t the objective function values in optimization simulation for model 2b and model 3 were 0 7 0 61 and 3 69 3 52 respectively we also tested the case when the target demand 80 of the mean annual flow was doubled because the larger the demand the more important the impact of incorporating deficit is expected to be the objective function values in optimization simulation were 1 31 9 63 and 7 15 6 81 for models 2b and 3 respectively we observe that for the newly derived objective function expressions model 3 the objective function values in simulation and optimization matched better however there was a big gap between these values with the old expressions model 2b where the optimization model always underestimated the real objective function value simulated value another interesting point to know is what we would lose if we modeled the second moment of storage accurately but still did not account for deficit model 2 the model 3 s objective function value both simulation and optimization as the correct value was about 3 62 estimated by averaging optimization and simulation values whereas it was underestimated as 0 70 by model 2 therefore 3 62 0 70 2 92 is due to not accounting for the role of deficits in the optimization model formulation on the other hand the difference between the objective functions values of model 3 and model 1 is 3 62 2 75 10 7 3 62 therefore from the two sources of error associated with model 1 considering neither the second moments of storages nor first and second moments of deficits 0 70 3 62 19 is because of not accounting for the second moments of storages and 2 92 3 62 81 is due to not modeling deficits appropriately e 2 role of surplus variables a similar analysis was conducted for evaluating the role of incorporating surpluses by running two other types of models one where the surplus term is not accounted for in the optimization model formulation model 3b versus another in which such term is included using the newly derived expressions for the first e s p t and second e s p t 2 moments of surplus model 4 note that for both cases the surplus term is included in the simulation model while determining reservoir releases and evaluating the objective function value additionally to be fair and to analyze only the effect of surpluses without having the results being affected by the influence of deficit the deficit term is considered in both optimization and simulation for both models 3b and 4 model 3b is the same as model 3 in optimization mode and their difference is only in simulation mode for model 3b surpluses are considered while simulating fp s optimal policies whereas they are not for model 3 for model 4 the objective function is eq 26 including all moments of storage deficit and surplus in both optimization and simulation to have the role of surpluses more sensed experiments were carried out for inflow mean values equal to 2 times of the normal inflows the objective function values in optimization simulation were 8 91 26 04 and 21 30 21 52 for models 3b and 4 respectively we see that model 4 has improved the agreement between optimization and simulation significantly as the difference between optimization and simulation objective function values is around 192 for model 3b whereas it is only 1 for model 4 see fig e1 for a comparison of simulation optimization results for 1000 years of simulated gaussian inflows 
90,the fletcher ponnambalam fp method is an explicit stochastic optimization method for design and operations management of real world storage systems including surface water reservoir and groundwater management problems the fp method faces no curse of dimensionality and no need for scenario generation the paper introduces a novel implementation for the fp method named fp 2022 here for clarity by removing the need for nonlinear constraints and by decreasing the number of decision variables to just one third of its original value significantly reducing solving time 27 times faster than the original formulation additionally new expressions derived for the first and second moments of both reservoir release deficit and surplus variables and the already derived expression for the second moments of reservoir storages are incorporated into the fp 2022 formulation enabling the method to reach an improved optimality for a nonlinear objective function the enhanced procedure is applied to solving a reservoir operation optimization problem for a major dam in brazil the result comparisons made with other methods along with a thorough analysis of release operation policies prove the optimality of this highly numerically efficient and convenient to use fp 2022 method finally a multi reservoir application of the model is also tested with corrective simulations for improved estimates of some additional variables of interest a specific constraint handling approach regarding reservoir release lower and upper bounds is also presented satisfactory results are obtained for solving the parambikulam aliyar reservoir system a real world five reservoir operation optimization problem from india keywords multireservoir operations optimization two stage stochastic programming stochastic dynamic programming fletcher ponnambalam method monte carlo simulation data availability i have shared in the manuscript a link to data code 1 introduction most complex natural phenomena modeled by means of the systems concept are affected by the presence of unpredictable variables this is the case of storage systems governed by the mass balance equation in which the input output are stochastic processes storage systems analysis is encountered in many areas today e g warehouse management energy management atm cash machines etc water resource systems planning and management in view of uncertain hydrology and changing climate is another mature field dealing with these problems for the past several decades which this paper is about explicit esp and implicit isp stochastic programming optimization techniques are recognized to be efficient tools for identifying optimal planning and operating strategies for multipurpose multireservoir systems under uncertainty alizadeh et al 2018 archibald and marshall 2018 pan et al 2015 fayaed et al 2013 celeste and billib 2009 labadie 2004 esp incorporates probabilistic inflow models directly into the optimization formulation in the practice of reservoir systems operation optimization under uncertainty stochastic dynamic programming sdp based models are typically the optimization approach of choice sdp finds steady state operating policies by means of a discretization scheme of reservoir inflows and storages loucks and van beek 2017 the need for discretization in multiple state variables results in the so called curse of dimensionality in this context several modifications and enhancements of the traditional sdp formulation have been introduced ponnambalam and adams 1987 turgeon 1981 adams and ponnambalam 1994 ponnambalam and adams 1996 mousavi et al 2004 saadat and asghari 2017 the isp method on the other hand applies perfect forecast deterministic optimization to operate the reservoir for several equally possible inflow scenarios and then examines the set of optimal results in order to define release policies the main inconvenience of isp especially for use in multireservoir systems is the need for many inflow scenarios and deterministic optimization problems to be solved which may turn to be laborious it also requires proper post processing methods to infer general operation rules from the optimization results labadie 2004 mousavi et al 2007 2014 cai and rosegrant 2004 compared the results of a two stage model and an isp model and demonstrated the possible bias with the isp model for when the number of scenarios are limited fletcher and ponnambalam fp fletcher and ponnambalam 1996 introduced a discretization free explicit stochastic optimization method that incorporates indicator functions into the reservoir mass balance equation in order to deal with storage bounds and to find statistical moments of storage together with probabilities of other variables of interest the fp method requires neither discretization of state variables nor generation of inflow scenarios to deal with uncertainty making it fast to easily address multireservoir problems without facing the curse of dimensionality the most recent version fletcher and ponnambalam 2008 of the fp method using s type linear decision rules rather than the original fletcher and ponnambalam 1996 open loop constant release policy has been applied successfully to single and multireservoir systems mahootchi and ponnambalam 2013 ganji and jowkarshorijeh 2012 mahootchi et al 2010 it has been also adapted to groundwater management problems joodavi et al 2017 and other storage systems such as energy ponnambalam et al 2010 and warehouse systems mahootchi et al 2012 although the fp method is well established there is still room to improve its performance in both its formulation and its computer implementation this paper introduces a novel implementation that formulates the fp method into an entirely unconstrained optimization problem with a drastic reduction in the number of decision variables that is easily implemented in a vectorized form facilitating its coding in numerical matrix computing environments such as matlab or octave additionally newly derived equations for reservoir release deficit and surplus variables as well as information already derived from the fp method second moments of storage will be fully used for the improvement of the model formulation in this regard most applications of the fp method so far have adopted zeroth order taylor series expansion of the expected value of the objective function thus in spite of expressions estimating the first and second moments of reservoir storage have been available only the first moments have been used in the objective function formulation in most applications one exception being mahootchi et al 2010 where a risk part using second moments of storage was included and the second moments have been left just for comparison purposes with the sample second moments calculated by simulating the policies derived by the fp optimization model that has been one reason for a gap between the simulated objective function values and those estimated by the fp optimization analyzing this gap we show that for a nonlinear quadratic objective function it is important to include the second moments of storage in the objective function to accurately estimate the expected value of the objective function moreover in the fp model applications so far there are expressions derived for the probabilities of containment deficit and surplus but not for the moments of deficit and surplus those derived probabilities are also not utilized in the objective function and are left just for comparison purposes with simulation results this study presents fp 2022 new explicit expressions for the moments of deficit and surplus that are incorporated in the objective function resulting in a much more accurate evaluation of the objective function compared to when they are not included the significance of the proposed enhancements and the performance of fp 2022 implementation is assessed by applying it to the operation of a real world system in brazil the very large sobradinho reservoir moments and variances of storage deficit and surplus variables as well as probabilities of containment deficit and surplus terms found by a single run of the vectorized fp 2022 method are compared with those obtained by monte carlo simulations of monthly reservoir operations for many different scenarios including for a scenario of over 1 000 years a comprehensive analysis of the derived optimal policies is also made by comparing the results of the enhanced fp model with those of sdp two stage stochastic programming tsp and isp models for different types of operating policies and both non gaussian correlated and independent gaussian inflows the enhanced very fast running method was also applied to the parambikulam aliyar multireservoir system india where a new way of estimating cross correlations among variables of interest and handling release lower and upper bound constraints was also presented using corrective simulations 2 models and methods this section presents the basic fp 2022 method new modifications to a quadratic objective function that provide a better accuracy the directions for vectorized implementation of the method time complexity and extensions to other nonlinear objective functions and multireservoir systems in order to compare the results of the fp 2022 method we present briefly the two stage stochastic programming method which also allowed us to test the linear decsion rules ldr policies used in different versions of the fp method with other more general policies and inflow scenarios readers are referred to other literature for descriptions of sdp which is also used to compare the results 2 1 the fp method the main function of a water supply reservoir is to accumulate water in periods of high flows in order to regulate streamflows and to meet demands to the greatest extent possible during dry seasons a general equation that describes the mass balance of a water supply reservoir may be written as follows 1 s t s t 1 i t u t s p t δ t where st and s t 1 represent the reservoir storage at the end of time periods t and t 1 respectively it is the net natural inflow into the reservoir during the time period t and ut is the proposed total release from the reservoir in time t spt represents the surplus when the reservoir is full while δ t is defined as the storage deficit when the reservoir storage goes below the minimum active storage with the proposed release ut in the actual operation ut is usually reduced to a level to make the storage stay within the minimum storage bound according to fig 1 we assume the storage st to be bounded by lower s t m i n and upper s t m a x limits let s t s t 1 i t u t denote the projected storage volume i e the storage at the end of time t if the proposed ut is released and the final storage is contained or remains within the bounds i e s t m i n s t s t m a x so s t s t when the projected storage is not contained there will be either but not simultaneously surplus spt or deficit δ t if releasing ut causes the projected storage to violate the upper bound i e s t s t m a x then the excess water must be released from the reservoir and the actual release is rt ut spt in this case the surplus variable spt denotes the volume of surplus so that the final storage becomes s t s t m a x the amount of surplus will be s p t s t s t m a x alternatively when s t s t m i n then ut cannot be fully met and there will be a release deficit δ t a situation that requires an alternative release rt ut so that the final storage becomes at least s t m i n in this case the amount of deficit will be δ t s t m i n s t and the actual release will be rt ut δ t note that both spt and δ t are nonnegative quantities consequently the actual total outflow rt from the system is 2 r t u t 0 0 u t i f c o n t a i n m e n t u t δ t 0 u t δ t r t i f d e f i c i t u t 0 s p t u t s p t i f s p i l l in the fp method the dynamics of a reservoir system taking all the above situations into account is written as follows 3 s t s t 1 i t η t u t 1 s t m i n s t m a x s t s t m i n 1 s t m i n s t s t m a x 1 s t m a x s t where the inflow is now split into i t i t η t the mean inflow i t plus a zero mean random component η t with variance var η t the notation 1 s t denotes the indicator characteristic function with the following properties 4 1 s t min s t max s t 1 for s t min s t s t max cont ainm ent 0 othe rwise 5 1 s t m i n s t 1 f o r s t s t m i n d e f i c i t 0 o t h e r w i s e 6 1 s t m a x s t 1 f o r s t s t m a x s p i l l 0 o t h e r w i s e therefore at a given time only one of three indicator functions can have a value of 1 and others must be zero continuity eq 3 can be simplified if we write the projected reservoir release in the form of an s type linear decision rule ldr revelle et al 1969 hence the proposed release is a function of current storage unlike in fletcher and ponnambalam 1996 1998 7 u t s t 1 k t eq 7 is an important assumption in the proposed fp model therefore we present in section 4 1 the results of a thorough analysis conducted for assessing how this simple ldr performs compared to other methods benefiting from more sophisticated release policies and a policy free isp method with a large number of scenarios which are the same used in simulation for comparison purposes by applying the above ldr the projected storage volume becomes 8 s t i t η t k t and substituting ut from eq 7 into eq 3 in order to eliminate s t 1 yields 9 s t i t η t k t 1 s t m i n s t m a x s t s t m i n 1 s t m i n s t s t m a x 1 s t m a x s t if we square the above equation the terms containing the products of different indicator functions will disappear resulting in the final expression below 10 s t 2 i t η t k t 2 1 s t m i n s t m a x s t s t m i n 2 1 s t m i n s t s t m a x 2 1 s t m a x s t where the indicator functions squared yield only binary outcomes and hence for simplicity not shown as squared taking expectation of eqs 9 and 10 enables the derivation of expressions for the storage first and second statistical moments respectively for the assumption of gaussian statistically independent random inflows such expressions are presented below see fletcher and ponnambalam 2008 for their detailed derivations see mahootchi et al 2010 for removing the gaussian assumption to arbitrary non gaussian inflows modelled by the kumaraswamy distribution 11 e s t i t k t 2 e r f u b e r f l b v a r η t 2 π exp u b 2 exp l b 2 s t m i n 2 1 e r f l b s t m a x 2 1 e r f u b 12 e s t 2 i t k t 2 2 erf ub erf lb 2 i t k t var η t 2 π exp u b 2 exp l b 2 var η t 2 π s t max i t k t exp u b 2 s t min i t k t exp l b 2 var η t 2 erf ub erf lb s t min 2 2 1 erf lb s t max 2 2 1 erf ub where e denotes the expectation operator η t is a zero mean random variable following a gaussian distribution of the form n 0 var η t l b s t m i n i t k t 2 v a r η t u b s t m a x i t k t 2 v a r η t and erf is the error function see appendix b for more details it is important to note that the right hand side terms of eqs 11 and 12 are only a function of kt the decision variable and other known values and hence are easily evaluated and are not considered in the constraints as in the original formulation of the fp method 2 2 implementation of the fp 2022 method since the implementation of the fp 2022 method explicitly accounts for the role of deficit and surplus variables to model a quadratic objective function exactly we first derive the expressions for these terms 2 2 1 new expressions for the moments of deficit and surplus as defined previously the actual total reservoir outflow rt ut spt δ t is the proposed release ut accounting for either deficit or surplus we determine the deficit and surplus terms as follows 13 δ t s t m i n s t 1 s t m i n s t s t m i n i t k t η t 1 s t m i n s t 14 s p t s t s t m a x 1 s t m a x s t i t k t s t m a x η t 1 s t m a x s t squaring the above expressions yields 15 δ t 2 s t m i n i t k t 2 1 s t m i n s t 2 η t s t m i n i t k t 1 s t m i n s t η t 2 1 s t m i n s t 16 s p t 2 i t k t s t m a x 2 1 s t m a x s t 2 η t i t k t s t m a x 1 s t m a x s t η t 2 1 s t m a x s t taking expectation of all above equations enables the derivation of expressions for the first and second statistical moments of deficit and surplus such expressions are presented below for the first time and their detailed derivation is given in appendix a 17 e δ t s t m i n i t k t s t m i n i t k t f η t η t d η t s t m i n i t k t η t f η t η t d η t 18 e δ t 2 s t min i t k t 2 s t min i t k t f η t η t d η t 2 s t min i t k t s t min i t k t η t f η t η t d η t s t min i t k t η t 2 f η t η t d η t 19 e s p t i t k t s t m a x s t m a x i t k t f η t η t d η t s t m a x i t k t η t f η t η t d η t 20 e s p t 2 i t k t s t m a x 2 s t m a x i t k t f η t η t d η t 2 i t k t s t m a x s t m a x i t k t η t f η t η t d η t s t m a x i t k t η t 2 f η t η t d η t where f η t η t is the probability density function of inflow random component η t based on eqs 19 and 20 we have derived and presented analytical expressions for the first and second moments of deficit and surplus in appendix b for gaussian inflows an important aspect to notice is that all expressions become a function of the known inflow moments system storage bounds s t m a x and s t m i n and the ldr parameters k t 1 nv which are the only decision variables to optimize note that the number of decision variables nv 12 for a single reservoir case 2 2 2 optimization problem formulation a monthly stochastic reservoir operation optimization problem may be formulated with the objective of minimizing the expected value of the sum of squared deviations between releases and demands 21 m i n i m i z e z e t 1 12 r t d t 2 e t 1 12 u t s p t δ t d t 2 where dt is the demand target release for month t adding terms for storage targets and minimizing the sum of deviations in 21 is possible and has been dealt with in fletcher and ponnambalam 1998 also see section 2 2 4 later for other general nonlinear objective functions the inclusion of deficit and surplus terms in the objective function means that both water supply and flood control are important for the operation if the only objective is water supply then rt may be used instead of rt and the objective function becomes z e t 1 12 r t d t 2 e t 1 12 u t δ t d t 2 currently release bound constraints are not considered during optimization because the objective function penalizes both surpluses and deficits however we account for release bounds and spillway through flow explicitly later in the multireservoir application the assumed ldr is ut s t 1 kt therefore the objective function becomes 22 z e t 1 t 12 s t 1 s p t δ t k t d t 2 which can be developed to 23 z t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 e δ t 2 2 e s t 1 δ t 2 k t d t e δ t e s p t 2 2 e s t 1 s p t 2 e s p t δ t 2 k t d t e s p t since for any time period t either spt or δ t is zero then e s p t δ t 0 assuming s t 1 to be independent of both δ t and spt the objective function finally becomes 24 z t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 e δ t 2 2 e s t 1 e δ t 2 k t d t e δ t e s p t 2 2 e s t 1 e s p 2 k t d t e s p t the assumption on independence of s t 1 and both δ t and spt was indeed verified using simulation results and it was found that the corresponding spearman correlation coefficients that measure nonlinear dependence better were very low for the case studied in the multireservoir section we use corrective simulations that not only relax this simplistic assumption but also account for all other necessary covariance terms in the model formulation because the optimization procedure and single simulation are very fast requiring only a few corrective optimization simulation runs the method remains efficient in optimizing long term operations of multi reservoir systems as all the first and second moments in the above expression are dependent only on kt so is the objective function z consequently the vector of decision variables of the final optimization problem is k k 1 k 12 i e one value of kt for each month of the year t 1 january through t 12 december the symbol represents the vector transpose operator the value of kt in ut s t 1 kt may be negative storage has enough water to meet proposed release or positive storage needs additional water to meet proposed release thus the decision vector k is free at sign and is bounded for a bounded zero mean inflow random variable η t as st is bounded see eq 8 since the only decision variables are the elements of vector k we face an unconstrained nonlinear optimization formulation only having box constraints regarding lower and upper bounds of the decision variables this formulation can be easily vectorized as detailed in appendix c once the optimal values of the ldr parameters k 1 k 12 are found the monthly values of the first and second moments variances of storage deficit and surplus variables can be calculated by the derived expressions presented earlier furthermore the probabilities of containment p t con deficit p t def and surplus p t sp for the projected storage s t t are simply the expected values of the three indicator functions in eq 5 as presented in the fp method fletcher and ponnambalam 2008 expressions of which are also known see also appendices a and b the assumed ldr can be used as a guide to operate the reservoir for a given initial storage s t 1 and inflow it a release of ut s t 1 kt is proposed first the actual total outflow for that month rt ut spt δ t can then be decided by checking the mass balance to identify whether surplus or deficit should be triggered note that in previous applications of the fp method a zeroth order taylor series expansion of the objective function has been used where neither second moments of storage nor deficit and surplus terms have been used leading to the following approximation 25 z 1 e t 1 12 u t d t 2 t 1 12 e u t d t 2 t 1 12 e s t 1 k t d t 2 t 1 12 e s t 1 k t d t 2 t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 in the above equation only the first moment of storage is needed e s t 1 has already been estimated by eq 11 considering storage bounds using indicator functions however a more exact objective function estimate from eq 24 if surplus and deficit terms are omitted is 26 z 2 t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 function z 2 requires the second moment of storage e s t 1 2 given in eq 12 the difference between eqs 26 and 25 objective functions z 2 and z 1 is simply equal to e s t 1 2 e s t 1 2 v a r s t 1 i e the variance of the initial storage therefore as expected the larger the variance of the monthly storage the greater the error in the zeroth order taylor approximation will be 2 2 3 reduction in computing time it was noted in section 2 2 2 that at any given time t all necessary expressions can be calculated as simply as a function of decision vector k leading to vectorization possibilities shown in appendix c the speed up one gets is a function of the software that we use but matlab allows for such vectorized calculations and executes much faster than nonvectorized equivalents however by removing the constraints the time to solve significantly decreases even in a linear programming case the time complexity is o nv 3 where nv is the number of variables to be optimized as defined in section 2 2 1 here i we remove the o nv constraints that was used to define the first and second moments in the original fp formulation fletcher and ponnambalam 2008 and ii because of i the number of variables becomes 1 3 of the original number of variables as the expressions for the moments are simply calculated and defined only by the decision vector k the time to solve now reduces to o 1 3 n v 3 therefore the speed up of this current formulation compared to the original formulation is at least 27 times 2 2 4 other nonlinear objective functions the derivation for the quadratic objective function in eq 21 produced eq 24 which required only the first and second order moments of any decision and storage variables that all of them are available in explicit analytical forms in the fp method on the other hand for other functions that are nonlinear but not quadratic it is possible to use the first order second moment taylor series methods to approximate such objective functions as they only need the first and second order moments of the required variables such as release and water level say hydraulic head for hydropower operations that is related to storage nonlinearly and are available one can also use the approximations suggested in loucks and van beek loucks and van beek 2017 page 504 which is simpler as it linearizes the equations around the mean values of variables and uses only the zeroth order taylor series terms the advantage of these approaches is that the point at which linearization is done is at the mean values of the storage and release variables that are available and are continuously updated as the optimization proceeds the problems we solve are non convex see analytical expressions derived so there is no global optimality guarantee but the use of monte carlo simulations help us validate the accuracy of the estimates between the fp method and the corresponding simulation results apart from the possibility explained above we can easily use an extended form of the objective function in which deviations from both target releases water demands and target storages starg are included as e t 1 12 r t d t 2 s t 1 s t a r g t 2 this objective function accounts for both release and storage dependent purposes such as navigation recreation and hydropower operations in many practical real world problems note that all we need in the above function are the newly derived first and second moments of release including surplus and deficit and the already derived moments of storage variables in the fp model additionally we have probabilities of surpluses and deficits that can be utilized in the model formulation for risk based operation or other specific planned purposes to the best of our knowledge there is no other explicit optimization model available where such terms and information are available with high accuracies as shown in section 4 i e results and discussion 2 3 two stage programing tsp the open loop constant release policy no direct dependence on the storage state and the s type linear decision rule dependent on the storage state are the policies used in previous fletcher and ponnambalam 1996 and fletcher and ponnambalam 2008 and current fp models respectively these decision rules make the model formulation tractable so as to derive analytical expressions for different variables of interest inflows were also assumed to be normally distributed and statistically independent ignoring serial persistence correlations to assess how these simplistic assumptions impact the performance of the proposed fp model we compare it with other methods including sdp in which a more general state dependent policy is available and the tsp as the implicit stochastic optimization counterpart of the fp and isp the tsp method can also be used to easily account for a variety of operating policies and to consider non gaussian serially correlated inflow time series as tsp can be implemented under any inflow scenarios such comparisons have been made in mahootchi et al 2010 2012 presenting good comparable results for all the three methods however the original objective function was linear where the current extension to quadratic objective function was not needed as well as deficits and surpluses and comparisons with the general sq type policy where release is a function of both storage and inflow and the policy free model were not considered following is the tsp model s formulation to compare with the fp method description above the formulation implemented here for random inflow scenarios uses the fan type as against the tree type scenarios although both fan and tree types give good results séguin et al 2017 fan type scenario generation is more commonly used in water resources as monte carlo simulations 27 min z m i n 1 n i 1 n t 1 t 12 u t d t δ t i s p t i 2 subject to the constraint set for each scenario 28 s t s t 1 u t δ t 1 s p t 1 i t 1 f o r t 1 t s t s t 1 u t δ t 2 s p t 2 i t 2 f o r t 1 t s t s t 1 u t δ t n s p t n i t n f o r t 1 t s 0 s t where n is the number of scenarios years other variables are already defined in the fp method description in this original tsp model that we have named it later model tsp1 storage variables st and the proposed release variables ut are first stage variables and do not change with the scenario number year however since for each scenario or sample one year with 12 months inflow to the reservoir in a season month is different the second stage variables of surplus s p t i and deficit δ t i are added to the balance equations of each scenario i to keep the model feasible we also examine other versions of tsp depending on the variability of storage variables over different scenarios and the release operating policies adopted we can set a specific type of release operating policy in the tsp model by replacing ut with the form or equation of that policy for example if equation ut s t 1 k t is added to the above formulation as an additional constraint we will have a tsp model equipped by the s type release operation policy the same policy employed in the fp model another well known stochastic optimization method we use to compare the fp model with is the sdp which works based on the bellman s principle of optimality and solves a recursive form of the objective function for different discrete values of the state variables vector within the state decision space we do not present here the sdp model formulation of the problem as sdp is a well documented approach see for example loucks and van beek 2017 vedula and mujumdar 2005 note that sdp faces the curse of dimensionality in multireservoir problems due to discretization of the state variables 2 4 extension to multireservoir systems the derivation of the means and variances of storage states of multireservoir systems using the model of fletcher and ponnambalam 2008 has been already presented in mahootchi et al 2010 which solved a five reservoir problem for both gaussian and non gaussian inflows however for the same objective function that used in this study the second moments of surpluses and deficits were not included in water balance equations extending the fp method and the vectorized implementations presented in this study to multireservoir systems or extending the previous multireservoir systems method to consider other objective functions is now possible expressions for the first and second moments of storages for multireservoir operations optimization using s type release policy have been presented in mahootchi et al mahootchi et al 2010 we have used the same storage equations herein additionally expressions for the first and second moments of surpluses and deficits presented for the single reservoir case in sections 2 2 1 and 2 2 2 have been extended to multireservoir operations straightforwardly in all cases the key terms are the expressions for the first and second moments of inflows to a downstream reservoir such as reservoir j receiving releases from upstream reservoir i i e ri plus the interbasin natural inflows intj as follows 29 s t j s t 1 j i t j u t j s p t j δ t j where the total inflow to the reservoir is i t j int t j r t i in which release from the upstream reservoir is r t i u t i sp t i δ t i therefore 30 e i t j e i n t t j e r t i 31 v a r i t j v a r i n t t j v a r r t i 2 c o v i n t t j r t i where 32 e r t i e u t i e s p t i e δ t i 33 v a r r t i e r t i 2 e 2 r t i 34 e r t i 2 e u t i 2 e s p t i 2 e δ t i 2 2 e u t i e s p t i 2 e u t i e δ t i 2 c o v u t i s p t i 2 c o v u t i δ t i 2 e s p t i δ t i note that the last term in above equation is zero additionally the expected values and the variances of releases from upstream reservoir i are known from previous calculations for the associated upstream reservoirs and those of interbasin flows i e e i n t t j and var int t j are estimated from historical records available another novel consideration addressed in this study is about how to estimate all the covariance terms appearing in above equations or in the formulation of any multireseroir applications in this regard we approximate all the unknown covariance terms such as cov int t j r t i using corrective simulations in other words after performing the fp optimization model the covariance terms are calculated by simulating the optimal policies found just a few iterations usually about six or less over successive optimization and simulation runs were found to be enough for the convergence of optimization and simulation objective function values above expressions 32 to 34 are used in 30 and 31 to estimate the moments of the total inflow to a downstream reservoir j receiving both upstream releases and interbasin flows having determined the moments of the total inflows to reservoir j one can use them in deriving the first and second moments of storages surpluses and deficits for reservoir j just like the single reservoir case therefore similar to the single reservoir case the use of the ldr removes the dependence of releases on the storage volumes as they become only a function of linear decision parameters k so the multireservoir expressions are much simpler than in fletcher and ponnambalam 1998 that solved the operations optimization problem of the great lakes system considering five of the lakes using an open loop constant release operating policy 3 case studies and data 3 1 single reservoir case one of the most important hydropower plants in brazil s grid is the one located at the sobradinho dam within the são francisco river basin sfrb in the southeast and northeast regions of the country fig 2 the reservoir holds approximately the same volume of water as the three gorges dam in china which of course has a much higher flow capacity due to being built on the yangtze river the sfrb covers an area of approximately 640 000 km² 7 5 of the brazilian national territory extending over six brazilian states fifty eight percent of the sfrb includes part of the so called polygon of droughts semiarid region characterized by critical periods of prolonged droughts as a result of low rainfall and high evapotranspiration agência nacional de águas 2015 the sobradinho reservoir provides multi year regulation of the são francisco river with a minimum flow of 2 060 m³ month allowing the full utilization of other hydroelectric plants located downstream the 34 billion cubic meter capacity of the sobradinho reservoir floods an area of 4 241 km² with a length of 327 5 km at the 393 5 meter water level design flood forming the largest artificial lake in latin america dantas 2005 hydropower production plays an important role in the sfrb we however assume the sobradinho reservoir is to be used only for water supply to test the fp 2022 model and hence the assumption of constant demand note that this is not a limitation of the method as shown in eq 26 and it only facilitates focusing on the main purpose of this study as explained in section 2 2 4 terms penalizing deviations from target storage levels can easily be added to the objective function of the proposed fp 2022 model to consider other storage related purposes such as recreation and hydropower operations in order to satisfy the real world system requirement of constant releases see above a constant demand equal to 80 of the mean annual flow was specified as dt first the fp method was performed in order to find the best ldr parameters together with the estimation of first and second moments as well as probabilities of containment deficit and surplus later a 1 000 year monthly reservoir operation implementing the derived ldr operating policy was carried out as explained in section 2 2 2 from this monte carlo simulation values of moments and frequencies were calculated to be compared with those already found by the fp method the 1 000 year monthly inflow scenario was synthetically generated from historical records provided by government established national electric system operator ons operador nacional do sistema elétrico for the period 1931 2015 85 years 3 2 multireservoir case the parambikulam aliyar project pap from india is taken as a demonstration example since it has already been studied using ad dp and fp methods adams and ponnambalam 1994 mahootchi and ponnambalam 2013 mahootchi et al 2010 we provide only the minimum details that are relevant to this study detailed introduction of the pap multi reservoir system including the data and characteristics can be obtained other works and references such as those mentioned above the pap system is shown in fig 3 the system comprises of two series of reservoirs in which the west side reservoirs have remarkably higher inflows and demands than the reservoir on the east side the main purpose of this project is to deliver water from the western slopes of anamalai mountains to irrigate the eastern arid area in two states tamilnadu and kerala the main physical and operating constraints agree upon the inter stage agreement including minimum water transfers through inter reservoir canals and canal discharge capacities the objective function here and in previous studies is as follows 35 m a x z m a x i 1 n 5 j 1 n 5 t 1 t 12 b i t r i j t where b i t is the benefit per unit release which is given in table 1 note that the above simple linear objective function has been chosen to be exactly the same as the objective function used in the literature for other methods with which we compare our proposed model however having determined the first and second moments of all the involved variables including new equations derived for the moments of surplus and deficit variables we can handle more complicated nonlinear functions including the first and second moments of controlled release deficit surplus and covariance values among the variables of interest it is worth noting that 20 loss should be considered for the release from the second reservoir parambikulam to the fourth reservoir tirumurthy because of the long tunnel connecting the two reservoirs the lower release bounds are set to zero for all periods table 2 lists the upper bounds for all different connections in the pap as canal discharge capacities i e the release from reservoir i to reservoir j the diagonal elements in this table indicate the total maximum releases for each of the five reservoirs all data used are available to the reader in ponnambalam 2020 see also acknowledgments 4 results and discussion single reservoir case in the original fp model in the objective function neither the second moment of storage nor the first and second moments of surplus and deficit have been incorporated into the model s formulation as done here in eq 26 and a zeroth order taylor series expansion approximation has been used in the objective function for example in fletcher and ponnambalam 1996 for solving the great lakes storage and release target operations optimization in the application to the single reservoir case study the role played separately by each additional element is presented separately in appendices d and e here we discuss the results of full eq 26 or what is called model 4 in appendix e initially we assume that monthly inflows to the sobradinho reservoir follow a gaussian distribution that means that the 1 000 year monthly inflow scenario is generated from normally distributed random numbers with same monthly means and standard deviations as historical records to simulate the reservoir operation this simulation is conducted using the optimal policies obtained by the fp method optimal kt values to assess how the optimal solutions obtained perform under different conditions and assumptions for example inclusion or not inclusion of the first or higher moments of storage deficit and surplus variables in the fp model s formulation we present results corresponding to actual historical inflow data from sobradinho later in section 4 4 4 1 analysis of release operating policies simple release policies of s type ut s t 1 kt and open loop ut kt have been used in the current and previously developed fp models respectively one can argue that such simple operation policies may not be efficient enough therefore the question to be assessed here is whether these simple policies affect the fp model s performance drastically compared to other stochastic optimization models such as sdp employing more sophisticated nonlinear state dependent policies this comparative analysis of various operating policies with fp results is new we compare in this section the proposed fp model with sdp tsp and policy free isp approaches the reason is that we can easily develop different versions of tsp or isp accounting for different operation policies from the simplest constant release policy to a policy free model therefore comparison of tsp and isp models as the implicit stochastic counterpart of the fp model with the fp model when their difference is only in their release operating polices can quantify what impact using those simple linear policies will have on the performance of the fp model to do so the following five alternative models are tested 1 the original tsp tsp1 open tsp1 s type in which a constant release open loop s type policy is considered respectively for a typical year with 12 seasons months the number of release decision variables in tsp1 is 12 with additional 12 1 13 storage variables these are called the first stage variables and do not vary from one scenario to another to these numbers 2 12 n additional surplus and deficit decision variables are added where n is the number of scenarios years 2 tsp2 open s type considers reservoir storage volumes to vary over both seasons and scenarios years it means that in addition to 12 constant release decision variables which are now the only first stage variables 12 n 1 storage volumes are also decision variables to be optimized so now these are second stage variables this allows for storage variances to be non zero like the fp method when the second moments of storages are accounted for in other words the fp model is the explicit stochastic equivalent version of the tsp2 model 3 tsp3 is similar to tsp2 in which a more general complete release rule called general sq type rt s t 1 kkt it kt is employed traditional sq type policy where kkt 1 for all months has already been used in chance constrained programming mousavi et al 2014 therefore here the variables kkt and kt are the first stage variables 4 the last one the implicit stochastic programming isp allows all release variables to vary both over different seasons and scenarios years it is a policy free model in terms of release rules mousavi et al 2014 in which time series of releases are among unknown decision variables in this model deficits or surpluses are part of total releases so no need to define and consider them as separate variables the importance of this model is because it provides the best possible objective function value that can ever be reached as it does not impose any additional constraint release policies on the tsp optimization model and it benefits from having perfect foresight on future inflows therefore any other model utilizing even a very sophisticated nonlinear state dependent release policy cannot perform better than this model and its global optimum objective function value will be the upper bound of the best possible objective function value therefore comparison of the tsp1 tsp2 tsp3 and fp models with such a policy free isp model will show what impact the release operation policies used in each of them can have on the optimality of their solutions the number of variables of each method and sample cpu times for the fp sdp and tsp2 methods are presented in tables 3 and 5 respectively it is clear that when the number of reservoirs increases the number of variables in the fp method increases linearly see also mahootchi et al 2010 for solving a five reservoir problem with fp method while other methods face the curse of dimensionality and cannot be solved one important point for tsp1 tsp2 and tsp3 models is that if we do not make them forced to activate surplus deficit variables second stage variables only if the end of month storage volume reaches the upper lower bound of the reservoir storage volume then they will be exactly the same as isp because of the freedom of surplus and deficit variables to take any arbitrary values in the balance equations additionally in each period simultaneously surplus and deficit terms cannot be nonzero to account for these requirements three additional penalty terms were added to the objective function of the tsp models as follows where z is the same as in eq 27 36 m i n i m i z e z z w 1 i 1 n t 1 12 s p t i s t m a x s t i w 2 i 1 n t 1 12 δ t i s t i s t m i n w 3 i 1 n t 1 12 s p t i δ t i the second and the third terms in the above formula ensure surplus deficit variable i e s p t i δ t i is not triggered until s t i s t m a x s t m i n and the last term guarantees the surplus and deficit terms do not take positive values concurrently our experiments showed that w 1 w 2 w 3 1 worked well table 3 presents the results in terms of objective function values both in simulation and optimization for all the models to be fair and focus only on the role of operations policies we have calculated sdp transition probabilities using a 125 year synthetic gaussian inflow series this is because other models results being reported are also for gaussian inflows later in the next section we present the sdp model results for correlated non gaussian historical inflows note that cpu time reported for the sdp method corresponds to ni 7 inflow classes resulted in the best obj function in optimization simulation ns 30 discrete storages and niter 10 cycles to reach steady state conditions from above results one can see that as expected the best objective function value is that of the policy free isp model 26 63 and the differences among the models solutions both in optimization and the simulation are between 1 and 15 and the worst is tsp1 open ignoring the unfinished tsp2 open after 200 000 iterations additionally the tsp2 stype s and tsp2 open s objective function value is 8 worse than the best possible result that can ever be achieved which is that of isp these results clearly indicate that simple open loop or s type release policies employed in the original or current fp 2022 models perform quite well the difference in the long term simulation with the best isp policy is 2 6 and close to the best possible state dependent more sophisticated release policies of sdp the fp s open loop policy is almost the same as fp s s type policy but it leads to more complicated expressions especially for mutireservoir systems fletcher and ponnambalam 1996 and is not clear that it is worth losing simplicity in practice therefore the concern about using simple optimal release rules in the proposed extended fp model is not really important at least for the problem approached which is a long term optimal reservoir operation planning problem on the other hand fp can solve multireservoir problems very fast while most of the other methods have to use other approximations to solve multireservoir problems the approximations are either in modeling the system e g in the aggregation method of turgeon 1981 and ponnambalam and adams 1987 ponnambalam and adams 1996 as explicit stochastic programming esp methods or in using a reduced number of scenarios in tsp isp which also produces suboptimal solutions 4 2 performance assessment for correlated inflows in this section we show the application of the proposed formulation and implementation of the proposed fp model to the sobradinho reservoir system without assuming that the synthetic inflows used in simulation follow a gaussian distribution as in section 4 1 this is because another concern with the proposed fp model is that of assuming serially independent gaussian inflows of course fp model is not restricted to only gaussian inflows and can easily applied by other distributions such as kumaraswamy distribution mahootchi et al 2010 however it is yet to be extended to cases considering serial and cross correlations therefore in this section we want to assess how significant the role of such simplification would be when compared to models accounting for inflows persistence such as sdp now the 1 000 year monthly inflow scenario for simulation is synthetically generated by the method of fragments svanidze 1980 trying to preserve the actual inflow structure of the historical records the final equations for storage deficit and surplus moments as well as those for their probabilities were still derived assuming normality of inflows for each month of the year january december therefore tests for normality lilliefors 1967 were performed for each month in the inflow records and results indicated that normality was reasonable only for january october november and december inflow data for all other eight months were rejected to follow a gaussian distribution after running the vectorized fp model optimization with input data from the sobradinho reservoir the following information were obtained for every month of the year t 1 12 and are presented below ldr parameters kt first e s t and second moments e s t 2 as well as variance var st of storage probabilities of containment p t c o n deficit p t d e f and surplus p t s p first e δ t and second moments e δ t 2 as well as variance var δ t of deficit first e s p t and second moments e s p t 2 as well as variance var spt of surplus next same statistics m1 and m2 stand for the first and second moments respectively were calculated using the optimal values of kt by performing a simulation model under the generated 1 000 year inflow scenario therefore fp model results were validated if they were close to those obtained by the long period simulation in terms of the objective function value and the storage deficit and surplus moments as well as the probabilities of containment deficit and surplus fig 4 presents the comparison of the fp model optimization and simulation results when the fp optimal policies derived under the gaussian inflow assumption are simulated against a 1 000 year independent non gaussian inflow series the agreement is very good and the difference between optimization and simulation objective function values is just 0 32 the only major issue was an underestimation of the moment of surplus for march optimization provided e s p t 0 0081 against the simulated m 1 spt 0 0401 therefore normality assumption has not been a restriction in the fp model for the case studied moreover the fp model provides accurate estimations of random variables up to second moments and also accurate estimations of probabilities of important storage states however to further investigate the issue and to quantify the impact of both normality and independence of inflow assumptions on the optimal polices derived by the fp model we subsequently compare the results of the fp sdp and tsp2 methods against different inflow scenarios these scenarios include non normal serially correlated historical inflow time series having lag 1 serial correlation coefficients as reported in table 4 among different tsp models tsp2 is used here because it is the implicit stochastic optimization counterpart model of the fp model as both of them consider non zero second moments of storages and employ the s type operation policy different optimization and simulation experiments are conducted including 1 simulating the derived by fp policies against a a 85 year historical inflow time series where inflows are neither gaussian nor independent b a 85 year gaussian independent synthetic inflow time series and c a 1 000 year gaussian independent synthetic inflow time series 2 running the tsp2 model using the 85 year non gaussian correlated historical inflows then simulating the resulting policies against the three inflow scenarios a c and 3 running the tsp2 model using the 85 year gaussian independent synthetic inflow series then simulating the resulting policies against the three mentioned a c inflow scenarios additionally sdp transitional probabilities are determined from the 85 year historical series scenarios a with nclass 7 and the derived policies are simulated against scenarios a c table 5 presents the results obtained tsp2 hist uses the 85 year historical monthly inflows whereas tsp2 gauss works with gaussian independent synthetic inflow time series having the same length and same first and second moments as those of the historical time series therefore in above results 28 43 is about simulating optimal policies obtained from correlated historical inflows 85 years against independent gaussian inflows of the same size 85 years and 27 77 is about simulating the policies obtained from 85 year gaussian inflow series against 85 year correlated historical inflows additionally 27 50 is for simulation the policies obtained from 85 year historical inflows against a 1000 year gaussian independent series whereas 27 12 is about simulating the optimal policies obtained from 85 year gaussian independent flows against a 1 000 year gaussian independent inflow time series we also mentioned that sdp policies have been derived using serially correlated non gaussian historical inflows and they are then simulated against three different scenarios of correlated and non correlated inflows note that the cpu time taken for solving fp sdp and tsp2 hist or gauss models were 1 44 4 01 and 6000 s respectively using matlab in a windows 10 intel r core tm i7 8850u cpu 1 80 ghz processor laptop the results presented in table 5 demonstrate that the assumption of normality and independence for inflows do not have significant impacts on the optimal policies derived by the proposed fp and sdp models as the objective function values resulted from optimization and simulation under the examined scenarios are close and their differences are between 1 and 4 even if we cannot generalize such an outcome to all other case studies we believe the same situation would be the case for long term reservoir operation problems according to previous experiences such as zhang and ponnambalam 2006 a same analysis and examination can be carried out for a multireservoir system with respect to the impact of cross correlations of inflows where the fp model has a significant advantage over other techniques such as sdp in dealing with the curse of dimensionality problem 4 3 results for the pap multireservoir case application of the proposed approach to stochastic optimization of pap multiresevoir operations is presented in this section the application of this extended approach to multireservoir systems still has the linear time complexity thus avoiding any curse of dimensionalities as mentioned earlier results for the pap problem are associated with a linear objective function as follows 37 m a x b e n e f i t m a x i 1 n t 1 t 12 b t i u t i benefit terms b t i are given in table 1 this is the objective function used in previous applications in which controlled release u have been used however here we note that u is neither total actual release r u sp δ nor the actual total release subtracted by flow passing through spillway spil that does not go through canals towards irrigation areas in other words in the pap releases passing through spillways are lost from the system and do not reach the associated downstream reservoirs except for the spilled release made from reservoir 1 tamilnadu shokayar to reservoir 3 kerala sholayar therefore it would be more meaningful to replace u with r spil as the spilled releases do not pass through canals so they do not contribute to irrigation benefits however the moments of spillway through releases are unknowns and no expressions are available for them simplistically even if we assume that spillway flow is the same as surplus sp we should then replace u with sp u δ something that has not been considered previously this approach is closer to real world situation than in other previous works such as hooshyar et al 2020 additionally a 20 loss should be considered for canal 2 4 s flow transferring water from reservoir 2 parambikulam to reservoir 4 tirumurthy table 6 presents optimization and simulation objective function values over six successive iterations where all covariance terms are set to zero at the first iteration of the fp optimization model after which they are estimated at the end of any simulation run which are used then in the next run of optimization the objective function values in simulation are comparable and better than those obtained previously using other optimization approaches applied to the same system for example the best objective function values obtained by aggregation decomposition reinforcement learning ad rl and multilevel ad dynamic programming approaches are equal to 1354 and 1432 respectively hooshyar et al 2020 improved covariance estimates from simulation did not change the objective function values significantly a reasonable match between optimization and simulation objective function values are observed the difference between the two values is less than 10 for all iterations and about 8 for the last iteration the reasons for differences are explained further below figs 5 and 6 compare the results of optimization and simulation for the mean and standard deviations of storages obtained respectively a reasonable match between the mentioned storage moments has also been resulted that shows how appropriately the proposed fp model accounts for dynamics of the pap operations the results for standard deviations in terms of the match between simulation and optimization are much better than that reported in mahootchi et al 2010 the authors found that the main reason for differences in the objective function values and storage moments of optimization and simulation are due to the canal discharge capacities and how they are accounted for in the fp model in other words an almost perfect match were resulted when canal capacities u t max i were considered unbounded for example the optimization and simulated obj function values at iteration 6 for the case of unbounded canal discharge capacities were equal to 2131 and 2104 respectively almost perfect matches for the first and the second moments of all variables were also resulted for this unbounded release case not shown here because of lack of space therefore we have made a significant effort to deal with this important issue for the current application of the proposed fp model to the pap multireservoir problem as explained below the following release bound constraints have been considered in previous applications of the fp model 38 u t m i n i u t i u t m a x i where u t min i is the minimum release required in month t and u t max i is the maximum possible release that can be made from reservoir i in month t which is in fact the discharge capacity of the canal transferring water from reservoir i to other downstream reservoirs in the first applications of the fp model fletcher and ponnambalam 1996 and fletcher and ponnambalam 1998 these constraints were deterministic as u t i were deterministic unknown variables to be optimized according to open loop constant release policy however in other applications such as mahootchi et al 2010 u t i has been a function of s t 1 i so it has been a random variable implying that the release bound constraints are chance constraints mahootchi et al 2010 proposed a method by which only the first moments of these constraints were met so other moments especially the second moments of the constraints left unsatisfied additionally u t i is just an approximation for actual release passing through the canal as it is neither total release r and u t max i is the capacity given in table 2 as the canal discharge capacity therefore the correct precise form of the above release upper bound constraint must be as follows 39 r t i s p i l t i u t m a x i s o u t i s p t i δ t i s p i l t i u t m a x i 40 r t i u t m i n i s o u t i s p t i δ t i u t m i n i where spil t i is the spillway through flow not passing through the associated reservoir interconnecting canal note that spillway flow spil is not equal to surplus sp as surplus is the excess release which is made if the end of month storage exceeds the storage capacity this excess release must first go through the canal and the spillway flow is activated only when the canal capacity is reached i e u t i sp t i u t max i in such a situation 41 s p i l t i u t i s p t i u t m a x i s p t i therefore we need to deal with how to account for inequalities 39 and 40 in our proposed unconstrained optimization model to do so these inequalities can be converted to equality constraints as follows 42 r t i s p i l t i a l p h a t i u t m a x i 43 r t i b e t a t i u t m i n i the slack variable alpha t i is a random variable having an atom mass of probability at zero value probability of the canal discharge to become full the first and second moments of alpha t i are determined below 44 e a l p h a t i u t m a x i e r t i e s p i l t i 45 e a l p h a t i 2 u t m a x i 2 e r t i 2 e s p i l t i 2 2 u t m a x i e r t i 2 u t m a x i e s p i l t i 2 e r t i e s p i l t i 2 c o v r t i s p i l t i 46 var alph a t i e alph a t i t i 2 e 2 alph a t i std alph a t i sqrt var alph a t i although we have already derived expressions for the first and second moments of surplus sp there are no expressions derived for the first and second moments of the spillway through flow spil and the associated variance covariance terms appeared in above equations in this regard one can approximate moments of spillway variables by those of surplus variables however since spill and surplus variables are not identical see 41 such approximation would impose a certain level of error on the model this error increases when the constraint on the canal discharge capacity becomes binding more frequently therefore the authors have proposed evaluation of all these unknown terms by using corrective simulations in other words all the mentioned terms are set to zero in the first run of the fp model and then they are estimated at the end of any simulation run where the obtained fp policies are simulated and all variables including spillway through flows are determined afterwards the first and second moments of slack variable alpha t i can be determined from above equations although this iterative approach has also some degree of approximation the exact detailed forms of the release upper and lower bound constraints are modeled exactly in the simulation model so simulating the derived by fp policies and comparing the simulation results with those obtained by fp optimization would evaluate how well any approximations made may perform we found that just a few iterations of optimization and simulation are enough while estimating the unknown covariance terms and the moments of spillway through flow variables through the proposed successive simulations after which there are no significant differences in the results obtained in next subsequent iterations on the other hand the following conditions for the slack variable alpha t i must be met 47 0 a l p h a t i u t m a x i although the probability distribution of alpha t i is unknown above conditions for this random variable imply that the following constraints are met using the first and second moments of alpha t i determined by 44 and 47 48 e a l p h a t i m 1 s t d a l p h a t i 0 49 e a l p h a t i m 2 s t d a l p h a t i u t m a x i where m1 and m2 are two parameters to be estimated empirically for instance m1 m2 3 if alpha t i would be normally distributed consequently meeting the release canal upper bound constraint 42 is equivalent to meeting the two above constraints 48 and 49 using the first and second moments of slack variable alpha t i to keep the proposed fp model formulation unconstrained we have considered constraints 48 and 49 by adding the associated quadratic penalty terms to the objective function when they are violated the same procedure has been used while handling the constraint on minimum release 43 by deriving expressions for the first and second moments of random variable beta t i 5 remarks and conclusions this paper proposed novel extensions to the fp explicit stochastic optimization method fp 2022 applied to reservoir systems operations the significance of the proposed modifications was investigated through the application of the fp 2022 to the monthly operation of the sobradinho single reservoir and the pap multireservoir case studies the main remarks conclusions and contributions are as follows 1 when the fp approach was introduced by fletcher and ponnambalam 17 the final optimization model had to include also the moments as decision variables these typically led to an optimization problem with 36 decision variables 12 equality constraints 12 inequality constraints and 24 bound constraints for single reservoir applications of the fp method the fp 2022 method needed only the linear decision rule ldr parameters as decision variables and considerably simplified the original highly constrained nonlinear optimization problem to a completely unconstrained vectorized 12 variable much faster at least a 27 times speedup optimization model in this regard each iteration of the proposed model when applied to the studied five reservoir system took only 55 s on a normal pc system so the total cpu time needed to solve the pap multireservoir application was less than 2 4 min considering the fact that the computational load would increase almost linearly with the number of reservoirs the proposed approach is especially valuable to the design and operation of multireservoir systems with an extremely large number of reservoirs 2 new expressions were proposed for the first and second moments of deficit and surplus these expressions together with already derived second moments of storage were then incorporated in the fp 2022 model s nonlinear objective function and provided useful information that considerably improved the model s ability to estimate the true objective function value we also elaborated on how the proposed fp 2022 method can deal with other objective functions accounting for storage dependent purposes such as recreation and hydropower operations the proposed fp 2022 model results revealed good agreements with those obtained by simulating the reservoir operation over a long period using the derived by fp 2022 release policies if necessary the various probabilities can also be calculated using the equations given in appendix b we also conducted detailed analyses to assess the role of simple linear decision rules ldr and the assumption of gaussian independent inflows employed in the fp method the fp 2022 method s results revealed that the derived by fp 2022 policies based on ldr performed quite satisfactorily compared to sdp tsp and isp methods benefiting from more sophisticated operation policies even when the derived policies were simulated against non gaussian correlated inflows more investigation is required regarding the simple ldr assumption for large reservoir systems that carry storage crossing years under different inflow and demand variabilities and correlation conditions but the authors think the reason the simple linear policies work well like in this case study where there are inter annual storage happens is that all future statistics are used when deriving the parameters kt to implement the proposed approach to the pap multireservoir system we proposed a new approach based on corrective simulations to estimate cross correlations covariance terms among random variables of interest additionally specific consideration was given to how to model canal discharge capacity constraints and the moments of spillway through flow variables rather than surplus variables the presented approach deals with the release lower upper bounds more realistically than previous versions of the fp model together with the non requirement for discretization of storage and inflow state variables the above mentioned characteristics and results of the newly proposed fp 2022 model can be of great advantage when compared to other methods such as sdp as a summary the fp 2022 model a accounts for stochasticity of independent gaussian and non gaussian inflows explicitly b has no dimensionality problem and c can handle the nonlinear objective functions that use only up to a second order approximation as both the first and second moments of all variables of interest are now available these advantages are important as there is still no explicit stochastic optimization method capable of addressing perfectly all these challenging aspects i e nonlinearity stochasticity and dimensionality at such a fast speed as this method jamshid mousavi fp 2022 tsp sdp and isp methodology software validation testing writing original draft of fp 2022 and multireservoir section kumaraswamy ponnambalam methodology of corrective simulations validations of unconstrained vectorized fp fp 2022 tsp and software testing and all drafts review and editing alcigeimes celeste methodology of unconstrained and vectorized fp method and software for single reservoir writing of original fp method and single reservoir case study and review of drafts ximing cai design of experiments draft corrections and review declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments and data the author sjm thanks amirkabir university of technology for partial support during his sabbatical research period at the university of waterloo the author kp thanks the natural sciences and engineering research council of canada nserc for their discovery and crd grants he acknowledges also the ontario power generation opg grant on dam safety we also thank ifeanyi e okwuchi for his help in tsp1 model implementation many thanks to reviewers who helped improve the presentation of this paper the data and and the programs associated with this study are available in ponnambalam 2020 a derivation of the expressions for the moments of deficit and surplus a 1 first moment of deficit taking expectation of eq 13 gives a1 e δ t e s t min i t k t η t 1 s t min s t s t min i t k t e 1 s t min s t e η t 1 s t min s t the expected value of the indicator function of a random variable over any region is the probability of that random variable occurring within that same region thus the first expectation in eq a1 represents the probability of deficit p t d e f i e projected storage below the minimum and can be calculated as a2 e l s t min s t p t def pr s t s t min pr i t η t k t s t min pr η t s t min i t k t s t min i t k t f η t η t d η t in which pr denotes probability the second term on the right hand side of eq a1 represents the expectation of a function g η t η t 1 s t m i n s t m a x s t of the random variable η t given the expectation property e g x g x f x d x in which x is a random variable and f x is its probability density function then a3 e η t 1 s t min s t η t 1 s t min s t f η t η t d η t this integral can be separated into two parts corresponding to intervals s t min i t k t and s t m i n i t k t and finally be expressed only for the limits where the indicator function is the unity first interval as a4 e η t 1 s t m i n s t s t m i n i t k t η t f η t η t d η t thus eq a1 for the first moment of deficit finally becomes a5 e δ t s t m i n i t k t s t m i n i t k t f η t η t d η t s t m i n i t k t η t f η t η t d η t a 2 second moment of deficit taking expectation of eq 15 gives a6 e δ t 2 s t m i n i t k t 2 e l s t m i n s t 2 s t m i n i t k t e η t l s t m i n s t e η t 2 l s t m i n s t using the same principle applied in eq a3 for the second and third terms and substituting eq a2 yields the expression for the second moment of deficit a7 e δ t 2 s t min i t k t 2 s t min i t k t f η t η t d η t 2 s t min i t k t s t min i t k t η t f η t η t d η t s t min i t k t η t 2 f η t η t d η t a 3 first moment of surplus taking expectation of eq 14 gives a8 e s p t e i t k t s t max η t l s t max s t i t k t s t max e l s t max s t e η t l s t max s t the first expectation in eq a8 represents the probability of surplus p t sp i e projected storage above maximum and can be calculated as a9 e l s t max s t p t sp pr s t s t max pr i t η t k t s t max pr η t s t max i t k t s t max i t k t f η t η t d η t using the same principle applied in eq a3 for the second expectation in a8 and substituting eq a9 yields the expression for the first moment of surplus a10 e s p t i t k t s t m a x s t m a x i t k t f η t η t d η t s t m a x i t k t η t f η t η t d η t a 4 second moment of surplus taking expectation of eq 16 gives a11 e s p t 2 i t k t s t max 2 e l s t max s t 2 i t k t s t max e η t l s t max s t e η t 2 l s t max s t using the same principle applied in eq a3 for the second and third terms and substituting eq a9 yields the expression for the second moment of surplus a12 e s p t 2 i t k t s t max 2 s t max i t k t f η t η t d η t 2 i t k t s t max s t max i t k t η t f η t η t d η t s t max i t k t η t 2 f η t η t d η t similar to eqs a2 and a9 the probability of containment p t con can be expressed as a13 e l s t min s t max s t p t c o n pr s t min s t s t max pr s t min i t η t k t s t max pr s t min i t k t η t s t max i t k t s t min i t k t s t max i t k t f η t η t d η t b expressions assuming gaussian inflows the probability density function of a zero mean random variable η following a gaussian distribution of the form n 0 var η t is given by b1 f η t η 1 2 π v a r η t exp η 2 2 v a r η t its correspondent cumulative distribution function cdf is b2 f η t η pr η t η η f η t t dt 1 2 1 erf η 2 var η t in which erf is the error function formulated as b3 e r f x 2 π 0 x e t 2 d t with these the solutions for the three types of integrals appearing in the expressions of moments of storage fletcher and ponnambalam 2008 deficit eqs 17 and 18 and surplus eqs 19 and 20 as well as in the expressions for probabilities eqs a2 a9 and a13 are given as below assuming generic lower l and upper u limits of integration b4 u l f η t η d η t f η t u f η t l 1 2 erf u 2 var η t erf l 2 var η t b5 u l η f η t η d η t 1 2 π var η t u l η exp η 2 2 var η t d η t var η t 2 π exp u 2 2 var η t exp l 2 2 var η t b6 u l η 2 f η t η d η t 1 2 π var η t u l η 2 exp η 2 2 var η t d η t var η t 2 π u exp u 2 2 var η t l exp l 2 2 var η t var η t 2 erf u 2 var η t erf l 2 var η t the limits of integration l and u can be changed accordingly in order to derive the final expressions the expressions for the storage moments were already shown in eqs 11 and 12 the final expressions for probabilities and moments of deficit and surplus are displayed below using lb and ub defined in section 2 1 probability of containment b7 p t c o n 1 2 e r f u b e r f l b probability of deficit b8 p t d e f 1 2 1 e r f l b probability of surplus b9 p t s p 1 2 1 e r f u b first moment of deficit b10 e δ t s t m i n i t k t p t d e f v a r η t 2 π exp l b 2 second moment of deficit b11 e δ t 2 s t min i t k t 2 p t d e f 2 s t min i t k t v a r η t 2 π exp l b 2 v a r η t 2 π s t min i t k t exp l b 2 v a r η t 2 1 e r f l b first moment of surplus b12 e s p t i t k t s t m a x p t s p v a r η t 2 π exp u b 2 second moment of surplus b13 e s p t 2 i t k t s t max 2 p t s p 2 i t k t s t max v a r η t 2 π exp u b 2 v a r η t 2 π s t max i t k t exp u b 2 v a r η t 2 1 e r f u b c vectorization let k k 1 k 12 be the vector formed by the twelve unknown ldr parameters similarly we can define vectors for minimum and maximum storages as well as for monthly mean inflow and inflow variances respectively c1 s m i n s 1 m i n s 12 m i n c2 s m a x s 1 m a x s 12 m a x c3 i i 1 i 12 c4 v a r η v a r η 1 v a r η 12 corresponding vectorized versions of lb and ub may be written as c5 l b s m i n i k 2 v a r η c6 u b s m a x i k 2 v a r η which in turn provide a means to write the vectorized expression for the first moment of storage eq 11 c7 e 1 i k 2 erf ub erf lb var η 2 π exp u b 2 exp l b 2 s min 2 1 erf lb s max 2 1 erf ub where e 1 e s 12 e s 1 e s 11 t and all operations are conducted element wise alternative vector expressions can be easily derived for second storage moment e2 and moments of deficit e1δ e2δ and surplus e1sp e2sp defining two other vectors c8 e 1 0 e s 12 e s 1 e s 11 t c9 e 2 0 e s 12 2 e s 1 2 e s 11 2 t the vectorized version of the objective function 24 may be written as c10 z sum e 2 0 2 k d e 1 0 k d 2 e 2 δ 2 e 1 0 e 1 δ 2 k d e 1 δ e 2 sp 2 e 1 0 e 1 sp 2 k d e 1 sp for demand vector d d 1 d 12 and operator sum representing the sum of array elements all these vectorized expressions are straightforwardly implemented in matrix programming environments such as matlab or octave d evaluating the role of the second moment of storage omitting the deficit and surplus terms at this stage by comparing the results of the fp method in which z 1 eq 25 and z 2 eq 26 are used as the objective function and simulating their policies we can evaluate how important the role of incorporating the second moments of storage is for convenience the implementations using z 1 and z 2 were named model 1 and model 2 respectively fig d1 model 1 of loucks and van beek 2017 and fig d2 model 2 show comparison of statistics obtained by optimization and simulation for both models note that in both optimization and simulation modes the values of variables inflow storage release surplus deficit in units of volume were scaled by the volume equivalent to the mean annual flow as mentioned before the simulation of the reservoir operation employed the ldr guided policies derived from optimization optimal kt values for 1000 years from which the simulated first and second sample moments of storage were calculated for every month of the year from the figures acceptable match between simulated and optimization based first and second moments are seen however to be more precise the sums of squared errors between optimized and simulated first sse 1 and second moments sse 2 for both models were used for comparison sse 1 values were 0 00094 0 00065 whereas sse 2 was 0 060 0 049 for model 1 model 2 therefore the match between optimization and simulation based raw moments of storage for model 2 with exact objective function was better than that for model 1 with zeroth order taylor approximation of the objective function it is also important to evaluate the performance of these models in terms of the most important optimality criterion i e the objective function value for model 1 while the objective function value of the optimization model was almost zero z 1 o p t 2 75 10 7 the simulated objective function value was quite different z 1 s i m 0 79 however for model 2 not only the simulated objective function z 2 s i m 0 61 was about 23 better than that of model 1 it also better matched the optimization objective function z 2 o p t 0 70 e evaluating the role of deficit and surplus variables looking carefully at the most important set of equations 3 representing the dynamics of a nonlinear bounded system one can notice that ut is not the total release from the reservoir but part of the release that makes the end of period storage volume contained in all applications of the fp model so far only ut has been used in the objective function meaning that the role of deficit and surplus terms have not been included in the objective function evaluation of any candidate solution however we show here that consideration of deficit and surplus terms is quite important when a nonlinear objective function like the one used in this study is being considered the importance of the issue is because penalizing the objective function due to deficit or surplus occurrences is all what the model s objective is about to account for these terms we derived new expressions for the first and second moments of deficit and surplus and used them in the expected value of the objective function we first analyze the role of incorporating the deficit term typically spillway capacity is very large so in cases where the downstream river s safe discharge is also large enough we may not care about surplus volumes to be penalized in the objective function e 1 role of deficit variables to evaluate how important the incorporation of the deficit term in the objective function is two other fp formulations were compared one that uses only ut in the objective function with consideration of the second moment of storage model 2b and another using the deficit term and new expressions for its first e δ t and second e δ t 2 moments added to the optimization model formulation model 3 however in both cases the release made in the simulation model is the actual total release including ut and δ t therefore the difference between simulated objective functions in model 2b and model 3 will be due to the impact of how the deficit term has been considered in the optimization model s formulation note that model 2b is the same as model 2 introduced in the previous section in optimization mode and their difference is just in simulation mode the deficit term is included in simulated releases in model 2b whereas they are not in model 2 for model 3 the objective function is e1 z 3 t 1 12 e s t 1 2 2 k t d t e s t 1 k t d t 2 e δ t 2 2 e s t 1 e δ t 2 k t d t e δ t the objective function values in optimization simulation for model 2b and model 3 were 0 7 0 61 and 3 69 3 52 respectively we also tested the case when the target demand 80 of the mean annual flow was doubled because the larger the demand the more important the impact of incorporating deficit is expected to be the objective function values in optimization simulation were 1 31 9 63 and 7 15 6 81 for models 2b and 3 respectively we observe that for the newly derived objective function expressions model 3 the objective function values in simulation and optimization matched better however there was a big gap between these values with the old expressions model 2b where the optimization model always underestimated the real objective function value simulated value another interesting point to know is what we would lose if we modeled the second moment of storage accurately but still did not account for deficit model 2 the model 3 s objective function value both simulation and optimization as the correct value was about 3 62 estimated by averaging optimization and simulation values whereas it was underestimated as 0 70 by model 2 therefore 3 62 0 70 2 92 is due to not accounting for the role of deficits in the optimization model formulation on the other hand the difference between the objective functions values of model 3 and model 1 is 3 62 2 75 10 7 3 62 therefore from the two sources of error associated with model 1 considering neither the second moments of storages nor first and second moments of deficits 0 70 3 62 19 is because of not accounting for the second moments of storages and 2 92 3 62 81 is due to not modeling deficits appropriately e 2 role of surplus variables a similar analysis was conducted for evaluating the role of incorporating surpluses by running two other types of models one where the surplus term is not accounted for in the optimization model formulation model 3b versus another in which such term is included using the newly derived expressions for the first e s p t and second e s p t 2 moments of surplus model 4 note that for both cases the surplus term is included in the simulation model while determining reservoir releases and evaluating the objective function value additionally to be fair and to analyze only the effect of surpluses without having the results being affected by the influence of deficit the deficit term is considered in both optimization and simulation for both models 3b and 4 model 3b is the same as model 3 in optimization mode and their difference is only in simulation mode for model 3b surpluses are considered while simulating fp s optimal policies whereas they are not for model 3 for model 4 the objective function is eq 26 including all moments of storage deficit and surplus in both optimization and simulation to have the role of surpluses more sensed experiments were carried out for inflow mean values equal to 2 times of the normal inflows the objective function values in optimization simulation were 8 91 26 04 and 21 30 21 52 for models 3b and 4 respectively we see that model 4 has improved the agreement between optimization and simulation significantly as the difference between optimization and simulation objective function values is around 192 for model 3b whereas it is only 1 for model 4 see fig e1 for a comparison of simulation optimization results for 1000 years of simulated gaussian inflows 
91,the exchange of gas components across the subsurface atmosphere interface influences multiphase flow and reactive transport in the subsurface and is crucial for many biogeochemical processes for the emission of greenhouse gases and for the fate of volatile contaminants in this study we present a modeling approach to simulate non isothermal multiphase flow and multicomponent reactive transport in coupled subsurface atmosphere compartments the model is based on a coupled porous medium free flow domain in which the navier stokes equation is used to describe single phase gaseous flow in the free flow subdomain and darcy s law is applied for two phase flow in the porous medium subdomain i e two domain approach the implementation is performed by coupling comsol multiphysics and phreeqcrm which enables the investigation of the interplay between multi physical processes i e flow mass and heat transport in the coupled compartments and geochemical reactions in the porous medium we first present a set of benchmark examples in which key features of the proposed model are tested against other numerical simulators and an analytical solution successively we take advantage of the unique capabilities of the proposed approach to explore conservative and reactive transport of gas components in a coupled porous medium free flow domain the results show that the exchange processes between the compartments control the location of reactive zones and the extent of geochemical reactions i e mineral dissolution by changing the spatiotemporal distribution of fluid phases and enhancing the interphase mass transfer of key gas components such as oxygen and carbon dioxide keywords soil atmosphere interactions multiphase flow in porous media multicomponent reactive transport comsol phreeqc coupling geochemical reactions data availability data will be made available on request 1 introduction the investigation of multiphase and multicomponent reactive transport processes in porous media has a wide range of scientific and engineering applications such flow and transport phenomena play a pivotal role in the understanding of many environmental systems including co2 sequestration class et al 2009 erfani et al 2021 hydrogen storage and release hagemann et al 2016 hashemi et al 2021 generation and attenuation of acid mine drainage casas et al 2015 molins and mayer 2007 muniruzzaman et al 2021 2020 muniruzzaman and pedretti 2021 pieretti et al 2022 leakage of greenhouse gases from deep underground storage to the shallow subsurface ershadnia et al 2021 solovský et al 2020 van de ven et al 2020 van de ven and mumford 2020 groundwater contamination by non aqueous phase liquids cochennec et al 2022 illangasekare et al 1995 and migration of gas components and volatile organic compounds class et al 2002 haberer et al 2012 2011 mosthaf et al 2014 qi et al 2020 you and zhan 2013 the key processes and mechanisms associated with the dynamics of fluid phases and reactive transport of components particularly gas components in many of these applications is affected by subsurface atmosphere interactions ahmadi et al 2021 2020 bahlmann et al 2020 these interactions include the exchange of mass momentum and energy across the soil atmosphere interface which are highly influenced by atmospheric conditions like air temperature humidity and wind speed ahmadi et al 2022 davarzani et al 2014 fetzer et al 2017 mosthaf et al 2014 trautz et al 2014 vanderborght et al 2017 interacting processes at the subsurface atmosphere interface control the dynamics of fluid phases in soil influence the interchange of important gas components e g oxygen water vapor and greenhouse gases between the soil and the atmosphere regulate the exchange of energy and thus affect subsurface biogeochemical processes one notable example is soil water evaporation that requires energy input from the atmospheric compartment and results in the exchange of water vapor across the soil atmosphere interface evaporation creates a two phase system in soil pores and induces a non linear and complex distribution of fluid phases i e liquid and gaseous phase and components in the individual phases under non isothermal conditions or et al 2013 shahraeeni et al 2012 shokri kuehni et al 2018 shokri et al 2008 the coupling between these processes across and within multiple phases in turn exerts important controls on key biogeochemical reactions such as salt precipitation gran et al 2011 shokri kuehni et al 2020 2018 dissolution precipitation of reactive minerals acero et al 2007 bao et al 2022 seigneur et al 2021 and microbial activity brangarí et al 2020 kim and or 2019 in the subsurface compartment reactive transport models are established tools to quantitatively describe physical and biogeochemical processes occurring in soil sediments and groundwater systems e g appelo and rolle 2010 steefel et al 2015 different simulators are used to solve single phase e g pht3d prommer et al 2003 phast parkhurst et al 2010 crunchflow steefel and lasaga 1994 np phreeqc rolle et al 2018 np phreeqc ek sprocati et al 2019 and unsaturated multiphase e g hp1 hpx šimunek et al 2012 min3p mayer et al 2002 amd phreeqc muniruzzaman et al 2020 and toughreact xu et al 2011 reactive transport problems the latter simulators can couple multiphase flow and transport equations with a wide variety of geochemical reactions however the description of reactive transport processes in current subsurface codes is typically limited to the porous medium domain and the feedbacks between the fluid flow and biogeochemical processes occurring within and across individual compartments i e atmosphere soil atmosphere interface and subsurface are not explicitly included the objective of this study is to present a modeling approach capable of simulating non isothermal multiphase and multicomponent reactive transport processes in porous media and to explicitly demonstrate the effects of soil atmosphere interactions on such reactive transport processes we consider a coupled porous medium free flow domain and solve a different set of equations in each subdomain suitable coupling conditions are applied at the interface to ensure the continuity of mass energy and momentum jamet et al 2009 mosthaf et al 2011 the implementation is based on the coupling between the simulator comsol multiphysics and the geochemical code phreeqc parkhurst and appelo 2013 by taking advantage of the phreeqcrm module parkhurst and wissmeier 2015 in the porous medium subdomain the flow of the fluids i e gaseous and liquid phases is simulated by solving the two phase darcy equation whereas the simulation of the free flow i e gaseous phase in the atmospheric compartment is performed by solving the navier stokes equation the heat and multicomponent transport in fluid s is described in each compartment whereas interphase mass transfer of gas components across the fluid phases and geochemical reactions are considered in the porous medium subdomain the reaction network includes both equilibrium and kinetically controlled geochemical reactions that are solved with phreeqcrm we first test the key features of the model comparing the outcomes of the proposed approach with other numerical codes i e min3p and dumux and with an analytical solution in a set of benchmark examples successively we perform conservative and reactive transport simulations in a coupled subsurface atmosphere domain to simulate evaporation migration of gas components e g o2 and co2 and geochemical reactions i e reactive mineral dissolution precipitation and aqueous speciation in a porous medium exposed to temperature gradients and wind in the adjacent atmospheric free flow 2 modeling approach a two domain approach is adopted for the description of coupled soil atmosphere systems using this approach in the porous medium subdomain we consider a non isothermal multiphase and multicomponent reactive transport model whereas in the free flow subdomain we employ a non isothermal single phase multicomponent transport formulation the governing equations in each subdomain as well as the coupling conditions are presented in the following sections 2 1 non isothermal flow and reactive transport in porous media the quantitative description of non isothermal multiphase and multicomponent reactive transport processes in porous media requires the formulation of the mass balance equations for the different phases in this study we consider two fluid phases i e liquid and gaseous in a porous medium and the mass balance equation for the fluid phase α l g can be written as 1 ε s α ρ α t ρ α u α r α where ε is the porosity and s α ρ α u α and r α are the saturation density darcy flux and the source sink term of the fluid phase α respectively the density of the liquid phase is assumed to be constant whereas the density of the gaseous phase is obtained using the ideal gas law as ρ g p g m r t with r being the universal gas constant m the mean molar mass of components in the gaseous phase and t the temperature the darcy flux in this formulation can be defined as 2 u α k i n t k r α μ α p α ρ α g where k int is the intrinsic permeability of the porous medium k r α is the saturation dependent relative permeability of phase α μ α and p α are the dynamic viscosity and the pressure of phase α respectively and g is the gravity vector we can consider eq 1 as the mass balance equation for one of the phases and specify the total mass balance equation by summing up eq 1 as 3 α l g ε s α ρ α t α l g ρ α u α α l g r α in the above formulation we can consider the saturation of the gaseous phase and the pressure of the liquid phase as the primary variables the pressure of the gaseous phase can then be determined by the capillary pressure relation defined as pc pg pl and the saturation of the liquid phase can be calculated using sl 1 sg as the sum of the phase saturations is equal to one this formulation is used for the full description of multiphase flow and can be simplified to one mass balance equation for the liquid phase i e richards equation by considering a passive gas phase pg 0 in order to close the system of equations the constitutive relations expressing the capillary pressure and relative permeability as a function of the fluid phase saturation are specified with the van genuchten mualem model mualem 1976 van genuchten 1980 4 s e l 1 α h c n m h c 0 1 h c 0 5 k r l s e l l 1 1 s e l 1 m m 2 6 k r g 1 s e l l 1 s e l 1 m 2 m where hc is the capillary pressure head computed as h c p c ρ l g s e l is the effective saturation of the liquid phase calculated as s e l s l s r 1 s r with sr being the residual saturation of the liquid phase and α n l and m 1 1 n are empirical parameters in addition to the phase balance equations we need to describe the reactive transport of components in the porous medium subdomain using the mass balance equations for components the components can be present in the liquid and the gaseous phase partition between the phases and participate in geochemical reactions the mass balance equation for the component k that is present in phase α l g can be written as 7 ε s α ρ α k t ρ α k u α d α e f f k ρ α k r α k where ρ α k d α e f f k and r α k are the mass concentration the effective diffusion coefficient and sinks sources of component k in the α phase respectively the diffusion of components in each fluid phase only occurs in the pore space occupied by the corresponding fluid phase and this needs to be accounted for by defining the effective diffusion coefficient for each phase as d α e f f k ε s α d α k t τ α in this formulation d α k t is the temperature dependent free diffusion coefficient of the component k in the α phase calculated as d α k t d α k t 273 15 2 campbell 1985 and τα is the tortuosity in the phase α which can be approximated as τ α ε 1 3 s α 7 3 millington 1959 the consumption and production of components due to chemical reactions e g dissolution of reactive minerals as well as the interphase mass transfer of components are included in the sink source term of eq 7 in this study we consider water oxygen and carbon dioxide as the three components that are present in the liquid and gaseous phases and can partition between them note that water is the main component of the liquid phase and thus the mass balance equation for this component in the liquid phase is already determined by the phase balance equation eq 1 however the mass balance equation for the water component in the gaseous phase i e water vapor and its exchange with the liquid phase i e phase change is provided by eq 7 we use a kinetic mass transfer expression to describe the partitioning of these components between the phases and to link their mass balance equations such expression for the water component can be written as trautz et al 2014 8 r g w r l k e q w ρ v s a t ρ g w ε s w where k eq w is the mass transfer coefficient and ρ v sat is the saturated vapor density that is a temperature dependent function determined by tetens equation as ρ v s a t p 0 10 7 5 t 273 15 t 35 85 r t m w with p 0 being 610 7 pa r the universal gas constant t the temperature in kelvin and mw the molecular weight of water monteith and unsworth 2014 the kinetic mass transfer expression for the component k o 2 co 2 can be formulated as solovský et al 2020 9 r l k r g k k e q k ρ g k h k ρ l k where hk is the dimensionless henry s law coefficient of the component k this coefficient is a function of temperature and can be calculated by van t hoff equation as h k h r e f e δ s o l h r 1 t 1 t r e f with tref href and δ sol h being the reference temperature the henry s constant at the reference temperature and the enthalpy of dissolution for the component k respectively the value of δ s o l h r for o2 and co2 is given as 1700 k and 2400 k respectively sander 2015 additionally we need to express the governing equations for the energy balance in the porous medium subdomain assuming a local thermal equilibrium between the porous medium liquid and gaseous phases the energy balance equation can be written as 10 ρ b c p e f f t t α l q ρ α c p α u α t λ e f f t δ h v a p r l where λ eff is the effective thermal conductivity δhvap is the enthalpy of vaporization and ρ b cp eff can be defined as 11 ρ b c p e f f 1 ε ρ s c p s α l q ε s α ρ α c p α with c p α c p s and ρ s being the heat capacity of the fluid phase α the heat capacity and density of the solid matrix respectively the model proposed by johansen 1977 was used to determine the effective thermal conductivity as λ eff λ dry ke λ sat λ dry with λ dry and λ sat being the effective thermal conductivities under dry and fully saturated conditions respectively and ke being a function of the liquid phase saturation given by côté and konrad 2005 as k e κ s l 1 κ 1 s l where κ is a dimensionless empirical fitting parameter 2 2 non isothermal flow and transport in free flow in the free flow subdomain we describe the non isothermal migration of a gaseous phase containing multiple components this is done by applying a non isothermal single phase multicomponent transport formulation the governing equations for the single phase flow i e gaseous phase in such system is determined by a combination of the momentum balance and total mass balance the momentum balance in the free flow can be described by the navier stokes equation as 12 ρ g u g t ρ g u g u g t τ p g ρ g g 0 where τ is the shear stress tensor defined as τ μ g u g u g t 2 3 μ g u g i with i being the identity matrix the total mass balance equation in the free flow can be written as 13 ρ g t ρ g u g q m where qm is the sink source term additionally we need to specify the component mass balance equation in the free flow subdomain this can be done by using the simplified form of eq 7 in which we consider α g the porosity ε and the fluid phase saturation sg are equal to one and assume the sink source term r g k to be zero similarly the heat balance equation in the free flow region can be expressed as 14 ρ g c p g t t ρ g c p g u g t λ g t 0 where λ g is the thermal conductivity of the gaseous phase 2 3 interface coupling conditions we adopt two different sets of equations in the porous medium and free flow subdomains and we therefore need to apply coupling conditions at the interface between these two subdomains the coupling conditions presented here are based on the general concept developed in previous works ahmadi et al 2021 davarzani et al 2014 mosthaf et al 2011 assuming that the flux of the liquid phase in the porous medium across the interface is zero the continuity of normal mass fluxes between the free flow ff and the porous medium pm is ensured by 15 ρ g u g n ff ρ g u g n pm with n being the outward pointing normal unit vector at the interface of each subdomain we also need to fulfill the momentum balance the normal part of the momentum balance is determined by applying the continuity of the normal stress as 16 τ p i n n ff p pm the tangential part of the momentum balance is set based on the beavers joseph saffman condition saffman 1971 as 17 u k α b j μ g τ n t ff 0 in which α bj is the dimensionless beavers joseph coefficient t is a tangential vector k kt t denotes the tangential component of the permeability tensor and τ the shear stress tensor additionally the continuity of the component concentrations and mass fluxes at the interface needs to be satisfied considering zero liquid phase flux at the interface such continuity equations can be written as 18 ρ g k ff ρ g k pm 19 ρ l k pm ρ g k ff h k 20 ρ g k u d α k t ρ g k n ff ρ g k u d α e f f k ρ g k n pm and the continuity of temperature and heat fluxes need to be applied as 21 t ff t pm 22 ρ g c p g u g t λ g t n ff α l q ρ α c p α u α t λ e f f t n pm 2 4 numerical implementation the equations governing the physical processes i e flow and component heat transport in the porous medium free flow domain with associated initial and boundary conditions are solved in comsol multiphysics for the solution of flow and transport processes we incorporate both a fully coupled and a simplified approach the latter can be used to simulate multicomponent reactive transport processes in partially saturated porous media without explicitly considering the free flow subdomain and assuming a passive gas phase in this approach the flow problem in the porous medium subdomain is simulated in the first step by solving the richards equation and then the obtained fluid phase saturations and the liquid phase velocity are used in the second step to simulate multicomponent transport processes in the partially saturated porous medium however for the simulation of the multiphysical processes i e flow and component heat transport either in the coupled porous medium free flow domain or in the two phase porous media where the gaseous phase movement cannot be neglected the entire set of equations describing the system of interest is solved in a fully coupled way the computational domains considered in this study include 1 d and 2 d geometries that were discretized using the finite element method the discretization of the 2 d domains was performed by a non uniform triangular mesh that was greatly refined near the boundaries as well as at the interface between the porous medium and the free flow subdomains supplementary material section s2 an implicit time marching scheme backward euler differentiation was used for the time discretization and a combination of a direct solver pardiso and a nonlinear newton solver was used to solve the equations of interest in the respective domains in order to describe the interactions between the physical processes and the geochemical reactions e g chemical speciation dissolution of reactive minerals we employed a sequential non iterative operator splitting approach in which the simulation of the physical and reactive processes is performed in two sequential steps as illustrated in fig 1 muniruzzaman and rolle 2019 2016 soulaine et al 2021 wissmeier and barry 2011 in this approach after computing the non isothermal multiphase flow and multicomponent transport either in a porous medium domain or in a coupled porous medium free flow system using comsol multiphysics within a time step δt the computed fluid phase saturations and component concentrations are passed to phreeqcrm for the calculation of reactive processes successively the updated values of the component concentrations are passed back to comsol multiphysics to perform the computation of the physical processes in the subsequent time step the time discretization and the time steps taken by the solvers in each software are internally adjusted but the coupling time step δt is chosen by the user and needs to be kept reasonably small in order to minimize operator splitting errors the coupling between the two codes is performed with livelink for matlab a module facilitating the communication of comsol multiphysics with other software 3 benchmark examples the capabilities and the key features of the proposed modeling approach are tested in four benchmark problems and the simulation outcomes are compared with the results of other numerical simulators i e min3p and dumux and an analytical solution in the first and second benchmark example we tested the capability of the proposed model to simulate dissolution of reactive minerals in a 1 d partially saturated porous medium domain and validated the simulation outcomes against the code min3p mayer et al 2015 in these examples we considered a porous medium domain without free flow and applied the simplified approach richards equation for the simulation of flow and multicomponent transport processes in the porous medium as detailed in section 2 4 these two benchmark examples allowed us to evaluate the capability of the proposed model to compute the distribution of phases resulting from the water infiltration into the variably saturated porous medium advective dispersive transport of components in the liquid phase diffusive transport of gas components in the gaseous phase interphase mass transfer of gas components aqueous speciation and mineral reactions in two phase systems the third benchmark example originally presented by udell 1985 as heat pipe problem assesses the capability of the proposed modeling approach to simulate simultaneous and coupled heat propagation evaporation condensation and compositional two phase flow in porous media using the full multiphase description in the fourth benchmark example we increased the complexity by considering a coupled porous medium free flow domain in order to simulate evaporation from a porous medium exposed to an adjacent wind flow with high temperature and low humidity we compared the outcomes of this benchmark example with another code dumux that is widely used for simulation of such dual continua problems koch et al 2021 this example allowed us to evaluate the correct implementation of the coupling concept that we applied to describe the exchange processes occurring at the interface between the porous medium and the free flow and to properly simulate heat transport fluid s flow and component transport in such coupled systems 3 1 reactive transport in a partially saturated porous medium benchmark 1 and 2 benchmark examples 1 and 2 focus on reactive transport in a 1 d partially saturated porous medium domain with length of 5 m the flow problem in these examples involves water infiltration into a porous medium domain simulated by solving richards equation under steady state conditions a flux boundary conditions u l 9 51 10 9 m s is applied at the top boundary and a constant head value h 2 5 m was prescribed at the bottom boundary the spatial distribution of the fluid phases was determined after such simulation and used for the calculation of the reactive transport processes considered for the two examples the first example originally presented by mayer et al 2015 considers oxidation of pyrite due to the oxygen penetration from the top boundary the oxidative dissolution of pyrite leads to the generation of acidity and release of components such as iron and sulfate in the liquid phase e g andersen et al 2001 battistel et al 2019 the second example uses the same flow conditions but adds complexity in the geochemistry by considering also the presence of calcite in the porous medium the dissolution of this reactive carbonate mineral triggered by the protons released during pyrite oxidation results in ph buffering and in the production of co2 in the system the total simulation time for both examples is 10 years the model input parameters for the simulation of flow and reactive transport in both examples are given in table 1 with detailed information about the composition of solutions applied as the initial and boundary conditions as well as the rate expressions in the supplementary material section s1 and s3 more details about the thermodynamic database used in the simulations can be found in mayer et al 2015 fig 2 presents the simulation outcomes for the first benchmark example and provides a comparison with the simulation results using the code min3p the simulated liquid phase saturation shows a transition from partially saturated to fully water saturated conditions in the middle of the domain fig 2a this creates a two phase system in the upper part of the domain in which oxygen propagates fig 2b causing the oxidative dissolution of pyrite the initially uniform pyrite content φ m is consumed upon the reaction with oxygen penetrating from the top boundary fig 2d the reaction induces a decrease of ph with depth and causes an abrupt change in the pe front between the upper part of the domain where oxidizing conditions prevail and the anoxic part at the bottom fig 2c the very good agreement between the outcomes of the two models validates the capability of the proposed code to compute the spatial distribution of phases in variably saturated porous media component transport in liquid gaseous phases and geochemical reactions the second benchmark example is designed based on the first example but considers a more complex geochemical system including both pyrite and calcite as reactive minerals the simulation results are shown in fig 3 and indicate the oxygen depletion and pyrite dissolution in the upper part of the domain similar to the first example however in this case the dissolution of calcite buffers the acidity and the ph stabilizes to a values 6 5 below the water table fig 3c in addition to the ph buffering the calcite dissolution leads to the production of dissolved carbonate species and also of co2 through the equilibrium reactions of the aqueous carbonic acid system the generated co2 transfers to the gaseous phase in the partially saturated region where it migrates upward via diffusive transport in the gaseous phase and downward through advective dispersive transport in the liquid phase fig 3b concerning the solid phases the content of both reactive minerals diminishes in the upper part of the domain in particular in this zone calcite is completely consumed and a sharp dissolution front for this reactive mineral occurs at a depth of approximately two meters the comparison between the simulation results of the proposed model and min3p demonstrates a good agreement for most components with some discrepancies observed for the computed co2 in the lower part of the domain similar discrepancies have been reported in other benchmark studies and can be attributed to internal differences in the geochemical calculation of the carbonate system in min3p and phreeqc e g mayer et al 2015 the outcomes confirm the ability of the model to simulate complex geochemical systems involving mineral dissolution ph buffering and production consumption of different gas components in partially saturated porous media 3 2 non isothermal multiphase flow and multicomponent transport in a porous medium benchmark 3 the third benchmark example i e heat pipe problem allows us to demonstrate the ability of the proposed model to simulate non isothermal two phase i e liquid and gaseous and two component i e air and water vapor flow and transport in a 2 d porous medium the domain is partially saturated sl 0 7 and the initial temperature is 70 c the liquid phase invades the porous medium from the left boundary sl 1 and t 68 4 c and advances toward the right hand side of the domain where a heat source qt 100 w increases the temperature up to the boiling point i e hot end of the domain this leads to evaporation heat propagation through conduction and convection and appearance of a gaseous phase on the right side of the domain due to the pressure gradients the gaseous phase progressively flows toward the left i e cool end of the domain where it condenses and moves back toward the hot end of the domain on the right this circulation flow induced by the opposite migration of the liquid and gaseous phases and evaporation condensation at the two opposite sides of the domain continues until the system reaches a steady state condition udell 1985 derived an analytical solution for the spatial distribution of temperature fluid phase pressures and saturations and mole fraction of components at steady state note that such solution is derived considering the leverett function for the capillary pressure saturation relation leverett 1941 and using a simplified function for the relative permeability saturation relation fatt and klikoff 1959 also the effect of temperature on the diffusion coefficient of components in the fluid phases is not considered in this example and a constant tortuosity value of 0 5 is used in the simulation the model input parameters used in the simulation of the heat pipe problem are provided in table 2 fig 4 illustrates the two dimensional system and shows the comparison between the numerical results and the analytical solution the good agreement between the numerical and analytical results shows that the model can accurately describe the heat pipe problem and can capture the complex interplay of non isothermal multiphase and multicomponent transport processes in the porous medium in particular the results demonstrate that the formation of three regions in the porous medium i e fully water saturated two phase and fully dry and the distribution of temperature in such regions are captured well by the numerical model the capability of the proposed model to simulate complex processes such as phase change heat transport phase displacement and component transport within and across phases during fluid s flow in porous media is successfully benchmarked 3 3 evaporation and non isothermal compositional two phase flow in a porous medium exposed to wind flow benchmark 4 in the fourth benchmark example we consider a coupled porous medium free flow domain and tested the performance of the proposed model to simulate exchange processes at the interface between the porous medium and free flow and to show the impact of such interchange dynamics on the fluids phase distribution and the heat propagation in the porous medium the model input parameters used for the simulation of this benchmark example is given in the supplementary material table s2 the porous medium is initially saturated sl 0 95 and the temperature is 293 15 k in the entire domain the porous medium is exposed to the free flow with an average velocity of 20 cm s temperature of 300 15 k and relative humidity of 0 1 fig 5 presents the setup considered for this benchmark example and provides a comparison between the simulation results of the proposed code and dumux the concentration and temperature gradients at the interface promote evaporative fluxes from the porous medium surface resulting in the appearance of a drying front that progressively penetrates into the porous medium as shown in fig 5b the imposed temperature gradients leads to a temperature rise at vicinity of the interface but this temperature increase is balanced by the cooling effect from water evaporation and causes the temperature to progressively decrease inside the porous medium the comparison between the outcomes of the two simulators shows a very good agreement which confirms that the proposed model is able to properly describe the non linear and interactive processes in coupled porous medium free flow systems thus the model can be extended to explore the exchange of multiple components across such coupled systems and to understand the effects of the interface processes on the dynamics of reactive transport phenomena in the porous medium 4 modeling soil atmosphere physico chemical interactions in this section we introduce two examples illustrating the capability of the proposed model to simulate non isothermal multiphase and multicomponent transport processes in porous media incorporating dynamic forcing from the atmospheric compartment first we investigate the conservative transport of water vapor oxygen and carbon dioxide in a porous medium domain exposed to an overlying free flow domain with wind high temperature and low humidity successively we extend the porous medium free flow scenario by considering geochemical reactions in the porous medium as well as different wind velocities in the free flow domain 4 1 exchange of gas components at the soil atmosphere interface in the first example we demonstrate the capability of the model to simulate evaporation from the soil surface i e exchange of water component at the soil atmosphere interface considering dynamic forcing in the atmospheric compartment and to describe the spatiotemporal distribution of phases and gas components in porous media during such interface driven processes the model input parameters used for this simulation are given in table 3 and more details about the initial and boundary conditions are given in the supplementary material section s2 and s3 the setup consists of a coupled porous medium free flow domain with gas flow occurring in a free flow domain in contact with a saturated porous medium where no oxygen and a 0 085 mol m3 of co2 were initially present fig 6 a at the beginning of the simulation the pore space in the porous medium is entirely filled with water and the temperature in the domain is 293 15 k a horizontal gas flow with an average velocity of 2 cm s and temperature of 300 15 k containing a mixture of water vapor oxygen and carbon dioxide enters the free flow subdomain this creates strong chemical and thermal gradients at the interface between the free flow and the porous medium the simulated spatial distribution of temperature and vapor concentration shown in fig 6b and c indicates the formation of boundary layers at the interface where such gradients lead to the exchange of mass and energy between the two subdomains this is reflected in the migration of water vapor toward the free flow fig 6c and in the heat propagation in the opposite direction fig 6b such interchange processes lead to phase change i e evaporation in the porous medium and to the formation of a drying front that penetrates into the porous medium as a consequence a variably saturated region is formed and a non linear spatiotemporal distribution of the fluid phases under non isothermal conditions occurs in the porous medium subdomain fig 6d in addition to the dynamics of fluid phases the propagation of gas components i e o2 and co2 is greatly impacted by the interface processes as evaporation proceeds and the gaseous phase appears in the porous medium the oxygen in the gaseous phase diffuses into the initially anoxic porous medium and dissolves in the liquid phase the downward migration of oxygen occurs in both phases via diffusion which is significantly faster in the gaseous phase i e 104 times higher diffusion coefficient than in the liquid phase such diffusive behavior can be seen in the spatial profile of oxygen at t 3 h fig 6e as the partially saturated region develops and the gaseous phase saturation increases due to evaporation e g t 30 h the interphase mass transfer of gas components between the two phases becomes stronger this leads to an abrupt increase in the o2 concentration in the partially saturated region and considerably accelerates the slow o2 diffusive transport to a fast propagation pattern at later times t 30 and 70 h in fig 6d and e a similar transport behavior but in the opposite direction governs the co2 transport co2 was initially present in the liquid phase at higher concentration than in the free flow domain therefore it migrates upward via diffusion and transfers to the gaseous phase as the drying front advances into the porous medium fig 6f the results of this conservative transport scenario show that the dynamic forcing in the atmospheric compartment e g wind temperature and humidity induces strong gradients at the soil atmosphere interface which can significantly influence the distribution and migration of key gas components e g o2 and co2 in the subsurface compartment 4 2 coupling of dynamic atmospheric forcing and geochemical reactions the migration of gas components controls a variety of biogeochemical reactions in the subsurface and therefore it is important to investigate the impact of physical atmospheric forcing on subsurface reactive processes to this end we extend the setup used in the first example by including geochemical reactions similar to the benchmark problem two presented in section 3 the reaction network comprises two reactive minerals i e pyrite and calcite in the porous medium as well as equilibrium speciation reactions in the pore water we also used the same rate expressions chemical composition and thermodynamic database as in the benchmark two different free flow velocities 2 and 20 cm s were considered in order to evaluate the impact of different external forcing conditions on the extent of mineral dissolution and the spatiotemporal distribution of the fluid phases and components in the porous medium the model input parameters for this example are provided in table 3 and the details of the chemical compositions applied as the initial and boundary conditions as well as the reactions rates are given in the supplementary material section s1 and s3 the simulation outcomes including the 2 d maps of the temperature distribution the water vapor concentration and the liquid phase saturation for the two free flow velocities at t 42 h are shown in fig 7 the comparison between the spatial distributions of the temperature and the water vapor concentration shows noticeable differences particularly close to the interface where strong gradients in the boundary layer control the dynamics of evaporation in the porous medium at the higher free flow velocity a thinner boundary layer and stronger concentration temperature gradients are formed resulting in a higher water vapor flux toward the free flow and therefore more evaporation in the porous medium this is reflected in the spatial distribution of the fluid phases showing a drier surface and a further penetration of the drying front as the free flow velocity increases fig 7c and f such differences are of great importance for the dynamics of the pyrite and calcite dissolution that highly rely on the distribution of the fluid phases and the availability of oxygen as the key reactant this can be investigated by interpreting the spatial profiles of the components in the porous medium calculated for the two free flow velocities with the proposed coupled comsol multiphysics phreeqcrm modeling approach fig 8 the appearance and the progressive invasion of the gaseous phase due to evaporation facilitates the diffusive transport of oxygen i e high diffusivity of oxygen in the gaseous phase enhances the interphase mass transfer processes and thus provides high oxygen concentrations in the liquid phase above the drying front i e the partially saturated zone as shown in fig 8a the available oxygen in the liquid phase is largely consumed during pyrite oxidation but it is continuously replenished by the progressively advancing gaseous phase via the interphase mass transfer mechanism meaning that the extent of the pyrite oxidation is not limited by the availability of oxygen in the partially saturated zone as explained above the oxidative dissolution of pyrite greatly impacts the dissolution of other minerals and the pore water quality for instance the extent of calcite dissolution increases due to the acidity released from pyrite oxidation triggered by the high oxygen availability resulting from the atmospheric forcing the outcome of these processes is the excessive production of carbon dioxide sulfate and calcium as the products of the mineral dissolution and speciation reactions and their migration in the porous medium fig 8b d all these components show an accumulation in the portion of the porous medium where the reaction occurs with milder spatial gradients towards the background concentration at the bottom and steeper gradients toward the reacted zone at the top of the domain these results highlight the feedback loop between the dynamics of the physical and reactive processes in the porous medium driven by the free flow porous medium interactions the impact of such interactions becomes more evident by comparing the spatial distribution of the reaction products for the two free flow velocities at the higher free flow velocity the gaseous phase invades a larger fraction of the pore space and progresses deeper within the porous medium fig 7c and f leading to higher availability and faster propagation of oxygen in the porous medium and thus higher concentration of the reaction products including carbonates sulfate and calcium fig 8 besides the concentration the spatial distribution of the reaction products is also influenced by the different free flow velocities as a deeper penetration depth can be observed for these components with increasing free flow velocity this is particularly pronounced for the co2 front because the mechanisms governing the co2 transport entail a complex interplay between the advancement of the drying front the diffusion in the individual liquid and gaseous phases and the interphase mass transfer the produced co2 due to the calcite dissolution and speciation reactions can propagate toward the bottom part of the domain by diffusive transport in the liquid phase and can also exchange to the gaseous phase and migrate toward the subsurface atmosphere interface this upward migration is driven by the low co2 concentrations in the free flow compartment i e atmospheric composition that leads to strong co2 concentration gradients in the upper part of the porous medium and promotes the diffusive migration of co2 from the porous medium toward the free flow the co2 concentration gradients and thus the exchange of this component between the two compartments become stronger as the free flow velocity increases and more gaseous phase becomes available in the porous medium due to the enhanced drying fig 8b the outcomes of these reactive transport simulations allowed us to quantitatively assess the influence of soil atmosphere interactions on the distribution of fluid phases transport of key components and dynamics of important geochemical reactions in subsurface porous media 5 conclusions in this study we have investigated the dynamics of subsurface atmosphere interactions for conservative and reactive transport scenarios we have presented a modeling approach capable of simulating non isothermal multiphase and multicomponent reactive transport processes in porous media and explicitly reflecting the influence of soil atmosphere exchange on the dynamics of flow transport and geochemical reactions in porous media the modeling framework is developed based on the two domain approach in which the system is conceptualized as a coupled domain composed of a porous medium and a free flow subdomain that are separated by a common interface the simulation of the governing physical i e fluid s flow component transport and heat transport in each subdomain and reactive processes i e geochemical reactions in the porous medium subdomain is performed by coupling the general partial differential equation solver comsol multiphysics and the geochemical code phreeqc the different features of the proposed code including the ability to accurately compute reactive transport processes in partially saturated porous media to simulate coupled heat transport evaporation condensation phase displacement and component transport within and across phases in porous media and to simulate evaporative drying and heat propagation in a coupled porous medium free flow system were tested in four well defined benchmark problems successively we presented two scenarios to simulate evaporation phase displacement and conservative as well as reactive transport of multiple aqueous and gaseous components in a porous medium that is exposed to an overlying free flow for the reactive transport we focused on mineral dissolution i e pyrite and calcite and speciation reactions in the porous medium to evaluate the impact of soil atmosphere interactions on the dynamics of geochemical reactions and pore water quality the simulation results illuminate the mechanistic interactions and the feedback loops between physical processes and geochemical reactions in a coupled soil atmosphere system key physical processes include coupled flow mass transport and heat transport in both compartments and at their interface for the conservative scenario the simulation outcomes show that the soil atmosphere interactions significantly influence the exchange of gas components by inducing strong gradients at the porous medium free flow interface and affect the transport behavior of the gas components in the porous medium by progressively changing the spatiotemporal distribution of the fluid phases and enhancing interphase mass transfer processes the results of the reactive scenario demonstrate that the enhanced supply of oxygen during the phase displacement increases pyrite and calcite dissolution and significantly changes the pore water chemistry we also showed that a higher free flow velocity in the atmospheric compartment increases the extent of mineral dissolution and changes the propagation of the reaction products particularly the gas component co2 in the porous medium by influencing the dynamics of the fluid phase distribution in the pore space the proposed modeling approach can be extended to simulate more complex conditions typical of natural systems that can be impacted by atmosphere subsurface exchange of gas components such developments can focus on heterogenous porous media different levels of water saturation as well as different geochemical reactions including salt precipitation induced by evaporation jambhekar et al 2015 dissolution precipitation of carbonates and iron oxides deng et al 2022 haberer et al 2015 oxidation of iron sulfide minerals and release of geogenic contaminants asta et al 2010 battistel et al 2021 fakhreddine et al 2016 furthermore the model could be applied to explore the coupling between physical and microbial processes in free flow porous medium systems this can be of great interest to study both natural environments such as arid and semi arid regions where soil drying can strongly influence microbial activity or permafrost regions where other phase change processes i e freezing and thawing can be a driving force for microbially mediated release of greenhouse gases brangarí et al 2020 kim and or 2019 and engineering applications such as landfills and contaminated sites in these systems biogeochemical processes such as respiration and methane oxidation scheutz et al 2009 xu et al 2014 as well as mixing controlled aerobic degradation of organic compounds rolle and le borgne 2019 can be greatly impacted by atmospheric forcing finally besides the application to the subsurface and atmosphere compartments the proposed modeling approach could be extended to describe the interactions between physical and geochemical processes in other coupled environmental systems by taking advantage of the dual continua approach implemented in the code carrillo et al 2020 stolze and rolle 2022 zhang et al 2022 an important example is the exchange of nutrients and gas components e g o2 and n2o between shallow groundwater and surface water which is critical for biogeochemical cycles and greenhouse gas emissions bu et al 2021 reeder et al 2018 wallace et al 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the independent research fund denmark project giga gas interchange between groundwater and air grant dff 7017 00130 m r also acknowledges the support of the project biodegrates grant 0136 00205b and k h acknowledges the support of the german research foundation dfg german research foundation within the collaborative research center sfb 1313 project no 327154368 
91,the exchange of gas components across the subsurface atmosphere interface influences multiphase flow and reactive transport in the subsurface and is crucial for many biogeochemical processes for the emission of greenhouse gases and for the fate of volatile contaminants in this study we present a modeling approach to simulate non isothermal multiphase flow and multicomponent reactive transport in coupled subsurface atmosphere compartments the model is based on a coupled porous medium free flow domain in which the navier stokes equation is used to describe single phase gaseous flow in the free flow subdomain and darcy s law is applied for two phase flow in the porous medium subdomain i e two domain approach the implementation is performed by coupling comsol multiphysics and phreeqcrm which enables the investigation of the interplay between multi physical processes i e flow mass and heat transport in the coupled compartments and geochemical reactions in the porous medium we first present a set of benchmark examples in which key features of the proposed model are tested against other numerical simulators and an analytical solution successively we take advantage of the unique capabilities of the proposed approach to explore conservative and reactive transport of gas components in a coupled porous medium free flow domain the results show that the exchange processes between the compartments control the location of reactive zones and the extent of geochemical reactions i e mineral dissolution by changing the spatiotemporal distribution of fluid phases and enhancing the interphase mass transfer of key gas components such as oxygen and carbon dioxide keywords soil atmosphere interactions multiphase flow in porous media multicomponent reactive transport comsol phreeqc coupling geochemical reactions data availability data will be made available on request 1 introduction the investigation of multiphase and multicomponent reactive transport processes in porous media has a wide range of scientific and engineering applications such flow and transport phenomena play a pivotal role in the understanding of many environmental systems including co2 sequestration class et al 2009 erfani et al 2021 hydrogen storage and release hagemann et al 2016 hashemi et al 2021 generation and attenuation of acid mine drainage casas et al 2015 molins and mayer 2007 muniruzzaman et al 2021 2020 muniruzzaman and pedretti 2021 pieretti et al 2022 leakage of greenhouse gases from deep underground storage to the shallow subsurface ershadnia et al 2021 solovský et al 2020 van de ven et al 2020 van de ven and mumford 2020 groundwater contamination by non aqueous phase liquids cochennec et al 2022 illangasekare et al 1995 and migration of gas components and volatile organic compounds class et al 2002 haberer et al 2012 2011 mosthaf et al 2014 qi et al 2020 you and zhan 2013 the key processes and mechanisms associated with the dynamics of fluid phases and reactive transport of components particularly gas components in many of these applications is affected by subsurface atmosphere interactions ahmadi et al 2021 2020 bahlmann et al 2020 these interactions include the exchange of mass momentum and energy across the soil atmosphere interface which are highly influenced by atmospheric conditions like air temperature humidity and wind speed ahmadi et al 2022 davarzani et al 2014 fetzer et al 2017 mosthaf et al 2014 trautz et al 2014 vanderborght et al 2017 interacting processes at the subsurface atmosphere interface control the dynamics of fluid phases in soil influence the interchange of important gas components e g oxygen water vapor and greenhouse gases between the soil and the atmosphere regulate the exchange of energy and thus affect subsurface biogeochemical processes one notable example is soil water evaporation that requires energy input from the atmospheric compartment and results in the exchange of water vapor across the soil atmosphere interface evaporation creates a two phase system in soil pores and induces a non linear and complex distribution of fluid phases i e liquid and gaseous phase and components in the individual phases under non isothermal conditions or et al 2013 shahraeeni et al 2012 shokri kuehni et al 2018 shokri et al 2008 the coupling between these processes across and within multiple phases in turn exerts important controls on key biogeochemical reactions such as salt precipitation gran et al 2011 shokri kuehni et al 2020 2018 dissolution precipitation of reactive minerals acero et al 2007 bao et al 2022 seigneur et al 2021 and microbial activity brangarí et al 2020 kim and or 2019 in the subsurface compartment reactive transport models are established tools to quantitatively describe physical and biogeochemical processes occurring in soil sediments and groundwater systems e g appelo and rolle 2010 steefel et al 2015 different simulators are used to solve single phase e g pht3d prommer et al 2003 phast parkhurst et al 2010 crunchflow steefel and lasaga 1994 np phreeqc rolle et al 2018 np phreeqc ek sprocati et al 2019 and unsaturated multiphase e g hp1 hpx šimunek et al 2012 min3p mayer et al 2002 amd phreeqc muniruzzaman et al 2020 and toughreact xu et al 2011 reactive transport problems the latter simulators can couple multiphase flow and transport equations with a wide variety of geochemical reactions however the description of reactive transport processes in current subsurface codes is typically limited to the porous medium domain and the feedbacks between the fluid flow and biogeochemical processes occurring within and across individual compartments i e atmosphere soil atmosphere interface and subsurface are not explicitly included the objective of this study is to present a modeling approach capable of simulating non isothermal multiphase and multicomponent reactive transport processes in porous media and to explicitly demonstrate the effects of soil atmosphere interactions on such reactive transport processes we consider a coupled porous medium free flow domain and solve a different set of equations in each subdomain suitable coupling conditions are applied at the interface to ensure the continuity of mass energy and momentum jamet et al 2009 mosthaf et al 2011 the implementation is based on the coupling between the simulator comsol multiphysics and the geochemical code phreeqc parkhurst and appelo 2013 by taking advantage of the phreeqcrm module parkhurst and wissmeier 2015 in the porous medium subdomain the flow of the fluids i e gaseous and liquid phases is simulated by solving the two phase darcy equation whereas the simulation of the free flow i e gaseous phase in the atmospheric compartment is performed by solving the navier stokes equation the heat and multicomponent transport in fluid s is described in each compartment whereas interphase mass transfer of gas components across the fluid phases and geochemical reactions are considered in the porous medium subdomain the reaction network includes both equilibrium and kinetically controlled geochemical reactions that are solved with phreeqcrm we first test the key features of the model comparing the outcomes of the proposed approach with other numerical codes i e min3p and dumux and with an analytical solution in a set of benchmark examples successively we perform conservative and reactive transport simulations in a coupled subsurface atmosphere domain to simulate evaporation migration of gas components e g o2 and co2 and geochemical reactions i e reactive mineral dissolution precipitation and aqueous speciation in a porous medium exposed to temperature gradients and wind in the adjacent atmospheric free flow 2 modeling approach a two domain approach is adopted for the description of coupled soil atmosphere systems using this approach in the porous medium subdomain we consider a non isothermal multiphase and multicomponent reactive transport model whereas in the free flow subdomain we employ a non isothermal single phase multicomponent transport formulation the governing equations in each subdomain as well as the coupling conditions are presented in the following sections 2 1 non isothermal flow and reactive transport in porous media the quantitative description of non isothermal multiphase and multicomponent reactive transport processes in porous media requires the formulation of the mass balance equations for the different phases in this study we consider two fluid phases i e liquid and gaseous in a porous medium and the mass balance equation for the fluid phase α l g can be written as 1 ε s α ρ α t ρ α u α r α where ε is the porosity and s α ρ α u α and r α are the saturation density darcy flux and the source sink term of the fluid phase α respectively the density of the liquid phase is assumed to be constant whereas the density of the gaseous phase is obtained using the ideal gas law as ρ g p g m r t with r being the universal gas constant m the mean molar mass of components in the gaseous phase and t the temperature the darcy flux in this formulation can be defined as 2 u α k i n t k r α μ α p α ρ α g where k int is the intrinsic permeability of the porous medium k r α is the saturation dependent relative permeability of phase α μ α and p α are the dynamic viscosity and the pressure of phase α respectively and g is the gravity vector we can consider eq 1 as the mass balance equation for one of the phases and specify the total mass balance equation by summing up eq 1 as 3 α l g ε s α ρ α t α l g ρ α u α α l g r α in the above formulation we can consider the saturation of the gaseous phase and the pressure of the liquid phase as the primary variables the pressure of the gaseous phase can then be determined by the capillary pressure relation defined as pc pg pl and the saturation of the liquid phase can be calculated using sl 1 sg as the sum of the phase saturations is equal to one this formulation is used for the full description of multiphase flow and can be simplified to one mass balance equation for the liquid phase i e richards equation by considering a passive gas phase pg 0 in order to close the system of equations the constitutive relations expressing the capillary pressure and relative permeability as a function of the fluid phase saturation are specified with the van genuchten mualem model mualem 1976 van genuchten 1980 4 s e l 1 α h c n m h c 0 1 h c 0 5 k r l s e l l 1 1 s e l 1 m m 2 6 k r g 1 s e l l 1 s e l 1 m 2 m where hc is the capillary pressure head computed as h c p c ρ l g s e l is the effective saturation of the liquid phase calculated as s e l s l s r 1 s r with sr being the residual saturation of the liquid phase and α n l and m 1 1 n are empirical parameters in addition to the phase balance equations we need to describe the reactive transport of components in the porous medium subdomain using the mass balance equations for components the components can be present in the liquid and the gaseous phase partition between the phases and participate in geochemical reactions the mass balance equation for the component k that is present in phase α l g can be written as 7 ε s α ρ α k t ρ α k u α d α e f f k ρ α k r α k where ρ α k d α e f f k and r α k are the mass concentration the effective diffusion coefficient and sinks sources of component k in the α phase respectively the diffusion of components in each fluid phase only occurs in the pore space occupied by the corresponding fluid phase and this needs to be accounted for by defining the effective diffusion coefficient for each phase as d α e f f k ε s α d α k t τ α in this formulation d α k t is the temperature dependent free diffusion coefficient of the component k in the α phase calculated as d α k t d α k t 273 15 2 campbell 1985 and τα is the tortuosity in the phase α which can be approximated as τ α ε 1 3 s α 7 3 millington 1959 the consumption and production of components due to chemical reactions e g dissolution of reactive minerals as well as the interphase mass transfer of components are included in the sink source term of eq 7 in this study we consider water oxygen and carbon dioxide as the three components that are present in the liquid and gaseous phases and can partition between them note that water is the main component of the liquid phase and thus the mass balance equation for this component in the liquid phase is already determined by the phase balance equation eq 1 however the mass balance equation for the water component in the gaseous phase i e water vapor and its exchange with the liquid phase i e phase change is provided by eq 7 we use a kinetic mass transfer expression to describe the partitioning of these components between the phases and to link their mass balance equations such expression for the water component can be written as trautz et al 2014 8 r g w r l k e q w ρ v s a t ρ g w ε s w where k eq w is the mass transfer coefficient and ρ v sat is the saturated vapor density that is a temperature dependent function determined by tetens equation as ρ v s a t p 0 10 7 5 t 273 15 t 35 85 r t m w with p 0 being 610 7 pa r the universal gas constant t the temperature in kelvin and mw the molecular weight of water monteith and unsworth 2014 the kinetic mass transfer expression for the component k o 2 co 2 can be formulated as solovský et al 2020 9 r l k r g k k e q k ρ g k h k ρ l k where hk is the dimensionless henry s law coefficient of the component k this coefficient is a function of temperature and can be calculated by van t hoff equation as h k h r e f e δ s o l h r 1 t 1 t r e f with tref href and δ sol h being the reference temperature the henry s constant at the reference temperature and the enthalpy of dissolution for the component k respectively the value of δ s o l h r for o2 and co2 is given as 1700 k and 2400 k respectively sander 2015 additionally we need to express the governing equations for the energy balance in the porous medium subdomain assuming a local thermal equilibrium between the porous medium liquid and gaseous phases the energy balance equation can be written as 10 ρ b c p e f f t t α l q ρ α c p α u α t λ e f f t δ h v a p r l where λ eff is the effective thermal conductivity δhvap is the enthalpy of vaporization and ρ b cp eff can be defined as 11 ρ b c p e f f 1 ε ρ s c p s α l q ε s α ρ α c p α with c p α c p s and ρ s being the heat capacity of the fluid phase α the heat capacity and density of the solid matrix respectively the model proposed by johansen 1977 was used to determine the effective thermal conductivity as λ eff λ dry ke λ sat λ dry with λ dry and λ sat being the effective thermal conductivities under dry and fully saturated conditions respectively and ke being a function of the liquid phase saturation given by côté and konrad 2005 as k e κ s l 1 κ 1 s l where κ is a dimensionless empirical fitting parameter 2 2 non isothermal flow and transport in free flow in the free flow subdomain we describe the non isothermal migration of a gaseous phase containing multiple components this is done by applying a non isothermal single phase multicomponent transport formulation the governing equations for the single phase flow i e gaseous phase in such system is determined by a combination of the momentum balance and total mass balance the momentum balance in the free flow can be described by the navier stokes equation as 12 ρ g u g t ρ g u g u g t τ p g ρ g g 0 where τ is the shear stress tensor defined as τ μ g u g u g t 2 3 μ g u g i with i being the identity matrix the total mass balance equation in the free flow can be written as 13 ρ g t ρ g u g q m where qm is the sink source term additionally we need to specify the component mass balance equation in the free flow subdomain this can be done by using the simplified form of eq 7 in which we consider α g the porosity ε and the fluid phase saturation sg are equal to one and assume the sink source term r g k to be zero similarly the heat balance equation in the free flow region can be expressed as 14 ρ g c p g t t ρ g c p g u g t λ g t 0 where λ g is the thermal conductivity of the gaseous phase 2 3 interface coupling conditions we adopt two different sets of equations in the porous medium and free flow subdomains and we therefore need to apply coupling conditions at the interface between these two subdomains the coupling conditions presented here are based on the general concept developed in previous works ahmadi et al 2021 davarzani et al 2014 mosthaf et al 2011 assuming that the flux of the liquid phase in the porous medium across the interface is zero the continuity of normal mass fluxes between the free flow ff and the porous medium pm is ensured by 15 ρ g u g n ff ρ g u g n pm with n being the outward pointing normal unit vector at the interface of each subdomain we also need to fulfill the momentum balance the normal part of the momentum balance is determined by applying the continuity of the normal stress as 16 τ p i n n ff p pm the tangential part of the momentum balance is set based on the beavers joseph saffman condition saffman 1971 as 17 u k α b j μ g τ n t ff 0 in which α bj is the dimensionless beavers joseph coefficient t is a tangential vector k kt t denotes the tangential component of the permeability tensor and τ the shear stress tensor additionally the continuity of the component concentrations and mass fluxes at the interface needs to be satisfied considering zero liquid phase flux at the interface such continuity equations can be written as 18 ρ g k ff ρ g k pm 19 ρ l k pm ρ g k ff h k 20 ρ g k u d α k t ρ g k n ff ρ g k u d α e f f k ρ g k n pm and the continuity of temperature and heat fluxes need to be applied as 21 t ff t pm 22 ρ g c p g u g t λ g t n ff α l q ρ α c p α u α t λ e f f t n pm 2 4 numerical implementation the equations governing the physical processes i e flow and component heat transport in the porous medium free flow domain with associated initial and boundary conditions are solved in comsol multiphysics for the solution of flow and transport processes we incorporate both a fully coupled and a simplified approach the latter can be used to simulate multicomponent reactive transport processes in partially saturated porous media without explicitly considering the free flow subdomain and assuming a passive gas phase in this approach the flow problem in the porous medium subdomain is simulated in the first step by solving the richards equation and then the obtained fluid phase saturations and the liquid phase velocity are used in the second step to simulate multicomponent transport processes in the partially saturated porous medium however for the simulation of the multiphysical processes i e flow and component heat transport either in the coupled porous medium free flow domain or in the two phase porous media where the gaseous phase movement cannot be neglected the entire set of equations describing the system of interest is solved in a fully coupled way the computational domains considered in this study include 1 d and 2 d geometries that were discretized using the finite element method the discretization of the 2 d domains was performed by a non uniform triangular mesh that was greatly refined near the boundaries as well as at the interface between the porous medium and the free flow subdomains supplementary material section s2 an implicit time marching scheme backward euler differentiation was used for the time discretization and a combination of a direct solver pardiso and a nonlinear newton solver was used to solve the equations of interest in the respective domains in order to describe the interactions between the physical processes and the geochemical reactions e g chemical speciation dissolution of reactive minerals we employed a sequential non iterative operator splitting approach in which the simulation of the physical and reactive processes is performed in two sequential steps as illustrated in fig 1 muniruzzaman and rolle 2019 2016 soulaine et al 2021 wissmeier and barry 2011 in this approach after computing the non isothermal multiphase flow and multicomponent transport either in a porous medium domain or in a coupled porous medium free flow system using comsol multiphysics within a time step δt the computed fluid phase saturations and component concentrations are passed to phreeqcrm for the calculation of reactive processes successively the updated values of the component concentrations are passed back to comsol multiphysics to perform the computation of the physical processes in the subsequent time step the time discretization and the time steps taken by the solvers in each software are internally adjusted but the coupling time step δt is chosen by the user and needs to be kept reasonably small in order to minimize operator splitting errors the coupling between the two codes is performed with livelink for matlab a module facilitating the communication of comsol multiphysics with other software 3 benchmark examples the capabilities and the key features of the proposed modeling approach are tested in four benchmark problems and the simulation outcomes are compared with the results of other numerical simulators i e min3p and dumux and an analytical solution in the first and second benchmark example we tested the capability of the proposed model to simulate dissolution of reactive minerals in a 1 d partially saturated porous medium domain and validated the simulation outcomes against the code min3p mayer et al 2015 in these examples we considered a porous medium domain without free flow and applied the simplified approach richards equation for the simulation of flow and multicomponent transport processes in the porous medium as detailed in section 2 4 these two benchmark examples allowed us to evaluate the capability of the proposed model to compute the distribution of phases resulting from the water infiltration into the variably saturated porous medium advective dispersive transport of components in the liquid phase diffusive transport of gas components in the gaseous phase interphase mass transfer of gas components aqueous speciation and mineral reactions in two phase systems the third benchmark example originally presented by udell 1985 as heat pipe problem assesses the capability of the proposed modeling approach to simulate simultaneous and coupled heat propagation evaporation condensation and compositional two phase flow in porous media using the full multiphase description in the fourth benchmark example we increased the complexity by considering a coupled porous medium free flow domain in order to simulate evaporation from a porous medium exposed to an adjacent wind flow with high temperature and low humidity we compared the outcomes of this benchmark example with another code dumux that is widely used for simulation of such dual continua problems koch et al 2021 this example allowed us to evaluate the correct implementation of the coupling concept that we applied to describe the exchange processes occurring at the interface between the porous medium and the free flow and to properly simulate heat transport fluid s flow and component transport in such coupled systems 3 1 reactive transport in a partially saturated porous medium benchmark 1 and 2 benchmark examples 1 and 2 focus on reactive transport in a 1 d partially saturated porous medium domain with length of 5 m the flow problem in these examples involves water infiltration into a porous medium domain simulated by solving richards equation under steady state conditions a flux boundary conditions u l 9 51 10 9 m s is applied at the top boundary and a constant head value h 2 5 m was prescribed at the bottom boundary the spatial distribution of the fluid phases was determined after such simulation and used for the calculation of the reactive transport processes considered for the two examples the first example originally presented by mayer et al 2015 considers oxidation of pyrite due to the oxygen penetration from the top boundary the oxidative dissolution of pyrite leads to the generation of acidity and release of components such as iron and sulfate in the liquid phase e g andersen et al 2001 battistel et al 2019 the second example uses the same flow conditions but adds complexity in the geochemistry by considering also the presence of calcite in the porous medium the dissolution of this reactive carbonate mineral triggered by the protons released during pyrite oxidation results in ph buffering and in the production of co2 in the system the total simulation time for both examples is 10 years the model input parameters for the simulation of flow and reactive transport in both examples are given in table 1 with detailed information about the composition of solutions applied as the initial and boundary conditions as well as the rate expressions in the supplementary material section s1 and s3 more details about the thermodynamic database used in the simulations can be found in mayer et al 2015 fig 2 presents the simulation outcomes for the first benchmark example and provides a comparison with the simulation results using the code min3p the simulated liquid phase saturation shows a transition from partially saturated to fully water saturated conditions in the middle of the domain fig 2a this creates a two phase system in the upper part of the domain in which oxygen propagates fig 2b causing the oxidative dissolution of pyrite the initially uniform pyrite content φ m is consumed upon the reaction with oxygen penetrating from the top boundary fig 2d the reaction induces a decrease of ph with depth and causes an abrupt change in the pe front between the upper part of the domain where oxidizing conditions prevail and the anoxic part at the bottom fig 2c the very good agreement between the outcomes of the two models validates the capability of the proposed code to compute the spatial distribution of phases in variably saturated porous media component transport in liquid gaseous phases and geochemical reactions the second benchmark example is designed based on the first example but considers a more complex geochemical system including both pyrite and calcite as reactive minerals the simulation results are shown in fig 3 and indicate the oxygen depletion and pyrite dissolution in the upper part of the domain similar to the first example however in this case the dissolution of calcite buffers the acidity and the ph stabilizes to a values 6 5 below the water table fig 3c in addition to the ph buffering the calcite dissolution leads to the production of dissolved carbonate species and also of co2 through the equilibrium reactions of the aqueous carbonic acid system the generated co2 transfers to the gaseous phase in the partially saturated region where it migrates upward via diffusive transport in the gaseous phase and downward through advective dispersive transport in the liquid phase fig 3b concerning the solid phases the content of both reactive minerals diminishes in the upper part of the domain in particular in this zone calcite is completely consumed and a sharp dissolution front for this reactive mineral occurs at a depth of approximately two meters the comparison between the simulation results of the proposed model and min3p demonstrates a good agreement for most components with some discrepancies observed for the computed co2 in the lower part of the domain similar discrepancies have been reported in other benchmark studies and can be attributed to internal differences in the geochemical calculation of the carbonate system in min3p and phreeqc e g mayer et al 2015 the outcomes confirm the ability of the model to simulate complex geochemical systems involving mineral dissolution ph buffering and production consumption of different gas components in partially saturated porous media 3 2 non isothermal multiphase flow and multicomponent transport in a porous medium benchmark 3 the third benchmark example i e heat pipe problem allows us to demonstrate the ability of the proposed model to simulate non isothermal two phase i e liquid and gaseous and two component i e air and water vapor flow and transport in a 2 d porous medium the domain is partially saturated sl 0 7 and the initial temperature is 70 c the liquid phase invades the porous medium from the left boundary sl 1 and t 68 4 c and advances toward the right hand side of the domain where a heat source qt 100 w increases the temperature up to the boiling point i e hot end of the domain this leads to evaporation heat propagation through conduction and convection and appearance of a gaseous phase on the right side of the domain due to the pressure gradients the gaseous phase progressively flows toward the left i e cool end of the domain where it condenses and moves back toward the hot end of the domain on the right this circulation flow induced by the opposite migration of the liquid and gaseous phases and evaporation condensation at the two opposite sides of the domain continues until the system reaches a steady state condition udell 1985 derived an analytical solution for the spatial distribution of temperature fluid phase pressures and saturations and mole fraction of components at steady state note that such solution is derived considering the leverett function for the capillary pressure saturation relation leverett 1941 and using a simplified function for the relative permeability saturation relation fatt and klikoff 1959 also the effect of temperature on the diffusion coefficient of components in the fluid phases is not considered in this example and a constant tortuosity value of 0 5 is used in the simulation the model input parameters used in the simulation of the heat pipe problem are provided in table 2 fig 4 illustrates the two dimensional system and shows the comparison between the numerical results and the analytical solution the good agreement between the numerical and analytical results shows that the model can accurately describe the heat pipe problem and can capture the complex interplay of non isothermal multiphase and multicomponent transport processes in the porous medium in particular the results demonstrate that the formation of three regions in the porous medium i e fully water saturated two phase and fully dry and the distribution of temperature in such regions are captured well by the numerical model the capability of the proposed model to simulate complex processes such as phase change heat transport phase displacement and component transport within and across phases during fluid s flow in porous media is successfully benchmarked 3 3 evaporation and non isothermal compositional two phase flow in a porous medium exposed to wind flow benchmark 4 in the fourth benchmark example we consider a coupled porous medium free flow domain and tested the performance of the proposed model to simulate exchange processes at the interface between the porous medium and free flow and to show the impact of such interchange dynamics on the fluids phase distribution and the heat propagation in the porous medium the model input parameters used for the simulation of this benchmark example is given in the supplementary material table s2 the porous medium is initially saturated sl 0 95 and the temperature is 293 15 k in the entire domain the porous medium is exposed to the free flow with an average velocity of 20 cm s temperature of 300 15 k and relative humidity of 0 1 fig 5 presents the setup considered for this benchmark example and provides a comparison between the simulation results of the proposed code and dumux the concentration and temperature gradients at the interface promote evaporative fluxes from the porous medium surface resulting in the appearance of a drying front that progressively penetrates into the porous medium as shown in fig 5b the imposed temperature gradients leads to a temperature rise at vicinity of the interface but this temperature increase is balanced by the cooling effect from water evaporation and causes the temperature to progressively decrease inside the porous medium the comparison between the outcomes of the two simulators shows a very good agreement which confirms that the proposed model is able to properly describe the non linear and interactive processes in coupled porous medium free flow systems thus the model can be extended to explore the exchange of multiple components across such coupled systems and to understand the effects of the interface processes on the dynamics of reactive transport phenomena in the porous medium 4 modeling soil atmosphere physico chemical interactions in this section we introduce two examples illustrating the capability of the proposed model to simulate non isothermal multiphase and multicomponent transport processes in porous media incorporating dynamic forcing from the atmospheric compartment first we investigate the conservative transport of water vapor oxygen and carbon dioxide in a porous medium domain exposed to an overlying free flow domain with wind high temperature and low humidity successively we extend the porous medium free flow scenario by considering geochemical reactions in the porous medium as well as different wind velocities in the free flow domain 4 1 exchange of gas components at the soil atmosphere interface in the first example we demonstrate the capability of the model to simulate evaporation from the soil surface i e exchange of water component at the soil atmosphere interface considering dynamic forcing in the atmospheric compartment and to describe the spatiotemporal distribution of phases and gas components in porous media during such interface driven processes the model input parameters used for this simulation are given in table 3 and more details about the initial and boundary conditions are given in the supplementary material section s2 and s3 the setup consists of a coupled porous medium free flow domain with gas flow occurring in a free flow domain in contact with a saturated porous medium where no oxygen and a 0 085 mol m3 of co2 were initially present fig 6 a at the beginning of the simulation the pore space in the porous medium is entirely filled with water and the temperature in the domain is 293 15 k a horizontal gas flow with an average velocity of 2 cm s and temperature of 300 15 k containing a mixture of water vapor oxygen and carbon dioxide enters the free flow subdomain this creates strong chemical and thermal gradients at the interface between the free flow and the porous medium the simulated spatial distribution of temperature and vapor concentration shown in fig 6b and c indicates the formation of boundary layers at the interface where such gradients lead to the exchange of mass and energy between the two subdomains this is reflected in the migration of water vapor toward the free flow fig 6c and in the heat propagation in the opposite direction fig 6b such interchange processes lead to phase change i e evaporation in the porous medium and to the formation of a drying front that penetrates into the porous medium as a consequence a variably saturated region is formed and a non linear spatiotemporal distribution of the fluid phases under non isothermal conditions occurs in the porous medium subdomain fig 6d in addition to the dynamics of fluid phases the propagation of gas components i e o2 and co2 is greatly impacted by the interface processes as evaporation proceeds and the gaseous phase appears in the porous medium the oxygen in the gaseous phase diffuses into the initially anoxic porous medium and dissolves in the liquid phase the downward migration of oxygen occurs in both phases via diffusion which is significantly faster in the gaseous phase i e 104 times higher diffusion coefficient than in the liquid phase such diffusive behavior can be seen in the spatial profile of oxygen at t 3 h fig 6e as the partially saturated region develops and the gaseous phase saturation increases due to evaporation e g t 30 h the interphase mass transfer of gas components between the two phases becomes stronger this leads to an abrupt increase in the o2 concentration in the partially saturated region and considerably accelerates the slow o2 diffusive transport to a fast propagation pattern at later times t 30 and 70 h in fig 6d and e a similar transport behavior but in the opposite direction governs the co2 transport co2 was initially present in the liquid phase at higher concentration than in the free flow domain therefore it migrates upward via diffusion and transfers to the gaseous phase as the drying front advances into the porous medium fig 6f the results of this conservative transport scenario show that the dynamic forcing in the atmospheric compartment e g wind temperature and humidity induces strong gradients at the soil atmosphere interface which can significantly influence the distribution and migration of key gas components e g o2 and co2 in the subsurface compartment 4 2 coupling of dynamic atmospheric forcing and geochemical reactions the migration of gas components controls a variety of biogeochemical reactions in the subsurface and therefore it is important to investigate the impact of physical atmospheric forcing on subsurface reactive processes to this end we extend the setup used in the first example by including geochemical reactions similar to the benchmark problem two presented in section 3 the reaction network comprises two reactive minerals i e pyrite and calcite in the porous medium as well as equilibrium speciation reactions in the pore water we also used the same rate expressions chemical composition and thermodynamic database as in the benchmark two different free flow velocities 2 and 20 cm s were considered in order to evaluate the impact of different external forcing conditions on the extent of mineral dissolution and the spatiotemporal distribution of the fluid phases and components in the porous medium the model input parameters for this example are provided in table 3 and the details of the chemical compositions applied as the initial and boundary conditions as well as the reactions rates are given in the supplementary material section s1 and s3 the simulation outcomes including the 2 d maps of the temperature distribution the water vapor concentration and the liquid phase saturation for the two free flow velocities at t 42 h are shown in fig 7 the comparison between the spatial distributions of the temperature and the water vapor concentration shows noticeable differences particularly close to the interface where strong gradients in the boundary layer control the dynamics of evaporation in the porous medium at the higher free flow velocity a thinner boundary layer and stronger concentration temperature gradients are formed resulting in a higher water vapor flux toward the free flow and therefore more evaporation in the porous medium this is reflected in the spatial distribution of the fluid phases showing a drier surface and a further penetration of the drying front as the free flow velocity increases fig 7c and f such differences are of great importance for the dynamics of the pyrite and calcite dissolution that highly rely on the distribution of the fluid phases and the availability of oxygen as the key reactant this can be investigated by interpreting the spatial profiles of the components in the porous medium calculated for the two free flow velocities with the proposed coupled comsol multiphysics phreeqcrm modeling approach fig 8 the appearance and the progressive invasion of the gaseous phase due to evaporation facilitates the diffusive transport of oxygen i e high diffusivity of oxygen in the gaseous phase enhances the interphase mass transfer processes and thus provides high oxygen concentrations in the liquid phase above the drying front i e the partially saturated zone as shown in fig 8a the available oxygen in the liquid phase is largely consumed during pyrite oxidation but it is continuously replenished by the progressively advancing gaseous phase via the interphase mass transfer mechanism meaning that the extent of the pyrite oxidation is not limited by the availability of oxygen in the partially saturated zone as explained above the oxidative dissolution of pyrite greatly impacts the dissolution of other minerals and the pore water quality for instance the extent of calcite dissolution increases due to the acidity released from pyrite oxidation triggered by the high oxygen availability resulting from the atmospheric forcing the outcome of these processes is the excessive production of carbon dioxide sulfate and calcium as the products of the mineral dissolution and speciation reactions and their migration in the porous medium fig 8b d all these components show an accumulation in the portion of the porous medium where the reaction occurs with milder spatial gradients towards the background concentration at the bottom and steeper gradients toward the reacted zone at the top of the domain these results highlight the feedback loop between the dynamics of the physical and reactive processes in the porous medium driven by the free flow porous medium interactions the impact of such interactions becomes more evident by comparing the spatial distribution of the reaction products for the two free flow velocities at the higher free flow velocity the gaseous phase invades a larger fraction of the pore space and progresses deeper within the porous medium fig 7c and f leading to higher availability and faster propagation of oxygen in the porous medium and thus higher concentration of the reaction products including carbonates sulfate and calcium fig 8 besides the concentration the spatial distribution of the reaction products is also influenced by the different free flow velocities as a deeper penetration depth can be observed for these components with increasing free flow velocity this is particularly pronounced for the co2 front because the mechanisms governing the co2 transport entail a complex interplay between the advancement of the drying front the diffusion in the individual liquid and gaseous phases and the interphase mass transfer the produced co2 due to the calcite dissolution and speciation reactions can propagate toward the bottom part of the domain by diffusive transport in the liquid phase and can also exchange to the gaseous phase and migrate toward the subsurface atmosphere interface this upward migration is driven by the low co2 concentrations in the free flow compartment i e atmospheric composition that leads to strong co2 concentration gradients in the upper part of the porous medium and promotes the diffusive migration of co2 from the porous medium toward the free flow the co2 concentration gradients and thus the exchange of this component between the two compartments become stronger as the free flow velocity increases and more gaseous phase becomes available in the porous medium due to the enhanced drying fig 8b the outcomes of these reactive transport simulations allowed us to quantitatively assess the influence of soil atmosphere interactions on the distribution of fluid phases transport of key components and dynamics of important geochemical reactions in subsurface porous media 5 conclusions in this study we have investigated the dynamics of subsurface atmosphere interactions for conservative and reactive transport scenarios we have presented a modeling approach capable of simulating non isothermal multiphase and multicomponent reactive transport processes in porous media and explicitly reflecting the influence of soil atmosphere exchange on the dynamics of flow transport and geochemical reactions in porous media the modeling framework is developed based on the two domain approach in which the system is conceptualized as a coupled domain composed of a porous medium and a free flow subdomain that are separated by a common interface the simulation of the governing physical i e fluid s flow component transport and heat transport in each subdomain and reactive processes i e geochemical reactions in the porous medium subdomain is performed by coupling the general partial differential equation solver comsol multiphysics and the geochemical code phreeqc the different features of the proposed code including the ability to accurately compute reactive transport processes in partially saturated porous media to simulate coupled heat transport evaporation condensation phase displacement and component transport within and across phases in porous media and to simulate evaporative drying and heat propagation in a coupled porous medium free flow system were tested in four well defined benchmark problems successively we presented two scenarios to simulate evaporation phase displacement and conservative as well as reactive transport of multiple aqueous and gaseous components in a porous medium that is exposed to an overlying free flow for the reactive transport we focused on mineral dissolution i e pyrite and calcite and speciation reactions in the porous medium to evaluate the impact of soil atmosphere interactions on the dynamics of geochemical reactions and pore water quality the simulation results illuminate the mechanistic interactions and the feedback loops between physical processes and geochemical reactions in a coupled soil atmosphere system key physical processes include coupled flow mass transport and heat transport in both compartments and at their interface for the conservative scenario the simulation outcomes show that the soil atmosphere interactions significantly influence the exchange of gas components by inducing strong gradients at the porous medium free flow interface and affect the transport behavior of the gas components in the porous medium by progressively changing the spatiotemporal distribution of the fluid phases and enhancing interphase mass transfer processes the results of the reactive scenario demonstrate that the enhanced supply of oxygen during the phase displacement increases pyrite and calcite dissolution and significantly changes the pore water chemistry we also showed that a higher free flow velocity in the atmospheric compartment increases the extent of mineral dissolution and changes the propagation of the reaction products particularly the gas component co2 in the porous medium by influencing the dynamics of the fluid phase distribution in the pore space the proposed modeling approach can be extended to simulate more complex conditions typical of natural systems that can be impacted by atmosphere subsurface exchange of gas components such developments can focus on heterogenous porous media different levels of water saturation as well as different geochemical reactions including salt precipitation induced by evaporation jambhekar et al 2015 dissolution precipitation of carbonates and iron oxides deng et al 2022 haberer et al 2015 oxidation of iron sulfide minerals and release of geogenic contaminants asta et al 2010 battistel et al 2021 fakhreddine et al 2016 furthermore the model could be applied to explore the coupling between physical and microbial processes in free flow porous medium systems this can be of great interest to study both natural environments such as arid and semi arid regions where soil drying can strongly influence microbial activity or permafrost regions where other phase change processes i e freezing and thawing can be a driving force for microbially mediated release of greenhouse gases brangarí et al 2020 kim and or 2019 and engineering applications such as landfills and contaminated sites in these systems biogeochemical processes such as respiration and methane oxidation scheutz et al 2009 xu et al 2014 as well as mixing controlled aerobic degradation of organic compounds rolle and le borgne 2019 can be greatly impacted by atmospheric forcing finally besides the application to the subsurface and atmosphere compartments the proposed modeling approach could be extended to describe the interactions between physical and geochemical processes in other coupled environmental systems by taking advantage of the dual continua approach implemented in the code carrillo et al 2020 stolze and rolle 2022 zhang et al 2022 an important example is the exchange of nutrients and gas components e g o2 and n2o between shallow groundwater and surface water which is critical for biogeochemical cycles and greenhouse gas emissions bu et al 2021 reeder et al 2018 wallace et al 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the independent research fund denmark project giga gas interchange between groundwater and air grant dff 7017 00130 m r also acknowledges the support of the project biodegrates grant 0136 00205b and k h acknowledges the support of the german research foundation dfg german research foundation within the collaborative research center sfb 1313 project no 327154368 
92,geologic disposal safety assessment framework is a state of the art simulation software toolkit for probabilistic post closure performance assessment of systems for deep geologic disposal of nuclear waste developed by the united states department of energy this paper presents a generic reference case and shows how it is being used to develop and demonstrate performance assessment methods within the geologic disposal safety assessment framework that mitigate some of the challenges posed by high uncertainty and limited computational resources variance based global sensitivity analysis is applied to assess the effects of spatial heterogeneity using graph based summary measures for scalar and time varying quantities of interest behavior of the system with respect to spatial heterogeneity is further investigated using ratios of water fluxes this analysis shows that spatial heterogeneity is a dominant uncertainty in predictions of repository performance which can be identified in global sensitivity analysis using proxy variables derived from graph descriptions of discrete fracture networks new quantities of interest defined using water fluxes proved useful for better understanding overall system behavior keywords nuclear waste spent nuclear fuel performance assessment uncertainty data availability data will be made available on request nomenclature dfn discrete fracture network doe u s department of energy drz disturbed rock zone ecpm equivalent continuous porous medium gdsa geologic disposal safety assessment mdrt median residence time mthm metric tons of heavy metal mtt mean travel time pa performance assessment pce polynomial chaos expansion qoi quantity of interest sfwst spent fuel and waste science technology stt shortest travel time ua uncertainty analysis uq uncertainty quantification wp waste package 1 introduction the united states department of energy doe is developing a state of the art simulation software toolkit geologic disposal safety assessment gdsa framework for probabilistic post closure performance assessment pa of systems for deep geologic disposal of nuclear waste post closure refers to the period after repository operations have ceased and performance relies on passive features of the site and engineered systems as well as institution controls to prevent intrusion international atomic energy agency 2011 the post closure period can be on the order of hundreds of thousands to millions of years due to the long half lives of some of the hazardous radionuclides present in nuclear waste this requires that pa simulations predict system behavior over much longer time scales than can be observed the gdsa framework couples increasingly higher fidelity models of subsystem processes into total system pa simulations these models can include multiphase flow reactive transport heat conduction and convection simple geomechanics and radionuclide decay and ingrowth because of the inherent complexity and time scale predictions from pa models are necessarily uncertain and the characterization quantification and analysis of uncertainty are all integral components of a full pa the paradigm for treatment of uncertainty in a typical pa can be described in five general steps 1 characterization of the uncertainty space using probability distributions to describe uncertain model parameters 2 sampling of the uncertainty space 3 propagation of the samples through the system model 4 quantification of uncertainty for the quantity of interest qoi and 5 sensitivity analysis the first four steps could be described as uncertainty analysis ua and the fifth step sensitivity analysis is fundamental to support correct interpretation of ua results in the context of pa saltelli et al 2019 helton et al 2012 due to domain size and simulation time scale the pa simulations in the gdsa framework require high performance computers to run this limits the number of samples from the uncertainty space that can be propagated through the system model which can limit the effectiveness of some tools for ua and sensitivity analysis swiler et al 2019 2020 2021 this paper presents a generic reference case referred to as the crystalline reference case which is not representative of a specific site the case is meant to be illustrative for the purpose of developing and demonstrating pa methods within the gdsa framework which mitigate some of the challenges posed by high uncertainty and limited computational resources limitations on the number of pa simulations highlight the need to consider methods that improve sampling efficiency and enable exploration of the full uncertainty space even when samples of the pa simulations are relatively sparse this paper focuses specifically on new qois and sensitivity analysis using variance based sensitivity indices the crystalline reference case is a generic repository modeled in a crystalline rock formation this type of repository would be placed in highly impermeable fractured rock where flow is assumed to be dominated by flow through the fractures the repository model includes nuclear waste packages which breach based on an uncertain normalized general corrosion rate mariner et al 2016 transport of 129i is modeled from the waste packages through the repository and surrounding host rock based on additional uncertain permeability porosity and radionuclide release parameters this radionuclide is a fission product in spent nuclear fuel which is of particular interest in repository performance assessment due to its abundance in spent nuclear fuel and its long half life concentrations of 129i are monitored at pre specified points in the model domain called observation points and the peak concentration is tracked throughout the aquifer this paper also discusses other potential qois that may be more illustrative for understanding specific repository characteristics that affect performance or for understanding how the model behaves however because nuclear waste repositories are meant to prevent hazardous radionuclides from reaching the biosphere 129i concentrations are an important qoi for pa the crystalline reference case has an additional level of complexity that presents challenges for sensitivity analysis the spatial heterogeneity of potential fracture flow paths through the host rock is a source of uncertainty in pa for a nuclear waste repository located in crystalline rock conceptually a long lived radionuclide released from a waste package will initially migrate through the buffer material surrounding a waste package and into the surrounding damaged rock zone drz from there it will migrate along the drz until it enters a fracture that takes it farther into the host rock and it may eventually reach the biosphere via connected fractures and connected fracture zones the spatial heterogeneity in the fracture network contributes to uncertainty in the qois used to measure system performance with respect to the uncertainty analysis approach inclusion of uncertainty in the fracture network can be done using a dual loop sampling structure which is important for understanding the effects of different types of uncertainty but further complicates the challenges caused by limitations on the number of simulations that can be performed swiler et al 2019 this paper describes the crystalline reference case discusses the graph metrics and qois that were developed to better understand the effects of spatial heterogeneity and presents sensitivity analyses that incorporate these developments 2 model description the crystalline reference case simulates a mined repository located at 600 m below land surface in a sparsely fractured crystalline host rock such as granite or metagranite in a stable cratonic terrain it is assumed that a commercial repository would hold up to 70 000 metric tons of heavy metal mthm of commercial used nuclear fuel which is the maximum allowed by the nuclear waste policy act of 1982 this inventory could be accommodated in 168 disposal drifts each 805 m in length arranged in facing pairs on either side of a central access hallway wang et al 2015 this layout has drift centers separated by 20 m and waste packages emplaced lengthwise within the drifts with a spacing of 10 m center to center 5 m spacing end to end repository access would be via vertical shafts and or a ramp freeze et al 2013b mariner et al 2011 wang et al 2014 this reference case was developed to be a realistic example of a crystalline repository but does not correspond to any specific or proposed site and models a smaller inventory than the statutory limit of 70 000 mthm repository performance can be affected by the structural strength of the host rock depth of burial host rock permeability the geometry of fractures in the subsurface and the chemical environment which affects corrosion and radionuclide solubility and sorption engineered barriers for this repository include the characteristics of the constructed repository the waste form waste packages and bentonite buffer freeze et al 2013b mariner et al 2011 2016 the model domain for the crystalline reference case stein et al 2017 is 3015 m in length 2025 m in width and 1260 m in height overlying the host rock is a 15 m thick overburden of glacial sediments the repository is located at a depth of 585 m forty two disposal drifts contain 80 waste packages each 3360 waste packages in total the waste packages have a capacity of 12 pressurized water reactor fuel assemblies rechard and voegele 2014 each waste package has a 5 225 mthm inventory so the total inventory for the crystalline reference case is 17 556 mthm freeze et al 2013a drifts are backfilled with bentonite buffer and are surrounded by a 1 67 m thick drz the domain is modeled with an unstructured grid with finer discretization in the repository grid cells which consist of nodes edges and boundary faces are as small as 1 67 m on their shortest edge within the repository and 15 m on their shortest edge elsewhere in the model domain the model domain contains 4 848 260 cells of these approximately 2 5 million are the smaller cells in and around the repository that allow representation of individual waste packages with surrounding buffer materials additional information on the grid and dimensions may be found in stein et al 2017 available for download at https pa sandia gov 2 1 discrete fracture networks as described by mariner et al 2016 the representation of fractured crystalline rock in the crystalline reference case is loosely based on the well characterized sparsely fractured metagranite at forsmark sweden joyce et al 2014 at forsmark large scale mappable features of concentrated brittle and or ductile deformation termed deformation zones bound volumes of relatively undeformed rock each volume of relatively undeformed rock termed a fracture domain is sparsely fractured and the fractures within each can be described in terms of a number of fracture sets distinguished from each other on the basis of fracture orientation each fracture set within a particular fracture domain and depth zone is described using a 3 dimensional fisher distribution to describe the orientation of fracture poles in space a truncated power law distribution for fracture radii and a fracture density p 32 which is defined as the area of fractures per volume of rock m 2 m 3 hedin 2008 for each depth zone within a fracture domain a relationship is given between fracture radius and fracture transmissivity fig 1 shows the fracture domain for the crystalline reference case the vertical and offset planes in the figure define deterministic large mappable features such as faults the discrete fracture networks dfns are generated stochastically in this domain using probabilistic characteristics these characteristics are defined based on three fracture sets from forsmark and three zone depths resulting in nine total probabilistic fracture set descriptions in the domain the probabilistic characteristics for each set include fracture density which decreases over the depth zones so the deepest regions of the domain have the lowest fracture density radius orientation and centroid location the stochastic fracture set characteristics and deterministic features employed in this study provided sufficient fracture connectedness such that each dfn realization resulted in direct fracture pathways from the repository to the top boundary simulating a repository for heat generating nuclear waste in sparsely fractured crystalline rock requires methods to simulate coupled heat and fluid flow and reactive radionuclide transport in both porous media e g engineered backfill sedimentary aquifer and the sparsely fractured otherwise impermeable rock stein et al 2017 the dfn must be coupled to the continuum to capture the effects of heat conduction through the rock for the purposes of probabilistic performance assessment where thousands of simulations are necessary a computationally efficient representation of the dfn is needed to this end the dfn is upscaled to an equivalent continuous porous medium ecpm using the procedure detailed in stein et al 2017 in each grid cell anisotropic permeability kinematic porosity and tortuosity are calculated using the outputs of fracture permeabilities apertures orientation normal vectors radii and location from the dfn model the benefits of this approach include the ability to model heat conduction computational efficiency for large domains stein et al 2017 reports on average 50x fewer degrees of freedom for ecpm vs dfn meshes and 6x speedup in simulation times and trivial coupling to porous medium materials stein et al 2017 sweeney et al 2020 hadgu et al 2017 other methods of upscaling a sparse fracture network to a heterogeneous ecpm include those described by jackson et al 2000 svensson 2001 and sweeney et al 2020 upscaling assumptions can lead to differences in behavior between the dfn and ecpm representations azizmohammadi and matthäi 2017 bisdom et al 2017 sweeney et al 2020 hadgu et al 2017 given the dfn properties and upscaling discretization used here the ecpm representations have slightly lower bulk permeabilities calculated for a 1 km3 domain than the original dfns and tracer breakthrough times are delayed stein et al 2017 this is likely due to the increased path length in the ecpm representations parashar and reeves 2011 sweeney et al 2020 a correction following sweeney et al 2020 will be added in future studies all dfns are generated according to the same distributions for fracture radii orientations and centroid locations table 1 however despite this they are still significantly different from each other this heterogeneity between dfns is consistent with the generic nature of the reference case and our inability to completely describe the fracture network at a hypothetical fixed site as can be seen in fig 3 which shows a cutaway of the streamwise velocity field flow throughout the domain is predominantly along the fractures in the dfns this effect on the flow makes the spatial heterogeneity a significant source of uncertainty affecting the qois as discussed in the sensitivity analysis section 4 2 2 waste package general corrosion model the treatment of uncertainty is discussed in section 3 1 but in this section we highlight the uncertainty characterization for the waste package general corrosion model which is handled a bit differently than the other parameter uncertainties most of the parameters permeabilities porosities sorption parameters are treated as epistemic uncertainties arising from lack of knowledge about the particular value to use this is in contrast to aleatory uncertainties which represent inherent random variability the waste package general corrosion model is unique in the context of uncertainty and sensitivity analysis because both aleatory and epistemic uncertainty are characterized and propagated through the model the waste package general corrosion model implemented in pflotran mariner et al 2016 calculates normalized thickness of the waste package wall at each time step as a function of a base waste package general corrosion rate a canister material constant and temperature waste package breach occurs when the wall thickness reaches zero the initial wall thickness is normalized to 1 and is reduced at each time step as a function of the effective waste package general corrosion rate r e f f 1 r e f f r e c 1 333 15 1 t where r yr 1 is the general corrosion rate at 60 c 333 15 kelvin t kelvin is the local temperature and c is the canister material constant this general corrosion rate model and its associated uncertainty are conceptual because additional specificity is not necessary for a generic reference case the normalized general corrosion rate is unknown an epistemic uncertainty but is also assumed to vary spatially within the repository so it is sampled differently than other uncertain parameters the spatial variability in r is assumed to follow a lognormal distribution which is itself uncertain a lognormal distribution is used because the corrosion rate may vary multiple orders of magnitude it was expert defined to be consistent with the original generic reference case conception weck et al 2014 in a performance assessment however the distribution would need to be refined to account for the materials and waste form for that specific case the mean and standard deviation of the lognormal distribution are assigned uncertainty distributions and are sampled once for each simulation to define the lognormal distribution on r for all waste packages in that simulation this distribution is then sampled independently for each waste package to define its general corrosion rate which is applied uniformly to the package surface sampling of the distribution parameters implements the epistemic uncertainty in the general corrosion rate sampling each waste package independently from that distribution implements spatial variability throughout the repository sensitivity analysis can detect the effects of uncertainty in the distribution parameters on qois however the spatial variability is not captured or described parametrically so it is more difficult to understand how it affects qois 2 3 software the repository for the crystalline reference case is modeled using pflotran in the gdsa framework pflotran is an open source state of the art massively parallel subsurface flow and reactive transport simulator hammond et al 2014 2008 lichtner et al 2015 written in object oriented fortran 2003 pflotran is a porous medium continuum code that can model multiphase flow reactive transport heat conduction and convection simple geomechanics and radionuclide decay and ingrowth pflotran development for gdsa framework is described by mariner et al 2015 2017 2016 and sevougian et al 2018 installation instructions are available at https www pflotran org fractured crystalline rock is modeled using stochastic discrete fracture networks which are two dimensional planes distributed in the three dimensional model domain the fracture networks are generated using dfnworks hyman et al 2015 and mapped to the equivalent continuous porous medium domain using mapdfn py a code that approximates hydraulic fracture properties by calculating and assigning permeability and porosity on a cell by cell basis stein et al 2017 the implementation of uncertainty in the crystalline reference case model is handled using dakota both to sample uncertain parameters for uncertainty propagation and to perform post simulation sensitivity analyses dakota is an open source toolkit of algorithms that contains both state of the art research and robust usable software for optimization and uncertainty quantification uq adams et al 2020 the algorithms allow a user to explore a computational simulation identify important parameters the effects of uncertainties on the system and optimize designs dakota contains the uncertainty and sensitivity analysis methods commonly used in assessment of computational models including sampling methods uncertainty propagation simple rank and partial correlation analysis surrogate models which can be constructed from a limited number of sample runs and then queried extensively for visualization sensitivity analysis or uq variance based decomposition methods and other approaches such as one at a time methods dakota also contains methods to perform nested sampling involving separate sample loops over epistemic and aleatory uncertain parameters this capability was used to sample the discrete fracture networks and individual waste package general corrosion rates in a separate loop from the epistemic uncertainties finally dakota has capabilities to submit runs to high performance computers and manage concurrent evaluations of a computational model where each model evaluation may be run in parallel on multiple cores the dakota software is publicly available at https dakota sandia gov 3 analysis methods the crystalline reference case was studied using an uncertainty analysis framework the focus however was on methods to better characterize the effects of spatial heterogeneity which were tested with global sensitivity analysis 3 1 analysis structure the uncertainty analysis was structured to separate spatial heterogeneity from parametric uncertainty to investigate the relative contributions to total uncertainty from each type this separation is accomplished with a dual loop sampling structure as shown in fig 2 the loops are nested without repeated sampling so that there are 40 unique samples of the variables in the parameter uncertainty loop for each realization of the spatial uncertainty loop there are 25 dfns in the spatial uncertainty loop thus a total of 1000 pflotran simulations were run the intention behind sampling the parameter loop 1000 times instead of repeating the same 40 samples for each spatial realization is to obtain better coverage of the epistemic space with a repeated sampling scheme there would be a total of 40 unique values for each epistemic uncertain variable over all 1000 simulations with the sampling scheme applied for this analysis there are 1000 unique values for each epistemic uncertain variable previous analyses have been performed with repeated samples swiler et al 2020 the sample sizes were chosen based on computational limitations with the epistemic loop containing more samples because it contains more independent uncertainties the uncertain inputs to the analysis are described in table 2 uncertain inputs in the parameter loop were sampled using latin hypercube sampling the sampled values for meanwprate and stdwprate from the parameter loop define a distribution on general corrosion rate for the waste packages and samples from this distribution were assigned to specific waste packages in the spatial sampling loop to perform the analysis samples were propagated through the deterministic repository model in pflotran which was executed to predict qois up to 106 years post closure of the repository simulations took around 30 min on average to complete on 512 processors 3 2 graph metrics the spatial heterogeneity included in this ua presents a particular challenge because it is not described parametrically like the other uncertainties it is however clearly significant the crystalline rock has low permeability so connected fractures are the dominant pathways for radionuclide transport from the repository to the aquifer in previous analysis of the reference case the same epistemic samples were repeated for each dfn yet the qois still differed by multiple orders of magnitude across dfns indicating a clearly important effect swiler et al 2020 summary characteristics were calculated to describe features of the dfns in an effort to quantify at least some of their effects parametrically we refer to these characteristics as graph metrics because of the manner in which they are calculated from graphs and because they attempt to measure key characteristics of the dfns graphs are a powerful tool for representing dfns because they are able to capture the inherent network topology of the dfn without the high computational cost of meshing thousands of fractures ranging in size from millimeter to kilometer the difference in computational cost is striking derived quantities from a graph representation of a dfn can be computed in a matter of seconds or minutes while a fully resolved 3d dfn flow and transport simulation would require several hours to run graphs have been used with some success as reduced order models with corrections for predicting flow and transport through the dfn and as surrogates in uq propagation studies in this context viswanathan et al 2018 srinivasan et al 2018 for this study however graphs were not applied within the simulations they simply summarize important information about the full dfns used in the simulations the graphs were constructed using dfnwork s dfngraph utility viswanathan et al 2018 srinivasan et al 2018 postprocessing was performed using dfnworks and networkx a python network analysis package hagberg et al 2008 to calculate the graph metrics there are two types of graphs that can be constructed from a dfn a fracture graph wherein fractures are assigned to nodes and intersections to edges and an intersection graph wherein intersections are assigned to nodes and fractures to edges the type of graph used depends on what needs to be calculated calculations are performed on the graphs to estimate characteristics of the dfns the graph metrics used in this analysis are avedegree intersections and stt shortest travel time avedegree is the average number of intersections per fracture this measures how connected the network is on average over the entire domain intersections is the number of fractures intersecting the repository this measures the number of potential flow pathways out of the repository region stt is the relative shortest travel time between the repository and the aquifer which is calculated by scaling the shortest travel time for each dfn by the median shortest travel time over all dfns this is a measure of ease of flow between the repository and the aquifer unlike the other two metrics the procedure for computing the shortest travel time is applied with the intersection graph with intersections as nodes and fracture as edges rather than the fracture graph for each dfn a shortest travel time was computed using dfngraph s flow functionality in dfnworks which assigns weights to the edges of the graph based on the area and permeability of each fracture these weights are used to estimate a flow time between input and output nodes in the graph instantiated with inflow and outflow pressures and fluid viscosity specified by the user the default values were used for these quantities set to 2 1 0 6 pa 1 1 0 6 pa and 8 9 1 0 4 pa s respectively there is no reason to assign any physical relevance to the absolute shortest travel time derived from this procedure since the pressures and viscosity were assigned in an arbitrary fashion instead this graph metric should be seen as a relative ranking of the speed with which fluid can move from the repository to the aquifer across dfns these graph metrics were selected for inclusion in sensitivity analysis because they summarize features of the dfns that affect flow out of repository and through the host rock so we expect them to capture some of the influence of the dfns on qois however this list of graph metrics cannot capture all effects from the dfns and research into additional graph metrics is ongoing though the graph metrics improve our understanding of dfn effects their use still interferes with sensitivity analyses because of the sampling structure for the ua the dual loop structure associates 40 realizations of the parametric uncertainty loop with each dfn this means that there are 1000 unique samples in the parameter loop but only 25 unique dfns in the spatial loop and thus each graph metric is repeated 40 times in the data set used for sensitivity analysis 3 3 quantities of interest the qoi for performance assessment research with the crystalline reference case has historically been the peak 129i concentration in the aquifer swiler et al 2019 2020 in a true performance assessment for specific proposed site the performance metric would likely be in the form of a dose to a member of the public however analysis of the crystalline reference case has not yet focused on the biosphere model so the dose is not calculated we treat the peak 129i concentration in the aquifer as a substitute performance metric for the current stage of the model because exposure to the public occurs via the aquifer the 129i concentration is monitored throughout the entire aquifer at each time step the peak 129i concentration is then calculated by taking the maximum of this concentration over both time and space in other words the peak concentration in a simulation is the highest concentration reached at any location in the aquifer at any time during the simulation additional qois have been developed to gain a better understanding of model behavior both with respect to the repository and with respect to the host rock they also contribute to our understanding of the dfn effects the qois include the mean travel time mtt of a conservative tracer from the repository to the aquifer the median residence time mdrt of an initial conservative tracer within the repository fraction of a conservative tracer remaining in the repository at certain times fractional mass fluxes of that tracer at certain times and rock boundary water mass flow rates swiler et al 2019 mariner et al 2020 swiler et al 2020 fluxes and flux ratios are the most recent additions to this list of potential qois these include a fractional mass flux and ratios of water fluxes fluxes and flux ratios are considered at different points in time based on physical phenomena immediately following the closure of the repository heat from the waste packages causes rapid expansion of water this pressure results in a very early peak within the first year in water flux to the aquifer after this initial peak however the surrounding rock absorbs heat from the waste packages more slowly and subsequently heats water entering the domain from the west this water expands affecting flow rates and directions this is referred to as the thermal pulse qois are evaluated at three thousand years to examine behavior during this pulse once the waste packages cool the surrounding rock and water subsequently cool and flow rates and directions may change again qois are evaluated at one million years to understand behavior during these relatively undisturbed conditions water flux ratios are indicators of the natural barrier system performance this analysis focused on two such flux ratios the first is the ratio of the aquifer to east boundary flux to the rock to east boundary flux this is the ratio of two water fluxes the flux from the aquifer to the east boundary normalized by the flux from the rock to the east boundary it indicates the multiplication factor on aquifer dominance of the east side effluent we evaluate this flux ratio at three thousand years during the thermal pulse and also at one million years where the conditions are nearly undisturbed the other flux ratio is the ratio of the rock to aquifer flux to the rock to the east boundary flux this indicates a multiplication factor on upward vs horizontal flow it is also evaluated at three thousand years and one million years the flow planes corresponding to the fluxes in these ratios are illustrated in fig 4 fluxes and flux ratios were also tracked over time so time dependent sensitivity analysis could identify the uncertainties driving changes in flow over time this also allows for a more continuous comparison between the pressure pulse and thermal pulse phases rather than comparing only two points in time the scalar qois discussed in section 4 are peak 129i concentration m the ratio between the aquifer to east boundary flux and the rock to east boundary flux at 1 million years and the ratio between the rock to aquifer flux and rock to east boundary flux at 1 million years the time dependent qois discussed in section 4 are the maximum 129i concentration in the aquifer the flux from the rock to the aquifer the rock to east boundary flux normalized by the flux at 1 million years and the aquifer to east boundary water normalized by the flux at 1 million years 3 4 sensitivity analysis methods this analysis focused on exploration of new qois and their utility for increasing understanding of the repository and its performance as well as performance of the model sensitivity analysis was performed using various surrogate models to estimate main and total effect sobol indices using dakota adams et al 2020 swiler et al 2019 2020 sobol indices are variance based indices the main effect index indicates the fraction of the total response variance that can be attributed to a particular parameter alone for example if a parameter has a main effect index of 0 3 with respect to a response that means that 30 of the variance in the response can be attributed to the effect of that parameter s variance the total effect index indicates the combined or total effect of a variable and its interaction with other variables on the variance of the response the main effect indices should sum to one if there are no interaction effects in the presence of interaction effects their sum will be less than one the total effect indices are bounded below by one because they include interactions in addition to the main effects if the main and total effect indices are the same value for a given parameter this indicates the parameter does not have significant interaction effects on the response with respect to other parameters sobol 1993 homma and saltelli 1996 the calculation of these sensitivity indices is a costly computation requiring tens of thousands of model evaluations because this is not feasible with this model the 1000 sample realizations were used to generate two types of surrogate models that were then used to perform the sobol variance based decomposition the two surrogates used were polynomial regression a second order quadratic regression and polynomial chaos expansion pce pce presents a computational advantage since sobol indices may be calculated directly from the expansion coefficients whereas sampling is required to estimate sobol indices with the quadratic regression sudret 2008 xiu 2010 ghanem et al 2017 ghanem and red horse 2017 le gratiet et al 2017 sensitivity results from the pce and quadratic regression models were almost equal so only the pce results are presented for simplicity one of the underlying assumptions for the surrogate models is independence between the input variables this is clearly true for the parametric uncertainties since they are independently sampled however it is not necessarily true for the graph metrics linear correlation coefficients were calculated between the graph metrics and showed that there are no significant correlations between the metrics for these simulations the highest correlation was 0 09 p value 0 66 however due to the small sample size 25 it is not clear if the lack of correlation could be by random chance future analyses should always include an assessment of graph metric correlations graph metrics also differ from the parametric uncertainties because the graph metrics are not sampled from a distribution and distributions are required for all input variables in dakota continuous distributions were fit to the graph metrics so dakota could include them in the pce surrogates the surrogates were applied twice once without including the graph metrics as inputs and a second time with the graph metrics as inputs both versions of the analysis attempt to explain the same variance the difference is whether the surrogate is allowed to attribute any of the variance to the spatial heterogeneity in the dfn the two analyses are compared to evaluate the influence of spatial uncertainties and the effectiveness of spatial uncertainty summary measures however interpretation of these analyses and their comparison is also complicated by the spatial uncertainties because of the dual loop sampling structure the spatial uncertainties are repeated for 40 epistemic realizations each whereas the epistemic samples are all unique this may over emphasize the spatial uncertainties and presents a challenge for the surrogate models when the graph metrics are not included in the analysis this may overemphasize the parametric uncertainties since they are the only attributable sources of variation the true relative contributions from the two sources of uncertainty are likely somewhere between the two biased sensitivity results the surrogate model methods and the calculation of the sobol indices are discussed extensively in swiler et al 2019 4 sensitivity analysis results sensitivity analysis results are presented for second order pce surrogate models sobol indices from quadratic regression were nearly equal to those from pce so they are omitted analyses were also performed with multivariate adaptive regression splines and gaussian process models but these surrogate models performed poorly for some qois though it would be valuable to study why some surrogates performed better than others and how to improve surrogate performance that was not the focus of this study first we discuss results for scalar qois where the sensitivity analysis was performed either for the qoi at a specified time in the simulation or for a qoi that is aggregated over time then we discuss results for qois that are tracked over time for which the sensitivity analysis was repeated at each time step results are presented for particularly interesting qois results for additional qois are discussed in swiler et al 2021 4 1 scalar qoi results this section presents analysis results for qois that are either aggregated over time or correspond to a specific point in time and so are scalars results are provided using two plots which contain the main and total sobol index values for the sensitivity analyses that were performed with and without the graph metrics in this analysis the pce surrogate is second order so the total index for a parameter includes its main effect and effects from two way interactions when the total index is significantly greater than the main index this indicates significant interactions the sensitivity analysis results for peak 129i concentration are plotted in fig 5 fig 5a shows the results when the sensitivity analysis excludes graph metrics and thus cannot attribute variance to the spatial uncertainty fig 5b shows the results when the sensitivity analysis includes graph metrics and thus variance can be attributed to the spatial uncertainty in the top plot which does not account for spatial heterogeneity in the surrogate model the spent fuel dissolution rate rateunf and glacial till permeability kglacial account for most of the variance the total index for rateunf is around 0 1 higher than its main index and the total index for kglacial is also higher than the main index which suggests interactions this does not necessarily mean that kglacial and rateunf have a significant interaction with each other rather it means that all of their interactions with the other parameters when combined are significant the analysis in fig 5b includes three graph metrics to represent spatial heterogeneity with these metrics included the indices for kglacial and rateunf are substantially reduced these results however should be interpreted together and through comparison with scatter plots since the repeated values in the spatial loop may bias the surrogate model to over attribute variance to the graph metrics the peak 129i concentration is plotted against the parameters and graph metrics in fig 6 the trend between peak 129i concentration and rateunf h is distinct and there are apparent minor trends with respect to kglacial i and irf g additionally the scatter plots show trends between the graph metrics a b c and peak 129i concentration that correspond to the high main indices for the graph metrics the importance of the graph metrics makes sense because the number of intersections influences 129i transport out of the repository and the average degree summarizes the densities of transport paths through the host rock interpreting both analyses and the scatter plots together it is clear that uncertainties in both loops are important both analyses were performed on the same simulations which included spatial heterogeneity with the only difference being whether the surrogate models used to estimate the sensitivity indices included a measure of spatial heterogeneity in the absence of graph metrics the surrogates estimate rateunf contributes to over 70 of the variance in 129i though rateunf is clearly important the analysis including graph metrics shows that this is an overestimate of the parameter s influence this demonstrates that surrogate models can provide inaccurate results and may misattribute variance due to spatial heterogeneity when measures of the spatial heterogeneity are not included in surrogate model construction results for the ratio between the aquifer to east boundary flux and the rock to east boundary flux are plotted in fig 7 these results are for the flux ratio at 1 ma end of simulation there is little difference between the analysis that excluded graph metrics a and the analysis that included them b for this qoi this makes sense because the trend with kglacial in fig 8 i is strong and obvious trends are not apparent for any of the graph metrics this effect from kglacial is expected increasing permeability enables increased flow in the aquifer and thus increases flux from the aquifer to the east boundary and from the rock into the aquifer however note that the trend with kglacial appears in nearly parallel lines these lines are different spatial realizations so while the general trend is strongly determined by kglacial that trend varies up or down due to spatial heterogeneity none of the graph metrics have very significant sensitivity index values so they do not appear to describe the spatial heterogeneity that drives this variation however the rock to east boundary flux depends on the bulk properties of the fractured rock domain the method used to generate dfns is intended to incorporate spatial heterogeneity while maintaining these bulk properties so the lack of significant sensitivity to the graph metrics for this qoi may indicate success in maintaining bulk properties across dfns this example may motivate exploration of additional graph metrics in the future the current graph metrics clearly improve understanding for some qois but are not sufficient to fully characterize the effects of spatial heterogeneity the ratio between the rock to aquifer flux and the rock to east boundary flux at 1ma is the final scalar qoi analyzed in this section the sensitivity analysis results in fig 9 are an interesting case because almost all of the variance is attributed to interaction effects when the graph metrics are not included in the analysis a this is an instance of over fitting which can be seen by looking at the scatterplots in fig 10 plots a b and c graph the flux ratio against the graph metrics at each value of the graph metric for all graph metrics the points are very closely clustered in other words the variation within the 40 parameter loop samples in a single spatial realization is small this trend occurs for all spatial realizations so most of the variance is between spatial realizations due to spatial heterogeneity and not due to within group or parameter uncertainty this means that when the surrogate model does not have access to any variables that characterize spatial heterogeneity it attributes that variance to the parameters since none of those parameters have a significant effect on their own the variance is attributed to interactions there are sufficiently many parameters compared to the total number of simulations that the pce surrogate model can explain much of the variance via two way interactions but these results are spurious the conclusion of over fitting is also intuitive because it does not make sense phenomenologically for rateunf irf stdwprate or meanwprate to affect these fluxes these parameters only affect the waste packages and waste form not the water flux in the surrounding rock this example demonstrates the necessity of interrogating sensitivity analysis results critically to ensure that conclusions make phenomenological sense and reflect patterns that truly exist in the data this type of analysis becomes much more challenging when higher order interaction effects are included in surrogate models than cannot be readily visualized 4 2 time dependent qoi results this section presents sensitivity analysis results for qois that are tracked over time the analyses were performed precisely as was done with the scalar qois except that the process of fitting the surrogate model and estimating sobol indices was repeated at each time step this means that the qoi value at each timestep was treated as an individual qoi and sensitivity results are plotted over time to demonstrate how sensitivity changes we note that the time dependent sobol sensitivity indices require significant computation for each qoi a surrogate must be constructed at each time point then used in the sensitivity index calculations this is a one time cost that is part of the postprocessing analysis the benefit is that it allows one to more clearly see how the importance of various parameters changes over time with that information we can better relate the sensitivity analysis results to the physics of the problem as in the scalar qoi analysis the sobol indices were estimated with surrogates that did not include graph metrics to represent spatial heterogeneity and with surrogates that included these metrics sensitivity results over time for maximum 129i are plotted in fig 11 fig 11a and 11b show the main and total index values over time for the surrogates that do not account for spatial heterogeneity fig 11c and 11d show the main and total index values over time for the surrogates that do account for spatial heterogeneity the bottom row of plots is meant to facilitate more in depth understanding of the sensitivity analysis results fig 11e shows the time history of the qoi and fig 11f shows the variance between the realizations over time this type of plot can be helpful for understanding seemingly aberrant behavior in sensitivity indices since this can happen when the variance is close to zero the indices are a variance decomposition tool for sensitivity analysis so they make sense only if there is enough variance to decompose fig 11g shows the temperature time histories for the centermost waste package in the repository observation point 4 see fig 1 and the point in the drz that is directly above the centermost waste package in the repository these temperatures are so similar across realizations that the curves are nearly indistinguishable observation point 4 is in the rock just below the aquifer in line with the center of the repository and on the first deterministic fracture connected to the aquifer to the east of the repository as shown in fig 1 note that due to differences in scale there are two separate vertical axes in fig 11g the temperature at observation point 4 is substantially cooler it is plotted on a different scale so the time at which it reaches its peak is discernable when the graph metrics are not included fig 11a and 11b the parameters driving waste package corrosion meanwprate stdwprate dominate especially in the period of 1 0 3 to 1 0 5 years when the graph metrics are included these effects are diminished and the average degree rises in importance with the number of intersections becoming important towards the end of the simulation recall from the scalar qoi analysis that the sensitivity results for peak 129i concentration see fig 5 identified rateunf as the most important parameter uncertainty and kglacial as somewhat important which is consistent with the results near the end of the simulation in fig 11 this means that the two analyses are in agreement since the peak 129i concentration occurs near the end of the simulation comparison of the sensitivity results over time to the temperature plots for the centermost wp the drz and observation point 4 fig 11g shows that the rising importance of rateunf kglacial and graph metrics corresponds with the leveling off of these temperatures this change in the order of importance for parameters can also be verified with scatter plots at individual time points fig 12 shows the maximum 129i concentration at 1 0 5 and 1 0 6 years versus meanwprate a c and the number of intersections b d the trend with meanwprate is strong up at 1 0 5 years but weaker at the end of the simulation whereas the trend with the number of intersections is strongest at the end of the simulation the increase of importance for rateunf at the end of the simulation makes sense because by that time the waste packages have failed so waste package degradation does not control radionuclide release waste form degradation controls it kglacial affects peak 129 i because high permeability causes rapid flow in the aquifer and therefore higher dilution when average degree and intersections increase this creates more fracture connections between the repository and other fractures which can affect upward flow this suggests that waste package corrosion drives uncertainty in the maximum 129i concentration up to around 1 0 5 years and waste form degradation drives uncertainty later this makes sense because radionuclides enter the repository when the waste form degrades and the engineered barrier fails when the engineered barrier the waste package is intact it prevents radionuclides from the waste form entering the repository but once the engineered barrier breaches any degradation of the waste form releases radionuclides into the repository the time dependent sobol index results are useful for understanding these types of changes over time without having to generate and examine hundreds of scatter plots fig 13 shows the global sensitivity analysis results for the water flux from the rock to the aquifer over time the total effects for the parameter uncertainties are not significant when the graph metrics are included d and only kglacial appears to indicate a potentially significant effect when the graph metrics are excluded b this combined with the low main effects for all of the other uncertain parameters in the analysis without graph metrics a suggests that the dominant uncertainty affecting water flux from the rock to the aquifer is spatial heterogeneity this also explains why this flux e and the corresponding sensitivity analysis results vary less over time than some other qois the spatial heterogeneity is fixed with respect to time of the different measures of spatial heterogeneity the number of intersections with the repository has the most significant main effect the results for this qoi show that the flux of water from the rock to the aquifer is primarily affected by differences in stochastically placed fractures though the analysis including graph metrics shows that spatial heterogeneity is the dominant uncertainty c d it is interesting and unexpected that uncertainty in number of intersections with the repository is more important than uncertainty in the average degree all of the graph metrics have higher total effect sobol indices d than main effect sobol indices c suggesting significant interactions with respect to the parameter uncertainties it makes sense that kglacial appears to have a small potentially significant main effect when the graph metrics are not included in the analysis a as kglacial increases the aquifer becomes less resistant to inflow the slight peak in the importance of kglacial corresponds to the increase in upward flow during the thermal period e as expected the relatively high total effects for some parameters in the analysis without graph metrics b may be the result of over fitting none of the parameter uncertainties can explain the variance but there are enough parameters for the surrogate to explain some of it via interactions global sensitivity results for the flux from the rock to the east boundary normalized by the flux at 1 ma are plotted in fig 14 this is another new qoi and conceptually the higher this ratio is the higher the flux from the rock to east boundary only the total effects indices are plotted for this qoi when the graph metrics are not included in the surrogate model a the sobol indices attribute most of the variance to kglacial which was expected with graph metrics included in the surrogate model b kglacial is still significant but the shortest travel time and number of intersections gain importance at around 1000 years the number of intersections dominates but the shortest travel time becomes more significant later on as the flux begins to decrease b c this seems reasonable because the number of intersections with the repository affects only flow out of the repository early but the shortest travel time characterizes travel into the aquifer later sensitivity results over time for the flux from the aquifer to the east boundary normalized by the flux at 1 ma are plotted in fig 15 only the total effects indices are plotted for this qoi the sharp drop in importance for kglacial just before 1000 years a b seems unreasonable however the time series plot for this qoi c shows that this drop corresponds to when the water flux increases and the variance drops to nearly zero as in the other analyses inclusion of the graph metrics decreases the importance of kglacial though it still explains more than half of the variance early in the simulation for the graph metrics the number of intersections with the repository has some importance early when flux is low but the shortest travel time is more important later in the simulation when flux is higher as was the case with the rock to east boundary flux 5 conclusions the results presented in this paper demonstrate significant advancements in ongoing study of the crystalline reference case for development of pa methods swiler et al 2019 2020 2021 the addition of graph metrics that quantify realized characteristics of the dfns clearly improved understanding of system performance and system behavior graph metrics tended to dominate sensitivity analysis results when included in the surrogate model construction suggesting that spatial heterogeneity may drive uncertainty in many of the qois however it was not clear from our analysis to what extent this dominance may be influenced by the repeated values of the graph metrics in the sampling structure e g one dfn run with 40 epistemic samples means those 40 samples all have the same set of graph metrics we were able to identify when parameters or spatial heterogeneity are important but it is not always clear which type of uncertainty dominates due to this potential effect however graph metrics were not significant in all analyses even when spatial heterogeneity should be a significant contributor to qoi uncertainty this indicates that the graph metrics selected do not describe all of the significant spatial heterogeneity refinement or addition of more graph metrics may be needed to better capture this uncertainty the addition of flux ratios also improved understanding of spatial effects especially when considered with the graph metrics sensitivity analyses confirmed that the dominant uncertainties driving uncertainty in fluxes make phenomenological sense the number of fracture intersections with the repository correlates strongly with the direction of flow through the rock there is more upward flow in the rock relative to horizontal flow as the number of intersections increases the number of intersections also correlates with higher peak 129i meaning fracture intersection avoidance has significant performance implications the average degree and stt also correlate well with peak 129i though the number of intersections has a stronger correlation time dependent sensitivity analyses were also a new capability applied in this study which has not previously been performed for the crystalline reference case swiler et al 2019 2020 2021 comparison with scalar analysis and scatter plots confirmed the accuracy of these sensitivity results comparison to thermal profiles for waste packages and the drz demonstrated that such analyses provide a more nuanced understanding of changes over time advancements for the crystalline reference case to support pa method development are ongoing this analysis supports further development of graph metrics to describe spatial heterogeneity use of flux ratio qois to supplement 129i analyses and inclusion of time dependent sensitivity analyses to understand performance changes over time credit authorship contribution statement d m brooks methodology software formal analysis writing original draft writing review editing visualization l p swiler conceptualization methodology software writing original draft writing review editing supervision project administration e stein conceptualization methodology writing review editing supervision project administration funding acquisition p e mariner conceptualization methodology writing review editing supervision project administration funding acquisition e basurto methodology software formal analysis writing review editing visualization t portone methodology software formal analysis writing original draft writing review editing visualization a eckert software validation r leone methodology software formal analysis writing review editing visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been funded by the geologic disposal safety assessment gdsa program under the spent fuel and waste science and technology sfwst campaign of the u s department of energy office of nuclear energy office of fuel cycle technology the authors thank their colleague mariah smith for generating figure 3 of this paper the work has been performed at sandia national laboratories sandia national laboratories is a multi mission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government this article has been authored by employees of national technology engineering solutions of sandia llc under contract no de na0003525 with the u s department of energy doe the employees own all right title and interest in and to the article and are solely responsible for its contents the united states government retains and the publisher by accepting the article for publication acknowledges that the united states government retains a non exclusive paid up irrevocable world wide license to publish or reproduce the published form of this article or allow others to do so for united states government purposes the doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan https www energy gov downloads doe public access plan 
92,geologic disposal safety assessment framework is a state of the art simulation software toolkit for probabilistic post closure performance assessment of systems for deep geologic disposal of nuclear waste developed by the united states department of energy this paper presents a generic reference case and shows how it is being used to develop and demonstrate performance assessment methods within the geologic disposal safety assessment framework that mitigate some of the challenges posed by high uncertainty and limited computational resources variance based global sensitivity analysis is applied to assess the effects of spatial heterogeneity using graph based summary measures for scalar and time varying quantities of interest behavior of the system with respect to spatial heterogeneity is further investigated using ratios of water fluxes this analysis shows that spatial heterogeneity is a dominant uncertainty in predictions of repository performance which can be identified in global sensitivity analysis using proxy variables derived from graph descriptions of discrete fracture networks new quantities of interest defined using water fluxes proved useful for better understanding overall system behavior keywords nuclear waste spent nuclear fuel performance assessment uncertainty data availability data will be made available on request nomenclature dfn discrete fracture network doe u s department of energy drz disturbed rock zone ecpm equivalent continuous porous medium gdsa geologic disposal safety assessment mdrt median residence time mthm metric tons of heavy metal mtt mean travel time pa performance assessment pce polynomial chaos expansion qoi quantity of interest sfwst spent fuel and waste science technology stt shortest travel time ua uncertainty analysis uq uncertainty quantification wp waste package 1 introduction the united states department of energy doe is developing a state of the art simulation software toolkit geologic disposal safety assessment gdsa framework for probabilistic post closure performance assessment pa of systems for deep geologic disposal of nuclear waste post closure refers to the period after repository operations have ceased and performance relies on passive features of the site and engineered systems as well as institution controls to prevent intrusion international atomic energy agency 2011 the post closure period can be on the order of hundreds of thousands to millions of years due to the long half lives of some of the hazardous radionuclides present in nuclear waste this requires that pa simulations predict system behavior over much longer time scales than can be observed the gdsa framework couples increasingly higher fidelity models of subsystem processes into total system pa simulations these models can include multiphase flow reactive transport heat conduction and convection simple geomechanics and radionuclide decay and ingrowth because of the inherent complexity and time scale predictions from pa models are necessarily uncertain and the characterization quantification and analysis of uncertainty are all integral components of a full pa the paradigm for treatment of uncertainty in a typical pa can be described in five general steps 1 characterization of the uncertainty space using probability distributions to describe uncertain model parameters 2 sampling of the uncertainty space 3 propagation of the samples through the system model 4 quantification of uncertainty for the quantity of interest qoi and 5 sensitivity analysis the first four steps could be described as uncertainty analysis ua and the fifth step sensitivity analysis is fundamental to support correct interpretation of ua results in the context of pa saltelli et al 2019 helton et al 2012 due to domain size and simulation time scale the pa simulations in the gdsa framework require high performance computers to run this limits the number of samples from the uncertainty space that can be propagated through the system model which can limit the effectiveness of some tools for ua and sensitivity analysis swiler et al 2019 2020 2021 this paper presents a generic reference case referred to as the crystalline reference case which is not representative of a specific site the case is meant to be illustrative for the purpose of developing and demonstrating pa methods within the gdsa framework which mitigate some of the challenges posed by high uncertainty and limited computational resources limitations on the number of pa simulations highlight the need to consider methods that improve sampling efficiency and enable exploration of the full uncertainty space even when samples of the pa simulations are relatively sparse this paper focuses specifically on new qois and sensitivity analysis using variance based sensitivity indices the crystalline reference case is a generic repository modeled in a crystalline rock formation this type of repository would be placed in highly impermeable fractured rock where flow is assumed to be dominated by flow through the fractures the repository model includes nuclear waste packages which breach based on an uncertain normalized general corrosion rate mariner et al 2016 transport of 129i is modeled from the waste packages through the repository and surrounding host rock based on additional uncertain permeability porosity and radionuclide release parameters this radionuclide is a fission product in spent nuclear fuel which is of particular interest in repository performance assessment due to its abundance in spent nuclear fuel and its long half life concentrations of 129i are monitored at pre specified points in the model domain called observation points and the peak concentration is tracked throughout the aquifer this paper also discusses other potential qois that may be more illustrative for understanding specific repository characteristics that affect performance or for understanding how the model behaves however because nuclear waste repositories are meant to prevent hazardous radionuclides from reaching the biosphere 129i concentrations are an important qoi for pa the crystalline reference case has an additional level of complexity that presents challenges for sensitivity analysis the spatial heterogeneity of potential fracture flow paths through the host rock is a source of uncertainty in pa for a nuclear waste repository located in crystalline rock conceptually a long lived radionuclide released from a waste package will initially migrate through the buffer material surrounding a waste package and into the surrounding damaged rock zone drz from there it will migrate along the drz until it enters a fracture that takes it farther into the host rock and it may eventually reach the biosphere via connected fractures and connected fracture zones the spatial heterogeneity in the fracture network contributes to uncertainty in the qois used to measure system performance with respect to the uncertainty analysis approach inclusion of uncertainty in the fracture network can be done using a dual loop sampling structure which is important for understanding the effects of different types of uncertainty but further complicates the challenges caused by limitations on the number of simulations that can be performed swiler et al 2019 this paper describes the crystalline reference case discusses the graph metrics and qois that were developed to better understand the effects of spatial heterogeneity and presents sensitivity analyses that incorporate these developments 2 model description the crystalline reference case simulates a mined repository located at 600 m below land surface in a sparsely fractured crystalline host rock such as granite or metagranite in a stable cratonic terrain it is assumed that a commercial repository would hold up to 70 000 metric tons of heavy metal mthm of commercial used nuclear fuel which is the maximum allowed by the nuclear waste policy act of 1982 this inventory could be accommodated in 168 disposal drifts each 805 m in length arranged in facing pairs on either side of a central access hallway wang et al 2015 this layout has drift centers separated by 20 m and waste packages emplaced lengthwise within the drifts with a spacing of 10 m center to center 5 m spacing end to end repository access would be via vertical shafts and or a ramp freeze et al 2013b mariner et al 2011 wang et al 2014 this reference case was developed to be a realistic example of a crystalline repository but does not correspond to any specific or proposed site and models a smaller inventory than the statutory limit of 70 000 mthm repository performance can be affected by the structural strength of the host rock depth of burial host rock permeability the geometry of fractures in the subsurface and the chemical environment which affects corrosion and radionuclide solubility and sorption engineered barriers for this repository include the characteristics of the constructed repository the waste form waste packages and bentonite buffer freeze et al 2013b mariner et al 2011 2016 the model domain for the crystalline reference case stein et al 2017 is 3015 m in length 2025 m in width and 1260 m in height overlying the host rock is a 15 m thick overburden of glacial sediments the repository is located at a depth of 585 m forty two disposal drifts contain 80 waste packages each 3360 waste packages in total the waste packages have a capacity of 12 pressurized water reactor fuel assemblies rechard and voegele 2014 each waste package has a 5 225 mthm inventory so the total inventory for the crystalline reference case is 17 556 mthm freeze et al 2013a drifts are backfilled with bentonite buffer and are surrounded by a 1 67 m thick drz the domain is modeled with an unstructured grid with finer discretization in the repository grid cells which consist of nodes edges and boundary faces are as small as 1 67 m on their shortest edge within the repository and 15 m on their shortest edge elsewhere in the model domain the model domain contains 4 848 260 cells of these approximately 2 5 million are the smaller cells in and around the repository that allow representation of individual waste packages with surrounding buffer materials additional information on the grid and dimensions may be found in stein et al 2017 available for download at https pa sandia gov 2 1 discrete fracture networks as described by mariner et al 2016 the representation of fractured crystalline rock in the crystalline reference case is loosely based on the well characterized sparsely fractured metagranite at forsmark sweden joyce et al 2014 at forsmark large scale mappable features of concentrated brittle and or ductile deformation termed deformation zones bound volumes of relatively undeformed rock each volume of relatively undeformed rock termed a fracture domain is sparsely fractured and the fractures within each can be described in terms of a number of fracture sets distinguished from each other on the basis of fracture orientation each fracture set within a particular fracture domain and depth zone is described using a 3 dimensional fisher distribution to describe the orientation of fracture poles in space a truncated power law distribution for fracture radii and a fracture density p 32 which is defined as the area of fractures per volume of rock m 2 m 3 hedin 2008 for each depth zone within a fracture domain a relationship is given between fracture radius and fracture transmissivity fig 1 shows the fracture domain for the crystalline reference case the vertical and offset planes in the figure define deterministic large mappable features such as faults the discrete fracture networks dfns are generated stochastically in this domain using probabilistic characteristics these characteristics are defined based on three fracture sets from forsmark and three zone depths resulting in nine total probabilistic fracture set descriptions in the domain the probabilistic characteristics for each set include fracture density which decreases over the depth zones so the deepest regions of the domain have the lowest fracture density radius orientation and centroid location the stochastic fracture set characteristics and deterministic features employed in this study provided sufficient fracture connectedness such that each dfn realization resulted in direct fracture pathways from the repository to the top boundary simulating a repository for heat generating nuclear waste in sparsely fractured crystalline rock requires methods to simulate coupled heat and fluid flow and reactive radionuclide transport in both porous media e g engineered backfill sedimentary aquifer and the sparsely fractured otherwise impermeable rock stein et al 2017 the dfn must be coupled to the continuum to capture the effects of heat conduction through the rock for the purposes of probabilistic performance assessment where thousands of simulations are necessary a computationally efficient representation of the dfn is needed to this end the dfn is upscaled to an equivalent continuous porous medium ecpm using the procedure detailed in stein et al 2017 in each grid cell anisotropic permeability kinematic porosity and tortuosity are calculated using the outputs of fracture permeabilities apertures orientation normal vectors radii and location from the dfn model the benefits of this approach include the ability to model heat conduction computational efficiency for large domains stein et al 2017 reports on average 50x fewer degrees of freedom for ecpm vs dfn meshes and 6x speedup in simulation times and trivial coupling to porous medium materials stein et al 2017 sweeney et al 2020 hadgu et al 2017 other methods of upscaling a sparse fracture network to a heterogeneous ecpm include those described by jackson et al 2000 svensson 2001 and sweeney et al 2020 upscaling assumptions can lead to differences in behavior between the dfn and ecpm representations azizmohammadi and matthäi 2017 bisdom et al 2017 sweeney et al 2020 hadgu et al 2017 given the dfn properties and upscaling discretization used here the ecpm representations have slightly lower bulk permeabilities calculated for a 1 km3 domain than the original dfns and tracer breakthrough times are delayed stein et al 2017 this is likely due to the increased path length in the ecpm representations parashar and reeves 2011 sweeney et al 2020 a correction following sweeney et al 2020 will be added in future studies all dfns are generated according to the same distributions for fracture radii orientations and centroid locations table 1 however despite this they are still significantly different from each other this heterogeneity between dfns is consistent with the generic nature of the reference case and our inability to completely describe the fracture network at a hypothetical fixed site as can be seen in fig 3 which shows a cutaway of the streamwise velocity field flow throughout the domain is predominantly along the fractures in the dfns this effect on the flow makes the spatial heterogeneity a significant source of uncertainty affecting the qois as discussed in the sensitivity analysis section 4 2 2 waste package general corrosion model the treatment of uncertainty is discussed in section 3 1 but in this section we highlight the uncertainty characterization for the waste package general corrosion model which is handled a bit differently than the other parameter uncertainties most of the parameters permeabilities porosities sorption parameters are treated as epistemic uncertainties arising from lack of knowledge about the particular value to use this is in contrast to aleatory uncertainties which represent inherent random variability the waste package general corrosion model is unique in the context of uncertainty and sensitivity analysis because both aleatory and epistemic uncertainty are characterized and propagated through the model the waste package general corrosion model implemented in pflotran mariner et al 2016 calculates normalized thickness of the waste package wall at each time step as a function of a base waste package general corrosion rate a canister material constant and temperature waste package breach occurs when the wall thickness reaches zero the initial wall thickness is normalized to 1 and is reduced at each time step as a function of the effective waste package general corrosion rate r e f f 1 r e f f r e c 1 333 15 1 t where r yr 1 is the general corrosion rate at 60 c 333 15 kelvin t kelvin is the local temperature and c is the canister material constant this general corrosion rate model and its associated uncertainty are conceptual because additional specificity is not necessary for a generic reference case the normalized general corrosion rate is unknown an epistemic uncertainty but is also assumed to vary spatially within the repository so it is sampled differently than other uncertain parameters the spatial variability in r is assumed to follow a lognormal distribution which is itself uncertain a lognormal distribution is used because the corrosion rate may vary multiple orders of magnitude it was expert defined to be consistent with the original generic reference case conception weck et al 2014 in a performance assessment however the distribution would need to be refined to account for the materials and waste form for that specific case the mean and standard deviation of the lognormal distribution are assigned uncertainty distributions and are sampled once for each simulation to define the lognormal distribution on r for all waste packages in that simulation this distribution is then sampled independently for each waste package to define its general corrosion rate which is applied uniformly to the package surface sampling of the distribution parameters implements the epistemic uncertainty in the general corrosion rate sampling each waste package independently from that distribution implements spatial variability throughout the repository sensitivity analysis can detect the effects of uncertainty in the distribution parameters on qois however the spatial variability is not captured or described parametrically so it is more difficult to understand how it affects qois 2 3 software the repository for the crystalline reference case is modeled using pflotran in the gdsa framework pflotran is an open source state of the art massively parallel subsurface flow and reactive transport simulator hammond et al 2014 2008 lichtner et al 2015 written in object oriented fortran 2003 pflotran is a porous medium continuum code that can model multiphase flow reactive transport heat conduction and convection simple geomechanics and radionuclide decay and ingrowth pflotran development for gdsa framework is described by mariner et al 2015 2017 2016 and sevougian et al 2018 installation instructions are available at https www pflotran org fractured crystalline rock is modeled using stochastic discrete fracture networks which are two dimensional planes distributed in the three dimensional model domain the fracture networks are generated using dfnworks hyman et al 2015 and mapped to the equivalent continuous porous medium domain using mapdfn py a code that approximates hydraulic fracture properties by calculating and assigning permeability and porosity on a cell by cell basis stein et al 2017 the implementation of uncertainty in the crystalline reference case model is handled using dakota both to sample uncertain parameters for uncertainty propagation and to perform post simulation sensitivity analyses dakota is an open source toolkit of algorithms that contains both state of the art research and robust usable software for optimization and uncertainty quantification uq adams et al 2020 the algorithms allow a user to explore a computational simulation identify important parameters the effects of uncertainties on the system and optimize designs dakota contains the uncertainty and sensitivity analysis methods commonly used in assessment of computational models including sampling methods uncertainty propagation simple rank and partial correlation analysis surrogate models which can be constructed from a limited number of sample runs and then queried extensively for visualization sensitivity analysis or uq variance based decomposition methods and other approaches such as one at a time methods dakota also contains methods to perform nested sampling involving separate sample loops over epistemic and aleatory uncertain parameters this capability was used to sample the discrete fracture networks and individual waste package general corrosion rates in a separate loop from the epistemic uncertainties finally dakota has capabilities to submit runs to high performance computers and manage concurrent evaluations of a computational model where each model evaluation may be run in parallel on multiple cores the dakota software is publicly available at https dakota sandia gov 3 analysis methods the crystalline reference case was studied using an uncertainty analysis framework the focus however was on methods to better characterize the effects of spatial heterogeneity which were tested with global sensitivity analysis 3 1 analysis structure the uncertainty analysis was structured to separate spatial heterogeneity from parametric uncertainty to investigate the relative contributions to total uncertainty from each type this separation is accomplished with a dual loop sampling structure as shown in fig 2 the loops are nested without repeated sampling so that there are 40 unique samples of the variables in the parameter uncertainty loop for each realization of the spatial uncertainty loop there are 25 dfns in the spatial uncertainty loop thus a total of 1000 pflotran simulations were run the intention behind sampling the parameter loop 1000 times instead of repeating the same 40 samples for each spatial realization is to obtain better coverage of the epistemic space with a repeated sampling scheme there would be a total of 40 unique values for each epistemic uncertain variable over all 1000 simulations with the sampling scheme applied for this analysis there are 1000 unique values for each epistemic uncertain variable previous analyses have been performed with repeated samples swiler et al 2020 the sample sizes were chosen based on computational limitations with the epistemic loop containing more samples because it contains more independent uncertainties the uncertain inputs to the analysis are described in table 2 uncertain inputs in the parameter loop were sampled using latin hypercube sampling the sampled values for meanwprate and stdwprate from the parameter loop define a distribution on general corrosion rate for the waste packages and samples from this distribution were assigned to specific waste packages in the spatial sampling loop to perform the analysis samples were propagated through the deterministic repository model in pflotran which was executed to predict qois up to 106 years post closure of the repository simulations took around 30 min on average to complete on 512 processors 3 2 graph metrics the spatial heterogeneity included in this ua presents a particular challenge because it is not described parametrically like the other uncertainties it is however clearly significant the crystalline rock has low permeability so connected fractures are the dominant pathways for radionuclide transport from the repository to the aquifer in previous analysis of the reference case the same epistemic samples were repeated for each dfn yet the qois still differed by multiple orders of magnitude across dfns indicating a clearly important effect swiler et al 2020 summary characteristics were calculated to describe features of the dfns in an effort to quantify at least some of their effects parametrically we refer to these characteristics as graph metrics because of the manner in which they are calculated from graphs and because they attempt to measure key characteristics of the dfns graphs are a powerful tool for representing dfns because they are able to capture the inherent network topology of the dfn without the high computational cost of meshing thousands of fractures ranging in size from millimeter to kilometer the difference in computational cost is striking derived quantities from a graph representation of a dfn can be computed in a matter of seconds or minutes while a fully resolved 3d dfn flow and transport simulation would require several hours to run graphs have been used with some success as reduced order models with corrections for predicting flow and transport through the dfn and as surrogates in uq propagation studies in this context viswanathan et al 2018 srinivasan et al 2018 for this study however graphs were not applied within the simulations they simply summarize important information about the full dfns used in the simulations the graphs were constructed using dfnwork s dfngraph utility viswanathan et al 2018 srinivasan et al 2018 postprocessing was performed using dfnworks and networkx a python network analysis package hagberg et al 2008 to calculate the graph metrics there are two types of graphs that can be constructed from a dfn a fracture graph wherein fractures are assigned to nodes and intersections to edges and an intersection graph wherein intersections are assigned to nodes and fractures to edges the type of graph used depends on what needs to be calculated calculations are performed on the graphs to estimate characteristics of the dfns the graph metrics used in this analysis are avedegree intersections and stt shortest travel time avedegree is the average number of intersections per fracture this measures how connected the network is on average over the entire domain intersections is the number of fractures intersecting the repository this measures the number of potential flow pathways out of the repository region stt is the relative shortest travel time between the repository and the aquifer which is calculated by scaling the shortest travel time for each dfn by the median shortest travel time over all dfns this is a measure of ease of flow between the repository and the aquifer unlike the other two metrics the procedure for computing the shortest travel time is applied with the intersection graph with intersections as nodes and fracture as edges rather than the fracture graph for each dfn a shortest travel time was computed using dfngraph s flow functionality in dfnworks which assigns weights to the edges of the graph based on the area and permeability of each fracture these weights are used to estimate a flow time between input and output nodes in the graph instantiated with inflow and outflow pressures and fluid viscosity specified by the user the default values were used for these quantities set to 2 1 0 6 pa 1 1 0 6 pa and 8 9 1 0 4 pa s respectively there is no reason to assign any physical relevance to the absolute shortest travel time derived from this procedure since the pressures and viscosity were assigned in an arbitrary fashion instead this graph metric should be seen as a relative ranking of the speed with which fluid can move from the repository to the aquifer across dfns these graph metrics were selected for inclusion in sensitivity analysis because they summarize features of the dfns that affect flow out of repository and through the host rock so we expect them to capture some of the influence of the dfns on qois however this list of graph metrics cannot capture all effects from the dfns and research into additional graph metrics is ongoing though the graph metrics improve our understanding of dfn effects their use still interferes with sensitivity analyses because of the sampling structure for the ua the dual loop structure associates 40 realizations of the parametric uncertainty loop with each dfn this means that there are 1000 unique samples in the parameter loop but only 25 unique dfns in the spatial loop and thus each graph metric is repeated 40 times in the data set used for sensitivity analysis 3 3 quantities of interest the qoi for performance assessment research with the crystalline reference case has historically been the peak 129i concentration in the aquifer swiler et al 2019 2020 in a true performance assessment for specific proposed site the performance metric would likely be in the form of a dose to a member of the public however analysis of the crystalline reference case has not yet focused on the biosphere model so the dose is not calculated we treat the peak 129i concentration in the aquifer as a substitute performance metric for the current stage of the model because exposure to the public occurs via the aquifer the 129i concentration is monitored throughout the entire aquifer at each time step the peak 129i concentration is then calculated by taking the maximum of this concentration over both time and space in other words the peak concentration in a simulation is the highest concentration reached at any location in the aquifer at any time during the simulation additional qois have been developed to gain a better understanding of model behavior both with respect to the repository and with respect to the host rock they also contribute to our understanding of the dfn effects the qois include the mean travel time mtt of a conservative tracer from the repository to the aquifer the median residence time mdrt of an initial conservative tracer within the repository fraction of a conservative tracer remaining in the repository at certain times fractional mass fluxes of that tracer at certain times and rock boundary water mass flow rates swiler et al 2019 mariner et al 2020 swiler et al 2020 fluxes and flux ratios are the most recent additions to this list of potential qois these include a fractional mass flux and ratios of water fluxes fluxes and flux ratios are considered at different points in time based on physical phenomena immediately following the closure of the repository heat from the waste packages causes rapid expansion of water this pressure results in a very early peak within the first year in water flux to the aquifer after this initial peak however the surrounding rock absorbs heat from the waste packages more slowly and subsequently heats water entering the domain from the west this water expands affecting flow rates and directions this is referred to as the thermal pulse qois are evaluated at three thousand years to examine behavior during this pulse once the waste packages cool the surrounding rock and water subsequently cool and flow rates and directions may change again qois are evaluated at one million years to understand behavior during these relatively undisturbed conditions water flux ratios are indicators of the natural barrier system performance this analysis focused on two such flux ratios the first is the ratio of the aquifer to east boundary flux to the rock to east boundary flux this is the ratio of two water fluxes the flux from the aquifer to the east boundary normalized by the flux from the rock to the east boundary it indicates the multiplication factor on aquifer dominance of the east side effluent we evaluate this flux ratio at three thousand years during the thermal pulse and also at one million years where the conditions are nearly undisturbed the other flux ratio is the ratio of the rock to aquifer flux to the rock to the east boundary flux this indicates a multiplication factor on upward vs horizontal flow it is also evaluated at three thousand years and one million years the flow planes corresponding to the fluxes in these ratios are illustrated in fig 4 fluxes and flux ratios were also tracked over time so time dependent sensitivity analysis could identify the uncertainties driving changes in flow over time this also allows for a more continuous comparison between the pressure pulse and thermal pulse phases rather than comparing only two points in time the scalar qois discussed in section 4 are peak 129i concentration m the ratio between the aquifer to east boundary flux and the rock to east boundary flux at 1 million years and the ratio between the rock to aquifer flux and rock to east boundary flux at 1 million years the time dependent qois discussed in section 4 are the maximum 129i concentration in the aquifer the flux from the rock to the aquifer the rock to east boundary flux normalized by the flux at 1 million years and the aquifer to east boundary water normalized by the flux at 1 million years 3 4 sensitivity analysis methods this analysis focused on exploration of new qois and their utility for increasing understanding of the repository and its performance as well as performance of the model sensitivity analysis was performed using various surrogate models to estimate main and total effect sobol indices using dakota adams et al 2020 swiler et al 2019 2020 sobol indices are variance based indices the main effect index indicates the fraction of the total response variance that can be attributed to a particular parameter alone for example if a parameter has a main effect index of 0 3 with respect to a response that means that 30 of the variance in the response can be attributed to the effect of that parameter s variance the total effect index indicates the combined or total effect of a variable and its interaction with other variables on the variance of the response the main effect indices should sum to one if there are no interaction effects in the presence of interaction effects their sum will be less than one the total effect indices are bounded below by one because they include interactions in addition to the main effects if the main and total effect indices are the same value for a given parameter this indicates the parameter does not have significant interaction effects on the response with respect to other parameters sobol 1993 homma and saltelli 1996 the calculation of these sensitivity indices is a costly computation requiring tens of thousands of model evaluations because this is not feasible with this model the 1000 sample realizations were used to generate two types of surrogate models that were then used to perform the sobol variance based decomposition the two surrogates used were polynomial regression a second order quadratic regression and polynomial chaos expansion pce pce presents a computational advantage since sobol indices may be calculated directly from the expansion coefficients whereas sampling is required to estimate sobol indices with the quadratic regression sudret 2008 xiu 2010 ghanem et al 2017 ghanem and red horse 2017 le gratiet et al 2017 sensitivity results from the pce and quadratic regression models were almost equal so only the pce results are presented for simplicity one of the underlying assumptions for the surrogate models is independence between the input variables this is clearly true for the parametric uncertainties since they are independently sampled however it is not necessarily true for the graph metrics linear correlation coefficients were calculated between the graph metrics and showed that there are no significant correlations between the metrics for these simulations the highest correlation was 0 09 p value 0 66 however due to the small sample size 25 it is not clear if the lack of correlation could be by random chance future analyses should always include an assessment of graph metric correlations graph metrics also differ from the parametric uncertainties because the graph metrics are not sampled from a distribution and distributions are required for all input variables in dakota continuous distributions were fit to the graph metrics so dakota could include them in the pce surrogates the surrogates were applied twice once without including the graph metrics as inputs and a second time with the graph metrics as inputs both versions of the analysis attempt to explain the same variance the difference is whether the surrogate is allowed to attribute any of the variance to the spatial heterogeneity in the dfn the two analyses are compared to evaluate the influence of spatial uncertainties and the effectiveness of spatial uncertainty summary measures however interpretation of these analyses and their comparison is also complicated by the spatial uncertainties because of the dual loop sampling structure the spatial uncertainties are repeated for 40 epistemic realizations each whereas the epistemic samples are all unique this may over emphasize the spatial uncertainties and presents a challenge for the surrogate models when the graph metrics are not included in the analysis this may overemphasize the parametric uncertainties since they are the only attributable sources of variation the true relative contributions from the two sources of uncertainty are likely somewhere between the two biased sensitivity results the surrogate model methods and the calculation of the sobol indices are discussed extensively in swiler et al 2019 4 sensitivity analysis results sensitivity analysis results are presented for second order pce surrogate models sobol indices from quadratic regression were nearly equal to those from pce so they are omitted analyses were also performed with multivariate adaptive regression splines and gaussian process models but these surrogate models performed poorly for some qois though it would be valuable to study why some surrogates performed better than others and how to improve surrogate performance that was not the focus of this study first we discuss results for scalar qois where the sensitivity analysis was performed either for the qoi at a specified time in the simulation or for a qoi that is aggregated over time then we discuss results for qois that are tracked over time for which the sensitivity analysis was repeated at each time step results are presented for particularly interesting qois results for additional qois are discussed in swiler et al 2021 4 1 scalar qoi results this section presents analysis results for qois that are either aggregated over time or correspond to a specific point in time and so are scalars results are provided using two plots which contain the main and total sobol index values for the sensitivity analyses that were performed with and without the graph metrics in this analysis the pce surrogate is second order so the total index for a parameter includes its main effect and effects from two way interactions when the total index is significantly greater than the main index this indicates significant interactions the sensitivity analysis results for peak 129i concentration are plotted in fig 5 fig 5a shows the results when the sensitivity analysis excludes graph metrics and thus cannot attribute variance to the spatial uncertainty fig 5b shows the results when the sensitivity analysis includes graph metrics and thus variance can be attributed to the spatial uncertainty in the top plot which does not account for spatial heterogeneity in the surrogate model the spent fuel dissolution rate rateunf and glacial till permeability kglacial account for most of the variance the total index for rateunf is around 0 1 higher than its main index and the total index for kglacial is also higher than the main index which suggests interactions this does not necessarily mean that kglacial and rateunf have a significant interaction with each other rather it means that all of their interactions with the other parameters when combined are significant the analysis in fig 5b includes three graph metrics to represent spatial heterogeneity with these metrics included the indices for kglacial and rateunf are substantially reduced these results however should be interpreted together and through comparison with scatter plots since the repeated values in the spatial loop may bias the surrogate model to over attribute variance to the graph metrics the peak 129i concentration is plotted against the parameters and graph metrics in fig 6 the trend between peak 129i concentration and rateunf h is distinct and there are apparent minor trends with respect to kglacial i and irf g additionally the scatter plots show trends between the graph metrics a b c and peak 129i concentration that correspond to the high main indices for the graph metrics the importance of the graph metrics makes sense because the number of intersections influences 129i transport out of the repository and the average degree summarizes the densities of transport paths through the host rock interpreting both analyses and the scatter plots together it is clear that uncertainties in both loops are important both analyses were performed on the same simulations which included spatial heterogeneity with the only difference being whether the surrogate models used to estimate the sensitivity indices included a measure of spatial heterogeneity in the absence of graph metrics the surrogates estimate rateunf contributes to over 70 of the variance in 129i though rateunf is clearly important the analysis including graph metrics shows that this is an overestimate of the parameter s influence this demonstrates that surrogate models can provide inaccurate results and may misattribute variance due to spatial heterogeneity when measures of the spatial heterogeneity are not included in surrogate model construction results for the ratio between the aquifer to east boundary flux and the rock to east boundary flux are plotted in fig 7 these results are for the flux ratio at 1 ma end of simulation there is little difference between the analysis that excluded graph metrics a and the analysis that included them b for this qoi this makes sense because the trend with kglacial in fig 8 i is strong and obvious trends are not apparent for any of the graph metrics this effect from kglacial is expected increasing permeability enables increased flow in the aquifer and thus increases flux from the aquifer to the east boundary and from the rock into the aquifer however note that the trend with kglacial appears in nearly parallel lines these lines are different spatial realizations so while the general trend is strongly determined by kglacial that trend varies up or down due to spatial heterogeneity none of the graph metrics have very significant sensitivity index values so they do not appear to describe the spatial heterogeneity that drives this variation however the rock to east boundary flux depends on the bulk properties of the fractured rock domain the method used to generate dfns is intended to incorporate spatial heterogeneity while maintaining these bulk properties so the lack of significant sensitivity to the graph metrics for this qoi may indicate success in maintaining bulk properties across dfns this example may motivate exploration of additional graph metrics in the future the current graph metrics clearly improve understanding for some qois but are not sufficient to fully characterize the effects of spatial heterogeneity the ratio between the rock to aquifer flux and the rock to east boundary flux at 1ma is the final scalar qoi analyzed in this section the sensitivity analysis results in fig 9 are an interesting case because almost all of the variance is attributed to interaction effects when the graph metrics are not included in the analysis a this is an instance of over fitting which can be seen by looking at the scatterplots in fig 10 plots a b and c graph the flux ratio against the graph metrics at each value of the graph metric for all graph metrics the points are very closely clustered in other words the variation within the 40 parameter loop samples in a single spatial realization is small this trend occurs for all spatial realizations so most of the variance is between spatial realizations due to spatial heterogeneity and not due to within group or parameter uncertainty this means that when the surrogate model does not have access to any variables that characterize spatial heterogeneity it attributes that variance to the parameters since none of those parameters have a significant effect on their own the variance is attributed to interactions there are sufficiently many parameters compared to the total number of simulations that the pce surrogate model can explain much of the variance via two way interactions but these results are spurious the conclusion of over fitting is also intuitive because it does not make sense phenomenologically for rateunf irf stdwprate or meanwprate to affect these fluxes these parameters only affect the waste packages and waste form not the water flux in the surrounding rock this example demonstrates the necessity of interrogating sensitivity analysis results critically to ensure that conclusions make phenomenological sense and reflect patterns that truly exist in the data this type of analysis becomes much more challenging when higher order interaction effects are included in surrogate models than cannot be readily visualized 4 2 time dependent qoi results this section presents sensitivity analysis results for qois that are tracked over time the analyses were performed precisely as was done with the scalar qois except that the process of fitting the surrogate model and estimating sobol indices was repeated at each time step this means that the qoi value at each timestep was treated as an individual qoi and sensitivity results are plotted over time to demonstrate how sensitivity changes we note that the time dependent sobol sensitivity indices require significant computation for each qoi a surrogate must be constructed at each time point then used in the sensitivity index calculations this is a one time cost that is part of the postprocessing analysis the benefit is that it allows one to more clearly see how the importance of various parameters changes over time with that information we can better relate the sensitivity analysis results to the physics of the problem as in the scalar qoi analysis the sobol indices were estimated with surrogates that did not include graph metrics to represent spatial heterogeneity and with surrogates that included these metrics sensitivity results over time for maximum 129i are plotted in fig 11 fig 11a and 11b show the main and total index values over time for the surrogates that do not account for spatial heterogeneity fig 11c and 11d show the main and total index values over time for the surrogates that do account for spatial heterogeneity the bottom row of plots is meant to facilitate more in depth understanding of the sensitivity analysis results fig 11e shows the time history of the qoi and fig 11f shows the variance between the realizations over time this type of plot can be helpful for understanding seemingly aberrant behavior in sensitivity indices since this can happen when the variance is close to zero the indices are a variance decomposition tool for sensitivity analysis so they make sense only if there is enough variance to decompose fig 11g shows the temperature time histories for the centermost waste package in the repository observation point 4 see fig 1 and the point in the drz that is directly above the centermost waste package in the repository these temperatures are so similar across realizations that the curves are nearly indistinguishable observation point 4 is in the rock just below the aquifer in line with the center of the repository and on the first deterministic fracture connected to the aquifer to the east of the repository as shown in fig 1 note that due to differences in scale there are two separate vertical axes in fig 11g the temperature at observation point 4 is substantially cooler it is plotted on a different scale so the time at which it reaches its peak is discernable when the graph metrics are not included fig 11a and 11b the parameters driving waste package corrosion meanwprate stdwprate dominate especially in the period of 1 0 3 to 1 0 5 years when the graph metrics are included these effects are diminished and the average degree rises in importance with the number of intersections becoming important towards the end of the simulation recall from the scalar qoi analysis that the sensitivity results for peak 129i concentration see fig 5 identified rateunf as the most important parameter uncertainty and kglacial as somewhat important which is consistent with the results near the end of the simulation in fig 11 this means that the two analyses are in agreement since the peak 129i concentration occurs near the end of the simulation comparison of the sensitivity results over time to the temperature plots for the centermost wp the drz and observation point 4 fig 11g shows that the rising importance of rateunf kglacial and graph metrics corresponds with the leveling off of these temperatures this change in the order of importance for parameters can also be verified with scatter plots at individual time points fig 12 shows the maximum 129i concentration at 1 0 5 and 1 0 6 years versus meanwprate a c and the number of intersections b d the trend with meanwprate is strong up at 1 0 5 years but weaker at the end of the simulation whereas the trend with the number of intersections is strongest at the end of the simulation the increase of importance for rateunf at the end of the simulation makes sense because by that time the waste packages have failed so waste package degradation does not control radionuclide release waste form degradation controls it kglacial affects peak 129 i because high permeability causes rapid flow in the aquifer and therefore higher dilution when average degree and intersections increase this creates more fracture connections between the repository and other fractures which can affect upward flow this suggests that waste package corrosion drives uncertainty in the maximum 129i concentration up to around 1 0 5 years and waste form degradation drives uncertainty later this makes sense because radionuclides enter the repository when the waste form degrades and the engineered barrier fails when the engineered barrier the waste package is intact it prevents radionuclides from the waste form entering the repository but once the engineered barrier breaches any degradation of the waste form releases radionuclides into the repository the time dependent sobol index results are useful for understanding these types of changes over time without having to generate and examine hundreds of scatter plots fig 13 shows the global sensitivity analysis results for the water flux from the rock to the aquifer over time the total effects for the parameter uncertainties are not significant when the graph metrics are included d and only kglacial appears to indicate a potentially significant effect when the graph metrics are excluded b this combined with the low main effects for all of the other uncertain parameters in the analysis without graph metrics a suggests that the dominant uncertainty affecting water flux from the rock to the aquifer is spatial heterogeneity this also explains why this flux e and the corresponding sensitivity analysis results vary less over time than some other qois the spatial heterogeneity is fixed with respect to time of the different measures of spatial heterogeneity the number of intersections with the repository has the most significant main effect the results for this qoi show that the flux of water from the rock to the aquifer is primarily affected by differences in stochastically placed fractures though the analysis including graph metrics shows that spatial heterogeneity is the dominant uncertainty c d it is interesting and unexpected that uncertainty in number of intersections with the repository is more important than uncertainty in the average degree all of the graph metrics have higher total effect sobol indices d than main effect sobol indices c suggesting significant interactions with respect to the parameter uncertainties it makes sense that kglacial appears to have a small potentially significant main effect when the graph metrics are not included in the analysis a as kglacial increases the aquifer becomes less resistant to inflow the slight peak in the importance of kglacial corresponds to the increase in upward flow during the thermal period e as expected the relatively high total effects for some parameters in the analysis without graph metrics b may be the result of over fitting none of the parameter uncertainties can explain the variance but there are enough parameters for the surrogate to explain some of it via interactions global sensitivity results for the flux from the rock to the east boundary normalized by the flux at 1 ma are plotted in fig 14 this is another new qoi and conceptually the higher this ratio is the higher the flux from the rock to east boundary only the total effects indices are plotted for this qoi when the graph metrics are not included in the surrogate model a the sobol indices attribute most of the variance to kglacial which was expected with graph metrics included in the surrogate model b kglacial is still significant but the shortest travel time and number of intersections gain importance at around 1000 years the number of intersections dominates but the shortest travel time becomes more significant later on as the flux begins to decrease b c this seems reasonable because the number of intersections with the repository affects only flow out of the repository early but the shortest travel time characterizes travel into the aquifer later sensitivity results over time for the flux from the aquifer to the east boundary normalized by the flux at 1 ma are plotted in fig 15 only the total effects indices are plotted for this qoi the sharp drop in importance for kglacial just before 1000 years a b seems unreasonable however the time series plot for this qoi c shows that this drop corresponds to when the water flux increases and the variance drops to nearly zero as in the other analyses inclusion of the graph metrics decreases the importance of kglacial though it still explains more than half of the variance early in the simulation for the graph metrics the number of intersections with the repository has some importance early when flux is low but the shortest travel time is more important later in the simulation when flux is higher as was the case with the rock to east boundary flux 5 conclusions the results presented in this paper demonstrate significant advancements in ongoing study of the crystalline reference case for development of pa methods swiler et al 2019 2020 2021 the addition of graph metrics that quantify realized characteristics of the dfns clearly improved understanding of system performance and system behavior graph metrics tended to dominate sensitivity analysis results when included in the surrogate model construction suggesting that spatial heterogeneity may drive uncertainty in many of the qois however it was not clear from our analysis to what extent this dominance may be influenced by the repeated values of the graph metrics in the sampling structure e g one dfn run with 40 epistemic samples means those 40 samples all have the same set of graph metrics we were able to identify when parameters or spatial heterogeneity are important but it is not always clear which type of uncertainty dominates due to this potential effect however graph metrics were not significant in all analyses even when spatial heterogeneity should be a significant contributor to qoi uncertainty this indicates that the graph metrics selected do not describe all of the significant spatial heterogeneity refinement or addition of more graph metrics may be needed to better capture this uncertainty the addition of flux ratios also improved understanding of spatial effects especially when considered with the graph metrics sensitivity analyses confirmed that the dominant uncertainties driving uncertainty in fluxes make phenomenological sense the number of fracture intersections with the repository correlates strongly with the direction of flow through the rock there is more upward flow in the rock relative to horizontal flow as the number of intersections increases the number of intersections also correlates with higher peak 129i meaning fracture intersection avoidance has significant performance implications the average degree and stt also correlate well with peak 129i though the number of intersections has a stronger correlation time dependent sensitivity analyses were also a new capability applied in this study which has not previously been performed for the crystalline reference case swiler et al 2019 2020 2021 comparison with scalar analysis and scatter plots confirmed the accuracy of these sensitivity results comparison to thermal profiles for waste packages and the drz demonstrated that such analyses provide a more nuanced understanding of changes over time advancements for the crystalline reference case to support pa method development are ongoing this analysis supports further development of graph metrics to describe spatial heterogeneity use of flux ratio qois to supplement 129i analyses and inclusion of time dependent sensitivity analyses to understand performance changes over time credit authorship contribution statement d m brooks methodology software formal analysis writing original draft writing review editing visualization l p swiler conceptualization methodology software writing original draft writing review editing supervision project administration e stein conceptualization methodology writing review editing supervision project administration funding acquisition p e mariner conceptualization methodology writing review editing supervision project administration funding acquisition e basurto methodology software formal analysis writing review editing visualization t portone methodology software formal analysis writing original draft writing review editing visualization a eckert software validation r leone methodology software formal analysis writing review editing visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been funded by the geologic disposal safety assessment gdsa program under the spent fuel and waste science and technology sfwst campaign of the u s department of energy office of nuclear energy office of fuel cycle technology the authors thank their colleague mariah smith for generating figure 3 of this paper the work has been performed at sandia national laboratories sandia national laboratories is a multi mission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government this article has been authored by employees of national technology engineering solutions of sandia llc under contract no de na0003525 with the u s department of energy doe the employees own all right title and interest in and to the article and are solely responsible for its contents the united states government retains and the publisher by accepting the article for publication acknowledges that the united states government retains a non exclusive paid up irrevocable world wide license to publish or reproduce the published form of this article or allow others to do so for united states government purposes the doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan https www energy gov downloads doe public access plan 
93,bottom up methods for water resources modeling rely on acceptability thresholds to find through a response surface which deeply uncertain futures lead to system failure they commonly treat water users as aggregate actors which may preclude analysis of the equity impacts of interventions this paper explores how aggregation choices for large groups of water users lead to different policy recommendations in response surface assessments two aggregation methods with varying parameters are considered percentile satisfaction targets and generalized mean a 2 dimensional stress test assessment across groundwater availability and population is applied to household water supply in jordan the study compares six different policies covering supply enhancement and rebalancing using a country wide multi agent model that characterizes households across socioeconomic strata for different aggregation levels policies are ordered by their associated robustness index results show that aggregation choices strongly determine policy preference a focus on the most vulnerable households favors the equalization of access to water in terms of regional allocation and weekly supply durations as it substantially reduces robustness disparity combined policies with additional resources allow to withstand higher levels of stress under most aggregation choices preferences defined by aggregation intervals provide a finer understanding of trade offs among water users and may improve deliberation over equity under deep uncertainty keywords aggregation equity jordan bottom up response surfaces water system multi agent data availability data will be made available on request 1 introduction water resources modeling and management can be hampered by the difficulties to anticipate the future state of a given water system the economic demographic or geopolitical upheavals of human societies are drivers of water demand fraught with deep uncertainties maier et al 2016 while climate change challenges the assumption of hydro climatic stationarity under which water systems could be designed milly et al 2008 at the same time model based planning under such uncertainties also needs to consider the fairness of any policy recommendation based on distributional outcomes of uncertain futures hallegatte and rozenberg 2017 jafino et al 2021 complementary approaches exist to assess and plan water systems under uncertainty the most common relies on building a discrete set of scenarios to explore internally coherent representative sets of future trajectories for climate economic growth land use or demographics riahi et al 2017 such scenarios are built upon different categories of projections often informed by models such approaches are often called top down mastrandrea et al 2010 brown and wilby 2012 or forward oriented maier et al 2016 another group of approaches often called bottom up flip the procedure instead focusing on the robustness of current decisions to deeply uncertain assumptions reducing reliance on predictive approaches or probabilistic assumptions bottom up approaches have been used across a diverse set of methodologies including inverse climate impact response functions füssel et al 2003 marcos garcia et al 2020 robust decision making lempert et al 2006 lempert 2019 info gap ben haim 2006 or decision scaling brown and wilby 2012 often in combination with other methods instead of calculating the impacts of projected changes on different performance indicators inverse approaches generally seek to identify the range of possible changes that can lead to adverse outcomes and thus usually expand the range of uncertainty in comparison to classic scenarios maier et al 2016 they do not seek to find the impacts of specific conditions but the conditions that lead to specific impacts on a system s performance this range of conditions typically supports the construction of a response function or surface prudhomme et al 2010 possible conditions are sampled through a few stressor variables which define an exposure space system performance is simulated over this exposure space with a computer based model an acceptability threshold then divides the domain of performance values into acceptable and unacceptable sub sets this allows one to draw acceptable and unacceptable sub spaces of the exposure space which are further used to compare the robustness of different policies and interventions based on their respective areas importantly water systems are inherently complex and entail a number of actors with diverse objectives that are often conflicting loucks and van beek 2017 in particular assessments using some form of robust or inverse approach have been considering increasingly large numbers of stakeholders or objectives such as in poff et al 2016 culley et al 2016 trindade et al 2017 kim et al 2019 and gold et al 2019 but each considered objective aggregates the stakes of multiple water users belonging to the same category of water use e g households supplied by the same utility farmers from the same irrigation scheme whereas in reality users can experience differential impacts based on physical geographic and socioeconomic characteristics thus response surfaces can be substantially different among water users of a same category this was illustrated in hadjimichael et al 2020 with the disparities of vulnerability profiles among farmers in the colorado river basin showing the need for case by case analysis for systems with a large number of actors model aggregation risks hiding potential inequalities and undermining the relevance of the vulnerability assessment and public support for selected policies eventually though if a group of water users is very large case by case assessments become impractical requiring some form of aggregation to evaluate system wide performances for example intermittent water supply systems can involve large numbers of households with very unequal access to water in such cases aggregation remains necessary to quantify the unequal vulnerabilities of different segments of the population a key issue at the heart of distributional assessments and fairness considerations is the adequacy of the aggregation method jafino et al 2021 aggregation does not only shape the description of a problem but also the preferred policies to solve it aggregation of potentially misaligned individual preferences is thus arguably central to political theory and more explicitly at the heart of social choice theory arrow 1951 a strictly egalitarian worldview such as j rawls maximin principle could consider a policy choice as fair if it maximizes the outcome for the worse off individual among a group rawls 1970 a more utilitarian worldview as often found with average based performance indicators would seek to maximize the sum of individual outcomes accepting that better and worse outcomes even each other out in the present paper the question of aggregation specifically applies to robustness of water availability understood as the acceptable share of the exposure space our goal is thus to explore how aggregation among the same type of water users affects response surfaces and policy recommendation in an inverse or bottom up framework we analyze a range of aggregation choices translating different attitudes towards inequalities and how it can affect response surfaces and the policy recommendation of a bottom up assessment furthermore in a similar manner to the inverse paradigm of the response function itself we identify the aggregation ranges that lead to preferring one policy over another to support equity and trade off analysis under uncertainty within a group of similar water users in section 2 the conceptual methodology of the paper presents how to parameterize the aggregation and to identify the aggregation ranges that lead to certain policy preferences section 3 presents the studied system the jordanian household water supply using the jordan water model yoon et al 2021 and describes the experimental design to apply an inverse approach using the model results are detailed in section 4 followed by a discussion regarding their potential implications and shortcomings 2 methodology we explore multiple approaches to vary the aggregation level of a response function in order to assess i the distribution of acceptable outcomes among a large group of water users and ii the effect of such aggregation choices on the policy recommendation just as the inverse approach looks for the conditions that lead to certain outcomes here the question is what levels or types of aggregation lead to certain options being favored over others the proposed methodology relies on a simple version of the response surface as a common tool among bottom up methodologies in its simplest expression a response function maps the values of a performance indicator r to a discrete number of stressor variables x 1 x 2 x n which define the exposure space or states of the world fig 1 the performance indicator r such as average consumption reliability resilience or vulnerability loucks and van beek 2017 is measured over a single time series an acceptability threshold θ separates performance values between acceptable and unacceptable and thus allows one to trace a frontier between acceptable and unacceptable shares of the exposure space the response surface can be calculated for different policies or interventions that modify the system comparing the positions of the frontiers associated with different policies allows for the selection of preferred policy options here we consider a response surface specific to a water user or an agent n agents can be grouped into a specific category of water users e g households with unequal access to water supply who share the same performance indicator r and acceptability threshold θ for a given agent i an individual response surface r i x 1 x 2 is obtained by expressing the agent specific performance of each simulation as a function of the two stressors x 1 x 2 e g changes in precipitation temperature demography etc the response surface is transformed into a binary acceptability surface the acceptability a i equals 1 if the measured performance r i for agent i satisfies an acceptability threshold θ 0 otherwise 1 a i x 1 x 2 1 r i x 1 x 2 θ 0 r i x 1 x 2 θ while the acceptability a i is specific to a given agent the objective is to produce aggregated response surfaces for the group of n agents and understand the effect of different aggregation choices on the acceptability surface exploring a range of aggregation options allows the representation of different social priorities for example if all agent performances are aggregated through an arithmetic mean extreme values will compensate for each other and the assessment will produce a policy recommendation that would ignore strong inequalities in contrast focusing on the 5 percent most vulnerable might lead to a different aggregation choice that might lead to policy recommendations that are more equalitarian but might not benefit the majority of users comparing a few isolated aggregation options would reveal the potential effect they can have on the assessment outcome however here we want to characterize i how unequally distributed the acceptability fronts can be depending on the aggregation and ii which exact aggregation choices produce different policy recommendations similarly to how inverse approaches look for the range of conditions that lead to a specific impact if a continuous parameter controls the aggregation it is possible to answer to explore points i and ii by regularly sampling the aggregation parameter it is thus a way to represent the effect of social preferences with a continuous approximation two parameterized aggregations are thus selected for this study a percentile based approach and a generalized mean approach they can be understood as generalizations or parameterizations of the particular cases that are the arithmetic mean and the median the percentile based method provides a simple composite indicator to control the aggregation choice a percentile operator considers a given position within a ranked sample as an adequate level of representation of the population for example the objective can be to satisfy a target of 90 of the population according to the threshold θ in that case the acceptability space is defined as the share of the exposure space where less than 10 of the population experiences an unacceptable performance r defining s x 1 x 2 as the percentage of the population whose performance r does not meet the threshold θ the parameterized acceptability function can then be defined for any percentile level l 2 a l x 1 x 2 1 s x 1 x 2 l 0 s x 1 x 2 l for example if the acceptability front should be drawn as to satisfy at least 90 of the population then l 10 the sub space of the response surface where more than 10 does not reach the threshold is deemed unacceptable by sampling l at different levels the distribution of acceptability ranges for different parts of the population are explored a composite response can thus be displayed tracing in the same exposure space the acceptability fronts corresponding to different percentiles of the population fig 2 over the exposure space this allows for the assessment of i the spread between levels an indication of how unequal the water users can be in terms of vulnerability ii the relative effect of percentile targets and policy choice on the front position possibly indicating that the policy is relatively ineffective for parts of the population and iii the possibility that preference between policies the respective position of their fronts switches for different percentile targets this aggregation method also allows for an explicit distributional assessment the difference with highly disaggregated impact assessments such as in e g hallegatte and rozenberg 2017 or jaeger et al 2017 is that in an inverse approach the key metric is not so much the impacts under certain conditions but the range of conditions before a specific impact is reached thus here the distribution measures the spread of robustness rather than the spread of impacts the second parameterization method the generalized mean is sometimes used in economic development research for example to monitor sustainable development goals sdgs rickels et al 2016 or design human development indices kawada et al 2019 for example the human poverty index for developing countries computed by undp uses the generalized mean with parameter p 3 mariani and ciommi 2022 this method first aggregates the performance values before tracing the acceptability front for any coordinate x 1 x 2 and for a number of agents n the generalized mean m p with parameter p of the n performance values r i x 1 x 2 is defined for positive values of r as 3 m p r 1 r n 1 n i 1 n r i p 1 p for p 0 the generalized mean is defined as equal to its limit when p approaches zero 4 m 0 r 1 r n i 1 n r i n a weighted version allows for further modulation of the generalized mean either to introduce additional priorities or in case each single agent represents a larger population 5 m p r 1 r n i 1 n w i r i p 1 p 6 m 0 r 1 r n i 1 n r i w i the parameter p controls how skewed the aggregation is towards lower or higher performance values r i for each value of p the aggregate acceptability at any given coordinate of the exposure space is given by 7 a p x 1 x 2 1 m p x 1 x 2 θ 0 m p x 1 x 2 θ the aggregated acceptability function over the response surface is thus the satisfaction of the threshold by the generalized mean at different values of p the front between accepted and rejected sub spaces is drawn for different values of p translating a different weighting given to performance values at different positions within a ranked sample a notable drawback is that the generalized mean is not defined for r 0 a few special cases illustrate the effect of the parameter p when p tends towards negative infinity the generalized mean is equal to the minimum value of the sample when p 1 it becomes the arithmetic mean when p tends towards positive infinity it gives the maximum performance value the generalized mean thus allows to parameterize in an almost continuous manner different aggregation choices between minimum mean and maximum just like choosing an acceptability percentile target choosing a value of p when applying a generalized mean operator also translates different collective choice paradigms tilmant et al 2007 that can be linked to social choice theory arrow 1951 moulin 1985 considering only the minimum water use p of the entire sample when drawing the acceptability surface would correspond to a strictly egalitarian worldview policies would be designed to improve the least robust water use following a maximin rule the rawlsian definition of justice rawls 1970 respectively considering only the maximum consumption would be considered as dictatorial as policies are selected to increase the robustness of the single agent with the highest performance in between different values of p express different degrees of utilitarianism the arithmetic mean p 1 corresponds to a fully utilitarian worldview seeking to improve the average performance among a population indifferent to the statistical distribution of such performance for both approaches percentile based and generalized mean interventions are then compared based on the respective position of acceptability fronts on the aggregated response surfaces this comparison is done for different levels of the controlling parameter either the percentile of unacceptability l or the generalized mean parameter p fig 2 the final goal of this method is to express policy preference as a function of the aggregation parameters to do so a single metric should represent the acceptability space as opposed to the performance indicator r this metric is not calculated over a time series but should be suited to qualify the whole set of simulations constituting the response surface for each acceptability surface a robustness index ri is calculated moody and brown 2013 to represent the distribution of the acceptable surface area across the population similar indices exist with additional weights to measure robustness over scenario ensembles or if the stressor domains can be weighted with probabilities in this case we consider the range of stressor values as equiprobable and select the simplest variant of the ri for any tested policy and for a given aggregation parameter value 8 r i a x 1 x 2 d x 1 d x 2 d x 1 d x 2 with acceptable ranges being simplified to a single metric alternative policies and interventions can be quantitatively compared either through the empirical cumulative distribution function ecdf across agent percentiles or as a function of the parameter p with the generalized mean aggregation r i thus becomes dependent on a parameterized acceptability a l or a p and can therefore be expressed as a function of l or p for any value taken by an aggregation parameter policies can thus be ordered by preference by comparing their robustness index break even points can then be identified for aggregation values where robustness is the same for two policies and thus policy ordering is indifferent such aggregation values form the boundaries of aggregation ranges each of these aggregation bins is thus defined by a specific ordering of policy preference based on their r i values ordering fig 3 it is thus possible to define the aggregation ranges that would lead to favor a policy over another when applied to a large number of agents in the end the objective is to acknowledge and quantify the winners and losers associated with each policy option the trade offs within a group of similar water users in the face of deep uncertainty and promote informed dialogue among stakeholders 3 application 3 1 case study the jordanian water system as a prime example of a tense water situation and looming uncertainties the country of jordan fig 4 a faces a widening gap between dwindling freshwater resources and rapidly increasing demand with difficult trade offs among water uses whitman 2019 yoon et al 2021 with an overall dry climate ranging from mediterranean to arid jordan relies on limited natural freshwater resources gunkel and lange 2012 its almost exclusive source of surface water the jordan river basin is shared with the neighboring countries with israel and syria using an important part of the upstream flow courcier et al 2005 avisse 2018 avisse et al 2020 groundwater resources are heavily overexploited leading to a rapid decline of water tables that can reach 3 5 meters per year goode et al 2013 ministry of water and irrigation mwi 2019 ecosystems are strongly affected with the disappearing of the ramsar classified azraq oasis al kharabsheh 2000 mustafa and tillotson 2019 and the shrinking of the dead sea salameh et al 2019 meanwhile water demand persistently increases agriculture remains a major water consumer despite efforts to curb groundwater abstraction for irrigation ministry of water and irrigation mwi 2019 demographic changes have been sudden with a population increase of 50 since 2010 in part due to migration from the syrian civil war reaching about 11 million today central intelligence agency cia 2021 even while the population growth rate has declined to 1 urban water consumption also includes industries and services with tourism playing an important role in the country s economy as a result jordan has one of the lowest per capita water availabilities in the world jordan has few options for developing new water resources wastewater is reused at 90 for agriculture ministry of water and irrigation mwi 2019 all fossil aquifers are now being exploited including the deep disi aquifer shared with saudi arabia müller et al 2017 desalination and conveyance from the red sea is expensive and depends on uncertain international financing increasing jordan s share of transboundary surface water requires complex negotiations with upstream countries haddadin 2009 though water imports from israel are substantially increasing under recent agreements important uncertainties are attached to many of the stressors external or internal that are relevant for jordan s water system since 1947 demographic growth has not followed a steady and predictable rate but has been punctuated by sudden increases from populations displaced by neighboring conflicts in israel lebanon iraq yemen and importantly syria since 2011 courcier et al 2005 müller et al 2016 rainfall has decreased over the 20th century rahman et al 2015 and climate change is expected to be particularly severe with droughts becoming twice as frequent long and intense by the end of the 21st century rajsekhar and gorelick 2017 meanwhile the state of groundwater resources at any point in the future is hard to predict as it depends on many factors and decisions made today jordan s water system characterized by such severe uncertainties is a prime candidate for analysis using a deep uncertainty paradigm with a stress test approach for example the time needed to reach current population levels was impossible to project before the syrian war thus hampering any form of predictive water planning in the view of high ranking officials mustafa and tillotson 2019 the disi aquifer and conveyance project developed with the objective of satisfying a projected demand strongly underestimated the demographic changes to come water availability at any given time in the future will also depend on climate change transboundary renegotiations and the previous trajectory of groundwater depletions decisions taken now be they infrastructure projects or reallocation policies can be hard to change later given their financial and political cost selecting a course of action based on its robustness to highly uncertain factors would thus make sense once propagated through the system these uncertainties affect a spatially and socially heterogeneous water supply the case of jordanian households is an example of a specific category of users domestic water consumption that can experience high disparity levels in terms of supply performance like many countries in the world jordan implements a rationing policy through intermittent water supply over most of the country rosenberg et al 2008 klassert et al 2018a such intermittency varies strongly between neighborhoods from less than one day per week in poorer districts to five days in wealthiest neighborhoods mustafa and talozi 2018 increasing reliance on private vendors the system comprised of jordanian households and their sources of water supply thus provides an adequate case to explore the question of representativeness of bottom up water vulnerability assessments for large numbers of water users 3 2 the jordan water model this work builds on the jordan water model jwm yoon et al 2021 watershed and groundwater modules are process based and spatially explicit watershed rainfall runoff is computed with swat providing inflows for the major reservoirs a groundwater response function is pre computed with a detailed modflow model at the subdistrict level and dynamically reacts to pumping decisions with a drawdown response the coupled multi agent hydro economic model employs an object oriented software architecture knox et al 2018 here we focus on the simulation of dynamic interactions between a hierarchy of diverse actors and the natural engineered water system primarily involving the piped water supply system fig 4 b components and features that are particularly relevant for this study are summarized in this section using monthly time steps the 1923 human agents make autonomous decisions based on inputs from natural engineered modules and other human agents in a hierarchical manner government bodies define high level constraints and decisions such as transboundary water availability or groundwater extraction limits among them the water authority of jordan waj determines monthly allocation and transfers of bulk water volumes between the twelve governorates of jordan based on regional per capita targets and physical topological constraints from the conveyance network from there local piped supply institutions distribute the available water among sub districts and among different and competing categories of households and commercial establishments the quantity of water made available to each sub district in the jwm is based on the number of agents and the rationing schedule following klassert et al 2015 weekly supply durations can range between 7 5 h and uninterrupted supply agents buy a certain amount from the public supply based on tariffs and their respective demand function estimates derived from 16 153 observations sigel et al 2017 klassert et al 2018b urban consumers can supplement piped water with purchases from private vendors who largely obtain groundwater sourced by farmers selby et al 2016 each of the 800 household agents represents a certain share of the jordanian population for specific characteristics such as location sub district income refugee status etc households decide purchases of piped water based on econometric demand estimates water demand functions notably depend on each household conservation options these can rely on storage capacities or water saving behaviors depending on the education level of the female household head klassert et al 2018b this multi agent framework provides an opportunity to test a response surface approach for jordanian households in a highly disaggregated manner as economic decisions are dynamic at the agent level one can interrogate how the frequency distribution function of water use changes under stress in a coherent calibrated manner the model also allows for the evaluation of results based on other household characteristics that shape the dynamics that are simulated for example income which is particularly relevant for equity oriented questions plays a central role in the amount of purchased water the district of residence and the rationing pattern of household users socio economic causes of vulnerability underlying the analyses here such as disparities in income and price elasticities are described in klassert et al 2018b 3 3 experimental design using the multi agent framework with a focus on household water use provides an opportunity to test a response surface approach for jordanian households in a highly disaggregated manner as economic decisions are dynamic at the agent level one can interrogate the distribution of agent water use to discover the impact of changes in system stresses the model allows for the evaluation of results based on other household characteristics that shape the dynamics that are simulated for example income which is particularly relevant for equity oriented questions plays a central role in the amount of purchased water the district of residence and the rationing pattern of household users to illustrate the general approach the bottom up methodology is implemented as a linear change applied to two variables of the system without any associated probabilities i e assuming a uniform probability distribution for all sampled states of the world in practice stress testing is often only a first screening step along more complete decision frameworks with probabilistic approaches directed exploration adaptive planning robust optimization etc this exploratory work is complementary to the scenario approach deployed with the jwm in yoon et al 2021 where many more variables were considered in a consistent set of time dependent narratives for example in the present stress test approach the time required to reach certain degrees of change on the selected variables is treated as a deep uncertainty for consistency and comparability the acceptable consumption threshold and the tested policies are adopted from yoon et al 2021 with some modifications that are described further below the experimental design is further described in the following sub sections 3 3 1 problem delineation while the jwm simulations involve many more modules and other agents that have dynamic effects on household water use such as highland farmers deciding to sell water to urban consumers this study focuses on household agents the two selected variables are groundwater availability and total population while it is also subject to deeply uncertain factors like climate change or geopolitical upheavals surface water is not selected as stressor in this study as it has a limited impact on household water use specifically urban water supply relies in majority on groundwater the surface water share comes from the jordan valley and takes precedence over other uses and is thus secured to a large degree from climatic or geopolitical perturbations groundwater availability can still reflect unknown changes in precipitation and temperature that would reduce the natural recharge and increase irrigation needs these two variables are considered as stressors in the sense that at any moment in time water availability in the system is affected by both variables stressors represent sets of possible future conditions the trajectory that led to any given condition or its associated probabilities are considered as unknown here groundwater availability can be the result at an unknown date of past depletion rates of political decisions without having to make a statement about which of these factors lead to a specific level of availability similarly the stress test assumes that population reaches a certain level at an unknown moment in time without the need to know if it comes from higher or lower growth rates or sudden shifts due to war or peace however with a different system delineation and approach those variables might not be considered as independent external stressors as they are heavily path dependant such a difference with time dependent simulations will be further addressed in the discussions section for groundwater a single capacity reduction factor is applied to all groundwater nodes reducing in such proportion the maximum allowed monthly extraction the model still dynamically determines abstractions within this limit and the drawdown response similarly for the population variable the same increase factor is applied to all representative households regardless of location income etc in practice demographic changes have been and will be much more heterogeneously applied 3 3 2 simulations policies and post processing for each tested intervention or policy 72 simulations of the jordan water model are performed they combine nine levels of groundwater extraction decline from 0 to 40 and eight levels of population growth from 0 to 175 such changes are consistent with those considered in the previous work with the jwm for the 2100 horizon simulations are performed over two years and results are recorded for the second year only this allows agents to adapt to the circumstances as applied to the first year typically expected market prices the baseline year is 2016 the last one for which the supporting data were available when developing the jwm the specific hydrological intra year variability has little impact in the present study though it would have to be considered if agriculture were included response surfaces seek to compare options based on the respective position of their acceptability fronts six different interventions or policies are stress tested consistent with those that were simulated in yoon et al 2021 as presented in table 1 the tested policies focus either on supply improvement adding new resources to the system in two stages supply and demand management reshaping the distribution without increasing the total available resources or a combination of both for each simulation we record monthly water use for each household agent to build response surfaces the common performance indicator is the average water consumption over a year in liters per capita per day l c d calculated over the second year the acceptability threshold is set at 40 l c d following yoon et al 2021 for a given household i the response surface r i x 1 x 2 is obtained by expressing the performance of each simulation the average water consumption per capita par day as a function of the two stressors x 1 x 2 here groundwater and population changes composite response surfaces are then calculated by sampling aggregation parameters satisfaction percentiles or generalized mean parameter finally a robustness index ri is expressed as a function of the aggregation parameters revealing the aggregation ranges that correspond to specific preference orderings of the 6 tested policies a sensitivity analysis showing how the value of the acceptability threshold θ testing 30 50 and 60 l c d affects policy preference is provided in supplementary information tables s i 2 and s i 3 the s i also includes additional results assessing the spread of the robustness index for different income deciles and governorates and how the gini coefficient of water use changes over the response surface 4 results across the 72 simulations sampling 9 levels of groundwater availability decline and 8 levels of population growth average water use declines are as expected along with average water per capita fig 5a the average consumption only gets below the acceptability threshold of 40 l cap d in the most extreme combinations of groundwater reduction and population growth to trace the frontier between acceptable and unacceptable subspaces linear interpolation is performed for each of the 800 individual response surfaces the average is then recalculated and the exposure space is divided between acceptable and unacceptable average use fig 5b the acceptability gradient mostly follows a constant anisotropy in all tested sub spaces thus in all figures hereafter the acceptable sub space is southeast of the front and the unacceptable sub space is northwest of the front the effect of different policies and interventions can be compared based on the position of their respective fronts though this policy comparison reveals the limitations of evaluating acceptability with an aggregate measure of average water use fig 5 for example shows that the baseline policy b seems to be preferable to the rebalancing policy r since households have a larger average water use under policy b for any given combination of stressors and the sub space that would be evaluated as acceptable under policy b is correspondingly larger the reason for this however is not a better supply situation under policy b rather policy b reflects the highly unequal distribution of piped water supply durations that currently prevails in jordan under the unequal distribution resulting from policy b some households are unable to meet their essential water demands with piped water and have to purchase expensive supplementary water from private vendors while others receive much more water when policy r distributes about the same overall quantity of piped water more equitably more households are satisfied with the amount of piped water they receive and fewer have additional demand for expensive supplementary water purchases this leads to a lower total water use quantity as a result the higher aggregate measure of average water use under policy b seems to indicate that policy b is strictly preferable while in most cases policy r is actually better at meeting households demands as the subsequent analyses show 4 1 percentile based approaches to further explore the disparity of policy preference among jordanian households through the 800 representative agents different aggregation levels are sampled for the two methods presented in section 2 percentile based and generalized mean we first proceed by percentile slicing the percentage of households with insufficient water use is calculated over the exposure space i e every combination of population and groundwater change acceptability fronts are defined by drawing contour plots for specific percentiles of unsatisfied users percentiles are weighted by the number of households that each agent represents thus the 5 line delineates the border of the region in which 95 of the population is satisfied the 10 line is the limit where 90 of households are satisfied etc in fig 6 the alternative policies and interventions are compared based on their acceptability frontiers for different percentiles with the baseline intervention fig 6a the acceptability fronts for different percentiles are widely spaced the 50 front is the median acceptability front where half the households have an acceptable water consumption the 5 front is not visible thus the corresponding share of the population is already in an unacceptable state under initial 2017 conditions gradient slopes slightly change across percentiles indicating that the more vulnerable percentage of households is also more vulnerable to demographic growth while more robust percentiles are more sensitive to groundwater availability decline as they rely more on private water sales and thus private wells another notable feature is that the spread between percentiles under baseline policy can be much wider than the difference between policies b and r based on the average water use while such a difference would have been used to select one option above another in fig 5 the local gradient change for the 25th percentile front southwest corner is due to the response surface of one particular agent which locally becomes the 25th percentile and thus modifies the aggregated front the location of that agent in aqaba governorate means it is more sensitive to groundwater changes than most others and thus shows a different performance gradient the following sub plots compare this baseline policy response with alternative policies and interventions fig 6b shows the effect of the supply and demand rebalancing policy detailed in table 1 on the distance between acceptability fronts for different percentiles it is much more compact than under baseline policy with obvious winners and losers by providing more water to households with lower water use the equalization of supply durations within rationing schedules combined with the raise in minimum regional targets massively expands the acceptable space for the 40 most vulnerable households while it decreases for the median or above this also shows that for moderate levels of stress changes in allocation rules are extremely effective at protecting the most vulnerable households while for higher levels of stress most households fall under unacceptable consumption if no additional resources are added the change in rationing schedule also means supplementary purchases from private vendors are decreased as described for fig 5 b the respective roles played by the rationing schedule and the bulk water allocation targets are further separated and discussed in s i additional results this difference between percentiles is also further analyzed in section 4 3 additional resource interventions at either half 6c or full 6e capacity shift the distribution away from the axis origin current conditions effectively increasing the acceptable space for all percentiles while slightly increasing the spread between them combined policies 6d drastically increase the acceptable space for all household categories as well as reducing the spread between them in the case of the combined policy at full capacity 6f no household reaches unacceptable water use in the sampled exposure space this is also a case where the rebalancing policy through supply duration equalization and increase in minimum bulk water regional supply considerably improves the robustness equity this time compared to the supply enhancement for most household percentiles increasing supply with new projects at half capacity provides a larger acceptable space than the rebalancing policy both interventions provide about the same acceptable space for the 10 percentile for the 5 most vulnerable share of households the rebalancing policy increases the acceptable space further than the new supply policy at half capacity combining policies has a massive effect in expanding the acceptable space for the most vulnerable percentiles while remaining positive for most percentiles a combined policy with new supply at half capacity 6d provides more acceptable space than a full supply expansion policy without rebalancing 6e for at least 25 of the population a more explicit distribution function of the varied responses in the household population can be obtained if the acceptable sub space area is computed for each agent this abstraction can be a loss of information as a single area value can hide varied shapes of acceptability fronts but in the present case the gradients of the acceptability range remain quite similar to quantify the acceptability ranges we use the simplest version of the robustness index ri moody and brown 2013 which is the ratio of the acceptable sub space over the entire exposure sub space the robustness index ri section 2 eq 8 is calculated for each policy and for different values of the aggregation parameters fig 7 shows the quantile function of the ri distribution weighted by the number of real households that each agent represents each distribution corresponds to a given policy or intervention it allows us to quantify the difference between interventions in terms of acceptability space break even points can be identified when comparing interventions to see the percentage of the population that benefits or is penalized by switching policies for example the rebalancing policy strongly increases the ri for the 40 of the population with lower water use and decreases it for the remaining 60 fig 6 shows that a combined policy rebalancing new supply at half capacity was more beneficial than new supply at full capacity for the low consumption households in fig 7 we see the break even point is at 30 thin dash dotted magenta line vs thick dashed blue line break even points can be considered as the boundaries of preference groups shares of the population defined by how they prioritize policies based on the robustness index metric table 2 percentile intervals corresponding to specific preferences can also be found in pair wise tables in the supplementary information appendix additional results table si2 among a sensitivity analysis changing the value of the threshold θ for certain metrics fig 7 also shows the compounding effect of combining policies as noted in yoon et al 2021 with the baseline scenario 60 of the population have a ri below 0 9 the rebalancing policy red line is detrimental in that regard increasing the share to 80 the supply enhancement at half capacity thin blue dashed line lowers the share to 35 combining both thin magenta dash dotted line leads to 0 of the population below 0 9 thus having far more than additive effects compared to the baseline and outperforming the full supply enhancement policy finally we further disaggregate results based on other household characteristics that shape the dynamics that are simulated in particular household income in fig 8 the same percentile based fronts are used for 3 interventions but only for households at the top and bottom 10 of incomes under the baseline policy 8a there is a large spread between households within each income category thus strongly overlapping with the other income group over the exposure space despite a notable difference for example the 75th percentile of poor households dotted red line 75 is roughly as robust as the 50th percentile of rich households solid black line 50 in a similar way to the overall population figures the rebalancing policy erases most of the differences 8b while a new supply policy half capacity 8c shifts the fronts towards higher stress levels without changing the spread further results in supplementary information show the spread of the robustness index for different income deciles and for the different governorates 4 2 generalized mean the alternative way to explore the disparity of the response surfaces is to first aggregate individual water use in a similar way to the mean operator but controlled by a parameter that can take different values for skewness fig 9 shows the use of the generalized mean interventions are compared for an array of values of p that control the skewness of the generalized mean towards lower or higher values as covered in section 2 the parameter p in eqs 5 and 6 can be seen as a continuous cursor between the household with the smallest water use p the arithmetic mean of water use p 1 and the household with highest water use p all particular cases of the generalized mean the acceptability threshold is then simply applied afterwards to divide the exposure space for each value of p results show similar dynamics compared to a percentile based approach the parameter p plays a similar role to the percentile l inferior values of p mean more weight is given to the households with lowest use while high values of p give more importance to the households with higher water use rebalancing reduces the spread between levels of aggregation while new supply shifts the fronts and tends to slightly increase the spread between levels fig 9b shows the effect that a single household agent can have when getting closer to a min operator as one outlier has different front slopes than the others the robustness index ri can similarly be computed for different values of p fig 10 policies can be compared based on their ri representing different social choices a more egalitarian approach lowest values of p favors rebalancing over new supply while a more utilitarian approach p around 1 prefers even the baseline policy over the rebalancing one same result as fig 5 high values of p give more importance to households with high water use which are more indifferent towards policy choice this figure shows a discrepancy with the percentile based method as this time combined policies are preferred in any case again break even points can be used to identify decision specific intervals of p each defined by a given policy ordering table 3 this classification is more abstract than percentages of the population it rather integrates all household consumptions like the arithmetic mean but with varying degrees of skewedness towards those with lowest use or those with highest use aggregation intervals corresponding to specific preferences can also be found in pair wise tables in the supplementary information appendix additional results table si3 among a sensitivity analysis changing the value of the threshold θ 4 3 distribution of water use to better understand the structure of the response surfaces for different segments of the population and the role played by the model dynamics we look at the consumption distribution functions sampled within the exposure space at different levels of stress groundwater availability decline of 15 population growth of 75 fig 11a respectively 30 150 fig 11b one apparent dynamics is that the benefits of supply enhancement policies blue dashed lines are shared unevenly across the population compared to the baseline policy this explains the low preference ranking of these policies on the lower end of the aggregation spectrum for both aggregation methods as new supply follows existing rationing patterns it tends to increase availability within neighborhoods with already good supply duration such policies thus benefit more the upper half of the population in terms of water consumption and is relatively inefficient at increasing the acceptability sub space for the population with the lowest use importantly fig 11 also exemplifies one of the reasons for the compounding effect of combining policies redistributing and increasing the resource pool are more effective combined than alone this is closely linked to the evaluation of policy performance based on the share of a population that is above a given threshold the more vertical is the distribution the more egalitarian is the water use dotted red and dash dotted magenta lines this mechanically makes those policies much more susceptible to resource fluctuations as it leads to very large shares of the population suddenly crossing a threshold in one way or the other thus producing strong non linearities with such metrics most households currently below the threshold generally benefit from more equal supply durations while those above the threshold see their consumption reduced without additional supply there is a point where many households fall below the threshold even though having the policy is still beneficial to the most vulnerable households combined policies benefit at the same time from the somewhat homogeneous shift upwards that added supply brings and from the more egalitarian distribution that lifts low use households much more effectively 5 discussion 5 1 significance even within the same category of water users aggregation choices can lead to different preferences when comparing possible policies in a water system this can be particularly relevant for bottom up methods in water vulnerability assessments as those commonly rely on limited numbers of acceptability thresholds in order to establish policy preferences under uncertainty this study shows how different aggregated response functions can be obtained using a multi agent hydro economic model for large groups of water users by continuously shifting an aggregation parameter either through percentile ordering or generalized mean results not only illustrate that water users within the same category can have differing preferences among a set of possible policies and interventions but also reveal how aggregation choices and thus socio political attitudes towards equity lead to the selection of one course of action over another one advantage the generalized mean has over the percentile based approach is that the generalized mean is affected by the actual value of water use not only its position compared to the threshold with the percentile based approach being slightly below the threshold or having no water consumption at all makes no difference however the generalized mean is also a more abstract method while the percentile based approach provides an explicit distribution of robustness both approaches do produce similar results in terms of how relative preferences change along with aggregation parameters this illustrates how in this given case study the aggregation parameter can matter more than the aggregation method it can also facilitate the interpretability of the generalized mean the jordan water model offers a high level of complexity and detail that allows for the exploration of such robustness distributions across a large number of representative agents and to design policies that modify the demand and the supply distribution apart from the development of new water resources statistical distributions are also affected by the internal dynamics of the model such as the rationing structure or the private water sales allowing them to evolve under stress and further justifying the use of a distributed stress test the disaggregation of the jwm enables analysis on the compounding benefits from combined policies compared to their standalone performance in this case within a bottom up framing the present stress test with its inherent emphasis on a satisfying metric meeting an availability threshold underlines the advantages and drawbacks of changing the slope of the water use distribution through policy it thus partially explains how it can lead to non linear benefits if combined with supply enhancement that shifts the distribution away from the acceptability threshold developing new water resources or reallocating existing ones often represent conflicting narratives with different national or international institutions favoring one or the other hussein 2018 in addition to their standalone or combined benefits the present work further asks for whom exploring how aggregation determines preference with the tested aggregation functions an aggregation parameter can modify the preference ordering for the set of different considered policies once such a divergence is acknowledged society is faced with a trade off between increasing the acceptable space for the most vulnerable households or increasing that for average or median households this exploratory work can inform such discussions and more generally the concept of equity in the face of uncertain change by quantifying such trade offs within groups of similar water users by applying different aggregation metrics managers can identify more effective solutions to reduce vulnerabilities more equally notably by combining new investments with changes in allocation rules in turn informing vulnerabilities for different socioeconomic strata may also facilitate a broader negotiation process to identify acceptable policies in face of deeply uncertain stress when producing a response surface at the scale of a country exploring aggregation ranges also allows one to circumvent loaded narratives about tracing a country s safe space with a single threshold that could be seen as an excessively malthusian perspective at best particularly when water already feeds into internal tensions over migration and jordanian identity mustafa and tillotson 2019 while still considering the tangible benefits of increasing available resources for specific levels of demographic growth disaggregating the notion of acceptable space of the time left for jordan as a whole before reaching some levels deemed as catastrophic shows that such levels will be reached at very different times by different parts of the population and that this is strongly influenced by allocation policies this can help design the most effective solutions to ensure equitable robustness under uncertainty 5 2 caveats and future research the readability of results is here favored by several circumstances and assumptions slopes and gradients can slightly change but the surfaces are still roughly oriented in the same direction for all household agents while the performance indicator itself remains the same for all in other cases the diversity of indicators of relevant stressors of response shapes and gradients as in hadjimichael et al 2020 can make aggregation much more challenging while this study remains a proof of concept the selected indicators the problem framing and their underlying values jafino et al 2021 should receive further scrutiny within an actual policy recommendation paper in particular the selection of the satisfaction threshold can shape the results considerably all the more so for the percentile aggregation method for the scope of this paper we have only considered a few levels of complexity that the jwm can handle leaving many others for future work for example it is highly unlikely that population growth would happen homogeneously over the country or social strata as was assumed in this simple stress test among the notable factors that were not considered but would affect the results income is considered as constant thus effectively assuming a stable null economic growth outside of population changes gdp per capita in jordan could increase 8 times by 2100 according to the ssp2 scenario from the shared socio economic pathways riahi et al 2017 different growth or crisis trajectories would have an impact on many levels of the models such as the ability of households to purchase water from the private sector besides integrating cost benefit analyses and socio political assessments of the tested policies within the present approach should be an avenue for future research the implementation of a cost benefit analysis would require estimated costs on the various intervention strategies including both supply infrastructure and demand management while benefits from the interventions can be estimated via modeling results the benefits analysis could further be enhanced to account for the distributive effects among the population utilizing the aggregation approaches introduced in this work another economic aspect that should be incorporated in a complete vulnerability analysis is the relationship between income and robustness of water use which in this modelization are only partially correlated over the entire sample geographic disparities being an important factor for example here the 10 most vulnerable share of the population in terms of water use does not correspond entirely to the bottom 10 of incomes households with acceptable water use might face other difficulties due to their low income households with average income might have other ways to mitigate a low water availability a related research continuation would be to assess the effects of household s conservation options technical and behavioral on their water vulnerability using the present framework fluctuations in surface water were not considered either as they have limited impact on direct drinking water supply however there would certainly be an influence through changes in the agriculture sector and the effect on mobile providers and while this study focuses on the household sub system a complete multi sectoral assessment should include agriculture with climate change as an additional stressor and rural to urban transfers as an additional policy these limitations highlight the trade offs and complementarity between a narrative scenario focused forward approach as originally used with the jwm and the present inverse stress test approach the bottom up method can identify the exact levels of stress from a few variables that would lead to unacceptable performance independently of time or without needing a mechanistic explanation to reach such levels however one of the challenges in applying a bottom up sensitivity framework to a large group of water users is that it requires a binary outcome acceptable unacceptable water use excluding information on the magnitude of the deficit besides if more variables were to be considered even after a preliminary selection of the most impactful ones a bottom up assessment would quickly run into a curse of dimensionality not only in terms of computational resources where each added dimension increases the number of simulations by an order of magnitude but also in terms of visualization for policy makers besides the non temporal stress test also precludes analysis of path dependent dynamics which are particularly important in the jordanian case e g groundwater depletion it is important to note that there is a likely degree of dependence between the stressor variables more population at any given time may prevent curbing groundwater abstractions and lead to a reduced availability later or a collapse in water availability could have dire economic impacts and lead to emigration for the present experiment under a deep uncertainty assumption we choose to apply a veil of ignorance on the relative likelihood of stressor combinations but further weighting could be applied to the response surfaces based on trustworthiness of future scenarios while the conceptual simplicity of the stress test is convenient for use with a complex model it should be viewed as complementary to other decision frameworks such as adaptive planning haasnoot et al 2019 future avenues of research would also involve using this framework with the option to screen more intermediate degrees of intervention in order explore trade offs more methodically and strategically design new policy portfolios that target specific robustness and equity outcomes 5 3 conclusions this study explores the effect of aggregation choices on water vulnerability assessments that rely on response surfaces when applied to a large number of water users to do so it relies on a dynamic multi agent model of the jordanian water system and tests combinations of supply enhancement and distributional policies under groundwater decline and population growth response functions are aggregated with percentile targets or generalized mean by relating the acceptable share of the exposure space to an aggregation parameter this work illustrates how the safe range provided by different supply enhancement and rebalancing polices depends on aggregation assumptions but also allows one to identify specific ranges of aggregation and thus social choices that lead to each different policy preference ordering the proposed methodology can be used to quantify the benefits of more equitable policy design under a deep uncertainty framework in the case of jordan different policy portfolios have different equity implications and changes in allocation and rationing patterns can be particularly effective to equitably reduce water vulnerabilities this exploratory work provides a proof of concept for more theoretical frameworks to define distributed freshwater security and thus formulate equity and trade offs within a given type of water user in the face of deeply uncertain changes credit authorship contribution statement thibaut lachaut conceptualization methodology model development formal analysis writing jim yoon model development writing christian klassert model development writing amaury tilmant conceptualization supervision writing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank staff in the jordanian ministry of water and irrigation mwi water authority of jordan waj jordan valley authority ministry of agriculture and department of statistics for provision of data and reports for the analysis we are particularly grateful for support provided by dr hazim el naser ali subah and susan kilani at mwi and refaat bani khalafat at waj we also thank steve gorelick for bringing comments and advice to this paper samer talozi yazzan haddad marwan shamekh and nicolas avisse for the help extended in jordan as well as all the contributors to the jordan water model we express our gratitude to david rosenberg and three anonymous reviewers for their constructive comments that greatly improved the paper additional data and information were provided by the united states geological survey and the united states agency for international development the economic research forum and the jordanian department of statistics granted the researchers access to relevant data after subjecting data to processing aiming to preserve the confidentiality of individual data the researchers are solely responsible for the conclusions and inferences drawn upon available data the authors acknowledge the financial support of the natural sciences and engineering research council of canada nserc through grant g8pj 437384 2012 this work was also supported by the us national science foundation nsf under grants geo oad 1342869 and icer ear 1829999 as part of the belmont forum sustainable urbanisation global initiative sugi food water energy nexus theme funding to the helmholtz centre for environmental research ufz was provided by the deutsche forschungsgemeinschaft dfg kl 2764 1 1 and the german federal ministry of education and research bmbf 033wu002 as part of the belmont forum any opinions findings and conclusions or recommendations expressed in this material are solely those of the authors and do not necessarily reflect the views of the nserc nsf dfg and bmbf appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j advwatres 2022 104311 appendix a supplementary data the following is the supplementary material related to this article mmc s1 policy design effect of income and location on r i sensitivity analysis on θ and gini coef response functions 
93,bottom up methods for water resources modeling rely on acceptability thresholds to find through a response surface which deeply uncertain futures lead to system failure they commonly treat water users as aggregate actors which may preclude analysis of the equity impacts of interventions this paper explores how aggregation choices for large groups of water users lead to different policy recommendations in response surface assessments two aggregation methods with varying parameters are considered percentile satisfaction targets and generalized mean a 2 dimensional stress test assessment across groundwater availability and population is applied to household water supply in jordan the study compares six different policies covering supply enhancement and rebalancing using a country wide multi agent model that characterizes households across socioeconomic strata for different aggregation levels policies are ordered by their associated robustness index results show that aggregation choices strongly determine policy preference a focus on the most vulnerable households favors the equalization of access to water in terms of regional allocation and weekly supply durations as it substantially reduces robustness disparity combined policies with additional resources allow to withstand higher levels of stress under most aggregation choices preferences defined by aggregation intervals provide a finer understanding of trade offs among water users and may improve deliberation over equity under deep uncertainty keywords aggregation equity jordan bottom up response surfaces water system multi agent data availability data will be made available on request 1 introduction water resources modeling and management can be hampered by the difficulties to anticipate the future state of a given water system the economic demographic or geopolitical upheavals of human societies are drivers of water demand fraught with deep uncertainties maier et al 2016 while climate change challenges the assumption of hydro climatic stationarity under which water systems could be designed milly et al 2008 at the same time model based planning under such uncertainties also needs to consider the fairness of any policy recommendation based on distributional outcomes of uncertain futures hallegatte and rozenberg 2017 jafino et al 2021 complementary approaches exist to assess and plan water systems under uncertainty the most common relies on building a discrete set of scenarios to explore internally coherent representative sets of future trajectories for climate economic growth land use or demographics riahi et al 2017 such scenarios are built upon different categories of projections often informed by models such approaches are often called top down mastrandrea et al 2010 brown and wilby 2012 or forward oriented maier et al 2016 another group of approaches often called bottom up flip the procedure instead focusing on the robustness of current decisions to deeply uncertain assumptions reducing reliance on predictive approaches or probabilistic assumptions bottom up approaches have been used across a diverse set of methodologies including inverse climate impact response functions füssel et al 2003 marcos garcia et al 2020 robust decision making lempert et al 2006 lempert 2019 info gap ben haim 2006 or decision scaling brown and wilby 2012 often in combination with other methods instead of calculating the impacts of projected changes on different performance indicators inverse approaches generally seek to identify the range of possible changes that can lead to adverse outcomes and thus usually expand the range of uncertainty in comparison to classic scenarios maier et al 2016 they do not seek to find the impacts of specific conditions but the conditions that lead to specific impacts on a system s performance this range of conditions typically supports the construction of a response function or surface prudhomme et al 2010 possible conditions are sampled through a few stressor variables which define an exposure space system performance is simulated over this exposure space with a computer based model an acceptability threshold then divides the domain of performance values into acceptable and unacceptable sub sets this allows one to draw acceptable and unacceptable sub spaces of the exposure space which are further used to compare the robustness of different policies and interventions based on their respective areas importantly water systems are inherently complex and entail a number of actors with diverse objectives that are often conflicting loucks and van beek 2017 in particular assessments using some form of robust or inverse approach have been considering increasingly large numbers of stakeholders or objectives such as in poff et al 2016 culley et al 2016 trindade et al 2017 kim et al 2019 and gold et al 2019 but each considered objective aggregates the stakes of multiple water users belonging to the same category of water use e g households supplied by the same utility farmers from the same irrigation scheme whereas in reality users can experience differential impacts based on physical geographic and socioeconomic characteristics thus response surfaces can be substantially different among water users of a same category this was illustrated in hadjimichael et al 2020 with the disparities of vulnerability profiles among farmers in the colorado river basin showing the need for case by case analysis for systems with a large number of actors model aggregation risks hiding potential inequalities and undermining the relevance of the vulnerability assessment and public support for selected policies eventually though if a group of water users is very large case by case assessments become impractical requiring some form of aggregation to evaluate system wide performances for example intermittent water supply systems can involve large numbers of households with very unequal access to water in such cases aggregation remains necessary to quantify the unequal vulnerabilities of different segments of the population a key issue at the heart of distributional assessments and fairness considerations is the adequacy of the aggregation method jafino et al 2021 aggregation does not only shape the description of a problem but also the preferred policies to solve it aggregation of potentially misaligned individual preferences is thus arguably central to political theory and more explicitly at the heart of social choice theory arrow 1951 a strictly egalitarian worldview such as j rawls maximin principle could consider a policy choice as fair if it maximizes the outcome for the worse off individual among a group rawls 1970 a more utilitarian worldview as often found with average based performance indicators would seek to maximize the sum of individual outcomes accepting that better and worse outcomes even each other out in the present paper the question of aggregation specifically applies to robustness of water availability understood as the acceptable share of the exposure space our goal is thus to explore how aggregation among the same type of water users affects response surfaces and policy recommendation in an inverse or bottom up framework we analyze a range of aggregation choices translating different attitudes towards inequalities and how it can affect response surfaces and the policy recommendation of a bottom up assessment furthermore in a similar manner to the inverse paradigm of the response function itself we identify the aggregation ranges that lead to preferring one policy over another to support equity and trade off analysis under uncertainty within a group of similar water users in section 2 the conceptual methodology of the paper presents how to parameterize the aggregation and to identify the aggregation ranges that lead to certain policy preferences section 3 presents the studied system the jordanian household water supply using the jordan water model yoon et al 2021 and describes the experimental design to apply an inverse approach using the model results are detailed in section 4 followed by a discussion regarding their potential implications and shortcomings 2 methodology we explore multiple approaches to vary the aggregation level of a response function in order to assess i the distribution of acceptable outcomes among a large group of water users and ii the effect of such aggregation choices on the policy recommendation just as the inverse approach looks for the conditions that lead to certain outcomes here the question is what levels or types of aggregation lead to certain options being favored over others the proposed methodology relies on a simple version of the response surface as a common tool among bottom up methodologies in its simplest expression a response function maps the values of a performance indicator r to a discrete number of stressor variables x 1 x 2 x n which define the exposure space or states of the world fig 1 the performance indicator r such as average consumption reliability resilience or vulnerability loucks and van beek 2017 is measured over a single time series an acceptability threshold θ separates performance values between acceptable and unacceptable and thus allows one to trace a frontier between acceptable and unacceptable shares of the exposure space the response surface can be calculated for different policies or interventions that modify the system comparing the positions of the frontiers associated with different policies allows for the selection of preferred policy options here we consider a response surface specific to a water user or an agent n agents can be grouped into a specific category of water users e g households with unequal access to water supply who share the same performance indicator r and acceptability threshold θ for a given agent i an individual response surface r i x 1 x 2 is obtained by expressing the agent specific performance of each simulation as a function of the two stressors x 1 x 2 e g changes in precipitation temperature demography etc the response surface is transformed into a binary acceptability surface the acceptability a i equals 1 if the measured performance r i for agent i satisfies an acceptability threshold θ 0 otherwise 1 a i x 1 x 2 1 r i x 1 x 2 θ 0 r i x 1 x 2 θ while the acceptability a i is specific to a given agent the objective is to produce aggregated response surfaces for the group of n agents and understand the effect of different aggregation choices on the acceptability surface exploring a range of aggregation options allows the representation of different social priorities for example if all agent performances are aggregated through an arithmetic mean extreme values will compensate for each other and the assessment will produce a policy recommendation that would ignore strong inequalities in contrast focusing on the 5 percent most vulnerable might lead to a different aggregation choice that might lead to policy recommendations that are more equalitarian but might not benefit the majority of users comparing a few isolated aggregation options would reveal the potential effect they can have on the assessment outcome however here we want to characterize i how unequally distributed the acceptability fronts can be depending on the aggregation and ii which exact aggregation choices produce different policy recommendations similarly to how inverse approaches look for the range of conditions that lead to a specific impact if a continuous parameter controls the aggregation it is possible to answer to explore points i and ii by regularly sampling the aggregation parameter it is thus a way to represent the effect of social preferences with a continuous approximation two parameterized aggregations are thus selected for this study a percentile based approach and a generalized mean approach they can be understood as generalizations or parameterizations of the particular cases that are the arithmetic mean and the median the percentile based method provides a simple composite indicator to control the aggregation choice a percentile operator considers a given position within a ranked sample as an adequate level of representation of the population for example the objective can be to satisfy a target of 90 of the population according to the threshold θ in that case the acceptability space is defined as the share of the exposure space where less than 10 of the population experiences an unacceptable performance r defining s x 1 x 2 as the percentage of the population whose performance r does not meet the threshold θ the parameterized acceptability function can then be defined for any percentile level l 2 a l x 1 x 2 1 s x 1 x 2 l 0 s x 1 x 2 l for example if the acceptability front should be drawn as to satisfy at least 90 of the population then l 10 the sub space of the response surface where more than 10 does not reach the threshold is deemed unacceptable by sampling l at different levels the distribution of acceptability ranges for different parts of the population are explored a composite response can thus be displayed tracing in the same exposure space the acceptability fronts corresponding to different percentiles of the population fig 2 over the exposure space this allows for the assessment of i the spread between levels an indication of how unequal the water users can be in terms of vulnerability ii the relative effect of percentile targets and policy choice on the front position possibly indicating that the policy is relatively ineffective for parts of the population and iii the possibility that preference between policies the respective position of their fronts switches for different percentile targets this aggregation method also allows for an explicit distributional assessment the difference with highly disaggregated impact assessments such as in e g hallegatte and rozenberg 2017 or jaeger et al 2017 is that in an inverse approach the key metric is not so much the impacts under certain conditions but the range of conditions before a specific impact is reached thus here the distribution measures the spread of robustness rather than the spread of impacts the second parameterization method the generalized mean is sometimes used in economic development research for example to monitor sustainable development goals sdgs rickels et al 2016 or design human development indices kawada et al 2019 for example the human poverty index for developing countries computed by undp uses the generalized mean with parameter p 3 mariani and ciommi 2022 this method first aggregates the performance values before tracing the acceptability front for any coordinate x 1 x 2 and for a number of agents n the generalized mean m p with parameter p of the n performance values r i x 1 x 2 is defined for positive values of r as 3 m p r 1 r n 1 n i 1 n r i p 1 p for p 0 the generalized mean is defined as equal to its limit when p approaches zero 4 m 0 r 1 r n i 1 n r i n a weighted version allows for further modulation of the generalized mean either to introduce additional priorities or in case each single agent represents a larger population 5 m p r 1 r n i 1 n w i r i p 1 p 6 m 0 r 1 r n i 1 n r i w i the parameter p controls how skewed the aggregation is towards lower or higher performance values r i for each value of p the aggregate acceptability at any given coordinate of the exposure space is given by 7 a p x 1 x 2 1 m p x 1 x 2 θ 0 m p x 1 x 2 θ the aggregated acceptability function over the response surface is thus the satisfaction of the threshold by the generalized mean at different values of p the front between accepted and rejected sub spaces is drawn for different values of p translating a different weighting given to performance values at different positions within a ranked sample a notable drawback is that the generalized mean is not defined for r 0 a few special cases illustrate the effect of the parameter p when p tends towards negative infinity the generalized mean is equal to the minimum value of the sample when p 1 it becomes the arithmetic mean when p tends towards positive infinity it gives the maximum performance value the generalized mean thus allows to parameterize in an almost continuous manner different aggregation choices between minimum mean and maximum just like choosing an acceptability percentile target choosing a value of p when applying a generalized mean operator also translates different collective choice paradigms tilmant et al 2007 that can be linked to social choice theory arrow 1951 moulin 1985 considering only the minimum water use p of the entire sample when drawing the acceptability surface would correspond to a strictly egalitarian worldview policies would be designed to improve the least robust water use following a maximin rule the rawlsian definition of justice rawls 1970 respectively considering only the maximum consumption would be considered as dictatorial as policies are selected to increase the robustness of the single agent with the highest performance in between different values of p express different degrees of utilitarianism the arithmetic mean p 1 corresponds to a fully utilitarian worldview seeking to improve the average performance among a population indifferent to the statistical distribution of such performance for both approaches percentile based and generalized mean interventions are then compared based on the respective position of acceptability fronts on the aggregated response surfaces this comparison is done for different levels of the controlling parameter either the percentile of unacceptability l or the generalized mean parameter p fig 2 the final goal of this method is to express policy preference as a function of the aggregation parameters to do so a single metric should represent the acceptability space as opposed to the performance indicator r this metric is not calculated over a time series but should be suited to qualify the whole set of simulations constituting the response surface for each acceptability surface a robustness index ri is calculated moody and brown 2013 to represent the distribution of the acceptable surface area across the population similar indices exist with additional weights to measure robustness over scenario ensembles or if the stressor domains can be weighted with probabilities in this case we consider the range of stressor values as equiprobable and select the simplest variant of the ri for any tested policy and for a given aggregation parameter value 8 r i a x 1 x 2 d x 1 d x 2 d x 1 d x 2 with acceptable ranges being simplified to a single metric alternative policies and interventions can be quantitatively compared either through the empirical cumulative distribution function ecdf across agent percentiles or as a function of the parameter p with the generalized mean aggregation r i thus becomes dependent on a parameterized acceptability a l or a p and can therefore be expressed as a function of l or p for any value taken by an aggregation parameter policies can thus be ordered by preference by comparing their robustness index break even points can then be identified for aggregation values where robustness is the same for two policies and thus policy ordering is indifferent such aggregation values form the boundaries of aggregation ranges each of these aggregation bins is thus defined by a specific ordering of policy preference based on their r i values ordering fig 3 it is thus possible to define the aggregation ranges that would lead to favor a policy over another when applied to a large number of agents in the end the objective is to acknowledge and quantify the winners and losers associated with each policy option the trade offs within a group of similar water users in the face of deep uncertainty and promote informed dialogue among stakeholders 3 application 3 1 case study the jordanian water system as a prime example of a tense water situation and looming uncertainties the country of jordan fig 4 a faces a widening gap between dwindling freshwater resources and rapidly increasing demand with difficult trade offs among water uses whitman 2019 yoon et al 2021 with an overall dry climate ranging from mediterranean to arid jordan relies on limited natural freshwater resources gunkel and lange 2012 its almost exclusive source of surface water the jordan river basin is shared with the neighboring countries with israel and syria using an important part of the upstream flow courcier et al 2005 avisse 2018 avisse et al 2020 groundwater resources are heavily overexploited leading to a rapid decline of water tables that can reach 3 5 meters per year goode et al 2013 ministry of water and irrigation mwi 2019 ecosystems are strongly affected with the disappearing of the ramsar classified azraq oasis al kharabsheh 2000 mustafa and tillotson 2019 and the shrinking of the dead sea salameh et al 2019 meanwhile water demand persistently increases agriculture remains a major water consumer despite efforts to curb groundwater abstraction for irrigation ministry of water and irrigation mwi 2019 demographic changes have been sudden with a population increase of 50 since 2010 in part due to migration from the syrian civil war reaching about 11 million today central intelligence agency cia 2021 even while the population growth rate has declined to 1 urban water consumption also includes industries and services with tourism playing an important role in the country s economy as a result jordan has one of the lowest per capita water availabilities in the world jordan has few options for developing new water resources wastewater is reused at 90 for agriculture ministry of water and irrigation mwi 2019 all fossil aquifers are now being exploited including the deep disi aquifer shared with saudi arabia müller et al 2017 desalination and conveyance from the red sea is expensive and depends on uncertain international financing increasing jordan s share of transboundary surface water requires complex negotiations with upstream countries haddadin 2009 though water imports from israel are substantially increasing under recent agreements important uncertainties are attached to many of the stressors external or internal that are relevant for jordan s water system since 1947 demographic growth has not followed a steady and predictable rate but has been punctuated by sudden increases from populations displaced by neighboring conflicts in israel lebanon iraq yemen and importantly syria since 2011 courcier et al 2005 müller et al 2016 rainfall has decreased over the 20th century rahman et al 2015 and climate change is expected to be particularly severe with droughts becoming twice as frequent long and intense by the end of the 21st century rajsekhar and gorelick 2017 meanwhile the state of groundwater resources at any point in the future is hard to predict as it depends on many factors and decisions made today jordan s water system characterized by such severe uncertainties is a prime candidate for analysis using a deep uncertainty paradigm with a stress test approach for example the time needed to reach current population levels was impossible to project before the syrian war thus hampering any form of predictive water planning in the view of high ranking officials mustafa and tillotson 2019 the disi aquifer and conveyance project developed with the objective of satisfying a projected demand strongly underestimated the demographic changes to come water availability at any given time in the future will also depend on climate change transboundary renegotiations and the previous trajectory of groundwater depletions decisions taken now be they infrastructure projects or reallocation policies can be hard to change later given their financial and political cost selecting a course of action based on its robustness to highly uncertain factors would thus make sense once propagated through the system these uncertainties affect a spatially and socially heterogeneous water supply the case of jordanian households is an example of a specific category of users domestic water consumption that can experience high disparity levels in terms of supply performance like many countries in the world jordan implements a rationing policy through intermittent water supply over most of the country rosenberg et al 2008 klassert et al 2018a such intermittency varies strongly between neighborhoods from less than one day per week in poorer districts to five days in wealthiest neighborhoods mustafa and talozi 2018 increasing reliance on private vendors the system comprised of jordanian households and their sources of water supply thus provides an adequate case to explore the question of representativeness of bottom up water vulnerability assessments for large numbers of water users 3 2 the jordan water model this work builds on the jordan water model jwm yoon et al 2021 watershed and groundwater modules are process based and spatially explicit watershed rainfall runoff is computed with swat providing inflows for the major reservoirs a groundwater response function is pre computed with a detailed modflow model at the subdistrict level and dynamically reacts to pumping decisions with a drawdown response the coupled multi agent hydro economic model employs an object oriented software architecture knox et al 2018 here we focus on the simulation of dynamic interactions between a hierarchy of diverse actors and the natural engineered water system primarily involving the piped water supply system fig 4 b components and features that are particularly relevant for this study are summarized in this section using monthly time steps the 1923 human agents make autonomous decisions based on inputs from natural engineered modules and other human agents in a hierarchical manner government bodies define high level constraints and decisions such as transboundary water availability or groundwater extraction limits among them the water authority of jordan waj determines monthly allocation and transfers of bulk water volumes between the twelve governorates of jordan based on regional per capita targets and physical topological constraints from the conveyance network from there local piped supply institutions distribute the available water among sub districts and among different and competing categories of households and commercial establishments the quantity of water made available to each sub district in the jwm is based on the number of agents and the rationing schedule following klassert et al 2015 weekly supply durations can range between 7 5 h and uninterrupted supply agents buy a certain amount from the public supply based on tariffs and their respective demand function estimates derived from 16 153 observations sigel et al 2017 klassert et al 2018b urban consumers can supplement piped water with purchases from private vendors who largely obtain groundwater sourced by farmers selby et al 2016 each of the 800 household agents represents a certain share of the jordanian population for specific characteristics such as location sub district income refugee status etc households decide purchases of piped water based on econometric demand estimates water demand functions notably depend on each household conservation options these can rely on storage capacities or water saving behaviors depending on the education level of the female household head klassert et al 2018b this multi agent framework provides an opportunity to test a response surface approach for jordanian households in a highly disaggregated manner as economic decisions are dynamic at the agent level one can interrogate how the frequency distribution function of water use changes under stress in a coherent calibrated manner the model also allows for the evaluation of results based on other household characteristics that shape the dynamics that are simulated for example income which is particularly relevant for equity oriented questions plays a central role in the amount of purchased water the district of residence and the rationing pattern of household users socio economic causes of vulnerability underlying the analyses here such as disparities in income and price elasticities are described in klassert et al 2018b 3 3 experimental design using the multi agent framework with a focus on household water use provides an opportunity to test a response surface approach for jordanian households in a highly disaggregated manner as economic decisions are dynamic at the agent level one can interrogate the distribution of agent water use to discover the impact of changes in system stresses the model allows for the evaluation of results based on other household characteristics that shape the dynamics that are simulated for example income which is particularly relevant for equity oriented questions plays a central role in the amount of purchased water the district of residence and the rationing pattern of household users to illustrate the general approach the bottom up methodology is implemented as a linear change applied to two variables of the system without any associated probabilities i e assuming a uniform probability distribution for all sampled states of the world in practice stress testing is often only a first screening step along more complete decision frameworks with probabilistic approaches directed exploration adaptive planning robust optimization etc this exploratory work is complementary to the scenario approach deployed with the jwm in yoon et al 2021 where many more variables were considered in a consistent set of time dependent narratives for example in the present stress test approach the time required to reach certain degrees of change on the selected variables is treated as a deep uncertainty for consistency and comparability the acceptable consumption threshold and the tested policies are adopted from yoon et al 2021 with some modifications that are described further below the experimental design is further described in the following sub sections 3 3 1 problem delineation while the jwm simulations involve many more modules and other agents that have dynamic effects on household water use such as highland farmers deciding to sell water to urban consumers this study focuses on household agents the two selected variables are groundwater availability and total population while it is also subject to deeply uncertain factors like climate change or geopolitical upheavals surface water is not selected as stressor in this study as it has a limited impact on household water use specifically urban water supply relies in majority on groundwater the surface water share comes from the jordan valley and takes precedence over other uses and is thus secured to a large degree from climatic or geopolitical perturbations groundwater availability can still reflect unknown changes in precipitation and temperature that would reduce the natural recharge and increase irrigation needs these two variables are considered as stressors in the sense that at any moment in time water availability in the system is affected by both variables stressors represent sets of possible future conditions the trajectory that led to any given condition or its associated probabilities are considered as unknown here groundwater availability can be the result at an unknown date of past depletion rates of political decisions without having to make a statement about which of these factors lead to a specific level of availability similarly the stress test assumes that population reaches a certain level at an unknown moment in time without the need to know if it comes from higher or lower growth rates or sudden shifts due to war or peace however with a different system delineation and approach those variables might not be considered as independent external stressors as they are heavily path dependant such a difference with time dependent simulations will be further addressed in the discussions section for groundwater a single capacity reduction factor is applied to all groundwater nodes reducing in such proportion the maximum allowed monthly extraction the model still dynamically determines abstractions within this limit and the drawdown response similarly for the population variable the same increase factor is applied to all representative households regardless of location income etc in practice demographic changes have been and will be much more heterogeneously applied 3 3 2 simulations policies and post processing for each tested intervention or policy 72 simulations of the jordan water model are performed they combine nine levels of groundwater extraction decline from 0 to 40 and eight levels of population growth from 0 to 175 such changes are consistent with those considered in the previous work with the jwm for the 2100 horizon simulations are performed over two years and results are recorded for the second year only this allows agents to adapt to the circumstances as applied to the first year typically expected market prices the baseline year is 2016 the last one for which the supporting data were available when developing the jwm the specific hydrological intra year variability has little impact in the present study though it would have to be considered if agriculture were included response surfaces seek to compare options based on the respective position of their acceptability fronts six different interventions or policies are stress tested consistent with those that were simulated in yoon et al 2021 as presented in table 1 the tested policies focus either on supply improvement adding new resources to the system in two stages supply and demand management reshaping the distribution without increasing the total available resources or a combination of both for each simulation we record monthly water use for each household agent to build response surfaces the common performance indicator is the average water consumption over a year in liters per capita per day l c d calculated over the second year the acceptability threshold is set at 40 l c d following yoon et al 2021 for a given household i the response surface r i x 1 x 2 is obtained by expressing the performance of each simulation the average water consumption per capita par day as a function of the two stressors x 1 x 2 here groundwater and population changes composite response surfaces are then calculated by sampling aggregation parameters satisfaction percentiles or generalized mean parameter finally a robustness index ri is expressed as a function of the aggregation parameters revealing the aggregation ranges that correspond to specific preference orderings of the 6 tested policies a sensitivity analysis showing how the value of the acceptability threshold θ testing 30 50 and 60 l c d affects policy preference is provided in supplementary information tables s i 2 and s i 3 the s i also includes additional results assessing the spread of the robustness index for different income deciles and governorates and how the gini coefficient of water use changes over the response surface 4 results across the 72 simulations sampling 9 levels of groundwater availability decline and 8 levels of population growth average water use declines are as expected along with average water per capita fig 5a the average consumption only gets below the acceptability threshold of 40 l cap d in the most extreme combinations of groundwater reduction and population growth to trace the frontier between acceptable and unacceptable subspaces linear interpolation is performed for each of the 800 individual response surfaces the average is then recalculated and the exposure space is divided between acceptable and unacceptable average use fig 5b the acceptability gradient mostly follows a constant anisotropy in all tested sub spaces thus in all figures hereafter the acceptable sub space is southeast of the front and the unacceptable sub space is northwest of the front the effect of different policies and interventions can be compared based on the position of their respective fronts though this policy comparison reveals the limitations of evaluating acceptability with an aggregate measure of average water use fig 5 for example shows that the baseline policy b seems to be preferable to the rebalancing policy r since households have a larger average water use under policy b for any given combination of stressors and the sub space that would be evaluated as acceptable under policy b is correspondingly larger the reason for this however is not a better supply situation under policy b rather policy b reflects the highly unequal distribution of piped water supply durations that currently prevails in jordan under the unequal distribution resulting from policy b some households are unable to meet their essential water demands with piped water and have to purchase expensive supplementary water from private vendors while others receive much more water when policy r distributes about the same overall quantity of piped water more equitably more households are satisfied with the amount of piped water they receive and fewer have additional demand for expensive supplementary water purchases this leads to a lower total water use quantity as a result the higher aggregate measure of average water use under policy b seems to indicate that policy b is strictly preferable while in most cases policy r is actually better at meeting households demands as the subsequent analyses show 4 1 percentile based approaches to further explore the disparity of policy preference among jordanian households through the 800 representative agents different aggregation levels are sampled for the two methods presented in section 2 percentile based and generalized mean we first proceed by percentile slicing the percentage of households with insufficient water use is calculated over the exposure space i e every combination of population and groundwater change acceptability fronts are defined by drawing contour plots for specific percentiles of unsatisfied users percentiles are weighted by the number of households that each agent represents thus the 5 line delineates the border of the region in which 95 of the population is satisfied the 10 line is the limit where 90 of households are satisfied etc in fig 6 the alternative policies and interventions are compared based on their acceptability frontiers for different percentiles with the baseline intervention fig 6a the acceptability fronts for different percentiles are widely spaced the 50 front is the median acceptability front where half the households have an acceptable water consumption the 5 front is not visible thus the corresponding share of the population is already in an unacceptable state under initial 2017 conditions gradient slopes slightly change across percentiles indicating that the more vulnerable percentage of households is also more vulnerable to demographic growth while more robust percentiles are more sensitive to groundwater availability decline as they rely more on private water sales and thus private wells another notable feature is that the spread between percentiles under baseline policy can be much wider than the difference between policies b and r based on the average water use while such a difference would have been used to select one option above another in fig 5 the local gradient change for the 25th percentile front southwest corner is due to the response surface of one particular agent which locally becomes the 25th percentile and thus modifies the aggregated front the location of that agent in aqaba governorate means it is more sensitive to groundwater changes than most others and thus shows a different performance gradient the following sub plots compare this baseline policy response with alternative policies and interventions fig 6b shows the effect of the supply and demand rebalancing policy detailed in table 1 on the distance between acceptability fronts for different percentiles it is much more compact than under baseline policy with obvious winners and losers by providing more water to households with lower water use the equalization of supply durations within rationing schedules combined with the raise in minimum regional targets massively expands the acceptable space for the 40 most vulnerable households while it decreases for the median or above this also shows that for moderate levels of stress changes in allocation rules are extremely effective at protecting the most vulnerable households while for higher levels of stress most households fall under unacceptable consumption if no additional resources are added the change in rationing schedule also means supplementary purchases from private vendors are decreased as described for fig 5 b the respective roles played by the rationing schedule and the bulk water allocation targets are further separated and discussed in s i additional results this difference between percentiles is also further analyzed in section 4 3 additional resource interventions at either half 6c or full 6e capacity shift the distribution away from the axis origin current conditions effectively increasing the acceptable space for all percentiles while slightly increasing the spread between them combined policies 6d drastically increase the acceptable space for all household categories as well as reducing the spread between them in the case of the combined policy at full capacity 6f no household reaches unacceptable water use in the sampled exposure space this is also a case where the rebalancing policy through supply duration equalization and increase in minimum bulk water regional supply considerably improves the robustness equity this time compared to the supply enhancement for most household percentiles increasing supply with new projects at half capacity provides a larger acceptable space than the rebalancing policy both interventions provide about the same acceptable space for the 10 percentile for the 5 most vulnerable share of households the rebalancing policy increases the acceptable space further than the new supply policy at half capacity combining policies has a massive effect in expanding the acceptable space for the most vulnerable percentiles while remaining positive for most percentiles a combined policy with new supply at half capacity 6d provides more acceptable space than a full supply expansion policy without rebalancing 6e for at least 25 of the population a more explicit distribution function of the varied responses in the household population can be obtained if the acceptable sub space area is computed for each agent this abstraction can be a loss of information as a single area value can hide varied shapes of acceptability fronts but in the present case the gradients of the acceptability range remain quite similar to quantify the acceptability ranges we use the simplest version of the robustness index ri moody and brown 2013 which is the ratio of the acceptable sub space over the entire exposure sub space the robustness index ri section 2 eq 8 is calculated for each policy and for different values of the aggregation parameters fig 7 shows the quantile function of the ri distribution weighted by the number of real households that each agent represents each distribution corresponds to a given policy or intervention it allows us to quantify the difference between interventions in terms of acceptability space break even points can be identified when comparing interventions to see the percentage of the population that benefits or is penalized by switching policies for example the rebalancing policy strongly increases the ri for the 40 of the population with lower water use and decreases it for the remaining 60 fig 6 shows that a combined policy rebalancing new supply at half capacity was more beneficial than new supply at full capacity for the low consumption households in fig 7 we see the break even point is at 30 thin dash dotted magenta line vs thick dashed blue line break even points can be considered as the boundaries of preference groups shares of the population defined by how they prioritize policies based on the robustness index metric table 2 percentile intervals corresponding to specific preferences can also be found in pair wise tables in the supplementary information appendix additional results table si2 among a sensitivity analysis changing the value of the threshold θ for certain metrics fig 7 also shows the compounding effect of combining policies as noted in yoon et al 2021 with the baseline scenario 60 of the population have a ri below 0 9 the rebalancing policy red line is detrimental in that regard increasing the share to 80 the supply enhancement at half capacity thin blue dashed line lowers the share to 35 combining both thin magenta dash dotted line leads to 0 of the population below 0 9 thus having far more than additive effects compared to the baseline and outperforming the full supply enhancement policy finally we further disaggregate results based on other household characteristics that shape the dynamics that are simulated in particular household income in fig 8 the same percentile based fronts are used for 3 interventions but only for households at the top and bottom 10 of incomes under the baseline policy 8a there is a large spread between households within each income category thus strongly overlapping with the other income group over the exposure space despite a notable difference for example the 75th percentile of poor households dotted red line 75 is roughly as robust as the 50th percentile of rich households solid black line 50 in a similar way to the overall population figures the rebalancing policy erases most of the differences 8b while a new supply policy half capacity 8c shifts the fronts towards higher stress levels without changing the spread further results in supplementary information show the spread of the robustness index for different income deciles and for the different governorates 4 2 generalized mean the alternative way to explore the disparity of the response surfaces is to first aggregate individual water use in a similar way to the mean operator but controlled by a parameter that can take different values for skewness fig 9 shows the use of the generalized mean interventions are compared for an array of values of p that control the skewness of the generalized mean towards lower or higher values as covered in section 2 the parameter p in eqs 5 and 6 can be seen as a continuous cursor between the household with the smallest water use p the arithmetic mean of water use p 1 and the household with highest water use p all particular cases of the generalized mean the acceptability threshold is then simply applied afterwards to divide the exposure space for each value of p results show similar dynamics compared to a percentile based approach the parameter p plays a similar role to the percentile l inferior values of p mean more weight is given to the households with lowest use while high values of p give more importance to the households with higher water use rebalancing reduces the spread between levels of aggregation while new supply shifts the fronts and tends to slightly increase the spread between levels fig 9b shows the effect that a single household agent can have when getting closer to a min operator as one outlier has different front slopes than the others the robustness index ri can similarly be computed for different values of p fig 10 policies can be compared based on their ri representing different social choices a more egalitarian approach lowest values of p favors rebalancing over new supply while a more utilitarian approach p around 1 prefers even the baseline policy over the rebalancing one same result as fig 5 high values of p give more importance to households with high water use which are more indifferent towards policy choice this figure shows a discrepancy with the percentile based method as this time combined policies are preferred in any case again break even points can be used to identify decision specific intervals of p each defined by a given policy ordering table 3 this classification is more abstract than percentages of the population it rather integrates all household consumptions like the arithmetic mean but with varying degrees of skewedness towards those with lowest use or those with highest use aggregation intervals corresponding to specific preferences can also be found in pair wise tables in the supplementary information appendix additional results table si3 among a sensitivity analysis changing the value of the threshold θ 4 3 distribution of water use to better understand the structure of the response surfaces for different segments of the population and the role played by the model dynamics we look at the consumption distribution functions sampled within the exposure space at different levels of stress groundwater availability decline of 15 population growth of 75 fig 11a respectively 30 150 fig 11b one apparent dynamics is that the benefits of supply enhancement policies blue dashed lines are shared unevenly across the population compared to the baseline policy this explains the low preference ranking of these policies on the lower end of the aggregation spectrum for both aggregation methods as new supply follows existing rationing patterns it tends to increase availability within neighborhoods with already good supply duration such policies thus benefit more the upper half of the population in terms of water consumption and is relatively inefficient at increasing the acceptability sub space for the population with the lowest use importantly fig 11 also exemplifies one of the reasons for the compounding effect of combining policies redistributing and increasing the resource pool are more effective combined than alone this is closely linked to the evaluation of policy performance based on the share of a population that is above a given threshold the more vertical is the distribution the more egalitarian is the water use dotted red and dash dotted magenta lines this mechanically makes those policies much more susceptible to resource fluctuations as it leads to very large shares of the population suddenly crossing a threshold in one way or the other thus producing strong non linearities with such metrics most households currently below the threshold generally benefit from more equal supply durations while those above the threshold see their consumption reduced without additional supply there is a point where many households fall below the threshold even though having the policy is still beneficial to the most vulnerable households combined policies benefit at the same time from the somewhat homogeneous shift upwards that added supply brings and from the more egalitarian distribution that lifts low use households much more effectively 5 discussion 5 1 significance even within the same category of water users aggregation choices can lead to different preferences when comparing possible policies in a water system this can be particularly relevant for bottom up methods in water vulnerability assessments as those commonly rely on limited numbers of acceptability thresholds in order to establish policy preferences under uncertainty this study shows how different aggregated response functions can be obtained using a multi agent hydro economic model for large groups of water users by continuously shifting an aggregation parameter either through percentile ordering or generalized mean results not only illustrate that water users within the same category can have differing preferences among a set of possible policies and interventions but also reveal how aggregation choices and thus socio political attitudes towards equity lead to the selection of one course of action over another one advantage the generalized mean has over the percentile based approach is that the generalized mean is affected by the actual value of water use not only its position compared to the threshold with the percentile based approach being slightly below the threshold or having no water consumption at all makes no difference however the generalized mean is also a more abstract method while the percentile based approach provides an explicit distribution of robustness both approaches do produce similar results in terms of how relative preferences change along with aggregation parameters this illustrates how in this given case study the aggregation parameter can matter more than the aggregation method it can also facilitate the interpretability of the generalized mean the jordan water model offers a high level of complexity and detail that allows for the exploration of such robustness distributions across a large number of representative agents and to design policies that modify the demand and the supply distribution apart from the development of new water resources statistical distributions are also affected by the internal dynamics of the model such as the rationing structure or the private water sales allowing them to evolve under stress and further justifying the use of a distributed stress test the disaggregation of the jwm enables analysis on the compounding benefits from combined policies compared to their standalone performance in this case within a bottom up framing the present stress test with its inherent emphasis on a satisfying metric meeting an availability threshold underlines the advantages and drawbacks of changing the slope of the water use distribution through policy it thus partially explains how it can lead to non linear benefits if combined with supply enhancement that shifts the distribution away from the acceptability threshold developing new water resources or reallocating existing ones often represent conflicting narratives with different national or international institutions favoring one or the other hussein 2018 in addition to their standalone or combined benefits the present work further asks for whom exploring how aggregation determines preference with the tested aggregation functions an aggregation parameter can modify the preference ordering for the set of different considered policies once such a divergence is acknowledged society is faced with a trade off between increasing the acceptable space for the most vulnerable households or increasing that for average or median households this exploratory work can inform such discussions and more generally the concept of equity in the face of uncertain change by quantifying such trade offs within groups of similar water users by applying different aggregation metrics managers can identify more effective solutions to reduce vulnerabilities more equally notably by combining new investments with changes in allocation rules in turn informing vulnerabilities for different socioeconomic strata may also facilitate a broader negotiation process to identify acceptable policies in face of deeply uncertain stress when producing a response surface at the scale of a country exploring aggregation ranges also allows one to circumvent loaded narratives about tracing a country s safe space with a single threshold that could be seen as an excessively malthusian perspective at best particularly when water already feeds into internal tensions over migration and jordanian identity mustafa and tillotson 2019 while still considering the tangible benefits of increasing available resources for specific levels of demographic growth disaggregating the notion of acceptable space of the time left for jordan as a whole before reaching some levels deemed as catastrophic shows that such levels will be reached at very different times by different parts of the population and that this is strongly influenced by allocation policies this can help design the most effective solutions to ensure equitable robustness under uncertainty 5 2 caveats and future research the readability of results is here favored by several circumstances and assumptions slopes and gradients can slightly change but the surfaces are still roughly oriented in the same direction for all household agents while the performance indicator itself remains the same for all in other cases the diversity of indicators of relevant stressors of response shapes and gradients as in hadjimichael et al 2020 can make aggregation much more challenging while this study remains a proof of concept the selected indicators the problem framing and their underlying values jafino et al 2021 should receive further scrutiny within an actual policy recommendation paper in particular the selection of the satisfaction threshold can shape the results considerably all the more so for the percentile aggregation method for the scope of this paper we have only considered a few levels of complexity that the jwm can handle leaving many others for future work for example it is highly unlikely that population growth would happen homogeneously over the country or social strata as was assumed in this simple stress test among the notable factors that were not considered but would affect the results income is considered as constant thus effectively assuming a stable null economic growth outside of population changes gdp per capita in jordan could increase 8 times by 2100 according to the ssp2 scenario from the shared socio economic pathways riahi et al 2017 different growth or crisis trajectories would have an impact on many levels of the models such as the ability of households to purchase water from the private sector besides integrating cost benefit analyses and socio political assessments of the tested policies within the present approach should be an avenue for future research the implementation of a cost benefit analysis would require estimated costs on the various intervention strategies including both supply infrastructure and demand management while benefits from the interventions can be estimated via modeling results the benefits analysis could further be enhanced to account for the distributive effects among the population utilizing the aggregation approaches introduced in this work another economic aspect that should be incorporated in a complete vulnerability analysis is the relationship between income and robustness of water use which in this modelization are only partially correlated over the entire sample geographic disparities being an important factor for example here the 10 most vulnerable share of the population in terms of water use does not correspond entirely to the bottom 10 of incomes households with acceptable water use might face other difficulties due to their low income households with average income might have other ways to mitigate a low water availability a related research continuation would be to assess the effects of household s conservation options technical and behavioral on their water vulnerability using the present framework fluctuations in surface water were not considered either as they have limited impact on direct drinking water supply however there would certainly be an influence through changes in the agriculture sector and the effect on mobile providers and while this study focuses on the household sub system a complete multi sectoral assessment should include agriculture with climate change as an additional stressor and rural to urban transfers as an additional policy these limitations highlight the trade offs and complementarity between a narrative scenario focused forward approach as originally used with the jwm and the present inverse stress test approach the bottom up method can identify the exact levels of stress from a few variables that would lead to unacceptable performance independently of time or without needing a mechanistic explanation to reach such levels however one of the challenges in applying a bottom up sensitivity framework to a large group of water users is that it requires a binary outcome acceptable unacceptable water use excluding information on the magnitude of the deficit besides if more variables were to be considered even after a preliminary selection of the most impactful ones a bottom up assessment would quickly run into a curse of dimensionality not only in terms of computational resources where each added dimension increases the number of simulations by an order of magnitude but also in terms of visualization for policy makers besides the non temporal stress test also precludes analysis of path dependent dynamics which are particularly important in the jordanian case e g groundwater depletion it is important to note that there is a likely degree of dependence between the stressor variables more population at any given time may prevent curbing groundwater abstractions and lead to a reduced availability later or a collapse in water availability could have dire economic impacts and lead to emigration for the present experiment under a deep uncertainty assumption we choose to apply a veil of ignorance on the relative likelihood of stressor combinations but further weighting could be applied to the response surfaces based on trustworthiness of future scenarios while the conceptual simplicity of the stress test is convenient for use with a complex model it should be viewed as complementary to other decision frameworks such as adaptive planning haasnoot et al 2019 future avenues of research would also involve using this framework with the option to screen more intermediate degrees of intervention in order explore trade offs more methodically and strategically design new policy portfolios that target specific robustness and equity outcomes 5 3 conclusions this study explores the effect of aggregation choices on water vulnerability assessments that rely on response surfaces when applied to a large number of water users to do so it relies on a dynamic multi agent model of the jordanian water system and tests combinations of supply enhancement and distributional policies under groundwater decline and population growth response functions are aggregated with percentile targets or generalized mean by relating the acceptable share of the exposure space to an aggregation parameter this work illustrates how the safe range provided by different supply enhancement and rebalancing polices depends on aggregation assumptions but also allows one to identify specific ranges of aggregation and thus social choices that lead to each different policy preference ordering the proposed methodology can be used to quantify the benefits of more equitable policy design under a deep uncertainty framework in the case of jordan different policy portfolios have different equity implications and changes in allocation and rationing patterns can be particularly effective to equitably reduce water vulnerabilities this exploratory work provides a proof of concept for more theoretical frameworks to define distributed freshwater security and thus formulate equity and trade offs within a given type of water user in the face of deeply uncertain changes credit authorship contribution statement thibaut lachaut conceptualization methodology model development formal analysis writing jim yoon model development writing christian klassert model development writing amaury tilmant conceptualization supervision writing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank staff in the jordanian ministry of water and irrigation mwi water authority of jordan waj jordan valley authority ministry of agriculture and department of statistics for provision of data and reports for the analysis we are particularly grateful for support provided by dr hazim el naser ali subah and susan kilani at mwi and refaat bani khalafat at waj we also thank steve gorelick for bringing comments and advice to this paper samer talozi yazzan haddad marwan shamekh and nicolas avisse for the help extended in jordan as well as all the contributors to the jordan water model we express our gratitude to david rosenberg and three anonymous reviewers for their constructive comments that greatly improved the paper additional data and information were provided by the united states geological survey and the united states agency for international development the economic research forum and the jordanian department of statistics granted the researchers access to relevant data after subjecting data to processing aiming to preserve the confidentiality of individual data the researchers are solely responsible for the conclusions and inferences drawn upon available data the authors acknowledge the financial support of the natural sciences and engineering research council of canada nserc through grant g8pj 437384 2012 this work was also supported by the us national science foundation nsf under grants geo oad 1342869 and icer ear 1829999 as part of the belmont forum sustainable urbanisation global initiative sugi food water energy nexus theme funding to the helmholtz centre for environmental research ufz was provided by the deutsche forschungsgemeinschaft dfg kl 2764 1 1 and the german federal ministry of education and research bmbf 033wu002 as part of the belmont forum any opinions findings and conclusions or recommendations expressed in this material are solely those of the authors and do not necessarily reflect the views of the nserc nsf dfg and bmbf appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j advwatres 2022 104311 appendix a supplementary data the following is the supplementary material related to this article mmc s1 policy design effect of income and location on r i sensitivity analysis on θ and gini coef response functions 
94,the control of multi reservoir hydropower systems is a crucial cogwheel in the current transition of power systems towards renewable energy in particular in northern regions where inflows vary enormously through the year the satisfaction of storage constraints cannot be ensured at all times we thus add chance constraints to the underlying markov decision process problem and solve it with a reinforcement learning policy gradient approach backoffs common in other areas of optimal control are introduced to better manage the numerous chance constraints as a single joint constraint stochastic dynamic programming sdp is used as a benchmark approach we show numerically that our approach can deliver high quality policies and just as importantly that a three reservoir system is solved in proportionally little more time than a one reservoir system keywords reservoir management hydropower reinforcement learning policy gradient chance constraint backoffs data availability the data that has been used is confidential 1 introduction hydropower generation is a key part in the worldwide effort to move energy systems towards renewable sources and away from ghg emissions reservoir based hydro is not only a major source of power its flexibility is central to the large scale introduction of intermittent renewable power and often pivotal to account simultaneously for multiple purposes such as irrigation recreational uses and urban water supply salazar et al 2017 compounding this role is the sheer number of new and planned reservoirs over 3700 of them in the world according to zarfl et al 2019 in this context there is more than ever a need to improve hydropower optimization techniques that handle multiple decisions i e reservoirs and multiple sources of information reinforcement learning rl techniques hold rich promises in this direction recent review papers on hydropower have established the most relevant features the solution techniques and research directions e g the need to abide environmental constraints the challenge of multiobjective problems and the relevance of new optimal control approaches including rl dobson et al 2019 macian sorribes and pulido velazquez 2019 giuliani et al 2021 the literature on hydropower is vast ranging from refined optimization techniques to societal impacts and game theoretic studies of water sharing between countries in the very recent years research topics include the identification of relevant inputs to the optimization models especially under climate changes crochemore et al 2020 dang et al 2020 libisch lehner et al 2019 mulligan et al 2020 the integration of hydropower and intermittent renewable energy sources xu et al 2019 multi objective optimization models zaniolo et al 2021 applications of reinforcement learning xu et al 2020 2021 hydropower management is characterized by a stochastic environment with spatially and temporally correlated inflows the most common way to address this type of problem is to model its uncertainties and recourse actions and solve it with stochastic programming or dynamic programming or one of their numerous variants see the reviews just mentioned and a few details below the uncertainties can also be dealt with by robust optimization the focus being on finding an initial decision that is robust to future outcomes not on modeling the recourses and multistage uncertainties see for example giuliani and castelletti 2016 and xu et al 2022 for systems with few reservoirs the stochastic optimization approach of choice is stochastic dynamic programming sdp nandalal and bogardi 2007 davidsen et al 2015 finding an optimal policy requires the discretization of the state space the algorithm then proceeds in a single pass backward through time to compute the value function of each grid point the approach suffers from the curse of dimensionality the computational effort increases very rapidly with the number of reservoirs and grid refinement to mitigate this issue other approaches have been studied such as sampling stochastic dynamic programming côté and arsenault 2019 and stochastic dual dynamic programming gjelsvik et al 2010 but they still have some limitations giuliani et al 2021 alternatively a group of optimal control approaches that is receiving increasing attention relies on reinforcement learning unlike sdp an optimal policy is built iteratively through several forward in time passes that rely on simulations this iterative process allows the identification of good if not optimal policies hence the oft heard expression approximate dynamic programming q learning is one such approach it still requires the discretization of the state space to evaluate a value function although some papers report better behavior against the curse of dimensionality lee and labadie 2007 castelletti et al 2010 xu et al 2020 2021 q learning and its variants such as deep q network cannot learn stochastic policies they require the discretization of the action space and are in general subject to instabilities and divergences sutton and barto 2018 another class of rl techniques called policy gradient directly update a parameterized policy sutton et al 1999 policy gradient avoids the state and action space discretization and can significantly mitigate the curse of dimensionality a main challenge in reservoir operations is the satisfaction of constraints given the major influence of stochastic factors in fact the volatile nature of inflows often render physically impossible the satisfaction of storage constraints it is then most appropriate to explicitly enforce the satisfaction of constraints probabilistically by so called chance constraints to the knowledge of the authors the optimization of reservoir operations under chance constraints has been addressed for example to handle the risk of floods but as far as we know not with true multi stage models i e with explicit recourses see the references in macian sorribes and pulido velazquez 2019 the most recent of which being xu et al 2017 chance constraints are difficult to enforce directly even more so in a dynamic setting penalizing the objective function according to constraint violations is a common approach but it is unwieldy to penalize each constraint separately taking strong cues from process engineering papers koller et al 2018 paulson and mesbah 2018 we introduce so called backoffs backoffs allow a good individual treatment of each chance constraint without adding much computational burden this paper contributes to the hydropower management problem on two fronts through detailed computational tests between a benchmark sdp method and an rl policy gradient approach we show that the latter provides excellent quality solutions and most importantly that the computational effort increases only slightly when the number of decisions increases from one to two and three interestingly and even though this was not set as a goal in itself the rl policy brings substantial reductions in water shortages and spills when compared to the benchmark policy our second contribution is to show that chance constraints and backoffs form a natural way to handle storage constraints after this introduction section 2 provides our formulation of the reservoir management problem under chance constraints section 3 presents how backoffs can enforce the chance constraint within our policy gradient method framework in section 4 we describe our sdp benchmark approach case study is presented in section 5 adjustment of the parameters of the rl and sdp algorithms are discussed in section 6 we compare the sdp and rl policies in section 7 section 8 concludes the paper and discusses future work 2 modeling of the hydropower problem in this section we present the formulation of the joint chance constrained problem the operation of a hydropower system can be represented with a markov decision process mdp in discrete time where the objective is to minimize the expected cost from the time t 0 to the time horizon t t which is one year throughout the paper the number of reservoirs is r we define period t as the time elapsed between t and t 1 the mdp is defined by a tuple s u p r to which we add constraints below the set s includes all state variables specifically this represents the endogenous state variables s t s 1 t s r t r r which correspond to the water in each reservoir at the time step t the exogenous state variable v t r which corresponds to a snow water equivalent swe measure it is a measure of the snow and ice layer which of course transforms into water when temperature rises in the summer côté et al 2011 show that swe is a good estimate of future inflows the exogenous state variables q t q 1 t q r t r r representing the natural inflows to each reservoir over the period t the set u defines the decision variables u t u 1 t u r t r r giving the amount of water released from each reservoir over the period t p is the transition function at the time step t 1 the endogenous state variables are updated by the following dynamic law 1 s t 1 p s t u t q t s t q t c u t where c is the connectivity matrix appropriate to the reservoirs topology r s t u t q t is the reward function giving the amount of electricity produced in gigawatt hours for all plants over the period t starting with reservoirs levels s t applying decisions u t and with inflows q t let us be clear on information time and thus anticipativity in our model when making a decision at time t the measure of swe v t is simply that which is available at that moment although it is an indicator of future inflows from snowmelt it is in no way anticipative in the stochastic sense inflows are associated to a time period of course not a time point and we assume that the information q t for the time period that starts at t are available at t this inflow information is thus anticipative over the duration of one period we use three day steps this practice is common in the literature and relies on the fact that good predictions of inflows are normally available especially over short periods of time up to several days of course the same information set is provided to the benchmarking sdp and rl approaches we aim to find an operating policy π which takes as input the time t and the three state variables s t v t and q t and provides as output the decision variables u t due to the uncertainty of the inflows the objective is to maximize under constraints the expected sum of rewards over the horizon also called the expected return 2 e t 0 t 1 r s t u t q t to introduce the constraints we denote respectively the sets u t and s t for the decisions and the reservoir levels 3 u t u t r r u r t u r t u r t r 1 r s t s t r r s r t s r t s r t r 1 r where u r t and u r t define the lower and upper bounds on the releases for reservoir r at time t and s r t and s r t are the lower and upper bounds on the storage level for reservoir r at time t we will represent with s the set of all sets s t for t 1 t on one hand we consider that the physical properties of the turbines have to be respected at each time step of the year thus the constraints on the releases u t are defined as hard constraints on the other hand hard turbine constraints and the uncertainty of the inflows mean that it may be impossible to satisfy the storage constraints s t at each time step so the storage constraints are considered to be soft the objective is to determine a policy that jointly satisfies these constraints over the horizon with a probability of 1 α 4 f s p t 1 t s t s t 1 α wrapping up our hydropower optimization problem p can be formulated as follows p max u 0 u t 1 e t 0 t 1 r s t u t q t s t u t π s t v t q t t t 0 t 1 s t 1 s t q t c u t t 0 t 1 u t u t t 0 t 1 p t 1 t s t s t 1 α 2 1 a note on exogenous variables and data let us discuss briefly the exogenous variables we use two exogenous variables the inflows which incidentaly are also necessary for state transitions and a snow water equivalent measure each provides a different set of information the inflows being useful for the near future the swe being longer sighted see desreumaux et al 2014 for example importantly we assume that time series simulations of the pair v t q t for t 0 t 1 are available typically the simulations are drawn from a hydrological model not strictly historical so that their number is not limited we assume a set of n simulations to be used in policy building the same set for the rl and benchmark approaches we also assume to have a distinct set of n oos out of sample simulations drawn from the same statistical model of course these are strictly used for policy evaluation 3 reinforcement learning with chance constraints this section presents our reinforcement learning policy gradient approach in fact we will have two distinct rl approaches denoted plain penalty rl and backoff rl sections 3 1 to 3 5 present the policy building while the short section 3 6 discusses policy evaluation in section 3 1 we present policy gradient more specifically the reinforce approach ignoring for the moment the chance constraint in problem p the so called baseline technique is also introduced section 3 2 discusses how the chance constraint can be enforced by the policy gradient algorithm by penalizing the objective and explains how the chance constraint can be approximated with empirical distributions the section ends with the plain penalty rl approach the next section 3 3 introduces backoffs an alternative way to enforce constraint satisfaction which improves on the approach of section 3 2 section 3 4 continues to explain how the backoffs can be adjusted within the policy gradient approach to form the backoff rl approach section 3 5 provides both conceptual and numerical implementation details to complete the description of our method for an excellent textbook introduction to reinforcement learning we refer the reader to sutton and barto 2018 3 1 policy building with rl the policy gradient approach reinforcement learning is one of the three main paradigms of machine learning with supervised and unsupervised learning in opposition to classical dynamic programming which proceeds backward in time reinforcement learning is an iterative method that improves the policy with successive time forward passes 3 1 1 reinforce a policy gradient approach there are many ways to represent a policy within the rl paradigm we only discuss our own choice here first the policy will be stochastic a probability distribution is associated with each action at each state and time second our policy will be represented by a neural network essentially a parameterized function which takes states and time as input and which provides action probabilities as output specifically our policy writes out as the probability of choosing water releases u t given the current state 5 π θ s t v t q t t p u t s t v t q t t θ n μ σ the neural network s parameters are denoted by θ its output are the mean μ and variance σ of the distribution the dimension of the output scales with the number of decisions at time t e g for the three reservoir case σ is 3 3 the choice of a gaussian distribution is very common we need to assume that the policy π θ is continuously differentiable with respect to its parameters θ common neural networks all have this property throughout this section the objective function of problem p is replaced by the discounted expected return over the time horizon which is denoted as follows 6 j π θ e τ π θ t 0 t 1 γ t r s t u t q t here the episode τ concisely represents the succession of events 1 1 the expression trajectory is also common in the hydropower literature τ s 0 v 0 q 0 u 0 s 1 v 1 q 1 u 1 s t 1 v t 1 q t 1 u t 1 s t where the decisions are made following the stochastic policy π θ u t π θ s t v t q t t note that τ is a random variable as both the decisions and the future states are uncertain the discount factor γ is introduced for the sake of the reinforcement learning it indicates how much the agent cares about immediate rewards versus future ones with γ equal to 0 the agent is myopic because it relies only on immediate rewards as γ approaches 1 the agent becomes more far sighted because future rewards are given more consideration the policy gradient approach for problem p without the chance constraint is essentially a gradient ascent method adjusting the vector θ that parameterizes the policy by taking steps in the direction of the gradient of the objective function θ j π θ from the policy gradient theorem sutton and barto 2018 the said gradient can be proved to be 7 θ j π θ e t 0 t 1 θ log π θ s t v t q t t r τ t where we define the return following t r τ t t t t 1 γ t r s t u t q t we implement a version of the reinforce policy gradient update that is slightly different from the original in williams 1992 the policy update i is decomposed in three steps first a sample episode is generated using the current policy π θ i and a time series simulation of the pair v t q t t 0 t 1 picked from the set of n simulations cf section 2 1 second the gradient θ j π θ i is estimated by replacing the expected value in 7 with the sample third we update the policy parameters using the gradient based adam method of kingma and ba 2014 θ i 1 a d a m θ i θ j π θ i λ where λ is a stepsize of course the gradient estimate changes with the sample episode to stabilize it and improve the quality of the policy update a mini batch approach is often used whereby several episodes are generated instead of one the state variables change but not the policy i e not θ i and the update is done with the average of the single episode gradients if the mini batch size is n mb and we denote the sample episodes with τ j j 1 n mb then the gradient estimates are 2 2 these estimates are replaced by a version with baseline in the next section θ j j π θ i t 0 t 1 θ log π θ i s t v t q t t r τ j t and the update is 8 θ i 1 a d a m θ i 1 n mb j 1 n mb θ j j π θ i λ note that with time forward passes it is easy to ensure the respect of the state transition equations and decision constraints u t u t we come back to storage constraints s t s t in section 3 2 on a more technical level let us mention that the automatic computation of the gradient θ log π θ s t v t q t t is a standard feature of all neural networks libraries 3 1 2 variance reduction with a baseline the variance associated to stochastic gradients impedes the algorithm s progress towards an optimum beyond mini batches a common variance reduction approach is to subtract a baseline b from the return the baseline does not impact the expectation of the update step only its variance θ j j π θ i t 0 t 1 θ log π θ i s t v t q t t r τ j t b τ j t slightly abusing notation we do not reflect the presence of the baseline in the expression θ j j π θ i the baseline is always present beyond this point one of the simplest baseline is the average return taken over several monte carlo simulations williams 1992 desreumaux 2016 suggests that an alternative choice provides better performances in a setting similar to ours he notes that our stochastic policy 5 can be decomposed in a deterministic part based on μ and a stochastic part based on σ the author defines a deterministic policy such that the actions rely only on μ i e with zero covariance matrix we denote τ π and τ μ the episodes associated to the stochastic and deterministic policies 3 3 their dependence on θ is not displayed for the sake of readability then as suggested by the author we use as baseline the return associated to the deterministic policy and the gradient for each of the mini batch sample episode is evaluated as 9 θ j j π θ i t 0 t 1 θ log π θ i s t v t q t t r τ π j t r τ μ j t the gradient with baseline 9 is used in the update step 8 in all our numerical experiments 3 2 the chance constraint and the plain penalty rl approach we have so far ignored the joint chance constraint in problem p which combines all upper and lowerbound storage constraints into a single constraint the event that all storage constraints s t s t be satisfied at every time step of one year recall that in an reinforcement learning approach decision constraints u t and state transition constraints can quite naturally be satisfied step by step time forward constraints on the endogenous storage states s t are a more complicated business altogether since the current decision can affect the feasibility of states at a later point in time 4 4 in general the current decision can also affect the feasibility of future decisions but of course our model avoids this pitfall with storage constraints that are soft there always exists a feasible decision eventually at the expense of storage constraints a further complication is that the chance constraint 4 sees no analytical expression to its underlying cumulative distribution function f s let us tackle this second issue before we deal with the first one that of introducing the chance constraint into reinforcement learning one simple choice is to consider the empirical cumulative distribution function f s a non parametric approximation of the underlying c d f 10 f s f s 1 n k 1 n 1 t 1 t s t k s t where s t k corresponds to the realized storage values under the current policy π θ and the estimation is done over the n available simulations 5 5 this is a choice fewer than n simulations could be used this simple approximation is improved later see section 3 5 back to our first issue let us rewrite all storage constraints as conventional inequalities of the format g s 0 with the definitions r 1 r t 1 t 11 g r t 1 s r t s r t s r t and g r t 2 s r t s r t s r t the usual technique to attain constraint satisfaction when direct enforcement is not possible is to penalize the original objective function j π θ with the constraints violations 12 j π θ κ t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t 0 where κ is a penalty factor the gradient of eq 12 associated to a simulated episode τ is now expressed as 13 t 0 t 1 θ log π θ s t v t q t t r τ π t r τ μ t where the penalized return following t is r τ t t t t 1 γ t r s t u t q t κ t t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t 0 a policy π θ maximizing this penalized objective for a fixed κ and under the constraints on decisions u t and state transitions 14 max θ j π θ κ t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t 0 s t u t π s t v t q t t t 0 t 1 s t 1 s t q t c u t t 0 t 1 u t u t t 0 t 1 can be found with policy gradient steps as described above the algorithmic procedure corresponds exactly to calling algorithm 1 with null backoffs for a sufficiently high penalty level κ an approximate solution to the original problem p can be found but the production level will be hindered by a too high penalty the plain penalty rl approach we dub plain penalty rl the approach of launching algorithm 1 with null backoffs and various penalty levels κ until a smallest κ is found that ensures the chance constraint satisfaction in the following sections we propose an alternative rl approach that better adapts to each constraint 3 3 reinforce with fixed backoffs we first introduce the idea of using backoffs in the storage constraints and then provide the algorithm reinforce with fixed backoffs which is the cornerstone of both our plain penalty rl approach section 3 2 and our backoff rl approach section 3 4 3 3 1 introducing backoffs in the storage constraints in optimal control theory backoffs are scalars one per inequality constraint which artificially tighten or relax the original feasible set s t in our case see for example koller et al 2018 and paulson and mesbah 2018 let us define the backoffed feasible sets 15 s t b t s t g r t ℓ s r t b r t ℓ 0 r 1 r and ℓ 1 2 where each constraint is altered by its backoff b r t ℓ and b t is the vector of all backoffs at time t this generalizes the original constraint sets as s t 0 s t to generalize the set s we write s b for the set of all s t b t with a vector b that contains all the backoffs b t t 1 t the purpose of backoffs is to modulate the level of satisfaction of the original constraints by pretending that they are tighter or slacker than they really are given our lumping together of all storage constraints into one chance constraint and one penalty parameter backoffs bring an supplementary level of control that is not available with the plain penalty approach of section 3 2 to integrate backoffs in a reinforce algorithm which we do next let us generalize our previous definitions to account for the backoffs with the two parametered objective function 16 j π θ b j π θ κ t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t b r t ℓ 0 and the gradient θ j π θ b associated to the simulated episode τ is 17 θ j π θ b t 0 t 1 θ log π θ s t v t q t t r τ π t b r τ μ t b where the penalized backoffed return following t is the three parametered r τ t b t t t 1 γ t r s t u t q t κ t t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t b r t ℓ 0 naturally j π θ 0 matches the objective function of the plain penalty approach 12 r τ t 0 corresponds to r τ t and θ j π θ 0 matches the gradient computation 13 3 3 2 the algorithm reinforce with fixed backoffs we have developed so far a reinforce algorithm section 3 1 with baseline section 3 1 2 and the chance constraint section 3 2 adding the idea of backoffs section 3 3 1 we obtain algorithm reinforce with fixed backoffs which is the cornerstone of both our plain penalty rl approach and backoff rl approach for a fixed penalty level κ and set of backoffs b the algorithm provides an approximate solution to the problem 18 max θ j π θ b s t u t π s t v t q t t t 0 t 1 s t 1 s t q t c u t t 0 t 1 u t u t t 0 t 1 let us point out that when policy updates are based on a stochastic gradient especially with a fixed step size the last policy π θ i may not be the best policy that has been generated thus throughout the update iterations we evaluate the joint constraint satisfaction level and average production among the policies that attain the required satisfaction probability we keep that with the best production see steps 6 to 11 of algo 1 if no policy satisfies the chance constraint the procedure returns the last policy also to reduce the computational effort it is advisable not to perform these steps 6 to 11 at every iteration in our numerical tests every 250 iterations was a good enough interval the pseudocode is as follows note that an adequate initial policy is automatically computed by the neural network library based solely on the network s features 3 4 the backoff rl approach this section explains how a joint chance constraint and backoffs can be worked into a rl algorithm that offers better performance than the plain penalty approach of section 3 2 the main fact that guides us is that just as it would be computationally unrealistic to adjust a penalty parameter for each storage constraint separately i e having multiple κ s in 16 it is not feasible to optimize each backoff value independently what is possible is to generate initial backoffs one for each storage constraint that are adequately enough sized with respect to their respective constraint violations and then unidimensionnaly scale these initial backoffs so as to fulfill the joint chance constraint at level 1 α the steps of the backoff rl approach are as follows the backoff rl approach step 1 identify an initial policy π θ init without backoffs run algorithm 1 with a loosely set κ penalty level and null backoffs effectively solving problem 14 and denote the resulting policy π θ init 6 6 notationally it is necessary to clearly distinguish this θ init from the θ i iterates of algorithm 1 especially its θ 0 importantly this policy needs not ensure the satisfaction of the chance constraint with precision the backoff adjustment procedure will handle this step 2 identify initial backoffs using policy π θ init initial backoff values b r t ℓ init are computed for all constraints of reservoir p time period t and type ℓ initial backoffs essentially gage the constraints violations one with respect to the other frequently violated constraints are assigned higher initial backoffs this computation relies on quantiles of the violation levels of each constraint step 3 adjust the backoffs backoffs must be adjusted to their smallest values that lead to joint chance constraint satisfaction the adjustment procedure is unidimensional the initial backoffs b r t ℓ init are scaled with a single parameter β i e to β b r t ℓ init which is adjusted by a bisection approach each bisection iteration entails a call to algorithm 1 with b β b r t ℓ init and the obtained policy π θ is used to update β based on the satisfaction of the chance constraint the bisection is applied to an interval β β with β 0 to ensure that backoffs remain backoffs and an upper bound chosen ad hoc high enough that the chance constraint satisfaction is attainable note that as initial policy π θ 0 each call to the algorithm 1 can use either the library s default policy or the policy π θ obtained at the previous iteration step 1 above is nothing but the application of the plain penalty rl approach explained in section 3 2 and we have no more to say other that we come back to the loosely set κ idea in the numerical results section 6 1 step 2 requires more details which are provided in section 3 4 1 step 3 is sufficiently simple that all necessary details were provided above 3 4 1 details of step 2 finding initial backoffs the initial backoffs b r t ℓ init are the starting point of our backoff adjustment procedure but they are also our only device to penalize one constraint more than another they are computed as follows with the resulting policy π θ init from step 1 we can generate n episodes 7 7 again this is a choice fewer than the full set of n available simulations could be used from these episodes we can extract a set of storage constraint values g r t ℓ s r t 1 g r t ℓ s r t n r t ℓ from which we can derive an empirical distribution for each r t ℓ constraint denoted 8 8 to avoid an overload of indices the empirical c d f s notation does not display their dependence on the underlying policy this was also the case for the empirical joint distribution eq 10 19 f r t ℓ b r t ℓ 1 n k 1 n 1 g r t ℓ s r t k b r t ℓ 0 let us denote by f r t ℓ 1 1 α the 1 α quantiles of these empirical distributions setting the initial backoffs with these quantiles b r t ℓ init f r t ℓ 1 1 α seems a good idea at first the higher the quantile the higher the backoff and generating a policy with these backoffs might yield storage levels that approximately satisfy each constraint at the level 1 α it would hopefully amount to shifting the distribution of the storage levels just the right amount but this hope ignores cross effects between constraints more importantly yet the joint constraint satisfaction is our goal and a 1 α level for each constraint will be insufficient to reach the 1 α level for the joint constraint a better idea is then to base the initial backoffs on a more stringent quantile f r t ℓ 1 1 δ of each of the constraints empirical c d f s with δ α of course furthermore we found that using the quantile on its own can have unpleasant side effects 9 9 the quantile minus mean backoff has the following advantages i the backoff is always positive so that no constraint will be inadvertently relaxed ii the backoff naturally adapts to the dispersion of the storage values wider distributions get larger initial backoffs which is intuitively right so that we subtract from it the empirical mean of the constraint values g r t ℓ 1 n k 1 n g r t ℓ s r t k the initial backoffs are then f r t ℓ 1 1 δ g r t ℓ for a single common quantile level 1 δ chosen such that 20 b r t ℓ init f r t ℓ 1 1 δ g r t ℓ f s b init 1 α where we use the backoffed set notation s b init introduced at the beginning of section 3 3 1 and where b init is simply the vector of all the initial backoffs b r t ℓ init identifying the largest δ that satisfies the implication 20 is an easy one dimensional root finding task with negligible computation time at this point the backoffs b r t ℓ init are fixed and remain so until the end since no policy was actually derived and tested with these backoffs i e by running algorithm 1 with them they may well not allow the satisfaction of the joint chance constraint nor ensure the best production adjustments are required hence the step 3 3 5 algorithmic details of the rl implementation in this section we provide some implementation details on the rl approaches concerning the time representation and a better approximation of the chance constraint 3 5 1 time representation an episode represents the simulation of a year and therefore corresponds to a full seasonal cycle the water available at the end of a year is also the initial water level of the following year it is important to provide this information on the periodicity of a year to the policy otherwise there will be no incentive to conserve water at the end of the year and the reservoirs will be emptied to maximize production in this perspective nilsson et al 2006 proposed to use the trigonometric representation of a circle to describe the temporal and cyclic evolution of a year the idea is to convert the original time steps into corresponding sine and cosine values plainly put scalar time t maps uniquely to the pair representation sin 2 π t t cos 2 π t t 3 5 2 a better representation of the chance constraint the empirical cumulative distribution function f s from section 3 2 is itself a random variable and we would like some level of reliability on the satisfaction of the chance constraint p f s 1 α note that as the indicator function underlying f s see eq 10 is a random variable following a bernoulli law it means that f s follows a binomial law b n f s divided by n from the relations between the binomial and beta laws clopper and pearson 1934 a lower bound f l b of f s can be estimated with a confidence level of 1 ϵ such that p f s f l b 1 ϵ where f l b is equal to the inverse of the beta c d f with parameters n f s and n n f s 1 at level ϵ then if the lower bound f l b is greater than the required probability 1 α we can assume that the chance constraint is satisfied with a level of confidence of 1 ϵ 21 f l b 1 α p f s 1 α 1 ϵ therefore when running algorithm 1 f l b 1 α rather than f s 1 α is checked to assess whether the chance constraint is satisfied 3 6 rl policy evaluation by policy evaluation we mean a measurement of the quality of a policy for the purpose of comparison with other policies as noted in section 2 1 all policy evaluations are done on a set of n oos simulations of the exogenous variables that is completely distinct from the simulations used in building the policies the same policy evaluation procedure applies to both the plain penalty rl and backoff rl approaches and is extremely simple the policy is used to determine actions by using only the deterministic part of the policy ignoring the variance covariance component in 5 for all n oos simulations the average production is then used as metric to compare policies between each other 4 the sdp benchmark we use a stochastic dynamic programming sdp approach to benchmark our numerical results all the elements of our policy building and policy evaluation are concisely provided below 4 1 policy building with sdp our goal of course remains to solve the optimization problem p sdp proceeds in the classical manner of finite horizon dynamic programming as follows for each point in time t and each point of a grid of the storage level endogenous variable s t and the snow water equivalent swe exogenous variable v t we find the best decision to maximize the expected profits until the end of horizon note that we do not build a grid of the inflows exogenous variables q t see section 4 5 on this relying on the bellman optimality principle we can solve these maximization problems by proceeding recursively from the end of horizon at each time t using the already found t 1 solutions mathematically we solve for each point of a storage swe grid 10 10 note that s t k refers to grid values in opposition to the notation of section 3 s t k v t ℓ k 1 k ℓ 1 l and for t t 1 t 2 0 the subproblem 22a max u t f t s t k v t ℓ e v t 1 q t v t ℓ r s t k u t q t f t 1 p s t k u t q t v t 1 22b s t p s t k u t q t s t k q t c u t 22c u t u t solving all subproblems back to t 0 produces a set of k l optimal solutions one for each point of the storage swe grid for each point in time note that the computation of f t 1 above requires numerical interpolations since the function is not being evaluated at grid points with one reservoir this is two dimensional interpolation with three reservoirs a four dimensional interpolation we tackle the missing parts in the next sections computing the conditional expectation finding an end of horizon starting point taking into account the until then missing chance constraint s t 1 k s t 1 and finally evaluating the quality of a policy 4 2 conditional expectations in sdp the treatment of the conditional expectation in 22a is an important feature of the various sdp methods we proceed as follows we can approximate the conditional expectation with the help of one snow water equivalent level v t 1 and m scenarios i e m possible values for the inflows q t to all reservoirs and their associated conditional probability conditioning is always on v t ℓ of course the scenarios of q t will be denoted q t scen m and v t 1 will be denoted v t 1 scen recall from section 2 1 that time series of pairs v t q t are available for policy building regression models are easy to build from these series and v t ℓ conditional scenarios can be generated as follows for the inflows for each reservoir p regress the q r t values on the corresponding v t values for a simple model linear or quadratic fit the residuals on a normal distribution and using the inverse c d f applied to a uniform grid on 0 1 generate a set of m equiprobable deviations from the model representative equiprobable inflows q t scen m given the grid value v t ℓ are then simply the scalar inflow value taken from the regression model read at v t ℓ of course plus the vector of m deviations for the swe exovar we regress our simulated v t 1 values on the corresponding v t values for a simple model linear or quadratic then the value v t 1 scen can be read directly from the regression model given the value of the grid v t ℓ 11 11 note the slight abuse of language where q t scen m and v t 1 scen do not show their dependance on the grid level ℓ clearly alternative avenues are possible to treat this conditional expectation but in our experience the approach above strikes a good balance between effort and effectiveness adapting the notation in a natural manner with f for f and a new s t k m the time t subproblem 22c can then be approximated by 23 max u t f t s t k v t ℓ 1 m m 1 m r s t k u t q t scen m f t 1 s t 1 k m v t 1 scen s t s t 1 k m s t k q t scen m c u t m 1 m u t u t in our numerical tests using a half dozen scenarios for q t scen m was enough to stabilize the optimal solution we fixed m 10 throughout all experiments to be on the safe side 4 3 end of horizon values the backward in time recursion of problem 23 requires end of horizon f t values we follow the suggestion of tilmant et al 2002 to account for the yearly seasonality of the exogenous variables essentially run the algorithm repeatitively pluging the f 0 values as f t values for the next run stop when a fixed point is attained meaning that the end of horizon values change little enough that the policy remains stable from one run to the next in various case study tests three runs were always sufficient all numerical experiments were then performed with three runs 4 4 the chance constraint in sdp the storage constraint still needs to be integrated in problem 23 we take the simple and usual approach of penalizing the objective f t with the constraints violations 24a max u t f t s t k v t ℓ κ 24b s t s t 1 k m s t k q t scen m c u t m 1 m 24c u t u t where f t s t k v t ℓ κ note the new third parameter is defined as 25 f t s t k v t ℓ κ 1 m m 1 m r s t k u t q t scen m f t 1 s t 1 k m v t 1 ℓ κ κ r 1 r max s r t 1 s r t 1 k m 0 max s r t 1 k m s r t 1 0 where κ is a penalty parameter f t and by consequence the subproblems and the complete problem are now parameterized with κ as before we need to set the penalty no higher than what is needed to ensure satisfaction of the chance constraint as high penalties impede production as a result the chance constrained sdp approach for a fixed penalty level κ is as follows the sdp approach step 1 solve problem 24a for each time step backwards from t t 1 to t 0 do so three times using f t 0 on the first run and using the most recent f 0 as f t for the second and third run step 2 evaluate the chance constraint satisfaction in step 1 the motivation of the three runs was provided in section 4 3 in step 2 the evaluation of the chance constraint satisfaction follows the same procedure as for the rl method including the lower bound that was described in section 3 5 2 estimate f s by solving problem 26 for n simulations see next section compute the lower bound f l b and check whether f l b 1 α the sdp approach is to be run with various κ until the smallest value that allows satisfaction the chance constraint has been found our description of this search for κ is loose because in all numerical experiments we assumed that κ was provided i e the run times do not account for the effort in finding the right κ 4 5 sdp policy evaluation the policy building procedure of sections 4 1 to 4 4 ends with for each time point and each point of the storage swe grid the optimal value functions f t s t k v t ℓ and the optimal releases u t s t k v t ℓ the quality of a policy is evaluated by running it in a time forward manner in sdp at least two approaches can be used to compute the optimal decision u t associated to the observed state values s t v t not grid values and inflows q t the first approach is to interpolate u t from the closest grid points and their associated releases it does not rely on the inflows q t the second approach is often called reoptimization it explicitly optimizes the releases at t using the current states storage swe and inflows and interpolating between the already found optimal values at t 1 the problem to solve for a given time t and storage level s t and exovars values v t q t is then 26 u t arg max u t r s t u t q t interp f t 1 s t 1 v t 1 κ r 1 r max s r t 1 s r t 1 0 max s r t 1 s r t 1 0 s t s t 1 s t q t c u t u t u t where the notation interp f t 1 s t 1 v t 1 is introduced to indicate that the optimal value derived from being in the states s t 1 v t 1 at t 1 is obtained by interpolation between points on the grid the v t 1 is the conditional expected value read on the same regression model that was adjusted during policy building the period s inflow q t is assumed known at t already so no average over representative inflows is required compare with 25 the problem is to be solved for each n o o s out of sample simulation recall sections 2 1 and 3 6 and the yearly productions are averaged over the simulations for comparison with the rl results the last bullet about q t is worth a word or two in policy building while we do have simulated pairs v t q t we do not have inflows q t that match the grid values of the hydrological variable v t ℓ in policy evaluation we again have simulated pairs v t q t but here the inflow is useable as is note that the reoptimization approach allows sdp to use the q t information at its fullest we will return to this in the numerical results tejada guibert et al 1993 concluded that the reoptimization procedure provides better system performance and our numerical checks concur this is the approach that we adopt 5 case study description of the hydropower system our case study is based on the hydropower system exploited by rio tinto in the saguenay lac saint jean area in québec canada the power is delivered to rio tinto s aluminum smelters our model is composed of three reservoirs and three generating stations see fig 1 to analyze the performances of the algorithms with respect to the number of reservoirs three configurations are studied with respectively one two and three reservoirs and thus control variables at each moment in time the first configuration considers only one reservoir lac saint jean its isle maligne powerhouse and the shipshaw and chute à caron run of river power stations the second configuration deals with two reservoirs the passes dangereuses reservoir and its chute des passes powerhouse are added to the previous layout finally the third configuration adds the lac manouane reservoir no powerhouse but a release decision nevertheless in the smaller configurations the upstream water inflows are included so that the total yearly inflows to lac st jean are the same in all configurations in other words adding a reservoir brings extra control but not extra inflows the constraints on the water releases are as follows the minimum and maximum releases are 0 and 700 m 3 s for the lac manouane reservoir 10 and 1650 m 3 s for the passes dangereuses reservoir 50 and 8000 m 3 s for the lac saint jean reservoir the constraints on the storage levels are as follows throughout the year the minimum and maximum storage volumes are 336 08 and 2657 hm 3 for the lac manouane reservoir and 68 97 and 5227 50 hm 3 for the passes dangereuses reservoir in the case of the lac saint jean reservoir the storage bounds are not constant over the horizon and are illustrated in red on fig 5 the minimum storage level is much higher during about half of the year the period of recreational use essentially from june 21st around time step 67 to the end of december around time step 10 the time series of natural inflows one per reservoir and snow water equivalent measure one for the whole system were provided by riotinto we had 1000 simulated years generated by the company s simulator whose model relies on a few decades of data the first 700 years define the set n which is used to build the operating policies while the set n o o s is defined by the remaining 300 years to evaluate the quality of the policies 6 case study parameters this section presents both the ad hoc hyperparameters e g the neural network s number of neurons and our adjustment of the main parameters of the rl and sdp algorithms section 6 1 the plain penalty rl and backoff rl approaches but the former is abandoned for good afterwards our parameter tests concentrate on the single reservoir case the two and three reservoir parameters are provided along with the numerical results in section 7 2 in the rl algorithms a policy is defined by a neural network with the following hyperparameters two hidden layers the first composed of 20 neurons and the second of ten neurons with tanh activation functions the outputs of the neural networks were discussed in section 3 1 we made the usual choice of a gaussian distribution for the stochasticity of the policy the update of the neural network parameters θ equation 8 is performed using the adam method kingma and ba 2014 with a fixed stepsize λ of 0 001 with regard to the discount factor we choose to give more importance to the immediate seasons than to those at the end of the year by fixing γ to 0 99 the last reward will be penalized by γ t 1 γ 121 0 296 allowing us to properly take into account the seasonal management from numerical experiments the parameter n m b is fixed to 3 as discussed in section 3 3 2 the steps 6 to 11 of the algorithm 1 are performed every 250 iterations to reduce the computational effort without loss of quality on the obtained policies for the plain penalty rl approach the number of policy building iterations i of the algorithm 1 is fixed to 100 000 for the backoff rl approach the step 1 also runs the algorithm 1 with an number of optimization iterations i set to 100 000 in the bisection approach step 3 each iteration considers as initial policy the policy obtained in the previous iteration this warm start allows us to reduce the number of iterations i to 20 000 when running algorithm 1 the upper bound β is fixed to 2 and the bisection method stops when the length of the search interval becomes smaller than a tolerance t o l set to 0 01 we used a three day timestep 122 periods per year t 122 in all tests three day periods allow faster run times and typically loose little representativity in comparison to daily time steps for the policy evaluations both rl and sdp the n o o s 300 years are simulated in serie the final annual storage levels of the reservoirs are used as initial storage levels of the next year the percentage of the 300 years for which the storage constraints are satisfied at every period within the year will be referred to as the satisfaction rate the reported productions correspond to the average production over the 300 years note that the productions were scaled at the request of the producer in other words the reported gwh can be compared between themselves but in the absolute are not linked to the actual power system the joint chance constraint aims at a satisfaction level of at least 95 α 0 05 recall from section 3 5 2 that in fact we require the satisfaction of the criterion f l b 95 on the policy building set of simulations which allows us to assume that the chance constraint will be satisfied with a high level of confidence 1 ϵ 0 99 in fact during the out of sample policy evaluation 99 of the time at least 19 out of 20 years satisfy the joint constraint of course even if the stringent f l b 95 is not quite satisfied by a policy it will still very often occur that on a specific set of simulations the satisfaction rate of 95 is attained therefore in the next sections we report both the satisfaction rate over the 300 years of out of sample simulations and whether f l b 1 α was satisfied over the 700 years of policy building simulations note that it may be impossible to avoid constraint violations for any system for example for the one reservoir configuration three day timestep and inflow simulations that we use the chance constraint physically cannot be fulfilled at the level 99 over the 300 simulated years that we use for evaluation though the level 95 is attainable all codes were written in python the pytorch library was used to manage the neural network in the rl approach the nonlinear optimization subproblems inherent to the sdp approach are solved with the scipy implementation of the nonlinear optimization slsqp algorithm kraft 1988 all tests were run on a single processor laptop with an intel core i7 8565u cpu 1 80 ghz and 16 gb of ram notation the constraint violation penalty parameter κ has the same general purpose for all algorithms but its numerical value will not be the same for the comparison of rl and sdp to be clear we introduce the notations κ rl and κ sdp 6 1 rl adjustment of the penalty in this section we compare the plain penalty rl and the backoff rl approaches and adjust their penalty level for different values of the parameter κ rl we report in table 1 the power production the satisfaction rate on the out of sample set as well as the satisfaction of the criterion f l b 1 α as expected for the plain penalty rl approach the higher the penalty coefficient the lower the production and the higher the satisfaction rate furthermore the penalty factor κ rl must be at least 200 to find a policy that satisfies the criterion f l b 1 α leading to an average production of 3 583 gwh for the backoff rl approach the criterion f l b 1 α is satisfied for all three reported levels of κ rl we select κ rl 30 which provides the best production of the three and incidently 2 5 more than the 3583 gwh of the plain penalty rl given these results the plain penalty rl approach is abandoned and only the backoff rl approach will be considered for all future experiments with a penalty factor κ rl fixed at 30 6 2 sdp adjustment of the parameters in this section we present an analysis of the penalty factor κ sdp and the state space discretization levels k and l for the benchmark method 6 2 1 adjustment of the sdp penalty factor κ sdp the production and satisfaction rate are provided in fig 2 for various penalty factor levels recall that the penalty factors of rl and sdp are different in their usages and cannot be compared directly the discretization of the state spaces was rather coarse with k and l set to 10 note that the criterion f l b 0 95 could not be satisfied given this coarse discretization a larger penalty factor influences positively the constraint satisfaction and negatively the production for values of κ sdp ranging from 10 000 to 100 000 the production and the satisfaction rate remain stable with an average of 3 717 gwh and 93 7 respectively a value of κ sdp 100 000 was used for all further experiments 6 2 2 adjustment of the sdp state space discretization levels the sdp algorithm requires the discretizations of the endogenous storage and exogenous snow water equivalent state spaces 12 12 recall that we do not condition on the second exovar the inflows and from section 4 2 that we account for inflows uncertainty with a number of scenarios fixed at m 10 a finer discretization can improve the policy performances but is more demanding in computation time reported in table 2 we investigate the influence of the discretization k and l of the endogenous and exogenous variable spaces on the production the satisfaction rate and the satisfaction of the criterion f l b 1 α unsurprisingly finer levels of discretization generally improve the performance measures note that a discretization with k l 20 leads to a policy that does not quite satisfy the criterion f l b 0 95 f l b 0 9479 in this instance but the satisfaction rate on the 300 out of sample simulations is higher than 95 given these results we decide on values of k l 25 which provide the most power production and satisfy the criterion f l b 1 α the policy building and evaluation times are presented in fig 3 as expected the computational effort increases with the level of discretization policy building time is proportional to the number of subproblems i e problem 24a which is proportional to k l fig 3 a illustrates the curse of dimensionality as the policy building time increases with the discretization level the policy evaluation times also increase with the discretization though much less significantly fig 3 b 7 case study numerical results in this section we compare the sdp and backoff rl policies on power productions satisfaction rates water shortages and spills and computational times we consider the one reservoir configuration first then two and three reservoir configurations in section 7 2 in all cases the sdp and backoff rl approaches relied on the same distinct sets of simulations for policy building in sample and policy evaluation out of sample 7 1 results for the one reservoir configuration in a one reservoir configuration we first compare the power productions and satisfaction rates under different values of the required chance constraint probability 1 α next we analyze the water spills and shortages finally we discuss the computational times fig 4 illustrates the productions and the satisfaction rates for the sdp and backoff rl policies for each value of 1 α the penalty factor κ sdp is adjusted by hand such that the criterion f l b 1 α is satisfied in the three cases the backoff rl policy returns a production about 1 7 lower than that of the sdp policy the satisfaction rates are similar fig 5 illustrates hydrographs the evolution of the water level in the reservoir for the 300 out of sample simulations we also provide the releases and power generation during the winter periods 10 to 40 and the fall periods 100 to 120 and 0 to 10 the sdp policy maintains a slightly higher average water level than the backoff rl policy resulting in slightly higher average releases and production over these periods on the other hand the backoff rl policy is slightly more prudent in emptying the reservoir in anticipation of the spring and the snowmelt periods 40 70 this later allows more flexibility to handle inflows we will see next that the number of shortages and spills is significantly reduced with the backoff rl policy recall that the chance constraint only accounts for the events of violations not the amounts of violation though the spill and shortage amounts do hurt the sdp and backoff rl policies by penalizing objective functions it is interesting to illustrate the shortages and spills see fig 6 and table 3 among the years with at least one shortage resp spill we compute the total amount of shortage resp spill over the year then we present these total quantities as small and large yearly shortages and spills the average amounts and frequencies are presented for each of the four classes results for 1 α 85 and 90 were similar the rl approach performs much better than the sdp approach on the shortage spill front with a much high proportion of small shortages and spills from a global perspective the backoff rl policy leads to an impressive 130 fold cut in overall shortages as well as a 3 fold cut in the overall average spills in comparison with the sdp policy in table 4 we report computational times of the policy building and policy evaluation phases the building of the backoff rl policy is slightly faster than that of the sdp policy the evaluation of the backoff rl policy is very fast while the evaluation time of the sdp policy is not negligible taking both building and evaluation into account the backoff rl approach is 20 faster than the sdp approach we conclude that in a one reservoir configuration the backoff rl approach results in an approximate 1 5 decrease in power generation over the sdp approach but also a substantial reduction in water shortages and spills the backoff rl approach is slightly faster than the sdp approach though both are admittedly slow note that neither approach has been parallelized please see our discussion of run times in section 7 3 7 2 results with two and three reservoirs in this section we continue the analysis of our numerical results but on more demanding configurations where the reinforcement learning approach begins to show its full potential the parameters of the sdp approach were adapted to account for the curse of dimensionality the computational effort increasing rapidly with the number of reservoirs n the number of subproblems to solve is equal to 3 t l k n where t is the time horizon k and l are the discretization levels and 3 refers to the number of the backward resolutions to find end of horizon values see section 4 3 to obtain results in reasonable run times we had to consider coarse levels of discretization at the expense of the quality of the results the parameters of the backoff rl approach needed no adjustment to perform adequately table 5 reports the productions satisfaction rates and run times for the two and three reservoir configurations productions levels are higher than in the one reservoir case because of the addition of the chute des passes powerhouse recall that the third reservoir does not have a powerhouse and is simply an extra control lever the satisfaction rates always on the n o o s 300 set of simulations are good with the backoff rl policy but barely over 50 with the coarse discretizations of the sdp approach the policy building time of sdp increases rapidly with the discretization levels as expected fig 7 illustrates the total computational times policy building and evaluation for the two approaches and each system configuration what stands out remarkably from both table 5 and fig 7 is that the rl approach shows good performances both in production and satisfaction rate with run times that increase only slightly with the configurations the complexity of the rl approach depends essentially on running algorithm 1 several times the main computational cost of algorithm 1 is the generation of episodes for policy update and evaluation and chance constraint analysis configurations with additional reservoirs only slightly increase the simulation time of an episode since the number of update iterations i needed no adjustment to find a good policy the run times do not increase much shortages and spills are analyzed in fig 8 and table 6 only the rl results are presented the sdp policy being too poor again we illustrate the average amounts and frequencies of small and large yearly shortages and spills the one reservoir rl results from fig 6 and table 3 are included for easy comparison for large inflows additional reservoirs allow us to keep water upstream and reduce spills from 651 hm 3 with the one reservoir configuration to 233 hm 3 with the three reservoir configurations on the other hand storing this water upstream also leads to increased shortages from 62 hm 3 with the one reservoir configuration to 322 hm 3 with the three reservoir configuration overall the total amount of spills and shortages taken together decreases from 713 hm 3 with one reservoir to 555 hm 3 with three reservoirs 7 3 on the run times of rl and sdp the comparison of run times is not a simple business for algorithms that are conceptually rather different the basic computational reference brick of the rl approach is a call to algorithm 1 reinforce each call involving i n forward passes the reference brick of the sdp approach is a backward pass through time not all bricks were included in the reported times here a short discussion of the choices we made for the backoff rl approach section 3 4 we have assumed the penalty level κ to be given so that only one call to algo 1 in entailed in step 1 step 2 is computationally negligible adjusting the backoffs in step 3 requires one call to algo 1 for each bisection step in our experiments eight steps were usually required the policy evaluation step takes only seconds for the sdp benchmark approach sections 4 1 and 4 5 both the penalty level κ and the discretization levels are considered given three backward passes were sufficient to find the end of horizon values so building an sdp policy consists in three backward passes the evaluation of an sdp policy takes time because of the reoptimization which applies to every out of sample path and every time step of course it is important to emphasize that providing computation free a right penalty parameter to the backoff rl and sdp approaches is not quite a fair treatment sdp relies completely on the penalty κ to satisfy the chance constraint with the best possible production for rl the largest part of the work is done by the backoff adjustment and the penalty value does not matter that much in that sense the rl computation times are put at a disadvantage with respect to the sdp times we also need to stress that reinforcement learning is an immature technology in its application to hydropower problems at least our implementation could benefit from many improvements that were considered out of scope for the purpose of this study let us add a final note on parallelization both backoff rl and sdp would benefit from a parallel implementation but the impact would require an analysis that is beyond the realm of this paper in the case of sdp policy building the subproblems at each time step can be solved in parallel this would help but the number of subproblems i e gridpoints still increases exponentially with the number of reservoirs the backoff rl policy building would benefit from a parallelization of the episode generation step 3 and 6 and the episode gradient computation step 4 in algorithm 1 as long as the policy evaluation is performed serially to represent the evolution of the system year after year neither of the approaches can benefit from parallelization 8 conclusion in this paper we explore a reinforcement learning policy gradient approach to solve a chance constrained hydropower problem with up to three reservoirs the chance constraints are managed as a single joint constraint with the help of a backoff technique our numerical results are benchmarked against a stochastic dynamic programming method on a one reservoir configuration the policy gradient approach finds good quality solutions with slightly less production but better shortages and spills than the benchmark the proposed approach shines on two and three reservoirs problems as the benchmark becomes increasingly cumbersome the policy gradient computing times are high for the single reservoir problem but increase only slightly with additional reservoirs in comparison the initially equivalent benchmark computing times quickly become unbearable we also find that backoffs are useful we explicitly test our rl approach with and without backoffs plain rl vs backoff rl to find that they allow better solutions research on solving hydropower problems with policy gradient with of without backoffs is still scarce but promises well in particular the addition of other even numerous exogenous variables such as soil moisture levels and weather forecasts should be possible without strong impacts on the computing times research in other areas of machine learning strongly hints at this in a multi objective problem or with an additional chance constraints such as minimum power production at each time step the level of control provided by backoffs could be even more critical this requires further study another line of research would rely on improving the performance of the rl approach by optimizing the hyperparameters e g the neural network characteristics as well as exploring other policy gradient methods also as discussed the parallelization of the programs would improve the computing times of both rl and the benchmark credit authorship contribution statement florian mitjana conceptualization methodology software formal analysis investigation writing original draft writing review editing michel denault conceptualization methodology formal analysis investigation writing original draft writing review editing funding acquisition kenjy demeester conceptualization resources writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank pascal côté riotinto and gerad dominique orban polytechnique montréal and gerad and caroline rocha gerad for their valuable inputs on this research the financial support of ivado is gratefully acknowedged 
94,the control of multi reservoir hydropower systems is a crucial cogwheel in the current transition of power systems towards renewable energy in particular in northern regions where inflows vary enormously through the year the satisfaction of storage constraints cannot be ensured at all times we thus add chance constraints to the underlying markov decision process problem and solve it with a reinforcement learning policy gradient approach backoffs common in other areas of optimal control are introduced to better manage the numerous chance constraints as a single joint constraint stochastic dynamic programming sdp is used as a benchmark approach we show numerically that our approach can deliver high quality policies and just as importantly that a three reservoir system is solved in proportionally little more time than a one reservoir system keywords reservoir management hydropower reinforcement learning policy gradient chance constraint backoffs data availability the data that has been used is confidential 1 introduction hydropower generation is a key part in the worldwide effort to move energy systems towards renewable sources and away from ghg emissions reservoir based hydro is not only a major source of power its flexibility is central to the large scale introduction of intermittent renewable power and often pivotal to account simultaneously for multiple purposes such as irrigation recreational uses and urban water supply salazar et al 2017 compounding this role is the sheer number of new and planned reservoirs over 3700 of them in the world according to zarfl et al 2019 in this context there is more than ever a need to improve hydropower optimization techniques that handle multiple decisions i e reservoirs and multiple sources of information reinforcement learning rl techniques hold rich promises in this direction recent review papers on hydropower have established the most relevant features the solution techniques and research directions e g the need to abide environmental constraints the challenge of multiobjective problems and the relevance of new optimal control approaches including rl dobson et al 2019 macian sorribes and pulido velazquez 2019 giuliani et al 2021 the literature on hydropower is vast ranging from refined optimization techniques to societal impacts and game theoretic studies of water sharing between countries in the very recent years research topics include the identification of relevant inputs to the optimization models especially under climate changes crochemore et al 2020 dang et al 2020 libisch lehner et al 2019 mulligan et al 2020 the integration of hydropower and intermittent renewable energy sources xu et al 2019 multi objective optimization models zaniolo et al 2021 applications of reinforcement learning xu et al 2020 2021 hydropower management is characterized by a stochastic environment with spatially and temporally correlated inflows the most common way to address this type of problem is to model its uncertainties and recourse actions and solve it with stochastic programming or dynamic programming or one of their numerous variants see the reviews just mentioned and a few details below the uncertainties can also be dealt with by robust optimization the focus being on finding an initial decision that is robust to future outcomes not on modeling the recourses and multistage uncertainties see for example giuliani and castelletti 2016 and xu et al 2022 for systems with few reservoirs the stochastic optimization approach of choice is stochastic dynamic programming sdp nandalal and bogardi 2007 davidsen et al 2015 finding an optimal policy requires the discretization of the state space the algorithm then proceeds in a single pass backward through time to compute the value function of each grid point the approach suffers from the curse of dimensionality the computational effort increases very rapidly with the number of reservoirs and grid refinement to mitigate this issue other approaches have been studied such as sampling stochastic dynamic programming côté and arsenault 2019 and stochastic dual dynamic programming gjelsvik et al 2010 but they still have some limitations giuliani et al 2021 alternatively a group of optimal control approaches that is receiving increasing attention relies on reinforcement learning unlike sdp an optimal policy is built iteratively through several forward in time passes that rely on simulations this iterative process allows the identification of good if not optimal policies hence the oft heard expression approximate dynamic programming q learning is one such approach it still requires the discretization of the state space to evaluate a value function although some papers report better behavior against the curse of dimensionality lee and labadie 2007 castelletti et al 2010 xu et al 2020 2021 q learning and its variants such as deep q network cannot learn stochastic policies they require the discretization of the action space and are in general subject to instabilities and divergences sutton and barto 2018 another class of rl techniques called policy gradient directly update a parameterized policy sutton et al 1999 policy gradient avoids the state and action space discretization and can significantly mitigate the curse of dimensionality a main challenge in reservoir operations is the satisfaction of constraints given the major influence of stochastic factors in fact the volatile nature of inflows often render physically impossible the satisfaction of storage constraints it is then most appropriate to explicitly enforce the satisfaction of constraints probabilistically by so called chance constraints to the knowledge of the authors the optimization of reservoir operations under chance constraints has been addressed for example to handle the risk of floods but as far as we know not with true multi stage models i e with explicit recourses see the references in macian sorribes and pulido velazquez 2019 the most recent of which being xu et al 2017 chance constraints are difficult to enforce directly even more so in a dynamic setting penalizing the objective function according to constraint violations is a common approach but it is unwieldy to penalize each constraint separately taking strong cues from process engineering papers koller et al 2018 paulson and mesbah 2018 we introduce so called backoffs backoffs allow a good individual treatment of each chance constraint without adding much computational burden this paper contributes to the hydropower management problem on two fronts through detailed computational tests between a benchmark sdp method and an rl policy gradient approach we show that the latter provides excellent quality solutions and most importantly that the computational effort increases only slightly when the number of decisions increases from one to two and three interestingly and even though this was not set as a goal in itself the rl policy brings substantial reductions in water shortages and spills when compared to the benchmark policy our second contribution is to show that chance constraints and backoffs form a natural way to handle storage constraints after this introduction section 2 provides our formulation of the reservoir management problem under chance constraints section 3 presents how backoffs can enforce the chance constraint within our policy gradient method framework in section 4 we describe our sdp benchmark approach case study is presented in section 5 adjustment of the parameters of the rl and sdp algorithms are discussed in section 6 we compare the sdp and rl policies in section 7 section 8 concludes the paper and discusses future work 2 modeling of the hydropower problem in this section we present the formulation of the joint chance constrained problem the operation of a hydropower system can be represented with a markov decision process mdp in discrete time where the objective is to minimize the expected cost from the time t 0 to the time horizon t t which is one year throughout the paper the number of reservoirs is r we define period t as the time elapsed between t and t 1 the mdp is defined by a tuple s u p r to which we add constraints below the set s includes all state variables specifically this represents the endogenous state variables s t s 1 t s r t r r which correspond to the water in each reservoir at the time step t the exogenous state variable v t r which corresponds to a snow water equivalent swe measure it is a measure of the snow and ice layer which of course transforms into water when temperature rises in the summer côté et al 2011 show that swe is a good estimate of future inflows the exogenous state variables q t q 1 t q r t r r representing the natural inflows to each reservoir over the period t the set u defines the decision variables u t u 1 t u r t r r giving the amount of water released from each reservoir over the period t p is the transition function at the time step t 1 the endogenous state variables are updated by the following dynamic law 1 s t 1 p s t u t q t s t q t c u t where c is the connectivity matrix appropriate to the reservoirs topology r s t u t q t is the reward function giving the amount of electricity produced in gigawatt hours for all plants over the period t starting with reservoirs levels s t applying decisions u t and with inflows q t let us be clear on information time and thus anticipativity in our model when making a decision at time t the measure of swe v t is simply that which is available at that moment although it is an indicator of future inflows from snowmelt it is in no way anticipative in the stochastic sense inflows are associated to a time period of course not a time point and we assume that the information q t for the time period that starts at t are available at t this inflow information is thus anticipative over the duration of one period we use three day steps this practice is common in the literature and relies on the fact that good predictions of inflows are normally available especially over short periods of time up to several days of course the same information set is provided to the benchmarking sdp and rl approaches we aim to find an operating policy π which takes as input the time t and the three state variables s t v t and q t and provides as output the decision variables u t due to the uncertainty of the inflows the objective is to maximize under constraints the expected sum of rewards over the horizon also called the expected return 2 e t 0 t 1 r s t u t q t to introduce the constraints we denote respectively the sets u t and s t for the decisions and the reservoir levels 3 u t u t r r u r t u r t u r t r 1 r s t s t r r s r t s r t s r t r 1 r where u r t and u r t define the lower and upper bounds on the releases for reservoir r at time t and s r t and s r t are the lower and upper bounds on the storage level for reservoir r at time t we will represent with s the set of all sets s t for t 1 t on one hand we consider that the physical properties of the turbines have to be respected at each time step of the year thus the constraints on the releases u t are defined as hard constraints on the other hand hard turbine constraints and the uncertainty of the inflows mean that it may be impossible to satisfy the storage constraints s t at each time step so the storage constraints are considered to be soft the objective is to determine a policy that jointly satisfies these constraints over the horizon with a probability of 1 α 4 f s p t 1 t s t s t 1 α wrapping up our hydropower optimization problem p can be formulated as follows p max u 0 u t 1 e t 0 t 1 r s t u t q t s t u t π s t v t q t t t 0 t 1 s t 1 s t q t c u t t 0 t 1 u t u t t 0 t 1 p t 1 t s t s t 1 α 2 1 a note on exogenous variables and data let us discuss briefly the exogenous variables we use two exogenous variables the inflows which incidentaly are also necessary for state transitions and a snow water equivalent measure each provides a different set of information the inflows being useful for the near future the swe being longer sighted see desreumaux et al 2014 for example importantly we assume that time series simulations of the pair v t q t for t 0 t 1 are available typically the simulations are drawn from a hydrological model not strictly historical so that their number is not limited we assume a set of n simulations to be used in policy building the same set for the rl and benchmark approaches we also assume to have a distinct set of n oos out of sample simulations drawn from the same statistical model of course these are strictly used for policy evaluation 3 reinforcement learning with chance constraints this section presents our reinforcement learning policy gradient approach in fact we will have two distinct rl approaches denoted plain penalty rl and backoff rl sections 3 1 to 3 5 present the policy building while the short section 3 6 discusses policy evaluation in section 3 1 we present policy gradient more specifically the reinforce approach ignoring for the moment the chance constraint in problem p the so called baseline technique is also introduced section 3 2 discusses how the chance constraint can be enforced by the policy gradient algorithm by penalizing the objective and explains how the chance constraint can be approximated with empirical distributions the section ends with the plain penalty rl approach the next section 3 3 introduces backoffs an alternative way to enforce constraint satisfaction which improves on the approach of section 3 2 section 3 4 continues to explain how the backoffs can be adjusted within the policy gradient approach to form the backoff rl approach section 3 5 provides both conceptual and numerical implementation details to complete the description of our method for an excellent textbook introduction to reinforcement learning we refer the reader to sutton and barto 2018 3 1 policy building with rl the policy gradient approach reinforcement learning is one of the three main paradigms of machine learning with supervised and unsupervised learning in opposition to classical dynamic programming which proceeds backward in time reinforcement learning is an iterative method that improves the policy with successive time forward passes 3 1 1 reinforce a policy gradient approach there are many ways to represent a policy within the rl paradigm we only discuss our own choice here first the policy will be stochastic a probability distribution is associated with each action at each state and time second our policy will be represented by a neural network essentially a parameterized function which takes states and time as input and which provides action probabilities as output specifically our policy writes out as the probability of choosing water releases u t given the current state 5 π θ s t v t q t t p u t s t v t q t t θ n μ σ the neural network s parameters are denoted by θ its output are the mean μ and variance σ of the distribution the dimension of the output scales with the number of decisions at time t e g for the three reservoir case σ is 3 3 the choice of a gaussian distribution is very common we need to assume that the policy π θ is continuously differentiable with respect to its parameters θ common neural networks all have this property throughout this section the objective function of problem p is replaced by the discounted expected return over the time horizon which is denoted as follows 6 j π θ e τ π θ t 0 t 1 γ t r s t u t q t here the episode τ concisely represents the succession of events 1 1 the expression trajectory is also common in the hydropower literature τ s 0 v 0 q 0 u 0 s 1 v 1 q 1 u 1 s t 1 v t 1 q t 1 u t 1 s t where the decisions are made following the stochastic policy π θ u t π θ s t v t q t t note that τ is a random variable as both the decisions and the future states are uncertain the discount factor γ is introduced for the sake of the reinforcement learning it indicates how much the agent cares about immediate rewards versus future ones with γ equal to 0 the agent is myopic because it relies only on immediate rewards as γ approaches 1 the agent becomes more far sighted because future rewards are given more consideration the policy gradient approach for problem p without the chance constraint is essentially a gradient ascent method adjusting the vector θ that parameterizes the policy by taking steps in the direction of the gradient of the objective function θ j π θ from the policy gradient theorem sutton and barto 2018 the said gradient can be proved to be 7 θ j π θ e t 0 t 1 θ log π θ s t v t q t t r τ t where we define the return following t r τ t t t t 1 γ t r s t u t q t we implement a version of the reinforce policy gradient update that is slightly different from the original in williams 1992 the policy update i is decomposed in three steps first a sample episode is generated using the current policy π θ i and a time series simulation of the pair v t q t t 0 t 1 picked from the set of n simulations cf section 2 1 second the gradient θ j π θ i is estimated by replacing the expected value in 7 with the sample third we update the policy parameters using the gradient based adam method of kingma and ba 2014 θ i 1 a d a m θ i θ j π θ i λ where λ is a stepsize of course the gradient estimate changes with the sample episode to stabilize it and improve the quality of the policy update a mini batch approach is often used whereby several episodes are generated instead of one the state variables change but not the policy i e not θ i and the update is done with the average of the single episode gradients if the mini batch size is n mb and we denote the sample episodes with τ j j 1 n mb then the gradient estimates are 2 2 these estimates are replaced by a version with baseline in the next section θ j j π θ i t 0 t 1 θ log π θ i s t v t q t t r τ j t and the update is 8 θ i 1 a d a m θ i 1 n mb j 1 n mb θ j j π θ i λ note that with time forward passes it is easy to ensure the respect of the state transition equations and decision constraints u t u t we come back to storage constraints s t s t in section 3 2 on a more technical level let us mention that the automatic computation of the gradient θ log π θ s t v t q t t is a standard feature of all neural networks libraries 3 1 2 variance reduction with a baseline the variance associated to stochastic gradients impedes the algorithm s progress towards an optimum beyond mini batches a common variance reduction approach is to subtract a baseline b from the return the baseline does not impact the expectation of the update step only its variance θ j j π θ i t 0 t 1 θ log π θ i s t v t q t t r τ j t b τ j t slightly abusing notation we do not reflect the presence of the baseline in the expression θ j j π θ i the baseline is always present beyond this point one of the simplest baseline is the average return taken over several monte carlo simulations williams 1992 desreumaux 2016 suggests that an alternative choice provides better performances in a setting similar to ours he notes that our stochastic policy 5 can be decomposed in a deterministic part based on μ and a stochastic part based on σ the author defines a deterministic policy such that the actions rely only on μ i e with zero covariance matrix we denote τ π and τ μ the episodes associated to the stochastic and deterministic policies 3 3 their dependence on θ is not displayed for the sake of readability then as suggested by the author we use as baseline the return associated to the deterministic policy and the gradient for each of the mini batch sample episode is evaluated as 9 θ j j π θ i t 0 t 1 θ log π θ i s t v t q t t r τ π j t r τ μ j t the gradient with baseline 9 is used in the update step 8 in all our numerical experiments 3 2 the chance constraint and the plain penalty rl approach we have so far ignored the joint chance constraint in problem p which combines all upper and lowerbound storage constraints into a single constraint the event that all storage constraints s t s t be satisfied at every time step of one year recall that in an reinforcement learning approach decision constraints u t and state transition constraints can quite naturally be satisfied step by step time forward constraints on the endogenous storage states s t are a more complicated business altogether since the current decision can affect the feasibility of states at a later point in time 4 4 in general the current decision can also affect the feasibility of future decisions but of course our model avoids this pitfall with storage constraints that are soft there always exists a feasible decision eventually at the expense of storage constraints a further complication is that the chance constraint 4 sees no analytical expression to its underlying cumulative distribution function f s let us tackle this second issue before we deal with the first one that of introducing the chance constraint into reinforcement learning one simple choice is to consider the empirical cumulative distribution function f s a non parametric approximation of the underlying c d f 10 f s f s 1 n k 1 n 1 t 1 t s t k s t where s t k corresponds to the realized storage values under the current policy π θ and the estimation is done over the n available simulations 5 5 this is a choice fewer than n simulations could be used this simple approximation is improved later see section 3 5 back to our first issue let us rewrite all storage constraints as conventional inequalities of the format g s 0 with the definitions r 1 r t 1 t 11 g r t 1 s r t s r t s r t and g r t 2 s r t s r t s r t the usual technique to attain constraint satisfaction when direct enforcement is not possible is to penalize the original objective function j π θ with the constraints violations 12 j π θ κ t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t 0 where κ is a penalty factor the gradient of eq 12 associated to a simulated episode τ is now expressed as 13 t 0 t 1 θ log π θ s t v t q t t r τ π t r τ μ t where the penalized return following t is r τ t t t t 1 γ t r s t u t q t κ t t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t 0 a policy π θ maximizing this penalized objective for a fixed κ and under the constraints on decisions u t and state transitions 14 max θ j π θ κ t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t 0 s t u t π s t v t q t t t 0 t 1 s t 1 s t q t c u t t 0 t 1 u t u t t 0 t 1 can be found with policy gradient steps as described above the algorithmic procedure corresponds exactly to calling algorithm 1 with null backoffs for a sufficiently high penalty level κ an approximate solution to the original problem p can be found but the production level will be hindered by a too high penalty the plain penalty rl approach we dub plain penalty rl the approach of launching algorithm 1 with null backoffs and various penalty levels κ until a smallest κ is found that ensures the chance constraint satisfaction in the following sections we propose an alternative rl approach that better adapts to each constraint 3 3 reinforce with fixed backoffs we first introduce the idea of using backoffs in the storage constraints and then provide the algorithm reinforce with fixed backoffs which is the cornerstone of both our plain penalty rl approach section 3 2 and our backoff rl approach section 3 4 3 3 1 introducing backoffs in the storage constraints in optimal control theory backoffs are scalars one per inequality constraint which artificially tighten or relax the original feasible set s t in our case see for example koller et al 2018 and paulson and mesbah 2018 let us define the backoffed feasible sets 15 s t b t s t g r t ℓ s r t b r t ℓ 0 r 1 r and ℓ 1 2 where each constraint is altered by its backoff b r t ℓ and b t is the vector of all backoffs at time t this generalizes the original constraint sets as s t 0 s t to generalize the set s we write s b for the set of all s t b t with a vector b that contains all the backoffs b t t 1 t the purpose of backoffs is to modulate the level of satisfaction of the original constraints by pretending that they are tighter or slacker than they really are given our lumping together of all storage constraints into one chance constraint and one penalty parameter backoffs bring an supplementary level of control that is not available with the plain penalty approach of section 3 2 to integrate backoffs in a reinforce algorithm which we do next let us generalize our previous definitions to account for the backoffs with the two parametered objective function 16 j π θ b j π θ κ t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t b r t ℓ 0 and the gradient θ j π θ b associated to the simulated episode τ is 17 θ j π θ b t 0 t 1 θ log π θ s t v t q t t r τ π t b r τ μ t b where the penalized backoffed return following t is the three parametered r τ t b t t t 1 γ t r s t u t q t κ t t 1 t r 1 r ℓ 1 2 max g r t ℓ s r t b r t ℓ 0 naturally j π θ 0 matches the objective function of the plain penalty approach 12 r τ t 0 corresponds to r τ t and θ j π θ 0 matches the gradient computation 13 3 3 2 the algorithm reinforce with fixed backoffs we have developed so far a reinforce algorithm section 3 1 with baseline section 3 1 2 and the chance constraint section 3 2 adding the idea of backoffs section 3 3 1 we obtain algorithm reinforce with fixed backoffs which is the cornerstone of both our plain penalty rl approach and backoff rl approach for a fixed penalty level κ and set of backoffs b the algorithm provides an approximate solution to the problem 18 max θ j π θ b s t u t π s t v t q t t t 0 t 1 s t 1 s t q t c u t t 0 t 1 u t u t t 0 t 1 let us point out that when policy updates are based on a stochastic gradient especially with a fixed step size the last policy π θ i may not be the best policy that has been generated thus throughout the update iterations we evaluate the joint constraint satisfaction level and average production among the policies that attain the required satisfaction probability we keep that with the best production see steps 6 to 11 of algo 1 if no policy satisfies the chance constraint the procedure returns the last policy also to reduce the computational effort it is advisable not to perform these steps 6 to 11 at every iteration in our numerical tests every 250 iterations was a good enough interval the pseudocode is as follows note that an adequate initial policy is automatically computed by the neural network library based solely on the network s features 3 4 the backoff rl approach this section explains how a joint chance constraint and backoffs can be worked into a rl algorithm that offers better performance than the plain penalty approach of section 3 2 the main fact that guides us is that just as it would be computationally unrealistic to adjust a penalty parameter for each storage constraint separately i e having multiple κ s in 16 it is not feasible to optimize each backoff value independently what is possible is to generate initial backoffs one for each storage constraint that are adequately enough sized with respect to their respective constraint violations and then unidimensionnaly scale these initial backoffs so as to fulfill the joint chance constraint at level 1 α the steps of the backoff rl approach are as follows the backoff rl approach step 1 identify an initial policy π θ init without backoffs run algorithm 1 with a loosely set κ penalty level and null backoffs effectively solving problem 14 and denote the resulting policy π θ init 6 6 notationally it is necessary to clearly distinguish this θ init from the θ i iterates of algorithm 1 especially its θ 0 importantly this policy needs not ensure the satisfaction of the chance constraint with precision the backoff adjustment procedure will handle this step 2 identify initial backoffs using policy π θ init initial backoff values b r t ℓ init are computed for all constraints of reservoir p time period t and type ℓ initial backoffs essentially gage the constraints violations one with respect to the other frequently violated constraints are assigned higher initial backoffs this computation relies on quantiles of the violation levels of each constraint step 3 adjust the backoffs backoffs must be adjusted to their smallest values that lead to joint chance constraint satisfaction the adjustment procedure is unidimensional the initial backoffs b r t ℓ init are scaled with a single parameter β i e to β b r t ℓ init which is adjusted by a bisection approach each bisection iteration entails a call to algorithm 1 with b β b r t ℓ init and the obtained policy π θ is used to update β based on the satisfaction of the chance constraint the bisection is applied to an interval β β with β 0 to ensure that backoffs remain backoffs and an upper bound chosen ad hoc high enough that the chance constraint satisfaction is attainable note that as initial policy π θ 0 each call to the algorithm 1 can use either the library s default policy or the policy π θ obtained at the previous iteration step 1 above is nothing but the application of the plain penalty rl approach explained in section 3 2 and we have no more to say other that we come back to the loosely set κ idea in the numerical results section 6 1 step 2 requires more details which are provided in section 3 4 1 step 3 is sufficiently simple that all necessary details were provided above 3 4 1 details of step 2 finding initial backoffs the initial backoffs b r t ℓ init are the starting point of our backoff adjustment procedure but they are also our only device to penalize one constraint more than another they are computed as follows with the resulting policy π θ init from step 1 we can generate n episodes 7 7 again this is a choice fewer than the full set of n available simulations could be used from these episodes we can extract a set of storage constraint values g r t ℓ s r t 1 g r t ℓ s r t n r t ℓ from which we can derive an empirical distribution for each r t ℓ constraint denoted 8 8 to avoid an overload of indices the empirical c d f s notation does not display their dependence on the underlying policy this was also the case for the empirical joint distribution eq 10 19 f r t ℓ b r t ℓ 1 n k 1 n 1 g r t ℓ s r t k b r t ℓ 0 let us denote by f r t ℓ 1 1 α the 1 α quantiles of these empirical distributions setting the initial backoffs with these quantiles b r t ℓ init f r t ℓ 1 1 α seems a good idea at first the higher the quantile the higher the backoff and generating a policy with these backoffs might yield storage levels that approximately satisfy each constraint at the level 1 α it would hopefully amount to shifting the distribution of the storage levels just the right amount but this hope ignores cross effects between constraints more importantly yet the joint constraint satisfaction is our goal and a 1 α level for each constraint will be insufficient to reach the 1 α level for the joint constraint a better idea is then to base the initial backoffs on a more stringent quantile f r t ℓ 1 1 δ of each of the constraints empirical c d f s with δ α of course furthermore we found that using the quantile on its own can have unpleasant side effects 9 9 the quantile minus mean backoff has the following advantages i the backoff is always positive so that no constraint will be inadvertently relaxed ii the backoff naturally adapts to the dispersion of the storage values wider distributions get larger initial backoffs which is intuitively right so that we subtract from it the empirical mean of the constraint values g r t ℓ 1 n k 1 n g r t ℓ s r t k the initial backoffs are then f r t ℓ 1 1 δ g r t ℓ for a single common quantile level 1 δ chosen such that 20 b r t ℓ init f r t ℓ 1 1 δ g r t ℓ f s b init 1 α where we use the backoffed set notation s b init introduced at the beginning of section 3 3 1 and where b init is simply the vector of all the initial backoffs b r t ℓ init identifying the largest δ that satisfies the implication 20 is an easy one dimensional root finding task with negligible computation time at this point the backoffs b r t ℓ init are fixed and remain so until the end since no policy was actually derived and tested with these backoffs i e by running algorithm 1 with them they may well not allow the satisfaction of the joint chance constraint nor ensure the best production adjustments are required hence the step 3 3 5 algorithmic details of the rl implementation in this section we provide some implementation details on the rl approaches concerning the time representation and a better approximation of the chance constraint 3 5 1 time representation an episode represents the simulation of a year and therefore corresponds to a full seasonal cycle the water available at the end of a year is also the initial water level of the following year it is important to provide this information on the periodicity of a year to the policy otherwise there will be no incentive to conserve water at the end of the year and the reservoirs will be emptied to maximize production in this perspective nilsson et al 2006 proposed to use the trigonometric representation of a circle to describe the temporal and cyclic evolution of a year the idea is to convert the original time steps into corresponding sine and cosine values plainly put scalar time t maps uniquely to the pair representation sin 2 π t t cos 2 π t t 3 5 2 a better representation of the chance constraint the empirical cumulative distribution function f s from section 3 2 is itself a random variable and we would like some level of reliability on the satisfaction of the chance constraint p f s 1 α note that as the indicator function underlying f s see eq 10 is a random variable following a bernoulli law it means that f s follows a binomial law b n f s divided by n from the relations between the binomial and beta laws clopper and pearson 1934 a lower bound f l b of f s can be estimated with a confidence level of 1 ϵ such that p f s f l b 1 ϵ where f l b is equal to the inverse of the beta c d f with parameters n f s and n n f s 1 at level ϵ then if the lower bound f l b is greater than the required probability 1 α we can assume that the chance constraint is satisfied with a level of confidence of 1 ϵ 21 f l b 1 α p f s 1 α 1 ϵ therefore when running algorithm 1 f l b 1 α rather than f s 1 α is checked to assess whether the chance constraint is satisfied 3 6 rl policy evaluation by policy evaluation we mean a measurement of the quality of a policy for the purpose of comparison with other policies as noted in section 2 1 all policy evaluations are done on a set of n oos simulations of the exogenous variables that is completely distinct from the simulations used in building the policies the same policy evaluation procedure applies to both the plain penalty rl and backoff rl approaches and is extremely simple the policy is used to determine actions by using only the deterministic part of the policy ignoring the variance covariance component in 5 for all n oos simulations the average production is then used as metric to compare policies between each other 4 the sdp benchmark we use a stochastic dynamic programming sdp approach to benchmark our numerical results all the elements of our policy building and policy evaluation are concisely provided below 4 1 policy building with sdp our goal of course remains to solve the optimization problem p sdp proceeds in the classical manner of finite horizon dynamic programming as follows for each point in time t and each point of a grid of the storage level endogenous variable s t and the snow water equivalent swe exogenous variable v t we find the best decision to maximize the expected profits until the end of horizon note that we do not build a grid of the inflows exogenous variables q t see section 4 5 on this relying on the bellman optimality principle we can solve these maximization problems by proceeding recursively from the end of horizon at each time t using the already found t 1 solutions mathematically we solve for each point of a storage swe grid 10 10 note that s t k refers to grid values in opposition to the notation of section 3 s t k v t ℓ k 1 k ℓ 1 l and for t t 1 t 2 0 the subproblem 22a max u t f t s t k v t ℓ e v t 1 q t v t ℓ r s t k u t q t f t 1 p s t k u t q t v t 1 22b s t p s t k u t q t s t k q t c u t 22c u t u t solving all subproblems back to t 0 produces a set of k l optimal solutions one for each point of the storage swe grid for each point in time note that the computation of f t 1 above requires numerical interpolations since the function is not being evaluated at grid points with one reservoir this is two dimensional interpolation with three reservoirs a four dimensional interpolation we tackle the missing parts in the next sections computing the conditional expectation finding an end of horizon starting point taking into account the until then missing chance constraint s t 1 k s t 1 and finally evaluating the quality of a policy 4 2 conditional expectations in sdp the treatment of the conditional expectation in 22a is an important feature of the various sdp methods we proceed as follows we can approximate the conditional expectation with the help of one snow water equivalent level v t 1 and m scenarios i e m possible values for the inflows q t to all reservoirs and their associated conditional probability conditioning is always on v t ℓ of course the scenarios of q t will be denoted q t scen m and v t 1 will be denoted v t 1 scen recall from section 2 1 that time series of pairs v t q t are available for policy building regression models are easy to build from these series and v t ℓ conditional scenarios can be generated as follows for the inflows for each reservoir p regress the q r t values on the corresponding v t values for a simple model linear or quadratic fit the residuals on a normal distribution and using the inverse c d f applied to a uniform grid on 0 1 generate a set of m equiprobable deviations from the model representative equiprobable inflows q t scen m given the grid value v t ℓ are then simply the scalar inflow value taken from the regression model read at v t ℓ of course plus the vector of m deviations for the swe exovar we regress our simulated v t 1 values on the corresponding v t values for a simple model linear or quadratic then the value v t 1 scen can be read directly from the regression model given the value of the grid v t ℓ 11 11 note the slight abuse of language where q t scen m and v t 1 scen do not show their dependance on the grid level ℓ clearly alternative avenues are possible to treat this conditional expectation but in our experience the approach above strikes a good balance between effort and effectiveness adapting the notation in a natural manner with f for f and a new s t k m the time t subproblem 22c can then be approximated by 23 max u t f t s t k v t ℓ 1 m m 1 m r s t k u t q t scen m f t 1 s t 1 k m v t 1 scen s t s t 1 k m s t k q t scen m c u t m 1 m u t u t in our numerical tests using a half dozen scenarios for q t scen m was enough to stabilize the optimal solution we fixed m 10 throughout all experiments to be on the safe side 4 3 end of horizon values the backward in time recursion of problem 23 requires end of horizon f t values we follow the suggestion of tilmant et al 2002 to account for the yearly seasonality of the exogenous variables essentially run the algorithm repeatitively pluging the f 0 values as f t values for the next run stop when a fixed point is attained meaning that the end of horizon values change little enough that the policy remains stable from one run to the next in various case study tests three runs were always sufficient all numerical experiments were then performed with three runs 4 4 the chance constraint in sdp the storage constraint still needs to be integrated in problem 23 we take the simple and usual approach of penalizing the objective f t with the constraints violations 24a max u t f t s t k v t ℓ κ 24b s t s t 1 k m s t k q t scen m c u t m 1 m 24c u t u t where f t s t k v t ℓ κ note the new third parameter is defined as 25 f t s t k v t ℓ κ 1 m m 1 m r s t k u t q t scen m f t 1 s t 1 k m v t 1 ℓ κ κ r 1 r max s r t 1 s r t 1 k m 0 max s r t 1 k m s r t 1 0 where κ is a penalty parameter f t and by consequence the subproblems and the complete problem are now parameterized with κ as before we need to set the penalty no higher than what is needed to ensure satisfaction of the chance constraint as high penalties impede production as a result the chance constrained sdp approach for a fixed penalty level κ is as follows the sdp approach step 1 solve problem 24a for each time step backwards from t t 1 to t 0 do so three times using f t 0 on the first run and using the most recent f 0 as f t for the second and third run step 2 evaluate the chance constraint satisfaction in step 1 the motivation of the three runs was provided in section 4 3 in step 2 the evaluation of the chance constraint satisfaction follows the same procedure as for the rl method including the lower bound that was described in section 3 5 2 estimate f s by solving problem 26 for n simulations see next section compute the lower bound f l b and check whether f l b 1 α the sdp approach is to be run with various κ until the smallest value that allows satisfaction the chance constraint has been found our description of this search for κ is loose because in all numerical experiments we assumed that κ was provided i e the run times do not account for the effort in finding the right κ 4 5 sdp policy evaluation the policy building procedure of sections 4 1 to 4 4 ends with for each time point and each point of the storage swe grid the optimal value functions f t s t k v t ℓ and the optimal releases u t s t k v t ℓ the quality of a policy is evaluated by running it in a time forward manner in sdp at least two approaches can be used to compute the optimal decision u t associated to the observed state values s t v t not grid values and inflows q t the first approach is to interpolate u t from the closest grid points and their associated releases it does not rely on the inflows q t the second approach is often called reoptimization it explicitly optimizes the releases at t using the current states storage swe and inflows and interpolating between the already found optimal values at t 1 the problem to solve for a given time t and storage level s t and exovars values v t q t is then 26 u t arg max u t r s t u t q t interp f t 1 s t 1 v t 1 κ r 1 r max s r t 1 s r t 1 0 max s r t 1 s r t 1 0 s t s t 1 s t q t c u t u t u t where the notation interp f t 1 s t 1 v t 1 is introduced to indicate that the optimal value derived from being in the states s t 1 v t 1 at t 1 is obtained by interpolation between points on the grid the v t 1 is the conditional expected value read on the same regression model that was adjusted during policy building the period s inflow q t is assumed known at t already so no average over representative inflows is required compare with 25 the problem is to be solved for each n o o s out of sample simulation recall sections 2 1 and 3 6 and the yearly productions are averaged over the simulations for comparison with the rl results the last bullet about q t is worth a word or two in policy building while we do have simulated pairs v t q t we do not have inflows q t that match the grid values of the hydrological variable v t ℓ in policy evaluation we again have simulated pairs v t q t but here the inflow is useable as is note that the reoptimization approach allows sdp to use the q t information at its fullest we will return to this in the numerical results tejada guibert et al 1993 concluded that the reoptimization procedure provides better system performance and our numerical checks concur this is the approach that we adopt 5 case study description of the hydropower system our case study is based on the hydropower system exploited by rio tinto in the saguenay lac saint jean area in québec canada the power is delivered to rio tinto s aluminum smelters our model is composed of three reservoirs and three generating stations see fig 1 to analyze the performances of the algorithms with respect to the number of reservoirs three configurations are studied with respectively one two and three reservoirs and thus control variables at each moment in time the first configuration considers only one reservoir lac saint jean its isle maligne powerhouse and the shipshaw and chute à caron run of river power stations the second configuration deals with two reservoirs the passes dangereuses reservoir and its chute des passes powerhouse are added to the previous layout finally the third configuration adds the lac manouane reservoir no powerhouse but a release decision nevertheless in the smaller configurations the upstream water inflows are included so that the total yearly inflows to lac st jean are the same in all configurations in other words adding a reservoir brings extra control but not extra inflows the constraints on the water releases are as follows the minimum and maximum releases are 0 and 700 m 3 s for the lac manouane reservoir 10 and 1650 m 3 s for the passes dangereuses reservoir 50 and 8000 m 3 s for the lac saint jean reservoir the constraints on the storage levels are as follows throughout the year the minimum and maximum storage volumes are 336 08 and 2657 hm 3 for the lac manouane reservoir and 68 97 and 5227 50 hm 3 for the passes dangereuses reservoir in the case of the lac saint jean reservoir the storage bounds are not constant over the horizon and are illustrated in red on fig 5 the minimum storage level is much higher during about half of the year the period of recreational use essentially from june 21st around time step 67 to the end of december around time step 10 the time series of natural inflows one per reservoir and snow water equivalent measure one for the whole system were provided by riotinto we had 1000 simulated years generated by the company s simulator whose model relies on a few decades of data the first 700 years define the set n which is used to build the operating policies while the set n o o s is defined by the remaining 300 years to evaluate the quality of the policies 6 case study parameters this section presents both the ad hoc hyperparameters e g the neural network s number of neurons and our adjustment of the main parameters of the rl and sdp algorithms section 6 1 the plain penalty rl and backoff rl approaches but the former is abandoned for good afterwards our parameter tests concentrate on the single reservoir case the two and three reservoir parameters are provided along with the numerical results in section 7 2 in the rl algorithms a policy is defined by a neural network with the following hyperparameters two hidden layers the first composed of 20 neurons and the second of ten neurons with tanh activation functions the outputs of the neural networks were discussed in section 3 1 we made the usual choice of a gaussian distribution for the stochasticity of the policy the update of the neural network parameters θ equation 8 is performed using the adam method kingma and ba 2014 with a fixed stepsize λ of 0 001 with regard to the discount factor we choose to give more importance to the immediate seasons than to those at the end of the year by fixing γ to 0 99 the last reward will be penalized by γ t 1 γ 121 0 296 allowing us to properly take into account the seasonal management from numerical experiments the parameter n m b is fixed to 3 as discussed in section 3 3 2 the steps 6 to 11 of the algorithm 1 are performed every 250 iterations to reduce the computational effort without loss of quality on the obtained policies for the plain penalty rl approach the number of policy building iterations i of the algorithm 1 is fixed to 100 000 for the backoff rl approach the step 1 also runs the algorithm 1 with an number of optimization iterations i set to 100 000 in the bisection approach step 3 each iteration considers as initial policy the policy obtained in the previous iteration this warm start allows us to reduce the number of iterations i to 20 000 when running algorithm 1 the upper bound β is fixed to 2 and the bisection method stops when the length of the search interval becomes smaller than a tolerance t o l set to 0 01 we used a three day timestep 122 periods per year t 122 in all tests three day periods allow faster run times and typically loose little representativity in comparison to daily time steps for the policy evaluations both rl and sdp the n o o s 300 years are simulated in serie the final annual storage levels of the reservoirs are used as initial storage levels of the next year the percentage of the 300 years for which the storage constraints are satisfied at every period within the year will be referred to as the satisfaction rate the reported productions correspond to the average production over the 300 years note that the productions were scaled at the request of the producer in other words the reported gwh can be compared between themselves but in the absolute are not linked to the actual power system the joint chance constraint aims at a satisfaction level of at least 95 α 0 05 recall from section 3 5 2 that in fact we require the satisfaction of the criterion f l b 95 on the policy building set of simulations which allows us to assume that the chance constraint will be satisfied with a high level of confidence 1 ϵ 0 99 in fact during the out of sample policy evaluation 99 of the time at least 19 out of 20 years satisfy the joint constraint of course even if the stringent f l b 95 is not quite satisfied by a policy it will still very often occur that on a specific set of simulations the satisfaction rate of 95 is attained therefore in the next sections we report both the satisfaction rate over the 300 years of out of sample simulations and whether f l b 1 α was satisfied over the 700 years of policy building simulations note that it may be impossible to avoid constraint violations for any system for example for the one reservoir configuration three day timestep and inflow simulations that we use the chance constraint physically cannot be fulfilled at the level 99 over the 300 simulated years that we use for evaluation though the level 95 is attainable all codes were written in python the pytorch library was used to manage the neural network in the rl approach the nonlinear optimization subproblems inherent to the sdp approach are solved with the scipy implementation of the nonlinear optimization slsqp algorithm kraft 1988 all tests were run on a single processor laptop with an intel core i7 8565u cpu 1 80 ghz and 16 gb of ram notation the constraint violation penalty parameter κ has the same general purpose for all algorithms but its numerical value will not be the same for the comparison of rl and sdp to be clear we introduce the notations κ rl and κ sdp 6 1 rl adjustment of the penalty in this section we compare the plain penalty rl and the backoff rl approaches and adjust their penalty level for different values of the parameter κ rl we report in table 1 the power production the satisfaction rate on the out of sample set as well as the satisfaction of the criterion f l b 1 α as expected for the plain penalty rl approach the higher the penalty coefficient the lower the production and the higher the satisfaction rate furthermore the penalty factor κ rl must be at least 200 to find a policy that satisfies the criterion f l b 1 α leading to an average production of 3 583 gwh for the backoff rl approach the criterion f l b 1 α is satisfied for all three reported levels of κ rl we select κ rl 30 which provides the best production of the three and incidently 2 5 more than the 3583 gwh of the plain penalty rl given these results the plain penalty rl approach is abandoned and only the backoff rl approach will be considered for all future experiments with a penalty factor κ rl fixed at 30 6 2 sdp adjustment of the parameters in this section we present an analysis of the penalty factor κ sdp and the state space discretization levels k and l for the benchmark method 6 2 1 adjustment of the sdp penalty factor κ sdp the production and satisfaction rate are provided in fig 2 for various penalty factor levels recall that the penalty factors of rl and sdp are different in their usages and cannot be compared directly the discretization of the state spaces was rather coarse with k and l set to 10 note that the criterion f l b 0 95 could not be satisfied given this coarse discretization a larger penalty factor influences positively the constraint satisfaction and negatively the production for values of κ sdp ranging from 10 000 to 100 000 the production and the satisfaction rate remain stable with an average of 3 717 gwh and 93 7 respectively a value of κ sdp 100 000 was used for all further experiments 6 2 2 adjustment of the sdp state space discretization levels the sdp algorithm requires the discretizations of the endogenous storage and exogenous snow water equivalent state spaces 12 12 recall that we do not condition on the second exovar the inflows and from section 4 2 that we account for inflows uncertainty with a number of scenarios fixed at m 10 a finer discretization can improve the policy performances but is more demanding in computation time reported in table 2 we investigate the influence of the discretization k and l of the endogenous and exogenous variable spaces on the production the satisfaction rate and the satisfaction of the criterion f l b 1 α unsurprisingly finer levels of discretization generally improve the performance measures note that a discretization with k l 20 leads to a policy that does not quite satisfy the criterion f l b 0 95 f l b 0 9479 in this instance but the satisfaction rate on the 300 out of sample simulations is higher than 95 given these results we decide on values of k l 25 which provide the most power production and satisfy the criterion f l b 1 α the policy building and evaluation times are presented in fig 3 as expected the computational effort increases with the level of discretization policy building time is proportional to the number of subproblems i e problem 24a which is proportional to k l fig 3 a illustrates the curse of dimensionality as the policy building time increases with the discretization level the policy evaluation times also increase with the discretization though much less significantly fig 3 b 7 case study numerical results in this section we compare the sdp and backoff rl policies on power productions satisfaction rates water shortages and spills and computational times we consider the one reservoir configuration first then two and three reservoir configurations in section 7 2 in all cases the sdp and backoff rl approaches relied on the same distinct sets of simulations for policy building in sample and policy evaluation out of sample 7 1 results for the one reservoir configuration in a one reservoir configuration we first compare the power productions and satisfaction rates under different values of the required chance constraint probability 1 α next we analyze the water spills and shortages finally we discuss the computational times fig 4 illustrates the productions and the satisfaction rates for the sdp and backoff rl policies for each value of 1 α the penalty factor κ sdp is adjusted by hand such that the criterion f l b 1 α is satisfied in the three cases the backoff rl policy returns a production about 1 7 lower than that of the sdp policy the satisfaction rates are similar fig 5 illustrates hydrographs the evolution of the water level in the reservoir for the 300 out of sample simulations we also provide the releases and power generation during the winter periods 10 to 40 and the fall periods 100 to 120 and 0 to 10 the sdp policy maintains a slightly higher average water level than the backoff rl policy resulting in slightly higher average releases and production over these periods on the other hand the backoff rl policy is slightly more prudent in emptying the reservoir in anticipation of the spring and the snowmelt periods 40 70 this later allows more flexibility to handle inflows we will see next that the number of shortages and spills is significantly reduced with the backoff rl policy recall that the chance constraint only accounts for the events of violations not the amounts of violation though the spill and shortage amounts do hurt the sdp and backoff rl policies by penalizing objective functions it is interesting to illustrate the shortages and spills see fig 6 and table 3 among the years with at least one shortage resp spill we compute the total amount of shortage resp spill over the year then we present these total quantities as small and large yearly shortages and spills the average amounts and frequencies are presented for each of the four classes results for 1 α 85 and 90 were similar the rl approach performs much better than the sdp approach on the shortage spill front with a much high proportion of small shortages and spills from a global perspective the backoff rl policy leads to an impressive 130 fold cut in overall shortages as well as a 3 fold cut in the overall average spills in comparison with the sdp policy in table 4 we report computational times of the policy building and policy evaluation phases the building of the backoff rl policy is slightly faster than that of the sdp policy the evaluation of the backoff rl policy is very fast while the evaluation time of the sdp policy is not negligible taking both building and evaluation into account the backoff rl approach is 20 faster than the sdp approach we conclude that in a one reservoir configuration the backoff rl approach results in an approximate 1 5 decrease in power generation over the sdp approach but also a substantial reduction in water shortages and spills the backoff rl approach is slightly faster than the sdp approach though both are admittedly slow note that neither approach has been parallelized please see our discussion of run times in section 7 3 7 2 results with two and three reservoirs in this section we continue the analysis of our numerical results but on more demanding configurations where the reinforcement learning approach begins to show its full potential the parameters of the sdp approach were adapted to account for the curse of dimensionality the computational effort increasing rapidly with the number of reservoirs n the number of subproblems to solve is equal to 3 t l k n where t is the time horizon k and l are the discretization levels and 3 refers to the number of the backward resolutions to find end of horizon values see section 4 3 to obtain results in reasonable run times we had to consider coarse levels of discretization at the expense of the quality of the results the parameters of the backoff rl approach needed no adjustment to perform adequately table 5 reports the productions satisfaction rates and run times for the two and three reservoir configurations productions levels are higher than in the one reservoir case because of the addition of the chute des passes powerhouse recall that the third reservoir does not have a powerhouse and is simply an extra control lever the satisfaction rates always on the n o o s 300 set of simulations are good with the backoff rl policy but barely over 50 with the coarse discretizations of the sdp approach the policy building time of sdp increases rapidly with the discretization levels as expected fig 7 illustrates the total computational times policy building and evaluation for the two approaches and each system configuration what stands out remarkably from both table 5 and fig 7 is that the rl approach shows good performances both in production and satisfaction rate with run times that increase only slightly with the configurations the complexity of the rl approach depends essentially on running algorithm 1 several times the main computational cost of algorithm 1 is the generation of episodes for policy update and evaluation and chance constraint analysis configurations with additional reservoirs only slightly increase the simulation time of an episode since the number of update iterations i needed no adjustment to find a good policy the run times do not increase much shortages and spills are analyzed in fig 8 and table 6 only the rl results are presented the sdp policy being too poor again we illustrate the average amounts and frequencies of small and large yearly shortages and spills the one reservoir rl results from fig 6 and table 3 are included for easy comparison for large inflows additional reservoirs allow us to keep water upstream and reduce spills from 651 hm 3 with the one reservoir configuration to 233 hm 3 with the three reservoir configurations on the other hand storing this water upstream also leads to increased shortages from 62 hm 3 with the one reservoir configuration to 322 hm 3 with the three reservoir configuration overall the total amount of spills and shortages taken together decreases from 713 hm 3 with one reservoir to 555 hm 3 with three reservoirs 7 3 on the run times of rl and sdp the comparison of run times is not a simple business for algorithms that are conceptually rather different the basic computational reference brick of the rl approach is a call to algorithm 1 reinforce each call involving i n forward passes the reference brick of the sdp approach is a backward pass through time not all bricks were included in the reported times here a short discussion of the choices we made for the backoff rl approach section 3 4 we have assumed the penalty level κ to be given so that only one call to algo 1 in entailed in step 1 step 2 is computationally negligible adjusting the backoffs in step 3 requires one call to algo 1 for each bisection step in our experiments eight steps were usually required the policy evaluation step takes only seconds for the sdp benchmark approach sections 4 1 and 4 5 both the penalty level κ and the discretization levels are considered given three backward passes were sufficient to find the end of horizon values so building an sdp policy consists in three backward passes the evaluation of an sdp policy takes time because of the reoptimization which applies to every out of sample path and every time step of course it is important to emphasize that providing computation free a right penalty parameter to the backoff rl and sdp approaches is not quite a fair treatment sdp relies completely on the penalty κ to satisfy the chance constraint with the best possible production for rl the largest part of the work is done by the backoff adjustment and the penalty value does not matter that much in that sense the rl computation times are put at a disadvantage with respect to the sdp times we also need to stress that reinforcement learning is an immature technology in its application to hydropower problems at least our implementation could benefit from many improvements that were considered out of scope for the purpose of this study let us add a final note on parallelization both backoff rl and sdp would benefit from a parallel implementation but the impact would require an analysis that is beyond the realm of this paper in the case of sdp policy building the subproblems at each time step can be solved in parallel this would help but the number of subproblems i e gridpoints still increases exponentially with the number of reservoirs the backoff rl policy building would benefit from a parallelization of the episode generation step 3 and 6 and the episode gradient computation step 4 in algorithm 1 as long as the policy evaluation is performed serially to represent the evolution of the system year after year neither of the approaches can benefit from parallelization 8 conclusion in this paper we explore a reinforcement learning policy gradient approach to solve a chance constrained hydropower problem with up to three reservoirs the chance constraints are managed as a single joint constraint with the help of a backoff technique our numerical results are benchmarked against a stochastic dynamic programming method on a one reservoir configuration the policy gradient approach finds good quality solutions with slightly less production but better shortages and spills than the benchmark the proposed approach shines on two and three reservoirs problems as the benchmark becomes increasingly cumbersome the policy gradient computing times are high for the single reservoir problem but increase only slightly with additional reservoirs in comparison the initially equivalent benchmark computing times quickly become unbearable we also find that backoffs are useful we explicitly test our rl approach with and without backoffs plain rl vs backoff rl to find that they allow better solutions research on solving hydropower problems with policy gradient with of without backoffs is still scarce but promises well in particular the addition of other even numerous exogenous variables such as soil moisture levels and weather forecasts should be possible without strong impacts on the computing times research in other areas of machine learning strongly hints at this in a multi objective problem or with an additional chance constraints such as minimum power production at each time step the level of control provided by backoffs could be even more critical this requires further study another line of research would rely on improving the performance of the rl approach by optimizing the hyperparameters e g the neural network characteristics as well as exploring other policy gradient methods also as discussed the parallelization of the programs would improve the computing times of both rl and the benchmark credit authorship contribution statement florian mitjana conceptualization methodology software formal analysis investigation writing original draft writing review editing michel denault conceptualization methodology formal analysis investigation writing original draft writing review editing funding acquisition kenjy demeester conceptualization resources writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank pascal côté riotinto and gerad dominique orban polytechnique montréal and gerad and caroline rocha gerad for their valuable inputs on this research the financial support of ivado is gratefully acknowedged 
