index,text
5525,the goal of the current study is to examine the relative role of watershed disturbance on the seasonal hydrological drought we propose a new methodology that combines the two parameter budyko equation and improved double mass curve idmc technique to remove the effect of climate change from the total change in the seasonal hydrological drought in particular we analyze the relative role of watershed disturbance and forest change in the changes of seasonal hydrological drought using yaak river watershed in montana u s as an example the hydrological drought is defined based on the threshold level approach in addition the sensitivity of hydrological drought to the changes of forest condition is performed based on the basin average normalized difference vegetation index ndvi the proposed method can be applied to quantify the effect of watershed disturbance on the seasonal drought seasonal flow shows the different nature of response to the watershed disturbance spring flow decrease is primarily due to the watershed disturbance on the other hand watershed disturbance offsets climate change effect on summer flows the changes in hydrological drought due to watershed disturbance decrease increase with the threshold during spring summer the contribution of forest disturbance to the change in drought due to watershed disturbance during the summer is as large as 75 the study has a promising potential in hydrological studies because watershed characteristics play a crucial role in terrestrial water storage evapotranspiration and discharge keywords hydrological drought double mass curve watershed disturbance ndvi 1 introduction drought is triggered due to the natural variability of climate and occurs in all regions in the world drought occurs primarily due to abnormal decrease increase in precipitation temperature andreadis and lettenmaier 2006 dai 2013 wilhite and glantz 1985 due to its effects on a wide range of environmental and societal sectors large spatial coverage and long persistence drought generally accounts for larger economic costs than other natural disasters e g das et al 2020 guo et al 2020 huang et al 2017b mishra and singh 2010 ross and lott 2003 past decades have witnessed frequent drought at global and regional scales due to changes in climate and environmental cycles e g andreadis and lettenmaier 2006 dai 2013 spinoni et al 2013 trenberth et al 2014 droughts are projected to be more frequent and severe in response to the warming over most parts of the continental united states conus e g cook et al 2014 2015 huang et al 2017a jeong et al 2014 sheffield and wood 2008 swain and hayhoe 2015 although the definition varies among interested groups droughts are broadly categorized into four types in the literature to facilitate communications e g mishra and singh 2010 wilhite and glantz 1985 1 meteorological drought precipitation deficit 2 agricultural drought soil moisture deficit 3 hydrological drought deficit in streamflow and reservoir water level and 4 socio economic drought water supply deficit the agricultural and hydrological droughts both start from the meteorological drought caused by precipitation deficit and or enhanced evapotranspiration loss over an extended period of time most drought impacts are related to the agricultural and hydrological droughts early detection of the onset and characteristics of the droughts is essential to the effective drought management system however due to the strong dependence on climate and watershed characteristics it is not always possible to directly infer agricultural and hydrological drought characteristics such as the onset frequency and severity from the meteorological drought characteristics e g das et al 2020 guo et al 2020 huang et al 2017b peters et al 2003 van loon and van lanen 2012 2015 vicente serrano and lopez moreno 2005 for example huang et al 2017b showed that the lag time between the onset of the meteorological and that of the hydrological drought exhibits remarkably seasonal characteristics and is influenced by both climate and basin characteristics yang et al 2018 showed that the changes in watershed retentions due to the changes in plant physiology significantly contribute to the total variations in the observed runoff worldwide the changes in river flow regimes due to watershed disturbance might offset or cumulate the climate change effect on the river flow e g li et al 2018 wei and zhang 2010 therefore understanding the river flow response to the watershed disturbance is of great interest to support sustainable natural resource management strategies there have been many studies on the streamflow response to the watershed disturbance the paired watershed approach sensitivity analysis and elasticity analysis are the most common methods to partition the role of climate and watershed in total variation of watershed hydrology e g amatya et al 2015 dooge et al 1999 jones et al 2006 koster and suarez 1999 zhao et al 2010 although these methods are widely used to quantify the hydrological effect of watershed disturbance in large basins several limitations including the difficulties in finding long term hydrological and climate records detail hydrological properties of the watershed e g vegetation cover soil type and topography and paired watersheds with identical climate and land use land change properties often constrain the applicability of the methods in many studies e g wei et al 2013 to overcome these limitations the modified double mass curve mdmc technique wei and zhang 2010 has recently been applied in several studies to quantify the relative contributions of forest change and climate variability to the streamflow characteristics e g eum et al 2016 li et al 2018 however studies on hydrological consequences of watershed disturbances have been mostly focused on the annual mean flow this study seeks to separate the seasons and analyze the role of watershed disturbance in seasonal hydrological drought characteristics as long as there is no significant disturbance in the watershed the relationship between cumulative discharge output and cumulative effective precipitation net input is linear the effect of climate change is already incorporated or normalized in the net precipitation any significant deviation from the linear relationship of the watershed double mass curve between the cumulative discharge and cumulative precipitation can be attributed to the effect of watershed disturbance our main hypothesis is that the effect of watershed disturbance on the streamflow drought is season dependent the specific objectives are 1 to quantify the effect of watershed disturbance on seasonal hydrological drought 2 to propose an improved double mass curve idmc approach to partition the effect of watershed disturbance in seasonal hydrological drought and 3 to attribute the change of seasonal hydrological drought as a result of watershed disturbance to the forest change the remainder of the paper is organized as follows section 2 and 3 describe the methodology data and study area results and discussion are presented in section 4 finally section 5 gives a summary of the findings and concludes the paper 2 study area and data as an example to illustrate the methods we analyze the discharge in the yaak river from the usgs stream gauge at tory montana usgs gage station 12304500 the main objective of this study is to analyze the effect of watershed disturbance at the seasonal timescale yaak river discharge data showed the effect of watershed disturbance in all seasons however individual seasons did not show statistically significant changes even though annual runoff change was statistically significant in addition we attribute the fraction of total change to forest disturbance or change in evapotranspiration based on the basin vegetation condition ndvi most area of the yaak river basin is covered with evergreen forest therefore we choose the yaak river basin as a test bed to demonstrate the approach the daily streamflow data from 1957 to 2018 are available the watershed extends in both the usa and canada with a total area of about 2054 km2 78 of which lies in the us the geographic location of the river is shown in fig 1 the dominant vegetation coverage in the watershed is an evergreen forest mdeq 2008 the land cover types and percent coverage of the watershed are shown in table 1 the effect of watershed disturbance on the drought is analyzed based on the changes in the slope of double mass curve between the cumulative discharge and cumulative precipitation ratio precipitation evapotranspiration e g wei and zhang 2010 we use basin average monthly precipitation temperature and evapotranspiration data available from the daily near surface gridded meteorological data and the derived hydro meteorological fluxes e g livneh et al 2013 archived at national oceanic and atmospheric administration noaa to determine the parameters of water balance equation in the reference period equation 6 in methodology the data are available at 1 160 latitude longitude grids from 1915 to 2011 national weather service 2018 as shown in fig 2 the river annual flow is spring and summer dominated while the watershed receives a significant amount of annual precipitation over all seasons the time series of annual discharge precipitation and evapotranspiration over the watershed are shown in fig 3 3 methodology 3 1 hydrological drought definition meteorological drought is often defined based on either precipitation alone e g spi mckee et al 1993 or precipitation and potential evapotranspiration e g spei vicente serrano et al 2010 which are the function of meteorological variables only therefore it is difficult to quantify the impact of changes in watershed conditions from the meteorological drought thus hydrological drought is selected to examine the effect of watershed disturbance on the drought since there are few data points in the reference period to fit the parametric statistical distribution we use a threshold level method in addition the actual volume of deficit is important for hydrological drought assessment such as irrigation water depth requirement in the threshold level method a drought occurs when the seasonal discharge is below a predefined threshold of τ s deficit volume is calculated as the difference between actual discharge and threshold level yevjevich 1967 in this study we used 30th 25th and 10th percentile flow as the threshold the chosen threshold is within the limits 5 30th percentile used in drought propagation studies e g van loon and van lanen 2012 streamflow deficit at any particular season is calculated van loon et al 2014 as 1 d t τ s x t i f x t τ s 0 i f x t τ s where x t is the seasonal streamflow time series and d t is the deviation of the streamflow from seasonal threshold τ s at time t the total deficit volume for the n observation years is given by 2 d t 0 n d t to compare the deficit volume in different seasons we compute the standardized seasonal deficit by dividing the total deficit volume by the long term average flow x t of the season 3 d d x t to compare the deficit volume between the reference and scenario periods within any season we also compute the seasonal average deficit d during the reference and scenario periods 4 d d n where n is the total number of observations which is 20 for the reference period 1957 1976 and 35 for the scenario period 1977 2011 in the current study 3 2 relative contributions of climate variability and watershed disturbance to hydrological drought in the mdmc approach the cumulative runoff versus the cumulative effective precipitation precipitation actual evapotranspiration is plotted as shown in fig 4 by subtracting evapotranspiration from precipitation the effect of climate variability on the cumulative precipitation and discharge relationship can be removed the basic premise in the mdmc approach is that as long as there are no significant changes in the watershed characteristics the relationship between the effective precipitation and discharge is approximately the same e g siriwardena et al 2006 wigbout 1973 zheng et al 2009 hence any deviation from this linear relationship is attributed to the streamflow change due to the watershed disturbance in the current study we apply the mdmc approach to quantify the relative contributions of watershed disturbance and climate change to the seasonal hydrological drought first we plot the mdmc at the annual timescale the annual effective precipitation p e is calculated from the difference between the total precipitation p and the total actual evapotranspiration a e t the aet is estimated by the equation proposed by zhang et al 2001 using the total precipitation and potential evapotranspiration p e t 5 aet p 1 ω pet p 1 ω pet p p pet where ω is a dimensionless parameter representing plant available water 2 in the current study for the perennial forest dominated watershed zhang et al 2001 the use of a particular equation for the estimation of pet is not central for the mdmc approach in this study the penman monteith pm equation for calculation of pet has often been used for the pet estimation allen et al 1998 walter et al 2000 however the pm equation requires routine measurements of solar radiation temperature wind speed and relative humidity which were not available for the study area the hargreaves hg equation hargreaves and samani 1985 can be used when this extensive data set is not available droogers and allen 2002 xu and singh 2001 the hg method is used to estimate monthly potential evapotranspiration from the monthly average daily minimum temperature t min 0 c maximum temperature t max 0 c and radiation r a m j m 2 â d as 6 pet 0 0023 r a t max t min 2 17 8 t max t min 5 the monthly pet so calculated is then summed to find the annual pet the modified double mass curve mdmc plotted in annual timescale is shown in fig 4 the plot shows that there was a remarkable change a breakpoint in the slope of mdmc around 1976 the study period is then divided as a reference period before the breakpoint 1957 1976 and scenario period after the breakpoint 1977 2011 after having the breakpoint in the mdmc we plot the mdmc at the seasonal scale at the annual timescale evapotranspiration is usually calculated from the precipitation and temperature assuming that the change in basin storage is negligible or precipitation is the only source for evapotranspiration e g choudhury 1999 pike 1964 zhang et al 2001 zhang et al 2004 however at the seasonal timescale the evapotranspiration often exceeds the total precipitation generally during spring summer and hence the additional water for evapotranspiration other than the seasonal precipitation has to be accounted for in the evapotranspiration model based on the precipitation and temperature chen et al 2013 greve et al 2016 zhang et al 2008 greve et al 2016 proposed a two parameter model that relates the seasonal evapotranspiration ratio aet p to the seasonal aridity index pet p based on the meteorological variables precipitation and temperature potential evapotranspiration only in the current study we use the modified budyko type relation proposed by greve et al 2016 to determine the seasonal actual evapotranspiration in scenario climate 7 aet p 1 pet p 1 1 y 0 k 1 pet p k 1 k where aet and pet are the basin average seasonal actual and potential evapotranspiration y0 and k are the parameters to be estimated the parameter y0 varies from 0 steady state condition as in the original budyko relation to 1 showing that demand limit has been reached the parameters are estimated from the observed precipitation temperature and evapotranspiration in the reference period the same parameters are then used to estimate the actual evapotranspiration based on the meteorological variables precipitation and temperature in the scenario period then by subtracting evapotranspiration from the total precipitation the climatic effect is eliminated in the scenario period at the seasonal timescale in watersheds with nearly equal precipitation and temperature it is possible that seasonal precipitation is greater than evapotranspiration in some years but smaller in other years the effective precipitation is not of the same sign in the time series consequently the double mass curve between the cumulative discharge and the cumulative effective precipitation is not monotonic as shown in fig 4a since the double mass curve based on effective precipitation pe is not monotonic it is difficult to detect the breakpoint therefore we plot the cumulative discharge and cumulative precipitation ratio pcr pc aetc and refer it as the improved double mass curve idmc which is shown in fig 4b the statistical difference of the idmc slope before and after the breakpoint is tested using the nonparametric mann whitney u test e g liu et al 2017 from the double mass curve we predict the cumulative discharge in the scenario period based on the reference period as shown in fig 5 the difference between the observed and predicted cumulative discharge in the scenario period is the effect of watershed disturbance on the cumulative discharge the change in cumulative discharge due to watershed disturbance or due to non climatic factors δ q c n c t is given by 8 δ q c n c t q c t q co t where q c 0 t and qc t are the observed and predicted cumulative discharge respectively the change in seasonal discharge due to watershed disturbance or due to non climatic factors δ q nc is calculated as 9 δ q nc t δ q c n c t δ q c n c t 1 then we compute the synthetic series of discharge in the scenario period by removing the effect of watershed disturbance q n c t 10 q n c t q t δ q nc t to quantify the effect of the watershed disturbance the hydrological drought from q n c t is compared with that from q t in the scenario period 3 3 quantifying the changes in hydrological drought due to forest disturbance we use the regression technique to attribute the changes in hydrological drought to the forest disturbance in this study the changes in hydrological drought due to the changes in vegetation conditions are solely based on the changes in evapotranspiration et among many indices used to characterize the vegetation status the ndvi goward et al 1991 is widely used to examine the changes in evapotranspiration and runoff due to the changes in vegetation condition in the watershed sruthi and aslam 2015 yang et al 2009 zhang et al 2001 the use of ndvi is also well established to monitor the drought in the literature e g nanzad et al 2019 xu et al 2016 in addition the ndvi is readily available in most basins to characterize basin conditions hence the forest disturbance is defined based on the normalized difference vegetation index ndvi however the proposed approach can be applied using other basin coverage indices if they are available theoretically the ndvi ranges from 1 to 1 the value of ndvi close to 1 means the highest possible density of green leaves while zero indicates no vegetation as shown in fig 6 change in the summer discharge due to watershed disturbance is correlated to the spring and summer ndvi even though the summer correlation is weak we fit a linear regression model with the summer and spring ndvi as a predictor of change in the summer discharge due to the watershed disturbance δ q nc as in equation 11 11 δ q nc β 1 n d v i summer β 2 n d v i spring where β 1 a n d β 2 are the regression coefficients since the forest conditions ndvi serve as predictors the predicted value is attributed to the change in discharge due to the forest change then changes in hydrological drought scenarios due to the changes in forest greenness are calculated by eqs 9 and 10 4 results and discussion 4 1 trends in hydro meteorological variables as shown in table 2 a significant increasing trend in the temperature and decreasing trends in the precipitation and discharge are detected during fall however the decreasing trend in discharge is smaller than that in precipitation although temperature shows a significant increasing trend during fall similarly the winter discharge does not show any trend while the winter precipitation and fall precipitation and temperature show significant trends results show the role of non climatic factors in the streamflow variations however given the non trivial role of other geographic factors in modulating the streamflow such as basin cover and characteristics it is difficult to infer the extent of streamflow change due to the temperature change alone 4 2 effect of watershed disturbance on streamflow deficit the double mass curve of cumulative annual streamflow qca against cumulative annual effective precipitation pcae for the reference period 1957 1976 and the scenario period 1977 2011 is shown in fig 5 the annual effective precipitation is computed as the difference between the annual precipitation and annual evapotranspiration the annual evapotranspiration is calculated from equation 5 the linear relationship is observed between the cumulative annual streamflow and effective precipitation pae until the slope change around 1977 when the observed line deviates from the original line the deviation of the double mass curve in the scenario period from the reference period suggests that less flow is generated than the prediction from the reference period reports show that the timber harvesting activities have declined in the watershed forests from the 1980s u s forest service 2003 having the result that the watershed disturbance changed the annual streamflow regime in 1977 we now determine the changes in streamflow at the seasonal timescale the two parameters in the budyko function eq 7 are calibrated in the reference period 1957 1976 based on the observed precipitation evapotranspiration and temperature potential evapotranspiration the resulting parameters and the fitted curves for different seasons are shown in fig 7 since the actual evapotranspiration during the winter is very small compared to the precipitation we exclude the winter season in further analysis the parameter k is within the range at the annual timescale in previous studies in the us watersheds e g greve et al 2015 parameter y0 nearly follows the demand line 1 1 particularly during the summer the evapotranspiration in the scenario period 1977 2011 is then calculated from the precipitation and potential evapotranspiration in the scenario period from equation 7 using the parameters k and y0 from the reference period the estimated evapotranspiration is used to plot the improved double mass curve idmc between the cumulative discharge and cumulative effective precipitation as shown in fig 8 the statistical significance of the difference in the idmc slope before and after the breakpoint is confirmed using the nonparametric mann whitney u test since the change in the idmc slope is not statistically significant during the fall we only analyze and plot the spring and the summer seasons similar to the annual flow fig 5 the seasonal flow during the spring and fall season decreases due to the watershed disturbance the decrease in the annual seasonal runoff due to the watershed disturbance afforestation in our case might be attributed to the increase in the transpiration and canopy interception losses stednick 2008 winkler et al 2005 the lowered flow during the spring can be attributed to the increased canopy loss and also the canopy obstruction to shortwave radiation reaching the ground snowpack ide et al 2013 stednick 2008 winkler et al 2005 similarly the reduced summer season flow can be explained by the enhanced evapotranspiration the decreased dry season flow due to reforestation is consistent with previous studies for instance ahiablame et al 2017 showed that the baseflow decreased with the cropland coverage in the watersheds in missouri similarly ma et al 2009 also found that reforestation decreased surface runoff however the response of dry season flow due to afforestation deforestation primarily from the change in soil structure and plant transpiration depends on forest structure and climatic regime chandler 2006 dijk and keenan 2007 hou et al 2018 liu et al 2016 this also shows that streamflow response to the watershed disturbance varies with the variables of interest and is watershed specific the offsetting effect of climate change and watershed disturbance in the cumulative streamflow in each season is shown in fig 9 as shown in fig 9a changes in the cumulative discharge during spring are primarily due to the watershed disturbance until around 1990 after 1990 the watershed disturbance offsets the climate change effect on the cumulative discharge on the other hand the watershed disturbance offsets the climatic effect on the cumulative discharge during summer fig 9b the offsetting effect of watershed disturbance is not consistent during both the spring and summer seasons this demonstrates that focusing on a single timescale and season might not give a holistic view of the hydrological response of streamflow to the watershed disturbance we further quantify the effect of watershed disturbance on the streamflow drought we particularly monitor the average deficit volume change in the scenario period 1978 2011 relative to that in the reference period 1957 1977 we apply the seasonal thresholds corresponding to the 10 25 and 30 percentile flow in the reference period the threshold levels are 184 51 mm 202 65 mm and 210 11 mm during spring and those during summer are 57 64 mm 81 75 mm and 84 92 mm for the 10 25 and 30 percentiles respectively since the average deficit volume of streamflow is not comparable across seasons we also compute the standardized deficit volume equations 2 4 for each season and threshold the standardized deficit quantitatively shows the number of seasons or the number of time steps in general that is required to average the streamflow in order to bring the deficit volume to zero the average and standardized deficits based on the given threshold in the reference period are given in table 3 the average deficit increases more dryness during the spring and summer seasons due to the watershed disturbance while the larger effect of watershed disturbance is observed in the standardized deficit during spring the change in the average deficit in the scenario period due to the watershed disturbance increases decreases with the increased threshold 10 percentile to 30 percentile during summer spring understanding the relative contributions of watershed disturbance and climatic change in the streamflow drought has significant implications in water resource management zhou et al 2015 the results show the possibility of hydrological drought onset due to changes in the watershed characteristics such as reforestation deforestation without the meteorological drought as a precursor the watershed modulation of streamflow drought severity followed by meteorological drought largely depends on the season on the other hand besides the changes in the hydrological drought due to climatic change meteorological drought itself may cause a severe effect on the forest and vegetation cover e g costa et al 2010 mcdowell 2011 the relative role of watershed disturbance to the changes in hydrological droughts varies among seasons to better understand the nature and extent of the cumulative effect of meteorological drought to hydrological drought and agricultural drought can guide managing forests wetlands and water resources in addition to the changing evapotranspiration from the watershed the watershed disturbance such as forest change also alters deep percolation and surface runoff hence changes in watershed forests and streamflow during the drought or low flow are particularly important in groundwater dynamics and quality of groundwater in addition to affecting hydrological drought the watershed disturbance also affects the river ecosystem such as increased sedimentation in the streams due to forest loss in such cases potential positive effect on the streamflow due to the watershed disturbance might be overshadowed 4 3 future drought scenarios for various forest change scenarios since the majority of the watershed is covered with evergreen cone type forest we quantify the forest change based on the temporal changes in the basin average ndvi the percentage change due to watershed disturbance during summer is significantly correlated with the ndvi lagged by one season i e spring ndvi the results are consistent with previous studies for instance sun et al 2009 showed a strong seasonality in the correlations of streamflow with ndvi that the correlations in august and september were particularly significant the linear regression of δ q nc with ndvi as a predictor is fitted to attribute the fraction of change in the discharge to the change in ndvi the changes in the hydrological drought due to forest change are shown in table 4 the contribution of forest change to the total change due to the watershed disturbance increases with the increased threshold the contribution of forest change to the change due to the watershed disturbance is as large as 103 during summer we further compute the percent change in the average deficits due to the different scenarios of forest ndvi changes we generate a synthetic ndvi time and then compute the change in discharge due to forest change based on the regression equation the results are shown in fig 10 extreme low flow e g 10 percentile threshold deficit is more sensitive to the change in ndvi compared to less extreme flow e g 25 and 30 percentile threshold deficit 5 summary and conclusions in this study we propose a new methodology to analyze the relative contributions of forest disturbance to the seasonal streamflow drought severity the approach explicitly considers the basin carryover effect at the seasonal timescale particularly when the seasonal evapotranspiration is greater than the seasonal precipitation the approach uses the improved double mass curve idmc between the cumulative precipitation ratio and cumulative discharge to partition the effect of watershed disturbance to the total change in streamflow drought severity to exclude the effect of climate variability on the changes in streamflow deficit we compute the precipitation ratio by dividing precipitation by evapotranspiration estimated based on meteorological variables precipitation and temperature potential evapotranspiration the hydrological drought is defined by the threshold level method streamflow data from the yaak river at northwest montana are analyzed the sensitivity of streamflow drought to the changes in forest disturbance over the basin is performed based on the changes in basin average ndvi using the regression technique the main findings are summarized as follows 1 the proposed approach based on the unsteady seasonal water balance equation and double mass curve technique can effectively separate the climate change effect from the watershed disturbance on the total changes in the seasonal hydrological drought the proposed method shows promising potential in hydrological studies because terrestrial water storage plays an important role in the watershed evapotranspiration and discharge 2 seasonal flow shows the different nature of the response to the watershed disturbance spring flow decrease is primarily due to the watershed disturbance while the watershed disturbance offsets the climate change effect during the summer for 1957 1977 3 the change in hydrological drought due to the watershed disturbance decreases increases with the threshold during spring summer percent change in the standardized deficit during summer is larger than that during spring for all threshold deficits 4 the contribution of forest disturbance to the change due to the watershed disturbance during summer could be as large as 75 the contribution increases with the threshold 5 the rate of change in the standardized deficit with increasing ndvi is larger for smaller threshold deficits i e more extreme low flows credit authorship contribution statement yog aryal conceptualization methodology writing original draft jianting zhu conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
5525,the goal of the current study is to examine the relative role of watershed disturbance on the seasonal hydrological drought we propose a new methodology that combines the two parameter budyko equation and improved double mass curve idmc technique to remove the effect of climate change from the total change in the seasonal hydrological drought in particular we analyze the relative role of watershed disturbance and forest change in the changes of seasonal hydrological drought using yaak river watershed in montana u s as an example the hydrological drought is defined based on the threshold level approach in addition the sensitivity of hydrological drought to the changes of forest condition is performed based on the basin average normalized difference vegetation index ndvi the proposed method can be applied to quantify the effect of watershed disturbance on the seasonal drought seasonal flow shows the different nature of response to the watershed disturbance spring flow decrease is primarily due to the watershed disturbance on the other hand watershed disturbance offsets climate change effect on summer flows the changes in hydrological drought due to watershed disturbance decrease increase with the threshold during spring summer the contribution of forest disturbance to the change in drought due to watershed disturbance during the summer is as large as 75 the study has a promising potential in hydrological studies because watershed characteristics play a crucial role in terrestrial water storage evapotranspiration and discharge keywords hydrological drought double mass curve watershed disturbance ndvi 1 introduction drought is triggered due to the natural variability of climate and occurs in all regions in the world drought occurs primarily due to abnormal decrease increase in precipitation temperature andreadis and lettenmaier 2006 dai 2013 wilhite and glantz 1985 due to its effects on a wide range of environmental and societal sectors large spatial coverage and long persistence drought generally accounts for larger economic costs than other natural disasters e g das et al 2020 guo et al 2020 huang et al 2017b mishra and singh 2010 ross and lott 2003 past decades have witnessed frequent drought at global and regional scales due to changes in climate and environmental cycles e g andreadis and lettenmaier 2006 dai 2013 spinoni et al 2013 trenberth et al 2014 droughts are projected to be more frequent and severe in response to the warming over most parts of the continental united states conus e g cook et al 2014 2015 huang et al 2017a jeong et al 2014 sheffield and wood 2008 swain and hayhoe 2015 although the definition varies among interested groups droughts are broadly categorized into four types in the literature to facilitate communications e g mishra and singh 2010 wilhite and glantz 1985 1 meteorological drought precipitation deficit 2 agricultural drought soil moisture deficit 3 hydrological drought deficit in streamflow and reservoir water level and 4 socio economic drought water supply deficit the agricultural and hydrological droughts both start from the meteorological drought caused by precipitation deficit and or enhanced evapotranspiration loss over an extended period of time most drought impacts are related to the agricultural and hydrological droughts early detection of the onset and characteristics of the droughts is essential to the effective drought management system however due to the strong dependence on climate and watershed characteristics it is not always possible to directly infer agricultural and hydrological drought characteristics such as the onset frequency and severity from the meteorological drought characteristics e g das et al 2020 guo et al 2020 huang et al 2017b peters et al 2003 van loon and van lanen 2012 2015 vicente serrano and lopez moreno 2005 for example huang et al 2017b showed that the lag time between the onset of the meteorological and that of the hydrological drought exhibits remarkably seasonal characteristics and is influenced by both climate and basin characteristics yang et al 2018 showed that the changes in watershed retentions due to the changes in plant physiology significantly contribute to the total variations in the observed runoff worldwide the changes in river flow regimes due to watershed disturbance might offset or cumulate the climate change effect on the river flow e g li et al 2018 wei and zhang 2010 therefore understanding the river flow response to the watershed disturbance is of great interest to support sustainable natural resource management strategies there have been many studies on the streamflow response to the watershed disturbance the paired watershed approach sensitivity analysis and elasticity analysis are the most common methods to partition the role of climate and watershed in total variation of watershed hydrology e g amatya et al 2015 dooge et al 1999 jones et al 2006 koster and suarez 1999 zhao et al 2010 although these methods are widely used to quantify the hydrological effect of watershed disturbance in large basins several limitations including the difficulties in finding long term hydrological and climate records detail hydrological properties of the watershed e g vegetation cover soil type and topography and paired watersheds with identical climate and land use land change properties often constrain the applicability of the methods in many studies e g wei et al 2013 to overcome these limitations the modified double mass curve mdmc technique wei and zhang 2010 has recently been applied in several studies to quantify the relative contributions of forest change and climate variability to the streamflow characteristics e g eum et al 2016 li et al 2018 however studies on hydrological consequences of watershed disturbances have been mostly focused on the annual mean flow this study seeks to separate the seasons and analyze the role of watershed disturbance in seasonal hydrological drought characteristics as long as there is no significant disturbance in the watershed the relationship between cumulative discharge output and cumulative effective precipitation net input is linear the effect of climate change is already incorporated or normalized in the net precipitation any significant deviation from the linear relationship of the watershed double mass curve between the cumulative discharge and cumulative precipitation can be attributed to the effect of watershed disturbance our main hypothesis is that the effect of watershed disturbance on the streamflow drought is season dependent the specific objectives are 1 to quantify the effect of watershed disturbance on seasonal hydrological drought 2 to propose an improved double mass curve idmc approach to partition the effect of watershed disturbance in seasonal hydrological drought and 3 to attribute the change of seasonal hydrological drought as a result of watershed disturbance to the forest change the remainder of the paper is organized as follows section 2 and 3 describe the methodology data and study area results and discussion are presented in section 4 finally section 5 gives a summary of the findings and concludes the paper 2 study area and data as an example to illustrate the methods we analyze the discharge in the yaak river from the usgs stream gauge at tory montana usgs gage station 12304500 the main objective of this study is to analyze the effect of watershed disturbance at the seasonal timescale yaak river discharge data showed the effect of watershed disturbance in all seasons however individual seasons did not show statistically significant changes even though annual runoff change was statistically significant in addition we attribute the fraction of total change to forest disturbance or change in evapotranspiration based on the basin vegetation condition ndvi most area of the yaak river basin is covered with evergreen forest therefore we choose the yaak river basin as a test bed to demonstrate the approach the daily streamflow data from 1957 to 2018 are available the watershed extends in both the usa and canada with a total area of about 2054 km2 78 of which lies in the us the geographic location of the river is shown in fig 1 the dominant vegetation coverage in the watershed is an evergreen forest mdeq 2008 the land cover types and percent coverage of the watershed are shown in table 1 the effect of watershed disturbance on the drought is analyzed based on the changes in the slope of double mass curve between the cumulative discharge and cumulative precipitation ratio precipitation evapotranspiration e g wei and zhang 2010 we use basin average monthly precipitation temperature and evapotranspiration data available from the daily near surface gridded meteorological data and the derived hydro meteorological fluxes e g livneh et al 2013 archived at national oceanic and atmospheric administration noaa to determine the parameters of water balance equation in the reference period equation 6 in methodology the data are available at 1 160 latitude longitude grids from 1915 to 2011 national weather service 2018 as shown in fig 2 the river annual flow is spring and summer dominated while the watershed receives a significant amount of annual precipitation over all seasons the time series of annual discharge precipitation and evapotranspiration over the watershed are shown in fig 3 3 methodology 3 1 hydrological drought definition meteorological drought is often defined based on either precipitation alone e g spi mckee et al 1993 or precipitation and potential evapotranspiration e g spei vicente serrano et al 2010 which are the function of meteorological variables only therefore it is difficult to quantify the impact of changes in watershed conditions from the meteorological drought thus hydrological drought is selected to examine the effect of watershed disturbance on the drought since there are few data points in the reference period to fit the parametric statistical distribution we use a threshold level method in addition the actual volume of deficit is important for hydrological drought assessment such as irrigation water depth requirement in the threshold level method a drought occurs when the seasonal discharge is below a predefined threshold of τ s deficit volume is calculated as the difference between actual discharge and threshold level yevjevich 1967 in this study we used 30th 25th and 10th percentile flow as the threshold the chosen threshold is within the limits 5 30th percentile used in drought propagation studies e g van loon and van lanen 2012 streamflow deficit at any particular season is calculated van loon et al 2014 as 1 d t τ s x t i f x t τ s 0 i f x t τ s where x t is the seasonal streamflow time series and d t is the deviation of the streamflow from seasonal threshold τ s at time t the total deficit volume for the n observation years is given by 2 d t 0 n d t to compare the deficit volume in different seasons we compute the standardized seasonal deficit by dividing the total deficit volume by the long term average flow x t of the season 3 d d x t to compare the deficit volume between the reference and scenario periods within any season we also compute the seasonal average deficit d during the reference and scenario periods 4 d d n where n is the total number of observations which is 20 for the reference period 1957 1976 and 35 for the scenario period 1977 2011 in the current study 3 2 relative contributions of climate variability and watershed disturbance to hydrological drought in the mdmc approach the cumulative runoff versus the cumulative effective precipitation precipitation actual evapotranspiration is plotted as shown in fig 4 by subtracting evapotranspiration from precipitation the effect of climate variability on the cumulative precipitation and discharge relationship can be removed the basic premise in the mdmc approach is that as long as there are no significant changes in the watershed characteristics the relationship between the effective precipitation and discharge is approximately the same e g siriwardena et al 2006 wigbout 1973 zheng et al 2009 hence any deviation from this linear relationship is attributed to the streamflow change due to the watershed disturbance in the current study we apply the mdmc approach to quantify the relative contributions of watershed disturbance and climate change to the seasonal hydrological drought first we plot the mdmc at the annual timescale the annual effective precipitation p e is calculated from the difference between the total precipitation p and the total actual evapotranspiration a e t the aet is estimated by the equation proposed by zhang et al 2001 using the total precipitation and potential evapotranspiration p e t 5 aet p 1 ω pet p 1 ω pet p p pet where ω is a dimensionless parameter representing plant available water 2 in the current study for the perennial forest dominated watershed zhang et al 2001 the use of a particular equation for the estimation of pet is not central for the mdmc approach in this study the penman monteith pm equation for calculation of pet has often been used for the pet estimation allen et al 1998 walter et al 2000 however the pm equation requires routine measurements of solar radiation temperature wind speed and relative humidity which were not available for the study area the hargreaves hg equation hargreaves and samani 1985 can be used when this extensive data set is not available droogers and allen 2002 xu and singh 2001 the hg method is used to estimate monthly potential evapotranspiration from the monthly average daily minimum temperature t min 0 c maximum temperature t max 0 c and radiation r a m j m 2 â d as 6 pet 0 0023 r a t max t min 2 17 8 t max t min 5 the monthly pet so calculated is then summed to find the annual pet the modified double mass curve mdmc plotted in annual timescale is shown in fig 4 the plot shows that there was a remarkable change a breakpoint in the slope of mdmc around 1976 the study period is then divided as a reference period before the breakpoint 1957 1976 and scenario period after the breakpoint 1977 2011 after having the breakpoint in the mdmc we plot the mdmc at the seasonal scale at the annual timescale evapotranspiration is usually calculated from the precipitation and temperature assuming that the change in basin storage is negligible or precipitation is the only source for evapotranspiration e g choudhury 1999 pike 1964 zhang et al 2001 zhang et al 2004 however at the seasonal timescale the evapotranspiration often exceeds the total precipitation generally during spring summer and hence the additional water for evapotranspiration other than the seasonal precipitation has to be accounted for in the evapotranspiration model based on the precipitation and temperature chen et al 2013 greve et al 2016 zhang et al 2008 greve et al 2016 proposed a two parameter model that relates the seasonal evapotranspiration ratio aet p to the seasonal aridity index pet p based on the meteorological variables precipitation and temperature potential evapotranspiration only in the current study we use the modified budyko type relation proposed by greve et al 2016 to determine the seasonal actual evapotranspiration in scenario climate 7 aet p 1 pet p 1 1 y 0 k 1 pet p k 1 k where aet and pet are the basin average seasonal actual and potential evapotranspiration y0 and k are the parameters to be estimated the parameter y0 varies from 0 steady state condition as in the original budyko relation to 1 showing that demand limit has been reached the parameters are estimated from the observed precipitation temperature and evapotranspiration in the reference period the same parameters are then used to estimate the actual evapotranspiration based on the meteorological variables precipitation and temperature in the scenario period then by subtracting evapotranspiration from the total precipitation the climatic effect is eliminated in the scenario period at the seasonal timescale in watersheds with nearly equal precipitation and temperature it is possible that seasonal precipitation is greater than evapotranspiration in some years but smaller in other years the effective precipitation is not of the same sign in the time series consequently the double mass curve between the cumulative discharge and the cumulative effective precipitation is not monotonic as shown in fig 4a since the double mass curve based on effective precipitation pe is not monotonic it is difficult to detect the breakpoint therefore we plot the cumulative discharge and cumulative precipitation ratio pcr pc aetc and refer it as the improved double mass curve idmc which is shown in fig 4b the statistical difference of the idmc slope before and after the breakpoint is tested using the nonparametric mann whitney u test e g liu et al 2017 from the double mass curve we predict the cumulative discharge in the scenario period based on the reference period as shown in fig 5 the difference between the observed and predicted cumulative discharge in the scenario period is the effect of watershed disturbance on the cumulative discharge the change in cumulative discharge due to watershed disturbance or due to non climatic factors δ q c n c t is given by 8 δ q c n c t q c t q co t where q c 0 t and qc t are the observed and predicted cumulative discharge respectively the change in seasonal discharge due to watershed disturbance or due to non climatic factors δ q nc is calculated as 9 δ q nc t δ q c n c t δ q c n c t 1 then we compute the synthetic series of discharge in the scenario period by removing the effect of watershed disturbance q n c t 10 q n c t q t δ q nc t to quantify the effect of the watershed disturbance the hydrological drought from q n c t is compared with that from q t in the scenario period 3 3 quantifying the changes in hydrological drought due to forest disturbance we use the regression technique to attribute the changes in hydrological drought to the forest disturbance in this study the changes in hydrological drought due to the changes in vegetation conditions are solely based on the changes in evapotranspiration et among many indices used to characterize the vegetation status the ndvi goward et al 1991 is widely used to examine the changes in evapotranspiration and runoff due to the changes in vegetation condition in the watershed sruthi and aslam 2015 yang et al 2009 zhang et al 2001 the use of ndvi is also well established to monitor the drought in the literature e g nanzad et al 2019 xu et al 2016 in addition the ndvi is readily available in most basins to characterize basin conditions hence the forest disturbance is defined based on the normalized difference vegetation index ndvi however the proposed approach can be applied using other basin coverage indices if they are available theoretically the ndvi ranges from 1 to 1 the value of ndvi close to 1 means the highest possible density of green leaves while zero indicates no vegetation as shown in fig 6 change in the summer discharge due to watershed disturbance is correlated to the spring and summer ndvi even though the summer correlation is weak we fit a linear regression model with the summer and spring ndvi as a predictor of change in the summer discharge due to the watershed disturbance δ q nc as in equation 11 11 δ q nc β 1 n d v i summer β 2 n d v i spring where β 1 a n d β 2 are the regression coefficients since the forest conditions ndvi serve as predictors the predicted value is attributed to the change in discharge due to the forest change then changes in hydrological drought scenarios due to the changes in forest greenness are calculated by eqs 9 and 10 4 results and discussion 4 1 trends in hydro meteorological variables as shown in table 2 a significant increasing trend in the temperature and decreasing trends in the precipitation and discharge are detected during fall however the decreasing trend in discharge is smaller than that in precipitation although temperature shows a significant increasing trend during fall similarly the winter discharge does not show any trend while the winter precipitation and fall precipitation and temperature show significant trends results show the role of non climatic factors in the streamflow variations however given the non trivial role of other geographic factors in modulating the streamflow such as basin cover and characteristics it is difficult to infer the extent of streamflow change due to the temperature change alone 4 2 effect of watershed disturbance on streamflow deficit the double mass curve of cumulative annual streamflow qca against cumulative annual effective precipitation pcae for the reference period 1957 1976 and the scenario period 1977 2011 is shown in fig 5 the annual effective precipitation is computed as the difference between the annual precipitation and annual evapotranspiration the annual evapotranspiration is calculated from equation 5 the linear relationship is observed between the cumulative annual streamflow and effective precipitation pae until the slope change around 1977 when the observed line deviates from the original line the deviation of the double mass curve in the scenario period from the reference period suggests that less flow is generated than the prediction from the reference period reports show that the timber harvesting activities have declined in the watershed forests from the 1980s u s forest service 2003 having the result that the watershed disturbance changed the annual streamflow regime in 1977 we now determine the changes in streamflow at the seasonal timescale the two parameters in the budyko function eq 7 are calibrated in the reference period 1957 1976 based on the observed precipitation evapotranspiration and temperature potential evapotranspiration the resulting parameters and the fitted curves for different seasons are shown in fig 7 since the actual evapotranspiration during the winter is very small compared to the precipitation we exclude the winter season in further analysis the parameter k is within the range at the annual timescale in previous studies in the us watersheds e g greve et al 2015 parameter y0 nearly follows the demand line 1 1 particularly during the summer the evapotranspiration in the scenario period 1977 2011 is then calculated from the precipitation and potential evapotranspiration in the scenario period from equation 7 using the parameters k and y0 from the reference period the estimated evapotranspiration is used to plot the improved double mass curve idmc between the cumulative discharge and cumulative effective precipitation as shown in fig 8 the statistical significance of the difference in the idmc slope before and after the breakpoint is confirmed using the nonparametric mann whitney u test since the change in the idmc slope is not statistically significant during the fall we only analyze and plot the spring and the summer seasons similar to the annual flow fig 5 the seasonal flow during the spring and fall season decreases due to the watershed disturbance the decrease in the annual seasonal runoff due to the watershed disturbance afforestation in our case might be attributed to the increase in the transpiration and canopy interception losses stednick 2008 winkler et al 2005 the lowered flow during the spring can be attributed to the increased canopy loss and also the canopy obstruction to shortwave radiation reaching the ground snowpack ide et al 2013 stednick 2008 winkler et al 2005 similarly the reduced summer season flow can be explained by the enhanced evapotranspiration the decreased dry season flow due to reforestation is consistent with previous studies for instance ahiablame et al 2017 showed that the baseflow decreased with the cropland coverage in the watersheds in missouri similarly ma et al 2009 also found that reforestation decreased surface runoff however the response of dry season flow due to afforestation deforestation primarily from the change in soil structure and plant transpiration depends on forest structure and climatic regime chandler 2006 dijk and keenan 2007 hou et al 2018 liu et al 2016 this also shows that streamflow response to the watershed disturbance varies with the variables of interest and is watershed specific the offsetting effect of climate change and watershed disturbance in the cumulative streamflow in each season is shown in fig 9 as shown in fig 9a changes in the cumulative discharge during spring are primarily due to the watershed disturbance until around 1990 after 1990 the watershed disturbance offsets the climate change effect on the cumulative discharge on the other hand the watershed disturbance offsets the climatic effect on the cumulative discharge during summer fig 9b the offsetting effect of watershed disturbance is not consistent during both the spring and summer seasons this demonstrates that focusing on a single timescale and season might not give a holistic view of the hydrological response of streamflow to the watershed disturbance we further quantify the effect of watershed disturbance on the streamflow drought we particularly monitor the average deficit volume change in the scenario period 1978 2011 relative to that in the reference period 1957 1977 we apply the seasonal thresholds corresponding to the 10 25 and 30 percentile flow in the reference period the threshold levels are 184 51 mm 202 65 mm and 210 11 mm during spring and those during summer are 57 64 mm 81 75 mm and 84 92 mm for the 10 25 and 30 percentiles respectively since the average deficit volume of streamflow is not comparable across seasons we also compute the standardized deficit volume equations 2 4 for each season and threshold the standardized deficit quantitatively shows the number of seasons or the number of time steps in general that is required to average the streamflow in order to bring the deficit volume to zero the average and standardized deficits based on the given threshold in the reference period are given in table 3 the average deficit increases more dryness during the spring and summer seasons due to the watershed disturbance while the larger effect of watershed disturbance is observed in the standardized deficit during spring the change in the average deficit in the scenario period due to the watershed disturbance increases decreases with the increased threshold 10 percentile to 30 percentile during summer spring understanding the relative contributions of watershed disturbance and climatic change in the streamflow drought has significant implications in water resource management zhou et al 2015 the results show the possibility of hydrological drought onset due to changes in the watershed characteristics such as reforestation deforestation without the meteorological drought as a precursor the watershed modulation of streamflow drought severity followed by meteorological drought largely depends on the season on the other hand besides the changes in the hydrological drought due to climatic change meteorological drought itself may cause a severe effect on the forest and vegetation cover e g costa et al 2010 mcdowell 2011 the relative role of watershed disturbance to the changes in hydrological droughts varies among seasons to better understand the nature and extent of the cumulative effect of meteorological drought to hydrological drought and agricultural drought can guide managing forests wetlands and water resources in addition to the changing evapotranspiration from the watershed the watershed disturbance such as forest change also alters deep percolation and surface runoff hence changes in watershed forests and streamflow during the drought or low flow are particularly important in groundwater dynamics and quality of groundwater in addition to affecting hydrological drought the watershed disturbance also affects the river ecosystem such as increased sedimentation in the streams due to forest loss in such cases potential positive effect on the streamflow due to the watershed disturbance might be overshadowed 4 3 future drought scenarios for various forest change scenarios since the majority of the watershed is covered with evergreen cone type forest we quantify the forest change based on the temporal changes in the basin average ndvi the percentage change due to watershed disturbance during summer is significantly correlated with the ndvi lagged by one season i e spring ndvi the results are consistent with previous studies for instance sun et al 2009 showed a strong seasonality in the correlations of streamflow with ndvi that the correlations in august and september were particularly significant the linear regression of δ q nc with ndvi as a predictor is fitted to attribute the fraction of change in the discharge to the change in ndvi the changes in the hydrological drought due to forest change are shown in table 4 the contribution of forest change to the total change due to the watershed disturbance increases with the increased threshold the contribution of forest change to the change due to the watershed disturbance is as large as 103 during summer we further compute the percent change in the average deficits due to the different scenarios of forest ndvi changes we generate a synthetic ndvi time and then compute the change in discharge due to forest change based on the regression equation the results are shown in fig 10 extreme low flow e g 10 percentile threshold deficit is more sensitive to the change in ndvi compared to less extreme flow e g 25 and 30 percentile threshold deficit 5 summary and conclusions in this study we propose a new methodology to analyze the relative contributions of forest disturbance to the seasonal streamflow drought severity the approach explicitly considers the basin carryover effect at the seasonal timescale particularly when the seasonal evapotranspiration is greater than the seasonal precipitation the approach uses the improved double mass curve idmc between the cumulative precipitation ratio and cumulative discharge to partition the effect of watershed disturbance to the total change in streamflow drought severity to exclude the effect of climate variability on the changes in streamflow deficit we compute the precipitation ratio by dividing precipitation by evapotranspiration estimated based on meteorological variables precipitation and temperature potential evapotranspiration the hydrological drought is defined by the threshold level method streamflow data from the yaak river at northwest montana are analyzed the sensitivity of streamflow drought to the changes in forest disturbance over the basin is performed based on the changes in basin average ndvi using the regression technique the main findings are summarized as follows 1 the proposed approach based on the unsteady seasonal water balance equation and double mass curve technique can effectively separate the climate change effect from the watershed disturbance on the total changes in the seasonal hydrological drought the proposed method shows promising potential in hydrological studies because terrestrial water storage plays an important role in the watershed evapotranspiration and discharge 2 seasonal flow shows the different nature of the response to the watershed disturbance spring flow decrease is primarily due to the watershed disturbance while the watershed disturbance offsets the climate change effect during the summer for 1957 1977 3 the change in hydrological drought due to the watershed disturbance decreases increases with the threshold during spring summer percent change in the standardized deficit during summer is larger than that during spring for all threshold deficits 4 the contribution of forest disturbance to the change due to the watershed disturbance during summer could be as large as 75 the contribution increases with the threshold 5 the rate of change in the standardized deficit with increasing ndvi is larger for smaller threshold deficits i e more extreme low flows credit authorship contribution statement yog aryal conceptualization methodology writing original draft jianting zhu conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
5526,reliable and accurate streamflow forecasting is vital for water resource management many streamflow prediction studies have demonstrated the excellent prediction ability of decomposition ensemble models several studies first extracted subsignals from a streamflow series and then split subsignals into training and validation sets to build prediction models capturing some future information not available in practical streamflow forecasting alternatively dividing a time series into training and validation sets first and later decomposing them into subsignals can expose the subsignals to the boundary effect making predicting streamflow difficult furthermore building one model for each subsignal is laborious and can cause error accumulation therefore establishing a robust and efficient decomposition ensemble model without future information to predict highly nonstationary and nonlinear streamflow is challenging hence a single model forecasting sf scheme that assesses the validation distribution during the training stage to adapt to the boundary effect was designed a sf scheme based on variational mode decomposition vmd and long short term memory lstm namely sf vmd lstm was proposed to predict daily streamflow 1 7 days ahead non decomposition based and decomposition ensemble based lstm models established using sf multi model ensemble forecasting mef and sf with the most influential subsignals sfmis and the ensemble empirical mode decomposition eemd and discrete wavelet transform dwt were compared additionally multi model ensemble hindcasting meh single model hindcasting sh and sh with the most influential subsignals shmis were used as benchmarks to evaluate the forecasting schemes adaptability to the boundary effect two daily streamflow series from han river and jing river china were investigated the results indicate that 1 sf is more robust and efficient than mef and sfmis 2 vmd performs better than eemd and dwt 3 sf vmd lstm with nse values larger than 0 8 for almost all the prediction scenarios outperformed other comparative models therefore sf vmd lstm is robust and efficient for forecasting highly nonstationary and nonlinear streamflow keywords forecasting experiment hindcasting experiment decomposition and ensemble deep learning data availability the data and codes that support the findings of this study are available from zuo et al 2020 https doi org 10 17632 bhjgdhgzjr 1 nomenclature he hindcasting experiment fe forecasting experiment vmd variational mode decomposition lstm long short term memory dwt discrete wavelet transform eemd ensemble empirical mode decomposition imf intrinsic mode function gbrt gradient boosting regression trees mef multi model ensemble forecasting sf single model forecasting sfmis single model forecasting with the most influential subsignals pacf partial autocorrelation coefficient ml machine learning mk mann kendall nrmse normalized root mean square error nse nash sutcliffe efficiency ppts peak percentage of threshold statistic yx yangxian hydrological station zjs zhangjiashan hydrological station meh multi model ensemble hindcasting sh single model hindcasting shmis single model hindcasting with the most influential subsignals 1 introduction streamflow forecasting is vital for water resource management he et al 2019 huang et al 2014 sepehri and sarrafzadeh 2018 2019 therefore this topic has been investigated by numerous researchers and many prediction models have been studied in past decades yaseen et al 2015 these models have generally been divided into physically based and data driven models kratzert et al 2018 zhang et al 2015 data driven models which are based on statistical modeling are popular due to their simplicity low information requirements and robustness coulibaly et al 2000 huang et al 2014 nilsson et al 2006 shiri and kisi 2010 young and beven 1994 zhang et al 2015 zhu and fujita 1993 chen et al 2018 in data driven flow forecasting many previous researchers have applied time series models including box jenkins castellano méndez et al 2004 huang et al 2014 autoregression ar li et al 2015 moving average ma adnan et al 2017 autoregressive moving average arma mohammadi et al 2006 toth et al 2000 and autoregressive integrated moving average arima valipour et al 2013 models to forecast streamflow however due to the linear hypothesis of these models they are not suitable for forecasting streamflow with nonlinear and nonstationary characteristics therefore machine learning ml models that can be used for nonlinear mapping e g support vector regression svr huang et al 2014 yu et al 2018b tongal and booij 2018 fuzzy inference systems fis he et al 2014 yaseen et al 2017 zhou et al 2019 yang et al 2019 dehghani et al 2019 bayesian regression br humphrey et al 2016 wang et al 2017 achieng and zhu 2019 and artificial neural networks anns moradkhani et al 2004 tan et al 2018 wang et al 2009 yaseen et al 2016 have been applied to streamflow forecasting however these pure ml models cannot always adequately handle highly nonstationary time series with many noise components without preprocessing the original data zhang et al 2015 because suitable time series preprocessing can be performed to extract multiscale features that are simpler than the original signals ml techniques can efficiently describe an original signal in statistical terms and other prior information hidden in an observed data set bai et al 2016 zhang et al 2015 therefore many decomposition ensemble models that combine time series preprocessing and ml techniques have been developed to forecast streamflow zhang et al 2015 wavelet analysis wa adamowski 2008 fourier transform ft yu et al 2018b singular spectrum analysis ssa unnikrishnan and jothiprakash 2018 empirical mode decomposition emd zhu et al 2016 ensemble empirical mode decomposition eemd huang et al 2019 and variational mode decomposition vmd he et al 2019 xie et al 2019 are commonly used time series preprocessing techniques that decompose the original nonstationary signal into several simpler subsignals that can be easily modeled by ml methods zhang et al 2015 used six decomposition ensemble models combined with three different time series preprocessing techniques emd ssa and wa and two modeling methods arma and ann to predict monthly runoff huang et al 2014 applied a modified version of emd to remove the most nonlinear and disorderly noise from the original series and then established an svr based model to predict monthly runoff bai et al 2016 used eemd to extract multiscale features and reconstructed three deep neural networks dnns with a summation strategy to forecast reservoir inflow yu et al 2018b exploited both ft and svr models to extract and learn features and forecast monthly reservoir inflow by summing all of the feature learning results the preprocessing of the original data can improve the prediction performance because of the advantage of feature extraction which removes noise components and detects the hidden structures in raw time series however applying time series preprocessing techniques directly to an entire streamflow series can result in the transmission of some information from the validation period into the training process of data driven models which leads to hindcasting experiments he zhang et al 2015 thus the streamflow prediction results at a specific moment are calculated using some future information that would not be available at that specific moment in a real practical application of streamflow forecasting known as forecasting experiments fe zhang et al 2015 however many previous studies such as those by bai et al 2016 huang et al 2014 kasiviswanathan et al 2016 and meng et al 2019 decomposed entire time series into subsignals first and later divided them into training and testing samples to establish models which is an incorrect or impractical approach because some data from the testing stage i e the future data are used as decomposition parameters for decomposing data from the training stage which will be further explained in section 3 5 but data from the testing stage are not available in real forecasting applications in other words the models trained on training samples cheat to obtain a better forecasting performance with testing samples because these models access some testing information in advance therefore these established models cannot be trusted in this context we would like to mention zhang et al 2015 du et al 2017 tan et al 2018 fang et al 2019 and quilty and adamowski 2018 more explicitly because they noted the incorrect or impractical usage of decomposition and ensemble models zhang et al 2015 compared the monthly streamflow forecasting performance of decomposition ensemble models coupled with wa emd and ssa in hindcasting and forecasting experiments du et al 2017 evaluated the decomposition and ensemble models based on ssa dwt and ann in a forecasting and hindcasting experiment tan et al 2018 used an eemd based ann model to assess the performance of monthly streamflow prediction in hindcasting and forecasting experiments quilty and adamowski 2018 analyzed the effect of using future data in wavelet based decomposition ensemble models fang et al 2019 used an overall decomposition based sampling technique and stepwise decomposition based sampling technique to perform hindcasting and forecasting experiments respectively however the results of these studies revealed that the decomposition ensemble models used in forecasting experiments seriously suffer from the boundary effect for instance in the emd or wa of a time series with finite length as the decomposition process gets closer to the end of the time series domain the decomposition values diverge from the corresponding actual decomposition values obtained from the hindcast experiment in which the actual future values near the end of the time series are observed xiong et al 2014 zhang et al 2015 the boundary effect can degrade the time series modeling quality and overall prediction performance when building a decomposition ensemble model for streamflow forecasting zhang et al 2015 therefore it is a challenge to establish a robust decomposition ensemble model for forecasting experiments zhang et al 2015 suggested that mathematical extension methods can relieve the influence of the boundary effect tan et al 2018 proposed the use of adaptive decomposition ensemble models which are adjusted as long as new streamflow is added to adapt to the boundary effect quilty and adamowski 2018 removed the boundary affected wavelet and scaling coefficients to overcome the influence of the boundary effect however it is very difficult to find a suitable mathematical extension method for different decomposition algorithms and different time series removing the boundary affected coefficients might result in the loss of some valuable information for streamflow forecasting additionally the aforementioned practical decomposition ensemble models build one predictand learning model for each subsignal and the final predictand results are obtained by summing the predictands of subsignals which is quite laborious and may sometimes cause an error accumulation problem therefore we would like to highlight solutions to two potential gaps between the existing practical decomposition ensemble models and practical reliable and efficient decomposition ensemble models i e 1 build a simple decomposition ensemble forecasting scheme to adapt to the boundary effect rather than correct or remove it and 2 build more reliable and efficient decomposition ensemble models that can directly obtain the streamflow predictands furthermore the high irregularity complex nonlinearity and multiscale variability of natural streamflow series are challenging aspects of real forecasting in general 1 shallow learning models cannot sufficiently represent distinct information bai et al 2016 he et al 2019 e g back propagation neural networks bpnns often have slow learning speeds converge to local minima liu et al 2019 and are affected by overfitting huang et al 2014 and svrs are very sensitive to hyperparameter selection he et al 2019 and 2 the commonly used time series preprocessing techniques are often subject to the drawbacks of the decomposition process e g emd is not suitable for practical streamflow forecasting because the decomposition level changes as the decomposition process unfolds fang et al 2019 and emd also has limitations associated with the sensitivity to sampling and noise dragomiretskiy and zosso 2014 eemd is not theoretically well founded liu et al 2018b and the effectiveness of wt strongly relies on the selection of the mother wavelet and decomposition level jiang et al 2019 naik et al 2018 to fill the aforementioned gaps we designed a novel decomposition ensemble forecasting scheme namely single model forecasting sf in which 1 the entire streamflow is first divided into training and validation sets the training set is later decomposed concurrently and the validation set is sequentially appended to the training set and decomposed 2 the training and validation samples are generated by combining the predictors of each subsignal as the final predictors and selecting the original streamflow as the predicted target a technique that was also used by maheswaran and khosa 2013 du et al 2017 and quilty and adamowski 2018 etc for the sf scheme and 3 the validation samples are further divided into development samples for tuning the hyperparameters of lstm models and testing samples for providing high confidence on the tuned lstm models the sf scheme tunes models based on training and development samples therefore it can assess the validation distribution during the training stage to adapt to the boundary effect furthermore to address the aforementioned challenges we introduce a deep learning model that can improve the prediction performance and a time series preprocessing technique that is robust to noise and theoretically well founded to effectively overcome these drawbacks lstm hochreiter and schmidhuber 1997 deep learning models have the ability to tackle this task lstm models have been used successfully in accident diagnosis yang and kim 2018 electricity price prediction ghoddusi et al 2019 peng et al 2018 water table depth forecasting zhang et al 2018 monthly runoff forecasting yuan et al 2018 rainfall runoff modeling kratzert et al 2018 etc recently a theoretically well founded and robust vmd method dragomiretskiy and zosso 2014 was successfully applied to container throughput forecasting niu et al 2018 vibro acoustic feature extraction mohanty et al 2018 and chatter detection in milling processors liu et al 2018a and streamflow forecasting he et al 2019 xie et al 2019 fang et al 2019 as well as in other applications vmd is more sensitive to noise and sampling than the existing decomposition algorithms such as emd and eemd dragomiretskiy and zosso 2014 therefore the sf scheme based on vmd and lstm referred to as sf vmd lstm is proposed to practically forecast daily streamflow 1 3 5 and 7 days ahead in this study in summary the main purpose of this study is to develop a skilled reliable and efficient decomposition ensemble model and identify good model parameter sets for forecasting daily streamflow series with high irregularity complex nonlinearity and multiscale variability without using any information from the future specifically to demonstrate the efficiency and reliability of sf vmd lstm non decomposition based lstm models and decomposition ensemble based lstm models established using two other practical decomposition ensemble forecasting schemes namely multi model ensemble forecasting mef and sf with the most influential subsignals sfmis and two other decomposition algorithms namely discrete wavelet transform dwt which is any wavelet transform or analysis for which the wavelet are discretely sampled and eemd were evaluated for comparative analysis additionally as the hindcast experiment used validation during the decomposition process the subsignals obtained in the hindcast experiment basically represent the true decomposition values therefore three decomposition ensemble hindcasting schemes namely multi model hindcasting meh single model hindcasting sh and single model hindcasting with the most influential subsignals shmis were evaluated as benchmarks to test the adaptability of decomposition ensemble forecasting models to the boundary effect the effectiveness of these models was assessed using the daily streamflow data from yangxian yx station han river china and zhangjiashan zjs station jing river china 2 study area and data two catchments the han river and the jing river illustrated in fig 1 were selected as case studies because the observations of meteorological factors such as rainfall and temperature are incomplete in the above catchments only the historical daily streamflow 1967 01 01 2014 12 31 from yx hydrological station the main upstream control station of the han river total drainage area of 14 484 km2 and zjs hydrological station the downstream control station of the jing river total drainage area of 43 216 km2 were collected to assess the proposed model the locations of the zjs and yx stations are also illustrated in fig 1 the hydrological information data center of the shaanxi hydrographic and water resources survey bureau provided the streamflow records the instantaneous value m3 s observed at 8 a m was selected as the average daily streamflow these records were also investigated by he et al 2019 xie et al 2019 and meng et al 2019 the han river the largest tributary of the yangtze river lies between 30 28 34 5 n and 106 42 114 55 e in the middle of china and has a total drainage area of 1 59 105 km2 yang et al 2017 yu et al 2018a this basin is located in a subtropical monsoon zone that has a humid climate and differentiated seasons yang et al 2017 the annual average precipitation of the han river basin is more than 900 mm yr therefore the water resources in this area are abundant yang et al 2017 the precipitation in the flood season july to september is approximately 75 of the annual total and the runoff displays similar seasonality yang et al 2017 yu et al 2018a the elevation of han river decreases from the western middle and low mountains with high topography to the eastern hilly and plains area with low topography xie et al 2019 forecasting the daily runoff of the han river at yangxian hydrological station can improve assessments of the incoming water from the middle south to north water transfer project in china yang et al 2017 the jing river a secondary tributary of the yellow river lies between 34 46 37 19 n and 106 14 108 42 e and has a total drainage area of 45 421 km2 he et al 2019 the jing river has complicated geomorphology and topography and it decreases in elevation from the loess high gully area in the west to the guanzhong plain in the east meng et al 2019 the climate in the jing river catchment is continental monsoonal and the average precipitation of this area is more than 400 mm yr he et al 2019 this catchment is located in a semiarid and semihumid climate transition zone and the climate and runoff of this catchment exhibit high variability he et al 2019 streamflow forecasting in the jing river basin is essential for the water resource security and regional economy in the context of the belt and road initiative in china meng et al 2019 3 methodology 3 1 mann kendall test for trend and abrupt change detection the mann kendall mk test developed by mann 1945 and kendall 1970 is used to detect the trend or abrupt changes of a streamflow series given the streamflow series x x 1 x 2 x n the null hypothesis h 0 is that x has no monotonic trend the rank series s k evaluating the difference in observations is computed as follows 1 s k i 1 i j k a ij k 2 3 n where 2 a ij s i g n x j x i 1 x i x j 0 x i x j 1 x i x j the mk test assumes x has independent observations which are identically distributed if no trend is present under this assumption the mean and variance of s k are respectively calculated by 3 e s k 0 4 v 0 s k k k 1 2 k 5 18 the test statistic of x is defined as 5 uf k s k 1 v 0 s k s 0 0 s 0 s k 1 v 0 s k s 0 the trend of x is detected by comparing uf n with the standard normal variate at the desired significance level 1 α z 1 α where α 0 α 0 5 is the tolerable probability that h 0 is falsely rejected by the mk test h 0 is rejected if uf n z 1 α otherwise h 0 is accepted under the condition that h 0 is rejected x has a monotonic increase decrease trend over time if uf n is larger smaller than zero to detect the abrupt change points the time series x was arranged in reverse order i e x x n x n 1 x 1 the above calculation process is repeated to get the test statistic sequence ub k k 2 n of x if the lines of uf k k 2 n and ub k k 2 n have intersection points within the significance interval z 1 α z 1 α these intersection points are the abrupt change point 3 2 variational mode decomposition for data decomposition a vmd algorithm developed by dragomiretskiy and zosso 2014 is applied to concurrently extract several band limited intrinsic modes from a sophisticated signal the band limited intrinsic modes of an original time series signal f t namely the intrinsic mode functions imfs are amplitude modulated frequency modulated am fm signals and they can be expressed by the following formula 6 u k t a k t cos ϕ k t where u k t a k t and ϕ k t are the imf instantaneous amplitude and phase respectively the following scheme suggested by dragomiretskiy and zosso 2014 is applied to evaluate the bandwidth of a mode the hilbert transform is first used to compute the related analytic signal of each mode u k t to obtain a one sided frequency spectrum then the frequency spectrum of u k t is shifted to the base band by mixing with an exponent adjusted to reflect the respective evaluated center frequency the h 1 gaussian smoothness of the demodulated signal i e the squared l 2 norm of the gradient is finally used to assess the bandwidth a constrained variational problem expressed by the following formula is used to solve the decomposition problem for a time series f t 7 min u k ω k k t δ t j π t u k t e j ω k t 2 2 s t k u k t f t where u k t u 1 t u 2 t u k t and ω k ω 1 ω 2 ω k are the decomposed modes and the corresponding center frequencies respectively the symbol is the convolutional operator t is time j 2 1 and δ is the dirac distribution a quadratic penalty term i e α and a lagrangian multiplier i e λ are used to obtain eq 7 unconstrained li et al 2019 niu et al 2018 the augmented lagrangian l is defined as follows 8 l u k ω k λ α k t δ t j π t u k t e j ω k t 2 2 f t k u k t 2 2 λ t f t k u k t the alternate direction method of multipliers admm is used to solve eq 8 in the vmd algorithm eq 9 is used to update the mode u k ω in the frequency domain the center frequencies ω k are updated by eq 10 and λ is simultaneously updated by eq 11 in the time domain the mode u k t is the real part of the inverse ft of u k ω expressed by eq 9 9 u k n 1 ω f ω i k u i n 1 ω i k u i n ω λ n ω 2 1 2 α ω ω k 2 10 ω k n 1 0 ω u k n 1 ω 2 d ω 0 u k n 1 ω 2 d ω 11 λ n 1 ω λ n ω τ f ω k u k n 1 ω where n is the iterations τ is the iterative factor indicating the noise tolerance of vmd and u k n 1 ω f ω and λ n ω represent the fts of u k n 1 t f t and λ n t respectively the iterative procedure continues until 12 k u k n 1 u k n 2 2 u k n 2 2 ɛ is reached in eq 12 ɛ is the convergence tolerance in general the decomposition level k the quadratic penalty term α the noise tolerance τ and the convergence tolerance ɛ affect the decomposition performance of vmd it is difficult to determine the appropriate decomposition level in vmd too few imfs can result in poor extraction of the signal components in the original time series whereas too many imfs can be computationally expensive in model training in addition too many imfs can cause duplicate information to be included among signal components niu et al 2018 xu et al 2019 a value of α that is too small leads to a large bandwidth which further leads to redundant information and the inclusion of additional noise in the signal components a value of α that is too large leads to a very small bandwidth which further leads to the loss of some signal information xu et al 2019 as shown in eqs 9 11 and 12 λ hinders the convergence when τ 0 which leads the decomposed signal components to have a large noise level setting τ to 0 can avoid this hindering problem however summing the signal components cannot precisely reconstruct the input signal if τ equals 0 additionally ɛ affects the reconstruction error of the vmd decomposition the implementation process of the vmd model is summarized in algorithm 1 algorithm 1 the vmd process initialize u k 1 ω k 1 λ 1 n 0 repeat n n 1 for k 1 k do update u k ω for all ω 0 using eq 9 update ω k using eq 10 end for update λ n ω for all ω 0 using eq 11 until convergence k u k n 1 u k n 2 2 u k n 2 2 ɛ obtain u k n 1 t by the fast ft of u k n 1 ω 3 3 long short term memory for predictand learning lstm neural networks are a specific type of rnn used for modeling time series designed to overcome the problem caused by gradient vanishing or exploding in the process of training the rnn using back propagation through time bptt y bengio et al 1994 goodfellow et al 2016 hochreiter 1998 an unfolded computational graph as illustrated in fig 2 is used to explain the working principle of the rnn and lstm methods the rnn and lstm output in our case is the predicted daily streamflow y at a particular step and the input is the n lagged daily streamflow x x 1 x n from the particular time step both rnn and lstm methods have chain like structures of repeating modules with self connected hidden units to help them remember previous information the lstm and rnn methods differ in that lstm has more complicated operations inside the repeating modules kratzert et al 2018 zhang et al 2018 which is depicted in fig 2 there is only one internal state h t for a traditional rnn see fig 2 a for each time step from t 1 to t n the following equation is used to compute h t goodfellow et al 2016 kratzert et al 2018 13 h t t a n h w x t u h t 1 b h where w u and b h are the input weight matrix recurrent weights matrix and bias vectors respectively the symbol x t represents the current input vector h t 1 denotes the last hidden cell state and the initial state of h t is h 0 0 lstm in contrast has a memory block including a cell state c t in which information is stored and three gates namely the forget gate f t the input gate i t and the output gate o t that control information propagation within the lstm memory block hochreiter and schmidhuber 1997 kratzert et al 2018 the forget gate which controls the last cell state c t 1 will be forgotten and is first computed by the following equation kratzert et al 2018 14 f t σ w f x t u f h t 1 b f where σ is the logistic sigmoid activation function and w f u f and b f are the input weight matrix recurrent weight matrix and bias vector for the forget gate respectively the initial state of h t for lstm is also h 0 0 then a potential cell state c t is calculated from the current input x t and the last hidden state h t 1 kratzert et al 2018 15 c t t a n h w c x t u c h t 1 b c where w c u c and b c are the input weight matrix recurrent weight matrix and bias vector for the potential cell state respectively the input gate which controls which information stored in the potential cell state is let through to update the current cell state c t is computed by the following equation kratzert et al 2018 16 i t σ w i x t u i h t 1 b i where w i u i and b i are the input weight matrix recurrent weight matrix and bias vector for the input gate respectively in the following step the current cell state is updated using the results of eqs 14 16 kratzert et al 2018 17 c t f t c t 1 i t c t where indicates elementwise multiplication because the logistic sigmoid function can map its input into the range 0 1 the vectors f t and i t both have values in this range therefore the previous information stored in the last cell state is remembered if f t is close to 1 and forgotten if f t is close to 0 the information stored in the current potential cell state is passed through the current cell state if i t is close to 1 and ignored if i t is close 0 moreover the initial cell state of c t is c 0 0 additionally the output gate which controls which information stored in the current cell state flows into the new hidden state is finally computed by the following equation kratzert et al 2018 18 o t σ w o x t u o h t 1 b o where w o u o and b o represent the input weight matrix recurrent weight matrix and bias vector for the output gate respectively similarly o t also has values in the range 0 1 the new hidden state h t can be computed by the following equation 19 h t o t t a n h c t due to the simple linear operation of the cell state at a particular time step it is very easy for information to simply flow along the cell state unchanged therefore the gradient does not vanish or explode when training an lstm with bptt hochreiter and schmidhuber 1997 kratzert et al 2018 the final predicted values can be computed from the output of a traditional dense layer which is connected to the last lstm layer at the last time step kratzert et al 2018 20 y w d h n b d where w d and b d are the hidden to output weight matrix and the bias vector of the dense layer respectively 3 4 decision tree for subsignal importance measurement decision trees are a commonly used attribute importance measurement tool deng et al 2011 decision trees can be explained by a flowchart like structure as diagrammed in fig 3 in which the root node and internal nodes represent the attribute test the branches of each node represent the possible outcomes of the attribute test the leaf nodes represent the class labels or predictand values and the paths from the root node to the leaf nodes represent the classification or regression rules mitchell 1997 given the training dataset x k y k k 1 n where x k x ki i 1 m represents the feature space y k represents the predicted target and m and n are the number of features and samples respectively the decision regression tree divides the feature space into l subspaces or l leaf nodes i e r j j 1 l by performing recursive partitioning the average of the predicted targets belongs to r j i e μ j 1 n j k 1 n j y k x k r j n j is the number of samples reaching node j and is the predictand value the feature importance is computed by the weighted decrease in node impurity the node impurity of the decision regression tree is defined as 21 c j 1 n j k 1 n j y k μ j 2 x k r j then the importance of feature i at node j is defined as 22 ni j w j c j w left c l e f t j w right c r i g h t j where w j n j n is weighted number of samples in node j and l e f t j and r i g h t j are the respective children nodes of node j the feature importance of feature i is defined as 23 fi i j n o d e j s p l i t s o n f e a t u r e i ni j j a l l n o d e s ni j in random forecast rf or gradient boosting regression trees gbrt the final feature importance is its average over all of the trees in this study the decomposed subsignals are the input features and the original streamflow is the output target 3 5 decomposition ensemble models for streamflow forecasting fig 4 shows a diagram of the boundary effect fig 4 a displays an example of using future data as decomposition parameters fig 4 b displays an example of decompositions diverging from the truth if no future data is used as decomposition parameters fig 4 c displays an example of the distribution of sequential decompositions red line i e the validation distribution in the forecasting experiment which is different from the distribution of concurrent decompositions blue line i e the validation distribution in the hindcasting experiment the validation distribution in the hindcasting experiment also represents the training distribution because the decomposition of both the training and validation sets uses future data as decomposition parameters as shown in fig 4 a given observations of a streamflow series the local maxima and minima are used as decomposition parameters to generate the upper and lower envelopes which are further used to extract decompositions however the local maxima and minima of future data are not observed yet therefore the upper and lower envelopes closer to the end of the observations are extrapolated and not precise the boundary effect is that the decompositions closer to the end of observations red line in fig 4 b diverge from actuality blue line in fig 4 b which is obtained by using future data as decomposition parameters therefore concurrently decomposing a streamflow series lead to every observation of this streamflow using its future data until no future data is available in the experiments of streamflow hindcasting the entire streamflow is first decomposed into subsignals and each subsignal is then trained and validated zhang et al 2015 a few decompositions close to the end of the entire streamflow suffer from the boundary effect because the future data of the entire streamflow is not known meanwhile the training decompositions using some information belong to the validation period however the validation information is not available at the present moment for streamflow forecasting therefore streamflow hindcasting is not practical in the experiments of streamflow forecasting to avoid using validation information the entire daily streamflow series is first divided into training and validation sets and the training set is then decomposed into subsignals and applied to train the forecasting models zhang et al 2015 to predict the daily streamflow based on the validation set the daily streamflow in the validation set is sequentially appended to the training set because streamflow is observed daily in many real streamflow forecasting applications and the decomposition process is repeated with the daily streamflow of the next day appended therefore a few decompositions close to the end of training period and the entire decompositions during validation period suffer from the boundary effect which lead to the training and validation decompositions having different distributions as shown in fig 4 c the model trained on the training decompositions hence may not generalize well to the validation decompositions as the streamflow hindcasting models use the validation data e g the local maxima and minima of future data in fig 4 a as decomposition parameters the decompositions in the hindcasting models basically represent the true decomposition values therefore streamflow hindcasting models can still be used as benchmarks for evaluating the adaptability of streamflow forecasting models to the boundary effect in this study we designed a skilled reliable and efficient decomposition ensemble forecasting scheme namely sf in which the validation distribution is assessed during the training stage to adapt to the boundary effect the sf scheme establishes one lstm model using all predictors of the subsignals as the input and the original streamflow as the output this technique was also used by maheswaran and khosa 2013 du et al 2017 and quilty and adamowski 2018 etc therefore the sf scheme can simultaneously simulate the predictor predictand and decomposition streamflow relationships and it is efficient to demonstrate the performance reliability and efficiency of the sf scheme two other decomposition ensemble forecasting schemes namely mef and sfmis were also evaluated the mef scheme establishes one lstm model for each subsignal and the ensemble predictands are obtained by summing the predictands of the subsignals establishing one lstm model for each subsignal in the mef scheme is quite laborious and may cause an error accumulation problem however combining subsignal predictors as the input of lstm in sf can sometimes result in an overfitting problem hence the sfmis scheme establishes one lstm model using predictors of the most influential subsignals as the input and the original streamflow as the output however the sfmis scheme can lead to poor prediction performance due to lost information in this study the sf scheme based on vmd and lstm namely sf vmd lstm is proposed to forecast daily streamflow to demonstrate the reliability and efficiency of sf vmd lstm the mef and sfmis schemes based on vmd and lstm namely mef vmd lstm and sfmis vmd lstm were evaluated to demonstrate that decomposition of streamflow contributes to improvements in forecasting performance the aforementioned decomposition ensemble schemes were compared with non decomposition based monoscale lstm models additionally to test the superiority of vmd in decomposing streamflow the aforementioned decomposition ensemble schemes were also compared with the decomposition ensemble schemes based on eemd dwt and lstm namely mef eemd lstm sf eemd lstm sfmis eemd lstm mef dwt lstm sf dwt lstm and sfmis dwt lstm furthermore to demonstrate the adaptability of decomposition ensemble forecasting models to the boundary effect three decomposition ensemble hindcasting schemes namely meh sh and single model hindcasting with most influential subsignals shmis based on vmd eemd dwt and lstm namely meh vmd lstm sh vmd lstm shmis vmd lstm meh eemd lstm sh eemd lstm shmis eemd lstm meh dwt lstm sh dwt lstm and shmis dwt lstm were evaluated these hindcasting schemes have the same modeling processes as the corresponding forecasting schemes except that the hindcasting schemes decompose the entire streamflow first and later split the decomposed subsignal into training and validation sets in this study we performed 1 3 5 and 7 day streamflow forecasting and hindcasting for all the decomposition ensemble models and monnscale lstm models the decomposition level k of vmd was determined by observing the center frequency aliasing phenomenon based on experimental validation the quadratic penalty term α and the convergence tolerance ɛ of vmd were set to 2000 and 1e 9 respectively to obtain uncorrelated vmd subsignals with a low noise level the noise tolerance τ was set to 0 specifically eemd decomposed the streamflow into several imfs and one residual and the number of ensemble members m and the amplitude of white noise ɛ were the critical parameters influencing the decomposition performance of eemd as suggested by wu and huang 2009 ɛ and m were set to 0 2 and 100 respectively additionally dwt decomposes the streamflow into l detail series and one approximation series given a decomposition level of l the mother wavelet and decomposition level are vital parameters that influence the decomposition performance of dwt as suggested by sachindra et al 2019 the mother wavelets including haar db2 bior 3 3 db5 coif3 db10 db15 db20 db25 db30 db35 db40 and db45 were evaluated to find the optimal mother wavelet and the decomposition level l was set to 1 2 and 3 to find the optimal decomposition level the decomposition ensemble models of streamflow forecasting are summarized as follows and presented in fig 5 step 1 collect the original daily streamflow signal q t t 1 2 n where n is the data length step 2 divide the streamflow signal into a training set q t t t 1 2 n t where n t is the training set length which is 80 of the data length and a validation set q v t t 1 2 n v where n v is the validation set length which is 20 of the data length step 3 decompose the training set q t t into k subsignals imfs s t 1 t s t 2 t s tk t using the decomposition algorithms such as vmd eemd and dwt specifically determining the optimal decomposition level k of vmd by observing the center frequency aliasing phenomenon of the last subsignal was suggested in this study step 4 sequentially append the validation data to the training set to generate new appended signals q tv 1 t q tv 2 t q tv n v t decompose each appended signal q tv j j 1 2 n v into k sets of appended subsignals s t v 1 j t s t v 2 j t s tvk j t using the same decomposition algorithm used in step 3 step 5 plot the partial autocorrelation coefficient figure pacf of each subsignal s tk t k 1 2 k to select the predictors and predicted targets for the lstm model assume that the predicted target for mef scheme is s k t m m is the leading time and the predicted target for sf and sfmis scheme is q t m the lag days of s tk t out of the confidence interval 1 96 n t 1 96 n t are selected as predictors step 6 according to the predictors and predicted target given in step 5 generate training samples of the mef sf or sfmis scheme for training the lstm model using the subsignals of the training set given in step 3 step 7 according to the predictors and predicted target given in step 5 generate samples of mef sf or sfmis using the j th appended subsignals s tvk j t k 1 k and extract the last sample of each set of appended subsignals as a validation sample divide the validation samples into two subsets development samples 50 of the validation samples for determining the optimal model structure and hyperparameters and testing samples 50 of the validation samples for evaluating the model performance step 8 train the lstm model for the mef sf or sfmis schemes using the training samples and monitor the prediction performance of each model based on the development samples to obtain the optimal lstm model input the test sample predictors obtained in step 7 into the optimal model to predict the results for each subsignal step 9 obtain the ensemble prediction results for the mef scheme by summing the predictands of the subsignals or obtain the prediction results of the sf or sfmis schemes by directly outputting the predictands of lstm perform an error analysis on the mef sf or sfmis schemes 3 6 criteria for performance evaluation to assess the prediction ability of the decomposition ensemble models three error analysis criteria were applied these criteria are expressed as shown in table 1 the normalized root mean square error nrmse assesses the model performance in predicting high streamflow values the nash sutcliffe efficiency nse indicates how well observations are replicated by the results of the decomposition ensemble models the peak percentage of threshold statistic lohani et al 2014 denotes the ability to forecast peak flow bai et al 2016 stojković et al 2017 the term ppts γ computes the average absolute relative error of predictands of the top γ streamflow data the lower the ppts the better the capability to forecast peak flow note that the recorded streamflow values are arranged in descending order to compute the ppts and that the threshold level γ denotes the percentage of data selected from the beginning of the arranged data sequence the parameter g is the number of values above the threshold level γ the symbol n is the number of samples x t is the original streamflow x t is the average of the original streamflow and x t is the predicted streamflow 4 case study 4 1 open source software this study relied heavily on open source software pandas mckinney 2010 and numpy stéfan et al 2011 were used to manage and process streamflow data matlab was used to perform the streamflow decomposition tasks and compute the pacf of subsignals the matlab implementations of vmd and eemd were derived from dragomiretskiy and zosso 2014 and wu and huang 2009 respectively the dwt was performed based on the matlab build in toolbox wavelet 1 d in wavelet analyzer the gbrt model in scikit learn pedregosa et al 2011 was used to measure the importance of the decomposed subsignals matplotlib hunter 2007 was used to draw figures and tensorflow abadi et al 2016 was used to train the lstm models these open source software have also partly been used by previous researchers e g kratzert et al 2018 4 2 data analysis and partitioning the mk test hamed 2008 was used to detect possible abrupt shifts and gradual trends in the data from the two hydrological stations the detection results and annual runoff are presented in fig 6 as shown in fig 6 a the fluctuation in annual runoff at yx station is more obvious than that at zjs station illustrating that the runoff at yx station is more nonstationary than the runoff at zjs station the linear trends of the two stations suggest that the average annual runoff at the two stations has been gradually decreasing as shown in fig 6 b and c 1996 was identified as a year when the annual runoff trend at the two stations passed the mk abrupt change test reflecting an abrupt shift because the runoff generation trends before the abrupt shift are inconsistent with those after the abrupt shift the time series after the abrupt shift was selected to evaluate the models the daily streamflow records obtained at the two hydrological stations from 1 1 1997 to 31 12 2014 a total of 6 574 samples were used to develop the present model as described in section 3 5 80 of the streamflow records from 1 1 1997 to 27 05 2011 were selected as the training set and 20 of the streamflow records from 28 05 2011 to 31 12 2014 were selected as the validation set the validation set was further divided into a development set from 28 05 2011 to 14 03 2013 and testing set from 15 03 2011 to 31 12 2014 4 3 data normalization because the range of the raw time series varies widely in some ml models the optimization algorithms applied to obtain the objective functions will not work properly without data normalization in addition data normalization can make the optimization algorithms converge much faster therefore all of the predictors and predicted targets used to model the original streamflow and the subsignals in this study were normalized to the same scale of 1 1 the normalization equation is given as follows 24 x 2 x x min x max x min 1 where x and x are the original and normalized value respectively the symbols x min and x max are the minimum and maximum value of the original series respectively note that the normalized indicators of the training samples i e x max and x min should also be used to normalize both the development and testing samples so that the samples satisfy the same distribution the reason we used eq 24 to normalize the predictors and predicted targets is that the decomposition results of vmd eemd and dwt contained no outliers but rather had negative values and this normalization equation will ensure that all predictors and predicted targets are equally scaled to 1 1 4 4 modeling process according to the forecasting schemes introduced in section 3 5 the decomposition ensemble models and monoscale lstm model were evaluated using the daily streamflow data from zjs station and yx station as described in section 4 2 the modeling process of the decomposition ensemble schemes includes data decomposition the determination of predictors and predicted targets and streamflow forecasting based on experiments we found that the optimal decomposition mode number of vmd can be determined by the aliasing phenomenon of the center frequency of the last imf taking vmd of zjs for example we evaluated k from k 2 to k 12 and the 12th imf of zjs exhibited obvious aliasing phenomena area surrounded by a red rectangular border in fig 7 to satisfy the orthogonality constraint and avoid spurious components as much as possible the k of vmd for zjs was chosen as 11 similarly the k of vmd for yx was chosen as 9 by observing the aliasing phenomena given the parameter set of eemd described in section 3 5 12 subsignals were decomposed from the original streamflow of zjs and yx the subsignal number of dwt depended on the decomposition level two three and four subsignals were decomposed from original streamflow of zjs and yx when the decomposition levels of dwt were set to 1 2 and 3 respectively gbrt was used to measure the importance of the subsignals the subsignals of vmd at zjs station was used as an example for determining the most important influential subsignals of original streamflow from fig 8 as shown in fig 8 the subsignals with importance larger than the median were imf2 imf3 imf4 imf9 and imf5 and the subsignals with importance larger than the mean were imf2 imf3 imf4 imf9 because there is no universally accepted threshold for most importance we chose the subsignals with importance larger than the mean as the most influential subsignals in this manner the most influential subsignals of vmd eemd and dwt at zjs and yx station could be determined the predictors and predicted targets should first be determined to generate training development and testing samples for lstm models based on the input selection methods suggested by wang et al 2009 and he et al 2014 the predictors used by training models to predict the subsignals obtained by vmd eemd and dwt can easily be obtained from the plots of pacfs the first subsignal i e imf1 of vmd for zjs was used as an example to illustrate the determination of predictors from fig 9 the 1 3 5 and 7 day ahead predicted targets for imf1 were s 1 t 1 s 1 t 3 s 1 t 5 and s 1 t 7 then the predictors were s 1 t and s 1 t 1 in other words we used the past two days to predict these targets according to the predictors and predicted targets of the mef sf sfmis meh sh and shmis schemes described in section 3 5 the training and validation samples were first generated using the training and validation decomposition results respectively the validation samples were then divided into two subsets the development samples for the selection of the optimal lstm model with the lowest nrmse and the testing samples for evaluating the performance of the selected lstm model selecting the hyperparameters of lstm models such as the number of hidden layers the number of hidden units the batch size the dropout rate and the learning rate remains a difficult task random search bergstra and bengio 2012 can be used to optimize these hyperparameters however due to computational resource limitations an experimental method that increases or decreases the values of these hyperparameters was used to determine the optimal hyperparameter values for the lstm modeling the learning rate was tuned first and the number of hidden layers the number of hidden units and the dropout rate were later adjusted together the number of batches was set to 512 as used by kratzert et al 2018 the other hyperparameters of the lstm models were set to the default values used by tensorflow the first subsignal of the vmd for zjs was used as an instance of hyperparameter tuning to demonstrate the experimental process in the experimental process of imf1 for zjs an lstm structure of 2 8 1 was initialized 2 input units 8 hidden units and one output unit the dropout rate was initialized as 0 the stopping condition was reached when the lstm model converged we trained 10 models for each lstm with specific hyperparameter values to reduce the impact of the random initialization of weights and the model with the lowest nrmse in the development period was selected as the optimal model in this study we first investigated 10 learning rates see fig 10 for training the lstm models fig 10 shows that learning rates of 0 007 0 01 0 03 and 0 1 rapidly converged however the loss lines of the 0 03 and 0 1 learning rates exhibited large variations in late training epochs and the learning rate of 0 007 displayed worse performance than the learning rate of 0 01 in the late training epochs therefore the learning rate of 0 01 is selected as the optimal learning rate for imf1 at zjs station then the number of hidden units was designed based on 4 levels ranging from 8 to 32 with an interval of 8 because decomposed subsignals are relatively stationary one hidden layer is sufficient for simulating the predictor predictand relationship of streamflow therefore the number of hidden layers was set as 1 to avoid overfitting of lstm the dropout rate was designed based on 10 levels ranging from 0 0 to 0 9 with an interval of 0 1 the imf1 of vmd for zjs was also employed as an example to illustrate tracking the optimal model structure from fig 11 the results indicate that the optimal nrmse values were obtained for the model structure 2 16 1 2 input units 16 hidden units and 1 output unit with a dropout rate of 0 0 therefore the optimal predicted structure of imf1 for zjs was the lstm model with one hidden layer i e 2 16 1 and a dropout rate of 0 0 according to the hyperparameter adjustment procedure used for imf1 of vmd at zjs the hyperparameters including optimal learning rates hidden units and dropout rates and the parameters including weights and biases of the mef sf sfmis meh sh and shmis schemes were optimized with these tuned hyperparameters and parameters the predictands were obtained by lstm models and renormalized into the original data scale using the inverse procedure of eq 24 note that the final predicted streamflow of the mef and meh schemes were obtained by summing the predictands of the subsignals and the predicted streamflow of the sf sfmis sh and shmis schemes exactly matched the predictands of the lstm models 5 results analysis 5 1 comparative analysis of decomposition ensemble forecasting models figs 12 14 show the nse nrmse and ppts 5 for training and development of the mef sf and sfmis based lstm models with different decomposition algorithms at zjs and yx stations in figs 12 14 the horizontal axis illustrates the forecasting scheme station leading time and model stage e g mef zjs l1 t mef zjs l1 d indicates the training development stage of the mef scheme for forecasting streamflow 1 day ahead at zjs station and the vertical axis illustrates the decomposition algorithms in the vertical axis monoscale indicates non decomposition based lstm models and the rest of the decomposition algorithms except for eemd and vmd are dwts e g haar 3 indicates that the wavelet and decomposition level of dwt are haar and 3 respectively the horizontal and vertical axes represent the decomposition ensemble based lstm models e g sf zjs l1 t and vmd represent the sf vmd lstm model for forecasting streamflow 1 day ahead during the training period at zjs station the red squares in figs 12 14 illustrating the optimal decomposition algorithms correspond to forecasting scheme station leading time and model stage as shown in figs 12 14 irrespective of decomposition algorithms stations leading times and model stages the sf based lstm models display relatively high nse low nrmse and ppts 5 compared with the mef based models and much higher nse and lower nrmse and ppts 5 compared with the sfmis based models the results indicate that the sf scheme improves the forecasting performance compared with the mef scheme and the sfmis scheme loses some valuable information which leads to poor forecasting performance irrespective of forecasting schemes stations leading times and model stages the non decomposition based lstm models display obviously low nse and high nrmse and ppts 5 compared with the decomposition based lstm models indicating that decomposition ensemble forecasting models significantly improved the forecasting performance irrespective of forecasting schemes stations leading times and model stages the vmd based lstm models display relatively high nse and low nrmse and ppts 5 compared with the other decomposition based lstm models indicating that vmd based lstm models generally have better streamflow forecasting performance irrespective of decomposition algorithms forecasting schemes stations and model stages the nse nrmse and ppts 5 of the 1 3 5 and 7 day ahead streamflow forecasting gradually decreased increased as the leading times increased indicating that the prediction performance decreases as the leading time increases furthermore irrespective of wavelets forecasting schemes stations leading times and model stages for the dwt based lstm model the decomposition level of 3 displays relatively high nse and low nrmse and ppts 5 compared with the decomposition levels of 1 and 2 indicating that increased decomposition level of dwt improves the streamflow forecasting performance as the best nse of dwt with a decomposition level of 3 is larger than 0 95 for forecasting streamflow 1 day ahead the decomposition level of 3 is sufficient for the dwt based lstm models for daily streamflow forecasting additionally the dwt based lstm models with wavelets including db25 db30 db35 db40 and db45 and a decomposition level of 3 have better forecasting performance compared with the other dwt based lstm models in this study we chose the wavelet and decomposition level of dwt as db45 and 3 respectively to practically predict streamflow 1 3 5 and 7 days ahead in the subsequent experiments in general sf vmd lstm outperforms other lstm models is more efficient than the mef based lstm models and is more reliable than the sfmis based lstm models therefore sf vmd lstm is suggested to forecast daily streamflow in this study fig 15 shows the nse nrmse and ppts 5 of sf vmd lstm sf dwt lstm with wavelet of db45 and decomposition level of 3 sf eemd lstm and monoscale lstm for forecasting streamflow 1 3 5 and 7 days ahead at zjs and yx stations during the testing period as shown in fig 15 sf vmd lstm displays relatively high nse and low nrmse and ppts 5 for both zjs and yx stations compared with sf dwt lstm and much higher nse and lower nrmse and ppts 5 compared with sf eemd lstm and lstm the nse values of sf vmd lstm except for 7 day ahead streamflow forecasting at yx station were larger than 0 8 which is a threshold value for reasonably well performing models used by newman et al 2015 the nse nrmse and ppts 5 values of sf dwt lstm were similar to those of sf vmd lstm for 1 day ahead streamflow forecasting but slight smaller larger than those of sf vmd lstm for forecasting streamflow 3 5 and 7 days ahead the nse nrmse and ppts 5 values of lstm were similar to sf eemd lstm and much smaller larger than those of sf vmd lstm furthermore the nse nrmse and ppts 5 values of lstm were sometimes larger smaller than those of sf eemd lstm in general these results indicate that 1 sf vmd lstm is skilled reliable and efficient for daily streamflow forecasting and has good generalization ability for different catchments 2 both sf vmd lstm and sf dwt lstm play positive roles in improving the streamflow forecasting performance of decomposition ensemble models and 3 eemd based decomposition ensemble models do not always improve streamflow forecasting performance 5 2 performance gap of forecasting and hindcasting schemes figs 16 and 17 present the scatter plots of sf vmd lstm and sh vmd lstm sf dwt lstm db45 3 and sh dwt lstm db45 3 and sf eemd lstm and sh eemd lstm for the 1 3 5 and 7 day ahead streamflow predictands at zjs and yx stations during the testing period as shown in figs 16 and 17 the correlation values of sf vmd lstm and sf dwt lstm are similar to those of sh vmd lstm and sh dwt lstm respectively the results indicate that the prediction performance of sf vmd lstm and sf dwt lstm is close to that of sh vmd lstm and sh dwt lstm respectively the correlation values of sf eemd lstm are mostly scattered above the ideal fit and the correlation values of sh eemd lstm are mostly scattered around the ideal fit indicating that the predictands of sf eemd lstm underestimate the records similar results can be obtained from the mef and meh and the sfmis and shmis schemes results not shown these results indicate that the prediction performance gap between decomposition ensemble forecasting and hindcasting models is very small except for the eemd based models the eemd based hindcasting models outperform the eemd based forecasting models therefore the vmd based and dwt based forecasting models have great adaptability to the boundary effect and eemd based forecasting models have poor adaptability to boundary effect 6 discussion section 5 1 indicates that the forecasting performance of vmd based lstm models is slightly better and much better than that of dwt based and eemd based lstm models respectively the reason that vmd outperforms dwt and eemd can be demonstrated by the pearson correlation coefficients of the subsignals illustrated by fig 18 and the frequency spectrum of the most difficultly predicted subsignals i e the last imf of vmd the first imf of eemd and the first detail subsignal of dwt as illustrated by fig 19 fig 18 shows that the pearson correlation coefficients of the subsignals of vmd and dwt are lower than that of eemd indicating that the subsignals of vmd and dwt are more uncorrelated than those of eemd in other words the subsignals of eemd have duplicate information and hence chaotically represent the period trend and noise as shown in fig 19 the most difficultly predicted subsignal of vmd displays a very low noise level in the low frequency domain the most difficultly predicted subsignal of dwt displays a large noise level in the low frequency domain and the most difficultly predicted subsignal of eemd displays a large noise level along the entire frequency domain this result indicates that the first subsignal of eemd is very difficult to predict therefore eemd leads to poor streamflow forecasting performance although the subsignals of dwt are more uncorrelated than those of vmd they have a larger noise level than those of vmd therefore dwt based lstm models have slightly poor streamflow forecasting performance than vmd but still are powerful decomposition ensemble forecasting models in general vmd is capable of controlling the center frequency aliasing and the noise level therefore it is more feasible for streamflow forecasting section 5 1 also indicates that the sf scheme is more skilled reliable and efficient than mef and sfmis the reason is that the sf scheme simultaneously simulates the predictor predictand and decomposition streamflow relationships additionally the sf scheme can not only avoid the error accumulation problem but also improve the prediction performance of decomposition ensemble models compared with the mef scheme the reconstruction or summation of subsignals sometimes cannot completely reproduce the original streamflow especially for the decomposition of vmd in this study because vmd controls the noise at a low level vmd removes some noise components from the original streamflow therefore the mef scheme might fail to completely predict the original streamflow even though each subsignal is precisely predicted and the sf scheme improves prediction performance compared with the mef scheme because sf simulates a relationship between subsignals and original streamflow instead of using summation of the subsignal predictands to predict original streamflow furthermore the sf scheme builds only one predictand learning model to predict original streamflow which can largely save computation resources and modeling time the sfmis scheme has much poorer prediction performance than mef and sf because sfmis only uses the most influential subsignals to predict streamflow this leads to some valuable information being removed from the decompositions although the sfmis scheme might reduce the risk of overfitting optimal prediction performance should be achieved first section 5 2 indicates that the performance gap between the decomposition ensemble forecasting and hindcasting models is very small except for in the eemd based lstm models this demonstrates that the vmd based dwt based forecasting models have great adaptability while eemd based models have poor adaptability to the boundary effect the reason can be demonstrated by the validation decomposition of eemd for hindcasting and forecasting experiments at zjs station as illustrated by fig 20 a similar result can be obtained at yx station which indicates that the validation decompositions of eemd are seriously affected by the boundary effect especially for imf5 imf11 and the residual r however as shown in figs 21 and 22 the validation decompositions of vmd and dwt at zjs station are slightly affected by the boundary effect a similar result can be obtained at yx station in general there are three ways of handling the boundary effect i e 1 improve the decomposition algorithms e g remove the boundary affected coefficients a method used by quilty and adamowski 2018 2 correct the boundary effect e g the mathematical extension method used by zhang et al 2015 or 3 tolerate the boundary effect rather than eliminate or reduce it e g the adaptive decomposition ensemble model proposed by tan et al 2018 because the boundary effect is very difficult to eliminate in the perspective of signal processing we suggest building an adaptive decomposition ensemble model that can adapt to the boundary effect notably the method used by tan et al 2018 can adapt to the boundary effect however the model must be retrained as long as new runoff information is added and the trained model does not provide users with a high confidence level for the unused data as shown in section 3 5 the main drawback of decomposition ensemble models is that the training and validation samples have different distributions as shown in fig 4 the training samples are smoother than the validation samples so the tuned models cannot generalize well from the training to validation samples therefore decomposition ensemble models trained on training samples and simultaneously accessing the validation distribution can adapt to the boundary effect in this study the decomposition ensemble forecasting models were adapted to the boundary effect using three sets of samples i e training samples for computing parameters development samples for tuning the hyperparameters and selecting the optimal model and testing samples for confirming the high confidence level of the tuned model note that the development and testing samples were from the validation distribution hence the decomposition ensemble models simultaneously trained and developed on training and development samples assessed the validation distribution during the training period and thus adapted to the boundary effect additionally testing samples are completely unused in the trained and developed models therefore they provide users with a high confidence in the prediction performance of the decomposition ensemble models in general adapting to the boundary effect by assessing the validation distribution during the training stage is simple and very easy to implement in general the keys to practically forecasting nonstationary and nonlinear streamflow in catchments with a lack of meteorological observations e g rainfall evaporation and temperature as input predictors are to 1 decompose the nonstationary and nonlinear streamflow into relatively stationary signals and appropriately deal with the noise terms hidden inside the original streamflow 2 avoid using validation information when decomposing the streamflow series and 3 automatically identify how the historical streamflow information affect the future streamflow this study demonstrates the proposed sf vmd lstm model is practical robust and efficient for forecasting highly nonstationary and nonlinear streamflow the reasons for the practicability robustness and efficiency of sf vmd lstm can be summarized as follows 1 only one lstm model is established to forecast the streamflow in the sf scheme and the established lstm model does not have an obvious overfitting problem as shown in section 5 1 therefore sf vmd lstm is efficient 2 vmd decomposes training and validation sets separately to avoid using future information in the sf scheme therefore sf vmd lstm is practical 3 as described in section 3 2 the decomposition level k quadratic penalty term α noise tolerance τ and convergence tolerance ɛ of vmd can be adjusted manually to obtain uncorrelated subsignals with a very low noise level therefore vmd is more controllable and robust to sampling and noise although vmd has a relatively high number of affecting parameters that should be preassigned this study and our experimental experiences indicate that setting the quadratic penalty term α noise tolerance τ and convergence tolerance ɛ to 2000 0 and 1e 9 respectively can obtain a reasonably good streamflow forecasting performance and the decomposition level is the only parameter that should be preassigned 4 as described in section 3 3 the lstm forget gate input gate and output gate can control information propagation within the lstm memory block hence lstm can automatically remember or forget the previous input information to enhance the streamflow forecasting performance which is also demonstrated by zhang et al 2018 and kratzert et al 2018 therefore sf vmd lstm is practical robust and efficient for forecasting nonstationary and nonlinear streamflow 7 conclusions to improve the performance reliability and efficiency of practical decomposition ensemble models an sf scheme in which the validation distribution was assessed during the training period to adapt to the boundary effect was designed the sf scheme based on vmd and lstm referred to as sf vmd lstm was proposed to forecast streamflow 1 3 5 and 7 days ahead to demonstrate the superiority reliability and efficiency of sf vmd lstm non decomposition based lstm models and decomposition ensemble based lstm models established from mef and sfmis schemes and vmd dwt and eemd were evaluated for comparative analysis additionally the meh sh and shmis schemes were used as benchmarks for evaluating the adaptability of decomposition ensemble forecasting models to the boundary effect all comparisons and analyses were based on historical daily river streamflow datasets 01 01 1997 31 12 2014 from yx station han river china and zjs station jing river china the main conclusions can be summarized as follows 1 determining the decomposition level of vmd by observing the center frequency aliasing phenomenon of the last subsignal i e avoiding the center frequency aliasing of the last subsignal makes the subsignals basically satisfy the uncorrelated constraint and avoid spurious components 2 the sf scheme improves the streamflow forecasting performance and efficiency compared with the mef scheme because the original streamflow sometimes cannot be reconstructed by summing the subsignals the sf scheme is more reliable than the sfmis scheme because the sfmis scheme loses some valuable information 3 monoscale lstm models sometimes have larger nse or smaller nrmse and ppts 5 than the eemd based forecasting lstm models indicating that the decomposition ensemble models do not always improve the forecasting performance 4 the subsignals of vmd are more uncorrelated than that of eemd the frequency spectrum of the most difficultly predicted subsignal of vmd has a lower noise level than that of eemd and dwt therefore vmd has better decomposition performance than eemd and dwt 5 the decomposition ensemble forecasting models have similar prediction performance as the decomposition ensemble hindcasting models except for the eemd based models demonstrating the decomposition ensemble forecasting models have great adaptability to the boundary effect except for the eemd based lstm models 6 the proposed sf vmd lstm with nse larger than 0 8 except for 7 day daily streamflow forecasting at yx station outperformed other the sf based lstm models and the mef and sfmis based lstm models therefore it is skilled reliable and efficient for streamflow forecasting in summary the sf vmd lstm model offers the significant advantage of avoiding using future streamflow information that is not available for the current stage additionally the sf vmd lstm model established only one lstm model which uses the predictors of subsignals as the input and the original streamflow as the output to forecast streamflow the sf vmd lstm models assess the validation distribution during the training and development period without using testing information therefore the sf vmd lstm model saves modeling time and computation resources and is more robust and efficient than the decomposition ensemble forecasting models using the summation ensemble strategy e g the models developed by zhang et al 2015 tan et al 2018 fang et al 2019 overall the results demonstrate that the optimal decomposition ensemble model sf vmd lstm is a useful tool for predicting highly nonstationary and nonlinear daily streamflow values in real forecasting applications additionally sf vmd lstm can also be used for forecasting different nonlinear processes such as streamflow rainfall evaporation temperature etc four constraints must be satisfied when using sf vmd lstm i e 1 the subsignals must satisfy the orthogonality constraint and avoid spurious components as much as possible and we suggest determining the decomposition level of vmd by observing the center frequency aliasing of the last subsignal 2 the entire data must be divided into training development and testing sets to adapt to the boundary effect and provide high confidence of the sf vmd lstm model 3 to avoid using future information from development and testing samples only the normalized indicators of training samples should be used to normalize the training development and testing samples and 4 the number of hidden layers the numbers of hidden units and the dropout rate or other regularization parameter must be tuned together and varying the number of hidden units by a factor of 8 or 16 is sufficient for hyperparameter tuning we also suggest using the sf vmd lstm model if the meteorological e g rainfall evaporation temperature or remote sensing data are incomplete and the available historical streamflow data are sufficient to train develop and test the models the entire date length should be at least larger than 500 dealing with the boundary effect using a cross validation strategy trains the forecasting models on the training and validation distribution therefore this should be explored further to improve the prediction performance of decomposition ensemble forecasting models the black box nature of the skilled reliable and efficient decomposition ensemble forecasting models is quite a barrier for applying them in practice therefore the interpretability of decomposition ensemble forecasting models should also be further explored declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china under grant nos 51679186 51679188 51979221 and 51709222 and the research fund of the state key laboratory of eco hydraulics in northwest arid region xi an university of technology under grant no 2019kjcxtd 5 sincere gratitude is extended to the editor and anonymous reviewers for their professional comments and corrections appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124776 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5526,reliable and accurate streamflow forecasting is vital for water resource management many streamflow prediction studies have demonstrated the excellent prediction ability of decomposition ensemble models several studies first extracted subsignals from a streamflow series and then split subsignals into training and validation sets to build prediction models capturing some future information not available in practical streamflow forecasting alternatively dividing a time series into training and validation sets first and later decomposing them into subsignals can expose the subsignals to the boundary effect making predicting streamflow difficult furthermore building one model for each subsignal is laborious and can cause error accumulation therefore establishing a robust and efficient decomposition ensemble model without future information to predict highly nonstationary and nonlinear streamflow is challenging hence a single model forecasting sf scheme that assesses the validation distribution during the training stage to adapt to the boundary effect was designed a sf scheme based on variational mode decomposition vmd and long short term memory lstm namely sf vmd lstm was proposed to predict daily streamflow 1 7 days ahead non decomposition based and decomposition ensemble based lstm models established using sf multi model ensemble forecasting mef and sf with the most influential subsignals sfmis and the ensemble empirical mode decomposition eemd and discrete wavelet transform dwt were compared additionally multi model ensemble hindcasting meh single model hindcasting sh and sh with the most influential subsignals shmis were used as benchmarks to evaluate the forecasting schemes adaptability to the boundary effect two daily streamflow series from han river and jing river china were investigated the results indicate that 1 sf is more robust and efficient than mef and sfmis 2 vmd performs better than eemd and dwt 3 sf vmd lstm with nse values larger than 0 8 for almost all the prediction scenarios outperformed other comparative models therefore sf vmd lstm is robust and efficient for forecasting highly nonstationary and nonlinear streamflow keywords forecasting experiment hindcasting experiment decomposition and ensemble deep learning data availability the data and codes that support the findings of this study are available from zuo et al 2020 https doi org 10 17632 bhjgdhgzjr 1 nomenclature he hindcasting experiment fe forecasting experiment vmd variational mode decomposition lstm long short term memory dwt discrete wavelet transform eemd ensemble empirical mode decomposition imf intrinsic mode function gbrt gradient boosting regression trees mef multi model ensemble forecasting sf single model forecasting sfmis single model forecasting with the most influential subsignals pacf partial autocorrelation coefficient ml machine learning mk mann kendall nrmse normalized root mean square error nse nash sutcliffe efficiency ppts peak percentage of threshold statistic yx yangxian hydrological station zjs zhangjiashan hydrological station meh multi model ensemble hindcasting sh single model hindcasting shmis single model hindcasting with the most influential subsignals 1 introduction streamflow forecasting is vital for water resource management he et al 2019 huang et al 2014 sepehri and sarrafzadeh 2018 2019 therefore this topic has been investigated by numerous researchers and many prediction models have been studied in past decades yaseen et al 2015 these models have generally been divided into physically based and data driven models kratzert et al 2018 zhang et al 2015 data driven models which are based on statistical modeling are popular due to their simplicity low information requirements and robustness coulibaly et al 2000 huang et al 2014 nilsson et al 2006 shiri and kisi 2010 young and beven 1994 zhang et al 2015 zhu and fujita 1993 chen et al 2018 in data driven flow forecasting many previous researchers have applied time series models including box jenkins castellano méndez et al 2004 huang et al 2014 autoregression ar li et al 2015 moving average ma adnan et al 2017 autoregressive moving average arma mohammadi et al 2006 toth et al 2000 and autoregressive integrated moving average arima valipour et al 2013 models to forecast streamflow however due to the linear hypothesis of these models they are not suitable for forecasting streamflow with nonlinear and nonstationary characteristics therefore machine learning ml models that can be used for nonlinear mapping e g support vector regression svr huang et al 2014 yu et al 2018b tongal and booij 2018 fuzzy inference systems fis he et al 2014 yaseen et al 2017 zhou et al 2019 yang et al 2019 dehghani et al 2019 bayesian regression br humphrey et al 2016 wang et al 2017 achieng and zhu 2019 and artificial neural networks anns moradkhani et al 2004 tan et al 2018 wang et al 2009 yaseen et al 2016 have been applied to streamflow forecasting however these pure ml models cannot always adequately handle highly nonstationary time series with many noise components without preprocessing the original data zhang et al 2015 because suitable time series preprocessing can be performed to extract multiscale features that are simpler than the original signals ml techniques can efficiently describe an original signal in statistical terms and other prior information hidden in an observed data set bai et al 2016 zhang et al 2015 therefore many decomposition ensemble models that combine time series preprocessing and ml techniques have been developed to forecast streamflow zhang et al 2015 wavelet analysis wa adamowski 2008 fourier transform ft yu et al 2018b singular spectrum analysis ssa unnikrishnan and jothiprakash 2018 empirical mode decomposition emd zhu et al 2016 ensemble empirical mode decomposition eemd huang et al 2019 and variational mode decomposition vmd he et al 2019 xie et al 2019 are commonly used time series preprocessing techniques that decompose the original nonstationary signal into several simpler subsignals that can be easily modeled by ml methods zhang et al 2015 used six decomposition ensemble models combined with three different time series preprocessing techniques emd ssa and wa and two modeling methods arma and ann to predict monthly runoff huang et al 2014 applied a modified version of emd to remove the most nonlinear and disorderly noise from the original series and then established an svr based model to predict monthly runoff bai et al 2016 used eemd to extract multiscale features and reconstructed three deep neural networks dnns with a summation strategy to forecast reservoir inflow yu et al 2018b exploited both ft and svr models to extract and learn features and forecast monthly reservoir inflow by summing all of the feature learning results the preprocessing of the original data can improve the prediction performance because of the advantage of feature extraction which removes noise components and detects the hidden structures in raw time series however applying time series preprocessing techniques directly to an entire streamflow series can result in the transmission of some information from the validation period into the training process of data driven models which leads to hindcasting experiments he zhang et al 2015 thus the streamflow prediction results at a specific moment are calculated using some future information that would not be available at that specific moment in a real practical application of streamflow forecasting known as forecasting experiments fe zhang et al 2015 however many previous studies such as those by bai et al 2016 huang et al 2014 kasiviswanathan et al 2016 and meng et al 2019 decomposed entire time series into subsignals first and later divided them into training and testing samples to establish models which is an incorrect or impractical approach because some data from the testing stage i e the future data are used as decomposition parameters for decomposing data from the training stage which will be further explained in section 3 5 but data from the testing stage are not available in real forecasting applications in other words the models trained on training samples cheat to obtain a better forecasting performance with testing samples because these models access some testing information in advance therefore these established models cannot be trusted in this context we would like to mention zhang et al 2015 du et al 2017 tan et al 2018 fang et al 2019 and quilty and adamowski 2018 more explicitly because they noted the incorrect or impractical usage of decomposition and ensemble models zhang et al 2015 compared the monthly streamflow forecasting performance of decomposition ensemble models coupled with wa emd and ssa in hindcasting and forecasting experiments du et al 2017 evaluated the decomposition and ensemble models based on ssa dwt and ann in a forecasting and hindcasting experiment tan et al 2018 used an eemd based ann model to assess the performance of monthly streamflow prediction in hindcasting and forecasting experiments quilty and adamowski 2018 analyzed the effect of using future data in wavelet based decomposition ensemble models fang et al 2019 used an overall decomposition based sampling technique and stepwise decomposition based sampling technique to perform hindcasting and forecasting experiments respectively however the results of these studies revealed that the decomposition ensemble models used in forecasting experiments seriously suffer from the boundary effect for instance in the emd or wa of a time series with finite length as the decomposition process gets closer to the end of the time series domain the decomposition values diverge from the corresponding actual decomposition values obtained from the hindcast experiment in which the actual future values near the end of the time series are observed xiong et al 2014 zhang et al 2015 the boundary effect can degrade the time series modeling quality and overall prediction performance when building a decomposition ensemble model for streamflow forecasting zhang et al 2015 therefore it is a challenge to establish a robust decomposition ensemble model for forecasting experiments zhang et al 2015 suggested that mathematical extension methods can relieve the influence of the boundary effect tan et al 2018 proposed the use of adaptive decomposition ensemble models which are adjusted as long as new streamflow is added to adapt to the boundary effect quilty and adamowski 2018 removed the boundary affected wavelet and scaling coefficients to overcome the influence of the boundary effect however it is very difficult to find a suitable mathematical extension method for different decomposition algorithms and different time series removing the boundary affected coefficients might result in the loss of some valuable information for streamflow forecasting additionally the aforementioned practical decomposition ensemble models build one predictand learning model for each subsignal and the final predictand results are obtained by summing the predictands of subsignals which is quite laborious and may sometimes cause an error accumulation problem therefore we would like to highlight solutions to two potential gaps between the existing practical decomposition ensemble models and practical reliable and efficient decomposition ensemble models i e 1 build a simple decomposition ensemble forecasting scheme to adapt to the boundary effect rather than correct or remove it and 2 build more reliable and efficient decomposition ensemble models that can directly obtain the streamflow predictands furthermore the high irregularity complex nonlinearity and multiscale variability of natural streamflow series are challenging aspects of real forecasting in general 1 shallow learning models cannot sufficiently represent distinct information bai et al 2016 he et al 2019 e g back propagation neural networks bpnns often have slow learning speeds converge to local minima liu et al 2019 and are affected by overfitting huang et al 2014 and svrs are very sensitive to hyperparameter selection he et al 2019 and 2 the commonly used time series preprocessing techniques are often subject to the drawbacks of the decomposition process e g emd is not suitable for practical streamflow forecasting because the decomposition level changes as the decomposition process unfolds fang et al 2019 and emd also has limitations associated with the sensitivity to sampling and noise dragomiretskiy and zosso 2014 eemd is not theoretically well founded liu et al 2018b and the effectiveness of wt strongly relies on the selection of the mother wavelet and decomposition level jiang et al 2019 naik et al 2018 to fill the aforementioned gaps we designed a novel decomposition ensemble forecasting scheme namely single model forecasting sf in which 1 the entire streamflow is first divided into training and validation sets the training set is later decomposed concurrently and the validation set is sequentially appended to the training set and decomposed 2 the training and validation samples are generated by combining the predictors of each subsignal as the final predictors and selecting the original streamflow as the predicted target a technique that was also used by maheswaran and khosa 2013 du et al 2017 and quilty and adamowski 2018 etc for the sf scheme and 3 the validation samples are further divided into development samples for tuning the hyperparameters of lstm models and testing samples for providing high confidence on the tuned lstm models the sf scheme tunes models based on training and development samples therefore it can assess the validation distribution during the training stage to adapt to the boundary effect furthermore to address the aforementioned challenges we introduce a deep learning model that can improve the prediction performance and a time series preprocessing technique that is robust to noise and theoretically well founded to effectively overcome these drawbacks lstm hochreiter and schmidhuber 1997 deep learning models have the ability to tackle this task lstm models have been used successfully in accident diagnosis yang and kim 2018 electricity price prediction ghoddusi et al 2019 peng et al 2018 water table depth forecasting zhang et al 2018 monthly runoff forecasting yuan et al 2018 rainfall runoff modeling kratzert et al 2018 etc recently a theoretically well founded and robust vmd method dragomiretskiy and zosso 2014 was successfully applied to container throughput forecasting niu et al 2018 vibro acoustic feature extraction mohanty et al 2018 and chatter detection in milling processors liu et al 2018a and streamflow forecasting he et al 2019 xie et al 2019 fang et al 2019 as well as in other applications vmd is more sensitive to noise and sampling than the existing decomposition algorithms such as emd and eemd dragomiretskiy and zosso 2014 therefore the sf scheme based on vmd and lstm referred to as sf vmd lstm is proposed to practically forecast daily streamflow 1 3 5 and 7 days ahead in this study in summary the main purpose of this study is to develop a skilled reliable and efficient decomposition ensemble model and identify good model parameter sets for forecasting daily streamflow series with high irregularity complex nonlinearity and multiscale variability without using any information from the future specifically to demonstrate the efficiency and reliability of sf vmd lstm non decomposition based lstm models and decomposition ensemble based lstm models established using two other practical decomposition ensemble forecasting schemes namely multi model ensemble forecasting mef and sf with the most influential subsignals sfmis and two other decomposition algorithms namely discrete wavelet transform dwt which is any wavelet transform or analysis for which the wavelet are discretely sampled and eemd were evaluated for comparative analysis additionally as the hindcast experiment used validation during the decomposition process the subsignals obtained in the hindcast experiment basically represent the true decomposition values therefore three decomposition ensemble hindcasting schemes namely multi model hindcasting meh single model hindcasting sh and single model hindcasting with the most influential subsignals shmis were evaluated as benchmarks to test the adaptability of decomposition ensemble forecasting models to the boundary effect the effectiveness of these models was assessed using the daily streamflow data from yangxian yx station han river china and zhangjiashan zjs station jing river china 2 study area and data two catchments the han river and the jing river illustrated in fig 1 were selected as case studies because the observations of meteorological factors such as rainfall and temperature are incomplete in the above catchments only the historical daily streamflow 1967 01 01 2014 12 31 from yx hydrological station the main upstream control station of the han river total drainage area of 14 484 km2 and zjs hydrological station the downstream control station of the jing river total drainage area of 43 216 km2 were collected to assess the proposed model the locations of the zjs and yx stations are also illustrated in fig 1 the hydrological information data center of the shaanxi hydrographic and water resources survey bureau provided the streamflow records the instantaneous value m3 s observed at 8 a m was selected as the average daily streamflow these records were also investigated by he et al 2019 xie et al 2019 and meng et al 2019 the han river the largest tributary of the yangtze river lies between 30 28 34 5 n and 106 42 114 55 e in the middle of china and has a total drainage area of 1 59 105 km2 yang et al 2017 yu et al 2018a this basin is located in a subtropical monsoon zone that has a humid climate and differentiated seasons yang et al 2017 the annual average precipitation of the han river basin is more than 900 mm yr therefore the water resources in this area are abundant yang et al 2017 the precipitation in the flood season july to september is approximately 75 of the annual total and the runoff displays similar seasonality yang et al 2017 yu et al 2018a the elevation of han river decreases from the western middle and low mountains with high topography to the eastern hilly and plains area with low topography xie et al 2019 forecasting the daily runoff of the han river at yangxian hydrological station can improve assessments of the incoming water from the middle south to north water transfer project in china yang et al 2017 the jing river a secondary tributary of the yellow river lies between 34 46 37 19 n and 106 14 108 42 e and has a total drainage area of 45 421 km2 he et al 2019 the jing river has complicated geomorphology and topography and it decreases in elevation from the loess high gully area in the west to the guanzhong plain in the east meng et al 2019 the climate in the jing river catchment is continental monsoonal and the average precipitation of this area is more than 400 mm yr he et al 2019 this catchment is located in a semiarid and semihumid climate transition zone and the climate and runoff of this catchment exhibit high variability he et al 2019 streamflow forecasting in the jing river basin is essential for the water resource security and regional economy in the context of the belt and road initiative in china meng et al 2019 3 methodology 3 1 mann kendall test for trend and abrupt change detection the mann kendall mk test developed by mann 1945 and kendall 1970 is used to detect the trend or abrupt changes of a streamflow series given the streamflow series x x 1 x 2 x n the null hypothesis h 0 is that x has no monotonic trend the rank series s k evaluating the difference in observations is computed as follows 1 s k i 1 i j k a ij k 2 3 n where 2 a ij s i g n x j x i 1 x i x j 0 x i x j 1 x i x j the mk test assumes x has independent observations which are identically distributed if no trend is present under this assumption the mean and variance of s k are respectively calculated by 3 e s k 0 4 v 0 s k k k 1 2 k 5 18 the test statistic of x is defined as 5 uf k s k 1 v 0 s k s 0 0 s 0 s k 1 v 0 s k s 0 the trend of x is detected by comparing uf n with the standard normal variate at the desired significance level 1 α z 1 α where α 0 α 0 5 is the tolerable probability that h 0 is falsely rejected by the mk test h 0 is rejected if uf n z 1 α otherwise h 0 is accepted under the condition that h 0 is rejected x has a monotonic increase decrease trend over time if uf n is larger smaller than zero to detect the abrupt change points the time series x was arranged in reverse order i e x x n x n 1 x 1 the above calculation process is repeated to get the test statistic sequence ub k k 2 n of x if the lines of uf k k 2 n and ub k k 2 n have intersection points within the significance interval z 1 α z 1 α these intersection points are the abrupt change point 3 2 variational mode decomposition for data decomposition a vmd algorithm developed by dragomiretskiy and zosso 2014 is applied to concurrently extract several band limited intrinsic modes from a sophisticated signal the band limited intrinsic modes of an original time series signal f t namely the intrinsic mode functions imfs are amplitude modulated frequency modulated am fm signals and they can be expressed by the following formula 6 u k t a k t cos ϕ k t where u k t a k t and ϕ k t are the imf instantaneous amplitude and phase respectively the following scheme suggested by dragomiretskiy and zosso 2014 is applied to evaluate the bandwidth of a mode the hilbert transform is first used to compute the related analytic signal of each mode u k t to obtain a one sided frequency spectrum then the frequency spectrum of u k t is shifted to the base band by mixing with an exponent adjusted to reflect the respective evaluated center frequency the h 1 gaussian smoothness of the demodulated signal i e the squared l 2 norm of the gradient is finally used to assess the bandwidth a constrained variational problem expressed by the following formula is used to solve the decomposition problem for a time series f t 7 min u k ω k k t δ t j π t u k t e j ω k t 2 2 s t k u k t f t where u k t u 1 t u 2 t u k t and ω k ω 1 ω 2 ω k are the decomposed modes and the corresponding center frequencies respectively the symbol is the convolutional operator t is time j 2 1 and δ is the dirac distribution a quadratic penalty term i e α and a lagrangian multiplier i e λ are used to obtain eq 7 unconstrained li et al 2019 niu et al 2018 the augmented lagrangian l is defined as follows 8 l u k ω k λ α k t δ t j π t u k t e j ω k t 2 2 f t k u k t 2 2 λ t f t k u k t the alternate direction method of multipliers admm is used to solve eq 8 in the vmd algorithm eq 9 is used to update the mode u k ω in the frequency domain the center frequencies ω k are updated by eq 10 and λ is simultaneously updated by eq 11 in the time domain the mode u k t is the real part of the inverse ft of u k ω expressed by eq 9 9 u k n 1 ω f ω i k u i n 1 ω i k u i n ω λ n ω 2 1 2 α ω ω k 2 10 ω k n 1 0 ω u k n 1 ω 2 d ω 0 u k n 1 ω 2 d ω 11 λ n 1 ω λ n ω τ f ω k u k n 1 ω where n is the iterations τ is the iterative factor indicating the noise tolerance of vmd and u k n 1 ω f ω and λ n ω represent the fts of u k n 1 t f t and λ n t respectively the iterative procedure continues until 12 k u k n 1 u k n 2 2 u k n 2 2 ɛ is reached in eq 12 ɛ is the convergence tolerance in general the decomposition level k the quadratic penalty term α the noise tolerance τ and the convergence tolerance ɛ affect the decomposition performance of vmd it is difficult to determine the appropriate decomposition level in vmd too few imfs can result in poor extraction of the signal components in the original time series whereas too many imfs can be computationally expensive in model training in addition too many imfs can cause duplicate information to be included among signal components niu et al 2018 xu et al 2019 a value of α that is too small leads to a large bandwidth which further leads to redundant information and the inclusion of additional noise in the signal components a value of α that is too large leads to a very small bandwidth which further leads to the loss of some signal information xu et al 2019 as shown in eqs 9 11 and 12 λ hinders the convergence when τ 0 which leads the decomposed signal components to have a large noise level setting τ to 0 can avoid this hindering problem however summing the signal components cannot precisely reconstruct the input signal if τ equals 0 additionally ɛ affects the reconstruction error of the vmd decomposition the implementation process of the vmd model is summarized in algorithm 1 algorithm 1 the vmd process initialize u k 1 ω k 1 λ 1 n 0 repeat n n 1 for k 1 k do update u k ω for all ω 0 using eq 9 update ω k using eq 10 end for update λ n ω for all ω 0 using eq 11 until convergence k u k n 1 u k n 2 2 u k n 2 2 ɛ obtain u k n 1 t by the fast ft of u k n 1 ω 3 3 long short term memory for predictand learning lstm neural networks are a specific type of rnn used for modeling time series designed to overcome the problem caused by gradient vanishing or exploding in the process of training the rnn using back propagation through time bptt y bengio et al 1994 goodfellow et al 2016 hochreiter 1998 an unfolded computational graph as illustrated in fig 2 is used to explain the working principle of the rnn and lstm methods the rnn and lstm output in our case is the predicted daily streamflow y at a particular step and the input is the n lagged daily streamflow x x 1 x n from the particular time step both rnn and lstm methods have chain like structures of repeating modules with self connected hidden units to help them remember previous information the lstm and rnn methods differ in that lstm has more complicated operations inside the repeating modules kratzert et al 2018 zhang et al 2018 which is depicted in fig 2 there is only one internal state h t for a traditional rnn see fig 2 a for each time step from t 1 to t n the following equation is used to compute h t goodfellow et al 2016 kratzert et al 2018 13 h t t a n h w x t u h t 1 b h where w u and b h are the input weight matrix recurrent weights matrix and bias vectors respectively the symbol x t represents the current input vector h t 1 denotes the last hidden cell state and the initial state of h t is h 0 0 lstm in contrast has a memory block including a cell state c t in which information is stored and three gates namely the forget gate f t the input gate i t and the output gate o t that control information propagation within the lstm memory block hochreiter and schmidhuber 1997 kratzert et al 2018 the forget gate which controls the last cell state c t 1 will be forgotten and is first computed by the following equation kratzert et al 2018 14 f t σ w f x t u f h t 1 b f where σ is the logistic sigmoid activation function and w f u f and b f are the input weight matrix recurrent weight matrix and bias vector for the forget gate respectively the initial state of h t for lstm is also h 0 0 then a potential cell state c t is calculated from the current input x t and the last hidden state h t 1 kratzert et al 2018 15 c t t a n h w c x t u c h t 1 b c where w c u c and b c are the input weight matrix recurrent weight matrix and bias vector for the potential cell state respectively the input gate which controls which information stored in the potential cell state is let through to update the current cell state c t is computed by the following equation kratzert et al 2018 16 i t σ w i x t u i h t 1 b i where w i u i and b i are the input weight matrix recurrent weight matrix and bias vector for the input gate respectively in the following step the current cell state is updated using the results of eqs 14 16 kratzert et al 2018 17 c t f t c t 1 i t c t where indicates elementwise multiplication because the logistic sigmoid function can map its input into the range 0 1 the vectors f t and i t both have values in this range therefore the previous information stored in the last cell state is remembered if f t is close to 1 and forgotten if f t is close to 0 the information stored in the current potential cell state is passed through the current cell state if i t is close to 1 and ignored if i t is close 0 moreover the initial cell state of c t is c 0 0 additionally the output gate which controls which information stored in the current cell state flows into the new hidden state is finally computed by the following equation kratzert et al 2018 18 o t σ w o x t u o h t 1 b o where w o u o and b o represent the input weight matrix recurrent weight matrix and bias vector for the output gate respectively similarly o t also has values in the range 0 1 the new hidden state h t can be computed by the following equation 19 h t o t t a n h c t due to the simple linear operation of the cell state at a particular time step it is very easy for information to simply flow along the cell state unchanged therefore the gradient does not vanish or explode when training an lstm with bptt hochreiter and schmidhuber 1997 kratzert et al 2018 the final predicted values can be computed from the output of a traditional dense layer which is connected to the last lstm layer at the last time step kratzert et al 2018 20 y w d h n b d where w d and b d are the hidden to output weight matrix and the bias vector of the dense layer respectively 3 4 decision tree for subsignal importance measurement decision trees are a commonly used attribute importance measurement tool deng et al 2011 decision trees can be explained by a flowchart like structure as diagrammed in fig 3 in which the root node and internal nodes represent the attribute test the branches of each node represent the possible outcomes of the attribute test the leaf nodes represent the class labels or predictand values and the paths from the root node to the leaf nodes represent the classification or regression rules mitchell 1997 given the training dataset x k y k k 1 n where x k x ki i 1 m represents the feature space y k represents the predicted target and m and n are the number of features and samples respectively the decision regression tree divides the feature space into l subspaces or l leaf nodes i e r j j 1 l by performing recursive partitioning the average of the predicted targets belongs to r j i e μ j 1 n j k 1 n j y k x k r j n j is the number of samples reaching node j and is the predictand value the feature importance is computed by the weighted decrease in node impurity the node impurity of the decision regression tree is defined as 21 c j 1 n j k 1 n j y k μ j 2 x k r j then the importance of feature i at node j is defined as 22 ni j w j c j w left c l e f t j w right c r i g h t j where w j n j n is weighted number of samples in node j and l e f t j and r i g h t j are the respective children nodes of node j the feature importance of feature i is defined as 23 fi i j n o d e j s p l i t s o n f e a t u r e i ni j j a l l n o d e s ni j in random forecast rf or gradient boosting regression trees gbrt the final feature importance is its average over all of the trees in this study the decomposed subsignals are the input features and the original streamflow is the output target 3 5 decomposition ensemble models for streamflow forecasting fig 4 shows a diagram of the boundary effect fig 4 a displays an example of using future data as decomposition parameters fig 4 b displays an example of decompositions diverging from the truth if no future data is used as decomposition parameters fig 4 c displays an example of the distribution of sequential decompositions red line i e the validation distribution in the forecasting experiment which is different from the distribution of concurrent decompositions blue line i e the validation distribution in the hindcasting experiment the validation distribution in the hindcasting experiment also represents the training distribution because the decomposition of both the training and validation sets uses future data as decomposition parameters as shown in fig 4 a given observations of a streamflow series the local maxima and minima are used as decomposition parameters to generate the upper and lower envelopes which are further used to extract decompositions however the local maxima and minima of future data are not observed yet therefore the upper and lower envelopes closer to the end of the observations are extrapolated and not precise the boundary effect is that the decompositions closer to the end of observations red line in fig 4 b diverge from actuality blue line in fig 4 b which is obtained by using future data as decomposition parameters therefore concurrently decomposing a streamflow series lead to every observation of this streamflow using its future data until no future data is available in the experiments of streamflow hindcasting the entire streamflow is first decomposed into subsignals and each subsignal is then trained and validated zhang et al 2015 a few decompositions close to the end of the entire streamflow suffer from the boundary effect because the future data of the entire streamflow is not known meanwhile the training decompositions using some information belong to the validation period however the validation information is not available at the present moment for streamflow forecasting therefore streamflow hindcasting is not practical in the experiments of streamflow forecasting to avoid using validation information the entire daily streamflow series is first divided into training and validation sets and the training set is then decomposed into subsignals and applied to train the forecasting models zhang et al 2015 to predict the daily streamflow based on the validation set the daily streamflow in the validation set is sequentially appended to the training set because streamflow is observed daily in many real streamflow forecasting applications and the decomposition process is repeated with the daily streamflow of the next day appended therefore a few decompositions close to the end of training period and the entire decompositions during validation period suffer from the boundary effect which lead to the training and validation decompositions having different distributions as shown in fig 4 c the model trained on the training decompositions hence may not generalize well to the validation decompositions as the streamflow hindcasting models use the validation data e g the local maxima and minima of future data in fig 4 a as decomposition parameters the decompositions in the hindcasting models basically represent the true decomposition values therefore streamflow hindcasting models can still be used as benchmarks for evaluating the adaptability of streamflow forecasting models to the boundary effect in this study we designed a skilled reliable and efficient decomposition ensemble forecasting scheme namely sf in which the validation distribution is assessed during the training stage to adapt to the boundary effect the sf scheme establishes one lstm model using all predictors of the subsignals as the input and the original streamflow as the output this technique was also used by maheswaran and khosa 2013 du et al 2017 and quilty and adamowski 2018 etc therefore the sf scheme can simultaneously simulate the predictor predictand and decomposition streamflow relationships and it is efficient to demonstrate the performance reliability and efficiency of the sf scheme two other decomposition ensemble forecasting schemes namely mef and sfmis were also evaluated the mef scheme establishes one lstm model for each subsignal and the ensemble predictands are obtained by summing the predictands of the subsignals establishing one lstm model for each subsignal in the mef scheme is quite laborious and may cause an error accumulation problem however combining subsignal predictors as the input of lstm in sf can sometimes result in an overfitting problem hence the sfmis scheme establishes one lstm model using predictors of the most influential subsignals as the input and the original streamflow as the output however the sfmis scheme can lead to poor prediction performance due to lost information in this study the sf scheme based on vmd and lstm namely sf vmd lstm is proposed to forecast daily streamflow to demonstrate the reliability and efficiency of sf vmd lstm the mef and sfmis schemes based on vmd and lstm namely mef vmd lstm and sfmis vmd lstm were evaluated to demonstrate that decomposition of streamflow contributes to improvements in forecasting performance the aforementioned decomposition ensemble schemes were compared with non decomposition based monoscale lstm models additionally to test the superiority of vmd in decomposing streamflow the aforementioned decomposition ensemble schemes were also compared with the decomposition ensemble schemes based on eemd dwt and lstm namely mef eemd lstm sf eemd lstm sfmis eemd lstm mef dwt lstm sf dwt lstm and sfmis dwt lstm furthermore to demonstrate the adaptability of decomposition ensemble forecasting models to the boundary effect three decomposition ensemble hindcasting schemes namely meh sh and single model hindcasting with most influential subsignals shmis based on vmd eemd dwt and lstm namely meh vmd lstm sh vmd lstm shmis vmd lstm meh eemd lstm sh eemd lstm shmis eemd lstm meh dwt lstm sh dwt lstm and shmis dwt lstm were evaluated these hindcasting schemes have the same modeling processes as the corresponding forecasting schemes except that the hindcasting schemes decompose the entire streamflow first and later split the decomposed subsignal into training and validation sets in this study we performed 1 3 5 and 7 day streamflow forecasting and hindcasting for all the decomposition ensemble models and monnscale lstm models the decomposition level k of vmd was determined by observing the center frequency aliasing phenomenon based on experimental validation the quadratic penalty term α and the convergence tolerance ɛ of vmd were set to 2000 and 1e 9 respectively to obtain uncorrelated vmd subsignals with a low noise level the noise tolerance τ was set to 0 specifically eemd decomposed the streamflow into several imfs and one residual and the number of ensemble members m and the amplitude of white noise ɛ were the critical parameters influencing the decomposition performance of eemd as suggested by wu and huang 2009 ɛ and m were set to 0 2 and 100 respectively additionally dwt decomposes the streamflow into l detail series and one approximation series given a decomposition level of l the mother wavelet and decomposition level are vital parameters that influence the decomposition performance of dwt as suggested by sachindra et al 2019 the mother wavelets including haar db2 bior 3 3 db5 coif3 db10 db15 db20 db25 db30 db35 db40 and db45 were evaluated to find the optimal mother wavelet and the decomposition level l was set to 1 2 and 3 to find the optimal decomposition level the decomposition ensemble models of streamflow forecasting are summarized as follows and presented in fig 5 step 1 collect the original daily streamflow signal q t t 1 2 n where n is the data length step 2 divide the streamflow signal into a training set q t t t 1 2 n t where n t is the training set length which is 80 of the data length and a validation set q v t t 1 2 n v where n v is the validation set length which is 20 of the data length step 3 decompose the training set q t t into k subsignals imfs s t 1 t s t 2 t s tk t using the decomposition algorithms such as vmd eemd and dwt specifically determining the optimal decomposition level k of vmd by observing the center frequency aliasing phenomenon of the last subsignal was suggested in this study step 4 sequentially append the validation data to the training set to generate new appended signals q tv 1 t q tv 2 t q tv n v t decompose each appended signal q tv j j 1 2 n v into k sets of appended subsignals s t v 1 j t s t v 2 j t s tvk j t using the same decomposition algorithm used in step 3 step 5 plot the partial autocorrelation coefficient figure pacf of each subsignal s tk t k 1 2 k to select the predictors and predicted targets for the lstm model assume that the predicted target for mef scheme is s k t m m is the leading time and the predicted target for sf and sfmis scheme is q t m the lag days of s tk t out of the confidence interval 1 96 n t 1 96 n t are selected as predictors step 6 according to the predictors and predicted target given in step 5 generate training samples of the mef sf or sfmis scheme for training the lstm model using the subsignals of the training set given in step 3 step 7 according to the predictors and predicted target given in step 5 generate samples of mef sf or sfmis using the j th appended subsignals s tvk j t k 1 k and extract the last sample of each set of appended subsignals as a validation sample divide the validation samples into two subsets development samples 50 of the validation samples for determining the optimal model structure and hyperparameters and testing samples 50 of the validation samples for evaluating the model performance step 8 train the lstm model for the mef sf or sfmis schemes using the training samples and monitor the prediction performance of each model based on the development samples to obtain the optimal lstm model input the test sample predictors obtained in step 7 into the optimal model to predict the results for each subsignal step 9 obtain the ensemble prediction results for the mef scheme by summing the predictands of the subsignals or obtain the prediction results of the sf or sfmis schemes by directly outputting the predictands of lstm perform an error analysis on the mef sf or sfmis schemes 3 6 criteria for performance evaluation to assess the prediction ability of the decomposition ensemble models three error analysis criteria were applied these criteria are expressed as shown in table 1 the normalized root mean square error nrmse assesses the model performance in predicting high streamflow values the nash sutcliffe efficiency nse indicates how well observations are replicated by the results of the decomposition ensemble models the peak percentage of threshold statistic lohani et al 2014 denotes the ability to forecast peak flow bai et al 2016 stojković et al 2017 the term ppts γ computes the average absolute relative error of predictands of the top γ streamflow data the lower the ppts the better the capability to forecast peak flow note that the recorded streamflow values are arranged in descending order to compute the ppts and that the threshold level γ denotes the percentage of data selected from the beginning of the arranged data sequence the parameter g is the number of values above the threshold level γ the symbol n is the number of samples x t is the original streamflow x t is the average of the original streamflow and x t is the predicted streamflow 4 case study 4 1 open source software this study relied heavily on open source software pandas mckinney 2010 and numpy stéfan et al 2011 were used to manage and process streamflow data matlab was used to perform the streamflow decomposition tasks and compute the pacf of subsignals the matlab implementations of vmd and eemd were derived from dragomiretskiy and zosso 2014 and wu and huang 2009 respectively the dwt was performed based on the matlab build in toolbox wavelet 1 d in wavelet analyzer the gbrt model in scikit learn pedregosa et al 2011 was used to measure the importance of the decomposed subsignals matplotlib hunter 2007 was used to draw figures and tensorflow abadi et al 2016 was used to train the lstm models these open source software have also partly been used by previous researchers e g kratzert et al 2018 4 2 data analysis and partitioning the mk test hamed 2008 was used to detect possible abrupt shifts and gradual trends in the data from the two hydrological stations the detection results and annual runoff are presented in fig 6 as shown in fig 6 a the fluctuation in annual runoff at yx station is more obvious than that at zjs station illustrating that the runoff at yx station is more nonstationary than the runoff at zjs station the linear trends of the two stations suggest that the average annual runoff at the two stations has been gradually decreasing as shown in fig 6 b and c 1996 was identified as a year when the annual runoff trend at the two stations passed the mk abrupt change test reflecting an abrupt shift because the runoff generation trends before the abrupt shift are inconsistent with those after the abrupt shift the time series after the abrupt shift was selected to evaluate the models the daily streamflow records obtained at the two hydrological stations from 1 1 1997 to 31 12 2014 a total of 6 574 samples were used to develop the present model as described in section 3 5 80 of the streamflow records from 1 1 1997 to 27 05 2011 were selected as the training set and 20 of the streamflow records from 28 05 2011 to 31 12 2014 were selected as the validation set the validation set was further divided into a development set from 28 05 2011 to 14 03 2013 and testing set from 15 03 2011 to 31 12 2014 4 3 data normalization because the range of the raw time series varies widely in some ml models the optimization algorithms applied to obtain the objective functions will not work properly without data normalization in addition data normalization can make the optimization algorithms converge much faster therefore all of the predictors and predicted targets used to model the original streamflow and the subsignals in this study were normalized to the same scale of 1 1 the normalization equation is given as follows 24 x 2 x x min x max x min 1 where x and x are the original and normalized value respectively the symbols x min and x max are the minimum and maximum value of the original series respectively note that the normalized indicators of the training samples i e x max and x min should also be used to normalize both the development and testing samples so that the samples satisfy the same distribution the reason we used eq 24 to normalize the predictors and predicted targets is that the decomposition results of vmd eemd and dwt contained no outliers but rather had negative values and this normalization equation will ensure that all predictors and predicted targets are equally scaled to 1 1 4 4 modeling process according to the forecasting schemes introduced in section 3 5 the decomposition ensemble models and monoscale lstm model were evaluated using the daily streamflow data from zjs station and yx station as described in section 4 2 the modeling process of the decomposition ensemble schemes includes data decomposition the determination of predictors and predicted targets and streamflow forecasting based on experiments we found that the optimal decomposition mode number of vmd can be determined by the aliasing phenomenon of the center frequency of the last imf taking vmd of zjs for example we evaluated k from k 2 to k 12 and the 12th imf of zjs exhibited obvious aliasing phenomena area surrounded by a red rectangular border in fig 7 to satisfy the orthogonality constraint and avoid spurious components as much as possible the k of vmd for zjs was chosen as 11 similarly the k of vmd for yx was chosen as 9 by observing the aliasing phenomena given the parameter set of eemd described in section 3 5 12 subsignals were decomposed from the original streamflow of zjs and yx the subsignal number of dwt depended on the decomposition level two three and four subsignals were decomposed from original streamflow of zjs and yx when the decomposition levels of dwt were set to 1 2 and 3 respectively gbrt was used to measure the importance of the subsignals the subsignals of vmd at zjs station was used as an example for determining the most important influential subsignals of original streamflow from fig 8 as shown in fig 8 the subsignals with importance larger than the median were imf2 imf3 imf4 imf9 and imf5 and the subsignals with importance larger than the mean were imf2 imf3 imf4 imf9 because there is no universally accepted threshold for most importance we chose the subsignals with importance larger than the mean as the most influential subsignals in this manner the most influential subsignals of vmd eemd and dwt at zjs and yx station could be determined the predictors and predicted targets should first be determined to generate training development and testing samples for lstm models based on the input selection methods suggested by wang et al 2009 and he et al 2014 the predictors used by training models to predict the subsignals obtained by vmd eemd and dwt can easily be obtained from the plots of pacfs the first subsignal i e imf1 of vmd for zjs was used as an example to illustrate the determination of predictors from fig 9 the 1 3 5 and 7 day ahead predicted targets for imf1 were s 1 t 1 s 1 t 3 s 1 t 5 and s 1 t 7 then the predictors were s 1 t and s 1 t 1 in other words we used the past two days to predict these targets according to the predictors and predicted targets of the mef sf sfmis meh sh and shmis schemes described in section 3 5 the training and validation samples were first generated using the training and validation decomposition results respectively the validation samples were then divided into two subsets the development samples for the selection of the optimal lstm model with the lowest nrmse and the testing samples for evaluating the performance of the selected lstm model selecting the hyperparameters of lstm models such as the number of hidden layers the number of hidden units the batch size the dropout rate and the learning rate remains a difficult task random search bergstra and bengio 2012 can be used to optimize these hyperparameters however due to computational resource limitations an experimental method that increases or decreases the values of these hyperparameters was used to determine the optimal hyperparameter values for the lstm modeling the learning rate was tuned first and the number of hidden layers the number of hidden units and the dropout rate were later adjusted together the number of batches was set to 512 as used by kratzert et al 2018 the other hyperparameters of the lstm models were set to the default values used by tensorflow the first subsignal of the vmd for zjs was used as an instance of hyperparameter tuning to demonstrate the experimental process in the experimental process of imf1 for zjs an lstm structure of 2 8 1 was initialized 2 input units 8 hidden units and one output unit the dropout rate was initialized as 0 the stopping condition was reached when the lstm model converged we trained 10 models for each lstm with specific hyperparameter values to reduce the impact of the random initialization of weights and the model with the lowest nrmse in the development period was selected as the optimal model in this study we first investigated 10 learning rates see fig 10 for training the lstm models fig 10 shows that learning rates of 0 007 0 01 0 03 and 0 1 rapidly converged however the loss lines of the 0 03 and 0 1 learning rates exhibited large variations in late training epochs and the learning rate of 0 007 displayed worse performance than the learning rate of 0 01 in the late training epochs therefore the learning rate of 0 01 is selected as the optimal learning rate for imf1 at zjs station then the number of hidden units was designed based on 4 levels ranging from 8 to 32 with an interval of 8 because decomposed subsignals are relatively stationary one hidden layer is sufficient for simulating the predictor predictand relationship of streamflow therefore the number of hidden layers was set as 1 to avoid overfitting of lstm the dropout rate was designed based on 10 levels ranging from 0 0 to 0 9 with an interval of 0 1 the imf1 of vmd for zjs was also employed as an example to illustrate tracking the optimal model structure from fig 11 the results indicate that the optimal nrmse values were obtained for the model structure 2 16 1 2 input units 16 hidden units and 1 output unit with a dropout rate of 0 0 therefore the optimal predicted structure of imf1 for zjs was the lstm model with one hidden layer i e 2 16 1 and a dropout rate of 0 0 according to the hyperparameter adjustment procedure used for imf1 of vmd at zjs the hyperparameters including optimal learning rates hidden units and dropout rates and the parameters including weights and biases of the mef sf sfmis meh sh and shmis schemes were optimized with these tuned hyperparameters and parameters the predictands were obtained by lstm models and renormalized into the original data scale using the inverse procedure of eq 24 note that the final predicted streamflow of the mef and meh schemes were obtained by summing the predictands of the subsignals and the predicted streamflow of the sf sfmis sh and shmis schemes exactly matched the predictands of the lstm models 5 results analysis 5 1 comparative analysis of decomposition ensemble forecasting models figs 12 14 show the nse nrmse and ppts 5 for training and development of the mef sf and sfmis based lstm models with different decomposition algorithms at zjs and yx stations in figs 12 14 the horizontal axis illustrates the forecasting scheme station leading time and model stage e g mef zjs l1 t mef zjs l1 d indicates the training development stage of the mef scheme for forecasting streamflow 1 day ahead at zjs station and the vertical axis illustrates the decomposition algorithms in the vertical axis monoscale indicates non decomposition based lstm models and the rest of the decomposition algorithms except for eemd and vmd are dwts e g haar 3 indicates that the wavelet and decomposition level of dwt are haar and 3 respectively the horizontal and vertical axes represent the decomposition ensemble based lstm models e g sf zjs l1 t and vmd represent the sf vmd lstm model for forecasting streamflow 1 day ahead during the training period at zjs station the red squares in figs 12 14 illustrating the optimal decomposition algorithms correspond to forecasting scheme station leading time and model stage as shown in figs 12 14 irrespective of decomposition algorithms stations leading times and model stages the sf based lstm models display relatively high nse low nrmse and ppts 5 compared with the mef based models and much higher nse and lower nrmse and ppts 5 compared with the sfmis based models the results indicate that the sf scheme improves the forecasting performance compared with the mef scheme and the sfmis scheme loses some valuable information which leads to poor forecasting performance irrespective of forecasting schemes stations leading times and model stages the non decomposition based lstm models display obviously low nse and high nrmse and ppts 5 compared with the decomposition based lstm models indicating that decomposition ensemble forecasting models significantly improved the forecasting performance irrespective of forecasting schemes stations leading times and model stages the vmd based lstm models display relatively high nse and low nrmse and ppts 5 compared with the other decomposition based lstm models indicating that vmd based lstm models generally have better streamflow forecasting performance irrespective of decomposition algorithms forecasting schemes stations and model stages the nse nrmse and ppts 5 of the 1 3 5 and 7 day ahead streamflow forecasting gradually decreased increased as the leading times increased indicating that the prediction performance decreases as the leading time increases furthermore irrespective of wavelets forecasting schemes stations leading times and model stages for the dwt based lstm model the decomposition level of 3 displays relatively high nse and low nrmse and ppts 5 compared with the decomposition levels of 1 and 2 indicating that increased decomposition level of dwt improves the streamflow forecasting performance as the best nse of dwt with a decomposition level of 3 is larger than 0 95 for forecasting streamflow 1 day ahead the decomposition level of 3 is sufficient for the dwt based lstm models for daily streamflow forecasting additionally the dwt based lstm models with wavelets including db25 db30 db35 db40 and db45 and a decomposition level of 3 have better forecasting performance compared with the other dwt based lstm models in this study we chose the wavelet and decomposition level of dwt as db45 and 3 respectively to practically predict streamflow 1 3 5 and 7 days ahead in the subsequent experiments in general sf vmd lstm outperforms other lstm models is more efficient than the mef based lstm models and is more reliable than the sfmis based lstm models therefore sf vmd lstm is suggested to forecast daily streamflow in this study fig 15 shows the nse nrmse and ppts 5 of sf vmd lstm sf dwt lstm with wavelet of db45 and decomposition level of 3 sf eemd lstm and monoscale lstm for forecasting streamflow 1 3 5 and 7 days ahead at zjs and yx stations during the testing period as shown in fig 15 sf vmd lstm displays relatively high nse and low nrmse and ppts 5 for both zjs and yx stations compared with sf dwt lstm and much higher nse and lower nrmse and ppts 5 compared with sf eemd lstm and lstm the nse values of sf vmd lstm except for 7 day ahead streamflow forecasting at yx station were larger than 0 8 which is a threshold value for reasonably well performing models used by newman et al 2015 the nse nrmse and ppts 5 values of sf dwt lstm were similar to those of sf vmd lstm for 1 day ahead streamflow forecasting but slight smaller larger than those of sf vmd lstm for forecasting streamflow 3 5 and 7 days ahead the nse nrmse and ppts 5 values of lstm were similar to sf eemd lstm and much smaller larger than those of sf vmd lstm furthermore the nse nrmse and ppts 5 values of lstm were sometimes larger smaller than those of sf eemd lstm in general these results indicate that 1 sf vmd lstm is skilled reliable and efficient for daily streamflow forecasting and has good generalization ability for different catchments 2 both sf vmd lstm and sf dwt lstm play positive roles in improving the streamflow forecasting performance of decomposition ensemble models and 3 eemd based decomposition ensemble models do not always improve streamflow forecasting performance 5 2 performance gap of forecasting and hindcasting schemes figs 16 and 17 present the scatter plots of sf vmd lstm and sh vmd lstm sf dwt lstm db45 3 and sh dwt lstm db45 3 and sf eemd lstm and sh eemd lstm for the 1 3 5 and 7 day ahead streamflow predictands at zjs and yx stations during the testing period as shown in figs 16 and 17 the correlation values of sf vmd lstm and sf dwt lstm are similar to those of sh vmd lstm and sh dwt lstm respectively the results indicate that the prediction performance of sf vmd lstm and sf dwt lstm is close to that of sh vmd lstm and sh dwt lstm respectively the correlation values of sf eemd lstm are mostly scattered above the ideal fit and the correlation values of sh eemd lstm are mostly scattered around the ideal fit indicating that the predictands of sf eemd lstm underestimate the records similar results can be obtained from the mef and meh and the sfmis and shmis schemes results not shown these results indicate that the prediction performance gap between decomposition ensemble forecasting and hindcasting models is very small except for the eemd based models the eemd based hindcasting models outperform the eemd based forecasting models therefore the vmd based and dwt based forecasting models have great adaptability to the boundary effect and eemd based forecasting models have poor adaptability to boundary effect 6 discussion section 5 1 indicates that the forecasting performance of vmd based lstm models is slightly better and much better than that of dwt based and eemd based lstm models respectively the reason that vmd outperforms dwt and eemd can be demonstrated by the pearson correlation coefficients of the subsignals illustrated by fig 18 and the frequency spectrum of the most difficultly predicted subsignals i e the last imf of vmd the first imf of eemd and the first detail subsignal of dwt as illustrated by fig 19 fig 18 shows that the pearson correlation coefficients of the subsignals of vmd and dwt are lower than that of eemd indicating that the subsignals of vmd and dwt are more uncorrelated than those of eemd in other words the subsignals of eemd have duplicate information and hence chaotically represent the period trend and noise as shown in fig 19 the most difficultly predicted subsignal of vmd displays a very low noise level in the low frequency domain the most difficultly predicted subsignal of dwt displays a large noise level in the low frequency domain and the most difficultly predicted subsignal of eemd displays a large noise level along the entire frequency domain this result indicates that the first subsignal of eemd is very difficult to predict therefore eemd leads to poor streamflow forecasting performance although the subsignals of dwt are more uncorrelated than those of vmd they have a larger noise level than those of vmd therefore dwt based lstm models have slightly poor streamflow forecasting performance than vmd but still are powerful decomposition ensemble forecasting models in general vmd is capable of controlling the center frequency aliasing and the noise level therefore it is more feasible for streamflow forecasting section 5 1 also indicates that the sf scheme is more skilled reliable and efficient than mef and sfmis the reason is that the sf scheme simultaneously simulates the predictor predictand and decomposition streamflow relationships additionally the sf scheme can not only avoid the error accumulation problem but also improve the prediction performance of decomposition ensemble models compared with the mef scheme the reconstruction or summation of subsignals sometimes cannot completely reproduce the original streamflow especially for the decomposition of vmd in this study because vmd controls the noise at a low level vmd removes some noise components from the original streamflow therefore the mef scheme might fail to completely predict the original streamflow even though each subsignal is precisely predicted and the sf scheme improves prediction performance compared with the mef scheme because sf simulates a relationship between subsignals and original streamflow instead of using summation of the subsignal predictands to predict original streamflow furthermore the sf scheme builds only one predictand learning model to predict original streamflow which can largely save computation resources and modeling time the sfmis scheme has much poorer prediction performance than mef and sf because sfmis only uses the most influential subsignals to predict streamflow this leads to some valuable information being removed from the decompositions although the sfmis scheme might reduce the risk of overfitting optimal prediction performance should be achieved first section 5 2 indicates that the performance gap between the decomposition ensemble forecasting and hindcasting models is very small except for in the eemd based lstm models this demonstrates that the vmd based dwt based forecasting models have great adaptability while eemd based models have poor adaptability to the boundary effect the reason can be demonstrated by the validation decomposition of eemd for hindcasting and forecasting experiments at zjs station as illustrated by fig 20 a similar result can be obtained at yx station which indicates that the validation decompositions of eemd are seriously affected by the boundary effect especially for imf5 imf11 and the residual r however as shown in figs 21 and 22 the validation decompositions of vmd and dwt at zjs station are slightly affected by the boundary effect a similar result can be obtained at yx station in general there are three ways of handling the boundary effect i e 1 improve the decomposition algorithms e g remove the boundary affected coefficients a method used by quilty and adamowski 2018 2 correct the boundary effect e g the mathematical extension method used by zhang et al 2015 or 3 tolerate the boundary effect rather than eliminate or reduce it e g the adaptive decomposition ensemble model proposed by tan et al 2018 because the boundary effect is very difficult to eliminate in the perspective of signal processing we suggest building an adaptive decomposition ensemble model that can adapt to the boundary effect notably the method used by tan et al 2018 can adapt to the boundary effect however the model must be retrained as long as new runoff information is added and the trained model does not provide users with a high confidence level for the unused data as shown in section 3 5 the main drawback of decomposition ensemble models is that the training and validation samples have different distributions as shown in fig 4 the training samples are smoother than the validation samples so the tuned models cannot generalize well from the training to validation samples therefore decomposition ensemble models trained on training samples and simultaneously accessing the validation distribution can adapt to the boundary effect in this study the decomposition ensemble forecasting models were adapted to the boundary effect using three sets of samples i e training samples for computing parameters development samples for tuning the hyperparameters and selecting the optimal model and testing samples for confirming the high confidence level of the tuned model note that the development and testing samples were from the validation distribution hence the decomposition ensemble models simultaneously trained and developed on training and development samples assessed the validation distribution during the training period and thus adapted to the boundary effect additionally testing samples are completely unused in the trained and developed models therefore they provide users with a high confidence in the prediction performance of the decomposition ensemble models in general adapting to the boundary effect by assessing the validation distribution during the training stage is simple and very easy to implement in general the keys to practically forecasting nonstationary and nonlinear streamflow in catchments with a lack of meteorological observations e g rainfall evaporation and temperature as input predictors are to 1 decompose the nonstationary and nonlinear streamflow into relatively stationary signals and appropriately deal with the noise terms hidden inside the original streamflow 2 avoid using validation information when decomposing the streamflow series and 3 automatically identify how the historical streamflow information affect the future streamflow this study demonstrates the proposed sf vmd lstm model is practical robust and efficient for forecasting highly nonstationary and nonlinear streamflow the reasons for the practicability robustness and efficiency of sf vmd lstm can be summarized as follows 1 only one lstm model is established to forecast the streamflow in the sf scheme and the established lstm model does not have an obvious overfitting problem as shown in section 5 1 therefore sf vmd lstm is efficient 2 vmd decomposes training and validation sets separately to avoid using future information in the sf scheme therefore sf vmd lstm is practical 3 as described in section 3 2 the decomposition level k quadratic penalty term α noise tolerance τ and convergence tolerance ɛ of vmd can be adjusted manually to obtain uncorrelated subsignals with a very low noise level therefore vmd is more controllable and robust to sampling and noise although vmd has a relatively high number of affecting parameters that should be preassigned this study and our experimental experiences indicate that setting the quadratic penalty term α noise tolerance τ and convergence tolerance ɛ to 2000 0 and 1e 9 respectively can obtain a reasonably good streamflow forecasting performance and the decomposition level is the only parameter that should be preassigned 4 as described in section 3 3 the lstm forget gate input gate and output gate can control information propagation within the lstm memory block hence lstm can automatically remember or forget the previous input information to enhance the streamflow forecasting performance which is also demonstrated by zhang et al 2018 and kratzert et al 2018 therefore sf vmd lstm is practical robust and efficient for forecasting nonstationary and nonlinear streamflow 7 conclusions to improve the performance reliability and efficiency of practical decomposition ensemble models an sf scheme in which the validation distribution was assessed during the training period to adapt to the boundary effect was designed the sf scheme based on vmd and lstm referred to as sf vmd lstm was proposed to forecast streamflow 1 3 5 and 7 days ahead to demonstrate the superiority reliability and efficiency of sf vmd lstm non decomposition based lstm models and decomposition ensemble based lstm models established from mef and sfmis schemes and vmd dwt and eemd were evaluated for comparative analysis additionally the meh sh and shmis schemes were used as benchmarks for evaluating the adaptability of decomposition ensemble forecasting models to the boundary effect all comparisons and analyses were based on historical daily river streamflow datasets 01 01 1997 31 12 2014 from yx station han river china and zjs station jing river china the main conclusions can be summarized as follows 1 determining the decomposition level of vmd by observing the center frequency aliasing phenomenon of the last subsignal i e avoiding the center frequency aliasing of the last subsignal makes the subsignals basically satisfy the uncorrelated constraint and avoid spurious components 2 the sf scheme improves the streamflow forecasting performance and efficiency compared with the mef scheme because the original streamflow sometimes cannot be reconstructed by summing the subsignals the sf scheme is more reliable than the sfmis scheme because the sfmis scheme loses some valuable information 3 monoscale lstm models sometimes have larger nse or smaller nrmse and ppts 5 than the eemd based forecasting lstm models indicating that the decomposition ensemble models do not always improve the forecasting performance 4 the subsignals of vmd are more uncorrelated than that of eemd the frequency spectrum of the most difficultly predicted subsignal of vmd has a lower noise level than that of eemd and dwt therefore vmd has better decomposition performance than eemd and dwt 5 the decomposition ensemble forecasting models have similar prediction performance as the decomposition ensemble hindcasting models except for the eemd based models demonstrating the decomposition ensemble forecasting models have great adaptability to the boundary effect except for the eemd based lstm models 6 the proposed sf vmd lstm with nse larger than 0 8 except for 7 day daily streamflow forecasting at yx station outperformed other the sf based lstm models and the mef and sfmis based lstm models therefore it is skilled reliable and efficient for streamflow forecasting in summary the sf vmd lstm model offers the significant advantage of avoiding using future streamflow information that is not available for the current stage additionally the sf vmd lstm model established only one lstm model which uses the predictors of subsignals as the input and the original streamflow as the output to forecast streamflow the sf vmd lstm models assess the validation distribution during the training and development period without using testing information therefore the sf vmd lstm model saves modeling time and computation resources and is more robust and efficient than the decomposition ensemble forecasting models using the summation ensemble strategy e g the models developed by zhang et al 2015 tan et al 2018 fang et al 2019 overall the results demonstrate that the optimal decomposition ensemble model sf vmd lstm is a useful tool for predicting highly nonstationary and nonlinear daily streamflow values in real forecasting applications additionally sf vmd lstm can also be used for forecasting different nonlinear processes such as streamflow rainfall evaporation temperature etc four constraints must be satisfied when using sf vmd lstm i e 1 the subsignals must satisfy the orthogonality constraint and avoid spurious components as much as possible and we suggest determining the decomposition level of vmd by observing the center frequency aliasing of the last subsignal 2 the entire data must be divided into training development and testing sets to adapt to the boundary effect and provide high confidence of the sf vmd lstm model 3 to avoid using future information from development and testing samples only the normalized indicators of training samples should be used to normalize the training development and testing samples and 4 the number of hidden layers the numbers of hidden units and the dropout rate or other regularization parameter must be tuned together and varying the number of hidden units by a factor of 8 or 16 is sufficient for hyperparameter tuning we also suggest using the sf vmd lstm model if the meteorological e g rainfall evaporation temperature or remote sensing data are incomplete and the available historical streamflow data are sufficient to train develop and test the models the entire date length should be at least larger than 500 dealing with the boundary effect using a cross validation strategy trains the forecasting models on the training and validation distribution therefore this should be explored further to improve the prediction performance of decomposition ensemble forecasting models the black box nature of the skilled reliable and efficient decomposition ensemble forecasting models is quite a barrier for applying them in practice therefore the interpretability of decomposition ensemble forecasting models should also be further explored declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china under grant nos 51679186 51679188 51979221 and 51709222 and the research fund of the state key laboratory of eco hydraulics in northwest arid region xi an university of technology under grant no 2019kjcxtd 5 sincere gratitude is extended to the editor and anonymous reviewers for their professional comments and corrections appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124776 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5527,growing concerns about the hydrological impacts of climate variability and climate change suggest an imperativeness to generate plausible climate scenarios suitable for the vulnerability assessment studies a frequency domain nonstationary framework for multi site rainfall generation is proposed for decision centric hydrological impact assessments the framework has three main components 1 a spatiotemporal rainfall field described as spatial modes and their corresponding temporal evolution based on empirical orthogonal function analysis eofa 2 the time series of these spatial modes decomposed into intrinsic mode functions imfs with characteristic frequencies periods using the hilbert huang transform hht and 3 stochastic simulation ss achieved by assigning random phases to the noise imfs in combination with adjustments both to the residual series and to the signal imfs a synthetic test function is first used to illustrate the power of the ehs eofa hht ss rainfall generator to detect and extract signals e g nonstationary oscillation and trend component from noisy data a real application of the ehs model is then presented for the xiang river basin to demonstrate its ability reproducibility and adaptivity the results showed that the ehs rainfall generator has sufficient capacity in reproducing the original spatiotemporal structure such as the spatial correlation and low frequency variability meanwhile the ehs model exhibits advantages in terms of perturbing the distribution characteristics of rainfall and altering their behavior according to the intrinsic spatial patterns these features give the ehs model high feasibility to act as a scenario generator for generating a wide range of possible rainfall scenarios reflecting different aspects of climate variability and climate change and hence bolster the hydrological impact analysis in the climate change context keywords empirical orthogonal function analysis hilbert huang transform stochastic simulation climate change climate variability weather generator nomenclature r original rainfall matrix r simulated rainfall matrix scenarios r rainfall variable z standard gaussian variable z normalized rainfall matrix z simulated normalized rainfall matrix eof eofs empirical orthogonal functions derived from the eofa pc pcs pc t principal component time series derived from the eofa imf imfs imf t intrinsic mode functions derived from the emd a t amplitude time series of imf f t frequency time series of imf r n residue derived from the emd analysis h imf t the hilbert transform of function imf t s t the analytic function of function imf t θ t the instantaneous phase function i i 1 ϕ the random phase α the adjustment factor for the amplitude β the adjustment factor for the residue 1 introduction climate change is predominantly manifested as increased mean temperature intensified extreme precipitation and elevated sea level on regional and global scales this has raised serious concerns about the potential consequences of climate change on hydrological cycles and water resources bates et al 2008 ipcc 2014 for a long time top down approaches have served as the primary strategy to explore the potential impacts of climate change on the hydrological system the gcm based method is currently the predominant top down approach in which the outputs from the gcms are downscaled bárdossy and pegram 2011 fowler et al 2007 langousis and kaleris 2014 wilby et al 2000 as inputs for hydrological models to quantify the hydrological response to climate change dibike and coulibaly 2005 minville et al 2008 wilby et al 1999 however even the most up to date gcms cannot thoroughly describe all the involved climate processes as they are simplified representations of the real complex climate system stephens et al 2010 tan et al 2018 hence the uncertainties accumulated and propagated through the gcm implementation processes could undermine the accuracy of the hydrological impact assessments that are based on such methods clark et al 2016 fowler et al 2007 maraun et al 2010 moreover the gcm based top down method limits the analysis to a set of climate model projections that are unable to exhaustively identify the vulnerability of a system to climate change which is critical to support a robust decision brown et al 2012 stainforth et al 2007 recently applying the bottom up or decision centric approaches to understand the vulnerability of a hydrological system to potential future climate changes has gained much attention e g brown et al 2019 brown and wilby 2012 prudhomme et al 2010 in contrast to the top down approaches that depend on a limited number of climate scenarios a critical step in the bottom up methods involves testing the performance of a system over a range of plausible climate change scenarios to identify the harmful climate states that could cause the hydrological system to fail climate stress test brown et al 2019 despite the growing interest in decision centric approaches a very limited number of tools have been investigated so far a crude and naive tool to conduct the bottom up vulnerability assessment might be the arbitrary change method also called change factor method it simply assumes that certain climatological meteorological variable could be changed by some usually small arbitrary quantity ficklin et al 2009 nemec and schaake 1982 xu 2000 for example one could presume a scenario in which the temperature increases by 2 c or 4 c the precipitation increases or decreases by 10 or 20 or both however such a method limits the exploration of potential risk to the mean climate shifts which might be inappropriate except for a few specific applications a suitable method and or model should provide flexible construction of climatic scenarios and allow changes not only in the mean values but also in the variance the higher order statistics or even the type of the distributions wherever appropriate stochastic weather generators wgs are commonly used to generate time series of climate variables that have statistical properties similar to those of observed data thus wgs can provide long synthetic time series of climate variables of interest representing a broad range of feasible climate scenarios however although very attractive in theory the potential of wgs for driving the bottom up vulnerability assessments has not yet been adequately explored as we recognized a good wg should have two features 1 reproducibility of statistical characteristics of the historical records usually assuming a stationary climate and 2 adaptivity to be used under a nonstationary climate by generating a wide range of plausible meteorological sequences that reasonably embody signals of climate variability and climate change on different time scales to date most efforts have been made to improve the reproducibility of the model performance e g wilks and wilby 1999 and references therein recently the growing awareness of the weakness in stationarity assumption milly et al 2008 has contributed to the interest in developing new wgs to contain the nonstationarity in climate e g agilan and umamahesh 2019 steinschneider and brown 2013 verdin et al 2018 from the perspective of reproducibility the task of wgs is essentially simulating more realizations other than the historical record one realization so that the simulations share the same spatiotemporal features of the historical records both parametric e g richardson type latent gaussian processes and copulas and non parametric weather generators e g k nearest neighbor knn resampling method and schaake shuffle have been proposed to preserve the spatiotemporal features bárdossy and pegram 2009 baxevani and lennartsson 2015 li 2014 papalexiou 2018 rajagopalan et al 1997 richardson 1981 tsoukalas et al 2018 most parametric wgs follow the richardson type wg richardson 1981 that models precipitation occurrence with a markov chain describes precipitation amount by certain given distribution function e g gamma distribution and estimates the associated parameters transition probabilities and distribution parameters for each month or each season in a year over the last several decades wgs of this type have been improved after continuous efforts lennartsson et al 2008 racsko et al 1991 on the other hand the non parametric modeling method e g the knn resampling method was introduced to avoid making specific distributional assumptions taking precipitation as an example the non parametric knn model considers sampling with replacement of the daily rainfall from the historical records to preserve the temporal correlations the resampling is conditioned on the days in the historical record that have similar characteristics as those of the previously simulated days an advantage of the knn model is that the spatial structure of the rainfall field can be inherently maintained by simultaneously resampling at all the stations however both parametric and non parametric wgs have a tendency to underestimate the low frequency variability also termed as the overdispersion phenomenon the missing low frequency variability should be ascribed to the commonly adopted assumption that the climate is stationary or piecewise stationary for example a typical richardson type wg parameterizes itself with the multi year average of each model parameter e g transition probability and distribution parameters and the knn resampling procedure indiscriminately chooses the potential nearest neighbors from a set of multi year windows both disregard the year to year variations or correlations despite the various efforts to address the overdispersion e g harrold et al 2003 katz and zheng 1999 mehrotra and sharma 2007 mehrotra et al 2004 developing wgs capable of representing the nonstationary nature of the climate is deemed the ultimate solution for ensuring the authentic reproducibility of wgs e g steinschneider and brown 2013 verdin et al 2018 adaptivity of a wg goes in parallel with the reproducibility but it may go further in the context of climate change in the context of vulnerability assessment of a hydrological system a good wg is required to reproduce the spatiotemporal features of the historical meteorological field and simultaneously alter the distributional characteristics hypothesized under climate change these alterations are expected to be intrinsically responsible for a suite of extreme events such as increased average or extreme precipitation increasing intensity and decreasing frequency extreme precipitation etc and thereby provide opportunities to test the vulnerability of the hydrological system to unfavorable climate scenarios nonparametric wgs by their own virtue do not allow a long term shift to represent a climate change scenario only a few parametric semi parametric wgs are reported to be able to shift the distribution of weather variables to represent climate changes while simultaneously maintaining the low frequency climate variability and inter and intra variable intersite correlations e g srikanthan and pegram 2009 steinschneider and brown 2013 nonetheless the wgs that can inherently represent the non stationarity of climate are expected to attain better variability as well as reproducibility recently zhou et al 2019 developed a frequency domain framework for multi site stochastic rainfall generator the ehs model based on the empirical orthogonal function analysis eofa and the hilbert huang transform hht and demonstrated its reproducibility to preserve the spatiotemporal statistics in the ehs model the hht is employed to capture the nonstationary oscillation components and a trend component embedded in the climate time series which renders it competent for generating nonstationary climate scenarios hence in this study we report a modified version of the ehs rainfall generator zhou et al 2019 this version has greater flexibility to generate various nonstationary climate scenarios and is therefore suitable for bottom up vulnerability assessments of the hydrological system in this study we refer to the rainfall generator of zhou et al 2019 as the original ehs and that of this research as the modified ehs rainfall generator this modified nonstationary rainfall model addresses all of the challenges mentioned above with three key components 1 a spatiotemporal rainfall field described as spatial modes and their corresponding temporal evolutions based on the empirical orthogonal function analysis eofa 2 the time series of these spatial modes decomposed into intrinsic mode functions imfs with characteristic frequencies periods using the hht and 3 the stochastic simulation ss achieved by assigning random phases to the insignificant imfs noise imfs and specifying adjustment factors to the residuals series and to the amplitudes of the significant imfs signal imfs respectively the former two components appear both in the original ehs model and in the modified version however the last component differentiates the modified ehs from the original where the ss stage includes no more than a random phases assignment consequently the original ehs model has remarkable capacity in terms of reproducibility for climate scenarios generation however its capacity to represent climate change scenarios remains questionable because the residuals and the amplitudes were left untreated to resolve this problem additional adjustments to the residuals and amplitudes are introduced to ensure both reproducibility and adaptivity of the ehs model in the context of hydrological impact analysis we report here the modified ehs method as a bottom up vulnerability assessment tool to achieve two specific objectives 1 to evaluate the power of the proposed modified nonstationary ehs model in detecting and extracting signals e g nonstationary oscillation and trend component from noisy data 2 to demonstrate the reproducibility and adaptivity of this rainfall generator to act as a scenario generator for generating a wide range of possible rainfall sequences the remainder of the paper is organized as follows section 2 outlines the major steps of the modified ehs framework of the multi site stochastic rainfall generator a synthetic data example is used in section 3 to illustrate the capability of the ehs model in extracting and reconstructing the nonstationary signals section 4 addresses the reproducibility and adaptivity by demonstrating the ability of the ehs model to produce various rainfall sequences suitable for the bottom up vulnerability assessments section 5 discusses the strengths and limitations and the ways to use the ehs model finally section 6 concludes the article with a summary of the research and planned future work 2 methodology modified ehs rainfall generator 2 1 modeling procedure an adaptive multi site rainfall generator is designed to accurately reproduce various spatiotemporal characteristics manifested in the historical rainfall field with a capacity to alter many of them in a systematic manner and hence is feasible for a decision centric climate change analysis the diagram of the ehs modeling framework is illustrated in fig 1 and the sub models and algorithms are elaborated below the ehs modeling procedure mainly includes 10 steps 1 denote the observed daily rainfall field in a matrix form as r with the time in rows and the stations in columns 2 use the censored latent gaussian transformation to map between the original rainfall field r and the transformed rainfall field z 3 subject the normalized matrix z to the empirical orthogonal function analysis eofa to produce mutually orthogonal spatial modes namely empirical orthogonal functions eofs and their corresponding principal component time series pcs 4 select first m pcs time series so that certain predefined percentage of total variance could be explained by the m pcs 70 of total variability is a common if subjective cut off point 5 decompose the selected pc time series into a few imfs and a residue r n employing the empirical mode decomposition emd 6 calculate the time varying amplitude a t and frequency f t of each imf using the hilbert spectral analysis hsa 7 test whether an imf is a true signal denoted by signal imf or just a white noise random component denoted by noise imf with statistical significance 8 simulate new pc time series to fulfill the first aim reproducibility new time series of pcs are stochastically simulated by assigning only random phases to the noise imfs to fulfill the second aim adaptivity a new pc is stochastically simulated by assigning adjustment factors to the amplitudes of the signal imfs and another adjustment factor to the residue in addition to randomizing the phases of the noise imfs such simulations are applied to each of the m selected pcs see step 4 9 put the simulated pc s time series and the non simulated ones deemed less significant in the eofa together to create a simulated pcs matrix pc which is subsequently multiplied to the original eofs matrix eof obtained in step 3 to obtain the simulated normalized matrix z 10 finally reverse the transformation in step 2 to turn the simulated normalized matrix z into the final simulated rainfall field r strategically the ehs modeling starts with observation r and ends with a simulated counterpart r the modified ehs model follows the original framework zhou et al 2019 using the eofa to maintain the spatial coherence indicated by the eofs representing the temporal characteristics of the resultant pcs time series with multiple time scale imfs series employing hht however in the modified ehs model the ss stage is upgraded from the original model zhou et al 2019 to simultaneously fulfill our two aims see step 8 and fig 1 in other words we alter the imfs and or residues in addition to randomizing the phases the generated rainfall scenarios are expected to exhibit nuanced differences when representing the climate characteristics and thus could be used to exhaustively explore the potential futures the ehs model coded in matlab language is available upon request 2 2 censored latent gaussian transformation the distribution of daily precipitation data is usually skewed non gaussian and contains a large number of or many zero values zero inflated such practical issues have to be addressed before implementing the ehs method in this study the censored latent gaussian transform method allard and bourotte 2015 baxevani and lennartsson 2015 is used to represent the original rainfall variable r with a latent gaussian variable z whereby z values below certain threshold corresponding to a precipitation observation limit e g 0 1 mm indicate no precipitation an advantage of the present pre processing is that it does not require separate treatments for the rainfall occurrence and the corresponding rainfall amount more details about the implementation of this transformation are given in zhou et al 2019 2 3 empirical orthogonal function analysis eofa the eofa is used in the ehs framework to identify spatial patterns modes eofs of the rainfall field and the associated temporal evolutions pcs in a descending order with respect to the variance explained in practice it is common to decide the number of eofs denoted by m if certain predefined or presumed adequate percentage e g 70 of total variance has been explained jolliffe ian and cadima 2016 the latent gaussian transformed rainfall field matrix z is subjected to the eofa to obtain the eofs and their time series pcs any zij of z relates to the eof and pc by eq 1 1 z ij k 1 m p c ik e o f jk where eofs denote loadings each reflecting the coefficients of correlation between the original variables and a principal component pc represents the scores of an eof with respect to the original variables i refers to the time index e g the time of observation j indexes the station and k indexes the eof 2 4 hilbert huang transform hht huang et al 1998 introduced the hht for analyzing data from nonstationary processes the hht uses the empirical mode decomposition emd method to decompose a sequence into a finite often a small number of time series called intrinsic mode functions imfs and subsequently uses the hilbert spectral analysis hsa to examine the non stationarity of time series data by evaluating both the instantaneous frequency and amplitude 2 4 1 empirical mode decomposition emd emd is technically a sifting process by which a time series is iteratively decomposed into a set of oscillatory components imfs with different frequencies details of the emd method are given in many of the previous literature huang et al 1998 wu and huang 2008 in short the emd method by defining imf as a zero centered oscillatory wave with every extrema necessarily followed by one zero crossing is a new approach to formalize the idea that an apparent time series can be embodied by a finite spectrum of waves with different periods i e imfs additively thus for a given pc series the emd is represented as 2 p c t l 1 n im f l r n where the original pc series is sequentially decomposed into n imfs cl l 1 n and a final residue r n the r n could be a constant or a monotonic function or a function with only one maximum and one minimum from which no more imf can be extracted by generalizing to all the m pcs the emd process can be written as 3 p c k t l 1 n im f k l r k n k 1 m to examine whether an imf obtained by the emd is a true signal or just a white noise component wu and huang 2004 developed a statistical significance test for imfs with the null hypothesis represented by eq 4 4 in e l in t l 0 where el and tl are the energy and the mean period of the l th imf respectively this test compares the averaged period and corresponding energy to determine whether a specific imf contains any statistically significant information if the energy of a test imf with a certain mean period is located above a certain confidence level the corresponding imf is considered statistically significant at that level different from white noise series in this study we use the variant version of the emd namely the ensemble emd eemd the eemd is a noise assisted emd method wu and huang 2008 that defines the true imf as the mean of an ensemble of emd trials each consisting of the signal and an additional white noise of finite amplitude as the added white noise in eemd is easily sifted into the first component imf1 and imf1 generally indicates the random noise the imf1 is excluded from the significance test 2 4 2 hilbert spectral analysis hsa with the emd generated imfs time series the hsa is introduced to express each imf with a time frequency representation as given by eq 5 5 i m f t a t cos θ t re a t e i 2 π f t d t where re denotes the real part of the complex a t is the instantaneous amplitude θ t is the instantaneous phase function and f t is the instantaneous frequency after applying the hsa to all of the imfs pertaining to the pc time series the original pcs as the sum of imfs and residues can be re expressed as 6 p c k t re l 1 n a k l t e i 2 π f k l t d t r k n t k 1 m eq 6 presents the final form of hht highlighting the amplitude and frequency of the emd derived empirical modes as functions of time 2 5 stochastic simulation ss to embody reproducibility and variability after the significance test the imfs are categorized into three types signal imfs noise imfs and residue representing the overall trend and modeled separately according to their characteristics in the original ehs framework zhou et al 2019 we introduced a random element ϕ for noise imfs to represent the underlying random process fig 2 a by assigning a random phase ϕ to every imf tested as noise the time series of simulated pc is obtained as sum of the simulated imfs and the original residue sequence r n 7 p c k t re l 1 n a k l t e i 2 π f k l t d t ϕ k l r k n t k 1 m where ϕ is an independent random phase angle uniformly distributed between π and π as all the amplitude sequences a t instantaneous frequency sequences f t and the trend sequence r n are left intact the original ehs framework put all its focus on preservation of the temporal characteristics of the original rainfall fields nonetheless to explore the plausible variability of rainfall fields part of the temporal characteristics has to be allowed to change to the extent plausible two types of plausible scenarios are considered from the stance of hydrological impact assessment under the climate change context one type scenarios contain a long term trend the other type scenarios bring about more extreme weather events using eq 8 the approach to generate both types of scenarios are readily identified the last residue sequence r n appearing as a trend represents the largest period component of the pc time series the imf intensity indicated by the signal amplitude a t presumably undertakes the severity of certain manifested weather events especially when the imf phases are simultaneously disturbed as eq 8 implies hence the long term shifting mean scenarios can be obtained by adjusting the residue sequence r n and the scenarios that are apt to exhibit more extreme events can be obtained by modifying usually increasing the amplitudes a t s of certain imfs as indicated by eq 8 therefore in the modified ehs framework we adjust the amplitudes of the signal imfs fig 2b and the residues fig 2c to enhance the variability 8 p c k t re l 1 n 1 α k l a k l t e i 2 π f k l t d t ϕ k l r k n t b k 1 m where α is the adjustment factor for the amplitudes of a signal imf and β is the adjustment factor for the residue to validate the proposed stochastic simulation scheme the correspondence between the adjustment factors in the frequency domain and the representative statistics in the time domain are explored using both synthetic and real world data sets 3 synthetic test data example 3 1 synthetic test data and its emd derived components firstly a simple test function is presented to help illustrate the power of hht the key procedure of our proposed ehs model in extracting the nonstationary signals from noisy data as fig 3 shows the test function y t is synthesized as eq 9 which is a combination of three components y 1 t is an oscillation component with an increasing amplitude a t 1 0 0006 t y 2 t is a linear decreasing trend component and y 3 t is a gaussian random component with εt being a gaussian random number this test time series is envisioned to represent a normal hydrometeorological process which is commonly considered to have three parts 1 a deterministic part which is periodic or quasi periodic and results from natural physical periodicities e g y 1 t 2 a deterministic part that is aperiodic to represent a gradual temporal change or trend in the physical processes e g y 2 t 3 a stationary random part e g y 3 t 9 y t y 1 t y 2 t y 3 t t 1 2 1000 y 1 t 1 0 0006 t cos 0 03 t y 2 t 0 3 0 0006 t y 3 t 0 0 ε t using emd the test function y t is decomposed into eight imfs and one residue fig 4 a among them imf5 c5 imf6 c6 and imf7 c7 are considered as signal imfs significantly different from noise at the given level p 0 05 since the energy of these imfs are located above the 95 confidence level fig 4b furthermore fig 5 compares three original components and three reconstructed components of the test function y t based on the hht all three original components y 1 t y 2 t and y3 t are well reconstructed the oscillation component with increasing amplitude y 1 t coincides with the sum of the three signal imfs c5 c6 and c7 particularly the increasing amplitude of y 1 t is detected and extracted using the hht the c5 a t yellow line in fig 5a confirming the competence of that ehs model in reproducing the nonstationary oscillation characteristics through stochastic simulation lee and ouarda 2012 for the linear decreasing trend component y 2 t the residue r n has accurately reproduced it fig 5b as for the gaussian random component y 3 t its amplitude is almost exactly duplicated by the reconstructed noise fig 5c which also guarantees the use of random phase subsection 2 5 to randomly simulate other realizations of the underlying stochastic process 3 2 response of simulated sequences to the adjustment factors employing the ss scheme presented above we generated three simple demonstrative scenarios sce1 sce2 and sec3 to display the deviation from the original y t sequence denoted as sce0 representing noise imfs c1 c2 c3 c4 and c8 by assigning random phases are common among the three simulated scenarios the sce1 and sce2 scenarios involve a reduced trend and inflated amplitude respectively and the sce3 scenario involves both the trend reduction and amplitude increase at the same time the scenario sce1 in which 0 2 is subtracted from the trend residue r n β 0 2 has seen a remarkable left shifting of the simulated y distribution in contrast to that of the original sce0 fig 6 a and b in scenario sce2 the amplitudes of the signal imfs c5 c6 and c7 are increased by 15 α 0 15 and the resultant sce2 shows expanded variance around the invariant mean fig 6a and c suggesting an elevated prevalence of extreme values in scenario sce3 both residue β 0 2 and the signal imfs α 0 15 are altered with the same magnitude as sce1 and sce2 and the overall effect is manifested by a remarkable shift only to the negative side fig 6a and d these three scenarios confirmed the response of the simulated sequences to the two adjustment factors i e α zooms the variability of specific oscillation component and β controls the long term trend the response surface against both α and β are further displayed in fig 7 the change in the mean is mainly affected by β rather than by α fig 7a whereas the change of standard deviation std is highly sensitive to α but insensitive to β fig 7b however both α and β cast influences on the 95th percentile 95p of the simulated sequence fig 7c overall the successful separation and reconstruction of the nonstationary signals embedded in random noises demonstrated with the synthetic test data confirms the effectiveness of hht in dealing with nonstationary datasets consequently the modified ehs method is expected to perform reasonably when applied as a weather generator for hydrological impact assessment 4 application to the real rainfall data 4 1 study area and data to evaluate the performance of the proposed rainfall generator under real circumstances we apply it to daily precipitation data distributed across the xiang river basin in the south central region of china fig 8 for this study a 25 year record from 1961 to 1985 of daily precipitation at 12 stations was obtained from the china meteorological data service center http data cma cn the inter station distances vary from 20 to 270 km table s1 in the supplementary information gives the coordinates of the rainfall stations and the annual precipitation recorded at these stations missing values at some stations totally 0 02 were filled in with the precipitation average of all the neighboring stations 4 2 scenarios customization and model evaluation zhou et al 2019 have already demonstrated the reproducibility of the original ehs framework in preserving multiple characteristics at several different time scales in this study we focus on evaluating the variability of the modified ehs framework to demonstrate how the modified ehs model could be used in a decision centric climate risk assessment the rainfall generator is applied to customize several rainfall scenarios each representing a certain type of climate change for the xiang river basin such scenarios might be useful for exploring changes whose occurrences are deemed plausible including 1 scenarios 1 3 which represent the shifting of precipitation distribution i e increasing or decreasing the mean and extreme precipitation such as the 95p and 2 scenarios 4 6 which represent the perturbing of tail characteristics of the precipitation distribution i e simultaneously increasing the intensity and decreasing the frequency of extreme precipitation for example the first eof eof1 represented a spatially consistent precipitation pattern mainly affected by the large scale weather system details in supplementary information section 3 we use such spatial pattern information to synthesize the spatially differentiated scenarios and define the first scenario sce1 by assigning random phases to the noise imfs of pc1 and adding 0 3 to the residue of pc1 ϕ π π α 0 and β 0 3 in eq 8 the details of the six scenarios are summarized in table 1 for each scenario the model is run 100 separate times each for 25 years the length of the historical record the spatial correlation and low frequency variability are also examined to test the reproducibility of our modified ehs model the following statistics were used to evaluate the generated rainfall scenarios mean and 95th percentile 95p of the 25 year daily precipitation series spatial coherence spatial correlation defined as the linear correlation coefficient of rainfall series between stations temporal coherence low frequency variability defined as the inter annual standard deviations of the monthly precipitation total 4 3 reproducibility and adaptivity of the modified ehs model 4 3 1 increased or decreased mean and 95p precipitation scenarios 1 3 one capability of the shift of precipitation distribution is the basic requirement for weather generator in the context of climate risk analysis in the ehs modeling framework the residue of one pc time series represents a gradual change in the mean state of that evolution hence simply increasing or decreasing the residue is expected to alter the mean precipitation correspondingly scenarios 1 3 in table 1 as fig 9 d shows when adding 0 3 to the residue of pc1 sce1 the mean of the 100 simulations at the 12 stations are all larger than the observation which indicates a unanimous increase in the simulated mean across the stations this is expected as the coefficients of eof1 are all positive and therefore eof1 represents a spatially consistent precipitation pattern fig 9a in addition to the homogeneous shift in space spatial heterogeneous mean shift could also be achieved by adjusting the residue of specific pcs fig 9e and f illustrate this point clearly displaying the spatial pattern of mean shift at different stations fig 9e shows the opposite sign of mean shifts from north to south sce 2 and fig 9f exhibits two distinct transitions of mean shifts in the study basin with positive shifts in northern and southern parts and negative shifts in the middle region sce 3 the increased extreme precipitations observed on the regional and global scales has raised serious concerns about their potential implications on hydrological cycles and water resources bates et al 2008 ipcc 2014 fig 9g i also demonstrate the responses of precipitation extremes using 95p to scenarios 1 3 as stated above when adding 0 3 to the residues of pc1 3 the shifts of the 95p also exhibit the spatial heterogeneities following the corresponding spatial patterns it is vital to preserve the spatial and temporal structure while shifting the distributions fig 10 a c present the scatter plots of pairwise correlation coefficients as modeled in comparison to those observed our proposed model reasonably preserves the spatial correlations of precipitation across all the stations indicated by the correlation coefficients being close to the 1 1 line in the ehs framework the spatial dependence is effectively preserved by keeping the eofs unchanged all the efforts to capture the nonstationary features exclusively involve the pcs both in the original zhou et al 2019 and in the current modified form in addition the modified ehs model also performs satisfactorily in reproducing the year to year variations in the monthly precipitation total fig 10d f as a frequency domain approach the ehs exhibits a salient advantage in considering the oscillations of all frequencies in the original data including the low frequency components overall these results indicate that the modified ehs model could shift the distribution of precipitation effectively and simultaneously maintain the spatial and temporal dependence moreover the ehs framework shows advantages in synthesizing spatially differentiated scenarios following the intrinsic spatial patterns eofs 4 3 2 perturbed tail characteristics of precipitation distribution scenarios 4 6 in this study the model ehs model is designed not only to alter the mean or extreme value e g 95p but also to perturb the tail characteristics of the precipitation distribution as mentioned earlier hydrological systems are often vulnerable not only to changes in the mean climate but also to changes in climate variability therefore the modified ehs model is also designed to generate such scenarios to embody the perturbation of tail characteristics specifically changing the residue component is one approach to generate more extreme precipitations as shown in fig 9 the imf amplitude determines the extent of the fluctuation and hence forms another way to affect the extremes it is thus expected to perturb the tail characteristics of the precipitation distribution when changing the residual and amplitudes of imfs together scenarios 4 6 in table 1 fig 11 a compares the exceedance probability of the precipitation for the 100 simulations blue lines of sce 4 to that of the observation red line at station s3 the observed 95p of station s3 a northern station during 1961 and 1985 is 21 2 mm the simulated 95p has the mean of 19 5 mm ranging from 16 2 mm to 22 0 mm suggesting a less likelihood for heavier precipitation events exceeding 21 2 mm observed 95p to occur however the simulated 99 9th percentile shows an evidently elevated likelihood for precipitation greater than 101 1 mm observed 99 9th percentile hence scenario 4 does create precipitation sequences that involve extreme precipitation events of even higher intensity and lower frequency as expected it should also be noted that the alterations of tail characteristics depend on the loadings of the corresponding eofs for instance station s10 a southern station has a similar loading as station s3 in eof1 see fig 9a therefore the responses of tail characteristics are almost the same fig 11a and d in contrast the loadings of eof2 at station s3 and s10 have opposite signs see fig 9b and the alterations of tail characteristics in scenario 5 are also opposite fig 11b and e in addition since the loading of eof3 at station s3 is nearly zero see fig 9c the variability in the 100 simulations of scenario 6 at station s3 is also trivial fig 11c and f the reproducibility of simulations in preserving the spatial and temporal structure when perturbing the tail characteristics are presented in fig 12 again our proposed model reasonably reproduces the spatial correlations of precipitation across all the stations fig 12a c under scenarios 4 6 also for the low frequency variability the ehs could also reproduce the unbiased inter annual variations in monthly precipitation total fig 12d f overall the proposed rainfall generator has exhibited capability of perturbing the distribution tail characteristics and maintaining the spatial and temporal dependence simultaneously however the resultant changes in tail characteristics will depend on the loadings of the corresponding eofs and on the way of conducting these perturbations combinations of adjustments of α and β 4 4 response of rainfall mean and 95p to the adjustment factors in contrast to the traditional parametric wgs e g richardson type wg whose parameters are relatively easy to see their effect on the simulated rainfalls the modified ehs model involves two adjustment factors α and β operating in the frequency domain which renders the model less straightforward to understand the effect of α and β on the simulated rainfalls we made a spectrum of one factor adjustment simulations and present the responses in the mean and 95p of the simulated daily rainfall accordingly as scatter plots shown in the supplementary information section 5 and fig s4 s15 when only α is increased decreased for the signal imfs of pc1 both the mean fig s4 and the 95p fig s5 of the daily rainfall significantly increased decreased which appears to be a unanimous response across all the stations the effect of α adjustment for pc2 signal imfs is also similar except that the effect becomes weaker or diminished for certain stations s5 s8 in the middle row see fig s6 s7 as for pc3 the adjustment in α did not evoke any response in the rainfall mean and 95p fig s8 s9 recalling the definition of imf zero centered oscillations the backward transformation from imfs back to z eq 8 eq 1 and fig 1 it is readily to understand that scaling the zero centered wave imf amplitude with 1 α will not change the mean of z but will elevate the 95p of z as the synthetic data manifests fig 7 what makes α exhibit a positive effect in rainfall mean roots in the censored latent gaussian transformation the transformation ignores the inflated shrunk negative z s that fall below the threshold corresponding to the precipitation observation limit of 0 1 mm but retains the equally inflated shrunk positive z s hence an elevated reduced mean is observed when a positive negative α is applied however whether this positive effect of α on the mean and 95p of a specific station turns to be significant depends on two factors 1 whether the associated pc has higher energy such as pc1 see fig s3a or lower energy such as pc3 see fig s3c and 2 whether the station has significant loading in the corresponding eof for instance the stations s5 s8 have scores close to zero in eof2 see fig 9b and could hardly exhibit any effect of α in contrast to α the effect of β on the mean and 95p of rainfall is largely following the eof spatial pattern that is if a station has significant positive negative score in a certain eof an increase in β will see an increased decreased precipitation mean and 95p at this station such a spatially heterogeneous effect is best grasped when applied onto imfs of pc2 fig s12 13 and pc3 fig s14 15 as eof2 and eof3 indicate two spatial gradients from north to south fig 9b c nonetheless fig s4 s15 are not expected to guide the readers to make adjustments to generate scenarios for their studies but rather they illustrate how the effects of α and β the former in particular could be manifested or interpreted in short the readers are expected to understand the spatial patterns and the relative energy distributions embedded in their rainfall data before designing their simulation scenarios and the ways to adjust the two factors 5 discussion 5 1 strengths and limitations of the ehs model compared to other rainfall generators adaptivity is the most distinguishable feature of the proposed ehs model specifically the eofa is considered as an adaptive analysis tool as the new variables the spatial patterns eofs are dependent on the dataset rather than the pre defined basis functions for the hht the imf serves as the adaptive basis for expanding the pc time series which is also based on and derived from the data in contrast most traditional spectral analysis methods e g fourier analysis and wavelet analysis use an a priori basis fourier basis and wavelet basis the non linear and nonstationary data from the complex nature could not be well represented by a priori basis assuming linear and stationary using an adaptive basis makes the ehs method highly flexible and physically sound although not proved explicitly here the ehs framework is quite flexible in introducing new weather variables e g temperature solar radiation wind and humidity to account for the inter and intra correlations of meteorological variables a multivariate eofa could be used to decompose the field dataset into multivariate spatial patterns and their temporal evolutions the adaptivity allows the ehs model to work well for the nonstationary process as mentioned in the introduction the simple stationary rainfall models cannot effectively reproduce the variability of a nonstationary climate only by explicitly incorporating the low frequency mechanism into the weather generator could it fully explain the underlying long term climatic variations various efforts have been made to address the overdispersion phenomenon one approach is to allow variations in the model parameters by conditioning on exogenous atmospheric signals e g katz and zheng 1999 mehrotra et al 2004 another approach is to extract low frequency signals from historical records and then use them to modulate the generated series e g harrold et al 2003 mehrotra and sharma 2007 the ehs shares the same idea with the second approach and it can inherently represent the non stationarity of climate because all the nonstationary oscillations embedded in the original data are considered albeit highly flexible the inherent adaptive nature of the ehs model necessarily suggests a limitation the ehs cannot generate series that are longer than the range of the observations this limit is evident as both the eofa step 3 and emd step 5 of the modeling procedure see section 2 1 and fig 1 depend explicitly on the observation time horizon when even longer rainfall series are needed additional modeling on the imfs becomes necessary which will bring about new assumptions and undermine the adaptivity of the model accordingly from the perspective of decision centric use of the model for hydrological impact assessment the ehs model users should be aware of the implicit assumption that an extremely low frequency signal with even longer period than the observation time horizon could not be captured and embodied another limitation of the current ehs model originates from the application of eofa as the first step to separate the rainfall spatial pattern eofs from their associated time series pcs as a traditional method in the field of atmospheric science the eofa presumes that the spatial pattern persists invariably throughout the time horizon such a presumption might not hold true as will be revealed when longer observations become available but it is probably impeccable when ehs is used always with up to date observations 5 2 how to use the ehs model in the bottom up framework in recognition of the risk of missing many unfavorable outcomes of a hydrological system when using only a few climate change scenario projections prudhomme et al 2010 proposed a scenario neutral decision centric scheme for climate change impact studies in their scheme weather generators are employed to produce many weather scenarios which forms the cornerstone for an exhaustive exploration of hydrological response later and recently brown et al 2012 and brown et al 2019 formally proposed the decision scaling ds framework for climate change studies which consists of three steps 1 decision framing 2 climate stress test and 3 estimating climate informed risks in the ds framework the climate stress test or the vulnerability assessment of a hydrological system should better be explored with an easy to use wg for this purpose wgs that generate a number of plausible climate scenarios at a low computational expenses are the most favored the modified ehs model is such a wg in light of its methodological clarity implementing convenience as well as scientific soundness as its applications to the synthetic and the real world data sets have indicated here we take the hydrological impacts as an example to show how the modified ehs model could be used within such a ds framework in the first step we determine the hydrological variables of interest in the second step of climate stress test the climate change scenarios generated by the modified ehs model are fed to a calibrated hydrological model exhibiting a comprehensive and unbiased climate response surface of the hydrological system in the final step of risk characterization the ds process is finalized by using the projection information from a gcm or an ensemble of gcms 6 conclusions given the growing awareness of weakness in the stationarity assumption a frequency domain nonstationary multi site rainfall generator is presented which could be used for hydrological impact assessment studies this ehs rainfall generator applies the eofa and hht in sequence to parse the spatiotemporal rainfall field into spatial patterns eofs and multiple time scale components series imfs the imfs are categorized into three types signal imfs noise imfs and residue overall trend and are modeled separately plausible rainfall scenarios are then stochastically simulated by assigning random phases to the noise imfs and adjustment factors to the residuals and amplitudes of the signal imfs a synthetic test function was presented to help illustrate the power of our model in extracting the nonstationary signals from noisy data the rainfall data of the xiang river basin was then examined as a real case study the results from the synthetic data set and the real world case study indicate that the proposed rainfall generator is designed to work not only for reproducing various characteristics of the historical rainfall fields but also for generating a wide range of possible rainfall field scenarios for vulnerability assessment through this preliminary demonstration the tool has exhibited its efficiency in exploring the hydrological impact under a climate change text such as identification of harmful rainfall scenarios that could cause the system to fail credit authorship contribution statement lingfeng zhou conceptualization methodology writing original draft yaobin meng methodology writing review editing chao lu formal analysis visualization shuiqing yin methodology writing review editing dandan ren formal analysis visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was jointly supported by the national key research and development program of china 2018yfc1801603 the state key laboratory of earth surface processes and resource ecology china and a scholarship from the china scholarship council csc 201806040139 we would like to thank the associate editor and the two anonymous reviewers for their valuable comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124770 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5527,growing concerns about the hydrological impacts of climate variability and climate change suggest an imperativeness to generate plausible climate scenarios suitable for the vulnerability assessment studies a frequency domain nonstationary framework for multi site rainfall generation is proposed for decision centric hydrological impact assessments the framework has three main components 1 a spatiotemporal rainfall field described as spatial modes and their corresponding temporal evolution based on empirical orthogonal function analysis eofa 2 the time series of these spatial modes decomposed into intrinsic mode functions imfs with characteristic frequencies periods using the hilbert huang transform hht and 3 stochastic simulation ss achieved by assigning random phases to the noise imfs in combination with adjustments both to the residual series and to the signal imfs a synthetic test function is first used to illustrate the power of the ehs eofa hht ss rainfall generator to detect and extract signals e g nonstationary oscillation and trend component from noisy data a real application of the ehs model is then presented for the xiang river basin to demonstrate its ability reproducibility and adaptivity the results showed that the ehs rainfall generator has sufficient capacity in reproducing the original spatiotemporal structure such as the spatial correlation and low frequency variability meanwhile the ehs model exhibits advantages in terms of perturbing the distribution characteristics of rainfall and altering their behavior according to the intrinsic spatial patterns these features give the ehs model high feasibility to act as a scenario generator for generating a wide range of possible rainfall scenarios reflecting different aspects of climate variability and climate change and hence bolster the hydrological impact analysis in the climate change context keywords empirical orthogonal function analysis hilbert huang transform stochastic simulation climate change climate variability weather generator nomenclature r original rainfall matrix r simulated rainfall matrix scenarios r rainfall variable z standard gaussian variable z normalized rainfall matrix z simulated normalized rainfall matrix eof eofs empirical orthogonal functions derived from the eofa pc pcs pc t principal component time series derived from the eofa imf imfs imf t intrinsic mode functions derived from the emd a t amplitude time series of imf f t frequency time series of imf r n residue derived from the emd analysis h imf t the hilbert transform of function imf t s t the analytic function of function imf t θ t the instantaneous phase function i i 1 ϕ the random phase α the adjustment factor for the amplitude β the adjustment factor for the residue 1 introduction climate change is predominantly manifested as increased mean temperature intensified extreme precipitation and elevated sea level on regional and global scales this has raised serious concerns about the potential consequences of climate change on hydrological cycles and water resources bates et al 2008 ipcc 2014 for a long time top down approaches have served as the primary strategy to explore the potential impacts of climate change on the hydrological system the gcm based method is currently the predominant top down approach in which the outputs from the gcms are downscaled bárdossy and pegram 2011 fowler et al 2007 langousis and kaleris 2014 wilby et al 2000 as inputs for hydrological models to quantify the hydrological response to climate change dibike and coulibaly 2005 minville et al 2008 wilby et al 1999 however even the most up to date gcms cannot thoroughly describe all the involved climate processes as they are simplified representations of the real complex climate system stephens et al 2010 tan et al 2018 hence the uncertainties accumulated and propagated through the gcm implementation processes could undermine the accuracy of the hydrological impact assessments that are based on such methods clark et al 2016 fowler et al 2007 maraun et al 2010 moreover the gcm based top down method limits the analysis to a set of climate model projections that are unable to exhaustively identify the vulnerability of a system to climate change which is critical to support a robust decision brown et al 2012 stainforth et al 2007 recently applying the bottom up or decision centric approaches to understand the vulnerability of a hydrological system to potential future climate changes has gained much attention e g brown et al 2019 brown and wilby 2012 prudhomme et al 2010 in contrast to the top down approaches that depend on a limited number of climate scenarios a critical step in the bottom up methods involves testing the performance of a system over a range of plausible climate change scenarios to identify the harmful climate states that could cause the hydrological system to fail climate stress test brown et al 2019 despite the growing interest in decision centric approaches a very limited number of tools have been investigated so far a crude and naive tool to conduct the bottom up vulnerability assessment might be the arbitrary change method also called change factor method it simply assumes that certain climatological meteorological variable could be changed by some usually small arbitrary quantity ficklin et al 2009 nemec and schaake 1982 xu 2000 for example one could presume a scenario in which the temperature increases by 2 c or 4 c the precipitation increases or decreases by 10 or 20 or both however such a method limits the exploration of potential risk to the mean climate shifts which might be inappropriate except for a few specific applications a suitable method and or model should provide flexible construction of climatic scenarios and allow changes not only in the mean values but also in the variance the higher order statistics or even the type of the distributions wherever appropriate stochastic weather generators wgs are commonly used to generate time series of climate variables that have statistical properties similar to those of observed data thus wgs can provide long synthetic time series of climate variables of interest representing a broad range of feasible climate scenarios however although very attractive in theory the potential of wgs for driving the bottom up vulnerability assessments has not yet been adequately explored as we recognized a good wg should have two features 1 reproducibility of statistical characteristics of the historical records usually assuming a stationary climate and 2 adaptivity to be used under a nonstationary climate by generating a wide range of plausible meteorological sequences that reasonably embody signals of climate variability and climate change on different time scales to date most efforts have been made to improve the reproducibility of the model performance e g wilks and wilby 1999 and references therein recently the growing awareness of the weakness in stationarity assumption milly et al 2008 has contributed to the interest in developing new wgs to contain the nonstationarity in climate e g agilan and umamahesh 2019 steinschneider and brown 2013 verdin et al 2018 from the perspective of reproducibility the task of wgs is essentially simulating more realizations other than the historical record one realization so that the simulations share the same spatiotemporal features of the historical records both parametric e g richardson type latent gaussian processes and copulas and non parametric weather generators e g k nearest neighbor knn resampling method and schaake shuffle have been proposed to preserve the spatiotemporal features bárdossy and pegram 2009 baxevani and lennartsson 2015 li 2014 papalexiou 2018 rajagopalan et al 1997 richardson 1981 tsoukalas et al 2018 most parametric wgs follow the richardson type wg richardson 1981 that models precipitation occurrence with a markov chain describes precipitation amount by certain given distribution function e g gamma distribution and estimates the associated parameters transition probabilities and distribution parameters for each month or each season in a year over the last several decades wgs of this type have been improved after continuous efforts lennartsson et al 2008 racsko et al 1991 on the other hand the non parametric modeling method e g the knn resampling method was introduced to avoid making specific distributional assumptions taking precipitation as an example the non parametric knn model considers sampling with replacement of the daily rainfall from the historical records to preserve the temporal correlations the resampling is conditioned on the days in the historical record that have similar characteristics as those of the previously simulated days an advantage of the knn model is that the spatial structure of the rainfall field can be inherently maintained by simultaneously resampling at all the stations however both parametric and non parametric wgs have a tendency to underestimate the low frequency variability also termed as the overdispersion phenomenon the missing low frequency variability should be ascribed to the commonly adopted assumption that the climate is stationary or piecewise stationary for example a typical richardson type wg parameterizes itself with the multi year average of each model parameter e g transition probability and distribution parameters and the knn resampling procedure indiscriminately chooses the potential nearest neighbors from a set of multi year windows both disregard the year to year variations or correlations despite the various efforts to address the overdispersion e g harrold et al 2003 katz and zheng 1999 mehrotra and sharma 2007 mehrotra et al 2004 developing wgs capable of representing the nonstationary nature of the climate is deemed the ultimate solution for ensuring the authentic reproducibility of wgs e g steinschneider and brown 2013 verdin et al 2018 adaptivity of a wg goes in parallel with the reproducibility but it may go further in the context of climate change in the context of vulnerability assessment of a hydrological system a good wg is required to reproduce the spatiotemporal features of the historical meteorological field and simultaneously alter the distributional characteristics hypothesized under climate change these alterations are expected to be intrinsically responsible for a suite of extreme events such as increased average or extreme precipitation increasing intensity and decreasing frequency extreme precipitation etc and thereby provide opportunities to test the vulnerability of the hydrological system to unfavorable climate scenarios nonparametric wgs by their own virtue do not allow a long term shift to represent a climate change scenario only a few parametric semi parametric wgs are reported to be able to shift the distribution of weather variables to represent climate changes while simultaneously maintaining the low frequency climate variability and inter and intra variable intersite correlations e g srikanthan and pegram 2009 steinschneider and brown 2013 nonetheless the wgs that can inherently represent the non stationarity of climate are expected to attain better variability as well as reproducibility recently zhou et al 2019 developed a frequency domain framework for multi site stochastic rainfall generator the ehs model based on the empirical orthogonal function analysis eofa and the hilbert huang transform hht and demonstrated its reproducibility to preserve the spatiotemporal statistics in the ehs model the hht is employed to capture the nonstationary oscillation components and a trend component embedded in the climate time series which renders it competent for generating nonstationary climate scenarios hence in this study we report a modified version of the ehs rainfall generator zhou et al 2019 this version has greater flexibility to generate various nonstationary climate scenarios and is therefore suitable for bottom up vulnerability assessments of the hydrological system in this study we refer to the rainfall generator of zhou et al 2019 as the original ehs and that of this research as the modified ehs rainfall generator this modified nonstationary rainfall model addresses all of the challenges mentioned above with three key components 1 a spatiotemporal rainfall field described as spatial modes and their corresponding temporal evolutions based on the empirical orthogonal function analysis eofa 2 the time series of these spatial modes decomposed into intrinsic mode functions imfs with characteristic frequencies periods using the hht and 3 the stochastic simulation ss achieved by assigning random phases to the insignificant imfs noise imfs and specifying adjustment factors to the residuals series and to the amplitudes of the significant imfs signal imfs respectively the former two components appear both in the original ehs model and in the modified version however the last component differentiates the modified ehs from the original where the ss stage includes no more than a random phases assignment consequently the original ehs model has remarkable capacity in terms of reproducibility for climate scenarios generation however its capacity to represent climate change scenarios remains questionable because the residuals and the amplitudes were left untreated to resolve this problem additional adjustments to the residuals and amplitudes are introduced to ensure both reproducibility and adaptivity of the ehs model in the context of hydrological impact analysis we report here the modified ehs method as a bottom up vulnerability assessment tool to achieve two specific objectives 1 to evaluate the power of the proposed modified nonstationary ehs model in detecting and extracting signals e g nonstationary oscillation and trend component from noisy data 2 to demonstrate the reproducibility and adaptivity of this rainfall generator to act as a scenario generator for generating a wide range of possible rainfall sequences the remainder of the paper is organized as follows section 2 outlines the major steps of the modified ehs framework of the multi site stochastic rainfall generator a synthetic data example is used in section 3 to illustrate the capability of the ehs model in extracting and reconstructing the nonstationary signals section 4 addresses the reproducibility and adaptivity by demonstrating the ability of the ehs model to produce various rainfall sequences suitable for the bottom up vulnerability assessments section 5 discusses the strengths and limitations and the ways to use the ehs model finally section 6 concludes the article with a summary of the research and planned future work 2 methodology modified ehs rainfall generator 2 1 modeling procedure an adaptive multi site rainfall generator is designed to accurately reproduce various spatiotemporal characteristics manifested in the historical rainfall field with a capacity to alter many of them in a systematic manner and hence is feasible for a decision centric climate change analysis the diagram of the ehs modeling framework is illustrated in fig 1 and the sub models and algorithms are elaborated below the ehs modeling procedure mainly includes 10 steps 1 denote the observed daily rainfall field in a matrix form as r with the time in rows and the stations in columns 2 use the censored latent gaussian transformation to map between the original rainfall field r and the transformed rainfall field z 3 subject the normalized matrix z to the empirical orthogonal function analysis eofa to produce mutually orthogonal spatial modes namely empirical orthogonal functions eofs and their corresponding principal component time series pcs 4 select first m pcs time series so that certain predefined percentage of total variance could be explained by the m pcs 70 of total variability is a common if subjective cut off point 5 decompose the selected pc time series into a few imfs and a residue r n employing the empirical mode decomposition emd 6 calculate the time varying amplitude a t and frequency f t of each imf using the hilbert spectral analysis hsa 7 test whether an imf is a true signal denoted by signal imf or just a white noise random component denoted by noise imf with statistical significance 8 simulate new pc time series to fulfill the first aim reproducibility new time series of pcs are stochastically simulated by assigning only random phases to the noise imfs to fulfill the second aim adaptivity a new pc is stochastically simulated by assigning adjustment factors to the amplitudes of the signal imfs and another adjustment factor to the residue in addition to randomizing the phases of the noise imfs such simulations are applied to each of the m selected pcs see step 4 9 put the simulated pc s time series and the non simulated ones deemed less significant in the eofa together to create a simulated pcs matrix pc which is subsequently multiplied to the original eofs matrix eof obtained in step 3 to obtain the simulated normalized matrix z 10 finally reverse the transformation in step 2 to turn the simulated normalized matrix z into the final simulated rainfall field r strategically the ehs modeling starts with observation r and ends with a simulated counterpart r the modified ehs model follows the original framework zhou et al 2019 using the eofa to maintain the spatial coherence indicated by the eofs representing the temporal characteristics of the resultant pcs time series with multiple time scale imfs series employing hht however in the modified ehs model the ss stage is upgraded from the original model zhou et al 2019 to simultaneously fulfill our two aims see step 8 and fig 1 in other words we alter the imfs and or residues in addition to randomizing the phases the generated rainfall scenarios are expected to exhibit nuanced differences when representing the climate characteristics and thus could be used to exhaustively explore the potential futures the ehs model coded in matlab language is available upon request 2 2 censored latent gaussian transformation the distribution of daily precipitation data is usually skewed non gaussian and contains a large number of or many zero values zero inflated such practical issues have to be addressed before implementing the ehs method in this study the censored latent gaussian transform method allard and bourotte 2015 baxevani and lennartsson 2015 is used to represent the original rainfall variable r with a latent gaussian variable z whereby z values below certain threshold corresponding to a precipitation observation limit e g 0 1 mm indicate no precipitation an advantage of the present pre processing is that it does not require separate treatments for the rainfall occurrence and the corresponding rainfall amount more details about the implementation of this transformation are given in zhou et al 2019 2 3 empirical orthogonal function analysis eofa the eofa is used in the ehs framework to identify spatial patterns modes eofs of the rainfall field and the associated temporal evolutions pcs in a descending order with respect to the variance explained in practice it is common to decide the number of eofs denoted by m if certain predefined or presumed adequate percentage e g 70 of total variance has been explained jolliffe ian and cadima 2016 the latent gaussian transformed rainfall field matrix z is subjected to the eofa to obtain the eofs and their time series pcs any zij of z relates to the eof and pc by eq 1 1 z ij k 1 m p c ik e o f jk where eofs denote loadings each reflecting the coefficients of correlation between the original variables and a principal component pc represents the scores of an eof with respect to the original variables i refers to the time index e g the time of observation j indexes the station and k indexes the eof 2 4 hilbert huang transform hht huang et al 1998 introduced the hht for analyzing data from nonstationary processes the hht uses the empirical mode decomposition emd method to decompose a sequence into a finite often a small number of time series called intrinsic mode functions imfs and subsequently uses the hilbert spectral analysis hsa to examine the non stationarity of time series data by evaluating both the instantaneous frequency and amplitude 2 4 1 empirical mode decomposition emd emd is technically a sifting process by which a time series is iteratively decomposed into a set of oscillatory components imfs with different frequencies details of the emd method are given in many of the previous literature huang et al 1998 wu and huang 2008 in short the emd method by defining imf as a zero centered oscillatory wave with every extrema necessarily followed by one zero crossing is a new approach to formalize the idea that an apparent time series can be embodied by a finite spectrum of waves with different periods i e imfs additively thus for a given pc series the emd is represented as 2 p c t l 1 n im f l r n where the original pc series is sequentially decomposed into n imfs cl l 1 n and a final residue r n the r n could be a constant or a monotonic function or a function with only one maximum and one minimum from which no more imf can be extracted by generalizing to all the m pcs the emd process can be written as 3 p c k t l 1 n im f k l r k n k 1 m to examine whether an imf obtained by the emd is a true signal or just a white noise component wu and huang 2004 developed a statistical significance test for imfs with the null hypothesis represented by eq 4 4 in e l in t l 0 where el and tl are the energy and the mean period of the l th imf respectively this test compares the averaged period and corresponding energy to determine whether a specific imf contains any statistically significant information if the energy of a test imf with a certain mean period is located above a certain confidence level the corresponding imf is considered statistically significant at that level different from white noise series in this study we use the variant version of the emd namely the ensemble emd eemd the eemd is a noise assisted emd method wu and huang 2008 that defines the true imf as the mean of an ensemble of emd trials each consisting of the signal and an additional white noise of finite amplitude as the added white noise in eemd is easily sifted into the first component imf1 and imf1 generally indicates the random noise the imf1 is excluded from the significance test 2 4 2 hilbert spectral analysis hsa with the emd generated imfs time series the hsa is introduced to express each imf with a time frequency representation as given by eq 5 5 i m f t a t cos θ t re a t e i 2 π f t d t where re denotes the real part of the complex a t is the instantaneous amplitude θ t is the instantaneous phase function and f t is the instantaneous frequency after applying the hsa to all of the imfs pertaining to the pc time series the original pcs as the sum of imfs and residues can be re expressed as 6 p c k t re l 1 n a k l t e i 2 π f k l t d t r k n t k 1 m eq 6 presents the final form of hht highlighting the amplitude and frequency of the emd derived empirical modes as functions of time 2 5 stochastic simulation ss to embody reproducibility and variability after the significance test the imfs are categorized into three types signal imfs noise imfs and residue representing the overall trend and modeled separately according to their characteristics in the original ehs framework zhou et al 2019 we introduced a random element ϕ for noise imfs to represent the underlying random process fig 2 a by assigning a random phase ϕ to every imf tested as noise the time series of simulated pc is obtained as sum of the simulated imfs and the original residue sequence r n 7 p c k t re l 1 n a k l t e i 2 π f k l t d t ϕ k l r k n t k 1 m where ϕ is an independent random phase angle uniformly distributed between π and π as all the amplitude sequences a t instantaneous frequency sequences f t and the trend sequence r n are left intact the original ehs framework put all its focus on preservation of the temporal characteristics of the original rainfall fields nonetheless to explore the plausible variability of rainfall fields part of the temporal characteristics has to be allowed to change to the extent plausible two types of plausible scenarios are considered from the stance of hydrological impact assessment under the climate change context one type scenarios contain a long term trend the other type scenarios bring about more extreme weather events using eq 8 the approach to generate both types of scenarios are readily identified the last residue sequence r n appearing as a trend represents the largest period component of the pc time series the imf intensity indicated by the signal amplitude a t presumably undertakes the severity of certain manifested weather events especially when the imf phases are simultaneously disturbed as eq 8 implies hence the long term shifting mean scenarios can be obtained by adjusting the residue sequence r n and the scenarios that are apt to exhibit more extreme events can be obtained by modifying usually increasing the amplitudes a t s of certain imfs as indicated by eq 8 therefore in the modified ehs framework we adjust the amplitudes of the signal imfs fig 2b and the residues fig 2c to enhance the variability 8 p c k t re l 1 n 1 α k l a k l t e i 2 π f k l t d t ϕ k l r k n t b k 1 m where α is the adjustment factor for the amplitudes of a signal imf and β is the adjustment factor for the residue to validate the proposed stochastic simulation scheme the correspondence between the adjustment factors in the frequency domain and the representative statistics in the time domain are explored using both synthetic and real world data sets 3 synthetic test data example 3 1 synthetic test data and its emd derived components firstly a simple test function is presented to help illustrate the power of hht the key procedure of our proposed ehs model in extracting the nonstationary signals from noisy data as fig 3 shows the test function y t is synthesized as eq 9 which is a combination of three components y 1 t is an oscillation component with an increasing amplitude a t 1 0 0006 t y 2 t is a linear decreasing trend component and y 3 t is a gaussian random component with εt being a gaussian random number this test time series is envisioned to represent a normal hydrometeorological process which is commonly considered to have three parts 1 a deterministic part which is periodic or quasi periodic and results from natural physical periodicities e g y 1 t 2 a deterministic part that is aperiodic to represent a gradual temporal change or trend in the physical processes e g y 2 t 3 a stationary random part e g y 3 t 9 y t y 1 t y 2 t y 3 t t 1 2 1000 y 1 t 1 0 0006 t cos 0 03 t y 2 t 0 3 0 0006 t y 3 t 0 0 ε t using emd the test function y t is decomposed into eight imfs and one residue fig 4 a among them imf5 c5 imf6 c6 and imf7 c7 are considered as signal imfs significantly different from noise at the given level p 0 05 since the energy of these imfs are located above the 95 confidence level fig 4b furthermore fig 5 compares three original components and three reconstructed components of the test function y t based on the hht all three original components y 1 t y 2 t and y3 t are well reconstructed the oscillation component with increasing amplitude y 1 t coincides with the sum of the three signal imfs c5 c6 and c7 particularly the increasing amplitude of y 1 t is detected and extracted using the hht the c5 a t yellow line in fig 5a confirming the competence of that ehs model in reproducing the nonstationary oscillation characteristics through stochastic simulation lee and ouarda 2012 for the linear decreasing trend component y 2 t the residue r n has accurately reproduced it fig 5b as for the gaussian random component y 3 t its amplitude is almost exactly duplicated by the reconstructed noise fig 5c which also guarantees the use of random phase subsection 2 5 to randomly simulate other realizations of the underlying stochastic process 3 2 response of simulated sequences to the adjustment factors employing the ss scheme presented above we generated three simple demonstrative scenarios sce1 sce2 and sec3 to display the deviation from the original y t sequence denoted as sce0 representing noise imfs c1 c2 c3 c4 and c8 by assigning random phases are common among the three simulated scenarios the sce1 and sce2 scenarios involve a reduced trend and inflated amplitude respectively and the sce3 scenario involves both the trend reduction and amplitude increase at the same time the scenario sce1 in which 0 2 is subtracted from the trend residue r n β 0 2 has seen a remarkable left shifting of the simulated y distribution in contrast to that of the original sce0 fig 6 a and b in scenario sce2 the amplitudes of the signal imfs c5 c6 and c7 are increased by 15 α 0 15 and the resultant sce2 shows expanded variance around the invariant mean fig 6a and c suggesting an elevated prevalence of extreme values in scenario sce3 both residue β 0 2 and the signal imfs α 0 15 are altered with the same magnitude as sce1 and sce2 and the overall effect is manifested by a remarkable shift only to the negative side fig 6a and d these three scenarios confirmed the response of the simulated sequences to the two adjustment factors i e α zooms the variability of specific oscillation component and β controls the long term trend the response surface against both α and β are further displayed in fig 7 the change in the mean is mainly affected by β rather than by α fig 7a whereas the change of standard deviation std is highly sensitive to α but insensitive to β fig 7b however both α and β cast influences on the 95th percentile 95p of the simulated sequence fig 7c overall the successful separation and reconstruction of the nonstationary signals embedded in random noises demonstrated with the synthetic test data confirms the effectiveness of hht in dealing with nonstationary datasets consequently the modified ehs method is expected to perform reasonably when applied as a weather generator for hydrological impact assessment 4 application to the real rainfall data 4 1 study area and data to evaluate the performance of the proposed rainfall generator under real circumstances we apply it to daily precipitation data distributed across the xiang river basin in the south central region of china fig 8 for this study a 25 year record from 1961 to 1985 of daily precipitation at 12 stations was obtained from the china meteorological data service center http data cma cn the inter station distances vary from 20 to 270 km table s1 in the supplementary information gives the coordinates of the rainfall stations and the annual precipitation recorded at these stations missing values at some stations totally 0 02 were filled in with the precipitation average of all the neighboring stations 4 2 scenarios customization and model evaluation zhou et al 2019 have already demonstrated the reproducibility of the original ehs framework in preserving multiple characteristics at several different time scales in this study we focus on evaluating the variability of the modified ehs framework to demonstrate how the modified ehs model could be used in a decision centric climate risk assessment the rainfall generator is applied to customize several rainfall scenarios each representing a certain type of climate change for the xiang river basin such scenarios might be useful for exploring changes whose occurrences are deemed plausible including 1 scenarios 1 3 which represent the shifting of precipitation distribution i e increasing or decreasing the mean and extreme precipitation such as the 95p and 2 scenarios 4 6 which represent the perturbing of tail characteristics of the precipitation distribution i e simultaneously increasing the intensity and decreasing the frequency of extreme precipitation for example the first eof eof1 represented a spatially consistent precipitation pattern mainly affected by the large scale weather system details in supplementary information section 3 we use such spatial pattern information to synthesize the spatially differentiated scenarios and define the first scenario sce1 by assigning random phases to the noise imfs of pc1 and adding 0 3 to the residue of pc1 ϕ π π α 0 and β 0 3 in eq 8 the details of the six scenarios are summarized in table 1 for each scenario the model is run 100 separate times each for 25 years the length of the historical record the spatial correlation and low frequency variability are also examined to test the reproducibility of our modified ehs model the following statistics were used to evaluate the generated rainfall scenarios mean and 95th percentile 95p of the 25 year daily precipitation series spatial coherence spatial correlation defined as the linear correlation coefficient of rainfall series between stations temporal coherence low frequency variability defined as the inter annual standard deviations of the monthly precipitation total 4 3 reproducibility and adaptivity of the modified ehs model 4 3 1 increased or decreased mean and 95p precipitation scenarios 1 3 one capability of the shift of precipitation distribution is the basic requirement for weather generator in the context of climate risk analysis in the ehs modeling framework the residue of one pc time series represents a gradual change in the mean state of that evolution hence simply increasing or decreasing the residue is expected to alter the mean precipitation correspondingly scenarios 1 3 in table 1 as fig 9 d shows when adding 0 3 to the residue of pc1 sce1 the mean of the 100 simulations at the 12 stations are all larger than the observation which indicates a unanimous increase in the simulated mean across the stations this is expected as the coefficients of eof1 are all positive and therefore eof1 represents a spatially consistent precipitation pattern fig 9a in addition to the homogeneous shift in space spatial heterogeneous mean shift could also be achieved by adjusting the residue of specific pcs fig 9e and f illustrate this point clearly displaying the spatial pattern of mean shift at different stations fig 9e shows the opposite sign of mean shifts from north to south sce 2 and fig 9f exhibits two distinct transitions of mean shifts in the study basin with positive shifts in northern and southern parts and negative shifts in the middle region sce 3 the increased extreme precipitations observed on the regional and global scales has raised serious concerns about their potential implications on hydrological cycles and water resources bates et al 2008 ipcc 2014 fig 9g i also demonstrate the responses of precipitation extremes using 95p to scenarios 1 3 as stated above when adding 0 3 to the residues of pc1 3 the shifts of the 95p also exhibit the spatial heterogeneities following the corresponding spatial patterns it is vital to preserve the spatial and temporal structure while shifting the distributions fig 10 a c present the scatter plots of pairwise correlation coefficients as modeled in comparison to those observed our proposed model reasonably preserves the spatial correlations of precipitation across all the stations indicated by the correlation coefficients being close to the 1 1 line in the ehs framework the spatial dependence is effectively preserved by keeping the eofs unchanged all the efforts to capture the nonstationary features exclusively involve the pcs both in the original zhou et al 2019 and in the current modified form in addition the modified ehs model also performs satisfactorily in reproducing the year to year variations in the monthly precipitation total fig 10d f as a frequency domain approach the ehs exhibits a salient advantage in considering the oscillations of all frequencies in the original data including the low frequency components overall these results indicate that the modified ehs model could shift the distribution of precipitation effectively and simultaneously maintain the spatial and temporal dependence moreover the ehs framework shows advantages in synthesizing spatially differentiated scenarios following the intrinsic spatial patterns eofs 4 3 2 perturbed tail characteristics of precipitation distribution scenarios 4 6 in this study the model ehs model is designed not only to alter the mean or extreme value e g 95p but also to perturb the tail characteristics of the precipitation distribution as mentioned earlier hydrological systems are often vulnerable not only to changes in the mean climate but also to changes in climate variability therefore the modified ehs model is also designed to generate such scenarios to embody the perturbation of tail characteristics specifically changing the residue component is one approach to generate more extreme precipitations as shown in fig 9 the imf amplitude determines the extent of the fluctuation and hence forms another way to affect the extremes it is thus expected to perturb the tail characteristics of the precipitation distribution when changing the residual and amplitudes of imfs together scenarios 4 6 in table 1 fig 11 a compares the exceedance probability of the precipitation for the 100 simulations blue lines of sce 4 to that of the observation red line at station s3 the observed 95p of station s3 a northern station during 1961 and 1985 is 21 2 mm the simulated 95p has the mean of 19 5 mm ranging from 16 2 mm to 22 0 mm suggesting a less likelihood for heavier precipitation events exceeding 21 2 mm observed 95p to occur however the simulated 99 9th percentile shows an evidently elevated likelihood for precipitation greater than 101 1 mm observed 99 9th percentile hence scenario 4 does create precipitation sequences that involve extreme precipitation events of even higher intensity and lower frequency as expected it should also be noted that the alterations of tail characteristics depend on the loadings of the corresponding eofs for instance station s10 a southern station has a similar loading as station s3 in eof1 see fig 9a therefore the responses of tail characteristics are almost the same fig 11a and d in contrast the loadings of eof2 at station s3 and s10 have opposite signs see fig 9b and the alterations of tail characteristics in scenario 5 are also opposite fig 11b and e in addition since the loading of eof3 at station s3 is nearly zero see fig 9c the variability in the 100 simulations of scenario 6 at station s3 is also trivial fig 11c and f the reproducibility of simulations in preserving the spatial and temporal structure when perturbing the tail characteristics are presented in fig 12 again our proposed model reasonably reproduces the spatial correlations of precipitation across all the stations fig 12a c under scenarios 4 6 also for the low frequency variability the ehs could also reproduce the unbiased inter annual variations in monthly precipitation total fig 12d f overall the proposed rainfall generator has exhibited capability of perturbing the distribution tail characteristics and maintaining the spatial and temporal dependence simultaneously however the resultant changes in tail characteristics will depend on the loadings of the corresponding eofs and on the way of conducting these perturbations combinations of adjustments of α and β 4 4 response of rainfall mean and 95p to the adjustment factors in contrast to the traditional parametric wgs e g richardson type wg whose parameters are relatively easy to see their effect on the simulated rainfalls the modified ehs model involves two adjustment factors α and β operating in the frequency domain which renders the model less straightforward to understand the effect of α and β on the simulated rainfalls we made a spectrum of one factor adjustment simulations and present the responses in the mean and 95p of the simulated daily rainfall accordingly as scatter plots shown in the supplementary information section 5 and fig s4 s15 when only α is increased decreased for the signal imfs of pc1 both the mean fig s4 and the 95p fig s5 of the daily rainfall significantly increased decreased which appears to be a unanimous response across all the stations the effect of α adjustment for pc2 signal imfs is also similar except that the effect becomes weaker or diminished for certain stations s5 s8 in the middle row see fig s6 s7 as for pc3 the adjustment in α did not evoke any response in the rainfall mean and 95p fig s8 s9 recalling the definition of imf zero centered oscillations the backward transformation from imfs back to z eq 8 eq 1 and fig 1 it is readily to understand that scaling the zero centered wave imf amplitude with 1 α will not change the mean of z but will elevate the 95p of z as the synthetic data manifests fig 7 what makes α exhibit a positive effect in rainfall mean roots in the censored latent gaussian transformation the transformation ignores the inflated shrunk negative z s that fall below the threshold corresponding to the precipitation observation limit of 0 1 mm but retains the equally inflated shrunk positive z s hence an elevated reduced mean is observed when a positive negative α is applied however whether this positive effect of α on the mean and 95p of a specific station turns to be significant depends on two factors 1 whether the associated pc has higher energy such as pc1 see fig s3a or lower energy such as pc3 see fig s3c and 2 whether the station has significant loading in the corresponding eof for instance the stations s5 s8 have scores close to zero in eof2 see fig 9b and could hardly exhibit any effect of α in contrast to α the effect of β on the mean and 95p of rainfall is largely following the eof spatial pattern that is if a station has significant positive negative score in a certain eof an increase in β will see an increased decreased precipitation mean and 95p at this station such a spatially heterogeneous effect is best grasped when applied onto imfs of pc2 fig s12 13 and pc3 fig s14 15 as eof2 and eof3 indicate two spatial gradients from north to south fig 9b c nonetheless fig s4 s15 are not expected to guide the readers to make adjustments to generate scenarios for their studies but rather they illustrate how the effects of α and β the former in particular could be manifested or interpreted in short the readers are expected to understand the spatial patterns and the relative energy distributions embedded in their rainfall data before designing their simulation scenarios and the ways to adjust the two factors 5 discussion 5 1 strengths and limitations of the ehs model compared to other rainfall generators adaptivity is the most distinguishable feature of the proposed ehs model specifically the eofa is considered as an adaptive analysis tool as the new variables the spatial patterns eofs are dependent on the dataset rather than the pre defined basis functions for the hht the imf serves as the adaptive basis for expanding the pc time series which is also based on and derived from the data in contrast most traditional spectral analysis methods e g fourier analysis and wavelet analysis use an a priori basis fourier basis and wavelet basis the non linear and nonstationary data from the complex nature could not be well represented by a priori basis assuming linear and stationary using an adaptive basis makes the ehs method highly flexible and physically sound although not proved explicitly here the ehs framework is quite flexible in introducing new weather variables e g temperature solar radiation wind and humidity to account for the inter and intra correlations of meteorological variables a multivariate eofa could be used to decompose the field dataset into multivariate spatial patterns and their temporal evolutions the adaptivity allows the ehs model to work well for the nonstationary process as mentioned in the introduction the simple stationary rainfall models cannot effectively reproduce the variability of a nonstationary climate only by explicitly incorporating the low frequency mechanism into the weather generator could it fully explain the underlying long term climatic variations various efforts have been made to address the overdispersion phenomenon one approach is to allow variations in the model parameters by conditioning on exogenous atmospheric signals e g katz and zheng 1999 mehrotra et al 2004 another approach is to extract low frequency signals from historical records and then use them to modulate the generated series e g harrold et al 2003 mehrotra and sharma 2007 the ehs shares the same idea with the second approach and it can inherently represent the non stationarity of climate because all the nonstationary oscillations embedded in the original data are considered albeit highly flexible the inherent adaptive nature of the ehs model necessarily suggests a limitation the ehs cannot generate series that are longer than the range of the observations this limit is evident as both the eofa step 3 and emd step 5 of the modeling procedure see section 2 1 and fig 1 depend explicitly on the observation time horizon when even longer rainfall series are needed additional modeling on the imfs becomes necessary which will bring about new assumptions and undermine the adaptivity of the model accordingly from the perspective of decision centric use of the model for hydrological impact assessment the ehs model users should be aware of the implicit assumption that an extremely low frequency signal with even longer period than the observation time horizon could not be captured and embodied another limitation of the current ehs model originates from the application of eofa as the first step to separate the rainfall spatial pattern eofs from their associated time series pcs as a traditional method in the field of atmospheric science the eofa presumes that the spatial pattern persists invariably throughout the time horizon such a presumption might not hold true as will be revealed when longer observations become available but it is probably impeccable when ehs is used always with up to date observations 5 2 how to use the ehs model in the bottom up framework in recognition of the risk of missing many unfavorable outcomes of a hydrological system when using only a few climate change scenario projections prudhomme et al 2010 proposed a scenario neutral decision centric scheme for climate change impact studies in their scheme weather generators are employed to produce many weather scenarios which forms the cornerstone for an exhaustive exploration of hydrological response later and recently brown et al 2012 and brown et al 2019 formally proposed the decision scaling ds framework for climate change studies which consists of three steps 1 decision framing 2 climate stress test and 3 estimating climate informed risks in the ds framework the climate stress test or the vulnerability assessment of a hydrological system should better be explored with an easy to use wg for this purpose wgs that generate a number of plausible climate scenarios at a low computational expenses are the most favored the modified ehs model is such a wg in light of its methodological clarity implementing convenience as well as scientific soundness as its applications to the synthetic and the real world data sets have indicated here we take the hydrological impacts as an example to show how the modified ehs model could be used within such a ds framework in the first step we determine the hydrological variables of interest in the second step of climate stress test the climate change scenarios generated by the modified ehs model are fed to a calibrated hydrological model exhibiting a comprehensive and unbiased climate response surface of the hydrological system in the final step of risk characterization the ds process is finalized by using the projection information from a gcm or an ensemble of gcms 6 conclusions given the growing awareness of weakness in the stationarity assumption a frequency domain nonstationary multi site rainfall generator is presented which could be used for hydrological impact assessment studies this ehs rainfall generator applies the eofa and hht in sequence to parse the spatiotemporal rainfall field into spatial patterns eofs and multiple time scale components series imfs the imfs are categorized into three types signal imfs noise imfs and residue overall trend and are modeled separately plausible rainfall scenarios are then stochastically simulated by assigning random phases to the noise imfs and adjustment factors to the residuals and amplitudes of the signal imfs a synthetic test function was presented to help illustrate the power of our model in extracting the nonstationary signals from noisy data the rainfall data of the xiang river basin was then examined as a real case study the results from the synthetic data set and the real world case study indicate that the proposed rainfall generator is designed to work not only for reproducing various characteristics of the historical rainfall fields but also for generating a wide range of possible rainfall field scenarios for vulnerability assessment through this preliminary demonstration the tool has exhibited its efficiency in exploring the hydrological impact under a climate change text such as identification of harmful rainfall scenarios that could cause the system to fail credit authorship contribution statement lingfeng zhou conceptualization methodology writing original draft yaobin meng methodology writing review editing chao lu formal analysis visualization shuiqing yin methodology writing review editing dandan ren formal analysis visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was jointly supported by the national key research and development program of china 2018yfc1801603 the state key laboratory of earth surface processes and resource ecology china and a scholarship from the china scholarship council csc 201806040139 we would like to thank the associate editor and the two anonymous reviewers for their valuable comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124770 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5528,while advection dispersion and their hydraulic background are broadly explored in water quality modelling conversion processes reaction and decay rates usually lack of attention they represent physical chemical and biological processes that reactive substances go through in contact with water typically assumed as calibration parameters this study introduces a method for model calibration considering a temporal aspect of transformation coefficients trats transformation rates time series daily decay rates are generated as result of i attributes of the river station ii reference values from literature and iii random variability these transformation parameters are then applied in water quality simulations under unsteady state in a river for biochemical oxygen demand bod organic nitrogen n org and dissolved oxygen do the proposed method for integrated modelling suggests that temporal variation of kinetic processes may play an import role in transport and fate of pollutants in terms of overall variability and attenuation over time and space and this aspect can be included in the calibration phase keywords water quality modelling calibration under unsteady state transformation processes 1 introduction once a pollutant reaches a watercourse its transport along space and time is the result of several processes besides advection diffusion and dispersion non conservative substances are affected by activities that result in substance transformations affecting transfers of matter through the system these mechanisms represent losses or gains of mass due to chemical chemical reactions in general physical particulate settling for example or biological processes such as growth and death of algae within the water in general such processes are described by first order kinetic reactions and reference values these rates can be obtained through analytical tests statistical conceptual and empirical analysis usual equations relate them to channel hydraulics such as velocity water level slope or discharges brown and barnwell 1987 these latter differ mainly by the suggested application range and results can be significantly divergent nuruzzaman et al 2018 fluctuation of these coefficients reported in the literature can also be wide a common example is the reaeration rate which oscillates from 0 to 100 d 1 according to brown and barnwell 1987 nonetheless these parameter s values are often unknown because of spatial and temporal variability measurement challenges simplification in model descriptions or commonly lack of data therefore they become calibration coefficients in most studies of water quality modelling e g tang et al 2016 chaudhary et al 2018 despite the efforts to develop analytical experiments to estimate them in laboratory conditions e g pittroff et al 2017 nuruzzaman et al 2018 the authors alam and dutta 2012 call attention to the significant number of studies highlighting the need for better understanding of in stream processes in advanced modelling this study introduces a new perspective to calibrate water quality models under unsteady state as an alternative to the traditional techniques based on trial and error and optimization procedures the first one is usually subjective and time consuming while the second often assumes criteria that solve the problem through a mathematical perspective therefore it may generate illogical values kumarasamy and belmont 2018 furthermore often a technique identifies multiple datasets that satisfy the optimization problem however depending on a given combination these parameters may superimpose transport effects and originate misleading interpretations dortch and johnson 1992 computation time also is increased due to the multiple operations required to reach a solution semiromi et al 2018 automatic routines are commonly applied in hydrological studies which involve many parameters related to the watershed scale e g kumarasamy and belmont 2018 calibration of river flow models under unsteady state also has been broadly investigated through optimization techniques for example in the study of siqueira et al 2016 and lin et al 2017 for water quality this type of technique is commonly conducted under steady state analysis as explored by sharma et al 2017 and cho and lee 2019 for transport of pollutants in rivers through a transient analysis sincock et al 2003 and mannina and viviani 2010 have applied a monte carlo procedure to generate multiple sets of decay rates the strategy is based on the investigation of numerous model runs with randomly parameter values in this procedure usually an objective function is used to discard unrealistic numbers the proposed method in this study estimates transformation rates to represent changes that reactive substances suffer when released in rivers time series of kinetic rates are generated through the integration of i attributes of the river station historical monitoring data ii reference values from other studies found in well established literature and iii random variability multiples options are generated and then the most adequate set of rates is defined using synthetic concentrations this strategy offers the advantage of assessing temporal evolution of pollutant s concentrations when monitoring data is scarce which is often the case in water quality modelling studies creaco et al 2016 the tool developed is named trats transformation rates time series tested through a model integrating hydrodynamic and water quality simulations under unsteady state 2 modelling concepts the sihqual model simulação hidrodinâmica e de qualidade de água 1 1 hydrodynamic and water quality simulation is a tool for transport analysis in rivers under steady and unsteady state integrating deterministic and statistical simulations ferreira et al 2019 the deterministic model refers to the saint venant equations and the one dimensional advection dispersion reaction expression while the second approach explores simplified methods of synthetic series generation this module aims to meet the temporal scale required for boundary conditions since water quality data is usually obtained as irregular samples in addition they are auxiliary for the calibration strategy explored in this research governing equations of hydrodynamic and water quality processes are written as 1 b y t ub y x a u x u a x q 2 u t u u x g y x q v l u a g s 0 s f 3 c t u c x d a a x c x d 2 c x 2 f 0 where b represents the cross section top width m which varies with flow depth y m u is the longitudinal velocity of the flow m s a is the cross section area m2 q is the lateral contribution per unit of channel length m3 sm g is the acceleration of gravity m s2 v l is the input velocity of the lateral contribution in the longitudinal direction m s s 0 is the bottom slope of the channel m m and s f refers to the friction slope m m c is the mean cross sectional concentration of a given constituent kg m3 d represents the longitudinal dispersion coefficient m2 s and f accounts for mass transformations and external loads kg m3s the numerical methods applied to solve the partial differential equations are explicit finite differences lax diffusive scheme for the hydrodynamic module and forward time centered space to solve the water quality equation these solutions have been validated in previous studies ferreira et al 2016 and ferreira et al 2017 these methods are useful to implement different modelling configurations as those introduced in this study the hydrodynamic module provides the advection field and cross sections areas in which the pollutants are diluted in the water quality model a constant dispersion coefficient is adopted while lateral contributions are calculated based on population data and export coefficients and then distributed along the control sections in the main channel the parameters simulated in this study are biochemical oxygen demand bod dissolved oxygen do and organic nitrogen n org the term for mass transformations dc dt accounted in the term f of eq 3 for each of these constituents can be expressed by a bod brown and barnwell 1987 4 dc bod dt k d k s c bod where c bod is the bod concentration mg o 2 l k d represents the deoxygenation rate d 1 and k s is a coefficient for bod removal by sedimentation d 1 b n org chapra 2008 5 dc n org dt k oa k so c n org where c n org is the organic nitrogen concentration mg n l k so is the sedimentation coefficient of organic nitrogen d 1 and k oa is a coefficient for conversion of organic nitrogen to ammonia d 1 c do brown and barnwell 1987 6 dc do dt k a o s c do k d c bod k 4 h α 5 β 1 n a α 6 β 2 n b where k a is the reaeration coefficient d 1 o s represents the dissolved oxygen saturation concentration mg o2 l c do is the do concentration mg o2 l k 4 is a rate for oxygen demand by the sediment g o2 m2d h defines the water mean depth in the channel m α 5 represents an oxygen rate consumed by each unit of oxidized ammonia mg o2 mg n2 α 6 is a rate of oxygen consumed by each unit of oxidized nitrite mg o2 mg n β 1 is ammonia oxidation rate d 1 β 2 is a nitrite oxidation rate d 1 n a is ammonia concentration mg n2 l and n b defines a concentration of nitrite mg n2 l n a and n b are considered as the monitored dataset average in each section table 1 presents ranges suggested in the literature and the values defined in this study for transformation rates involved in bod n org and do balances variations due to temperature are not considered as well as the effect of sediment resuspension the parameters α 5 α 6 β 1 and β 2 are set as constants in this study assumed values are 3 5 1 14 0 12 and 1 0 d 1 respectively according to the ranges suggested by brown and barnwell 1987 the synthetic series are obtained with an autoregressive model ar considering statistical indicators of the historical monitoring dataset mean standard deviation quartiles and concentration with 10 and 90 of occurrence persistence and a random variability which adds a stochastic component the ar model of first order is applied because it is a fast and parsimonious method to investigate multiple scenarios loucks and beek 2017 7 c j 1 μ ρ c j μ z j σ 1 ρ 2 where c j is the concentration at the interval j ρ represents the deterministic component μ is the mean and σ considers the standard deviation of c z j is the random variability component 3 case study the upper iguaçu watershed has 3000 km2 and it is located in curitiba capital of paraná state brazil as shown in fig 1 the river length simulated is 90 km where monitoring points ig2 upstream ig3 ig4 ig5 and ig6 downstream are separated by distances between 18 and 24 km the first three sections drain an urban area while ig5 and ig6 receive contributions mainly from agriculture activities the dataset for water quality parameters is from the period of 2005 to 2017 concentration and respective discharge collected at approximately quarterly field samplings the number of pairs oscillates between 36 and 55 for each section of interest flow data on the other hand is available as daily records the primary simulation period is one year 2010 further details about the case study and dataset are presented in ferreira et al 2019 4 calibration strategy trats method the algorithm to define transformation rates along time follows the steps summarized in fig 2 based on the comparison of monitoring dataset and synthetic series which establish an integration between deterministic and stochastic processes as represented by eq 7 i based on values defined by the state of art literature three intervals of variation are defined for each kinetic rate ii a hundred time series of kinetic rates are randomly generated at daily scale with uniform distribution and variation inside each of the three intervals defined in the previous step r1 r2 and r3 iii quartiles q1 q2 and q3 of the historical monitoring dataset are used to separate the synthetic daily time series of the first river section c ig 2 in four parts c ig 2 q 1 q 1 c ig 2 q2 q2 c ig 2 q3 and c ig 2 q3 iv for high concentrations q 2 c ig 2 q 3 and c ig 2 q 3 the time series of kinetic rates is one of the options of r 1 for intermediary concentrations q 1 c ig 2 q 2 r 2 and for smaller concentrations c ig 2 q 1 r 3 the options are arbitrary among the hundred datasets 5 results and discussion this section firstly presents hydrodynamic solutions that are input for the water quality module following this discussion the synthetic series generation is presented applied as upstream boundary condition for water quality modelling and to support calibration auxiliary experiments to validate the method showed in item 4 are presented as i application of the proposed method to calibrate a water quality model under unsteady state showing that the method is suitable for modelling of different water quality parameters ii comparison of observed and simulated data as statistical indicators with different method configurations tests v1 to v8 iii comparison of observed and simulated data as boxplots with and without the proposed temporal analysis highlighting the relevance of such aspect in overall variability of water quality simulations 5 1 hydrodynamic simulations fig 3 compares observed and simulated flow in the period of 2010 upstream boundary condition is a level time series in section ig2 while lateral inflow is the difference between observed hydrographs in each river section calibration is performed through the procedure of trial and error using the manning roughness coefficient values obtained are between 0 03 and 0 045 results show that the model reproduces fluctuations over time and space with nash sutcliffe coefficients e ns from 0 74 to 0 97 5 2 synthetic series the first strategy was generating daily data td with ρ 0 5 using seasonal μ and σ of the entire dataset the historical dataset is separated according to measurements during winter summer autumn and spring the statistical measures are then calculated for each season originating a hybrid model to represent the simulated year the next case is a hourly test th it applies μ and σ of the monitored dataset and ρ 0 9 generating data at each hour daily concentrations are set as means because the part with random variation of the ar 1 model allows evaluating multiple scenarios a thousand different sets of concentrations are generated a series close to the data measured in the simulated period is then selected considering a minimum difference between quantiles of the observed dataset and the quantiles of the synthetic series and b smallest difference between synthetic values and the data measured in the simulated year the synthetic series selected are th with option b and td with criterion a presented in fig 4 for the three water quality parameters considered in this study in section ig2 upstream boundary condition the test with hourly generation do not differ significantly from daily data in terms of overall variability since both time series are generated from monitoring metrics pollutographs of th show a consistent behavior over time fig 4 without large peaks as those generated with td this possibly occurs because persistence over time is better represented by hourly data for large do concentrations a limit of 8 5 mg o2 l was set arbitrated saturation limit although this value changes due to temperature salinity and pressure conditions chapra 2008 synthetic series analysis is also conducted verifying data normality ferreira et al 2019 and checking residuals of fitted models this latter is presented in the supplementary material for the parameter bod 5 3 water quality simulations figs 5 7 present simulations under unsteady state for bod n org and do concentrations over one year daily kinetic rates table 1 column assumed limits for section ig2 are generated applying the trats method the synthetic series to support the kinetic rates generation is series th also set as upstream boundary condition for the deterministic model rates for downstream sections are defined as parcels of ig2 k ig 3 is 90 k ig 4 60 k ig 5 50 and k ig 6 30 of k ig 2 interpolations are applied to define time series rates at the required time step for numerical solution of the water quality model at last sets of kinetic rates varying over time are calculated for each interval between the control points as averages of the selected series it should be stated that the selected approach presumes that less impacted reaches have smaller transformation activities minor rates to downstream of the iguaçu river so concentrations and rate s values are directly proportional such results are in agreement with the physics of the phenomenon although these rates are affected by several conditions in general decomposition rates are higher at locations that receive larger amounts of organic matter ig2 to ig4 in the case study setting removal rates also tend to be more significant in shallow areas of the channel that receive sewage chapra 2008 in addition reaeration rates tend to be higher for low discharges since rivers in this condition may have a behavior similar to rapids melching and flores 1999 final results for kinetic rates in this research are represented in the supplementary material which also shows statistical measures to evaluate water quality modelling performance other simulation details are presented in ferreira et al 2016 ferreira et al 2017 and ferreira et al 2019 in the pollutographs of figs 5 7 it is observed that overall variability is well represented verifying the method capability in reproducing the behavior of multiple processes represented by the transformation rates results are compared in fig 8 in terms of dataset measures concentrations of 10 and 90 of occurrence c 10 and c 90 which represents critical and common events respectively quartiles are also contrasted since they represent the overall data distribution prediction of characteristic measures is better for n org and do simulations because these parameters have small variability compared to bod despite that as bod modeling n org and do representations tend to underestimate c 10 and overestimate c 90 nonetheless most quartiles difference are less than 2 mg l while c 90 estimations tend to security in terms of water resources management the underestimation of extreme events could lead to misleading decisions predictions of such conditions could be improved including the location of wastewater treatment plants and tributaries for example domestic and industrial wastes are usually released during defined periods of the day therefore these inputs are characterized by a curve with a high peak that decreases until a steady behavior since the model is not representing this fluctuation overall results underestimated critical concentrations 5 4 experiments for trats validation multiple experiments v1 to v8 were compared leading to the proposed trats method as the most adequate these tests are described in table 2 considering different criteria in the procedure presented in section 4 mainly on the steps to generate random series and how to select one of them simulations for bod with series td as boundary condition and each calibration strategy are summarized in figs 9 and 10 comparing characteristic measures c 10 c 90 q 1 q 2 and q 3 and coefficients of variation cv to evaluate the overall variability produced by each calibration scenario the tests are also supported by resultant pollutographs presented in the supplementary material comparison between tests v1 and v2 shows that although there is an increase in overall variability for ig4 to ig6 cv s cv m closer to 1 in v2 presented in fig 10 the pollutographs suffer more attenuation with v2 which can be verified in the supplementary material in addition although higher transformation rates resulted in pollutographs closer to the actual data especially in ig3 the estimation of c 10 q 2 and c 90 except for ig3 is more distinct than the previous approach v1 therefore variation range defined for k d and k s in table 1 is verified part of the tests did not differ significantly in comparison with v1 such as v3 constant rates over time which is the most common approach in deterministic modeling v4 weekly rates v5 daily rates and v6 other series from the hundred options procedure v7 on the other hand generated results with higher temporal variability figure in the supplementary material v7 this is consistent because daily series have higher temporal variability as previously shown in fig 4 and the test generates rates with corresponding oscillation over time test v8 uses the estimated synthetic series in all river sections results show overestimated bod distributions with poor estimation of c 90 and quartiles simulated c 10 on the other hand is closer to c 10 of the monitored dataset fig 9 overall most strategies overestimated c 90 and underestimated c 10 reviewing all tests v0 is the test that produced reasonable estimates for concentration quartiles in the studied period fig 9 and a coefficient of variation close to the monitored dataset fig 10 in comparison with experiment v3 in which the temporal variation is disregarded the proposed methodology v0 adds ten seconds to water quality simulations under unsteady state although this interval increases with the quantity of kinetic rates 5 5 temporal variability of lateral loads and kinetic rates this section explores the relevance of temporal assessment of internal transformation rates in simulated results through two experiments in addition the role of lateral inputs is evaluated since they can also be involved in the calibration phase e g mattson and isaac 1999 bui et al 2019 5 5 1 experiment a the first test compares the original results presented in item section 5 3 here refereed as case i with simulations using kinetic coefficients constant in time case ii and with a scenario in which both transformation rates and lateral inputs not varying over time case iii these tests are conducted for organic matter and nutrient simulations figs 11 13 show the results in terms of boxplots and annual loads estimations respective pollutographs also reaffirm the tests findings and are presented in the supplementary material disregarding the temporal variability of calibration parameters leads to higher attenuation of pollutant s distribution over time so boxplots have smaller interquartile intervals fig 11 for case ii case iii shows that this behavior is highlighted if dilution effects on lateral inputs are simplified fig 11 iii and more likely to generate overestimated concentrations the comparison of these tests shows that temporal variation of kinetic rates has an important role in the overall variability of bod concentrations simulations of n org on the other hand are less affected similar behavior is verified for annual mean loads estimation fig 13 results suggest that loss of temporal variability in transformation rates generates smaller annual loads on the other hand when lateral input is constant over time overestimated values are calculated by the model 5 5 2 experiment b the following experiment is conducted only for bod with the same conditions as the previous tests this test on the other hand apply as boundary condition a series in a sub daily scale tsd it uses μ and σ of entire dataset ρ 0 99 generating data at each 50 s which is the required time step for numerical solution of the deterministic model with option a for series selection due to processing limitations only twenty options are evaluated in this test instead of one thousand although this series considers a high persistence ρ 0 99 as expected in a series with a small time step some variations are still abrupt from one time step to the next as presented in the supplementary material therefore daily means of series have small variations as detected in fig 14 case i is the resulting simulation with tsd as boundary condition while cases ii and iii have the same configuration from experiment a boxplots for comparison are presented in fig 15 the objective of experiment b is to show that even with a misleading upstream boundary condition results are improved when the unsteady behavior is represented in the calibration phase and in the dilution effects of input lateral loads when these aspects are neglected the model is able to reproduce only median values since natural persistence over time is not being respected 6 conclusions transport and fate of reactive substances in the environment are result of several interactions among physical chemical and biological activities because the investigation of ecological processes are often neglected in water quality simulations they become calibration parameters this study introduces a strategy for definition of transformation rates over time aiming improved predictions of mass transport in rivers the significance of temporal variability in kinetic rates and lateral contributions observed mostly for bod simulations is summarized by the following statements i comparison of tests removing temporal oscillation of these aspects and with boundary condition of higher variability daily data from hourly samples temporal assessment of boundary condition is not enough to guarantee variation in downstream sections without variability in kinetic rates and lateral inputs ii boundary condition with small daily variability produces reasonable results due to the developed calibration procedure and due to lateral inputs temporal variation without these aspects overall variability is not well represented i e if boundary condition does not represent persistence over time calibration with temporal variation is even more relevant trats provides a unique calibration strategy for modelling water quality under unsteady state based on system characteristics that allow to overcome a common issue in calibration virtual values that solve the mathematical problem but without physical meaning random variation that incorporate the inherent uncertainty and temporal variation through a link that relates transformation rates to intervals of concentration the method is different from traditional association of kinetic rates to empirical equations and other conditions hydraulic characteristics or composition of wastewater for example the procedure also overcomes the usual large processing time required in automatic techniques and it less subjective than traditional trial and error analysis the proposed strategy to estimate temporal variation of kinetic rates presumes two main aspects i daily transformation rates have a uniform distribution ii less impacted reaches have smaller activity for sink sources analysis minor rates to downstream this means that concentrations and rate s values are directly proportional however such hypothesis might not be valid for all parameters in the same context bod simulations showed to be more affected by the strategy in defining time series of transformation rates than other water quality parameters this aspect does not necessarily indicates that temporal variation of kinetic processes are not relevant for the other components but that further investigations towards other distributions different processes such as interaction sediment water and analysis to understand spatial scales should be conducted such sediment resuspension effects of temperature hydraulic characteristics among others factors additionally the calibration procedure also inserts uncertainty in model results which could be further assessed to increase reliability in future efforts credit authorship contribution statement danieli mara ferreira conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization cristovão vicente scapulatempo fernandes conceptualization methodology resources supervision writing review editing project administration eloy kaviski conceptualization methodology supervision writing review editing darrell fontane conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank the brazilian agencies capes coordenação de aperfeiçoamento de pessoal de nível superior coordination for the improvement of higher level personnel and cnpq conselho nacional de desenvolvimento científico e tecnológico national council for scientific and technological development for financial support during the research development the department of civil and environmental engineering in colorado state university and the anonymous reviewers for their valuable comments appendix a supplementary data supplementary data associated with this article can be found in the online version athttps doi org 10 1016 j jhydrol 2020 124769 supplementary data the following are the supplementary data to this article supplementary data 1 
5528,while advection dispersion and their hydraulic background are broadly explored in water quality modelling conversion processes reaction and decay rates usually lack of attention they represent physical chemical and biological processes that reactive substances go through in contact with water typically assumed as calibration parameters this study introduces a method for model calibration considering a temporal aspect of transformation coefficients trats transformation rates time series daily decay rates are generated as result of i attributes of the river station ii reference values from literature and iii random variability these transformation parameters are then applied in water quality simulations under unsteady state in a river for biochemical oxygen demand bod organic nitrogen n org and dissolved oxygen do the proposed method for integrated modelling suggests that temporal variation of kinetic processes may play an import role in transport and fate of pollutants in terms of overall variability and attenuation over time and space and this aspect can be included in the calibration phase keywords water quality modelling calibration under unsteady state transformation processes 1 introduction once a pollutant reaches a watercourse its transport along space and time is the result of several processes besides advection diffusion and dispersion non conservative substances are affected by activities that result in substance transformations affecting transfers of matter through the system these mechanisms represent losses or gains of mass due to chemical chemical reactions in general physical particulate settling for example or biological processes such as growth and death of algae within the water in general such processes are described by first order kinetic reactions and reference values these rates can be obtained through analytical tests statistical conceptual and empirical analysis usual equations relate them to channel hydraulics such as velocity water level slope or discharges brown and barnwell 1987 these latter differ mainly by the suggested application range and results can be significantly divergent nuruzzaman et al 2018 fluctuation of these coefficients reported in the literature can also be wide a common example is the reaeration rate which oscillates from 0 to 100 d 1 according to brown and barnwell 1987 nonetheless these parameter s values are often unknown because of spatial and temporal variability measurement challenges simplification in model descriptions or commonly lack of data therefore they become calibration coefficients in most studies of water quality modelling e g tang et al 2016 chaudhary et al 2018 despite the efforts to develop analytical experiments to estimate them in laboratory conditions e g pittroff et al 2017 nuruzzaman et al 2018 the authors alam and dutta 2012 call attention to the significant number of studies highlighting the need for better understanding of in stream processes in advanced modelling this study introduces a new perspective to calibrate water quality models under unsteady state as an alternative to the traditional techniques based on trial and error and optimization procedures the first one is usually subjective and time consuming while the second often assumes criteria that solve the problem through a mathematical perspective therefore it may generate illogical values kumarasamy and belmont 2018 furthermore often a technique identifies multiple datasets that satisfy the optimization problem however depending on a given combination these parameters may superimpose transport effects and originate misleading interpretations dortch and johnson 1992 computation time also is increased due to the multiple operations required to reach a solution semiromi et al 2018 automatic routines are commonly applied in hydrological studies which involve many parameters related to the watershed scale e g kumarasamy and belmont 2018 calibration of river flow models under unsteady state also has been broadly investigated through optimization techniques for example in the study of siqueira et al 2016 and lin et al 2017 for water quality this type of technique is commonly conducted under steady state analysis as explored by sharma et al 2017 and cho and lee 2019 for transport of pollutants in rivers through a transient analysis sincock et al 2003 and mannina and viviani 2010 have applied a monte carlo procedure to generate multiple sets of decay rates the strategy is based on the investigation of numerous model runs with randomly parameter values in this procedure usually an objective function is used to discard unrealistic numbers the proposed method in this study estimates transformation rates to represent changes that reactive substances suffer when released in rivers time series of kinetic rates are generated through the integration of i attributes of the river station historical monitoring data ii reference values from other studies found in well established literature and iii random variability multiples options are generated and then the most adequate set of rates is defined using synthetic concentrations this strategy offers the advantage of assessing temporal evolution of pollutant s concentrations when monitoring data is scarce which is often the case in water quality modelling studies creaco et al 2016 the tool developed is named trats transformation rates time series tested through a model integrating hydrodynamic and water quality simulations under unsteady state 2 modelling concepts the sihqual model simulação hidrodinâmica e de qualidade de água 1 1 hydrodynamic and water quality simulation is a tool for transport analysis in rivers under steady and unsteady state integrating deterministic and statistical simulations ferreira et al 2019 the deterministic model refers to the saint venant equations and the one dimensional advection dispersion reaction expression while the second approach explores simplified methods of synthetic series generation this module aims to meet the temporal scale required for boundary conditions since water quality data is usually obtained as irregular samples in addition they are auxiliary for the calibration strategy explored in this research governing equations of hydrodynamic and water quality processes are written as 1 b y t ub y x a u x u a x q 2 u t u u x g y x q v l u a g s 0 s f 3 c t u c x d a a x c x d 2 c x 2 f 0 where b represents the cross section top width m which varies with flow depth y m u is the longitudinal velocity of the flow m s a is the cross section area m2 q is the lateral contribution per unit of channel length m3 sm g is the acceleration of gravity m s2 v l is the input velocity of the lateral contribution in the longitudinal direction m s s 0 is the bottom slope of the channel m m and s f refers to the friction slope m m c is the mean cross sectional concentration of a given constituent kg m3 d represents the longitudinal dispersion coefficient m2 s and f accounts for mass transformations and external loads kg m3s the numerical methods applied to solve the partial differential equations are explicit finite differences lax diffusive scheme for the hydrodynamic module and forward time centered space to solve the water quality equation these solutions have been validated in previous studies ferreira et al 2016 and ferreira et al 2017 these methods are useful to implement different modelling configurations as those introduced in this study the hydrodynamic module provides the advection field and cross sections areas in which the pollutants are diluted in the water quality model a constant dispersion coefficient is adopted while lateral contributions are calculated based on population data and export coefficients and then distributed along the control sections in the main channel the parameters simulated in this study are biochemical oxygen demand bod dissolved oxygen do and organic nitrogen n org the term for mass transformations dc dt accounted in the term f of eq 3 for each of these constituents can be expressed by a bod brown and barnwell 1987 4 dc bod dt k d k s c bod where c bod is the bod concentration mg o 2 l k d represents the deoxygenation rate d 1 and k s is a coefficient for bod removal by sedimentation d 1 b n org chapra 2008 5 dc n org dt k oa k so c n org where c n org is the organic nitrogen concentration mg n l k so is the sedimentation coefficient of organic nitrogen d 1 and k oa is a coefficient for conversion of organic nitrogen to ammonia d 1 c do brown and barnwell 1987 6 dc do dt k a o s c do k d c bod k 4 h α 5 β 1 n a α 6 β 2 n b where k a is the reaeration coefficient d 1 o s represents the dissolved oxygen saturation concentration mg o2 l c do is the do concentration mg o2 l k 4 is a rate for oxygen demand by the sediment g o2 m2d h defines the water mean depth in the channel m α 5 represents an oxygen rate consumed by each unit of oxidized ammonia mg o2 mg n2 α 6 is a rate of oxygen consumed by each unit of oxidized nitrite mg o2 mg n β 1 is ammonia oxidation rate d 1 β 2 is a nitrite oxidation rate d 1 n a is ammonia concentration mg n2 l and n b defines a concentration of nitrite mg n2 l n a and n b are considered as the monitored dataset average in each section table 1 presents ranges suggested in the literature and the values defined in this study for transformation rates involved in bod n org and do balances variations due to temperature are not considered as well as the effect of sediment resuspension the parameters α 5 α 6 β 1 and β 2 are set as constants in this study assumed values are 3 5 1 14 0 12 and 1 0 d 1 respectively according to the ranges suggested by brown and barnwell 1987 the synthetic series are obtained with an autoregressive model ar considering statistical indicators of the historical monitoring dataset mean standard deviation quartiles and concentration with 10 and 90 of occurrence persistence and a random variability which adds a stochastic component the ar model of first order is applied because it is a fast and parsimonious method to investigate multiple scenarios loucks and beek 2017 7 c j 1 μ ρ c j μ z j σ 1 ρ 2 where c j is the concentration at the interval j ρ represents the deterministic component μ is the mean and σ considers the standard deviation of c z j is the random variability component 3 case study the upper iguaçu watershed has 3000 km2 and it is located in curitiba capital of paraná state brazil as shown in fig 1 the river length simulated is 90 km where monitoring points ig2 upstream ig3 ig4 ig5 and ig6 downstream are separated by distances between 18 and 24 km the first three sections drain an urban area while ig5 and ig6 receive contributions mainly from agriculture activities the dataset for water quality parameters is from the period of 2005 to 2017 concentration and respective discharge collected at approximately quarterly field samplings the number of pairs oscillates between 36 and 55 for each section of interest flow data on the other hand is available as daily records the primary simulation period is one year 2010 further details about the case study and dataset are presented in ferreira et al 2019 4 calibration strategy trats method the algorithm to define transformation rates along time follows the steps summarized in fig 2 based on the comparison of monitoring dataset and synthetic series which establish an integration between deterministic and stochastic processes as represented by eq 7 i based on values defined by the state of art literature three intervals of variation are defined for each kinetic rate ii a hundred time series of kinetic rates are randomly generated at daily scale with uniform distribution and variation inside each of the three intervals defined in the previous step r1 r2 and r3 iii quartiles q1 q2 and q3 of the historical monitoring dataset are used to separate the synthetic daily time series of the first river section c ig 2 in four parts c ig 2 q 1 q 1 c ig 2 q2 q2 c ig 2 q3 and c ig 2 q3 iv for high concentrations q 2 c ig 2 q 3 and c ig 2 q 3 the time series of kinetic rates is one of the options of r 1 for intermediary concentrations q 1 c ig 2 q 2 r 2 and for smaller concentrations c ig 2 q 1 r 3 the options are arbitrary among the hundred datasets 5 results and discussion this section firstly presents hydrodynamic solutions that are input for the water quality module following this discussion the synthetic series generation is presented applied as upstream boundary condition for water quality modelling and to support calibration auxiliary experiments to validate the method showed in item 4 are presented as i application of the proposed method to calibrate a water quality model under unsteady state showing that the method is suitable for modelling of different water quality parameters ii comparison of observed and simulated data as statistical indicators with different method configurations tests v1 to v8 iii comparison of observed and simulated data as boxplots with and without the proposed temporal analysis highlighting the relevance of such aspect in overall variability of water quality simulations 5 1 hydrodynamic simulations fig 3 compares observed and simulated flow in the period of 2010 upstream boundary condition is a level time series in section ig2 while lateral inflow is the difference between observed hydrographs in each river section calibration is performed through the procedure of trial and error using the manning roughness coefficient values obtained are between 0 03 and 0 045 results show that the model reproduces fluctuations over time and space with nash sutcliffe coefficients e ns from 0 74 to 0 97 5 2 synthetic series the first strategy was generating daily data td with ρ 0 5 using seasonal μ and σ of the entire dataset the historical dataset is separated according to measurements during winter summer autumn and spring the statistical measures are then calculated for each season originating a hybrid model to represent the simulated year the next case is a hourly test th it applies μ and σ of the monitored dataset and ρ 0 9 generating data at each hour daily concentrations are set as means because the part with random variation of the ar 1 model allows evaluating multiple scenarios a thousand different sets of concentrations are generated a series close to the data measured in the simulated period is then selected considering a minimum difference between quantiles of the observed dataset and the quantiles of the synthetic series and b smallest difference between synthetic values and the data measured in the simulated year the synthetic series selected are th with option b and td with criterion a presented in fig 4 for the three water quality parameters considered in this study in section ig2 upstream boundary condition the test with hourly generation do not differ significantly from daily data in terms of overall variability since both time series are generated from monitoring metrics pollutographs of th show a consistent behavior over time fig 4 without large peaks as those generated with td this possibly occurs because persistence over time is better represented by hourly data for large do concentrations a limit of 8 5 mg o2 l was set arbitrated saturation limit although this value changes due to temperature salinity and pressure conditions chapra 2008 synthetic series analysis is also conducted verifying data normality ferreira et al 2019 and checking residuals of fitted models this latter is presented in the supplementary material for the parameter bod 5 3 water quality simulations figs 5 7 present simulations under unsteady state for bod n org and do concentrations over one year daily kinetic rates table 1 column assumed limits for section ig2 are generated applying the trats method the synthetic series to support the kinetic rates generation is series th also set as upstream boundary condition for the deterministic model rates for downstream sections are defined as parcels of ig2 k ig 3 is 90 k ig 4 60 k ig 5 50 and k ig 6 30 of k ig 2 interpolations are applied to define time series rates at the required time step for numerical solution of the water quality model at last sets of kinetic rates varying over time are calculated for each interval between the control points as averages of the selected series it should be stated that the selected approach presumes that less impacted reaches have smaller transformation activities minor rates to downstream of the iguaçu river so concentrations and rate s values are directly proportional such results are in agreement with the physics of the phenomenon although these rates are affected by several conditions in general decomposition rates are higher at locations that receive larger amounts of organic matter ig2 to ig4 in the case study setting removal rates also tend to be more significant in shallow areas of the channel that receive sewage chapra 2008 in addition reaeration rates tend to be higher for low discharges since rivers in this condition may have a behavior similar to rapids melching and flores 1999 final results for kinetic rates in this research are represented in the supplementary material which also shows statistical measures to evaluate water quality modelling performance other simulation details are presented in ferreira et al 2016 ferreira et al 2017 and ferreira et al 2019 in the pollutographs of figs 5 7 it is observed that overall variability is well represented verifying the method capability in reproducing the behavior of multiple processes represented by the transformation rates results are compared in fig 8 in terms of dataset measures concentrations of 10 and 90 of occurrence c 10 and c 90 which represents critical and common events respectively quartiles are also contrasted since they represent the overall data distribution prediction of characteristic measures is better for n org and do simulations because these parameters have small variability compared to bod despite that as bod modeling n org and do representations tend to underestimate c 10 and overestimate c 90 nonetheless most quartiles difference are less than 2 mg l while c 90 estimations tend to security in terms of water resources management the underestimation of extreme events could lead to misleading decisions predictions of such conditions could be improved including the location of wastewater treatment plants and tributaries for example domestic and industrial wastes are usually released during defined periods of the day therefore these inputs are characterized by a curve with a high peak that decreases until a steady behavior since the model is not representing this fluctuation overall results underestimated critical concentrations 5 4 experiments for trats validation multiple experiments v1 to v8 were compared leading to the proposed trats method as the most adequate these tests are described in table 2 considering different criteria in the procedure presented in section 4 mainly on the steps to generate random series and how to select one of them simulations for bod with series td as boundary condition and each calibration strategy are summarized in figs 9 and 10 comparing characteristic measures c 10 c 90 q 1 q 2 and q 3 and coefficients of variation cv to evaluate the overall variability produced by each calibration scenario the tests are also supported by resultant pollutographs presented in the supplementary material comparison between tests v1 and v2 shows that although there is an increase in overall variability for ig4 to ig6 cv s cv m closer to 1 in v2 presented in fig 10 the pollutographs suffer more attenuation with v2 which can be verified in the supplementary material in addition although higher transformation rates resulted in pollutographs closer to the actual data especially in ig3 the estimation of c 10 q 2 and c 90 except for ig3 is more distinct than the previous approach v1 therefore variation range defined for k d and k s in table 1 is verified part of the tests did not differ significantly in comparison with v1 such as v3 constant rates over time which is the most common approach in deterministic modeling v4 weekly rates v5 daily rates and v6 other series from the hundred options procedure v7 on the other hand generated results with higher temporal variability figure in the supplementary material v7 this is consistent because daily series have higher temporal variability as previously shown in fig 4 and the test generates rates with corresponding oscillation over time test v8 uses the estimated synthetic series in all river sections results show overestimated bod distributions with poor estimation of c 90 and quartiles simulated c 10 on the other hand is closer to c 10 of the monitored dataset fig 9 overall most strategies overestimated c 90 and underestimated c 10 reviewing all tests v0 is the test that produced reasonable estimates for concentration quartiles in the studied period fig 9 and a coefficient of variation close to the monitored dataset fig 10 in comparison with experiment v3 in which the temporal variation is disregarded the proposed methodology v0 adds ten seconds to water quality simulations under unsteady state although this interval increases with the quantity of kinetic rates 5 5 temporal variability of lateral loads and kinetic rates this section explores the relevance of temporal assessment of internal transformation rates in simulated results through two experiments in addition the role of lateral inputs is evaluated since they can also be involved in the calibration phase e g mattson and isaac 1999 bui et al 2019 5 5 1 experiment a the first test compares the original results presented in item section 5 3 here refereed as case i with simulations using kinetic coefficients constant in time case ii and with a scenario in which both transformation rates and lateral inputs not varying over time case iii these tests are conducted for organic matter and nutrient simulations figs 11 13 show the results in terms of boxplots and annual loads estimations respective pollutographs also reaffirm the tests findings and are presented in the supplementary material disregarding the temporal variability of calibration parameters leads to higher attenuation of pollutant s distribution over time so boxplots have smaller interquartile intervals fig 11 for case ii case iii shows that this behavior is highlighted if dilution effects on lateral inputs are simplified fig 11 iii and more likely to generate overestimated concentrations the comparison of these tests shows that temporal variation of kinetic rates has an important role in the overall variability of bod concentrations simulations of n org on the other hand are less affected similar behavior is verified for annual mean loads estimation fig 13 results suggest that loss of temporal variability in transformation rates generates smaller annual loads on the other hand when lateral input is constant over time overestimated values are calculated by the model 5 5 2 experiment b the following experiment is conducted only for bod with the same conditions as the previous tests this test on the other hand apply as boundary condition a series in a sub daily scale tsd it uses μ and σ of entire dataset ρ 0 99 generating data at each 50 s which is the required time step for numerical solution of the deterministic model with option a for series selection due to processing limitations only twenty options are evaluated in this test instead of one thousand although this series considers a high persistence ρ 0 99 as expected in a series with a small time step some variations are still abrupt from one time step to the next as presented in the supplementary material therefore daily means of series have small variations as detected in fig 14 case i is the resulting simulation with tsd as boundary condition while cases ii and iii have the same configuration from experiment a boxplots for comparison are presented in fig 15 the objective of experiment b is to show that even with a misleading upstream boundary condition results are improved when the unsteady behavior is represented in the calibration phase and in the dilution effects of input lateral loads when these aspects are neglected the model is able to reproduce only median values since natural persistence over time is not being respected 6 conclusions transport and fate of reactive substances in the environment are result of several interactions among physical chemical and biological activities because the investigation of ecological processes are often neglected in water quality simulations they become calibration parameters this study introduces a strategy for definition of transformation rates over time aiming improved predictions of mass transport in rivers the significance of temporal variability in kinetic rates and lateral contributions observed mostly for bod simulations is summarized by the following statements i comparison of tests removing temporal oscillation of these aspects and with boundary condition of higher variability daily data from hourly samples temporal assessment of boundary condition is not enough to guarantee variation in downstream sections without variability in kinetic rates and lateral inputs ii boundary condition with small daily variability produces reasonable results due to the developed calibration procedure and due to lateral inputs temporal variation without these aspects overall variability is not well represented i e if boundary condition does not represent persistence over time calibration with temporal variation is even more relevant trats provides a unique calibration strategy for modelling water quality under unsteady state based on system characteristics that allow to overcome a common issue in calibration virtual values that solve the mathematical problem but without physical meaning random variation that incorporate the inherent uncertainty and temporal variation through a link that relates transformation rates to intervals of concentration the method is different from traditional association of kinetic rates to empirical equations and other conditions hydraulic characteristics or composition of wastewater for example the procedure also overcomes the usual large processing time required in automatic techniques and it less subjective than traditional trial and error analysis the proposed strategy to estimate temporal variation of kinetic rates presumes two main aspects i daily transformation rates have a uniform distribution ii less impacted reaches have smaller activity for sink sources analysis minor rates to downstream this means that concentrations and rate s values are directly proportional however such hypothesis might not be valid for all parameters in the same context bod simulations showed to be more affected by the strategy in defining time series of transformation rates than other water quality parameters this aspect does not necessarily indicates that temporal variation of kinetic processes are not relevant for the other components but that further investigations towards other distributions different processes such as interaction sediment water and analysis to understand spatial scales should be conducted such sediment resuspension effects of temperature hydraulic characteristics among others factors additionally the calibration procedure also inserts uncertainty in model results which could be further assessed to increase reliability in future efforts credit authorship contribution statement danieli mara ferreira conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization cristovão vicente scapulatempo fernandes conceptualization methodology resources supervision writing review editing project administration eloy kaviski conceptualization methodology supervision writing review editing darrell fontane conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank the brazilian agencies capes coordenação de aperfeiçoamento de pessoal de nível superior coordination for the improvement of higher level personnel and cnpq conselho nacional de desenvolvimento científico e tecnológico national council for scientific and technological development for financial support during the research development the department of civil and environmental engineering in colorado state university and the anonymous reviewers for their valuable comments appendix a supplementary data supplementary data associated with this article can be found in the online version athttps doi org 10 1016 j jhydrol 2020 124769 supplementary data the following are the supplementary data to this article supplementary data 1 
5529,understanding the generation mechanisms of floods is important to estimating future flood hazards by improving flood frequency analysis and flood trend interpretation from the hydrological perspective of flood classification flood types are determined by the seasonality or magnitudes of hydrological drivers e g rainfall snowmelt catchment wetness few studies attribute floods to the causative drivers by quantifying the contributions of each driver to the flood peaks therefore we propose the quantification of driver contributions for flood classification qdc fc method which combines a data driven hydrological model the shapley value which quantifies driver contributions to flood peaks and a classification tree for dividing floods into five types long rain floods 1 day rain on snow floods snowmelt floods short rain floods 1 day and antecedent wet floods in the eastern monsoon region emr of china the method is applied in 225 catchments where the data driven model has high nash sutcliffe efficiency nse 0 6 in daily runoff simulation and a high r squared value r 2 0 6 in flood peak simulation under the impact of the eastern monsoon short rain floods and long rain floods are the most frequent types in 143 and 69 catchments respectively in 213 catchments all extreme floods with return periods larger than 10 years are caused by short rain or long rain antecedent wetness causes more than 20 of floods in 26 out of 66 catchments in northeastern china and northern china with cumulative catchment wetness in summer although snowmelt induced floods occur in april in some catchments of northeastern china only one catchment has more than 20 of floods caused by snowmelt as a new method for flood classification the qdc fc sheds light on the mixed generation mechanisms of floods and possible drivers of flood trends in the emr of china keywords china data driven model flood generation mechanisms flood type interpretable machine learning shapley value 1 introduction river flooding causes severe damage to human society climate change may further exacerbate flood hazards according to future projections by hydrological models flood magnitudes and frequencies will increase significantly in many regions of the world dankers et al 2014 li et al 2016 asadieh and krakauer 2017 thober et al 2018 a better understanding of flood generation mechanisms facilitates accurate estimations of future flood hazards in two aspects first with knowledge about flood generation mechanisms the historical trends of floods can be attributed to the changing characteristics of causative drivers previous studies detect the various trends of flood magnitudes and frequencies based on long term streamflow observations at a global or continental scale archfield et al 2016 berghuijs et al 2017 do et al 2017 blöschl et al 2019 the climate driven trends of floods correlate with the changing magnitudes of hydrological drivers such as extreme rainfall slater and villarini 2016 snowmelt burn and whitfield 2017 blöschl et al 2019 and soil moisture blöschl et al 2019 wasko and nathan 2019 knowing flood generation mechanisms helps extract the predominant driver from an interaction of all those hydrological drivers second considering flood generation mechanisms improves flood frequency analysis mixing floods with all generation mechanisms may violate the assumption that all samples have an identical distribution in estimating frequency distributions gumbel 1941 tarasova et al 2019 adding information about flood generation processes helps discover the controls of the upper tail and lower tail of flood frequency curves alila and mtiraoui 2002 villarini and smith 2010 as reviewed by tarasova et al 2019 several studies determined flood generation mechanisms by classifying floods from three perspectives hydroclimatic hydrological and hydrograph based in this study we only adopt the hydrological perspective of flood classification i e classifying floods based on hydrometeorological input e g rainfall temperature and catchment state e g snow cover local wetness conditions floods can be classified by two categories of approaches from a hydrological perspective the first category of approaches determines predominant generation mechanisms based on the timing of floods and hydrological drivers these approaches assume that floods are mainly caused by hydrological drivers with similar seasonality specifically if the mean date of peak flow approximates the mean date of peak rainfall snowmelt or soil moisture excess the predominant generation process of floods can be determined correspondingly berghuijs et al 2016 blöschl et al 2017 do et al 2019 along with the mean occurring dates of floods and hydrological drivers some studies also use the consistency of seasonality concentration between floods and hydrological variables ye et al 2017 berghuijs et al 2019 the second category of approaches determines generation mechanisms for each event by setting criteria related to the magnitudes of hydrological drivers in the event in the pioneering work conducted by merz and blöschl 2003 they set regional threshold criteria for several hydrological drivers e g storm duration rainfall depth catchment wetness and proposed five flood types in austria long rain flood short rain flood flash flood rain on snow flood and snowmelt flood this classification scheme was then applied to many other regions by modifying the thresholds nied et al 2014 sikorska et al 2015 stein et al 2019 rather than setting subjective thresholds other studies used clustering algorithms to group flood events automatically and objectively turkington et al 2016 keller et al 2018 however all existing approaches suffer from certain limitations in the first category of approaches i e the seasonality based approaches although the hydrological drivers and floods have similar mean occurrence dates it is unclear whether the hydrological driver occurs before or after the flood for each flood event therefore those hydrological drivers are not necessarily causative drivers in the second category of approaches i e the criteria based approaches flood types are defined according to the combination of hydrological drivers delineated by criteria from either subjective expert knowledge or objective clustering algorithms it is difficult to assess the importance of each driver on flood peaks and subsequently determine the causative drivers for example in catchments where floods are not sensitive to soil moisture the 95 quantile of daily rainfall has far larger effects on flood peaks than the 95 quantile of soil moisture however existing criteria based approaches cannot distinguish such effects furthermore the results are highly dependent on specific catchments impeding the transfer of applications in different regions in summary we need a standardized and transferable method for flood classification that does not confuse hydrological drivers during flood events with causative drivers in addition to the methodology gap mentioned above another gap is the absence of a comprehensive understanding of flood generation mechanisms in china although yang et al 2019 describe the flood seasonality and corresponding meteorological systems e g monsoon tropical cyclones that drive floods across china a large scale investigation of the flood generation mechanisms is still unavailable from a hydrological perspective a large part of china experiences a rainy season led by the movement of the eastern monsoon yang et al 2013 wu et al 2016 it is unclear whether in some cases catchment wetness or snowmelt becomes the main driver of floods the aim of this study is to quantify the contributions of hydrological drivers to flood peaks so that the causative drivers can be extracted and linked to flood types we design the quantification of driver contributions for flood classification qdc fc a novel scheme that combines a data driven hydrological model the shapley value shapley 1953 štrumbelj and kononenko 2014 a concept in game theory that distributes the gains of the game to several players and a classification tree the shapley value is now popular in machine learning since it can interpret black box models by partitioning the contributions of the predictors to the target value for each sample štrumbelj and kononenko 2014 it has been used for infrastructure stormwater management william et al 2017 and transboundary river cooperation li et al 2019 however this concept has not been used in any flood study in this study a large scale analysis of flood generation mechanisms is conducted in hundreds of catchments in the eastern monsoon region emr liu and shi 2018 of china we are concerned with the following understanding of floods 1 the most frequent type of flood 2 the most extreme type of flood and 3 the seasonal distribution of different types of floods the paper is organized as follows section 2 presents the study area and data section 3 introduces the methods including the data driven model the shapley value and the classification tree in section 4 the results unravel the ratios of different flood types extreme flood types and intra annual variations in flood types section 5 discusses the strengths and limitations of the study we also state the implications of the results in flood trend interpretations and flood frequency analysis in section 5 section 6 gives the conclusions of the study 2 data 2 1 study area we select catchments in the eastern monsoon region emr liu and shi 2018 in this study for the following reasons first the emr contains floodplains with frequent and severe floods since it includes several humid major basins in china i e the pearl river basin the southeast basin the huaihe river basin the haihe river basin the songliao river basin the middle lower yangtze river basin and the middle lower yellow river basin see fig 1 second the emr has high flood vulnerability with dense populations large urban areas and vast agricultural lands zhang et al 2016 du et al 2018 du et al 2019 third most floods in the emr have similar large scale meteorological origins since they are mainly affected by the east asian summer monsoon easm kundzewicz et al 2019 2 2 runoff data daily runoff data from 551 streamflow gauge stations in the emr were obtained from the historic hydrological database compiled by the water resources information center of the ministry of water resources in china http www mwr gov cn english the research period was from 1960 to 2001 since most of the data records fell into this time span we only selected mesoscale catchments from 100 k m 2 to 100 000 k m 2 where the subdaily process was not important a quality check was carried out to select catchments with limited impacts from the following data problems 1 measurement errors and inconsistency e g changing instruments or station datum and 2 human impacts including dam construction and land use changes we had little information about the measurements dams and land use conditions in 1960 2001 in all catchments therefore the quality check totally relied on the measured flow data the cemaneige gr4j perrin et al 2003 valéry et al 2014 model was applied to each catchment in 1960 2001 for calibrating parameters with available daily runoff data we assumed that measurement problems or severe human impacts would violate the rainfall runoff relationship and cause low values of the nash sutcliffe efficiency coefficient nse in model calibration therefore only catchments with satisfactory model performance nse 0 5 without a log transformation of data moriasi et al 2007 in daily runoff simulation were included in the study although low nses might result from the insufficient representation in the rainfall runoff processes of the model we did not need to distinguish data and model problems in causing low nses since we only focused on catchments with nse 0 5 the information of cemaneige gr4j is introduced in section 3 1 we used the airgr r package coron et al 2017 which developed tools for running the cemaneige gr4j model and implementing calibration algorithms filtered by catchment areas and the quality check 441 catchments remained to get sufficient flood samples we selected 411 from 441 catchments with 10 valid years of data a valid year was a year when the days with missing data were less than 10 of days from april to october in a station we selected april october because flood seasons occur in these months in china yang et al 2019 in the following sections of the paper we proceed with the data driven hydrological modeling and flood sampling only in the valid years for each catchment 2 3 meteorological data precipitation data were obtained from the china gauge based daily precipitation analysis cgdpa shen and xiong 2016 based on approximately 2400 rainfall gauge stations cgdpa was a daily precipitation product with a 0 25 0 25 resolution from 1960 to 2015 we used the penman monteith equation allen et al 1998 to calculate the daily potential evapotranspiration rates the equation required daily temperature relative humidity wind speed and solar hours which were obtained from the stations in the china meteorological data service center http data cma cn en the numbers of available stations were approximately 800 and 2400 in 1960 1987 and 1988 2001 respectively all in site meteorological data were interpolated into 0 25 0 25 raster maps on a daily scale using the universal kriging method in the gstat r tool pebesma and heuvelink 2016 the elevation data from the srtm 90 m dem jarvis et al 2008 were used as covariates in the universal kriging method we averaged values from all raster grids within the catchment boundary to obtain the areal mean series of precipitation potential evapotranspiration and air temperature on a daily scale 2 4 catchment properties the catchment boundaries were extracted using the srtm 90 m dem jarvis et al 2008 the catchment areas and mean elevations could be calculated consequently the baseflow was extracted by the recursive digital filter developed by lyne and hollick 1979 then the baseflow index bfi was calculated by dividing the baseflow volume by the total runoff volume the saturated hydraulic conductivity k sat was obtained from dai et al 2013 by calculating the weighted sum of k sat in all soil layers down to approximately 1 5 m depth 3 methods the qdc fc method for a catchment includes four steps see fig 2 first the cemaneige gr4j hydrological model is calibrated to provide the time of concentration soil moisture simulation and snowmelt simulation in cold winter catchments second a data driven hydrological model is trained to predict daily runoff using meteorological input within the time of concentration third the shapley value quantifies the contributions of the input drivers to the predicted flood peaks based on the model fourth a classification tree assigns floods to different types according to the contributions of the input drivers the details of these four steps are introduced as follows 3 1 cemaneige gr4j hydrological model gr4j is a conceptual rainfall runoff model with a soil moisture accounting module and a routing module perrin et al 2003 gr4j runs on a daily scale in a lumped way and contains only four parameters the input data of the gr4j include areal average precipitation and potential evapotranspiration on a daily scale the cemaneige module is a two parameter semi distributed snow accounting routine considering the sub catchment variability of the snow process in 5 elevation zones valéry et al 2014 cemaneige can be coupled with any rainfall runoff model to simulate snowpack and snowmelt at the catchment scale the input data of cemaneige include areal average precipitation and air temperature the cemaneige gr4j model was applied in cold winter catchments where the temperature was lower than 0 c on more than 15 of days the gr4j model was applied in other catchments we used the airgr r package to implement the model coron et al 2017 for each catchment we applied the model in 1960 2001 and calibrated the parameters using all days with available runoff daily in those 42 years the cemaneige gr4j model provides the time of concentration c states of soil moisture s and daily snowmelt m for the data driven hydrological model in the next step the time of concentration c is the travel time of rainwater from the most hydraulically distant point of the catchment to the outlet grimaldi et al 2012 since a unit hydrograph represents the process when all surface runoff is routed in the catchment the variable c can be approximated by the length of the unit hydrograph in gr4j i e 2 x 4 where x 4 is the time parameter in cemaneige gr4j the states of soil moisture are the daily production storage levels s output by the cemaneige gr4j model 3 2 data driven hydrological model to build a data driven hydrological model for each catchment the runoff value r on each day was regarded as the target value the model input should consider the precipitation and snowmelt in cold winter catchments that contributed to the runoff on the target day therefore we traced the input back to the time of concentration c all daily precipitation and snowmelt in cold winter catchments from the 0th to the c th day before the target day were assigned to the model input the antecedent wetness of the catchment included the states of baseflow and soil moisture van steenbergen and willems 2013 baseflow bf was calculated with the recursive digital filter developed by lyne and hollick 1979 soil moisture was represented by the production storage level s of the cemaneige gr4j model since daily bf and s were highly correlated principal component analysis pca was used to combine the two variables into one wetness indicator w by extracting the first principal component the w on the c 1th day before the target day i e w c 1 was considered the antecedent wetness of the target runoff as shown in fig 3 the data driven model can be expressed as 1 r f p 0 p 1 p c m 0 m 1 m c w c 1 p e t sum where r is the target runoff p 0 p 1 p c are daily liquid precipitation on the 0th 1st c th day before the target day m 0 m 1 m c are the daily snowmelt on the 0th 1st c th day before the target day w c 1 is the antecedent wetness on the c 1th day before the target day pet mean is the mean potential evapotranspiration on the 0th c th days before the target day and f is the model notably snowmelt is considered only in catchments where the temperature is lower than 0 c on more than 15 of days in this study we used the bayesian regularized neural networks brnn as f brnn is a one layer artificial neural network adopting bayesian regularization in the feed forward training process foresee and hagan 1997 all daily data samples in eq 1 were divided into training validation and test sets with a proportion of 60 20 20 respectively the validation set was used for tuning the hyperparameters of the model and the test set was used for assessing model performance the tool to implement brnn is the caret r package kuhn et al 2015 3 3 shapley value for quantifying driver contributions originating from the n person game theory the shapley value shapley 1953 depicts the average change in the rewards across all possible coalitions of players who are already playing when a new player joins them the shapley value is now a popular concept in prediction problems using interpretable machine learning štrumbelj and kononenko 2014 in runoff predictions the shapley value of one driver value is its average marginal contribution to one flood peak across all possible coalitions of other driver values the mathematical details of the shapley value are described as follows štrumbelj and kononenko 2014 a prediction model can be written as y f x 1 x 2 x p f x 2 suppose x 1 x p is one of the input samples and s x 1 x p is a coalition of driver values we denote v s as the predicted deviation from the mean marginalized over drivers that are not included in set s v s f x s x s x s d f x s e x f x 3 where x s x i x i s x s x i x i s s x 1 x p s f is the cumulative distribution function and e x f x is the expected value of f x for each data sample the contribution of the driver value x j under coalition s is δ v s x j v s x j v s 4 the average contribution of the driver value x j over all coalitions with the same size s n can be expressed as δ v s x j s n s n s x 1 x p x j δ v s x j c p 1 n s n s x 1 x p x j v s x j v s c p 1 n 5 the average contribution of the driver value x j over all coalitions of all sizes i e the shapley value is expressed as ϕ x j 1 p n 0 p 1 δ v s x j s n 1 p n 0 p 1 s n s x 1 x p x j v s x j v s c p 1 n s x 1 x p x j s p s 1 p v s x j v s 6 it can be proven that all driver contributions add up to the predicted deviation from the mean štrumbelj and kononenko 2014 which means that the prediction can be partitioned into the independent contributions of all driver values j 1 p ϕ x j f x 1 x 2 x p e x f x 7 shapley values are implemented in the iml r package molnar et al 2018 an approximation algorithm štrumbelj and kononenko 2014 is used to reduce computational complexity the algorithm requires the independence of all drivers we examined the correlations among p 0 p 1 p c m 0 m 1 m c w c 1 pet mean and found that only m 0 m 1 m c had interdependence in a few cold winter catchments we performed principal component analysis pca to convert m 0 m 1 m c into independent variables in those catchments for convenience in the following parts of the paper we still use m 0 m 1 m c to represent the snowmelt variables after the pca conversion 3 4 flood type classification based on driver contributions we sampled daily flood peaks from observed streamflow for each catchment using the peaks over threshold pot method lang et al 1999 the threshold was set to ensure that three flood peaks per year were selected on average the independence of the flood peaks was guaranteed according to uswrc 1976 θ 5 d a y s log a r min 3 4 min r 1 r 2 8 where θ is the time interval between two threshold excess peaks r 1 and r 2 measured in days r m i n is the minimum runoff between r 1 and r 2 and a is the catchment area for each flood peak in a catchment the corresponding input drivers i e p 0 p 1 p c m 0 m 1 m c w c 1 of the data driven model were also extracted pet mean was not considered a flood driver we then calculated the shapley value for each driver value of the flood peak i e ϕ p 0 ϕ p 1 ϕ p c ϕ m 0 ϕ m 1 ϕ m c ϕ w c 1 the flood type can be determined by the classification tree shown in fig 4 the general idea of flood classification is to relate the drivers with maximum shapley values to five flood types long rain flood lr rain on snow flood ros snowmelt flood sm short rain flood sr and antecedent wet flood wet let ϕ m sum 0 c ϕ m i and ϕ p sum 0 c ϕ p i when the largest shapley values are ϕ p i ϕ p sum ϕ m sum and ϕ w c 1 the flood types are sr lr sm and wet respectively however there are two exceptional criteria 1 if ϕ p sum and ϕ m sum are comparable i e ϕ m sum t ros ϕ p sum when ϕ p sum is the largest shapley value or ϕ p sum t ros ϕ m sum when ϕ m sum is the largest shapley value the flood will be classified as ros t ros was set to 1 2 2 when ϕ p i contributes more than t r 2 3 of ϕ p sum even if ϕ p sum is the largest shapley value among all drivers the flood will still be classified as sr the first criterion considers the joint effect of rainfall and snowmelt on generating floods in the melting season the second criterion distinguishes the roles of short intense rain and long persistent rain by assessing whether a majority contribution of total rainfall to floods comes from one day rainfall this classification approach is standardized and requires no structure or threshold modification in different catchments t ros and t r in the classification tree are the only manually set parameters of the qdc fc method therefore we conducted a sensitivity test of those two parameters using a variance based approach saltelli et al 2010 for each catchment floods were classified with 100 parameter samples generated by latin hypercube sampling over a uniform distribution the ranges of parameters were t ros 1 2 0 1 and t r 2 3 0 1 for each flood type the standard deviation of its ratio in total floods was derived to assess the sensitivity the sensitivity of the wet ratio was not tested since it was not affected by t ros and t r 4 results 4 1 model performance fig 5 shows the performance of the brnn model in 411 catchments the nse values of daily runoff simulation in the test set evaluate the ability of the model to capture the rainfall runoff relationship the r 2 values of the flood peak simulation evaluate the ability of the model to reproduce the flood peaks pot samples we divide the emr into three regions northeastern china nec 42 54 n 116 135 e northern china nc 33 5 42 n 103 123 e and southern china sc 21 33 5 n 103 123 e all catchments are assigned to those three regions according to their outlet locations the numbers of catchments in nec nc and sc are 79 72 and 260 respectively the median nse values are 0 72 0 59 and 0 80 for catchments in nec nc and sc respectively indicating better performance in the south however the median r 2 values are 0 62 0 70 and 0 66 for catchments in nec nc and sc respectively indicating similar performance in flood peak simulation for the three regions in general the brnn model has good performance in simulating both normal and extreme flow conditions with 225 catchments 34 in nec 32 in nc and 159 in sc having nse 0 6 and r 2 0 6 in the following parts of the paper we only include those 225 catchments for flood type classification 4 2 ratios of different flood types the ratios of different flood types for 225 catchments are presented in fig 6 short rain is the most common driver of floods across the emr since sr is the most frequent type of flood in 143 catchments long rain is the second predominant driver of floods since lr is the most frequent type of flood in 69 catchments floods in the whole emr are dominated by rainfall antecedent wetness plays a role in triggering floods in nec and nc where the ratio of wet exceeds 20 in 26 out of 66 catchments snowmelt causes a small number of floods even in nec the coldest part of china only one catchment has an sm ratio larger than 20 in the sensitivity test of classification parameters t ros and t r the maximum and median values of standard deviation are 18 2 and 8 7 respectively for both sr and lr ratios in 225 catchments in 48 catchments with snow processes the maximum and median values of the standard deviation are 1 9 and 0 respectively for the ros ratio and 1 0 and 0 respectively for sm ratio therefore the sr and lr ratios are far more sensitive to classification parameters than those of ros and sm although with relatively high standard deviations the sensitivity of the ratios of sr and lr lies within an acceptable range in the following parts of the paper we keep t ros 1 2 and t r 2 3 for analysis the ratios of different flood types are controlled by catchment properties such as climate topography and subsurface physical characteristics according to fig 7 a the larger the catchment area is the larger smaller the proportion of lr sr in the sum of lr and sr specifically the proportion of lr in the sum of lr and sr linearly correlates with the log value of catchment areas with a r 2 value of 0 59 this relationship results from the longer response time of runoff to precipitation in larger catchments where flood peaks are caused by consecutive precipitation in several days according to fig 7 b and c the ratio of wet has positive correlations with bfi and k sat in catchments where a considerable amount of total runoff comes from baseflow and a large amount of precipitation infiltrates to be soil moisture floods are significantly affected by antecedent wetness conditions 4 3 extreme flood types the flood peaks larger than a 10 year return period are defined as extreme floods the return periods are calculated by the empirical quantiles proposed by gringorten 1963 we explore the number of catchments with different extreme flood types in fig 8 almost all catchments 213 out of 225 have extreme floods caused by only long rain or short rain or a combination of them wet is one of the extreme flood types in only 11 catchments these results indicate that in the emr with increasing flood magnitudes the contribution of antecedent wetness decreases this phenomenon is also found in other regions of the world such as australia bennett et al 2018 wasko and nathan 2019 finally only 1 catchment has snowmelt induced extreme floods combining the results of the frequent flood types and the extreme flood types we conclude that rainfall is the first order control of floods in terms of both frequency and magnitude in the emr 4 4 intra annual variations in flood types fig 9 shows the main flood type in each month for each catchment i e the intra annual variations in flood types the monthly main flood type is the most frequent type in a month fig 10 summarizes the monthly occurring frequency of each flood type in each region the monthly occurring frequency is the ratio between the number of floods of a specific type in a month and the total number of floods from all catchments in the region in general floods occur from the south to the north with the movement of the easm which lands in april and leaves in october in nec the flood frequencies are larger than 10 from may to august local extratropical systems cause an earlier onset of the flood season compared with that of nc sm in april accounts for 5 of total floods in all months although sm is the main flood type in april april is not a flood frequent month most floods occur in july and august the rainfall brought by the northward propagation of the easm is the predominant driver of floods yang et al 2019 however rainfall is not the only important driver because 20 of floods are wet from may to august in nc floods mainly occur from july to september with frequencies of 31 30 and 15 similar to nec storms brought by the northward easm create flood prone conditions in summer yang et al 2019 the function of catchment wetness cannot be ignored since wet accounts for 23 of all floods in those three months for both nc and nec the highest frequencies of lr and sr occur in july while the frequencies of wet keep increasing and reach the top in august therefore northern china experiences a transition from rainfall induced floods to antecedent wet floods in late summer with cumulative catchment wetness in sc the flood season begins in april for each month from april to august the flood frequencies stay above 11 almost all the floods are lr or sr which are caused by the heavy rainfall with different durations brought by the easm after august the flood frequency drops because during this time only two small regions in sc experience floods the coastal areas affected by tropical cyclones and the northern yangtze basin affected by the weakening retreating easm yang et al 2013 the frequency of wet never exceeds 1 in any month indicating a weak influence of catchment wetness 5 discussion 5 1 strengths and limitations under the hydrological perspective tarasova et al 2019 previous studies classified floods according to the hydrological drivers during flood events such as rainfall intensity rainfall duration antecedent soil moisture flood timing and snowmelt the seasonality based approaches matched the seasonality of floods and drivers to classify floods berghuijs et al 2016 blöschl et al 2017 ye et al 2017 berghuijs et al 2019 do et al 2019 the criteria based approaches defined flood types by setting criteria from either subjective expert knowledge merz and blöschl 2003 nied et al 2014 sikorska et al 2015 stein et al 2019 or objective clustering algorithms turkington et al 2016 keller et al 2018 all these approaches have the risk of confusing hydrological drivers during floods with causative drivers for the following reasons in seasonality based approaches the similar timing of floods and hydrological drivers provides little information about whether the hydrological drivers occur before or after the floods causative drivers should occur slightly earlier than flood peaks in the criteria based approaches in catchments where floods are not sensitive to some drivers the causative driver cannot be determined only by comparing the values of drivers without considering their contributions to flood peaks compared with the seasonality based approaches the qdc fc method classifies floods for individual events and only considers drivers before the flood peaks which prevents the false causative relationship between drivers and floods compared with the criteria based approaches the qdc fc method addresses the problems in two situations when the criteria based approaches give unsatisfying results in finding the causative drivers of floods 1 when more than one driver is high the qdc fc method can determine the driver that contributes the most to the flood peak in fig 11 a both the maximum 1 day precipitation p 1 85 87 and total precipitation p sum 128 88 are high in one flood event the criteria based approaches may link this flood to the type driven by long persistent rainfall in the qdc fc although p 0 43 01 is approximately 1 2 of p 1 85 87 ϕ p 0 3 92 is far less than the 1 2 of ϕ p 1 15 88 therefore ϕ p 1 accounts for the largest portion of ϕ p sum which makes the flood belong to sr 2 when both precipitation and antecedent wetness are extreme the qdc fc method makes it possible to compare their effects on floods in fig 11 b both the total precipitation p sum 217 54 mm and the antecedent wetness w c 1 3 42 which equals the 99 quantile of w c 1 are high since precipitation and antecedent wetness affect floods through different processes the main driver is unclear using criteria based approaches the qdc fc method answers how many portions of the flood peak magnitude can be attributed to p sum and w c 1 therefore the qdc fc method builds a unified standard to compare the contributions of all kinds of hydrological drivers during floods the limitations of the study are as follows 1 the applicability of the qdc fc method highly relies on the accurate estimation of driver values uncertainties exist when snowmelt and soil moisture are simulated by a conceptual hydrological model furthermore since the qdc fc method uses the data driven hydrological model in a lumped way we neglect the spatial variability of hydrological drivers 2 the qdc fc method requires continuous streamflow data for training a data driven hydrological model in previous methods only the values of flood peaks and the dates of peaks are required 3 without physical restrictions the data driven model embedded in the qdc fc method cannot simulate runoff under nonstationary conditions e g vegetation and land use changes therefore the method cannot be used to understand future changes in flood generation mechanisms so far 5 2 flood frequency analysis under multiple flood generation mechanisms in this study we classified all floods into five types for each catchment in the emr floods from various types may have different magnitudes and thus form different frequency distributions tarasova et al 2019 when one type of flood dominates the high extreme samples the extrapolation of the flood frequency is highly related to that type however the flood frequency distribution is often shaped by a large number of floods from other types that are less relevant to the high extremes klemeš 2000 alila and mtiraoui 2002 fischer et al 2016 consequently the extrapolation suffers from bias we found that the most frequent type did not match the type of maximum peak in 83 out of 225 catchments therefore in those catchments the upper tails of flood frequency distributions may be related to the infrequent generation mechanism this phenomenon also occurs in other regions such as the us where the most severe floods are commonly caused by a rare generating mechanism orographic thunderstorms smith et al 2018 in addition tropical cyclones control the upper tail of flood distributions in the eastern us since they are highly related to the shape parameter of the flood frequency distributions villarini and smith 2010 our study provides a method to distinguish flood types so that flood frequency distributions can be estimated with information about multiple flood populations future studies can focus on deriving compound distributions combining all flood types fischer et al 2016 de niel et al 2017 5 3 interpreting flood trends under climate change in this study we calculate the ratios of different flood types in 225 catchments across the whole emr and present the regional patterns of important flood types in nec nc and sc these patterns can aid in the interpretations of flood trends under climate change yang et al 2019 explored the monotonic trends of annual maximum daily streamflow in thousands of catchments in china many northern catchments experienced decreasing trends probably caused by the decreasing trends of annual maximum daily rainfall yang et al 2013 yang et al 2019 the annual maximum daily streamflow showed increasing trends in a small number of southern catchments yang et al 2019 while the annual maximum daily rainfall was increasing in a large part of southern china yang et al 2013 highlighting the roles of catchment wetness and space time rainfall properties in floods yang et al 2019 additional interpretations of the flood trends can be gained from our study for northern china abnormally high antecedent wetness causes a large proportion of floods in nc and nec therefore decreasing annual maximum daily rainfall is not the only driver of decreasing flood magnitudes considering the soil conservation practices in nc bai et al 2016 the role of antecedent wetness may become increasingly important in producing floods for southern china the contribution of long rain to floods may be the reason for the disconnection of the trends between annual maximum daily streamflow and annual maximum daily rainfall in sc the ratios of lr are high among all flood types in many catchments therefore the trends of floods should be linked to the changes in long rain characteristics in those catchments the interpretation of flood trends requires prior knowledge and assumptions about the possible drivers based on the regional patterns of important flood types produced in this study we can have more choices for the possible drivers that change floods 6 conclusions this study reveals the flood generation mechanisms in the eastern monsoon region emr of china we developed a new method the quantification of driver contributions for flood classification qdc fc to attribute floods to causative drivers by quantifying the contributions of drivers to flood peaks the method was applied in 225 catchments across china for each catchment all floods were classified into five types long rain flood lr 1 day rain on snow flood ros snowmelt flood sm short rain flood sr 1 day and antecedent wet flood wet we explored the ratios of the different flood types the physical controls of flood type ratios the extreme flood types and the intra annual variations in flood types in three regions of the emr northeastern china nec northern china nc and southern china sc the major findings are listed as follows 1 the data driven hydrological model embedded in the qdc fc produces good simulations of daily runoff nse greater than 0 6 and flood peaks r 2 0 6 in 225 out of 411 catchments 2 rainfall is the most predominant driver of floods sr and lr are the most frequent types in 143 and 69 catchments respectively moreover sr and lr are the only types for all floods larger than a 10 year return period in 213 out of 225 catchments the flood seasons correspond with the periods when sr and lr are frequent in nec nc and sc 3 antecedent wetness plays a role in triggering floods in nec and nc more than 20 of floods are wet in 26 out of 66 catchments in nec and nc a transition from rainfall induced floods to antecedent wet floods occurs during summer in northern china 4 snowmelt is the least important driver even in nec the coldest region only one catchment has an sm ratio larger than 0 2 even in the month when the sm frequency is at its peak i e april sm accounts for only 5 of the total floods in all months in nec 5 catchment areas control the relative dominance of sr and lr the baseflow index and hydraulic conductivity are possible controls of the ratio of wet the study further shows the added value of the flood types in flood frequency analysis and flood trend interpretation in china in the context of climate change the qdc fc may aid in the accurate estimations of future flood hazards by attributing floods to the causative drivers credit authorship contribution statement wencong yang conceptualization methodology software visualization writing original draft hanbo yang conceptualization supervision writing review editing funding acquisition dawen yang data curation funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partially supported by funding from the national natural science foundation of china grant nos 51622903 and 41661144031 the national program for support of top notch young professionals and the program from the state key laboratory of hydro science and engineering of china grant no 2017 ky 01 the streamflow data are obtained from the water resources information center of the ministry of water resources in china http www mwr gov cn english due to a license issue the dataset is not currently available for public access 
5529,understanding the generation mechanisms of floods is important to estimating future flood hazards by improving flood frequency analysis and flood trend interpretation from the hydrological perspective of flood classification flood types are determined by the seasonality or magnitudes of hydrological drivers e g rainfall snowmelt catchment wetness few studies attribute floods to the causative drivers by quantifying the contributions of each driver to the flood peaks therefore we propose the quantification of driver contributions for flood classification qdc fc method which combines a data driven hydrological model the shapley value which quantifies driver contributions to flood peaks and a classification tree for dividing floods into five types long rain floods 1 day rain on snow floods snowmelt floods short rain floods 1 day and antecedent wet floods in the eastern monsoon region emr of china the method is applied in 225 catchments where the data driven model has high nash sutcliffe efficiency nse 0 6 in daily runoff simulation and a high r squared value r 2 0 6 in flood peak simulation under the impact of the eastern monsoon short rain floods and long rain floods are the most frequent types in 143 and 69 catchments respectively in 213 catchments all extreme floods with return periods larger than 10 years are caused by short rain or long rain antecedent wetness causes more than 20 of floods in 26 out of 66 catchments in northeastern china and northern china with cumulative catchment wetness in summer although snowmelt induced floods occur in april in some catchments of northeastern china only one catchment has more than 20 of floods caused by snowmelt as a new method for flood classification the qdc fc sheds light on the mixed generation mechanisms of floods and possible drivers of flood trends in the emr of china keywords china data driven model flood generation mechanisms flood type interpretable machine learning shapley value 1 introduction river flooding causes severe damage to human society climate change may further exacerbate flood hazards according to future projections by hydrological models flood magnitudes and frequencies will increase significantly in many regions of the world dankers et al 2014 li et al 2016 asadieh and krakauer 2017 thober et al 2018 a better understanding of flood generation mechanisms facilitates accurate estimations of future flood hazards in two aspects first with knowledge about flood generation mechanisms the historical trends of floods can be attributed to the changing characteristics of causative drivers previous studies detect the various trends of flood magnitudes and frequencies based on long term streamflow observations at a global or continental scale archfield et al 2016 berghuijs et al 2017 do et al 2017 blöschl et al 2019 the climate driven trends of floods correlate with the changing magnitudes of hydrological drivers such as extreme rainfall slater and villarini 2016 snowmelt burn and whitfield 2017 blöschl et al 2019 and soil moisture blöschl et al 2019 wasko and nathan 2019 knowing flood generation mechanisms helps extract the predominant driver from an interaction of all those hydrological drivers second considering flood generation mechanisms improves flood frequency analysis mixing floods with all generation mechanisms may violate the assumption that all samples have an identical distribution in estimating frequency distributions gumbel 1941 tarasova et al 2019 adding information about flood generation processes helps discover the controls of the upper tail and lower tail of flood frequency curves alila and mtiraoui 2002 villarini and smith 2010 as reviewed by tarasova et al 2019 several studies determined flood generation mechanisms by classifying floods from three perspectives hydroclimatic hydrological and hydrograph based in this study we only adopt the hydrological perspective of flood classification i e classifying floods based on hydrometeorological input e g rainfall temperature and catchment state e g snow cover local wetness conditions floods can be classified by two categories of approaches from a hydrological perspective the first category of approaches determines predominant generation mechanisms based on the timing of floods and hydrological drivers these approaches assume that floods are mainly caused by hydrological drivers with similar seasonality specifically if the mean date of peak flow approximates the mean date of peak rainfall snowmelt or soil moisture excess the predominant generation process of floods can be determined correspondingly berghuijs et al 2016 blöschl et al 2017 do et al 2019 along with the mean occurring dates of floods and hydrological drivers some studies also use the consistency of seasonality concentration between floods and hydrological variables ye et al 2017 berghuijs et al 2019 the second category of approaches determines generation mechanisms for each event by setting criteria related to the magnitudes of hydrological drivers in the event in the pioneering work conducted by merz and blöschl 2003 they set regional threshold criteria for several hydrological drivers e g storm duration rainfall depth catchment wetness and proposed five flood types in austria long rain flood short rain flood flash flood rain on snow flood and snowmelt flood this classification scheme was then applied to many other regions by modifying the thresholds nied et al 2014 sikorska et al 2015 stein et al 2019 rather than setting subjective thresholds other studies used clustering algorithms to group flood events automatically and objectively turkington et al 2016 keller et al 2018 however all existing approaches suffer from certain limitations in the first category of approaches i e the seasonality based approaches although the hydrological drivers and floods have similar mean occurrence dates it is unclear whether the hydrological driver occurs before or after the flood for each flood event therefore those hydrological drivers are not necessarily causative drivers in the second category of approaches i e the criteria based approaches flood types are defined according to the combination of hydrological drivers delineated by criteria from either subjective expert knowledge or objective clustering algorithms it is difficult to assess the importance of each driver on flood peaks and subsequently determine the causative drivers for example in catchments where floods are not sensitive to soil moisture the 95 quantile of daily rainfall has far larger effects on flood peaks than the 95 quantile of soil moisture however existing criteria based approaches cannot distinguish such effects furthermore the results are highly dependent on specific catchments impeding the transfer of applications in different regions in summary we need a standardized and transferable method for flood classification that does not confuse hydrological drivers during flood events with causative drivers in addition to the methodology gap mentioned above another gap is the absence of a comprehensive understanding of flood generation mechanisms in china although yang et al 2019 describe the flood seasonality and corresponding meteorological systems e g monsoon tropical cyclones that drive floods across china a large scale investigation of the flood generation mechanisms is still unavailable from a hydrological perspective a large part of china experiences a rainy season led by the movement of the eastern monsoon yang et al 2013 wu et al 2016 it is unclear whether in some cases catchment wetness or snowmelt becomes the main driver of floods the aim of this study is to quantify the contributions of hydrological drivers to flood peaks so that the causative drivers can be extracted and linked to flood types we design the quantification of driver contributions for flood classification qdc fc a novel scheme that combines a data driven hydrological model the shapley value shapley 1953 štrumbelj and kononenko 2014 a concept in game theory that distributes the gains of the game to several players and a classification tree the shapley value is now popular in machine learning since it can interpret black box models by partitioning the contributions of the predictors to the target value for each sample štrumbelj and kononenko 2014 it has been used for infrastructure stormwater management william et al 2017 and transboundary river cooperation li et al 2019 however this concept has not been used in any flood study in this study a large scale analysis of flood generation mechanisms is conducted in hundreds of catchments in the eastern monsoon region emr liu and shi 2018 of china we are concerned with the following understanding of floods 1 the most frequent type of flood 2 the most extreme type of flood and 3 the seasonal distribution of different types of floods the paper is organized as follows section 2 presents the study area and data section 3 introduces the methods including the data driven model the shapley value and the classification tree in section 4 the results unravel the ratios of different flood types extreme flood types and intra annual variations in flood types section 5 discusses the strengths and limitations of the study we also state the implications of the results in flood trend interpretations and flood frequency analysis in section 5 section 6 gives the conclusions of the study 2 data 2 1 study area we select catchments in the eastern monsoon region emr liu and shi 2018 in this study for the following reasons first the emr contains floodplains with frequent and severe floods since it includes several humid major basins in china i e the pearl river basin the southeast basin the huaihe river basin the haihe river basin the songliao river basin the middle lower yangtze river basin and the middle lower yellow river basin see fig 1 second the emr has high flood vulnerability with dense populations large urban areas and vast agricultural lands zhang et al 2016 du et al 2018 du et al 2019 third most floods in the emr have similar large scale meteorological origins since they are mainly affected by the east asian summer monsoon easm kundzewicz et al 2019 2 2 runoff data daily runoff data from 551 streamflow gauge stations in the emr were obtained from the historic hydrological database compiled by the water resources information center of the ministry of water resources in china http www mwr gov cn english the research period was from 1960 to 2001 since most of the data records fell into this time span we only selected mesoscale catchments from 100 k m 2 to 100 000 k m 2 where the subdaily process was not important a quality check was carried out to select catchments with limited impacts from the following data problems 1 measurement errors and inconsistency e g changing instruments or station datum and 2 human impacts including dam construction and land use changes we had little information about the measurements dams and land use conditions in 1960 2001 in all catchments therefore the quality check totally relied on the measured flow data the cemaneige gr4j perrin et al 2003 valéry et al 2014 model was applied to each catchment in 1960 2001 for calibrating parameters with available daily runoff data we assumed that measurement problems or severe human impacts would violate the rainfall runoff relationship and cause low values of the nash sutcliffe efficiency coefficient nse in model calibration therefore only catchments with satisfactory model performance nse 0 5 without a log transformation of data moriasi et al 2007 in daily runoff simulation were included in the study although low nses might result from the insufficient representation in the rainfall runoff processes of the model we did not need to distinguish data and model problems in causing low nses since we only focused on catchments with nse 0 5 the information of cemaneige gr4j is introduced in section 3 1 we used the airgr r package coron et al 2017 which developed tools for running the cemaneige gr4j model and implementing calibration algorithms filtered by catchment areas and the quality check 441 catchments remained to get sufficient flood samples we selected 411 from 441 catchments with 10 valid years of data a valid year was a year when the days with missing data were less than 10 of days from april to october in a station we selected april october because flood seasons occur in these months in china yang et al 2019 in the following sections of the paper we proceed with the data driven hydrological modeling and flood sampling only in the valid years for each catchment 2 3 meteorological data precipitation data were obtained from the china gauge based daily precipitation analysis cgdpa shen and xiong 2016 based on approximately 2400 rainfall gauge stations cgdpa was a daily precipitation product with a 0 25 0 25 resolution from 1960 to 2015 we used the penman monteith equation allen et al 1998 to calculate the daily potential evapotranspiration rates the equation required daily temperature relative humidity wind speed and solar hours which were obtained from the stations in the china meteorological data service center http data cma cn en the numbers of available stations were approximately 800 and 2400 in 1960 1987 and 1988 2001 respectively all in site meteorological data were interpolated into 0 25 0 25 raster maps on a daily scale using the universal kriging method in the gstat r tool pebesma and heuvelink 2016 the elevation data from the srtm 90 m dem jarvis et al 2008 were used as covariates in the universal kriging method we averaged values from all raster grids within the catchment boundary to obtain the areal mean series of precipitation potential evapotranspiration and air temperature on a daily scale 2 4 catchment properties the catchment boundaries were extracted using the srtm 90 m dem jarvis et al 2008 the catchment areas and mean elevations could be calculated consequently the baseflow was extracted by the recursive digital filter developed by lyne and hollick 1979 then the baseflow index bfi was calculated by dividing the baseflow volume by the total runoff volume the saturated hydraulic conductivity k sat was obtained from dai et al 2013 by calculating the weighted sum of k sat in all soil layers down to approximately 1 5 m depth 3 methods the qdc fc method for a catchment includes four steps see fig 2 first the cemaneige gr4j hydrological model is calibrated to provide the time of concentration soil moisture simulation and snowmelt simulation in cold winter catchments second a data driven hydrological model is trained to predict daily runoff using meteorological input within the time of concentration third the shapley value quantifies the contributions of the input drivers to the predicted flood peaks based on the model fourth a classification tree assigns floods to different types according to the contributions of the input drivers the details of these four steps are introduced as follows 3 1 cemaneige gr4j hydrological model gr4j is a conceptual rainfall runoff model with a soil moisture accounting module and a routing module perrin et al 2003 gr4j runs on a daily scale in a lumped way and contains only four parameters the input data of the gr4j include areal average precipitation and potential evapotranspiration on a daily scale the cemaneige module is a two parameter semi distributed snow accounting routine considering the sub catchment variability of the snow process in 5 elevation zones valéry et al 2014 cemaneige can be coupled with any rainfall runoff model to simulate snowpack and snowmelt at the catchment scale the input data of cemaneige include areal average precipitation and air temperature the cemaneige gr4j model was applied in cold winter catchments where the temperature was lower than 0 c on more than 15 of days the gr4j model was applied in other catchments we used the airgr r package to implement the model coron et al 2017 for each catchment we applied the model in 1960 2001 and calibrated the parameters using all days with available runoff daily in those 42 years the cemaneige gr4j model provides the time of concentration c states of soil moisture s and daily snowmelt m for the data driven hydrological model in the next step the time of concentration c is the travel time of rainwater from the most hydraulically distant point of the catchment to the outlet grimaldi et al 2012 since a unit hydrograph represents the process when all surface runoff is routed in the catchment the variable c can be approximated by the length of the unit hydrograph in gr4j i e 2 x 4 where x 4 is the time parameter in cemaneige gr4j the states of soil moisture are the daily production storage levels s output by the cemaneige gr4j model 3 2 data driven hydrological model to build a data driven hydrological model for each catchment the runoff value r on each day was regarded as the target value the model input should consider the precipitation and snowmelt in cold winter catchments that contributed to the runoff on the target day therefore we traced the input back to the time of concentration c all daily precipitation and snowmelt in cold winter catchments from the 0th to the c th day before the target day were assigned to the model input the antecedent wetness of the catchment included the states of baseflow and soil moisture van steenbergen and willems 2013 baseflow bf was calculated with the recursive digital filter developed by lyne and hollick 1979 soil moisture was represented by the production storage level s of the cemaneige gr4j model since daily bf and s were highly correlated principal component analysis pca was used to combine the two variables into one wetness indicator w by extracting the first principal component the w on the c 1th day before the target day i e w c 1 was considered the antecedent wetness of the target runoff as shown in fig 3 the data driven model can be expressed as 1 r f p 0 p 1 p c m 0 m 1 m c w c 1 p e t sum where r is the target runoff p 0 p 1 p c are daily liquid precipitation on the 0th 1st c th day before the target day m 0 m 1 m c are the daily snowmelt on the 0th 1st c th day before the target day w c 1 is the antecedent wetness on the c 1th day before the target day pet mean is the mean potential evapotranspiration on the 0th c th days before the target day and f is the model notably snowmelt is considered only in catchments where the temperature is lower than 0 c on more than 15 of days in this study we used the bayesian regularized neural networks brnn as f brnn is a one layer artificial neural network adopting bayesian regularization in the feed forward training process foresee and hagan 1997 all daily data samples in eq 1 were divided into training validation and test sets with a proportion of 60 20 20 respectively the validation set was used for tuning the hyperparameters of the model and the test set was used for assessing model performance the tool to implement brnn is the caret r package kuhn et al 2015 3 3 shapley value for quantifying driver contributions originating from the n person game theory the shapley value shapley 1953 depicts the average change in the rewards across all possible coalitions of players who are already playing when a new player joins them the shapley value is now a popular concept in prediction problems using interpretable machine learning štrumbelj and kononenko 2014 in runoff predictions the shapley value of one driver value is its average marginal contribution to one flood peak across all possible coalitions of other driver values the mathematical details of the shapley value are described as follows štrumbelj and kononenko 2014 a prediction model can be written as y f x 1 x 2 x p f x 2 suppose x 1 x p is one of the input samples and s x 1 x p is a coalition of driver values we denote v s as the predicted deviation from the mean marginalized over drivers that are not included in set s v s f x s x s x s d f x s e x f x 3 where x s x i x i s x s x i x i s s x 1 x p s f is the cumulative distribution function and e x f x is the expected value of f x for each data sample the contribution of the driver value x j under coalition s is δ v s x j v s x j v s 4 the average contribution of the driver value x j over all coalitions with the same size s n can be expressed as δ v s x j s n s n s x 1 x p x j δ v s x j c p 1 n s n s x 1 x p x j v s x j v s c p 1 n 5 the average contribution of the driver value x j over all coalitions of all sizes i e the shapley value is expressed as ϕ x j 1 p n 0 p 1 δ v s x j s n 1 p n 0 p 1 s n s x 1 x p x j v s x j v s c p 1 n s x 1 x p x j s p s 1 p v s x j v s 6 it can be proven that all driver contributions add up to the predicted deviation from the mean štrumbelj and kononenko 2014 which means that the prediction can be partitioned into the independent contributions of all driver values j 1 p ϕ x j f x 1 x 2 x p e x f x 7 shapley values are implemented in the iml r package molnar et al 2018 an approximation algorithm štrumbelj and kononenko 2014 is used to reduce computational complexity the algorithm requires the independence of all drivers we examined the correlations among p 0 p 1 p c m 0 m 1 m c w c 1 pet mean and found that only m 0 m 1 m c had interdependence in a few cold winter catchments we performed principal component analysis pca to convert m 0 m 1 m c into independent variables in those catchments for convenience in the following parts of the paper we still use m 0 m 1 m c to represent the snowmelt variables after the pca conversion 3 4 flood type classification based on driver contributions we sampled daily flood peaks from observed streamflow for each catchment using the peaks over threshold pot method lang et al 1999 the threshold was set to ensure that three flood peaks per year were selected on average the independence of the flood peaks was guaranteed according to uswrc 1976 θ 5 d a y s log a r min 3 4 min r 1 r 2 8 where θ is the time interval between two threshold excess peaks r 1 and r 2 measured in days r m i n is the minimum runoff between r 1 and r 2 and a is the catchment area for each flood peak in a catchment the corresponding input drivers i e p 0 p 1 p c m 0 m 1 m c w c 1 of the data driven model were also extracted pet mean was not considered a flood driver we then calculated the shapley value for each driver value of the flood peak i e ϕ p 0 ϕ p 1 ϕ p c ϕ m 0 ϕ m 1 ϕ m c ϕ w c 1 the flood type can be determined by the classification tree shown in fig 4 the general idea of flood classification is to relate the drivers with maximum shapley values to five flood types long rain flood lr rain on snow flood ros snowmelt flood sm short rain flood sr and antecedent wet flood wet let ϕ m sum 0 c ϕ m i and ϕ p sum 0 c ϕ p i when the largest shapley values are ϕ p i ϕ p sum ϕ m sum and ϕ w c 1 the flood types are sr lr sm and wet respectively however there are two exceptional criteria 1 if ϕ p sum and ϕ m sum are comparable i e ϕ m sum t ros ϕ p sum when ϕ p sum is the largest shapley value or ϕ p sum t ros ϕ m sum when ϕ m sum is the largest shapley value the flood will be classified as ros t ros was set to 1 2 2 when ϕ p i contributes more than t r 2 3 of ϕ p sum even if ϕ p sum is the largest shapley value among all drivers the flood will still be classified as sr the first criterion considers the joint effect of rainfall and snowmelt on generating floods in the melting season the second criterion distinguishes the roles of short intense rain and long persistent rain by assessing whether a majority contribution of total rainfall to floods comes from one day rainfall this classification approach is standardized and requires no structure or threshold modification in different catchments t ros and t r in the classification tree are the only manually set parameters of the qdc fc method therefore we conducted a sensitivity test of those two parameters using a variance based approach saltelli et al 2010 for each catchment floods were classified with 100 parameter samples generated by latin hypercube sampling over a uniform distribution the ranges of parameters were t ros 1 2 0 1 and t r 2 3 0 1 for each flood type the standard deviation of its ratio in total floods was derived to assess the sensitivity the sensitivity of the wet ratio was not tested since it was not affected by t ros and t r 4 results 4 1 model performance fig 5 shows the performance of the brnn model in 411 catchments the nse values of daily runoff simulation in the test set evaluate the ability of the model to capture the rainfall runoff relationship the r 2 values of the flood peak simulation evaluate the ability of the model to reproduce the flood peaks pot samples we divide the emr into three regions northeastern china nec 42 54 n 116 135 e northern china nc 33 5 42 n 103 123 e and southern china sc 21 33 5 n 103 123 e all catchments are assigned to those three regions according to their outlet locations the numbers of catchments in nec nc and sc are 79 72 and 260 respectively the median nse values are 0 72 0 59 and 0 80 for catchments in nec nc and sc respectively indicating better performance in the south however the median r 2 values are 0 62 0 70 and 0 66 for catchments in nec nc and sc respectively indicating similar performance in flood peak simulation for the three regions in general the brnn model has good performance in simulating both normal and extreme flow conditions with 225 catchments 34 in nec 32 in nc and 159 in sc having nse 0 6 and r 2 0 6 in the following parts of the paper we only include those 225 catchments for flood type classification 4 2 ratios of different flood types the ratios of different flood types for 225 catchments are presented in fig 6 short rain is the most common driver of floods across the emr since sr is the most frequent type of flood in 143 catchments long rain is the second predominant driver of floods since lr is the most frequent type of flood in 69 catchments floods in the whole emr are dominated by rainfall antecedent wetness plays a role in triggering floods in nec and nc where the ratio of wet exceeds 20 in 26 out of 66 catchments snowmelt causes a small number of floods even in nec the coldest part of china only one catchment has an sm ratio larger than 20 in the sensitivity test of classification parameters t ros and t r the maximum and median values of standard deviation are 18 2 and 8 7 respectively for both sr and lr ratios in 225 catchments in 48 catchments with snow processes the maximum and median values of the standard deviation are 1 9 and 0 respectively for the ros ratio and 1 0 and 0 respectively for sm ratio therefore the sr and lr ratios are far more sensitive to classification parameters than those of ros and sm although with relatively high standard deviations the sensitivity of the ratios of sr and lr lies within an acceptable range in the following parts of the paper we keep t ros 1 2 and t r 2 3 for analysis the ratios of different flood types are controlled by catchment properties such as climate topography and subsurface physical characteristics according to fig 7 a the larger the catchment area is the larger smaller the proportion of lr sr in the sum of lr and sr specifically the proportion of lr in the sum of lr and sr linearly correlates with the log value of catchment areas with a r 2 value of 0 59 this relationship results from the longer response time of runoff to precipitation in larger catchments where flood peaks are caused by consecutive precipitation in several days according to fig 7 b and c the ratio of wet has positive correlations with bfi and k sat in catchments where a considerable amount of total runoff comes from baseflow and a large amount of precipitation infiltrates to be soil moisture floods are significantly affected by antecedent wetness conditions 4 3 extreme flood types the flood peaks larger than a 10 year return period are defined as extreme floods the return periods are calculated by the empirical quantiles proposed by gringorten 1963 we explore the number of catchments with different extreme flood types in fig 8 almost all catchments 213 out of 225 have extreme floods caused by only long rain or short rain or a combination of them wet is one of the extreme flood types in only 11 catchments these results indicate that in the emr with increasing flood magnitudes the contribution of antecedent wetness decreases this phenomenon is also found in other regions of the world such as australia bennett et al 2018 wasko and nathan 2019 finally only 1 catchment has snowmelt induced extreme floods combining the results of the frequent flood types and the extreme flood types we conclude that rainfall is the first order control of floods in terms of both frequency and magnitude in the emr 4 4 intra annual variations in flood types fig 9 shows the main flood type in each month for each catchment i e the intra annual variations in flood types the monthly main flood type is the most frequent type in a month fig 10 summarizes the monthly occurring frequency of each flood type in each region the monthly occurring frequency is the ratio between the number of floods of a specific type in a month and the total number of floods from all catchments in the region in general floods occur from the south to the north with the movement of the easm which lands in april and leaves in october in nec the flood frequencies are larger than 10 from may to august local extratropical systems cause an earlier onset of the flood season compared with that of nc sm in april accounts for 5 of total floods in all months although sm is the main flood type in april april is not a flood frequent month most floods occur in july and august the rainfall brought by the northward propagation of the easm is the predominant driver of floods yang et al 2019 however rainfall is not the only important driver because 20 of floods are wet from may to august in nc floods mainly occur from july to september with frequencies of 31 30 and 15 similar to nec storms brought by the northward easm create flood prone conditions in summer yang et al 2019 the function of catchment wetness cannot be ignored since wet accounts for 23 of all floods in those three months for both nc and nec the highest frequencies of lr and sr occur in july while the frequencies of wet keep increasing and reach the top in august therefore northern china experiences a transition from rainfall induced floods to antecedent wet floods in late summer with cumulative catchment wetness in sc the flood season begins in april for each month from april to august the flood frequencies stay above 11 almost all the floods are lr or sr which are caused by the heavy rainfall with different durations brought by the easm after august the flood frequency drops because during this time only two small regions in sc experience floods the coastal areas affected by tropical cyclones and the northern yangtze basin affected by the weakening retreating easm yang et al 2013 the frequency of wet never exceeds 1 in any month indicating a weak influence of catchment wetness 5 discussion 5 1 strengths and limitations under the hydrological perspective tarasova et al 2019 previous studies classified floods according to the hydrological drivers during flood events such as rainfall intensity rainfall duration antecedent soil moisture flood timing and snowmelt the seasonality based approaches matched the seasonality of floods and drivers to classify floods berghuijs et al 2016 blöschl et al 2017 ye et al 2017 berghuijs et al 2019 do et al 2019 the criteria based approaches defined flood types by setting criteria from either subjective expert knowledge merz and blöschl 2003 nied et al 2014 sikorska et al 2015 stein et al 2019 or objective clustering algorithms turkington et al 2016 keller et al 2018 all these approaches have the risk of confusing hydrological drivers during floods with causative drivers for the following reasons in seasonality based approaches the similar timing of floods and hydrological drivers provides little information about whether the hydrological drivers occur before or after the floods causative drivers should occur slightly earlier than flood peaks in the criteria based approaches in catchments where floods are not sensitive to some drivers the causative driver cannot be determined only by comparing the values of drivers without considering their contributions to flood peaks compared with the seasonality based approaches the qdc fc method classifies floods for individual events and only considers drivers before the flood peaks which prevents the false causative relationship between drivers and floods compared with the criteria based approaches the qdc fc method addresses the problems in two situations when the criteria based approaches give unsatisfying results in finding the causative drivers of floods 1 when more than one driver is high the qdc fc method can determine the driver that contributes the most to the flood peak in fig 11 a both the maximum 1 day precipitation p 1 85 87 and total precipitation p sum 128 88 are high in one flood event the criteria based approaches may link this flood to the type driven by long persistent rainfall in the qdc fc although p 0 43 01 is approximately 1 2 of p 1 85 87 ϕ p 0 3 92 is far less than the 1 2 of ϕ p 1 15 88 therefore ϕ p 1 accounts for the largest portion of ϕ p sum which makes the flood belong to sr 2 when both precipitation and antecedent wetness are extreme the qdc fc method makes it possible to compare their effects on floods in fig 11 b both the total precipitation p sum 217 54 mm and the antecedent wetness w c 1 3 42 which equals the 99 quantile of w c 1 are high since precipitation and antecedent wetness affect floods through different processes the main driver is unclear using criteria based approaches the qdc fc method answers how many portions of the flood peak magnitude can be attributed to p sum and w c 1 therefore the qdc fc method builds a unified standard to compare the contributions of all kinds of hydrological drivers during floods the limitations of the study are as follows 1 the applicability of the qdc fc method highly relies on the accurate estimation of driver values uncertainties exist when snowmelt and soil moisture are simulated by a conceptual hydrological model furthermore since the qdc fc method uses the data driven hydrological model in a lumped way we neglect the spatial variability of hydrological drivers 2 the qdc fc method requires continuous streamflow data for training a data driven hydrological model in previous methods only the values of flood peaks and the dates of peaks are required 3 without physical restrictions the data driven model embedded in the qdc fc method cannot simulate runoff under nonstationary conditions e g vegetation and land use changes therefore the method cannot be used to understand future changes in flood generation mechanisms so far 5 2 flood frequency analysis under multiple flood generation mechanisms in this study we classified all floods into five types for each catchment in the emr floods from various types may have different magnitudes and thus form different frequency distributions tarasova et al 2019 when one type of flood dominates the high extreme samples the extrapolation of the flood frequency is highly related to that type however the flood frequency distribution is often shaped by a large number of floods from other types that are less relevant to the high extremes klemeš 2000 alila and mtiraoui 2002 fischer et al 2016 consequently the extrapolation suffers from bias we found that the most frequent type did not match the type of maximum peak in 83 out of 225 catchments therefore in those catchments the upper tails of flood frequency distributions may be related to the infrequent generation mechanism this phenomenon also occurs in other regions such as the us where the most severe floods are commonly caused by a rare generating mechanism orographic thunderstorms smith et al 2018 in addition tropical cyclones control the upper tail of flood distributions in the eastern us since they are highly related to the shape parameter of the flood frequency distributions villarini and smith 2010 our study provides a method to distinguish flood types so that flood frequency distributions can be estimated with information about multiple flood populations future studies can focus on deriving compound distributions combining all flood types fischer et al 2016 de niel et al 2017 5 3 interpreting flood trends under climate change in this study we calculate the ratios of different flood types in 225 catchments across the whole emr and present the regional patterns of important flood types in nec nc and sc these patterns can aid in the interpretations of flood trends under climate change yang et al 2019 explored the monotonic trends of annual maximum daily streamflow in thousands of catchments in china many northern catchments experienced decreasing trends probably caused by the decreasing trends of annual maximum daily rainfall yang et al 2013 yang et al 2019 the annual maximum daily streamflow showed increasing trends in a small number of southern catchments yang et al 2019 while the annual maximum daily rainfall was increasing in a large part of southern china yang et al 2013 highlighting the roles of catchment wetness and space time rainfall properties in floods yang et al 2019 additional interpretations of the flood trends can be gained from our study for northern china abnormally high antecedent wetness causes a large proportion of floods in nc and nec therefore decreasing annual maximum daily rainfall is not the only driver of decreasing flood magnitudes considering the soil conservation practices in nc bai et al 2016 the role of antecedent wetness may become increasingly important in producing floods for southern china the contribution of long rain to floods may be the reason for the disconnection of the trends between annual maximum daily streamflow and annual maximum daily rainfall in sc the ratios of lr are high among all flood types in many catchments therefore the trends of floods should be linked to the changes in long rain characteristics in those catchments the interpretation of flood trends requires prior knowledge and assumptions about the possible drivers based on the regional patterns of important flood types produced in this study we can have more choices for the possible drivers that change floods 6 conclusions this study reveals the flood generation mechanisms in the eastern monsoon region emr of china we developed a new method the quantification of driver contributions for flood classification qdc fc to attribute floods to causative drivers by quantifying the contributions of drivers to flood peaks the method was applied in 225 catchments across china for each catchment all floods were classified into five types long rain flood lr 1 day rain on snow flood ros snowmelt flood sm short rain flood sr 1 day and antecedent wet flood wet we explored the ratios of the different flood types the physical controls of flood type ratios the extreme flood types and the intra annual variations in flood types in three regions of the emr northeastern china nec northern china nc and southern china sc the major findings are listed as follows 1 the data driven hydrological model embedded in the qdc fc produces good simulations of daily runoff nse greater than 0 6 and flood peaks r 2 0 6 in 225 out of 411 catchments 2 rainfall is the most predominant driver of floods sr and lr are the most frequent types in 143 and 69 catchments respectively moreover sr and lr are the only types for all floods larger than a 10 year return period in 213 out of 225 catchments the flood seasons correspond with the periods when sr and lr are frequent in nec nc and sc 3 antecedent wetness plays a role in triggering floods in nec and nc more than 20 of floods are wet in 26 out of 66 catchments in nec and nc a transition from rainfall induced floods to antecedent wet floods occurs during summer in northern china 4 snowmelt is the least important driver even in nec the coldest region only one catchment has an sm ratio larger than 0 2 even in the month when the sm frequency is at its peak i e april sm accounts for only 5 of the total floods in all months in nec 5 catchment areas control the relative dominance of sr and lr the baseflow index and hydraulic conductivity are possible controls of the ratio of wet the study further shows the added value of the flood types in flood frequency analysis and flood trend interpretation in china in the context of climate change the qdc fc may aid in the accurate estimations of future flood hazards by attributing floods to the causative drivers credit authorship contribution statement wencong yang conceptualization methodology software visualization writing original draft hanbo yang conceptualization supervision writing review editing funding acquisition dawen yang data curation funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partially supported by funding from the national natural science foundation of china grant nos 51622903 and 41661144031 the national program for support of top notch young professionals and the program from the state key laboratory of hydro science and engineering of china grant no 2017 ky 01 the streamflow data are obtained from the water resources information center of the ministry of water resources in china http www mwr gov cn english due to a license issue the dataset is not currently available for public access 
