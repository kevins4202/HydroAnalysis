index,text
25570,poor air quality impacts life expectancy and quality of life of populations worldwide numerical models generate comprehensive air pollutant concentration datasets useful for quantifying historic current and through projections future air pollution effects on health however it is challenging to accommodate the full range of dispersion and chemistry processes affecting both regional and local scales in a single model the scientific formulation and evaluation of a new quasi gaussian road source dispersion model adms local is presented this model has been developed as a component for the multi model air quality system for health research maqs health adms local accounts for the influence of urban morphology on dispersion as well as near road nox chemistry processes leading to the spatial resolution of pollutant concentration gradients occurring over distances of a few metres the model is optimised for use within the maqs health system where local modelling for each regional model grid cell is run in parallel graphical abstract image 1 keywords dispersion air quality adms coupled model evaluation development maqs health software availability adms local forms an integral part of the maqs health coupled system the maqs health system will be available to the uk research community for air quality and health via the spf clean air framework platform from 2022 nelson 2021 the maqs health system is designed to run on linux based hpcs alongside chemical transport models adms urban carruthers et al 2000 upon which adms local is based is commercially available 1 introduction each year in the uk around 40 000 deaths are attributable to exposure to outdoor air pollution royal college of physicians 2016 reducing morbidity associated with air quality aq requires an understanding of the relationship between exposure to atmospheric pollutants and health outcomes in the uk the committee on the medical effects of air pollutants comeap 2009 has collated evidence that strongly suggests an association between long term exposure to particulate air pollution and mortality effects research has shown that the time periods over which pollutant exposure is harmful differs for different pollutant species for instance short term hourly and sub hourly exposures to high levels of gaseous pollutants no2 and so2 increase the risk of hospital admissions mills et al 2015 dab et al 1996 whereas longer term exposure to particulate species may exacerbate health conditions such as lung cancer and cardiopulmonary mortality pope et al 2002 as a consequence of improved knowledge of the medical effects of air pollution the world health organisation who has recently revised its air quality guidelines world health organization 2021 most notable are the revised guideline values for annual average concentrations for pm2 5 and no2 of 5 μg m³ and 10 μg m³ respectively these values are currently exceeded in many areas of the world the uk has over 400 reference aq monitoring stations that record concentrations in air of a wide range of air pollutants including nox no2 o3 pm10 pm2 5 and other particulate species usually at hourly temporal resolution significant networks include the uk automatic urban and rural network and separate networks in england scotland wales northern ireland and london measurement stations have been deployed in locations ranging from rural to kerbside sites in cities with additional monitoring in industrialised areas in recent years there has been an increase in the availability of low cost aq sensors which can be deployed as relatively high density networks mead et al 2013 but such equipment is generally less accurate than reference monitors castell et al 2017 also as is the case for reference equipment sensors require calibration and maintenance and data ratification procedures are necessary there is also the increasing prospect of satellite data being of sufficiently high resolution that it may routinely characterize local air quality for example pope et al 2018 made use of ozone monitoring instrument omi data to observe changes in no2 pollution hotspots in the and pope and provod 2016 used omi data to determine tropospheric column no2 levels around three power stations computational models can replicate physical and chemical atmospheric processes and in conjunction with meteorological models they can be used to calculate air pollutant concentrations measured data are often used to improve model predictions for instance to define boundary conditions as well as for calibration and evaluation an important advantage that such models have over measurements is their comprehensive spatial coverage also models can be used to investigate the impact of policy scenarios on air quality and calculate projections of future aq taking into account future emissions and climate estimates modelled air pollutant concentration datasets are used by health researchers in their studies of population exposure and health outcomes mizen et al 2018 gulliver et al 2018 alcock et al 2017 there are a number of well established regional and global chemical transport models ctms that are used to model gaseous and particulate atmospheric pollutant species in the atmosphere usually at hourly temporal resolution and at spatial resolutions of up to 1 km the us environmental protection agency epa community model for air quality cmaq byun and schere 2006 is used internationally for policy and research chemel et al 2014 as well as within forecasting applications che et al 2020 ramboll s camx model environ 2020 is used for similar applications and includes a model that facilitates source apportionment calculations bove et al 2014 the emep model has been developed as part of the co operative programme for monitoring and evaluation of the long range transmission of air pollutants in europe and has been used for a range of policy and health research applications in the uk vieno et al 2014 heal et al 2013 ctms are usually implemented using one way coupling with meteorological models however a two way coupled version of wrf cmaq has been developed wong et al 2012 ncar s wrf chem grell et al 2005 has also been developed to allow for the influence that pollutant concentrations have on meteorological processes and has been used to model air quality in many regions worldwide ansari et al 2019 whilst regional ctms have large spatial coverage their finest spatial resolution does not resolve the high pollutant concentration gradients that occur within the vicinity of specific air pollutant emission sources there are however a number of atmospheric dispersion models that replicate the dispersion of emissions on a source by source basis with spatial resolution of a few metres these models are required not only to take account of source geometries but also to allow for urban morphological features that influence dispersion processes for instance buildings that generate re circulatory flows leading to pollutant accumulation tunnel portals where vehicle emissions from the whole tunnel length can be released from a small area and bridges flyovers where elevated traffic emissions are exposed to higher wind speeds and enhanced dispersion local models must also account for nearfield chemical reactions specifically the conversion of no to no2 that occurs over short temporal and spatial scales in the vicinity of road sources valencia et al 2018 and other sources cambridge environmental research consultants cerc develop the quasi gaussian adms urban local air dispersion model that is used worldwide carruthers et al 2000 for assessment research and policy application the us epa aermod model cimorelli et al 2005 is used for regulatory applications in the us and also internationally aermod includes an open road source dispersion model rline currently non regulatory snyder et al 2013 other models used for urban air quality include sirane soulhac et al 2011 gral oettl 2015 and caline benson 1992 regional and local air quality cannot be considered independently long range pollutant transport strongly influences concentration levels of some pollutants in urban areas for instance particulates and o3 conversely anthropogenic emissions from urban and industrial areas are key drivers in some atmospheric chemical processes that lead to regional air pollution episodes for instance nox and voc emissions influence particulate and o3 concentrations in rural areas chen et al 2020 consequently systems that couple regional and local air quality models are necessary in order to quantify baseline air pollutant concentration for population exposure studies that incorporate the full range of spatial scales such nested models can also be used to assess the likely impact of proposed national and local pollution mitigation strategies coupled air quality modelling systems spanning a range of spatial scales may be configured in different ways silveira et al 2019 multiple modelling scales implemented within a single model with external boundary conditions is a typical approach for many meso scale meteorology and regional chemical transport models such as wrf cmaq and camx the models commonly use one way nesting with boundary and initial conditions feeding from large low resolution domains to smaller and higher resolution domains examples in the literature include chatani et al 2011 which presents results from a nested wrf cmaq configuration and russell et al 2019 which describes the canadian national operational forecast model gem mach however this nesting approach is not appropriate for resolving air pollutant concentrations at street scales finer scale modelling can be achieved by using meso scale meteorological models to drive local cfd models kadaverugu et al 2019 however cfd modelling of urban areas is computationally expensive and compatible only with a limited range of meteorological conditions so this modelling approach is generally applied only to short time periods and very small spatial domains for instance kwak et al 2015 presents results from modelling a domain covering less than 2 km2 for nine hours the most practical approach to accounting for a large range of spatial scales is to use regional meteorological and chemistry transport modelling systems to drive local parameterised aq models the goal of this type of system is to combine the complex large scale gas and aerosol chemistry of regional modelling with high resolution and computationally efficient parameterised modelling of local dispersion especially from road sources in urban areas one general challenge for linking regional with local dispersion modelling is that of double counting local emissions which are also included in the regional modelling the effect of this on the final modelled concentrations will be more significant for finer gridded model resolution its impact has been omitted for some coarser scale regional modelling for example beevers et al 2012 isakov et al 2009 mensink et al 2003 looser coupling methods use regional modelling to substitute for measured rural background concentrations outside an urban area for example kukkonen et al 2016 the development of a multi model air quality system for health research maqs health has been funded as part of the uk government s strategic priorities fund spf clean air programme cap maqs health is a coupled air quality modelling system spanning national to urban street scales accounting for physical and chemical processes at all relevant temporal and spatial scales the system fig 1 links a wide range of regional ctms to a newly developed road source dispersion model adms local which is the topic of this paper maqs health incorporates a verification system to facilitate evaluation of model outputs against measurement datasets maqs health is one of three linked spf cap projects the goal of the project developing a uk community emissions modelling system dukems led by the uk centre for ecology and hydrology ukceh is to deliver a tool able to process and generate emissions data for air quality modelling applications the goal of the project data integration model for exposure modelling dimex uk led by the university of exeter is to develop a modelling framework to integrate ambient and indoor concentrations with human activity to estimate personal exposures to air pollution there are two broad categories of maqs health stakeholders aq modellers within the research community will use the system to generate air pollutant concentration datasets for use in health research and health research and policy makers will use the pollutant concentration datasets for health analysis adms local is a new urban dispersion model derived from the adms urban model the primary purpose of adms local is to calculate the dispersion of traffic emissions at spatial resolutions that resolve the sharp concentration gradients in the vicinity of road sources as well as modelling road sources explicitly the local model is able to calculate the dispersion of gridded emissions representing all other air pollutant sources in the model domain this is necessary because the total pollutant concentrations including contributions from all sources are required in order to explicitly model fast local chemical reactions in addition within the coupled system the local model is used to replicate the local dispersion of the gridded emissions from the regional model so that the resulting concentrations can be subtracted from the regional model contributions to avoid double counting the meteorological data driving dispersion processes in adms local are derived from those calculated by commonly used mesoscale meteorological models such as wrf and the uk met office unified model brown et al 2012 concentration outputs from adms local are sufficiently flexible to allow calculation of predicted hourly pollutant concentrations suitable for use in health research at specific receptor locations for example postcodes and on regularly and irregularly spaced grids to calculate for example spatially averaged concentrations within wards and other geographies adms local has been developed to model the pollutants that relate to current health related air quality metrics for instance no2 pm2 5 pm10 so2 and o3 but is sufficiently flexible that it can be configured to model other pollutant species that may be of interest for future health assessments for instance particulate components such as black carbon and metals adms local like adms urban has been coded in fortran following the fortran 2008 standards supported by most compilers the program proceeds as follows the input data is read in and undergoes initial processing including calculation of atmospheric properties from meteorological data the program then loops over each of the hours being modelled calculating the concentrations at each receptor due to each road source applying time varying emissions factors calculating open road dispersion if canyon effects are required applying parameterised canyon factors to canyon centreline open road concentrations for in canyon receptors calculating canyon top volume source dispersion and combining with weighted open road dispersion for out of canyon receptors calculating the concentrations at each receptor due to each cell of gridded emissions representing non road sources summing the concentrations from all road and grid sources at each receptor adding regional upwind background concentrations then undertaking chemical reactions the concentrations for each hour are written to the output files all code has been peer reviewed and testing has been undertaken to ensure that model outputs are entirely consistent with model specification for a wide range of inputs adms urban has undergone extensive model evaluation owen et al 2000 lao and teixidó 2011 hood et al 2018 valencia et al 2018 biggart et al 2020 zhong et al 2021 adms local shares core scientific formulation with adms urban but has been fully re coded with some improvements to computational efficiency and simplifications to the available user inputs for concentrations dispersed from open road sources and 3d grid sources with identical meteorological and background concentration inputs any differences in the outputs of the two models should arise only from minor differences in the computational algorithms used in the internal calculations therefore for some simple scenarios adms local model performance can be tested by direct comparison with adms urban for more complex model configurations including where the scientific formulations of the models diverge for example in the approach to modelling the effects of street canyons on pollutant dispersion adms local model performance requires additional assessment by comparison with available measurement datasets this paper presents the formulation and evaluation of the adms local model subsequent articles will describe formulation and evaluation of the maqs health coupled system a full technical description of adms local is given in section 2 the results of adms local evaluation are presented in section 3 a discussion and outline of further work is given in section 4 2 adms local model formulation adms local is a gaussian plume dispersion model with the atmospheric boundary layer structure defined by boundary layer depth and monin obukhov length a skewed gaussian distribution is used in convective conditions the model includes a meteorological processor to calculate boundary layer properties from the input meteorological data section 2 1 road sources are modelled as line sources with additional mixing due to vehicle induced turbulence section 2 2 adms local includes a simplified street canyon parameterisation to ensure that the model can be used over large domains without requiring extensive local input datasets aggregate gridded emission sources are modelled using a 3d grid of volume sources section 2 3 for each of these source types concentrations can be calculated at any output point location to ensure the required spatial resolution section 2 5 fast local chemistry can also be applied during the concentration calculations section 2 4 the model has been created to run on linux based hpc systems for compatibility with the other components of the maqs health system model code is written in fortran following the fortran 2008 standards as these are supported by the majority of fortran compilers netcdf libraries have been compiled within adms local to allow for the reading and writing of netcdf files the model outputs a log file containing run information including any warnings and errors issued during operation within the maqs health system adms local model runs are executed separately for each regional model grid cell within the local modelling domain execution of these separate regions on multiple cores and nodes forms the primary parallelisation of the system fig 2 presents an example regional model grid corresponding to an emep model run vieno et al 2016 this figure also shows the major road network modelled explicitly within adms local the regional model grid cells have a dimension of 1 km by 1 km so the corresponding adms local model domain size for each grid cell is 1 km by 1 km plus a buffer region to ensure continuity of modelled concentrations over grid cell boundaries as shown in fig 2 inset 2 1 meteorological data and boundary layer profiles adms local takes as input meteorological data extracted from the regional meteorological model for each grid cell note that meteorological data input is one aspect of adms local which has some differences from adms urban the adms urban meteorological pre processor additionally allows input of wind speed and turbulence at heights other than 10 m and includes algorithms to determine the surface heat flux from surface measurements 2 1 1 meteorological data input the local model uses a plain text file of hourly meteorological data as input which is generated by the meteorological processing utility of the coupled system this ensures that the meteorological variables are consistent with those used by the regional model the variables that may be included in the meteorological file are summarised in table 1 also shown are units where relevant minimum and maximum permitted values the last column indicates which variables are required or optional the local model requires three additional time independent parameters specifically the site latitude decimal degrees an average surface roughness z 0 m and the minimum monin obukhov length m i n l m o m used to represent limitations on atmospheric stability arising from urban mixing and heating 2 1 2 urban morphology data the local model also takes account of the impact of buildings in an urban area on the local airflow through local modifications to the vertical wind speed and turbulence profiles urban canopy flow the building characteristics of the urban area are specified through the following optional parameters average building height within the cell h m average street canyon width within the cell g m ratio of the sum of the plan area occupied by buildings to the total plan area within the cell λ p ratio of total frontal area of buildings perpendicular to wind direction to the total plan area within the cell λ f for a range of different wind sectors and roughness length within the urban canopy z 0 s m 2 1 3 meteorological calculations and boundary layer profiles the adms local meteorological processor routines take the input meteorological data for a given hour along with the additional time independent parameters specified in the previous section and use similarity theory to derive further boundary layer parameters as required for running the dispersion model the meteorological processor routines use many components of the meteorological pre processor used in cerc s other adms models as required to process data from a regional meteorological model a full technical description of the adms meteorological pre processor can be found in technical specification document p05 01 cerc 2021 some key concepts are included here though readers should refer to the referenced document for full details as well as for the original references on which the equations are based key parameters that are calculated by the meteorological processor include friction velocity u convective velocity scale w if the surface sensible heat flux is positive zero otherwise monin obukhov length l m o and boundary layer depth h if not provided as an input parameter an iterative procedure is used to calculate u and l m o assuming a modified logarithmic surface layer wind profile following 1 κ u s u ln z z 0 z 0 ψ z z 0 l m o z 0 l m o where κ 0 4 is the von kármán constant u s z is the wind speed at height z above ground which is known at 10 m from the input meteorological data ψ is a stability dependent function and l m o is defined as 2 l m o u 3 ρ a c p t 0 k κ f θ 0 g where ρ a 1 225 kg m 3 and c p 1012 j kg 1 k 1 are the assumed air density and specific heat capacity at constant pressure respectively t 0 k is the screen height temperature in kelvin and g is gravitational acceleration if the boundary layer depth is not provided directly it is calculated as follows in stable conditions f θ 0 0 it is taken as 3 h 0 6 u ω 1 1 2 28 u ω l m o where ω is the coriolis parameter in unstable conditions f θ 0 0 h is assumed to evolve according to 4a d h d t s δ θ 4b d δ θ d t γ θ s δ θ f θ 0 ρ a c p h s h 4c s c f f θ 0 ρ a c p a u 3 t 0 k g h where t is time δ θ is the potential temperature jump across the boundary layer top γ θ is the rate of increase of potential temperature above the boundary layer and c f 0 2 and a 5 are constants s relates to the magnitude of convective and mechanical turbulence available to drive entrainment into the boundary layer finally w is calculated using 5 w 3 u 3 h κ l m o using the derived boundary layer parameters output by the meteorological processor the local model is able to calculate vertical profiles of mean wind speed turbulence temperature and other quantities throughout the boundary layer the boundary layer structure algorithms used are described in detail in technical specification document p09 01 cerc 2021 some are summarised below the mean wind profile is the same as that used in the meteorological processor equation 1 that is a stability dependent modified logarithmic profile is assumed above h the wind speed is held constant if urban canopy flow parameters are provided the wind profile is implemented in three parts depending on the displacement height d m above the ground 6 d h 1 λ p 1 α λ p where α is a constant with value 4 43 representing a staggered array of buildings the profile above the buildings z 2 d is calculated by an expression similar to equation 1 using z d as the height to displace the upwind profile vertically upwards 7 u a z u b κ ln z d z 0 b ψ z d l m o here u b is the frictional velocity representative of the buildings and z 0 b is the local surface roughness derived from the input parameters near the ground z d the flow within the building canopy is related in magnitude to the upwind flow decreasing logarithmically with decreasing height 8 u c z u s κ ln z z 0 s here u s is the frictional velocity representative of the wind profile within the building canopy the roughness length z 0 s represents the value of extrusions from the roads and pavements in an urban area with a typical value of 10 cm a transition layer exists between the urban canopy flow regimes d z 2 d where the profile is calculated using linear interpolation to allow for a continuous vertical profile the technical specification document p34 01b cerc 2021 contains a full technical description of the urban canopy flow formulation turbulence profiles are calculated using one of three sets of formulae depending on the atmospheric stability namely whether h l m o 0 3 convective 0 3 h l m o 1 neutral or h l m o 1 stable hunt et al 1988 in convective conditions contributions from convectively driven and mechanically driven turbulence are included while in neutral conditions only mechanically driven turbulence contributes in stable conditions the turbulence profiles depend on the friction velocity roughness length and boundary layer depth alone a minimum value is also applied to the turbulence components depending on the specified minimum monin obukhov length if the urban canopy flow is considered turbulence profiles above the building canopy z d are calculated using the same stability dependent formulae within the building canopy z d the turbulence profile decays exponentially towards the ground the urban canopy flow modelling approach in adms local is the same as that previously implemented in adms urban as described in hood et al 2014 2 2 road source dispersion road sources are modelled explicitly as line sources within adms local detailed information about each of the road sources is specified as input road and volume sources are modelled using the gaussian plume formulations used in adms urban in contrast to adms urban adms local uses a simpler parameterised approach to account for the influence of street canyons on road source dispersion 2 2 1 road source data input formats the local model requires a number of input text files describing various aspects of each road source included in the model run these are summarised in table 2 time varying emission factors for a given road source for each pollutant can be input to the model either as regularly repeating profiles or hour by hour factors the profiles can be specified as 3 day 24 hour profile factors for weekdays saturdays and sundays 7 day and monthly an option also exists to automatically shift the profile factors by one hour during british summer time or an equivalent daylight saving time period to account for the fact that traffic patterns typically follow local clock time rather than local solar time 2 2 2 dispersion from road sources the gaussian plume dispersion equation used to model a two dimensional crosswind aligned ground level line source of length l centred on x y 0 0 and source strength q in neutral and stable conditions is 9 c x y z q 2 2 π σ z u z m erf y l 2 2 σ y erf y l 2 2 σ y exp z 2 2 σ z 2 reflection terms where σ y x and σ z x are the crosswind and vertical plume spread and u z m is the wind speed calculated at the mean plume height in convective conditions a skewed gaussian profile is used in line with field experiments that show the probability density function of the vertical velocity to be non gaussian the reflection terms are used to characterize the limits on plume spread formed by the ground surface and when present the temperature inversion at the top of the boundary layer to make use of this equation the model divides up each road source straight line segment into a number of two dimensional crosswind elements each with a source strength that varies in proportion to the fractional area of the segment that it represents with the sum over all elements equalling the original source strength to conserve the mass released the concentration c at a particular receptor location x y z due to emissions from that road segment is then the sum of the individual contributions from each element the division of each road segment into elements is done on a receptor by receptor basis since the part of a segment that can contribute to the concentration at a particular receptor will vary depending on their relative locations once the region of influence of a given segment for a particular receptor has been determined this region is divided into a maximum of 10 crosswind elements the along wind spacing between elements is such that the elements closest to the receptor are closer together to reduce fractional error while the spacing between any two adjacent elements is in general limited to change by no more than 10 the initial vertical plume depth for road sources is taken as 2 m while the crosswind plume spread is calculated on a road by road basis allowing for the extra turbulence caused by the flow of traffic to be taken into account the approach to accounting for vehicle induced turbulence in adms local matches that in adms urban with vehicle numbers and speeds for each road either supplied by the user or back calculated from emission rates with an assumed speed and proportion of heavy vehicles technical specification p31 01c 20 cerc 2021 when the road source is located within a street canyon adjustments are made to the dispersion calculations as outlined in the next section 2 3 street canyon influencing road source dispersion street canyons play an important role in altering the dispersion of pollutants from roads hood et al 2021 adms local incorporates the effects of street canyons on the dispersion from road sources in two ways firstly within street canyons concentrations resulting from road source dispersion are modified based on the properties of the street canyon using factors derived from analysis of more detailed modelling with the advanced street canyon module in adms urban then outside the street canyon the dispersion from the road is modelled as a porosity weighted combination of the standard no canyon road source representing the dispersion of pollutants through gaps in the canyon walls and a volume source representing the release of material from the top of the canyon the canyon top volume source for a given road segment has the same horizontal footprint as the canyon containing that segment and is vertically centred on the canyon top building height with a depth of twice the initial road mixing height 2 m the porosity of the canyon is calculated based on the proportion of the road without buildings on the side for a given road source segment located within a canyon of width w and average height h the concentration contribution at an output point within the canyon is calculated as a coverage weighted combination of the standard no canyon road source segment concentration and the full canyon unit coverage segment concentration at that output point fig 3 the full canyon concentration c c a n y o n at a given output point within the canyon is calculated using a series of factors applied to the standard no canyon road source segment centrepoint concentration c ˆ n o n c a n y o n at the release height 1 m in the following way 10 c c a n y o n c ˆ n o n c a n y o n f l a t e r a l f c a n y o n f h w f φ f c a n y o n f h w f φ where f c a n y o n introduces a vertical profile that adjusts the non canyon centrepoint concentration c ˆ n o n c a n y o n to a canyon centreline concentration at the output height z f h w adjusts concentrations to account for the canyon aspect ratio h w f φ adjusts concentrations to account for the wind direction relative to the road centreline φ f l a t e r a l is a lateral factor to convert from canyon centreline to canyon off centreline and superscripts and denote factors relating to along road parallel and across road perpendicular upstream flow respectively table 3 summarises the canyon factor dependencies and provides variable definitions values for the four factors have been derived from analyses of concentration outputs from multiple runs of adms urban where the model has been configured to represent a single in canyon road source using the advanced canyon model hood et al 2021 a wide range of representative source parameter ranges have been considered for example aspect ratios up to 2 0 3 r w w 0 95 and wind speeds and atmospheric stabilities representative of those experienced in urban areas example plots of the vertical variation of the core canyon factors for parallel and perpendicular wind directions are shown in fig 4 the increased concentration factor on the upwind kerbside relative to above canyon wind direction represents the effect of recirculating flow within the canyon 2 4 volume source dispersion dispersion from a volume source is calculated in a similar way to a road source but with each element representing a vertical slice of finite length and height the concentration contribution from an individual crosswind element of length l s height l v vertical mid point z c and source strength q in neutral and stable conditions is given by 11 c x y z q 4 u erf y l s 2 2 σ y erf y l s 2 2 σ y erf z l v 2 z c 2 σ z erf z l v 2 z c 2 σ z reflection terms a modified profile is adopted in convective conditions 2 5 chemistry a wide range of chemical processes which take place over large temporal and spatial scales are modelled within the regional models however chemical reactions on short time scales also strongly affect concentrations of some pollutants for instance nox chemistry strongly influences no2 and o3 concentrations in the vicinity of road sources adms local incorporates some important local scale reactions taking account of both locally emitted pollutant as modelled by adms local and regional pollutant from the regional model specifically adms local includes two chemistry schemes using the same basic principles and approach in both schemes dispersion processes are modelled first to generate an initial estimate of concentrations for each emitted pollutant then individual travel times are calculated as the time taken for each plume to reach each receptor from a source if only one plume is being modelled this travel time is the time over which the chemical reactions are applied in the case of multiple sources a concentration weighted average travel time is used an adaptive stepping technique is adopted to reduce finite difference errors during periods of fast chemical conversions while allowing faster computation during periods when concentration changes are small the two chemistry schemes are described in more detail below these schemes are also used in adms urban nox grs chemistry scheme the generic reaction set grs is used to model chemical processes that occur due to emissions of oxides of nitrogen nox consisting of no and no2 and volatile organic compounds vocs and their reactions with ozone o3 this scheme venkatram et al 1994 is a semi empirical photochemical model that reduces the complex series of chemical reactions involving the above pollutants to just seven reactions n1 n7 in the following list in adms local an additional eighth reaction is added for the reaction between no and o2 which is important for very high values of no n1 roc hv rp roc n2 rp no no2 n3 no2 hv no o3 n4 no o3 no2 n5 rp rp rp n6 rp no2 sgn n7 rp no2 sngn n8 2no o2 2no2 roc reactive organic compounds a fixed proportion of vocs rp radical pool sgn stable gaseous nitrogen products sngn stable non gaseous nitrogen products reactions n3 n4 and n8 represent exact chemical reactions the remaining equations are approximations involving bulk species during the day the reaction rate for reaction n3 is calculated by assuming the background values are in chemical equilibrium equation 21 with an upper limit applied based on the theoretical maximum available solar radiation no such adjustment is made at night time the full list of reaction coefficients are given below where rn1 corresponds to the reaction coefficient used for reaction n1 above and so on 12 r n 1 aroc r n 3 exp 4700 1 t 0 k 1 316 13 r n 2 358 1 6 t 0 k 14 r n 3 min r n 3 k max r n 3 15 r n 4 4 405 10 2 exp 1370 t 0 k 16 r n 5 1 6 17 r n 6 2 10 3 18 r n 7 2 10 3 19 r n 8 4 61 10 10 exp 530 t 0 k where 20 r n 3 k 8 10 4 exp 10 k 7 4 10 6 k k 0 0 k 0 21 r n 3 c bgd o 3 c bgd n o r 4 c bgd n o 2 k is incoming solar radiation in w m 2 k max is the maximum value of k possible assuming a solar elevation angle of 90 and zero cloud cover and aroc represents the weighted reactivity coefficient for roc taken to be 0 05 by default as a result of validation work though this value is modifiable via adms local s primary input file r 3 is calculated from the background concentrations of ozone no and no2 c bgd o 3 c bgd n o and c bgd n o 2 respectively if any of these values are zero r 3 r 3 k is used the above formulation assumes that a photochemical reaction rate derived from the background concentrations is more representative than a rate calculated from solar radiation values in case of erroneous background data a minimum of r 3 k max and r 3 is taken to ensure that the reactions occur at a realistic rate application of this chemistry scheme can affect the modelled concentrations of nox no2 and o3 so2 particulate sulphate chemistry scheme the sulphate reaction scheme represents the generation of particulate sulphate from so2 the scheme is the same as that currently used in adms urban and in the emep model tsyro 2001 the three reactions considered are as follows s1 2so2 o2 2so3 s2 so3 h2o h2so4 s3 h2so4 2nh3 nh3 2so4 with a rate constant r s 1 defined as 22 r s 1 3 10 6 2 10 6 sin 2 π day 80 365 where day is the julian day of the year the current formulation is correct only for model domains in the northern hemisphere reactions 2 and 3 are assumed to be instantaneous oxygen o2 water vapour h2o and ammonia nh3 are assumed to be available in abundance practically the model can therefore perform the above three reactions in one step namely 2so2 nh3 2so4 using rate constant r s 1 this ammonium sulphate is added to the particulate matter concentrations leading to reductions in so2 and increases in pm2 5 and pm10 2 6 output specifications to allow for concentrations to be calculated at street scale resolution adms local takes as input a full list of points at which the concentrations should be calculated in simple text file format pollutant concentrations can be calculated at hourly resolution and or as period averages in order to reduce output file size when generating high temporal and spatial resolution pollutant concentration datasets the majority of output data are written to netcdf format files a subset of meteorological data used by the model is also output to this file 2 7 model execution adms local run times are dependent on the number of hours being modelled as well as the number of output points and the number of road sources within the domain when creating high resolution pollution datasets the number of output points used by adms local is proportional to the number of roads to ensure the high concentration gradients across roads are captured within the maqs health system the adms local domains are regional model grid cells with one domain for each grid cell these different domains are run in parallel by the system as such the overall run time of the maqs health coupled system is dependent on the execution time of the most densely trafficked regional model grid cell within the domain as this cell will contain the most road sources and therefore the most output points a consequence of this is that coupled system runs using rms with fine resolution for example a resolution of 1 km will run faster than those with lower resolution because the adms local domains are smaller the example highly trafficked 1 km domain with 1 km buffer containing 1432 road sources and 2456 output points described in section 3 4 took approximately 5 5 hours to run a full year of hourly data this is indicative of run time of the overall system for a large domain providing sufficient computational resources are available validation runs where output is only obtained at a few monitor locations execute much quicker 3 model evaluation this section presents an evaluation of the standalone adms local model necessary prior to its use within the maqs health coupled system for simple configurations where minimal differences between adms local and adms urban predicted concentrations are expected this has been achieved by an inter comparison of adms urban and adms local modelled concentrations section 3 1 a new evaluation study of three continuous monitoring locations adjacent to relatively isolated uk major roads has been carried out using adms local only and is described in section 3 2 to demonstrate real world model performance for open roads where detailed traffic data are available further evaluation is performed by using evaluation databases that have been compiled for adms urban evaluation and previously published in the literature sections 3 3 for individual street canyons and 3 4 for city scale an example of high resolution output is shown in section 3 5 for each of the evaluation studies the statistics defined in equations 23 27 below are used to evaluate the modelled concentrations m in relation to the observed concentrations o where n is the number of pairs of modelled and observed concentrations a bar indicates the mean value m for example and a subscript indicates a single parameter value ranked between unity and n m i for example mean observed and modelled concentrations are presented alongside mean bias m o and the ratio of the standard deviation of modelled concentrations σ m to the standard deviation of observed concentrations σ o fractional bias f b is a measure of the mean difference between the modelled and observed concentrations 23 f b m o 0 5 o m normalised mean square error n m s e is a measure of the mean difference between matched pairs of modelled and observed concentrations 24 n m s e m o 2 m o pearson s correlation coefficient r is a measure of the extent of a linear relationship between the modelled and observed concentrations 25 r 1 n 1 i 1 n m i m σ m o i o σ o fraction of modelled hourly concentrations within a factor of two of observations f a c 2 is given by the fraction of model predictions that satisfy 26 0 5 m i o i 2 0 the statistic used to quantify overall model performance is the index of agreement ioa willmott et al 2012 given by 27 i o a 1 i m i o i 2 i o i o i m i o i 2 i o i o 2 i o i o i m o i 1 i m i o i 2 i o i o the ioa spans between 1 and 1 with values approaching 1 representing better model performance 3 1 road source model intercomparison a single road source has been configured in both adms local and adms urban with length 1 km and carriageway width 25 m the traffic activity data flows speeds and corresponding emission rates are representative of a busy london road including hourly time variation consistent hourly meteorological data are used in both models with the processed meteorological parameters output from the adms urban meteorological processor used as input to adms local in order to model the effects of near road nox chemistry the model requires hourly background concentrations of nox no2 and o3 hourly values measured upwind of london are used as input both models are run to generate hourly nox and no2 concentrations at 1 m above the ground for a set of output points perpendicular to the road centreline to a distance of 150 m fig 5 compares predicted annual average concentrations left hand axis and concentration ratios right hand axis between the two dispersion models with the full lines showing the adms local concentrations and the cross symbols indicating the corresponding adms urban values the concentration and concentration ratio cross sectional profiles for these annual means are indistinguishable in the figure demonstrating consistency between the implementation of the dispersion and chemistry equations in the model codes there are small differences on an hour by hour basis maximum standard deviation of percentage differences are less than 0 1 0 01 for nox and less than 2 0 0 1 for no2 for the road centreline receptor the nox concentration profile captured by both models in fig 5 shows the sharp decrease in concentrations that occurs in the vicinity of trafficked roads in urban areas for this example annual nox concentrations reduce to approximately 72 of the road centreline concentration at the carriageway edge 12 5 m and to 26 by 50 m the no2 concentration gradient is less steep with corresponding fractions of centreline concentrations of 80 at the carriageway edge and 43 at 50 m from the centreline the difference between nox and no2 near road concentration gradients is a consequence of nox being influenced primarily by dispersion processes whereas no2 is also influenced by fast chemical conversion of no to no2 and the higher relative contribution of the background concentrations the specified ratio of emissions no2 nox is 22 the road centreline annual average concentration ratio no2 nox is 38 with the increase of 16 above the emissions ratio showing the very local influence of chemical reactions and the influence of the background concentrations the concentration ratio increases to 69 within 50 m of the road centreline and is 83 at a distance of 1 km from the road these values are typical of observed values demonstrating the efficacy of adms local in representing dispersion and chemical processes that occur in the vicinity of road sources 3 2 open road model performance evaluation in order to evaluate real world model performance for isolated open roads concentrations at three continuous monitoring sites on england s strategic road network have been modelled using adms local cambridge salford and sandy each site is located adjacent to a single major road for which hourly traffic activity data are available specifically vehicle counts and speeds and simple fleet composition data highways england 2016 these traffic data were used for calculating hourly nox and no2 emissions based on the uk emissions factor toolkit version 10 1 england motorway emission factors department for environment food rural affairs 2020 with real world adjustments based on remote sensing data consistent with the approach described in hood et al 2018 the modelled years were chosen for adequate availability of the hourly traffic data which varies between locations and years all emissions from nearby urban areas are taken from the uk national atmospheric emissions inventory naei modelled as a 1 1 km2 grid source with 10 m depth and an initial fraction of nox released as no2 set to 12 meteorological data from local measurement sites were used as input to adms local while hourly average background concentration data for nox no2 and o3 were obtained from rural monitoring sites in the relevant regions the three sites are summarised in table 4 and map visualisations for each site are given in fig 6 there are additional uncertainties for some sites for example only southbound traffic data for the carriageway nearer to the monitor were available for the sandy site leading to an assumption of equal traffic flow on the more distant northbound carriageway at the sandy site there is a 4 m wide parking bay between the main carriageway and the monitor in addition to the monitor being located in a vehicle sales car park that is adjacent to an mot centre while at the salford site there is a 3 m wide hard shoulder between the main carriageway and the monitor none of these areas were included in the modelled road carriageways but they may in reality have intermittent emissions there is some uncertainty relating to the monitor inlet heights in particular with heights of both 1 5 m and 3 m stated in different sources for the sandy monitor the height of 1 5 m from a local authority report was used for the model configuration the modelling of the cambridge and sandy sites includes periods when covid 19 restrictions were in force in the uk during 2021 and 2020 respectively changes to traffic flow patterns along the road adjacent to each monitor are included in the detailed traffic data but changes in other activities and hence emissions may not be represented model evaluation statistics for the open road studies are given in table 5 these statistics show best performance at the cambridge site with fractional bias values of 0 07 0 10 for nox no2 and correlation coefficients of 0 75 0 72 the other sites generally show somewhat better performance for no2 than nox for example salford correlation of 0 69 for no2 but 0 65 for nox the largest magnitudes of fractional bias occur at the sandy site 0 50 0 38 for nox no2 where there were the greatest uncertainties in model input data also reflected in poorer nmse values 1 49 for nox 0 62 for no2 the nox correlation value at this site is lower than for the other sites 0 58 whereas the no2 correlation value 0 67 is similar to the other sites further analysis has been carried out for no2 as this is a regulatory pollutant of interest for local modelling frequency scatter plots of hourly no2 concentrations for each of the three major road modelling sites are shown in fig 7 the graph for cambridge shows the greatest concentration of hours near the 1 1 line while for salford there is more scatter for sandy the model generally under predicts in addition to a small number of hours with unusually high observed concentrations which may indicate specific local emission events in the parking bay or vehicle sales mot centre adjacent to the monitor which are not included in the modelled emissions the variation of monthly average modelled and observed no2 concentrations for each of the major road sites are shown in fig 8 the pattern of increased concentrations in winter and decreased concentrations in summer is seen in both modelled and observed data for all sites for the cambridge site the modelled concentrations underestimate the observed values for the first few months of the year this coincides with a period of covid 19 restrictions in the uk which may have led to increased domestic combustion emissions from two adjacent residential properties a change which is not represented in the modelling for salford and sandy the monthly variation of concentrations is generally well represented although there is a consistent underestimate of concentrations at sandy likely due to omission of local emissions already discussed 3 3 street canyon model performance evaluation the performance of the parameterised canyon model within adms local is evaluated by comparison with measurement data from the optimization of modelling methods for traffic pollution in streets research network study campaigns trapos 2020 these data have been used extensively for validation of the ospm model kakosimos et al 2010 and for the evaluation of the advanced street canyon model within adms urban hood et al 2021 the trapos dataset consists of air pollutant concentration measurements recorded in three street canyons jagtvej copenhagen sweden goettingerstrasse hanover germany and schildhornstrasse berlin germany the datasets include meteorological measurements and background pollutant concentrations recorded in the vicinity of the sites in addition to traffic flow data that can be used to estimate emissions using available emission factor datasets hbefa keller et al 2017 table 6 summarises the campaign sites and datasets readers are referred to hood et al 2021 for a more comprehensive description of the measurement datasets evaluation for nox is presented in order to focus on dispersion effects in these canyons and due to additional uncertainties relating to no2 and co emissions in order to understand the importance of accounting for street canyons in detail model predictions when no canyon has been modelled are compared with the observations alongside the street canyon modelling table 7 summarises model evaluation statistics for the trapos sites calculated from hourly modelled and measured concentrations of nox adms local demonstrates good performance when the model is configured to account for street canyons canyon m cases for instance at all three sites the correlation exceeds 0 8 and the number of points within a factor of two of the observations is greater than 0 9 the likely explanation for the slight under prediction of concentrations at jagtvej fb 0 206 and over prediction at goettingerstrasse fb 0 074 is uncertainty in the emissions estimates the emission rates for each site were calculated using the hbefa database and detailed vehicle count data from the campaigns however assumptions had to be made relating to traffic situation congestion level and traffic speeds regional variations in the introduction of catalytic converters into the vehicle fleet during the period of the trapos studies also increases the uncertainty in emissions when the influence of street canyons on dispersion is neglected no canyon m cases there are large negative fractional biases at all sites and the catch all ioa statistics are significantly lower than for the canyon m case which demonstrates the importance of accounting for street canyons when using models to predict aq in urban locations where buildings alter the flow and dispersion of pollutants the relative importance of spatial and temporal variations of concentration is shown in fig 9 for the three trapos sites horizontal dashed lines show the inter quartile range of observed and modelled hourly concentrations while vertical lines show the variation of average modelled concentration with output height there are differences in inlet measurement height between the three sites but the variation of average modelled concentration with height is much smaller than the variation of hourly concentrations the modelled inter quartile concentration ranges match the observed ranges fairly closely although there is some underestimate of higher concentrations at jagtvej and overestimate of higher concentrations at schildhornstrasse the observed and modelled variations of hourly nox concentrations with observed wind speed and direction at each of the trapos study monitoring sites are shown in fig 10 as polar plots these plots demonstrate that adms local dispersion modelling with parameterised canyon effects captures the main observed concentration distribution features for all sites of increased concentration on the upwind side of the canyon relative to upstream wind direction due to recirculating flow within the canyon as well as elevated concentrations for along canyon wind directions due to channelled flow within the canyon 3 4 city scale adms local performance evaluation the next stage of adms local evaluation is the application of the model at the city scale data from london s extensive reference monitor data network with the highest data capture 45 sites is used to evaluate the model over a domain of approximately 1500 km2 for a full year 2012 this model configuration is derived from one previously used to evaluate adms urban performance hood et al 2018 but with some simplifications including the use of homogeneous meteorological site properties rather than spatially varying urban canopy parameters and accounting for point sources within grid sources rather than explicit modelling of dispersion from individual point sources a range of air quality pollutants are modelled nox no2 o3 pm10 and pm2 5 the model s ability to account for near road chemical reactions can be tested by evaluating no2 and o3 alongside nox london s air quality monitoring network includes reference measurement stations located at urban background industrial roadside and kerbside locations for the purpose of this adms local evaluation the monitors are categorised as near road and background table 8 summarises model performance by site type for the five pollutants listed above overall the evaluation statistics reflect good model performance for all pollutants and site types with fractional biases less than 0 16 correlations greater than 0 55 number of points within a factor of two of the observed greater than 0 62 and ioa s greater than 0 60 for all pollutants there is a consistency in nox and no2 statistics with average modelled observed ratios being similar for both pollutants 0 83 0 90 for urban background sites and 1 01 1 01 for near road sites for nox no2 respectively the spread of model performance across all near road and background sites is shown using scatter plots of annual average concentrations in fig 11 the model predictions for all sites are within a factor of two of observed concentrations for all pollutants with the exception of two nox measurement sites which are just outside this limit shown by the dashed lines many of the sites have modelled predictions within smaller factors of the observed values with a good representation of the spread of concentrations between urban background and near road sites concurrent analysis of nox no2 and o3 concentrations can be helpful for assessing the model s representation of nox chemistry fig 12 shows the diurnal variation of concentrations averaged over 11 background and 9 near road sites in london which monitor all three of these pollutants in general the modelled concentrations match the magnitude and variation of measured concentrations closely at both background and near road sites for all three pollutants although there is some over estimation of daytime o3 at near road sites 3 5 high resolution output fig 13 shows an example pollution map of annual average no2 concentrations for a central london 1 km 1 km domain calculated by adms local concentrations correspond to the model configuration evaluated and discussed in section 3 4 elevated concentrations within and adjacent to the roads with explicit emissions are well resolved modelled concentrations vary along these roads due to changes in the parameterised building properties used as input to the model building height and coverage and canyon width emissions from less busy roads are included within the model grid sources which is why these roads show no local elevation in concentration the 200 m 200 m inset shows a section of concentration map in detail with explicit buildings overlaid shaded by building height higher concentrations clearly correspond to street canyons that are created by taller buildings with fewer gaps between them because the pollution is trapped within the canyons where there are more gaps between buildings the in road concentrations are relatively low but concentrations in adjacent areas are higher because the pollution disperses at ground level the resolution to which the dispersion model reflects the variation in building porosity relates to the number of road segments that are modelled shorter segments allow the influence on dispersion of changes in building dimensions along a single road to be accounted for however road segment lengths must be sufficiently long to encompass multiple buildings as adms local does not model flow around explicit buildings street canyons are assumed to be symmetric as can be seen from the near symmetric dispersion from road sources in fig 13 asymmetry arises from prevailing meteorology and other factors more detailed local modelling of street canyons including canyon asymmetry can be allowed for within maqs health by use of the adms urban model which includes an advanced street canyon module hood et al 2021 modelled concentrations throughout this example area of central london exceed the uk and eu air quality objective for annual average no2 of 40 μg m3 adms local modelled concentration contours for nox o3 pm2 5 and pm10 are presented in fig 14 for the same 1 km 1 km cell as shown in fig 13 the pattern of concentrations for nox is very similar to those for no2 the concentrations of o3 have an inverse relationship to those of no2 with reduced concentrations at roadside and higher concentrations at background locations due to chemical titration by local traffic emissions there is a smaller magnitude of increment between background and in canyon concentrations for particulate pollutants than for no2 but a similar overall pattern of elevated concentrations within the deepest and most continuous canyons with qualitatively similar distributions for both pm2 5 and pm10 uk annual average air quality objectives of 25 μg m3 for pm2 5 and 40 μg m3 for pm10 are only exceeded in a few locations in general at junctions and road carriageways which would be excluded from exposure calculations there are no uk air quality objectives defined in relation to human health for annual average nox or o3 4 discussion and further work for the purposes of health research models have distinct advantages over measurements in view of their capability both to generate high resolution spatial coverage over large domains and to make future projections of air quality under the spf caf the uk government has funded the development of a regional to local scale aq modelling system for use in health research maqs health this paper presents the scientific formulation and evaluation of the adms local road source dispersion model which has been developed to be a local model component for use within the maqs health the adms local model formulation is derived from the road source component of the adms urban aq model for evaluation purposes this paper presents an intercomparison of the adms local and adms urban models the small differences in modelled concentrations relate to differences in computational implementation the choice of features chosen for inclusion in adms local takes account of the requirements of maqs health as a whole for instance the model uses meteorological parameters generated by regional models as input road and volume sources can be explicitly represented in the model thereby resolving sharp near road concentration gradients the influence of the urban morphology and infrastructure on pollutant dispersion is allowed for through the modelling of street canyons including allowance for pavements urban canopy flow and the impacts of road elevation the model accounts for simplified short timescale nox and sox chemical reactions and output points can be specified at any horizontal or vertical location within the model domain output may be at specific receptor locations for instance postcodes or on an irregular grid which can be used to generate pollution maps hourly concentrations are calculated the scientific formulation of adms local is evaluated through the use of three measurement datasets the first for open major roads the second specifically for street canyons and the third testing overall model performance within an urban domain the statistical evaluation results presented show that the model performs well for instance the ioa values exceed 0 60 for the three open major road sites 0 75 for all three street canyon studies and 0 60 for each of the pollutants modelled for the study of overall performance the model execution times of adms local are dependent on the number of road sources and output points within the domain within maqs health these domains will be the size of regional model grid cells with the system running each of these separate domains in parallel the overall runtime of the system is then dependent on the densest of these regions and finer scale resolution regional model configurations can have faster run times than corresponding coarser resolution model runs adms local is structured so that model enhancements would be relatively straightforward to implement these could include for example refinement of the canyon parameterisations making use of the adms urban advanced canyon model further comparison with measurements and allowance for canyon asymmetry personal exposure of individuals as they follow a particular route is often of interest for health studies such pollutant dose calculations could be facilitated by the introduction of mobile receptors temporally varying locations for model output into adms local subsequent papers will present details of the full maqs health coupled system and associated detailed system evaluation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding this work was supported by the uk government s strategic priorities fund spf clean air programme administered by the met office grant agreement p107531 
25570,poor air quality impacts life expectancy and quality of life of populations worldwide numerical models generate comprehensive air pollutant concentration datasets useful for quantifying historic current and through projections future air pollution effects on health however it is challenging to accommodate the full range of dispersion and chemistry processes affecting both regional and local scales in a single model the scientific formulation and evaluation of a new quasi gaussian road source dispersion model adms local is presented this model has been developed as a component for the multi model air quality system for health research maqs health adms local accounts for the influence of urban morphology on dispersion as well as near road nox chemistry processes leading to the spatial resolution of pollutant concentration gradients occurring over distances of a few metres the model is optimised for use within the maqs health system where local modelling for each regional model grid cell is run in parallel graphical abstract image 1 keywords dispersion air quality adms coupled model evaluation development maqs health software availability adms local forms an integral part of the maqs health coupled system the maqs health system will be available to the uk research community for air quality and health via the spf clean air framework platform from 2022 nelson 2021 the maqs health system is designed to run on linux based hpcs alongside chemical transport models adms urban carruthers et al 2000 upon which adms local is based is commercially available 1 introduction each year in the uk around 40 000 deaths are attributable to exposure to outdoor air pollution royal college of physicians 2016 reducing morbidity associated with air quality aq requires an understanding of the relationship between exposure to atmospheric pollutants and health outcomes in the uk the committee on the medical effects of air pollutants comeap 2009 has collated evidence that strongly suggests an association between long term exposure to particulate air pollution and mortality effects research has shown that the time periods over which pollutant exposure is harmful differs for different pollutant species for instance short term hourly and sub hourly exposures to high levels of gaseous pollutants no2 and so2 increase the risk of hospital admissions mills et al 2015 dab et al 1996 whereas longer term exposure to particulate species may exacerbate health conditions such as lung cancer and cardiopulmonary mortality pope et al 2002 as a consequence of improved knowledge of the medical effects of air pollution the world health organisation who has recently revised its air quality guidelines world health organization 2021 most notable are the revised guideline values for annual average concentrations for pm2 5 and no2 of 5 μg m³ and 10 μg m³ respectively these values are currently exceeded in many areas of the world the uk has over 400 reference aq monitoring stations that record concentrations in air of a wide range of air pollutants including nox no2 o3 pm10 pm2 5 and other particulate species usually at hourly temporal resolution significant networks include the uk automatic urban and rural network and separate networks in england scotland wales northern ireland and london measurement stations have been deployed in locations ranging from rural to kerbside sites in cities with additional monitoring in industrialised areas in recent years there has been an increase in the availability of low cost aq sensors which can be deployed as relatively high density networks mead et al 2013 but such equipment is generally less accurate than reference monitors castell et al 2017 also as is the case for reference equipment sensors require calibration and maintenance and data ratification procedures are necessary there is also the increasing prospect of satellite data being of sufficiently high resolution that it may routinely characterize local air quality for example pope et al 2018 made use of ozone monitoring instrument omi data to observe changes in no2 pollution hotspots in the and pope and provod 2016 used omi data to determine tropospheric column no2 levels around three power stations computational models can replicate physical and chemical atmospheric processes and in conjunction with meteorological models they can be used to calculate air pollutant concentrations measured data are often used to improve model predictions for instance to define boundary conditions as well as for calibration and evaluation an important advantage that such models have over measurements is their comprehensive spatial coverage also models can be used to investigate the impact of policy scenarios on air quality and calculate projections of future aq taking into account future emissions and climate estimates modelled air pollutant concentration datasets are used by health researchers in their studies of population exposure and health outcomes mizen et al 2018 gulliver et al 2018 alcock et al 2017 there are a number of well established regional and global chemical transport models ctms that are used to model gaseous and particulate atmospheric pollutant species in the atmosphere usually at hourly temporal resolution and at spatial resolutions of up to 1 km the us environmental protection agency epa community model for air quality cmaq byun and schere 2006 is used internationally for policy and research chemel et al 2014 as well as within forecasting applications che et al 2020 ramboll s camx model environ 2020 is used for similar applications and includes a model that facilitates source apportionment calculations bove et al 2014 the emep model has been developed as part of the co operative programme for monitoring and evaluation of the long range transmission of air pollutants in europe and has been used for a range of policy and health research applications in the uk vieno et al 2014 heal et al 2013 ctms are usually implemented using one way coupling with meteorological models however a two way coupled version of wrf cmaq has been developed wong et al 2012 ncar s wrf chem grell et al 2005 has also been developed to allow for the influence that pollutant concentrations have on meteorological processes and has been used to model air quality in many regions worldwide ansari et al 2019 whilst regional ctms have large spatial coverage their finest spatial resolution does not resolve the high pollutant concentration gradients that occur within the vicinity of specific air pollutant emission sources there are however a number of atmospheric dispersion models that replicate the dispersion of emissions on a source by source basis with spatial resolution of a few metres these models are required not only to take account of source geometries but also to allow for urban morphological features that influence dispersion processes for instance buildings that generate re circulatory flows leading to pollutant accumulation tunnel portals where vehicle emissions from the whole tunnel length can be released from a small area and bridges flyovers where elevated traffic emissions are exposed to higher wind speeds and enhanced dispersion local models must also account for nearfield chemical reactions specifically the conversion of no to no2 that occurs over short temporal and spatial scales in the vicinity of road sources valencia et al 2018 and other sources cambridge environmental research consultants cerc develop the quasi gaussian adms urban local air dispersion model that is used worldwide carruthers et al 2000 for assessment research and policy application the us epa aermod model cimorelli et al 2005 is used for regulatory applications in the us and also internationally aermod includes an open road source dispersion model rline currently non regulatory snyder et al 2013 other models used for urban air quality include sirane soulhac et al 2011 gral oettl 2015 and caline benson 1992 regional and local air quality cannot be considered independently long range pollutant transport strongly influences concentration levels of some pollutants in urban areas for instance particulates and o3 conversely anthropogenic emissions from urban and industrial areas are key drivers in some atmospheric chemical processes that lead to regional air pollution episodes for instance nox and voc emissions influence particulate and o3 concentrations in rural areas chen et al 2020 consequently systems that couple regional and local air quality models are necessary in order to quantify baseline air pollutant concentration for population exposure studies that incorporate the full range of spatial scales such nested models can also be used to assess the likely impact of proposed national and local pollution mitigation strategies coupled air quality modelling systems spanning a range of spatial scales may be configured in different ways silveira et al 2019 multiple modelling scales implemented within a single model with external boundary conditions is a typical approach for many meso scale meteorology and regional chemical transport models such as wrf cmaq and camx the models commonly use one way nesting with boundary and initial conditions feeding from large low resolution domains to smaller and higher resolution domains examples in the literature include chatani et al 2011 which presents results from a nested wrf cmaq configuration and russell et al 2019 which describes the canadian national operational forecast model gem mach however this nesting approach is not appropriate for resolving air pollutant concentrations at street scales finer scale modelling can be achieved by using meso scale meteorological models to drive local cfd models kadaverugu et al 2019 however cfd modelling of urban areas is computationally expensive and compatible only with a limited range of meteorological conditions so this modelling approach is generally applied only to short time periods and very small spatial domains for instance kwak et al 2015 presents results from modelling a domain covering less than 2 km2 for nine hours the most practical approach to accounting for a large range of spatial scales is to use regional meteorological and chemistry transport modelling systems to drive local parameterised aq models the goal of this type of system is to combine the complex large scale gas and aerosol chemistry of regional modelling with high resolution and computationally efficient parameterised modelling of local dispersion especially from road sources in urban areas one general challenge for linking regional with local dispersion modelling is that of double counting local emissions which are also included in the regional modelling the effect of this on the final modelled concentrations will be more significant for finer gridded model resolution its impact has been omitted for some coarser scale regional modelling for example beevers et al 2012 isakov et al 2009 mensink et al 2003 looser coupling methods use regional modelling to substitute for measured rural background concentrations outside an urban area for example kukkonen et al 2016 the development of a multi model air quality system for health research maqs health has been funded as part of the uk government s strategic priorities fund spf clean air programme cap maqs health is a coupled air quality modelling system spanning national to urban street scales accounting for physical and chemical processes at all relevant temporal and spatial scales the system fig 1 links a wide range of regional ctms to a newly developed road source dispersion model adms local which is the topic of this paper maqs health incorporates a verification system to facilitate evaluation of model outputs against measurement datasets maqs health is one of three linked spf cap projects the goal of the project developing a uk community emissions modelling system dukems led by the uk centre for ecology and hydrology ukceh is to deliver a tool able to process and generate emissions data for air quality modelling applications the goal of the project data integration model for exposure modelling dimex uk led by the university of exeter is to develop a modelling framework to integrate ambient and indoor concentrations with human activity to estimate personal exposures to air pollution there are two broad categories of maqs health stakeholders aq modellers within the research community will use the system to generate air pollutant concentration datasets for use in health research and health research and policy makers will use the pollutant concentration datasets for health analysis adms local is a new urban dispersion model derived from the adms urban model the primary purpose of adms local is to calculate the dispersion of traffic emissions at spatial resolutions that resolve the sharp concentration gradients in the vicinity of road sources as well as modelling road sources explicitly the local model is able to calculate the dispersion of gridded emissions representing all other air pollutant sources in the model domain this is necessary because the total pollutant concentrations including contributions from all sources are required in order to explicitly model fast local chemical reactions in addition within the coupled system the local model is used to replicate the local dispersion of the gridded emissions from the regional model so that the resulting concentrations can be subtracted from the regional model contributions to avoid double counting the meteorological data driving dispersion processes in adms local are derived from those calculated by commonly used mesoscale meteorological models such as wrf and the uk met office unified model brown et al 2012 concentration outputs from adms local are sufficiently flexible to allow calculation of predicted hourly pollutant concentrations suitable for use in health research at specific receptor locations for example postcodes and on regularly and irregularly spaced grids to calculate for example spatially averaged concentrations within wards and other geographies adms local has been developed to model the pollutants that relate to current health related air quality metrics for instance no2 pm2 5 pm10 so2 and o3 but is sufficiently flexible that it can be configured to model other pollutant species that may be of interest for future health assessments for instance particulate components such as black carbon and metals adms local like adms urban has been coded in fortran following the fortran 2008 standards supported by most compilers the program proceeds as follows the input data is read in and undergoes initial processing including calculation of atmospheric properties from meteorological data the program then loops over each of the hours being modelled calculating the concentrations at each receptor due to each road source applying time varying emissions factors calculating open road dispersion if canyon effects are required applying parameterised canyon factors to canyon centreline open road concentrations for in canyon receptors calculating canyon top volume source dispersion and combining with weighted open road dispersion for out of canyon receptors calculating the concentrations at each receptor due to each cell of gridded emissions representing non road sources summing the concentrations from all road and grid sources at each receptor adding regional upwind background concentrations then undertaking chemical reactions the concentrations for each hour are written to the output files all code has been peer reviewed and testing has been undertaken to ensure that model outputs are entirely consistent with model specification for a wide range of inputs adms urban has undergone extensive model evaluation owen et al 2000 lao and teixidó 2011 hood et al 2018 valencia et al 2018 biggart et al 2020 zhong et al 2021 adms local shares core scientific formulation with adms urban but has been fully re coded with some improvements to computational efficiency and simplifications to the available user inputs for concentrations dispersed from open road sources and 3d grid sources with identical meteorological and background concentration inputs any differences in the outputs of the two models should arise only from minor differences in the computational algorithms used in the internal calculations therefore for some simple scenarios adms local model performance can be tested by direct comparison with adms urban for more complex model configurations including where the scientific formulations of the models diverge for example in the approach to modelling the effects of street canyons on pollutant dispersion adms local model performance requires additional assessment by comparison with available measurement datasets this paper presents the formulation and evaluation of the adms local model subsequent articles will describe formulation and evaluation of the maqs health coupled system a full technical description of adms local is given in section 2 the results of adms local evaluation are presented in section 3 a discussion and outline of further work is given in section 4 2 adms local model formulation adms local is a gaussian plume dispersion model with the atmospheric boundary layer structure defined by boundary layer depth and monin obukhov length a skewed gaussian distribution is used in convective conditions the model includes a meteorological processor to calculate boundary layer properties from the input meteorological data section 2 1 road sources are modelled as line sources with additional mixing due to vehicle induced turbulence section 2 2 adms local includes a simplified street canyon parameterisation to ensure that the model can be used over large domains without requiring extensive local input datasets aggregate gridded emission sources are modelled using a 3d grid of volume sources section 2 3 for each of these source types concentrations can be calculated at any output point location to ensure the required spatial resolution section 2 5 fast local chemistry can also be applied during the concentration calculations section 2 4 the model has been created to run on linux based hpc systems for compatibility with the other components of the maqs health system model code is written in fortran following the fortran 2008 standards as these are supported by the majority of fortran compilers netcdf libraries have been compiled within adms local to allow for the reading and writing of netcdf files the model outputs a log file containing run information including any warnings and errors issued during operation within the maqs health system adms local model runs are executed separately for each regional model grid cell within the local modelling domain execution of these separate regions on multiple cores and nodes forms the primary parallelisation of the system fig 2 presents an example regional model grid corresponding to an emep model run vieno et al 2016 this figure also shows the major road network modelled explicitly within adms local the regional model grid cells have a dimension of 1 km by 1 km so the corresponding adms local model domain size for each grid cell is 1 km by 1 km plus a buffer region to ensure continuity of modelled concentrations over grid cell boundaries as shown in fig 2 inset 2 1 meteorological data and boundary layer profiles adms local takes as input meteorological data extracted from the regional meteorological model for each grid cell note that meteorological data input is one aspect of adms local which has some differences from adms urban the adms urban meteorological pre processor additionally allows input of wind speed and turbulence at heights other than 10 m and includes algorithms to determine the surface heat flux from surface measurements 2 1 1 meteorological data input the local model uses a plain text file of hourly meteorological data as input which is generated by the meteorological processing utility of the coupled system this ensures that the meteorological variables are consistent with those used by the regional model the variables that may be included in the meteorological file are summarised in table 1 also shown are units where relevant minimum and maximum permitted values the last column indicates which variables are required or optional the local model requires three additional time independent parameters specifically the site latitude decimal degrees an average surface roughness z 0 m and the minimum monin obukhov length m i n l m o m used to represent limitations on atmospheric stability arising from urban mixing and heating 2 1 2 urban morphology data the local model also takes account of the impact of buildings in an urban area on the local airflow through local modifications to the vertical wind speed and turbulence profiles urban canopy flow the building characteristics of the urban area are specified through the following optional parameters average building height within the cell h m average street canyon width within the cell g m ratio of the sum of the plan area occupied by buildings to the total plan area within the cell λ p ratio of total frontal area of buildings perpendicular to wind direction to the total plan area within the cell λ f for a range of different wind sectors and roughness length within the urban canopy z 0 s m 2 1 3 meteorological calculations and boundary layer profiles the adms local meteorological processor routines take the input meteorological data for a given hour along with the additional time independent parameters specified in the previous section and use similarity theory to derive further boundary layer parameters as required for running the dispersion model the meteorological processor routines use many components of the meteorological pre processor used in cerc s other adms models as required to process data from a regional meteorological model a full technical description of the adms meteorological pre processor can be found in technical specification document p05 01 cerc 2021 some key concepts are included here though readers should refer to the referenced document for full details as well as for the original references on which the equations are based key parameters that are calculated by the meteorological processor include friction velocity u convective velocity scale w if the surface sensible heat flux is positive zero otherwise monin obukhov length l m o and boundary layer depth h if not provided as an input parameter an iterative procedure is used to calculate u and l m o assuming a modified logarithmic surface layer wind profile following 1 κ u s u ln z z 0 z 0 ψ z z 0 l m o z 0 l m o where κ 0 4 is the von kármán constant u s z is the wind speed at height z above ground which is known at 10 m from the input meteorological data ψ is a stability dependent function and l m o is defined as 2 l m o u 3 ρ a c p t 0 k κ f θ 0 g where ρ a 1 225 kg m 3 and c p 1012 j kg 1 k 1 are the assumed air density and specific heat capacity at constant pressure respectively t 0 k is the screen height temperature in kelvin and g is gravitational acceleration if the boundary layer depth is not provided directly it is calculated as follows in stable conditions f θ 0 0 it is taken as 3 h 0 6 u ω 1 1 2 28 u ω l m o where ω is the coriolis parameter in unstable conditions f θ 0 0 h is assumed to evolve according to 4a d h d t s δ θ 4b d δ θ d t γ θ s δ θ f θ 0 ρ a c p h s h 4c s c f f θ 0 ρ a c p a u 3 t 0 k g h where t is time δ θ is the potential temperature jump across the boundary layer top γ θ is the rate of increase of potential temperature above the boundary layer and c f 0 2 and a 5 are constants s relates to the magnitude of convective and mechanical turbulence available to drive entrainment into the boundary layer finally w is calculated using 5 w 3 u 3 h κ l m o using the derived boundary layer parameters output by the meteorological processor the local model is able to calculate vertical profiles of mean wind speed turbulence temperature and other quantities throughout the boundary layer the boundary layer structure algorithms used are described in detail in technical specification document p09 01 cerc 2021 some are summarised below the mean wind profile is the same as that used in the meteorological processor equation 1 that is a stability dependent modified logarithmic profile is assumed above h the wind speed is held constant if urban canopy flow parameters are provided the wind profile is implemented in three parts depending on the displacement height d m above the ground 6 d h 1 λ p 1 α λ p where α is a constant with value 4 43 representing a staggered array of buildings the profile above the buildings z 2 d is calculated by an expression similar to equation 1 using z d as the height to displace the upwind profile vertically upwards 7 u a z u b κ ln z d z 0 b ψ z d l m o here u b is the frictional velocity representative of the buildings and z 0 b is the local surface roughness derived from the input parameters near the ground z d the flow within the building canopy is related in magnitude to the upwind flow decreasing logarithmically with decreasing height 8 u c z u s κ ln z z 0 s here u s is the frictional velocity representative of the wind profile within the building canopy the roughness length z 0 s represents the value of extrusions from the roads and pavements in an urban area with a typical value of 10 cm a transition layer exists between the urban canopy flow regimes d z 2 d where the profile is calculated using linear interpolation to allow for a continuous vertical profile the technical specification document p34 01b cerc 2021 contains a full technical description of the urban canopy flow formulation turbulence profiles are calculated using one of three sets of formulae depending on the atmospheric stability namely whether h l m o 0 3 convective 0 3 h l m o 1 neutral or h l m o 1 stable hunt et al 1988 in convective conditions contributions from convectively driven and mechanically driven turbulence are included while in neutral conditions only mechanically driven turbulence contributes in stable conditions the turbulence profiles depend on the friction velocity roughness length and boundary layer depth alone a minimum value is also applied to the turbulence components depending on the specified minimum monin obukhov length if the urban canopy flow is considered turbulence profiles above the building canopy z d are calculated using the same stability dependent formulae within the building canopy z d the turbulence profile decays exponentially towards the ground the urban canopy flow modelling approach in adms local is the same as that previously implemented in adms urban as described in hood et al 2014 2 2 road source dispersion road sources are modelled explicitly as line sources within adms local detailed information about each of the road sources is specified as input road and volume sources are modelled using the gaussian plume formulations used in adms urban in contrast to adms urban adms local uses a simpler parameterised approach to account for the influence of street canyons on road source dispersion 2 2 1 road source data input formats the local model requires a number of input text files describing various aspects of each road source included in the model run these are summarised in table 2 time varying emission factors for a given road source for each pollutant can be input to the model either as regularly repeating profiles or hour by hour factors the profiles can be specified as 3 day 24 hour profile factors for weekdays saturdays and sundays 7 day and monthly an option also exists to automatically shift the profile factors by one hour during british summer time or an equivalent daylight saving time period to account for the fact that traffic patterns typically follow local clock time rather than local solar time 2 2 2 dispersion from road sources the gaussian plume dispersion equation used to model a two dimensional crosswind aligned ground level line source of length l centred on x y 0 0 and source strength q in neutral and stable conditions is 9 c x y z q 2 2 π σ z u z m erf y l 2 2 σ y erf y l 2 2 σ y exp z 2 2 σ z 2 reflection terms where σ y x and σ z x are the crosswind and vertical plume spread and u z m is the wind speed calculated at the mean plume height in convective conditions a skewed gaussian profile is used in line with field experiments that show the probability density function of the vertical velocity to be non gaussian the reflection terms are used to characterize the limits on plume spread formed by the ground surface and when present the temperature inversion at the top of the boundary layer to make use of this equation the model divides up each road source straight line segment into a number of two dimensional crosswind elements each with a source strength that varies in proportion to the fractional area of the segment that it represents with the sum over all elements equalling the original source strength to conserve the mass released the concentration c at a particular receptor location x y z due to emissions from that road segment is then the sum of the individual contributions from each element the division of each road segment into elements is done on a receptor by receptor basis since the part of a segment that can contribute to the concentration at a particular receptor will vary depending on their relative locations once the region of influence of a given segment for a particular receptor has been determined this region is divided into a maximum of 10 crosswind elements the along wind spacing between elements is such that the elements closest to the receptor are closer together to reduce fractional error while the spacing between any two adjacent elements is in general limited to change by no more than 10 the initial vertical plume depth for road sources is taken as 2 m while the crosswind plume spread is calculated on a road by road basis allowing for the extra turbulence caused by the flow of traffic to be taken into account the approach to accounting for vehicle induced turbulence in adms local matches that in adms urban with vehicle numbers and speeds for each road either supplied by the user or back calculated from emission rates with an assumed speed and proportion of heavy vehicles technical specification p31 01c 20 cerc 2021 when the road source is located within a street canyon adjustments are made to the dispersion calculations as outlined in the next section 2 3 street canyon influencing road source dispersion street canyons play an important role in altering the dispersion of pollutants from roads hood et al 2021 adms local incorporates the effects of street canyons on the dispersion from road sources in two ways firstly within street canyons concentrations resulting from road source dispersion are modified based on the properties of the street canyon using factors derived from analysis of more detailed modelling with the advanced street canyon module in adms urban then outside the street canyon the dispersion from the road is modelled as a porosity weighted combination of the standard no canyon road source representing the dispersion of pollutants through gaps in the canyon walls and a volume source representing the release of material from the top of the canyon the canyon top volume source for a given road segment has the same horizontal footprint as the canyon containing that segment and is vertically centred on the canyon top building height with a depth of twice the initial road mixing height 2 m the porosity of the canyon is calculated based on the proportion of the road without buildings on the side for a given road source segment located within a canyon of width w and average height h the concentration contribution at an output point within the canyon is calculated as a coverage weighted combination of the standard no canyon road source segment concentration and the full canyon unit coverage segment concentration at that output point fig 3 the full canyon concentration c c a n y o n at a given output point within the canyon is calculated using a series of factors applied to the standard no canyon road source segment centrepoint concentration c ˆ n o n c a n y o n at the release height 1 m in the following way 10 c c a n y o n c ˆ n o n c a n y o n f l a t e r a l f c a n y o n f h w f φ f c a n y o n f h w f φ where f c a n y o n introduces a vertical profile that adjusts the non canyon centrepoint concentration c ˆ n o n c a n y o n to a canyon centreline concentration at the output height z f h w adjusts concentrations to account for the canyon aspect ratio h w f φ adjusts concentrations to account for the wind direction relative to the road centreline φ f l a t e r a l is a lateral factor to convert from canyon centreline to canyon off centreline and superscripts and denote factors relating to along road parallel and across road perpendicular upstream flow respectively table 3 summarises the canyon factor dependencies and provides variable definitions values for the four factors have been derived from analyses of concentration outputs from multiple runs of adms urban where the model has been configured to represent a single in canyon road source using the advanced canyon model hood et al 2021 a wide range of representative source parameter ranges have been considered for example aspect ratios up to 2 0 3 r w w 0 95 and wind speeds and atmospheric stabilities representative of those experienced in urban areas example plots of the vertical variation of the core canyon factors for parallel and perpendicular wind directions are shown in fig 4 the increased concentration factor on the upwind kerbside relative to above canyon wind direction represents the effect of recirculating flow within the canyon 2 4 volume source dispersion dispersion from a volume source is calculated in a similar way to a road source but with each element representing a vertical slice of finite length and height the concentration contribution from an individual crosswind element of length l s height l v vertical mid point z c and source strength q in neutral and stable conditions is given by 11 c x y z q 4 u erf y l s 2 2 σ y erf y l s 2 2 σ y erf z l v 2 z c 2 σ z erf z l v 2 z c 2 σ z reflection terms a modified profile is adopted in convective conditions 2 5 chemistry a wide range of chemical processes which take place over large temporal and spatial scales are modelled within the regional models however chemical reactions on short time scales also strongly affect concentrations of some pollutants for instance nox chemistry strongly influences no2 and o3 concentrations in the vicinity of road sources adms local incorporates some important local scale reactions taking account of both locally emitted pollutant as modelled by adms local and regional pollutant from the regional model specifically adms local includes two chemistry schemes using the same basic principles and approach in both schemes dispersion processes are modelled first to generate an initial estimate of concentrations for each emitted pollutant then individual travel times are calculated as the time taken for each plume to reach each receptor from a source if only one plume is being modelled this travel time is the time over which the chemical reactions are applied in the case of multiple sources a concentration weighted average travel time is used an adaptive stepping technique is adopted to reduce finite difference errors during periods of fast chemical conversions while allowing faster computation during periods when concentration changes are small the two chemistry schemes are described in more detail below these schemes are also used in adms urban nox grs chemistry scheme the generic reaction set grs is used to model chemical processes that occur due to emissions of oxides of nitrogen nox consisting of no and no2 and volatile organic compounds vocs and their reactions with ozone o3 this scheme venkatram et al 1994 is a semi empirical photochemical model that reduces the complex series of chemical reactions involving the above pollutants to just seven reactions n1 n7 in the following list in adms local an additional eighth reaction is added for the reaction between no and o2 which is important for very high values of no n1 roc hv rp roc n2 rp no no2 n3 no2 hv no o3 n4 no o3 no2 n5 rp rp rp n6 rp no2 sgn n7 rp no2 sngn n8 2no o2 2no2 roc reactive organic compounds a fixed proportion of vocs rp radical pool sgn stable gaseous nitrogen products sngn stable non gaseous nitrogen products reactions n3 n4 and n8 represent exact chemical reactions the remaining equations are approximations involving bulk species during the day the reaction rate for reaction n3 is calculated by assuming the background values are in chemical equilibrium equation 21 with an upper limit applied based on the theoretical maximum available solar radiation no such adjustment is made at night time the full list of reaction coefficients are given below where rn1 corresponds to the reaction coefficient used for reaction n1 above and so on 12 r n 1 aroc r n 3 exp 4700 1 t 0 k 1 316 13 r n 2 358 1 6 t 0 k 14 r n 3 min r n 3 k max r n 3 15 r n 4 4 405 10 2 exp 1370 t 0 k 16 r n 5 1 6 17 r n 6 2 10 3 18 r n 7 2 10 3 19 r n 8 4 61 10 10 exp 530 t 0 k where 20 r n 3 k 8 10 4 exp 10 k 7 4 10 6 k k 0 0 k 0 21 r n 3 c bgd o 3 c bgd n o r 4 c bgd n o 2 k is incoming solar radiation in w m 2 k max is the maximum value of k possible assuming a solar elevation angle of 90 and zero cloud cover and aroc represents the weighted reactivity coefficient for roc taken to be 0 05 by default as a result of validation work though this value is modifiable via adms local s primary input file r 3 is calculated from the background concentrations of ozone no and no2 c bgd o 3 c bgd n o and c bgd n o 2 respectively if any of these values are zero r 3 r 3 k is used the above formulation assumes that a photochemical reaction rate derived from the background concentrations is more representative than a rate calculated from solar radiation values in case of erroneous background data a minimum of r 3 k max and r 3 is taken to ensure that the reactions occur at a realistic rate application of this chemistry scheme can affect the modelled concentrations of nox no2 and o3 so2 particulate sulphate chemistry scheme the sulphate reaction scheme represents the generation of particulate sulphate from so2 the scheme is the same as that currently used in adms urban and in the emep model tsyro 2001 the three reactions considered are as follows s1 2so2 o2 2so3 s2 so3 h2o h2so4 s3 h2so4 2nh3 nh3 2so4 with a rate constant r s 1 defined as 22 r s 1 3 10 6 2 10 6 sin 2 π day 80 365 where day is the julian day of the year the current formulation is correct only for model domains in the northern hemisphere reactions 2 and 3 are assumed to be instantaneous oxygen o2 water vapour h2o and ammonia nh3 are assumed to be available in abundance practically the model can therefore perform the above three reactions in one step namely 2so2 nh3 2so4 using rate constant r s 1 this ammonium sulphate is added to the particulate matter concentrations leading to reductions in so2 and increases in pm2 5 and pm10 2 6 output specifications to allow for concentrations to be calculated at street scale resolution adms local takes as input a full list of points at which the concentrations should be calculated in simple text file format pollutant concentrations can be calculated at hourly resolution and or as period averages in order to reduce output file size when generating high temporal and spatial resolution pollutant concentration datasets the majority of output data are written to netcdf format files a subset of meteorological data used by the model is also output to this file 2 7 model execution adms local run times are dependent on the number of hours being modelled as well as the number of output points and the number of road sources within the domain when creating high resolution pollution datasets the number of output points used by adms local is proportional to the number of roads to ensure the high concentration gradients across roads are captured within the maqs health system the adms local domains are regional model grid cells with one domain for each grid cell these different domains are run in parallel by the system as such the overall run time of the maqs health coupled system is dependent on the execution time of the most densely trafficked regional model grid cell within the domain as this cell will contain the most road sources and therefore the most output points a consequence of this is that coupled system runs using rms with fine resolution for example a resolution of 1 km will run faster than those with lower resolution because the adms local domains are smaller the example highly trafficked 1 km domain with 1 km buffer containing 1432 road sources and 2456 output points described in section 3 4 took approximately 5 5 hours to run a full year of hourly data this is indicative of run time of the overall system for a large domain providing sufficient computational resources are available validation runs where output is only obtained at a few monitor locations execute much quicker 3 model evaluation this section presents an evaluation of the standalone adms local model necessary prior to its use within the maqs health coupled system for simple configurations where minimal differences between adms local and adms urban predicted concentrations are expected this has been achieved by an inter comparison of adms urban and adms local modelled concentrations section 3 1 a new evaluation study of three continuous monitoring locations adjacent to relatively isolated uk major roads has been carried out using adms local only and is described in section 3 2 to demonstrate real world model performance for open roads where detailed traffic data are available further evaluation is performed by using evaluation databases that have been compiled for adms urban evaluation and previously published in the literature sections 3 3 for individual street canyons and 3 4 for city scale an example of high resolution output is shown in section 3 5 for each of the evaluation studies the statistics defined in equations 23 27 below are used to evaluate the modelled concentrations m in relation to the observed concentrations o where n is the number of pairs of modelled and observed concentrations a bar indicates the mean value m for example and a subscript indicates a single parameter value ranked between unity and n m i for example mean observed and modelled concentrations are presented alongside mean bias m o and the ratio of the standard deviation of modelled concentrations σ m to the standard deviation of observed concentrations σ o fractional bias f b is a measure of the mean difference between the modelled and observed concentrations 23 f b m o 0 5 o m normalised mean square error n m s e is a measure of the mean difference between matched pairs of modelled and observed concentrations 24 n m s e m o 2 m o pearson s correlation coefficient r is a measure of the extent of a linear relationship between the modelled and observed concentrations 25 r 1 n 1 i 1 n m i m σ m o i o σ o fraction of modelled hourly concentrations within a factor of two of observations f a c 2 is given by the fraction of model predictions that satisfy 26 0 5 m i o i 2 0 the statistic used to quantify overall model performance is the index of agreement ioa willmott et al 2012 given by 27 i o a 1 i m i o i 2 i o i o i m i o i 2 i o i o 2 i o i o i m o i 1 i m i o i 2 i o i o the ioa spans between 1 and 1 with values approaching 1 representing better model performance 3 1 road source model intercomparison a single road source has been configured in both adms local and adms urban with length 1 km and carriageway width 25 m the traffic activity data flows speeds and corresponding emission rates are representative of a busy london road including hourly time variation consistent hourly meteorological data are used in both models with the processed meteorological parameters output from the adms urban meteorological processor used as input to adms local in order to model the effects of near road nox chemistry the model requires hourly background concentrations of nox no2 and o3 hourly values measured upwind of london are used as input both models are run to generate hourly nox and no2 concentrations at 1 m above the ground for a set of output points perpendicular to the road centreline to a distance of 150 m fig 5 compares predicted annual average concentrations left hand axis and concentration ratios right hand axis between the two dispersion models with the full lines showing the adms local concentrations and the cross symbols indicating the corresponding adms urban values the concentration and concentration ratio cross sectional profiles for these annual means are indistinguishable in the figure demonstrating consistency between the implementation of the dispersion and chemistry equations in the model codes there are small differences on an hour by hour basis maximum standard deviation of percentage differences are less than 0 1 0 01 for nox and less than 2 0 0 1 for no2 for the road centreline receptor the nox concentration profile captured by both models in fig 5 shows the sharp decrease in concentrations that occurs in the vicinity of trafficked roads in urban areas for this example annual nox concentrations reduce to approximately 72 of the road centreline concentration at the carriageway edge 12 5 m and to 26 by 50 m the no2 concentration gradient is less steep with corresponding fractions of centreline concentrations of 80 at the carriageway edge and 43 at 50 m from the centreline the difference between nox and no2 near road concentration gradients is a consequence of nox being influenced primarily by dispersion processes whereas no2 is also influenced by fast chemical conversion of no to no2 and the higher relative contribution of the background concentrations the specified ratio of emissions no2 nox is 22 the road centreline annual average concentration ratio no2 nox is 38 with the increase of 16 above the emissions ratio showing the very local influence of chemical reactions and the influence of the background concentrations the concentration ratio increases to 69 within 50 m of the road centreline and is 83 at a distance of 1 km from the road these values are typical of observed values demonstrating the efficacy of adms local in representing dispersion and chemical processes that occur in the vicinity of road sources 3 2 open road model performance evaluation in order to evaluate real world model performance for isolated open roads concentrations at three continuous monitoring sites on england s strategic road network have been modelled using adms local cambridge salford and sandy each site is located adjacent to a single major road for which hourly traffic activity data are available specifically vehicle counts and speeds and simple fleet composition data highways england 2016 these traffic data were used for calculating hourly nox and no2 emissions based on the uk emissions factor toolkit version 10 1 england motorway emission factors department for environment food rural affairs 2020 with real world adjustments based on remote sensing data consistent with the approach described in hood et al 2018 the modelled years were chosen for adequate availability of the hourly traffic data which varies between locations and years all emissions from nearby urban areas are taken from the uk national atmospheric emissions inventory naei modelled as a 1 1 km2 grid source with 10 m depth and an initial fraction of nox released as no2 set to 12 meteorological data from local measurement sites were used as input to adms local while hourly average background concentration data for nox no2 and o3 were obtained from rural monitoring sites in the relevant regions the three sites are summarised in table 4 and map visualisations for each site are given in fig 6 there are additional uncertainties for some sites for example only southbound traffic data for the carriageway nearer to the monitor were available for the sandy site leading to an assumption of equal traffic flow on the more distant northbound carriageway at the sandy site there is a 4 m wide parking bay between the main carriageway and the monitor in addition to the monitor being located in a vehicle sales car park that is adjacent to an mot centre while at the salford site there is a 3 m wide hard shoulder between the main carriageway and the monitor none of these areas were included in the modelled road carriageways but they may in reality have intermittent emissions there is some uncertainty relating to the monitor inlet heights in particular with heights of both 1 5 m and 3 m stated in different sources for the sandy monitor the height of 1 5 m from a local authority report was used for the model configuration the modelling of the cambridge and sandy sites includes periods when covid 19 restrictions were in force in the uk during 2021 and 2020 respectively changes to traffic flow patterns along the road adjacent to each monitor are included in the detailed traffic data but changes in other activities and hence emissions may not be represented model evaluation statistics for the open road studies are given in table 5 these statistics show best performance at the cambridge site with fractional bias values of 0 07 0 10 for nox no2 and correlation coefficients of 0 75 0 72 the other sites generally show somewhat better performance for no2 than nox for example salford correlation of 0 69 for no2 but 0 65 for nox the largest magnitudes of fractional bias occur at the sandy site 0 50 0 38 for nox no2 where there were the greatest uncertainties in model input data also reflected in poorer nmse values 1 49 for nox 0 62 for no2 the nox correlation value at this site is lower than for the other sites 0 58 whereas the no2 correlation value 0 67 is similar to the other sites further analysis has been carried out for no2 as this is a regulatory pollutant of interest for local modelling frequency scatter plots of hourly no2 concentrations for each of the three major road modelling sites are shown in fig 7 the graph for cambridge shows the greatest concentration of hours near the 1 1 line while for salford there is more scatter for sandy the model generally under predicts in addition to a small number of hours with unusually high observed concentrations which may indicate specific local emission events in the parking bay or vehicle sales mot centre adjacent to the monitor which are not included in the modelled emissions the variation of monthly average modelled and observed no2 concentrations for each of the major road sites are shown in fig 8 the pattern of increased concentrations in winter and decreased concentrations in summer is seen in both modelled and observed data for all sites for the cambridge site the modelled concentrations underestimate the observed values for the first few months of the year this coincides with a period of covid 19 restrictions in the uk which may have led to increased domestic combustion emissions from two adjacent residential properties a change which is not represented in the modelling for salford and sandy the monthly variation of concentrations is generally well represented although there is a consistent underestimate of concentrations at sandy likely due to omission of local emissions already discussed 3 3 street canyon model performance evaluation the performance of the parameterised canyon model within adms local is evaluated by comparison with measurement data from the optimization of modelling methods for traffic pollution in streets research network study campaigns trapos 2020 these data have been used extensively for validation of the ospm model kakosimos et al 2010 and for the evaluation of the advanced street canyon model within adms urban hood et al 2021 the trapos dataset consists of air pollutant concentration measurements recorded in three street canyons jagtvej copenhagen sweden goettingerstrasse hanover germany and schildhornstrasse berlin germany the datasets include meteorological measurements and background pollutant concentrations recorded in the vicinity of the sites in addition to traffic flow data that can be used to estimate emissions using available emission factor datasets hbefa keller et al 2017 table 6 summarises the campaign sites and datasets readers are referred to hood et al 2021 for a more comprehensive description of the measurement datasets evaluation for nox is presented in order to focus on dispersion effects in these canyons and due to additional uncertainties relating to no2 and co emissions in order to understand the importance of accounting for street canyons in detail model predictions when no canyon has been modelled are compared with the observations alongside the street canyon modelling table 7 summarises model evaluation statistics for the trapos sites calculated from hourly modelled and measured concentrations of nox adms local demonstrates good performance when the model is configured to account for street canyons canyon m cases for instance at all three sites the correlation exceeds 0 8 and the number of points within a factor of two of the observations is greater than 0 9 the likely explanation for the slight under prediction of concentrations at jagtvej fb 0 206 and over prediction at goettingerstrasse fb 0 074 is uncertainty in the emissions estimates the emission rates for each site were calculated using the hbefa database and detailed vehicle count data from the campaigns however assumptions had to be made relating to traffic situation congestion level and traffic speeds regional variations in the introduction of catalytic converters into the vehicle fleet during the period of the trapos studies also increases the uncertainty in emissions when the influence of street canyons on dispersion is neglected no canyon m cases there are large negative fractional biases at all sites and the catch all ioa statistics are significantly lower than for the canyon m case which demonstrates the importance of accounting for street canyons when using models to predict aq in urban locations where buildings alter the flow and dispersion of pollutants the relative importance of spatial and temporal variations of concentration is shown in fig 9 for the three trapos sites horizontal dashed lines show the inter quartile range of observed and modelled hourly concentrations while vertical lines show the variation of average modelled concentration with output height there are differences in inlet measurement height between the three sites but the variation of average modelled concentration with height is much smaller than the variation of hourly concentrations the modelled inter quartile concentration ranges match the observed ranges fairly closely although there is some underestimate of higher concentrations at jagtvej and overestimate of higher concentrations at schildhornstrasse the observed and modelled variations of hourly nox concentrations with observed wind speed and direction at each of the trapos study monitoring sites are shown in fig 10 as polar plots these plots demonstrate that adms local dispersion modelling with parameterised canyon effects captures the main observed concentration distribution features for all sites of increased concentration on the upwind side of the canyon relative to upstream wind direction due to recirculating flow within the canyon as well as elevated concentrations for along canyon wind directions due to channelled flow within the canyon 3 4 city scale adms local performance evaluation the next stage of adms local evaluation is the application of the model at the city scale data from london s extensive reference monitor data network with the highest data capture 45 sites is used to evaluate the model over a domain of approximately 1500 km2 for a full year 2012 this model configuration is derived from one previously used to evaluate adms urban performance hood et al 2018 but with some simplifications including the use of homogeneous meteorological site properties rather than spatially varying urban canopy parameters and accounting for point sources within grid sources rather than explicit modelling of dispersion from individual point sources a range of air quality pollutants are modelled nox no2 o3 pm10 and pm2 5 the model s ability to account for near road chemical reactions can be tested by evaluating no2 and o3 alongside nox london s air quality monitoring network includes reference measurement stations located at urban background industrial roadside and kerbside locations for the purpose of this adms local evaluation the monitors are categorised as near road and background table 8 summarises model performance by site type for the five pollutants listed above overall the evaluation statistics reflect good model performance for all pollutants and site types with fractional biases less than 0 16 correlations greater than 0 55 number of points within a factor of two of the observed greater than 0 62 and ioa s greater than 0 60 for all pollutants there is a consistency in nox and no2 statistics with average modelled observed ratios being similar for both pollutants 0 83 0 90 for urban background sites and 1 01 1 01 for near road sites for nox no2 respectively the spread of model performance across all near road and background sites is shown using scatter plots of annual average concentrations in fig 11 the model predictions for all sites are within a factor of two of observed concentrations for all pollutants with the exception of two nox measurement sites which are just outside this limit shown by the dashed lines many of the sites have modelled predictions within smaller factors of the observed values with a good representation of the spread of concentrations between urban background and near road sites concurrent analysis of nox no2 and o3 concentrations can be helpful for assessing the model s representation of nox chemistry fig 12 shows the diurnal variation of concentrations averaged over 11 background and 9 near road sites in london which monitor all three of these pollutants in general the modelled concentrations match the magnitude and variation of measured concentrations closely at both background and near road sites for all three pollutants although there is some over estimation of daytime o3 at near road sites 3 5 high resolution output fig 13 shows an example pollution map of annual average no2 concentrations for a central london 1 km 1 km domain calculated by adms local concentrations correspond to the model configuration evaluated and discussed in section 3 4 elevated concentrations within and adjacent to the roads with explicit emissions are well resolved modelled concentrations vary along these roads due to changes in the parameterised building properties used as input to the model building height and coverage and canyon width emissions from less busy roads are included within the model grid sources which is why these roads show no local elevation in concentration the 200 m 200 m inset shows a section of concentration map in detail with explicit buildings overlaid shaded by building height higher concentrations clearly correspond to street canyons that are created by taller buildings with fewer gaps between them because the pollution is trapped within the canyons where there are more gaps between buildings the in road concentrations are relatively low but concentrations in adjacent areas are higher because the pollution disperses at ground level the resolution to which the dispersion model reflects the variation in building porosity relates to the number of road segments that are modelled shorter segments allow the influence on dispersion of changes in building dimensions along a single road to be accounted for however road segment lengths must be sufficiently long to encompass multiple buildings as adms local does not model flow around explicit buildings street canyons are assumed to be symmetric as can be seen from the near symmetric dispersion from road sources in fig 13 asymmetry arises from prevailing meteorology and other factors more detailed local modelling of street canyons including canyon asymmetry can be allowed for within maqs health by use of the adms urban model which includes an advanced street canyon module hood et al 2021 modelled concentrations throughout this example area of central london exceed the uk and eu air quality objective for annual average no2 of 40 μg m3 adms local modelled concentration contours for nox o3 pm2 5 and pm10 are presented in fig 14 for the same 1 km 1 km cell as shown in fig 13 the pattern of concentrations for nox is very similar to those for no2 the concentrations of o3 have an inverse relationship to those of no2 with reduced concentrations at roadside and higher concentrations at background locations due to chemical titration by local traffic emissions there is a smaller magnitude of increment between background and in canyon concentrations for particulate pollutants than for no2 but a similar overall pattern of elevated concentrations within the deepest and most continuous canyons with qualitatively similar distributions for both pm2 5 and pm10 uk annual average air quality objectives of 25 μg m3 for pm2 5 and 40 μg m3 for pm10 are only exceeded in a few locations in general at junctions and road carriageways which would be excluded from exposure calculations there are no uk air quality objectives defined in relation to human health for annual average nox or o3 4 discussion and further work for the purposes of health research models have distinct advantages over measurements in view of their capability both to generate high resolution spatial coverage over large domains and to make future projections of air quality under the spf caf the uk government has funded the development of a regional to local scale aq modelling system for use in health research maqs health this paper presents the scientific formulation and evaluation of the adms local road source dispersion model which has been developed to be a local model component for use within the maqs health the adms local model formulation is derived from the road source component of the adms urban aq model for evaluation purposes this paper presents an intercomparison of the adms local and adms urban models the small differences in modelled concentrations relate to differences in computational implementation the choice of features chosen for inclusion in adms local takes account of the requirements of maqs health as a whole for instance the model uses meteorological parameters generated by regional models as input road and volume sources can be explicitly represented in the model thereby resolving sharp near road concentration gradients the influence of the urban morphology and infrastructure on pollutant dispersion is allowed for through the modelling of street canyons including allowance for pavements urban canopy flow and the impacts of road elevation the model accounts for simplified short timescale nox and sox chemical reactions and output points can be specified at any horizontal or vertical location within the model domain output may be at specific receptor locations for instance postcodes or on an irregular grid which can be used to generate pollution maps hourly concentrations are calculated the scientific formulation of adms local is evaluated through the use of three measurement datasets the first for open major roads the second specifically for street canyons and the third testing overall model performance within an urban domain the statistical evaluation results presented show that the model performs well for instance the ioa values exceed 0 60 for the three open major road sites 0 75 for all three street canyon studies and 0 60 for each of the pollutants modelled for the study of overall performance the model execution times of adms local are dependent on the number of road sources and output points within the domain within maqs health these domains will be the size of regional model grid cells with the system running each of these separate domains in parallel the overall runtime of the system is then dependent on the densest of these regions and finer scale resolution regional model configurations can have faster run times than corresponding coarser resolution model runs adms local is structured so that model enhancements would be relatively straightforward to implement these could include for example refinement of the canyon parameterisations making use of the adms urban advanced canyon model further comparison with measurements and allowance for canyon asymmetry personal exposure of individuals as they follow a particular route is often of interest for health studies such pollutant dose calculations could be facilitated by the introduction of mobile receptors temporally varying locations for model output into adms local subsequent papers will present details of the full maqs health coupled system and associated detailed system evaluation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding this work was supported by the uk government s strategic priorities fund spf clean air programme administered by the met office grant agreement p107531 
25571,the importance of rainfall patterns in triggering landslides has been recognized worldwide in this study the triggering and evolution of rainfall induced landslides were investigated by a regional scale physical model based on one dimensional richard equation and infinite slope model where a bounded random cascade model brcm was adopted to generate random rainfall patterns rrps for rainfall events derived from extreme value analysis the results indicated that the rrps affect the evolution of triggered landslide and the cumulative landslide area the shape of rrps determined the time evolution of triggered landslide due to the randomness of rainfall peaks under rrps and limited hydraulic conductivity of soil an upper bound existed for the cumulative landslide area under rrps and the dispersion of the cumulative landslide area increased with an increase in the return period this study aids in the understanding of the diversity and discipline of the evolution of rainfall induced landslides keywords random rainfall pattern extreme value distribution triggering and evolution rainfall induced landslides unsaturated loess 1 introduction rainfall induced landslides are a common geological hazard worldwide an et al 2016 wang et al 2020 guo et al 2022 with global climate change in recent years rainfall events have increased in intensity dehn et al 2000 dore 2005 sandink et al 2016 kristo et al 2017 consequently this may affect the stability of natural and artificial slopes dixon and brook 2007 and cause landslides in the future crozier 2010 dijkstra and dixon 2010 damiano and mercogliano 2013 chaithong et al 2017 komori et al 2018 rainfall patterns determine the temporal distribution characteristics of rainfall intensity which directly affect the flux boundary conditions of the soil surface kristo et al 2017 and the infiltration process of rainwater in the soil assouline and mualem 1997 foley and silburn 2002 hawke et al 2006 alvioli and baum 2016 the infiltration changes the pore water pressure and causes the stress of the soil skeleton to weaken which may results in a decrease in shear strength and the occurrence of landslides to date many studies have analyzed the slope stability under different rainfall patterns on the scale of site specific slopes typical rainfall patterns including advanced intermediate and delayed rainfall tsaparas et al 2002 rahimi et al 2011 kristo et al 2017 wu et al 2017 lin and zhong 2018 ran et al 2018 and random rainfall patterns rrps white and singham 2012 tang et al 2018 yuan et al 2018 have been considered in the evaluation of slope stability the research results show that differences in slope stability under different rainfall patterns are mainly characterized by the time and type of slope failure and the minimum safety factor of the slope in particular compared with the stability analysis of a single slope under rrps considering uniform rainfall is less accurate and may overestimate the rainfall threshold required to cause slope instability white and singham 2012 moreover these studies emphasize the importance of rainfall patterns in the stability analysis of site specific slopes however in previous studies where regional scale rainfall induced landslide hazards were assessed the processing of rainfall events tended to focus on changes in rainfall threshold cumulative rainfall average rainfall intensity and rainfall duration guzzetti et al 2007 martelloni et al 2011 park et al 2013 piciullo et al 2016 melillo et al 2018 guo et al 2019 dai et al 2020 particularly the rainfall pattern was often assumed to be uniform or similar to other typical rainfall patterns when performing regional slope stability analysis using data from the prediction of extreme rainfall salciarini et al 2008 kawagoe et al 2010 grelle et al 2013 bregoli et al 2014 kirschbaum et al 2015 schilirò et al 2015 komori et al 2018 chen et al 2019 lee et al 2020 hürlimann et al 2022 obtained using the generalized extreme value gev distribution hosking and wallis 2005 the occurrence of rainfall induced landslides under different return periods from 1 to 100 years or durations from 6 h to 72 h has been reported in previous studies salciarini et al 2008 grelle et al 2013 chen et al 2019 the cumulative percentage of landslide area for a certain area increased with an increase in the recurrence interval that is return period and rainfall duration however they failed to provide a specific rainfall pattern consequently random fluctuations in rainfall intensity during the actual rainfall process have been ignored schertzer and lovejoy 1997 although random rainfall time series were performed to investigate the impacts on the regional landslide assessment fan et al 2020 the random nature of rainfall intensity over time is artificial and not consistent with the actual rainfall history of the local area further based on previous studies on the stability analysis of site specific slopes only the characteristic variables of rainfall events adopted particularly the average rainfall intensity and duration are not sufficient to cover all the rainfall characteristics that may cause slope failure in fact many different rainfall patterns can actually exist for rainfall events with the same average rainfall intensity and duration peres and cancelliere 2014 therefore in the assessment of regional rainfall induced landslide hazards ignoring the inherent randomness characteristics of rainfall intensity over time may be insufficient to reflect all the information of potential landslide hazards to overcome this shortcoming we reconsidered a series of rainfall events under different return periods and further downscaled the rainfall to yeild a series of random rainfall patterns the bounded random cascade model menabde and sivapalan 2000 was introduced to describe the random fluctuating characteristics of rainfall intensity in the actual rainfall process the recurrence of rainfall was performed by the idf rainfall intensity duration frequencey curve which is derived from the gev distribution tobin 1989 hosking and wallis 2005 lee et al 2020 and can provide basic information of rainfall except the rainfall pattern the above two steps constituted the rainfall generator and it was incorporated into a regional finite element physical model based on a one dimensional flow equation and an infinite slope model lizarraga et al 2017 song et al 2020 for rainfall induced landslide assessment and the simulation results were visualized using a geographic information system gis platform this study aimed to investigate the triggering and evolution of landslides by considering random rainfall patterns that are derived from rainfall history it can provide a more complete insight for decision makers to manage the natural hazards associated with rainfall induced landslides in the future 2 theoretical basis 2 1 recurrence of rainfall events gev distribution the probability of extreme events based on past observations was predicted employing the extreme value analysis el adlouni et al 2007 the gev distribution was adopted to perform extreme events with the assumption that the relative maximum values are stable and independent coles et al 2001 further a maximum value was selected for the timescale of a year and subsequently the maximum values of years were sequenced to estimate the model parameters in other words the gev distribution was combined into three types 1 gumbel distribution 2 frechet distribution 3 weibull distribution tobin 1989 the cumulative distribution form is 1 f g e v x e x p 1 ζ x λ η 1 ζ where ζ η λ are the shape scale and location parameters that control the gev model form respectively each type refers to one case 1 for ζ 0 gumbel distribution 2 for ζ 0 frechet distribution and 3 for ζ 0 weibull distribution the maximum extreme value to predict extreme events such as rainfall storms and earthquakes is often determined using the gumbel model finlay et al 1997 zêzere et al 2008 therefore the gumbel model was used to obtain the cumulative intensity frequency duration of rainfall events the cumulative distribution form of gumbel model grelle et al 2013 lee et al 2020 is 2 f g u m x exp exp x λ η where x η λ denote the scale and location parameters which control the gumbel model form then the return period for a specified rainfall event can be expressed as 3a t 1 1 f g u m x the rainfall event obtained using the gumbel model exhibits three macroscopic characteristics cumulative rainfall depth return period and rainfall duration which are shown in fig 1 further the linear combination of the l moment or probability weighted moments pwms method ramachandra rao and hamed 2000 hosking and wallis 2005 gubareva and gartsman 2010 drissia et al 2019 lee et al 2020 was used to estimate the gumbel model parameters 3b β 1 b 0 λ γ η 4 β 2 b 0 2 b 1 l n 2 η where β 1 and β 2 are the first and second population l moments respectively b 0 and b 1 are the first and second population pwms of the gumbel model respectively γ is euler s constant and η and γ are the scale and location parameters of the gumbel model respectively furthermore b ˆ r probability weighted moments from the r th sample is obtained by hosking et al 1985 for r 0 5 b ˆ r n 1 j r 1 n j 1 j 2 j r n 1 n 2 n r x j n where n is the size of the sample which is composed of the annual maximum value for rainfall events with a certain duration j is the ascending order for samples x 1 n x 2 n x j n x n n are the sequences of annual maximum values in ascending order moreover when r 0 6 b ˆ 0 1 n j 1 n x j n whereas when r 1 b ˆ 1 is expressed as 7 b ˆ 1 1 n j 1 n j 1 n 1 x j n after obtaining b ˆ 0 and b ˆ 1 the estimated gumbel model parameters η ˆ and λ ˆ can be exported as 8 η ˆ b ˆ 0 2 b ˆ 1 l n 2 9 λ ˆ γ η ˆ b ˆ 0 2 2 within rainfall temporal patterns bounded random cascade model however in the previous step only the cumulative rainfall depth of a single rainfall event was obtained thus to obtain the rainfall distribution value on a smaller time scale temporal downscaling for individual rainfall events must be performed which can be modeled employing the bounded random cascade model brcm based on the characteristics of the stochastically self affine of rainfall time series menabde et al 1997 1999 menabde and sivapalan 2000 the breakdown coefficient was used to downscale the timescale of rainfall events wherein in each scaling process the parent interval was divided into two equal subintervals further the rainfall intensity of the subinterval was calculated via the multiplication of the rain intensity of the parent interval by the corresponding breakdown coefficient as shown in fig 2 10 q k i q 0 j k w j p i j i 1 2 3 2 k where k is the breakdown step q k i is the rainfall intensity for the i sub interval in the decomposition of the kth step while downscaling q 0 is the rainfall depth which denotes cumulative rainfall depth under a specific return period in the previous section w is sequence of breakdown coefficients that is the ratio of the rainfall intensity of the parent interval transferring to a subinterval during downscaling and p is the function that w obeys upon determining the cumulative rainfall the rainfall intensity of the sub units during each decomposition can be determined by the breakdown coefficient w the breakdown coefficient w can be characterized by b e t a distribution menabde and sivapalan 2000 11 p w 1 b a w a 1 1 w a 1 where b a is b e t a function including parameter a which varies with the time scale and a 0 moreover a depends on the size of the considered time scale and can be further expressed as 12 a t a 0 t h where t d r 2 k is the timescale of the basic cell after the k th breakdown and k 1 2 3 n a 0 and h are constants that control the variation of a the parameter a can be estimated employing the method of sample moments molnar and burlando 2005 13 a 1 8 v a r w 0 5 where the breakdown coefficient w of the sample is calculated by the non overlapping adjacent pairs of rainfall measurements rupp et al 2009 14 w j τ r j τ r j τ r j 1 τ j 1 3 5 n r 1 where r j τ is the rainfall depth recorded over the time interval of length τ at position j in the time series and n r is the total number of records at time scale τ 2 3 governing equation richard equation considering the water mass balance equation and darcy s law the governing equation for one dimensional seepage flow considering unsaturated soil can be expressed as follows richards 1931 16a n s r t k 2 h k z c o s α where n is the soil porosity h is the water pressure head s r is the degree of soil saturation k is the hydraulic conductivity z is the normal direction of the slope surface α is the slope angle during rainfall infiltration the soil moisture content and hydraulic conductivity rate continue to change thus the gardner model was used to describe the constitutive relationship of soil hydraulic variables to capture the changes in soil hydraulic properties water retention curve wrc is expressed as 16b θ h θ r θ s θ r e x p β h further hydraulic conductivity function hcf is expressed as 17 k h k s e x p β h where θ and k are the soil water content and hydraulic conductivity respectively θ r and θ s are the residual and saturated volumetric water contents of the soil respectively k s is the saturated hydraulic conductivity and β is a parameter that determines the sensitivity of soil water content and hydraulic conductivity to matric suction 2 4 stability model infinite slope model to assess the stability of an unsaturated slope the slope safety factor calculation based on the infinite slope model is described as the ratio of the critical slope stress level to the current actual stress level of the slope lizarraga et al 2017 song et al 2020 18 f s t a n φ t a n α 1 k s σ n e t where φ and α are the friction angle of the soil and slope angle respectively s is the suction σ n e t is the net stress and k is a parameter that quantifies the influence of suction increment on the increase in soil shear strength which can be described as k t a n φ t a n φ b fredlund et al 1978 2 5 implementation the calculation process for the triggering and evolution of rainfall induced landslides are performed through a regional physically based model solving by finite element method the part of performing spatially distributed analyses through the vectorization scheme was proposed by lizárraga and buscarnera 2018 in detail which involves three stages referred to input processing and output it is further elaborated and employed by lizárraga and buscarnera 2020 song et al 2020 in this study the procedure was further elaborated to solve the problems of random rainfall induced landslides for single rainfall event derived from extreme value analyses the first step requires a discretization procedure of the digital terrain to establish the fe model by which the mesh size and time steps are further difined and the input variables i e hydro mechnical properties initial and boundary conditions are assigned into each georegerenced grids this step can be readily conducted from geographical information system gis platform the processing stage is schematically shown in fig 3 rrps of the target time scale were generated after the cumulative rainfall depth and downscaling parameters a 0 and h for individual rainfall events were obtained further details in menabde and sivapalan 2000 each sequence of random rainfall over time i e random rainfall pattern from the rrp generator was identified and loaded by the finite element system through a step function and time steps then the transient infiltration and slope stability are performed for each slope unit at each time step if the safety factor of any unit at a certain time step was less than 1 its corresponding time and depth were marked and extracted into output column vectors and subsequently the unit was no longer involved in the subsequent search for soil failure otherwise the fe system keeps searching until the end of the rainfall and assigns non data index to the slope units which safe factor is larger than 1 at all the stage of the analysis then the next sequence of random rainfall from the rrp generator will drive the next analysis until the last sequence of the random rainfall finally the output column vector under each random rainfall pattern is remapped to a raster file in the original geographic coordinate system for postprocessing and visualization which can be performed through a gis platform 3 case study 3 1 location and characteristics the study area was located along the portion of provincial highway s316 in xinyuan county ili kazakh autonomous prefecture xinjiang china where the highway extends north south across awulale mountain that runs through the ili valley the total length of the route is 27 95 km the northern segment is connected to the s315 highway along the kashi river in an approximately east west direction whereas the south is connected to the g218 highway along the gongnaisi river in an approximately east west direction this is an important trade channel that connects the xinyuan and nilek counties further the mountainside is steep with a slope angle ranging between 25 and 40 in addition the mountains on both sides of the river are primarily composed of granite porphyry diorite glutenite and mudstone and the slope surface is covered with significant amount of loose loess the local climate is warm and humid with an annual average precipitation exceeding 400 mm rainfall was concentrated from april to october and approximately 50 of the annual precipitation was concentrated from april to june the lishi loess in the middle pleistocene and malan loess in the late pleistocene comprise the loess formation here the former is relatively dense exhibits good mechanical properties weak collapsibility and low water permeability the latter is classified as intoaeolian loess exhibiting high porosity well developed vertical joints strong collapsibility and relatively poor mechanical properties further owing to its poor physical and mechanical properties it provides the most basic geological conditions for driving loess landslides in this area liu et al 2017 the geological disasters in this area are dominated by loess landslides and loess debris flows and have resulted in excessive casualties and property losses in history in addition the abundant precipitation in the yili area has also resulted in creation of meteorological and hydrological conditions for the occurrence of loess landslides an and liu 2010 loess landslides are the primary type of geological hazards that occur on the surface of slopes along s316 they are typically small and medium sized landslides with a scattered distribution and shallow sliding surface depth and mostly occur on steep loess slopes approximately 300 loess landslides have occurred along the highway wei 2017 which can be used to establish a historical landslide dataset for the area moreover it provides an approximate location of the historical landslides and can provide a geographical reference for the potential recurrence of future landslides and numerical calculations in addition a georeferenced database based on local geographic characteristics was established for the stability analysis of regional slopes under rrps thus a digital elevation model with 12 m resolution obtained from the tandem x datasets of dlr http tandemx science dlr de was used for inputting terrain information in the area and locating the historical landslides wei 2017 as shown in fig 4 3 2 hydrological and mechanical characters initialization of the initial hydrological conditions supported by the data from the local geological survey report was performed in this area the distribution of the initial suction in the soil was calculated employing a wrc calibrated for malan loess considering the soil saturation measured gsr 2017 along the depth of the soil since the direct hydrological experiment of the local soil was not available the published test data of the same type of weathered malan loess in other loess areas wu et al 2011 wen and yan 2014 li et al 2015 2018 were used to provide a reasonable reference approved by song et al 2020 consequently based on the large amount of hydrological test data of the soil the soil water retention curve was maintained between the upper and lower limits further to compensate for the differences between different experiments to the best extent possible a mean curve based on the series of data scatter was used to calibrate the parameters of the water retention curve for the soil in this area as shown in fig 5 the hydraulic conductivity function of the malan loess was calibrated based on the saturated value of the hydraulic conductivity li et al 2016 as shown in fig 6 the parameter k which denotes the influence of suction increment on soil shear strength was calibrated by fitting direct shear test data reported by hu et al 2012 it must be noted that direct evidence specific for samples taken from the study area would be ideal for quantitative analyses since the information was not available the estimate derived from the data in this study should be regarded only as a first approximation which will be assessed against previous investigations wherein the natual randomness of rainfall intensity was neglected 3 3 random rainfall generation historical records of local daily rainfall for approximately ten years from january 2011 to december 2020 were obtained from the national oceanic and atmospheric administration climate prediction center of the united states noaa cpc data http www ncei noaa gov and are shown in fig 7 a series of maximum annual daily rainfall data was extracted from the historical rainfall records and further used to produce an ordered sample sequence that is x j n in formula 5 then the η ˆ and λ ˆ of the gumbel model for local rainfall recurrence were calculated based on formulas 6 to 9 the historical daily rainfall is shown in fig 7 and the cumulative intensity of daily rainfall events versus return periods are shown in the subgraph of fig 7 short term heavy rainfall was found to have occurred from june 7 2017 to june 8 2017 with a cumulative rainfall depth of approximately 271 mm in 24 h which caused several loess landslides with sliding depths mostly less than 5 m according to the survey song et al 2020 based on the calibrated gumbel model the return period of the extreme daily rainfall event with a cumulative rainfall intensity of approximately 271 mm in 2017 was approximately 519 years that is the rainfall event i3 t3 in the subgraph of fig 7 the timescale of the extreme rainfall event was further downscaled to reanalyze the effects of random rainfall patterns on landslides occurrence when it regressed consequently the cumulative rainfall depth was used as the total rainfall q 0 for downscaling and time downscaling of the rainfall event was performed based on the principal diagram in fig 2 additionally the rainfall events under the 50 y and 250 y return period that is the rainfall events i1 t1 and i2 t2 respectively were downscaled to provide a complete insight into the stochastic analysis to calibrate the parameters of the brc model for local rainfall events the hourly rainfall data of xinyuan county from january 2011 to october 2020 86 208 h were obtained using the second generation product merra2 tavg1 2d flx nx which was developed by nasa ges disc http disc gsfc nasa gov and focuses on atmospheric reanalysis in the satellite era the 1 h continuous rainfall records were used to synthesize adjacent non repeating 3 6 12 24 h rainfall datasets which were used for calculating a series of parameter a at different time scales in formula 12 using formulas 14 and 13 the series of parameter a was further used to calibrate the a 0 and h in formula 12 thereafter the beta distribution in formula 11 which regulates the breakdown coefficient w was determined and used to generate a series of w finally the rainfall intensity of each subinterval cell the random rainfall pattern was calculated based on the series of w using formula 10 moreover the ratio of two gamma distributed random variables is distributed according to the beta distribution mood et al 1974 if x1 and x2 are two independent random variables belonging to the single parameter gamma pdf then y1 x1 x1 x2 and y2 x2 x1 x2 are random variables distributed according to the beta distribution in formula 11 with an identical parameter a thus the procedure for generating the breakdown coefficient w at each cascade level is to simply generate two gamma distributed numbers x1 and x2 with an identical parameter a and then form the ratios w1 x1 x1 x2 and w2 x2 x1 x2 details of the procedure can be found in menabde and sivapalan 2000 the procedure of calibration and generation was implemented using matlab programming a total of seven cascaded decompositions were performed from the 24 hr timescale and 128 basic cells were stochastically generated for a single rainfall event thus the time scale for each basic cell was 675 s the calibrated parameters a a 0 and h based on local rainfall history are shown in the subgraph of fig 8 all the model parameters and input values that were adopted to analyze the triggering process of landslides in the study area under rrps are summarized in table 1 4 results and discussion in this section the influence of different rrps on the evolution of triggered landslides in the study area is investigated in a previous study the stochastic model was run 100 times to assess the sensitivity of site specific slope failure to rainfall patterns white and singham 2012 and we generated 100 rrps for each rainfall event section 3 3 to evaluate the susceptibility of regional slope failure to rainfall patterns a uniform rainfall pattern was used for comparison analysis first we demonstrated the evolution of triggered landslides under random rainfall patterns for different rainfall events fig 9 then we calculated the initial time to trigger landslides and the corresponding cumulative landslide acreage under the rrps for these rainfall events fig 10 analogous to the definition of rainfall pattern that is the distribution of rainfall over time we defined the distribution of landslide area over time as regional slope failure pattern rsfp thereafter we compared a series of rsfps with rrps based on the difference in the initial time to trigger landslides and the cumulative landslide area figs 11 14 finally the peak rainfall intensity under rrps and cumulative landslide area was fitted based on the case with the largest difference in landslide area fig 15 additionally we demonstrated the differences in landslide distribution over the entire landscape under different rrps using a geographic information system gis fig 16 the time evolution of the triggered landslide acreage over the entire landscape under each rainfall pattern with different return periods is shown in fig 9 each colored square in fig 9 represents the landslide acreage generated during the current time period per hour over the entire landscape under a random rainfall pattern the time evolution of triggered landslides under rrps with different return periods was observed to be significantly different displaying strong fluctuations for example uniform rainfall induced evolution of landslides under a 50 y return period began at the 13th hour ended at approximately the 60th hour thereby lasting approximately 47 h however the rrp induced landslides under a 50 y return period were triggered during a time period from the 7th to the 27th hour and ended during a period from the 50th to the 77th hour thus lasting for 43 50 h therefore compared with the uniform rainfall pattern regional landslide susceptibility analysis considering the inherent randomness of rainfall intensity could reflect the variability in the time evolution of triggered landslides additionally the rrp induced evolution of landslide was intensified with an increase in the return period of rainfall events such as the increase in the maximum landslide area generated per hour the initial time to trigger landslide and the corresponding cumulative landslide acreage under rrps and urp with different return periods are displayed in fig 10 each scatter represents three states the initial time to trigger landslides cumulative landslide acreage and the corresponding rainfall pattern from the perspective of the cumulative landslide area we observed from each subgraph of fig 10 that there was an approximate upper bound for the cumulative landslide area under rrps which appeared to be close to that under urp for these results near the upper bound the initial time to trigger landslides varied significantly the upper bound of the cumulative landslide area increased with an increase in the return period additionally the degree of dispersion of the cumulative landslide acreage under rrps increased with an increase in the return period to perform an in depth investigation of the differences in the initial time to trigger landslides and the cumulative landslide acreage two series of results from fig 10 that is rrp1a rrp6a in the subgraph a of fig 10 and rrp1c rrp7c in the subgraph c of fig 10 were extracted to investigate the corresponding random rainfall patterns rrp1a rrp6a were used to investigate the differences in the initial time to trigger landslides under an identical cumulative landslide acreage fig 11 rrp1c rrp7c were used to investigate the largest difference in cumulative landslide acreage under identical initial time to trigger landslides figs 12 14 additionally we aligned the unit time length of the distribution of landslides over time that is rsfp with the basic cell in the rrps to compare the rsfp with the rrp the cumulative landslide acreage of each rsfp under the corresponding rrp is presented in the center of each figure where a green vertical line is used to mark the end of the rainfall as shown in fig 11 the rsfps were strongly dependent on the shape of the rrps for rrps with rainfall concentrated in the early stage landslides were triggered earlier than those with rainfall concentrated in the middle or final stage therefore the initial time to trigger landslides under rrps was dependent on the stage where the rainfall was concentrated the variation in the initial time to trigger landslides could not be provided by the urp assumption it should be noted that the rainfall peak or the shape of the rrps were generated using the bounded random cascade model which was calibrated using the local rainfall history thus the rrps derived from the local rainfall history could reflect the natural characteristics of the local rainfall and help us determine the approximate range of the initial time to trigger landslides under a regressed rainfall event with a certain return period which may improve the ability of rainfall induced landslide warning however as shown in figs 12 14 the cumulative landslide acreage varied significantly under an approximately identical initial time to trigger landslides when the return period increased nearly tenfold the differenced in these rrps rrp1c rrp2c rrp3c rrp5c rrp6c rrp7c were concentrated in the values of the rainfall peak hence the cumulative landslide acreage under different rrps appeared to increase with a decrease in the rainfall peak as shown in figs 12 14 particularly when the rainfall peak decreased from 1 to approximately 0 3 unit 10 4 m s the cumulative landslide area increased from 6 2 to approximately 11 unit 105 m2 in fig 13 generally when the rainfall intensity of the short term rainfall was larger than the soil hydraulic conductivity the rainfall could not fully infiltrate the soil and excess rainfall runs off thus compared with the saturated hydraulic conductivity 2 6e 6 m s listed in table 1 of soil in this study area each of the rrps shown in figs 12 14 as having large peaks rrp1c rrp3c rrp4c and rrp6c each of which exceeds 5e 5 m s had a few hours of intense rainfall much of which would run off rather than infiltrating accompanied by few hours of low or moderate intensity rainfall in contrast rrp2c rrp5c and rrp7c with much lower peaks had many hours of low and moderate intensity rainfall although these patterns also produced some runoff these rrps had much more time for rainfall to infiltrate and raise pore pressures leading to additional amount of the slope failure on the other hand it is widely accepted that the cumulative rainfall increases with an increase in the return period in the analysis of extreme rainfall events thus when the return period of a rainfall event increases the rainfall peak in the rrps for the rainfall event also increases stochastically this is negative for rainfall infiltration which helps to weaken the triggering of landslides this can be responsible for the variation in the degree of dispersion of the cumulative landslide acreage under rrps when the return period of the rainfall event increases based on this analysis we have recognized part of the important influence of random rainfall patterns on the regional rainfall induced landslides susceptibility analysis which has been overlooked in many previous studies using extreme value analysis salciarini et al 2008 kawagoe et al 2010 grelle et al 2013 komori et al 2018 chen et al 2019 lee et al 2020 kim et al 2021 in the following sections the dispersion of the cumulative landslide area under different rrps is further discussed in depth the peak rainfall intensity and cumulative landslide area under rrps were fitted fig 15 based on the rrps in the subgraph c of fig 10 the rainfall peak and cumulative landslide acreage under the rrps were fitted using a quadratic polynomial function blue solid line and the r squared value was 0 8217 the fitting with 95 confidence bounds was determined as y 8 92e13 x 2 3 29e9 1 08e6 where x denotes the value of the rainfall peak under the rrps and y denotes the cumulative landslide area furthermore the rainfall intensity under the urp was the minimum value of the rainfall peak for a regressed rainfall event with a certain return period we observed from fig 15 that the urp was within the confidence bounds and fit well with the polynomial function under the rrps mathematically the extreme value of the polynomial function was consistent with the upper bound of the cumulative landslide area under the rrps and the slope of the polynomial fitting curve in the approximate range near the extreme point was close to 0 in a physical sense this indicated that the cumulative landslide area was unchanged when the rainfall peak under the rrps was within this range as discussed before the rrps with lower peaks produce more infiltration and raise pore pressures rather than produce runoff resulting in additional slope failure futhermore we observed from fig 15 that the cumulative landslide area was nearly constant and reached the upper bound when the rainfall peaks were less than an approximate threshold that is an approximate rainfall peak threshold of 3e 5m s in fig 15 this was further corroborated in fig 11 the cumulative landslide area under rrp1a to rrp6a was unchanged with a rainfall peak less than 3e 5m s though the shape of the rrps with peaks less than the threshold varies the cumulative landslides area remains approximately constant this indicated that when there is enough time for rainfall to infiltrate fully and reach equilibrium these rrps with peaks less than the threshold caused a similar final distribution of regional slope failure subjected to pore pressure consistent with slope parallel flow with the water table at the ground surface or other fully saturated steady flow condition it should be noted that the threshold of the rainfall peak is influenced by the local geographical condition which also affects the infiltration of rainfall that is the local digital elevation model and hydro mechanical parameters additionally the threshold of the rainfall peaks was an approximate estimation which implied that the cumulative landslide area under the rrps existed an unchanged range rather than an absolute invariant reviewing the discussion of figs 9 15 certain insights were drawn from both perspectives that is the time distribution of landslides and the cumulative landslide area to understand the influence of the rrps the rrps affected the time evolution of the triggered landslide the time evolution of triggered landslides under rrps was significantly different thus displaying strong fluctuations the rrps could provide a range of the initial time taken to trigger landslides instead of a fixed value which was more in line with the natural characteristics of the local rainfall event this differentiated it from the previous usage of the rough rainfall profile from the rainfall intensity duration frequency idf curve section 2 1 which was often used as a design criterion for hydrological engineering and environment the rrps affected the cumulative landslide area an upper bound existed for the cumulative landslide area under the rrps and the dispersion of the cumulative landslide area under the rrps increased with an increase in the return period this phenomenon was determined by the randomness of rainfall peaks and the saturated hydraulic conductivity of local soil compared with rrps having large peaks the rrps with much lower peaks is more positive for rainfall to infiltrate and raise pore pressure leading to additional slope failure in this study the cumulative landslide area was essentially unchanged and reached the upper bound under rrps with rainfall peaks less than an approximate threshold the cumulative landslide area tended to decrease with an increase in the rainfall peaks when they were greater than the threshold this further indicated that the influence of the rrps could not be neglected for the rainfall induced landslide susceptibility analysis under different return periods additionally the spatial evolution of triggered landslides under the rrps rrp3c rrp4c and rrp5c in fig 13 was displayed using a geographic information system gis to provide an in depth insight into the influence of the rrps the slope failure at different times was distinguished by the distinct colored cells in fig 16 thus readily providing a visual snapshot of the spatial distribution of the simulated landslide additionally the same locations of the simulated landslide area were enlarged to facilitate the visualization of the temporal performance of the simulated landslide we observed from fig 16 that the spatial distribution of triggered landslide under the rrps for the extreme rainfall event differed but was concentrated in the historical landslide area and thus may result in the reoccurrence of the historical landslide comparatively the triggered landslide under a lower peak rrp displayed a larger range of spatial distribution and covered more of the historical landslide area this further emphasized the different influences of the rrps on the regional landslide susceptibility analysis after downscaling a series of rainfall events under different return periods the difference in the influence of random rainfall patterns on the regional landslide susceptibility analysis was further investigated in this study a multitude of different landslide responses can be generated for a certain area due to the randomness of rainfall intensity when rainfall occurs for example an overall assessment can be performed based on the aforementioned research methods including the time evolution cumulative amount and geographical distribution of triggered landslides thus this study aids in understanding the diversity and discipline of the evolution of triggered landslides under different rrps it can provide a complete insight for decision makers to manage the natural hazards associated with rainfall induced landslides particularly in the field of geological hazard warnings that need to be released in advance 5 conclusion in this study we investigated the influence of random rainfall patterns rrps on the evolution of triggered landslides under different return periods a widely used typical rainfall pattern that is uniform rainfall was introduced to conduct a comparative analysis the results indicated the following conclusions the rrps affected the time evolution of triggered landslides the time evolution of triggered landslides under different rrps differed significantly and was strongly dependent on the shape of the rrps based on the natural characteristics of rainfall events the rrps could provide a range of the initial time taken to trigger landslides instead of a fixed value this overcomes the shortcoming of directly using the idf curve wherein the rough profile of rainfall events is often used as a design criterion for hydrological engineering and environment the rrps affected the cumulative landslide area by calculating the cumulative landslide area under the rrps with different return periods we found that an upper bound existed for the cumulative landslide area and the dispersion of the cumulative landslide area increased with an increase in the return period this was determined based on the randomness of rainfall peaks and the saturated hydraulic conductivity of local soil compared to rrps with large peaks lower peaks in rrps is positive for rainfall infiltration and pore pressure increasement thus leading to additional slope failure in this study the cumulative landslide area reached the upper bound and remains nearly constant when the rainfall peak under the rrps was less than an approximate threshold and it tended to decrease with an increase in the rainfall peaks when the rainfall peak was greater than the threshold in the long run establishing the theoretical upper bound of the cumulative landslide area can be a useful way for a in depth understanding of the effect of rrps on the rainfall infiltration and regional slope failure and this will be further investigated through improving our research framework in the future this study provides a better understanding of the diversity and discipline of the evolution of triggered landslides under different rrps and offers an overall assessment for decision makers to manage the natural hazards associated with rainfall induced landslides particularly in the field of geological hazard warnings that need to be released in advance declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this study was financially supported by the national natural science foundation of china no 51978666 51878668 and the science foundation for outstanding youth of hunan province no 2021jj10063 and the hunan provincial department of transportation science and technology progress and innovation project no 202017 202115 all financial support is greatly appreciated 
25571,the importance of rainfall patterns in triggering landslides has been recognized worldwide in this study the triggering and evolution of rainfall induced landslides were investigated by a regional scale physical model based on one dimensional richard equation and infinite slope model where a bounded random cascade model brcm was adopted to generate random rainfall patterns rrps for rainfall events derived from extreme value analysis the results indicated that the rrps affect the evolution of triggered landslide and the cumulative landslide area the shape of rrps determined the time evolution of triggered landslide due to the randomness of rainfall peaks under rrps and limited hydraulic conductivity of soil an upper bound existed for the cumulative landslide area under rrps and the dispersion of the cumulative landslide area increased with an increase in the return period this study aids in the understanding of the diversity and discipline of the evolution of rainfall induced landslides keywords random rainfall pattern extreme value distribution triggering and evolution rainfall induced landslides unsaturated loess 1 introduction rainfall induced landslides are a common geological hazard worldwide an et al 2016 wang et al 2020 guo et al 2022 with global climate change in recent years rainfall events have increased in intensity dehn et al 2000 dore 2005 sandink et al 2016 kristo et al 2017 consequently this may affect the stability of natural and artificial slopes dixon and brook 2007 and cause landslides in the future crozier 2010 dijkstra and dixon 2010 damiano and mercogliano 2013 chaithong et al 2017 komori et al 2018 rainfall patterns determine the temporal distribution characteristics of rainfall intensity which directly affect the flux boundary conditions of the soil surface kristo et al 2017 and the infiltration process of rainwater in the soil assouline and mualem 1997 foley and silburn 2002 hawke et al 2006 alvioli and baum 2016 the infiltration changes the pore water pressure and causes the stress of the soil skeleton to weaken which may results in a decrease in shear strength and the occurrence of landslides to date many studies have analyzed the slope stability under different rainfall patterns on the scale of site specific slopes typical rainfall patterns including advanced intermediate and delayed rainfall tsaparas et al 2002 rahimi et al 2011 kristo et al 2017 wu et al 2017 lin and zhong 2018 ran et al 2018 and random rainfall patterns rrps white and singham 2012 tang et al 2018 yuan et al 2018 have been considered in the evaluation of slope stability the research results show that differences in slope stability under different rainfall patterns are mainly characterized by the time and type of slope failure and the minimum safety factor of the slope in particular compared with the stability analysis of a single slope under rrps considering uniform rainfall is less accurate and may overestimate the rainfall threshold required to cause slope instability white and singham 2012 moreover these studies emphasize the importance of rainfall patterns in the stability analysis of site specific slopes however in previous studies where regional scale rainfall induced landslide hazards were assessed the processing of rainfall events tended to focus on changes in rainfall threshold cumulative rainfall average rainfall intensity and rainfall duration guzzetti et al 2007 martelloni et al 2011 park et al 2013 piciullo et al 2016 melillo et al 2018 guo et al 2019 dai et al 2020 particularly the rainfall pattern was often assumed to be uniform or similar to other typical rainfall patterns when performing regional slope stability analysis using data from the prediction of extreme rainfall salciarini et al 2008 kawagoe et al 2010 grelle et al 2013 bregoli et al 2014 kirschbaum et al 2015 schilirò et al 2015 komori et al 2018 chen et al 2019 lee et al 2020 hürlimann et al 2022 obtained using the generalized extreme value gev distribution hosking and wallis 2005 the occurrence of rainfall induced landslides under different return periods from 1 to 100 years or durations from 6 h to 72 h has been reported in previous studies salciarini et al 2008 grelle et al 2013 chen et al 2019 the cumulative percentage of landslide area for a certain area increased with an increase in the recurrence interval that is return period and rainfall duration however they failed to provide a specific rainfall pattern consequently random fluctuations in rainfall intensity during the actual rainfall process have been ignored schertzer and lovejoy 1997 although random rainfall time series were performed to investigate the impacts on the regional landslide assessment fan et al 2020 the random nature of rainfall intensity over time is artificial and not consistent with the actual rainfall history of the local area further based on previous studies on the stability analysis of site specific slopes only the characteristic variables of rainfall events adopted particularly the average rainfall intensity and duration are not sufficient to cover all the rainfall characteristics that may cause slope failure in fact many different rainfall patterns can actually exist for rainfall events with the same average rainfall intensity and duration peres and cancelliere 2014 therefore in the assessment of regional rainfall induced landslide hazards ignoring the inherent randomness characteristics of rainfall intensity over time may be insufficient to reflect all the information of potential landslide hazards to overcome this shortcoming we reconsidered a series of rainfall events under different return periods and further downscaled the rainfall to yeild a series of random rainfall patterns the bounded random cascade model menabde and sivapalan 2000 was introduced to describe the random fluctuating characteristics of rainfall intensity in the actual rainfall process the recurrence of rainfall was performed by the idf rainfall intensity duration frequencey curve which is derived from the gev distribution tobin 1989 hosking and wallis 2005 lee et al 2020 and can provide basic information of rainfall except the rainfall pattern the above two steps constituted the rainfall generator and it was incorporated into a regional finite element physical model based on a one dimensional flow equation and an infinite slope model lizarraga et al 2017 song et al 2020 for rainfall induced landslide assessment and the simulation results were visualized using a geographic information system gis platform this study aimed to investigate the triggering and evolution of landslides by considering random rainfall patterns that are derived from rainfall history it can provide a more complete insight for decision makers to manage the natural hazards associated with rainfall induced landslides in the future 2 theoretical basis 2 1 recurrence of rainfall events gev distribution the probability of extreme events based on past observations was predicted employing the extreme value analysis el adlouni et al 2007 the gev distribution was adopted to perform extreme events with the assumption that the relative maximum values are stable and independent coles et al 2001 further a maximum value was selected for the timescale of a year and subsequently the maximum values of years were sequenced to estimate the model parameters in other words the gev distribution was combined into three types 1 gumbel distribution 2 frechet distribution 3 weibull distribution tobin 1989 the cumulative distribution form is 1 f g e v x e x p 1 ζ x λ η 1 ζ where ζ η λ are the shape scale and location parameters that control the gev model form respectively each type refers to one case 1 for ζ 0 gumbel distribution 2 for ζ 0 frechet distribution and 3 for ζ 0 weibull distribution the maximum extreme value to predict extreme events such as rainfall storms and earthquakes is often determined using the gumbel model finlay et al 1997 zêzere et al 2008 therefore the gumbel model was used to obtain the cumulative intensity frequency duration of rainfall events the cumulative distribution form of gumbel model grelle et al 2013 lee et al 2020 is 2 f g u m x exp exp x λ η where x η λ denote the scale and location parameters which control the gumbel model form then the return period for a specified rainfall event can be expressed as 3a t 1 1 f g u m x the rainfall event obtained using the gumbel model exhibits three macroscopic characteristics cumulative rainfall depth return period and rainfall duration which are shown in fig 1 further the linear combination of the l moment or probability weighted moments pwms method ramachandra rao and hamed 2000 hosking and wallis 2005 gubareva and gartsman 2010 drissia et al 2019 lee et al 2020 was used to estimate the gumbel model parameters 3b β 1 b 0 λ γ η 4 β 2 b 0 2 b 1 l n 2 η where β 1 and β 2 are the first and second population l moments respectively b 0 and b 1 are the first and second population pwms of the gumbel model respectively γ is euler s constant and η and γ are the scale and location parameters of the gumbel model respectively furthermore b ˆ r probability weighted moments from the r th sample is obtained by hosking et al 1985 for r 0 5 b ˆ r n 1 j r 1 n j 1 j 2 j r n 1 n 2 n r x j n where n is the size of the sample which is composed of the annual maximum value for rainfall events with a certain duration j is the ascending order for samples x 1 n x 2 n x j n x n n are the sequences of annual maximum values in ascending order moreover when r 0 6 b ˆ 0 1 n j 1 n x j n whereas when r 1 b ˆ 1 is expressed as 7 b ˆ 1 1 n j 1 n j 1 n 1 x j n after obtaining b ˆ 0 and b ˆ 1 the estimated gumbel model parameters η ˆ and λ ˆ can be exported as 8 η ˆ b ˆ 0 2 b ˆ 1 l n 2 9 λ ˆ γ η ˆ b ˆ 0 2 2 within rainfall temporal patterns bounded random cascade model however in the previous step only the cumulative rainfall depth of a single rainfall event was obtained thus to obtain the rainfall distribution value on a smaller time scale temporal downscaling for individual rainfall events must be performed which can be modeled employing the bounded random cascade model brcm based on the characteristics of the stochastically self affine of rainfall time series menabde et al 1997 1999 menabde and sivapalan 2000 the breakdown coefficient was used to downscale the timescale of rainfall events wherein in each scaling process the parent interval was divided into two equal subintervals further the rainfall intensity of the subinterval was calculated via the multiplication of the rain intensity of the parent interval by the corresponding breakdown coefficient as shown in fig 2 10 q k i q 0 j k w j p i j i 1 2 3 2 k where k is the breakdown step q k i is the rainfall intensity for the i sub interval in the decomposition of the kth step while downscaling q 0 is the rainfall depth which denotes cumulative rainfall depth under a specific return period in the previous section w is sequence of breakdown coefficients that is the ratio of the rainfall intensity of the parent interval transferring to a subinterval during downscaling and p is the function that w obeys upon determining the cumulative rainfall the rainfall intensity of the sub units during each decomposition can be determined by the breakdown coefficient w the breakdown coefficient w can be characterized by b e t a distribution menabde and sivapalan 2000 11 p w 1 b a w a 1 1 w a 1 where b a is b e t a function including parameter a which varies with the time scale and a 0 moreover a depends on the size of the considered time scale and can be further expressed as 12 a t a 0 t h where t d r 2 k is the timescale of the basic cell after the k th breakdown and k 1 2 3 n a 0 and h are constants that control the variation of a the parameter a can be estimated employing the method of sample moments molnar and burlando 2005 13 a 1 8 v a r w 0 5 where the breakdown coefficient w of the sample is calculated by the non overlapping adjacent pairs of rainfall measurements rupp et al 2009 14 w j τ r j τ r j τ r j 1 τ j 1 3 5 n r 1 where r j τ is the rainfall depth recorded over the time interval of length τ at position j in the time series and n r is the total number of records at time scale τ 2 3 governing equation richard equation considering the water mass balance equation and darcy s law the governing equation for one dimensional seepage flow considering unsaturated soil can be expressed as follows richards 1931 16a n s r t k 2 h k z c o s α where n is the soil porosity h is the water pressure head s r is the degree of soil saturation k is the hydraulic conductivity z is the normal direction of the slope surface α is the slope angle during rainfall infiltration the soil moisture content and hydraulic conductivity rate continue to change thus the gardner model was used to describe the constitutive relationship of soil hydraulic variables to capture the changes in soil hydraulic properties water retention curve wrc is expressed as 16b θ h θ r θ s θ r e x p β h further hydraulic conductivity function hcf is expressed as 17 k h k s e x p β h where θ and k are the soil water content and hydraulic conductivity respectively θ r and θ s are the residual and saturated volumetric water contents of the soil respectively k s is the saturated hydraulic conductivity and β is a parameter that determines the sensitivity of soil water content and hydraulic conductivity to matric suction 2 4 stability model infinite slope model to assess the stability of an unsaturated slope the slope safety factor calculation based on the infinite slope model is described as the ratio of the critical slope stress level to the current actual stress level of the slope lizarraga et al 2017 song et al 2020 18 f s t a n φ t a n α 1 k s σ n e t where φ and α are the friction angle of the soil and slope angle respectively s is the suction σ n e t is the net stress and k is a parameter that quantifies the influence of suction increment on the increase in soil shear strength which can be described as k t a n φ t a n φ b fredlund et al 1978 2 5 implementation the calculation process for the triggering and evolution of rainfall induced landslides are performed through a regional physically based model solving by finite element method the part of performing spatially distributed analyses through the vectorization scheme was proposed by lizárraga and buscarnera 2018 in detail which involves three stages referred to input processing and output it is further elaborated and employed by lizárraga and buscarnera 2020 song et al 2020 in this study the procedure was further elaborated to solve the problems of random rainfall induced landslides for single rainfall event derived from extreme value analyses the first step requires a discretization procedure of the digital terrain to establish the fe model by which the mesh size and time steps are further difined and the input variables i e hydro mechnical properties initial and boundary conditions are assigned into each georegerenced grids this step can be readily conducted from geographical information system gis platform the processing stage is schematically shown in fig 3 rrps of the target time scale were generated after the cumulative rainfall depth and downscaling parameters a 0 and h for individual rainfall events were obtained further details in menabde and sivapalan 2000 each sequence of random rainfall over time i e random rainfall pattern from the rrp generator was identified and loaded by the finite element system through a step function and time steps then the transient infiltration and slope stability are performed for each slope unit at each time step if the safety factor of any unit at a certain time step was less than 1 its corresponding time and depth were marked and extracted into output column vectors and subsequently the unit was no longer involved in the subsequent search for soil failure otherwise the fe system keeps searching until the end of the rainfall and assigns non data index to the slope units which safe factor is larger than 1 at all the stage of the analysis then the next sequence of random rainfall from the rrp generator will drive the next analysis until the last sequence of the random rainfall finally the output column vector under each random rainfall pattern is remapped to a raster file in the original geographic coordinate system for postprocessing and visualization which can be performed through a gis platform 3 case study 3 1 location and characteristics the study area was located along the portion of provincial highway s316 in xinyuan county ili kazakh autonomous prefecture xinjiang china where the highway extends north south across awulale mountain that runs through the ili valley the total length of the route is 27 95 km the northern segment is connected to the s315 highway along the kashi river in an approximately east west direction whereas the south is connected to the g218 highway along the gongnaisi river in an approximately east west direction this is an important trade channel that connects the xinyuan and nilek counties further the mountainside is steep with a slope angle ranging between 25 and 40 in addition the mountains on both sides of the river are primarily composed of granite porphyry diorite glutenite and mudstone and the slope surface is covered with significant amount of loose loess the local climate is warm and humid with an annual average precipitation exceeding 400 mm rainfall was concentrated from april to october and approximately 50 of the annual precipitation was concentrated from april to june the lishi loess in the middle pleistocene and malan loess in the late pleistocene comprise the loess formation here the former is relatively dense exhibits good mechanical properties weak collapsibility and low water permeability the latter is classified as intoaeolian loess exhibiting high porosity well developed vertical joints strong collapsibility and relatively poor mechanical properties further owing to its poor physical and mechanical properties it provides the most basic geological conditions for driving loess landslides in this area liu et al 2017 the geological disasters in this area are dominated by loess landslides and loess debris flows and have resulted in excessive casualties and property losses in history in addition the abundant precipitation in the yili area has also resulted in creation of meteorological and hydrological conditions for the occurrence of loess landslides an and liu 2010 loess landslides are the primary type of geological hazards that occur on the surface of slopes along s316 they are typically small and medium sized landslides with a scattered distribution and shallow sliding surface depth and mostly occur on steep loess slopes approximately 300 loess landslides have occurred along the highway wei 2017 which can be used to establish a historical landslide dataset for the area moreover it provides an approximate location of the historical landslides and can provide a geographical reference for the potential recurrence of future landslides and numerical calculations in addition a georeferenced database based on local geographic characteristics was established for the stability analysis of regional slopes under rrps thus a digital elevation model with 12 m resolution obtained from the tandem x datasets of dlr http tandemx science dlr de was used for inputting terrain information in the area and locating the historical landslides wei 2017 as shown in fig 4 3 2 hydrological and mechanical characters initialization of the initial hydrological conditions supported by the data from the local geological survey report was performed in this area the distribution of the initial suction in the soil was calculated employing a wrc calibrated for malan loess considering the soil saturation measured gsr 2017 along the depth of the soil since the direct hydrological experiment of the local soil was not available the published test data of the same type of weathered malan loess in other loess areas wu et al 2011 wen and yan 2014 li et al 2015 2018 were used to provide a reasonable reference approved by song et al 2020 consequently based on the large amount of hydrological test data of the soil the soil water retention curve was maintained between the upper and lower limits further to compensate for the differences between different experiments to the best extent possible a mean curve based on the series of data scatter was used to calibrate the parameters of the water retention curve for the soil in this area as shown in fig 5 the hydraulic conductivity function of the malan loess was calibrated based on the saturated value of the hydraulic conductivity li et al 2016 as shown in fig 6 the parameter k which denotes the influence of suction increment on soil shear strength was calibrated by fitting direct shear test data reported by hu et al 2012 it must be noted that direct evidence specific for samples taken from the study area would be ideal for quantitative analyses since the information was not available the estimate derived from the data in this study should be regarded only as a first approximation which will be assessed against previous investigations wherein the natual randomness of rainfall intensity was neglected 3 3 random rainfall generation historical records of local daily rainfall for approximately ten years from january 2011 to december 2020 were obtained from the national oceanic and atmospheric administration climate prediction center of the united states noaa cpc data http www ncei noaa gov and are shown in fig 7 a series of maximum annual daily rainfall data was extracted from the historical rainfall records and further used to produce an ordered sample sequence that is x j n in formula 5 then the η ˆ and λ ˆ of the gumbel model for local rainfall recurrence were calculated based on formulas 6 to 9 the historical daily rainfall is shown in fig 7 and the cumulative intensity of daily rainfall events versus return periods are shown in the subgraph of fig 7 short term heavy rainfall was found to have occurred from june 7 2017 to june 8 2017 with a cumulative rainfall depth of approximately 271 mm in 24 h which caused several loess landslides with sliding depths mostly less than 5 m according to the survey song et al 2020 based on the calibrated gumbel model the return period of the extreme daily rainfall event with a cumulative rainfall intensity of approximately 271 mm in 2017 was approximately 519 years that is the rainfall event i3 t3 in the subgraph of fig 7 the timescale of the extreme rainfall event was further downscaled to reanalyze the effects of random rainfall patterns on landslides occurrence when it regressed consequently the cumulative rainfall depth was used as the total rainfall q 0 for downscaling and time downscaling of the rainfall event was performed based on the principal diagram in fig 2 additionally the rainfall events under the 50 y and 250 y return period that is the rainfall events i1 t1 and i2 t2 respectively were downscaled to provide a complete insight into the stochastic analysis to calibrate the parameters of the brc model for local rainfall events the hourly rainfall data of xinyuan county from january 2011 to october 2020 86 208 h were obtained using the second generation product merra2 tavg1 2d flx nx which was developed by nasa ges disc http disc gsfc nasa gov and focuses on atmospheric reanalysis in the satellite era the 1 h continuous rainfall records were used to synthesize adjacent non repeating 3 6 12 24 h rainfall datasets which were used for calculating a series of parameter a at different time scales in formula 12 using formulas 14 and 13 the series of parameter a was further used to calibrate the a 0 and h in formula 12 thereafter the beta distribution in formula 11 which regulates the breakdown coefficient w was determined and used to generate a series of w finally the rainfall intensity of each subinterval cell the random rainfall pattern was calculated based on the series of w using formula 10 moreover the ratio of two gamma distributed random variables is distributed according to the beta distribution mood et al 1974 if x1 and x2 are two independent random variables belonging to the single parameter gamma pdf then y1 x1 x1 x2 and y2 x2 x1 x2 are random variables distributed according to the beta distribution in formula 11 with an identical parameter a thus the procedure for generating the breakdown coefficient w at each cascade level is to simply generate two gamma distributed numbers x1 and x2 with an identical parameter a and then form the ratios w1 x1 x1 x2 and w2 x2 x1 x2 details of the procedure can be found in menabde and sivapalan 2000 the procedure of calibration and generation was implemented using matlab programming a total of seven cascaded decompositions were performed from the 24 hr timescale and 128 basic cells were stochastically generated for a single rainfall event thus the time scale for each basic cell was 675 s the calibrated parameters a a 0 and h based on local rainfall history are shown in the subgraph of fig 8 all the model parameters and input values that were adopted to analyze the triggering process of landslides in the study area under rrps are summarized in table 1 4 results and discussion in this section the influence of different rrps on the evolution of triggered landslides in the study area is investigated in a previous study the stochastic model was run 100 times to assess the sensitivity of site specific slope failure to rainfall patterns white and singham 2012 and we generated 100 rrps for each rainfall event section 3 3 to evaluate the susceptibility of regional slope failure to rainfall patterns a uniform rainfall pattern was used for comparison analysis first we demonstrated the evolution of triggered landslides under random rainfall patterns for different rainfall events fig 9 then we calculated the initial time to trigger landslides and the corresponding cumulative landslide acreage under the rrps for these rainfall events fig 10 analogous to the definition of rainfall pattern that is the distribution of rainfall over time we defined the distribution of landslide area over time as regional slope failure pattern rsfp thereafter we compared a series of rsfps with rrps based on the difference in the initial time to trigger landslides and the cumulative landslide area figs 11 14 finally the peak rainfall intensity under rrps and cumulative landslide area was fitted based on the case with the largest difference in landslide area fig 15 additionally we demonstrated the differences in landslide distribution over the entire landscape under different rrps using a geographic information system gis fig 16 the time evolution of the triggered landslide acreage over the entire landscape under each rainfall pattern with different return periods is shown in fig 9 each colored square in fig 9 represents the landslide acreage generated during the current time period per hour over the entire landscape under a random rainfall pattern the time evolution of triggered landslides under rrps with different return periods was observed to be significantly different displaying strong fluctuations for example uniform rainfall induced evolution of landslides under a 50 y return period began at the 13th hour ended at approximately the 60th hour thereby lasting approximately 47 h however the rrp induced landslides under a 50 y return period were triggered during a time period from the 7th to the 27th hour and ended during a period from the 50th to the 77th hour thus lasting for 43 50 h therefore compared with the uniform rainfall pattern regional landslide susceptibility analysis considering the inherent randomness of rainfall intensity could reflect the variability in the time evolution of triggered landslides additionally the rrp induced evolution of landslide was intensified with an increase in the return period of rainfall events such as the increase in the maximum landslide area generated per hour the initial time to trigger landslide and the corresponding cumulative landslide acreage under rrps and urp with different return periods are displayed in fig 10 each scatter represents three states the initial time to trigger landslides cumulative landslide acreage and the corresponding rainfall pattern from the perspective of the cumulative landslide area we observed from each subgraph of fig 10 that there was an approximate upper bound for the cumulative landslide area under rrps which appeared to be close to that under urp for these results near the upper bound the initial time to trigger landslides varied significantly the upper bound of the cumulative landslide area increased with an increase in the return period additionally the degree of dispersion of the cumulative landslide acreage under rrps increased with an increase in the return period to perform an in depth investigation of the differences in the initial time to trigger landslides and the cumulative landslide acreage two series of results from fig 10 that is rrp1a rrp6a in the subgraph a of fig 10 and rrp1c rrp7c in the subgraph c of fig 10 were extracted to investigate the corresponding random rainfall patterns rrp1a rrp6a were used to investigate the differences in the initial time to trigger landslides under an identical cumulative landslide acreage fig 11 rrp1c rrp7c were used to investigate the largest difference in cumulative landslide acreage under identical initial time to trigger landslides figs 12 14 additionally we aligned the unit time length of the distribution of landslides over time that is rsfp with the basic cell in the rrps to compare the rsfp with the rrp the cumulative landslide acreage of each rsfp under the corresponding rrp is presented in the center of each figure where a green vertical line is used to mark the end of the rainfall as shown in fig 11 the rsfps were strongly dependent on the shape of the rrps for rrps with rainfall concentrated in the early stage landslides were triggered earlier than those with rainfall concentrated in the middle or final stage therefore the initial time to trigger landslides under rrps was dependent on the stage where the rainfall was concentrated the variation in the initial time to trigger landslides could not be provided by the urp assumption it should be noted that the rainfall peak or the shape of the rrps were generated using the bounded random cascade model which was calibrated using the local rainfall history thus the rrps derived from the local rainfall history could reflect the natural characteristics of the local rainfall and help us determine the approximate range of the initial time to trigger landslides under a regressed rainfall event with a certain return period which may improve the ability of rainfall induced landslide warning however as shown in figs 12 14 the cumulative landslide acreage varied significantly under an approximately identical initial time to trigger landslides when the return period increased nearly tenfold the differenced in these rrps rrp1c rrp2c rrp3c rrp5c rrp6c rrp7c were concentrated in the values of the rainfall peak hence the cumulative landslide acreage under different rrps appeared to increase with a decrease in the rainfall peak as shown in figs 12 14 particularly when the rainfall peak decreased from 1 to approximately 0 3 unit 10 4 m s the cumulative landslide area increased from 6 2 to approximately 11 unit 105 m2 in fig 13 generally when the rainfall intensity of the short term rainfall was larger than the soil hydraulic conductivity the rainfall could not fully infiltrate the soil and excess rainfall runs off thus compared with the saturated hydraulic conductivity 2 6e 6 m s listed in table 1 of soil in this study area each of the rrps shown in figs 12 14 as having large peaks rrp1c rrp3c rrp4c and rrp6c each of which exceeds 5e 5 m s had a few hours of intense rainfall much of which would run off rather than infiltrating accompanied by few hours of low or moderate intensity rainfall in contrast rrp2c rrp5c and rrp7c with much lower peaks had many hours of low and moderate intensity rainfall although these patterns also produced some runoff these rrps had much more time for rainfall to infiltrate and raise pore pressures leading to additional amount of the slope failure on the other hand it is widely accepted that the cumulative rainfall increases with an increase in the return period in the analysis of extreme rainfall events thus when the return period of a rainfall event increases the rainfall peak in the rrps for the rainfall event also increases stochastically this is negative for rainfall infiltration which helps to weaken the triggering of landslides this can be responsible for the variation in the degree of dispersion of the cumulative landslide acreage under rrps when the return period of the rainfall event increases based on this analysis we have recognized part of the important influence of random rainfall patterns on the regional rainfall induced landslides susceptibility analysis which has been overlooked in many previous studies using extreme value analysis salciarini et al 2008 kawagoe et al 2010 grelle et al 2013 komori et al 2018 chen et al 2019 lee et al 2020 kim et al 2021 in the following sections the dispersion of the cumulative landslide area under different rrps is further discussed in depth the peak rainfall intensity and cumulative landslide area under rrps were fitted fig 15 based on the rrps in the subgraph c of fig 10 the rainfall peak and cumulative landslide acreage under the rrps were fitted using a quadratic polynomial function blue solid line and the r squared value was 0 8217 the fitting with 95 confidence bounds was determined as y 8 92e13 x 2 3 29e9 1 08e6 where x denotes the value of the rainfall peak under the rrps and y denotes the cumulative landslide area furthermore the rainfall intensity under the urp was the minimum value of the rainfall peak for a regressed rainfall event with a certain return period we observed from fig 15 that the urp was within the confidence bounds and fit well with the polynomial function under the rrps mathematically the extreme value of the polynomial function was consistent with the upper bound of the cumulative landslide area under the rrps and the slope of the polynomial fitting curve in the approximate range near the extreme point was close to 0 in a physical sense this indicated that the cumulative landslide area was unchanged when the rainfall peak under the rrps was within this range as discussed before the rrps with lower peaks produce more infiltration and raise pore pressures rather than produce runoff resulting in additional slope failure futhermore we observed from fig 15 that the cumulative landslide area was nearly constant and reached the upper bound when the rainfall peaks were less than an approximate threshold that is an approximate rainfall peak threshold of 3e 5m s in fig 15 this was further corroborated in fig 11 the cumulative landslide area under rrp1a to rrp6a was unchanged with a rainfall peak less than 3e 5m s though the shape of the rrps with peaks less than the threshold varies the cumulative landslides area remains approximately constant this indicated that when there is enough time for rainfall to infiltrate fully and reach equilibrium these rrps with peaks less than the threshold caused a similar final distribution of regional slope failure subjected to pore pressure consistent with slope parallel flow with the water table at the ground surface or other fully saturated steady flow condition it should be noted that the threshold of the rainfall peak is influenced by the local geographical condition which also affects the infiltration of rainfall that is the local digital elevation model and hydro mechanical parameters additionally the threshold of the rainfall peaks was an approximate estimation which implied that the cumulative landslide area under the rrps existed an unchanged range rather than an absolute invariant reviewing the discussion of figs 9 15 certain insights were drawn from both perspectives that is the time distribution of landslides and the cumulative landslide area to understand the influence of the rrps the rrps affected the time evolution of the triggered landslide the time evolution of triggered landslides under rrps was significantly different thus displaying strong fluctuations the rrps could provide a range of the initial time taken to trigger landslides instead of a fixed value which was more in line with the natural characteristics of the local rainfall event this differentiated it from the previous usage of the rough rainfall profile from the rainfall intensity duration frequency idf curve section 2 1 which was often used as a design criterion for hydrological engineering and environment the rrps affected the cumulative landslide area an upper bound existed for the cumulative landslide area under the rrps and the dispersion of the cumulative landslide area under the rrps increased with an increase in the return period this phenomenon was determined by the randomness of rainfall peaks and the saturated hydraulic conductivity of local soil compared with rrps having large peaks the rrps with much lower peaks is more positive for rainfall to infiltrate and raise pore pressure leading to additional slope failure in this study the cumulative landslide area was essentially unchanged and reached the upper bound under rrps with rainfall peaks less than an approximate threshold the cumulative landslide area tended to decrease with an increase in the rainfall peaks when they were greater than the threshold this further indicated that the influence of the rrps could not be neglected for the rainfall induced landslide susceptibility analysis under different return periods additionally the spatial evolution of triggered landslides under the rrps rrp3c rrp4c and rrp5c in fig 13 was displayed using a geographic information system gis to provide an in depth insight into the influence of the rrps the slope failure at different times was distinguished by the distinct colored cells in fig 16 thus readily providing a visual snapshot of the spatial distribution of the simulated landslide additionally the same locations of the simulated landslide area were enlarged to facilitate the visualization of the temporal performance of the simulated landslide we observed from fig 16 that the spatial distribution of triggered landslide under the rrps for the extreme rainfall event differed but was concentrated in the historical landslide area and thus may result in the reoccurrence of the historical landslide comparatively the triggered landslide under a lower peak rrp displayed a larger range of spatial distribution and covered more of the historical landslide area this further emphasized the different influences of the rrps on the regional landslide susceptibility analysis after downscaling a series of rainfall events under different return periods the difference in the influence of random rainfall patterns on the regional landslide susceptibility analysis was further investigated in this study a multitude of different landslide responses can be generated for a certain area due to the randomness of rainfall intensity when rainfall occurs for example an overall assessment can be performed based on the aforementioned research methods including the time evolution cumulative amount and geographical distribution of triggered landslides thus this study aids in understanding the diversity and discipline of the evolution of triggered landslides under different rrps it can provide a complete insight for decision makers to manage the natural hazards associated with rainfall induced landslides particularly in the field of geological hazard warnings that need to be released in advance 5 conclusion in this study we investigated the influence of random rainfall patterns rrps on the evolution of triggered landslides under different return periods a widely used typical rainfall pattern that is uniform rainfall was introduced to conduct a comparative analysis the results indicated the following conclusions the rrps affected the time evolution of triggered landslides the time evolution of triggered landslides under different rrps differed significantly and was strongly dependent on the shape of the rrps based on the natural characteristics of rainfall events the rrps could provide a range of the initial time taken to trigger landslides instead of a fixed value this overcomes the shortcoming of directly using the idf curve wherein the rough profile of rainfall events is often used as a design criterion for hydrological engineering and environment the rrps affected the cumulative landslide area by calculating the cumulative landslide area under the rrps with different return periods we found that an upper bound existed for the cumulative landslide area and the dispersion of the cumulative landslide area increased with an increase in the return period this was determined based on the randomness of rainfall peaks and the saturated hydraulic conductivity of local soil compared to rrps with large peaks lower peaks in rrps is positive for rainfall infiltration and pore pressure increasement thus leading to additional slope failure in this study the cumulative landslide area reached the upper bound and remains nearly constant when the rainfall peak under the rrps was less than an approximate threshold and it tended to decrease with an increase in the rainfall peaks when the rainfall peak was greater than the threshold in the long run establishing the theoretical upper bound of the cumulative landslide area can be a useful way for a in depth understanding of the effect of rrps on the rainfall infiltration and regional slope failure and this will be further investigated through improving our research framework in the future this study provides a better understanding of the diversity and discipline of the evolution of triggered landslides under different rrps and offers an overall assessment for decision makers to manage the natural hazards associated with rainfall induced landslides particularly in the field of geological hazard warnings that need to be released in advance declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this study was financially supported by the national natural science foundation of china no 51978666 51878668 and the science foundation for outstanding youth of hunan province no 2021jj10063 and the hunan provincial department of transportation science and technology progress and innovation project no 202017 202115 all financial support is greatly appreciated 
25572,fire simulation models are useful to advance fire research and improve landscape management however a better understanding of these tools is crucial to increase their reliability and expansion into research fields where their application remains limited e g ecosystem services we evaluated several components of the bfolds fire regime module and then tested its ability to simulate fire regime attributes in a mediterranean mountainous landscape based on model outputs we assessed the landscape fire regulation capacity over time and its implications for supporting the climate regulation ecosystem service we found that input data quality and the adjustment of fuel and fire behaviour parameters are crucial to accurately emulating key fire regime attributes besides the high predictive capacity shown by bfolds frm allows to reliably inform the planning and sustainable management of fire prone mountainous areas of the mediterranean moreover we identified and discussed modelling limitations and made recommendations to improve future model applications keywords landscape change fire regime model ecosystem services mediterranean mountains model evaluation 1 introduction modelling of landscape disturbances such as fire has expanded over the last decades in forest ecology studies albrich et al 2020 seidl et al 2011 because of the increasing ability of models to capture complex phenomena and of the advances in computational capacity to run models that adequately simulate those phenomena perera et al 2015 the growing interest in landscape scale processes for management and planning turner and gardner 2015 and the potential application of modelling tools to support decision making seidl et al 2011 further increased the relevance of landscape studies using a simulation modelling approach in addition real world field experiments over large areas are costly in time and money and often unfeasible turner and gardner 2015 simulation models are mathematical simplifications of an ecological system perera et al 2015 whose implementation as a computer program seeks either to solve complex relationships or to describe and understand patterns of behaviour of a target system durán 2021 their ability to test a wide range of conditions makes simulation models suitable to study complex socio ecological phenomena such as wildfires and their impacts on real landscapes across space and time he 2008 landscape and ecological modelling has been applied to the development of tools synes et al 2016 that can be used to simulate fire disturbance processes on a broad range of spatial and temporal scales and levels of complexity dai et al 2015 keane et al 2004 simulation modelling tools addressing fire can be typified into three major categories herawati et al 2015 i landscape fire behaviour models e g flammap finney 2006 ii integrated fire vegetation models e g fsim finney et al 2011 and iii dynamic global vegetation e g lpj dgvm sitch et al 2003 and landscape models e g landis ii scheller et al 2007 the application of a simulation approach to model landscape level disturbances is a useful way to predict and forecast wildfire events estimating their occurrence and behaviour to support fire management planning and fighting operations pacheco et al 2015 it also allows to explore and simulate what if scenarios to assess long term patterns e g fire regimes keane et al 2004 or the ecological impacts of fires pais et al 2020 notwithstanding the usefulness of these tools in supporting fire related research limitations in their application may arise often derived from incomplete understanding and knowledge about the modelled processes perera et al 2015 besides modellers have to handle difficulties related to limitations in the quality of data resources e g spatial temporal or thematic resolution spatial or temporal extent needed to calibrate and validate models perera et al 2015 uncertainties about model parameters or the accuracy of model predictions alexander and cruz 2013 or regional model extrapolation seidl et al 2011 in this regard comprehensive model evaluation methods are available cruz et al 2003b jorgensen and fath 2011 to better understand model behaviour and provide information to identify and adjust key inputs increasing accuracy and reliability which is crucial when models are used to support fire management and decision making riley and thompson 2016 some examples of model evaluation can be found in the scientific literature addressing fire modelling focused on model sensitivity cary et al 2006 hummel et al 2013 sturtevant et al 2009 uncertainty analysis benali et al 2016 pinto et al 2016 or scenario analysis perera and cui 2010 perera et al 2009 such approaches allow for a comprehensive assessment of these modelling tools which may partially overcome difficulties in their application perera et al 2015 despite the widespread use of fire simulation tools in multiple fire related fields their application to characterize fire related ecosystem functions services and disservices remains limited baskent 2020 sil et al 2019b still the identification of ecosystem attributes that promote their fire regulation capacity i e the ability of ecosystems and landscapes to regulate spatiotemporal attributes of ﬁre regimes through the control of factors affecting ﬁre behaviour resulting from the interaction between ecosystem processes fire and biophysical structures vegetation types and spatial patterns depietri and orenstein 2019 haines young and potschin 2018 pettorelli et al 2018 sil et al 2019a 2019b has been fostering the application of fire modelling tools to assess fire related services at the landscape scale such as the fire protection ecosystem service sil et al 2019b or the impact of fire on the climate regulation ecosystem service pais et al 2020 in this study we aimed to evaluate and apply the boreal forest landscape dynamics simulator fire regime module bfolds frm ouellette et al 2020 perera et al 2014 in a dynamic mountain landscape under mediterranean type of climate in europe bfolds frm is a spatially explicit process based model that simulates ﬁre growth implemented in the landis ii ecosystem modelling framework scheller et al 2007 bfolds frm was designed primarily to simulate fire regimes in boreal forests and has been used to study fire regimes and to support forest management and policymaking perera and cui 2010 rempel et al 2007 however its application and evaluation in other biogeographic and landscape settings is scarce this study builds on experience from a previous application of bfolds frm sil et al 2019b and seeks to better understand bfolds frm behaviour and its ability to simulate fire regimes in a fire prone mediterranean mountain landscape and to support the assessment of its fire regulation capacity frc and the potential provision of the climate regulation ecosystem service cres through the carbon stocks balance we addressed the following research questions i how sensitive are the model outputs to changes in input data quality attributes and user assumptions ii do model parameters need adjustments to emulate real world conditions in the study area iii does bfolds frm accurately predict observed fire patterns in the study area iv to what extent do bfolds frm outcomes support the assessment of frc and its impact on cres in the study area v what are the potential strengths and limitations of the model and how can its application be improved this study contributes to identify relevant aspects of the bfolds frm model for broader applications particularly in the context of fire regimes in mountain regions under a mediterranean type of climate as well as to support the application of the fire regulation concept in the management of fire prone landscapes 2 methods 2 1 general approach we evaluated the boreal forest landscape dynamics simulator fire regime module bfolds frm in terms of its behaviour and capabilities to explore and predict fire regime attributes in the context of fire prone mountain landscapes in the mediterranean region we run bfolds frm as a stand alone module using the sabor river upper basin ne portugal as the focal area to carry out model sensitivity analysis to support calibration and validation besides we applied bfolds frm according to an ecosystem services based conceptual framework to assess fire regulation capacity frc and the supply of the climate regulation ecosystem service cres in the study area fig 1 a framework was developed to evaluate bfolds frm based on the available standardized ecological modelling guidelines for developing and evaluating simulation models cruz et al 2003b jorgensen and fath 2011 swannack et al 2012 waveren et al 1999 four main steps were taken in the process 1 baseline simulations to define model reference behaviour for subsequent model evaluation steps 2 assessment of model outputs sensitivity to changes in inputs and parameters to better understand their relative effects on model outcomes 3 adjustment of parameters to improve the correspondence between model behaviour and expected fire patterns calibration and 4 evaluation of its predictive accuracy when applied to a set of observed data validation we then used bfolds frm to assess shifts in fire related ecosystem functions i e frc and its impact on services i e cres in the focal area we used the simulated fire regime attributes burned area and fire intensity to characterize frc and combined fire modelling with carbon storage modelling to assess cres supply in the study area 2 2 context of the application the european mediterranean basin accounts for most of the annual burned area 94 and fire ignitions 74 in the eu san miguel ayanz et al 2019 fire in the mediterranean basin has historically been human driven due to intensive land use and continuous use of fire in rural activities e g for pasture improvement which shaped landscapes ecosystems and ultimately fire regimes keeley et al 2012 the fire regimes or pyromes associated with mediterranean vegetation can be typified as high intensity large fires and low intensity small fires with short fire seasons archibald et al 2013 the high fire hazard of mediterranean regions is partially a consequence of favourable conditions for vegetation growth during the rainy season followed by warm dry summers keeley et al 2012 which is particularly relevant to the fire regime in mediterranean mountains fréjaville and curt 2015 in addition socio economic factors particularly relevant in mountainous areas lasanta et al 2017 and contemporary land management and fire suppression policies across the mediterranean rigolot et al 2009 further increased landscape homogenization and fuel accumulation and connectivity moreira et al 2011 the study was conducted in the sabor river upper basin a fire prone mountain area located in northeast portugal at the southwestern end of the cantabrian mountain range körner et al 2016 fig 2 the area is approximately 30650 ha and elevation ranges from 484 to 1487 m fig 2 climate is mediterranean beck et al 2018 with average annual precipitation ranging from 806 to 1262 mm and average annual temperature ranging from 8 5 to 12 8 c sil et al 2017 seminatural habitats cistus spp cytisus spp and erica spp dominate the landscape although the areas of native forests quercus pyrenaica and q rotundifolia forest stands of maritime pine pinus pinaster and agroforestry systems castanea sativa have been increasing over the last decades sil et al 2016 on the other hand demographic and socioeconomic factors contributed to farmland abandonment and the conversion of these areas over time which modified landscape composition and configuration and favoured more hazardous fuels and fuel continuity azevedo et al 2011 which potentially decreased the fire regulating capacity over time sil et al 2019b 2 3 bfolds frm model description boreal forest landscape dynamics simulator fire regime module bfolds frm simulates fire mechanistically i e computing fire on the landscape at hourly time steps from ﬁre weather fuel type and slope inputs providing spatially explicit information about the location of the ignition the pixels burned in each simulated fire event and the intensity at which each pixel burned bfolds frm uses the cffwis canadian forest fire weather index system van wagner and pickett 1985 to describe fuel moisture conditions and provide weather related inputs to fire simulation the weather parameters wind speed and wind direction and the cffwis components computed from consecutive sequences of daily weather conditions measured at noon temperature relative humidity and wind speed and the 24 h cumulative rainfall are supplied to bfolds frm to interpolate weather data across the simulated area the cffwis indexes are numeric ratings for the moisture content of litter and other dead fine fuels ffmc the moisture content of loosely compacted organic layers of moderate depth dmc the amount of fuel available for combustion bui each land cover category is assigned to one of the 16 fuel types described in the canadian forest fire behaviour prediction fbp system forestry canada fire danger group 1992 to predict fire behaviour in bfolds frm standard fuel types in bfolds frm are partially user modifiable allowing incorporation into the simulations of vegetation type features for a given area or region fire ignition information is supplied daily and its placement on the landscape is spatially explicit but whether it spreads or not is determined by the combination of daily fire weather fuel type at the location point and a user defined dmc threshold fire ignition location is seeded randomly either weighted or unweighted the weighted mode allows the user to define the exact location of the ignition to start a fire event or to supply a surface that spatially weights the probability of ignition on the landscape while the unweighted ignition seeding is spatially random fire extinguishment is caused by absence of burnable cells adjoining the fire perimeter weather conditions below the dmc threshold for fire spread or when the fire season ends meaning that no fire management activities are implemented and used to control the process 2 4 model evaluation 2 4 1 baseline simulations baseline simulations comprised the application of bfolds frm to the sabor river upper basin taking 2007 as the reference year appendix a and appendix b overall model parameterization included the reclassification of land use and land cover lulc spatial data for 2007 dgt 2019 as burnable i e natural grasslands agroforestry systems broadleaved forests coniferous forests mixed forests shrublands and non burnable i e urban areas agricultural areas open areas with either scarce or no vegetation and water bodies then burnable land covers were assigned to five standard fuel types from the canadian fire behaviour prediction system forestry canada fire danger group 1992 wotton et al 2009 c6 conifer plantation for the coniferous forests and the conifer dominated mixed forest m2 boreal mixewood green 25 conifers for the broadleaved forest and the broadleaved dominated mixed forest m2 boreal mixewood green 50 conifers for the broadleaved and the coniferous mixed forest o1 grass for grasslands and shrublands and o1b grass for agroforestry systems weather data used in baseline simulations were retrieved from the bragança weather station for the year 2007 a complete fire season i e 1 year containing noon daily records was used to compute the previously mentioned canadian forest fire weather index fwi system codes information on daily fire ignitions for the year 2007 was retrieved from the portuguese fire database icnf 2021 and used to feed the bfolds frm model data entries registered as false alarms as well as data entries for ignitions that originated fires smaller than 0 06 ha were excluded for model evaluation we constrained the model to simulate one fire season at a time i e time step 1 using the ignition and fire weather data of the correspondent year while the random number seeds parameter of the landis ii core model was kept constant thus preventing the model from determining ignition success in a fully stochastic manner appendix b as such we run the model one time one simulation assuming that the results represent the model reference behaviour under the defined set of conditions 2 4 2 model sensitivity assessment the one at a time oat sensitivity analysis method was applied varying one input factor at a time while keeping all other constant pianosi et al 2016 bfolds frm previously parameterized in the baseline simulation step was run n 31 to assess model sensitivity to variation in the quality of spatial data i e spatial resolution 25 m vs 100 and 300 m cell size and thematic resolution 1 ha vs 25 ha minimum map unit and fire weather data i e local weather station vs era5 land database as well as in the user assumptions regarding the spatial distribution of fire ignitions i e unweighted vs weighted fire ignition seed probability surface the spatialization of the fire weather data i e weather point data vs splined weather surface land cover to fuel type conversion i e standard vs custom fuel types and model built in parameters i e ignition and fire spread dmc thresholds smouldering fire interval crown base height of forest fuel types and fuel load and curing of grass fuel types appendix c the outputs analysed annual number of fires annual mean fire size and annual mean fire intensity appendix c were compared against the outputs of the baseline simulation we applied a statistical analysis and an index based analysis to evaluate model sensitivity the statistical approach evaluated the sensitivity of model outputs i e annual mean fire size and annual mean fire intensity to changes in the spatial and non spatial components of the model the non parametric wilcoxon mann whitney test neuhäuser 2011 was applied to test for difference in medians between baseline and sensitivity simulations outputs and the stochastic superiority measure a vargha and delaney 2000 a nonparametric statistic of effect size that informs about the magnitude of the differences between outputs from tested t and baseline d simulations by computing the probability that a random observation of t is higher than a random observation of d the a measure statistic was ranked in four classes vargha and delaney 2000 no difference 0 44 a 0 56 small 0 36 a 0 44 or 0 56 a 0 64 medium 0 29 a 0 36 or 0 64 a 0 71 large a 0 29 or a 0 71 all statistical analyses were conducted using the software r r core team 2020 the sensitivity index approach was applied to evaluate the sensitivity of the model outputs i e annual number of fires annual mean fire size and annual mean fire intensity to changes in the model built in parameters a dimensionless sensitivity index i lenhart et al 2002 was computed through variation of the parameter range by 20 and 40 from their default value tested in the baseline simulation step the sensitivity of each parameter was assessed by ranking the sensitivity index in four classes lenhart et al 2002 class i small to negligible 0 00 i 0 05 class ii medium 0 05 i 0 20 class iii high 0 20 i 1 00 and class iv very high i 1 00 2 4 3 model calibration we carried out the calibration procedure by manually adjusting model parameters used in the baseline simulation appendix d either one parameter at a time or multiple parameters at once n 318 the calibration was informed by the results of the sensitivity analysis in a trial and error exercise jorgensen and fath 2011 waveren et al 1999 an evaluation of calibration results was applied to minimize the error differences between simulated and observed data by comparing outputs from the calibration exercise against fire regime attributes krebs et al 2010 of annual number of fires annual total burned area and annual mean fire size observed in the sabor river upper basin for 2007 using these as reference criteria appendix d for that we calculated the relative error of simulated data in the study area for the selected attributes of the fire regime using equations 1 3 equation 1 renf nfsim nfobs nfobs x 100 equation 2 reba basim baobs baobs x 100 equation 3 remfs mfssim mfsobs mfsobs x 100 where renf is the relative error for the annual number of fires reba is the relative error for the annual total burned area and remfs is the relative error for the annual mean fire size nfsim basim and mfssim are the simulated annual number of fires total burned area and mean fire size respectively for year 2007 and nfobs baobs and mfsobs are the observed annual number of fires burned area and mean fire size respectively for the same year simulations with different parameter settings whose outputs showed a relative error ranging between 35 were assumed as acceptable for model prediction cruz and alexander 2013 and selected for the following model evaluation step 2 4 4 model validation we carried out model validation by applying bfolds frm to the sabor river upper basin to years 2015 and 2018 appendix e we used the portuguese fire database icnf 2021 to retrieve observed data on the annual number of fires and the annual burned area for the sabor river upper basin in these years based on results of model calibration tests in 2007 we identified different model parameters settings n 13 whose outputs resulted in a relative error within 35 and applied them to the 2015 and 2018 intermediate simulations therefore we run bfolds frm n 1 independently for each year analysed and model settings appendix e then we compared three selected attributes of the fire regime in the study area for 2015 and 2018 against simulated outputs appendix e we evaluated model accuracy by applying equations 1 3 to quantify model deviation from observed data assuming as acceptable an error of up to 35 in model predictions cruz and alexander 2013 finally from the intermediate simulations we kept the model parameter configuration that generated the outputs with the lowest relative error 2 5 assessment of the fire regulation capacity and of the climate regulation ecosystem service we modelled frc based on five fire regime attributes proxies derived from simulations with bfolds frm total annual burned area annual mean fire size annual mean fire intensity and number of fires with size above 100 ha and mean fire intensity above 500 kw m 1 in years 2007 reference year 2015 and 2018 we used two main criteria to assess frc 1 landscape capacity to regulate overall fire regime attributes and 2 landscape capacity to regulate large and potentially severe fires i e fires larger than 100 ha the oﬃcial size threshold for large fires in portugal icnf 2019 and mean fire intensity above 500 kw m 1 which is generally accepted as the limit for direct fire control by firefighting crews using hand tools hirsch and martell 1996 and the threshold for tree injury by fire van wagner 1973 we assumed that the sabor river upper basin landscape was able to regulate the spatiotemporal attributes of fire if 1 the total annual burned area the annual mean fire size and the mean fire intensity decreased compared to the year 2007 and or 2 the number of fires larger than 100 ha and the mean fire intensity above 500 kw m 1 remained at 2007 levels besides we investigated the effect of frc dynamics on the supply of the climate regulation ecosystem service cres is the avoided release of carbon to the atmosphere resulting from the capacity of terrestrial ecosystems to act as carbon sinks haines young and potschin 2018 keith et al 2021 sil et al 2017 and was modelled by applying the invest integrated valuation of ecosystem services and tradeoffs carbon storage and sequestration module sharp et al 2020 to the sabor river upper basin in 2007 2015 and 2018 using data from previous work in the area sil et al 2017 appendix f we assumed the carbon stored in aboveground biomass agb as a proxy for the climate regulation es supply haines young and potschin 2018 keith et al 2021 then we used the total annual burned area simulated with bfolds frm in each year to estimate the potential losses of carbon stored in the landscape assuming that all carbon stored was released into the atmosphere therefore we expressed the effect of frc on cres as the difference between the estimated maximum potential supply of the cres and its potential loss resulting from simulated fires in the study area we used 2007 as the reference year from which we compared the corresponding carbon losses due to fire additionally to improve results interpretation we carried out an ancillary analysis on land use and land cover changes for the study area and the distribution of the number of days per fire weather index fwi class for each of the years analysed appendix g 3 results 3 1 bfolds frm model evaluation 3 1 1 sensitivity analysis statistical analysis showed that the distribution of bfolds frm outputs when tested for sensitivity to changes in the quality and assumptions of spatial fuel type and weather inputs only differed significantly for changes to spatial grid resolution cell size from 25 to 300 m and thematic resolution 1 25 ha table 1 and appendix c still all these inputs had meaningful effects on the annual mean fire size and mean fire intensity showing differences that ranged from small to large except when tested for changes to fire weather data spatialization and fuel types parameters table 1 and appendix c regarding the relative influence of the tested built in parameters on output variables significant differences only were found when varied fuel curing parameters decreased by 40 and 20 showing a small to large effect on both the annual mean fire size and mean fire intensity outputs also both the ignition dmc threshold and smoulder interval parameters showed a small effect on model outputs table 1 and appendix c nevertheless sensitivity index based analysis indicates that all output variables were sensitive to changes in spread dmc threshold and smoulder interval parameters also all output variables showed very high sensitivity to changes in ignition dmc threshold parameter and fuel curing parameters table 1 and appendix c in turn changes made to crown base height and fuel load parameters showed a more pronounced sensitivity for the annual mean fire intensity output than for the annual number of fires and mean fire size table 1 and appendix c 3 1 2 model calibration results from the baseline simulation indicate that the fire regime attributes simulated with bfolds frm deviated substantially from the data observed in the study area in 2007 overestimating total burned area and mean fire size and underestimating the number of fires table 2 conversely the estimated relative error re of the fire regime attributes simulated after model calibration fig 3 appendix d became within the range of 35 indicating satisfactory model predictions table 2 3 1 3 model validation overall bfolds frm was able to emulate observed trends of selected attributes of fire regime for the three years analysed with an estimated mean relative error within the 35 interval considered as reasonable for model predictions in the validation step table 3 on average bfolds frm overpredicted annual number of fires and annual total burned area and underpredicted annual mean fire size table 3 despite the satisfactory overall model performance the area burned in 2018 was overpredicted with a relative error outside the range accepted as reasonable although predictions for both the number of fires and mean fire size were within that interval table 3 the predicted burned area for 2015 had a substantially lower relative error although the simulated annual number of fires was greater than the number of fires observed which increased the associated relative error in turn underpredicting the mean fire size table 3 3 2 assessment of modelled fire related functions and services 3 2 1 fire regulation capacity overall most of the simulated fires in the sabor river upper basin were small and of low intensity although differences were observed among the three dates analysed fig 4 simulated fire regime attributes increased in 2015 compared to 2007 with fires exceeding 100 ha and the 500 kw m 1 mean fire intensity table 4 on the other hand overall simulated attributes in 2018 decreased compared to 2007 table 4 in 2018 simulated fires were smaller and less intense than in 2007 but one attained more than 100 ha with mean fire intensity 500 kw m 1 fig 4 and table 4 3 2 2 impacts of fire on the carbon storage balance and the climate regulation ecosystem service the maximum potential for carbon storage in the landscape increased from 2007 on compared to 2007 our estimates indicate that carbon stocks in 2015 increased by 40 while in 2018 increased by 70 table 5 on the other hand the losses in the maximum potential carbon stored in the landscape due to the simulated fires increased over time fig 5 and table 5 in 2015 losses represented 1 2 of the maximum potential for carbon storage in the landscape indicating an increase of 80 compared to 2007 table 5 in 2018 losses represented 0 7 of the maximum potential for carbon storage in the landscape increasing 20 compared to 2007 4 discussion 4 1 model evaluation changes made to the model spatial inputs did affect its outputs see table 1 which is in line with findings reported for other modelling platforms that simulate fire mechanistically cary et al 2006 as an ecological process fire creates landscape heterogeneity and responds to the spatial patterns of fuel composition and configuration which are crucial for its regulation turner 2010 turner et al 2012 fire spread is simulated in bfolds frm as a mechanistic process where among other factors spatial patterns of pixel based fuel types and topographic layers are crucial perera et al 2008 in this regard it is important to ensure that the data used in the parameterization of model spatial inputs has the best quality possible in terms of spatial and thematic resolution to describe dynamics accurately since they affect modelling and simulation of landscape fire processes saura 2002 taneja et al 2021 turner and gardner 2015 on the other hand although our indicators did not show a clear effect of the spatial weighting pattern based on historical fire ignition records on model outputs see table 1 similar to other studies bar massada et al 2011 perera et al 2009 we consider that applying this information to fire simulations may be relevant particularly in the context of mountains in the mediterranean region since fire ignition patterns in these areas are influenced by the use of fire as a management tool catry et al 2009 sequeira et al 2020 our results suggest that the quality of spatial data inputs is important since it affects the model outputs as such the user must be aware of the trade offs that may arise when selecting spatial inputs for model parameterization for example calibrating the model with the most detailed data available benefits the accuracy of model predictions but may also increase the computational costs to perform the simulations taneja et al 2021 besides integrating the fire ignition patterns in the simulations is useful to represent real world dynamics in mountainous landscapes and thereby to characterize the fire regime in these areas fernandes et al 2014 keeley et al 2012 as expected bfolds frm output variables were also sensitive to changes made to weather inputs see table 1 since these comprise the most important information for simulating fire patterns perera et al 2008 our results agree with findings from other fire modelling platforms where climate variables were very important for fire simulation cary et al 2006 hummel et al 2013 inconsistencies in the rainfall variable between observed bragança weather station and estimated era5 land dataset appendix a can partially explain the variation found in our results with the latter underestimating the fire weather indices in the study area and thereby decreasing both burned area and fire intensity the availability of fire weather data is a critical point in the application of fire simulation models riley and thompson 2016 despite recent advances in the supply of ready to use fwi and subindices data at the global scale e g era5 reanalysis products vitolo et al 2020 their application at the local scale remains limited due to their spatial resolution e g 28 56 km grid cell size as for prediction of the future fire danger conditions using meteorologically based indices e g the canadian fire weather index uncertainties regarding the use of weather data e g regional climate models have been described in the scientific literature indicating either a potential negative bias in fire predictions herrera et al 2013 or acceptable agreement between weather information and fire predictions amatulli et al 2013 ultimately our results stress that the weather data quality is important for estimates of fire weather conditions since those are critical for fire simulations in bfolds frm regarding the influence of modifications in the fbp standard fuel types although bfolds frm has responded to those changes the indicators did not show a relevant influence on model outputs see table 1 which partially supports the results reported by sturtevant et al 2009 the recalculation of the fire spread rate ros for the standard fbp o1 fuel type reduced the potential fire spread rate in turn restricting the size of the simulated fires however more substantial ros differences would have been observed if fire weather conditions used in simulations were more severe especially in terms of wind speed as it affects the computation of the initial spread index isi forestry canada fire danger group 1992 wotton et al 2009 on the other hand outputs were highly sensitive to changes made to the degree of grass curing see table 1 since ros is greatly enhanced by fully cured grass forestry canada fire danger group 1992 wotton et al 2009 the replacement of fbp o1 fuel subtypes with custom fuel types allowed to improve the representation of the vegetation found in the study area namely semi natural and agroforestry areas since o1 fuel subtypes are limited to grasslands either matted o1a or standing dead o1b forestry canada fire danger group 1992 also in shrublands which are abundant in the study area ros is mostly controlled by vegetation height anderson et al 2015 whereas grassland ros varies mainly with the degree of grass curing cruz et al 2015 and fuel load cruz et al 2017 thus underlining the need for adjustments see tables 2 and 3 studies carried out elsewhere have emphasized the need to either develop or adapt fuel models to improve fire behaviour estimates for local vegetation types clark et al 2008 cruz and fernandes 2008 fogarty et al 1998 number of fires and mean fire intensity and size variables were very sensitive to changes made to parameters based on the duff moisture code dmc see table 1 this behaviour was expected since the dmc is an indicator of fuel consumption in boreal forests wotton 2008 hence our results reflect how bfolds frm applies the principles underlying the canadian forest fire danger rating system to simulate fire ignition spread and extinguishment perera et al 2008 calibration of the dmc based parameters indicated improvements in model ability to emulate the fire regime patterns observed in the study area see tables 2 and 4 which agrees with other studies carried out in the mediterranean region dimitrakopoulos et al 2011 that suggest the need to adapt and or modify assumptions related to the dmc to improve the predictions of both fuels moisture and burned area on the other hand the need for adjusting dmc based parameters may reflect in part the variability associated with the prediction of burned area and number of fires when the fwi indices are applied across the mediterranean basin amatulli et al 2013 such differences may be partially related to the type of dominating vegetation i e forest or shrubland cruz et al 2003a dimitrakopoulos et al 2011 fernandes 2016 or to the different characteristics of the duff layer in mediterranean ecosystems and boreal areas in terms of quantity depth and moisture dimitrakopoulos et al 2011 particularly when related to shrubland fernandes 2005 2016 also strategies of full fire suppression in mediterranean countries rigolot et al 2009 may hamper the fwi based simulation of fire nevertheless the dmc is a useful predictor of fuel consumption in maritime pine stands fernandes and loureiro 2013 while the moisture content of the duff layer variable used in the calculation of the dmc code has been correlated to re ignition events and the occurrence of smouldering in aleppo pine stands xifré salvadó et al 2020 given the importance of these parameters to simulate fire in bfolds frm and the limitations and uncertainties regarding the use of fwi indices in the mediterranean region particularly the role of dmc in shrubland fires careful calibration of the dmc based parameters in bfolds frm should be considered 4 2 patterns in fire related functions and services overall our results indicate that frc in 2015 decreased compared to 2007 while it increased in 2018 although the capacity to regulate potential large and intense fires decreased from 2007 onward see table 4 and figs 3 and 4 land cover changes that took place in the sabor river upper basin landscape over time see appendix g together with the bfolds frm outputs can explain in part our results forest expansion between 2007 and 2015 mostly at the expenses of seminatural areas but also of some non burnable areas e g agriculture led to increasing fuel continuity and fuel hazard in the landscape allowing fire to spread more easily over larger areas and with high intensity see table 4 and figs 3 and 4 on the other hand between 2015 and 2018 although the vegetated area continues to grow the transition from vegetated to non burnable areas may have enabled in some cases the disruption of fire propagation in the landscape also the conversion of coniferous forests mainly to deciduous forests may have balanced the general fire intensity although such changes have not prevented the occurrence of a large fire in the simulations see table 4 and figs 3 and 4 similar patterns were observed in previous studies carried out in the study area concerning the effect of lulc changes on the dynamics in the capacity of the landscape to regulate fire azevedo et al 2011 sil et al 2019b as well as for other areas the mediterranean basin regarding the effects of changes in lulc on fire hazard and fire regime moreira et al 2011 san miguel ayanz et al 2012 and fire regulating functions depietri and orenstein 2019 landscape changes between 2007 and 2018 promoted carbon storage in the sabor river upper basin landscape see table 5 particularly through the expansion of forest areas together with vegetation growth and increase of carbon stocks in aboveground biomass see appendices f and g our results are in line with previous work assessing the cres carried out in the area sil et al 2017 as well as for other mountainous areas where similar patterns of landscape changes tend to increase the supply of cres locatelli et al 2017 pais et al 2020 on the other hand the observed changes have modified the landscape capacity to regulate fire in the years analysed threatening the supply of cres differently as suggested by the variation in carbon losses see table 5 and fig 5 for example in 2015 the growth of carbon losses by 80 compared to 2007 can be partially related to the decrease in frc due to important changes in landscape structure and composition that were reflected in the increase in the simulated total burned area in 2018 although frc increased as suggested by the reduction in the burned area carbon losses due to fires increased by 20 compared to 2007 such an increase reflects a large fire partially driven by increasing forest fuels continuity in the landscape together with an increase of carbon stocks over time which resulted in greater carbon losses our results are in line with findings reported by thom and seidl 2016 concerning the impact of fire disturbances on carbon storage as well as the potential risks associated with the increase of forest areas e g to balance carbon emission envisioned in the european green deal european comission 2019 discussed in hermoso et al 2021 in addition to the effects of landscape changes on the frc dynamics and the supply of cres our results also reflect the influence of annual fire weather conditions for example although 2015 showed the lowest number of fire ignitions among dates it accounted for more days with very high and extreme fire danger fwi 45 compared to 2007 and 2018 see appendix g in turn the growing fuel continuity and fuel hazard in the landscape overtime along with more severe weather conditions may have increased susceptibility for large and intense fires in 2015 simulations increasing the impact of fires on cres supply see fig 5 conversely 2018 accounted for more days with low fire danger fwi 23 which may have limited the conditions for fire spread and the reduction of the overall fire size and fire intensity see table 4 and figs 3 and 4 despite the number of simulated ignitions exceeded 2015 however when weather conditions worsened during the summer season and fuel continuity and fuel hazard in the landscape were relatively high a large and intense fire was simulated that impacted high carbon stock area see fig 5 our results agree with findings that show a relationship between fwi thresholds and fire size fernandes 2019 however a more detailed analysis should be carried out to reduce uncertainty regarding the effect of landscape structure and fire weather conditions in driving fire regime attributes fernandes et al 2016 4 3 modelling strengths limitations and recommendations for future research overall bfolds frm is a valuable tool for informing sustainable planning and management of mountainous areas prone to fires in the mediterranean wherein similarly to the sabor river upper basin increasing fire hazard in the landscape is an ongoing process driven by rural depopulation and vegetation encroachment in abandoned areas azevedo et al 2011 lack of effective forest management pérez rodríguez et al 2018 and potential intensification of afforestation activities due to demand for natural resources e g bioenergy pérez rodríguez and azevedo 2020 or actions to cope with climate change hermoso et al 2021 the high predictive capacity of the model allowed to estimate the selected fire regime attributes of the sabor river upper basin within an acceptable range of error regarding the observed data see table 3 which in turn enabled characterizing the dynamics of the landscape capacity to regulate fire see table 4 and fig 4 as well as to estimate potential impacts on the supply of the climate regulation ecosystem service see table 5 and fig 5 ultimately bfolds frm is particularly useful for testing fire smart management alternatives that aim fire hazard mitigation while benefiting the supply of ecosystem services at the landscape scale damianidis et al 2021 fernandes 2013 pais et al 2020 enabling mediterranean mountains to cope with challenges related to their high vulnerability to global changes schroter et al 2005 although bfolds frm was able to emulate the observed pattern of the relative dominance of small fires in the study area see fig 4 after comparing simulated outputs with fire records it was clear that the model was not able to accurately capture the occurrence of large fires except in year 2015 appendix e such deviation may be related to oversimplification of fuel types used in the simulations after aggregation of land cover data in major lulc classes as well as to simplification of the fire weather conditions based on daily rather than hourly estimates for the whole study area since these are of particular relevance concerning large fires fernandes et al 2016 in addition the conceptualization and mechanics of the model itself can explain these differences because unlike other models e g medfire brotons et al 2013 the size and extinction of the simulated fires in bfolds frm are neither predefined nor depend on modelled fire suppression efforts but rather are emergent properties of the simulation process arising from the interaction between the theoretical assumptions of the model the assumptions of the users and the inputs provided perera et al 2014 notwithstanding our effort to assess and apply bfolds frm we acknowledge some limitations in our modelling framework model evaluation results were obtained based on model runs without replicates as we set the parameter random number seeds as a constant that is preventing the model from having a stochastic behaviour this approach is common in this type of assessment and useful when testing the sensitivity of several model parameters grant and swannack 2008 because it allows evaluating the relative effect of each parameter avoiding the noise that comes from the model stochasticity besides it decreases the number of simulations and the computational costs and time spent to complete the model evaluation steps grant and swannack 2008 however we acknowledge that this approach fails to simulate and assess the uncertainty in model predictions that can derive from variability in the frequency and the location of fire ignitions riley and thompson 2016 therefore we recommend that future work should consider model output variability as a way to optimize and improve model predictions for its implementation as a management and decision support tool uusitalo et al 2015 sensitivity analysis allowed a deeper understanding of the relative influence of bfolds frm inputs and parameters on model outputs see table 1 however we recognize that the approach used i e one at a time oat method has some limitations for instance it does not allow to effectively explore the interaction of inputs and parameters and thus quantifying their combined influence on model outputs saltelli et al 2019 although more comprehensive approaches do exist e g all at a time aat methods pianosi et al 2016 these were not considered due to their implementation complexity which particularly increases when the model relies heavily on spatial inputs pianosi et al 2016 as well as due to the high number of simulations and time required to process model results and proceed with model evaluations uusitalo et al 2015 moreover we carried out our analysis considering only three years 2007 2015 and 2018 we acknowledge that these years may not represent the full range of conditions existing in the study area which may restrain model response during the calibration step and raise uncertainties regarding model behaviour validation on the other hand complete and compatible data to parameterize the model and perform the calibration and validation steps are limited for the study area although statistical methods available in the scientific literature e g cross validation refaeilzadeh et al 2009 can be useful in model training and validation when available data is scarce their application would be unfeasible due to the model mechanics to overcome these issues at least partially we used data from the same system but for different years waveren et al 1999 which introduced some variability in the environmental conditions such as the daily weather conditions as well as in the spatial composition and configuration of land cover classes fuel types or the daily fire ignition data although our approach allowed us to evaluate the behaviour and capabilities of the model we acknowledge that improving the model evaluation process with more model applications is needed either testing the model in the study area using independent data or applying it to similar systems outside the study area will strengthen the applicability and usefulness of this tool in fire related research in mountain mediterranean landscapes our modelling approach captured the influence of landscape dynamics on fire behaviour see table 4 and figs 4 and 5 but it was not able to capture the effect of fire on the landscape simulating such interaction is essential to represent feedbacks between disturbances and landscape dynamics turner 2010 as such we acknowledge that uncertainties may persist regarding whether the model can effectively characterize the fire regime and the frc in the study area despite its ability to emulate the selected fire regime attributes in the years evaluated therefore future application of bfolds frm may benefit from coupling the model with e g a succession extension of the landis ii platform scheller et al 2007 allowing to simulate interactions and feedbacks between landscape and fire besides it would enable continuous spatiotemporal outputs to explore future changes in fire regime lastly our approach to assess the impacts of fire on carbon stored simplifies the potential interactions between fire and carbon dynamics as such carbon stocks represent mean values for generic lulc classes and those are only for the aerial biomass appendix f while disregarding other carbon pools in terrestrial ecosystems such as litter and soil lorenz 2013 that can also be affected by fires garcia hurtado et al 2013 we assumed that fire fully consumed aerial biomass in turn releasing all the stored carbon into the atmosphere which is seldom the case since part of the carbon resulting from biomass burning can incorporate dead organic matter and soil pools as pyrogenic carbon jones et al 2019 therefore future work on the effects of fire on carbon balance should consider a more comprehensive assessment of carbon stocks assigned to each lulc class and sub processes that are part of carbon dynamics as a result of fire activity thom and seidl 2016 5 conclusions the evaluation of bfolds frm contributed to a more in depth understanding of the relative influence of inputs on model behaviour and to improve the identification of key parameters for fire simulation moreover adjustments made to model inputs and parameters improved bfolds frm accuracy and emulated fire regime attributes in the sabor river upper basin with a relative error within acceptable bounds in this regard we emphasize that the application of bfolds frm in the context of the fire regime of mediterranean mountainous areas must consider i supplying the model with accurate and precise data to properly characterize the spatial patterns of fuels and fire weather conditions ii carefully calibrating parameters based on fire weather conditions due to the high model sensitivity to these parameters iii customizing standard fuel types to meet the characteristics of the existing vegetation and iv including ignition patterns associated with human activities our work underlined that bfolds frm can provide useful outputs to support the characterization of fire related ecosystem functions and services in a mediterranean mountainous area still we have identified potential limitations that may arise when applying the model to different areas which should thus be considered in future applications software name boreal forest landscape dynamics simulator fire regime module bfolds frm for landis ii programming language c developers ajith h perera ontario forest research institute marc r ouellette ontario forest research institute den boychuk ontario fire science and technology program marc ouellette ontario ca available at https www landis ii org extensions funding â sil received support from the portuguese foundation for science and technology fct through ph d grant sfrh bd 132838 2017 funded by the ministry of science technology and higher education and by the european social fund operational program human capital within the 2014 2020 eu strategic framework p m fernandes contributed in the framework of the uidb 04033 2020 project funded by the portuguese foundation for science and technology fct declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge marc ouellette for his technical support on issues related to the bfolds frm software author contributions â s j c a p m f j a and j p h designed the research â s performed the simulations and analysed the data â s j c a p m f j a j p h contributed to the final version of the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105464 
25572,fire simulation models are useful to advance fire research and improve landscape management however a better understanding of these tools is crucial to increase their reliability and expansion into research fields where their application remains limited e g ecosystem services we evaluated several components of the bfolds fire regime module and then tested its ability to simulate fire regime attributes in a mediterranean mountainous landscape based on model outputs we assessed the landscape fire regulation capacity over time and its implications for supporting the climate regulation ecosystem service we found that input data quality and the adjustment of fuel and fire behaviour parameters are crucial to accurately emulating key fire regime attributes besides the high predictive capacity shown by bfolds frm allows to reliably inform the planning and sustainable management of fire prone mountainous areas of the mediterranean moreover we identified and discussed modelling limitations and made recommendations to improve future model applications keywords landscape change fire regime model ecosystem services mediterranean mountains model evaluation 1 introduction modelling of landscape disturbances such as fire has expanded over the last decades in forest ecology studies albrich et al 2020 seidl et al 2011 because of the increasing ability of models to capture complex phenomena and of the advances in computational capacity to run models that adequately simulate those phenomena perera et al 2015 the growing interest in landscape scale processes for management and planning turner and gardner 2015 and the potential application of modelling tools to support decision making seidl et al 2011 further increased the relevance of landscape studies using a simulation modelling approach in addition real world field experiments over large areas are costly in time and money and often unfeasible turner and gardner 2015 simulation models are mathematical simplifications of an ecological system perera et al 2015 whose implementation as a computer program seeks either to solve complex relationships or to describe and understand patterns of behaviour of a target system durán 2021 their ability to test a wide range of conditions makes simulation models suitable to study complex socio ecological phenomena such as wildfires and their impacts on real landscapes across space and time he 2008 landscape and ecological modelling has been applied to the development of tools synes et al 2016 that can be used to simulate fire disturbance processes on a broad range of spatial and temporal scales and levels of complexity dai et al 2015 keane et al 2004 simulation modelling tools addressing fire can be typified into three major categories herawati et al 2015 i landscape fire behaviour models e g flammap finney 2006 ii integrated fire vegetation models e g fsim finney et al 2011 and iii dynamic global vegetation e g lpj dgvm sitch et al 2003 and landscape models e g landis ii scheller et al 2007 the application of a simulation approach to model landscape level disturbances is a useful way to predict and forecast wildfire events estimating their occurrence and behaviour to support fire management planning and fighting operations pacheco et al 2015 it also allows to explore and simulate what if scenarios to assess long term patterns e g fire regimes keane et al 2004 or the ecological impacts of fires pais et al 2020 notwithstanding the usefulness of these tools in supporting fire related research limitations in their application may arise often derived from incomplete understanding and knowledge about the modelled processes perera et al 2015 besides modellers have to handle difficulties related to limitations in the quality of data resources e g spatial temporal or thematic resolution spatial or temporal extent needed to calibrate and validate models perera et al 2015 uncertainties about model parameters or the accuracy of model predictions alexander and cruz 2013 or regional model extrapolation seidl et al 2011 in this regard comprehensive model evaluation methods are available cruz et al 2003b jorgensen and fath 2011 to better understand model behaviour and provide information to identify and adjust key inputs increasing accuracy and reliability which is crucial when models are used to support fire management and decision making riley and thompson 2016 some examples of model evaluation can be found in the scientific literature addressing fire modelling focused on model sensitivity cary et al 2006 hummel et al 2013 sturtevant et al 2009 uncertainty analysis benali et al 2016 pinto et al 2016 or scenario analysis perera and cui 2010 perera et al 2009 such approaches allow for a comprehensive assessment of these modelling tools which may partially overcome difficulties in their application perera et al 2015 despite the widespread use of fire simulation tools in multiple fire related fields their application to characterize fire related ecosystem functions services and disservices remains limited baskent 2020 sil et al 2019b still the identification of ecosystem attributes that promote their fire regulation capacity i e the ability of ecosystems and landscapes to regulate spatiotemporal attributes of ﬁre regimes through the control of factors affecting ﬁre behaviour resulting from the interaction between ecosystem processes fire and biophysical structures vegetation types and spatial patterns depietri and orenstein 2019 haines young and potschin 2018 pettorelli et al 2018 sil et al 2019a 2019b has been fostering the application of fire modelling tools to assess fire related services at the landscape scale such as the fire protection ecosystem service sil et al 2019b or the impact of fire on the climate regulation ecosystem service pais et al 2020 in this study we aimed to evaluate and apply the boreal forest landscape dynamics simulator fire regime module bfolds frm ouellette et al 2020 perera et al 2014 in a dynamic mountain landscape under mediterranean type of climate in europe bfolds frm is a spatially explicit process based model that simulates ﬁre growth implemented in the landis ii ecosystem modelling framework scheller et al 2007 bfolds frm was designed primarily to simulate fire regimes in boreal forests and has been used to study fire regimes and to support forest management and policymaking perera and cui 2010 rempel et al 2007 however its application and evaluation in other biogeographic and landscape settings is scarce this study builds on experience from a previous application of bfolds frm sil et al 2019b and seeks to better understand bfolds frm behaviour and its ability to simulate fire regimes in a fire prone mediterranean mountain landscape and to support the assessment of its fire regulation capacity frc and the potential provision of the climate regulation ecosystem service cres through the carbon stocks balance we addressed the following research questions i how sensitive are the model outputs to changes in input data quality attributes and user assumptions ii do model parameters need adjustments to emulate real world conditions in the study area iii does bfolds frm accurately predict observed fire patterns in the study area iv to what extent do bfolds frm outcomes support the assessment of frc and its impact on cres in the study area v what are the potential strengths and limitations of the model and how can its application be improved this study contributes to identify relevant aspects of the bfolds frm model for broader applications particularly in the context of fire regimes in mountain regions under a mediterranean type of climate as well as to support the application of the fire regulation concept in the management of fire prone landscapes 2 methods 2 1 general approach we evaluated the boreal forest landscape dynamics simulator fire regime module bfolds frm in terms of its behaviour and capabilities to explore and predict fire regime attributes in the context of fire prone mountain landscapes in the mediterranean region we run bfolds frm as a stand alone module using the sabor river upper basin ne portugal as the focal area to carry out model sensitivity analysis to support calibration and validation besides we applied bfolds frm according to an ecosystem services based conceptual framework to assess fire regulation capacity frc and the supply of the climate regulation ecosystem service cres in the study area fig 1 a framework was developed to evaluate bfolds frm based on the available standardized ecological modelling guidelines for developing and evaluating simulation models cruz et al 2003b jorgensen and fath 2011 swannack et al 2012 waveren et al 1999 four main steps were taken in the process 1 baseline simulations to define model reference behaviour for subsequent model evaluation steps 2 assessment of model outputs sensitivity to changes in inputs and parameters to better understand their relative effects on model outcomes 3 adjustment of parameters to improve the correspondence between model behaviour and expected fire patterns calibration and 4 evaluation of its predictive accuracy when applied to a set of observed data validation we then used bfolds frm to assess shifts in fire related ecosystem functions i e frc and its impact on services i e cres in the focal area we used the simulated fire regime attributes burned area and fire intensity to characterize frc and combined fire modelling with carbon storage modelling to assess cres supply in the study area 2 2 context of the application the european mediterranean basin accounts for most of the annual burned area 94 and fire ignitions 74 in the eu san miguel ayanz et al 2019 fire in the mediterranean basin has historically been human driven due to intensive land use and continuous use of fire in rural activities e g for pasture improvement which shaped landscapes ecosystems and ultimately fire regimes keeley et al 2012 the fire regimes or pyromes associated with mediterranean vegetation can be typified as high intensity large fires and low intensity small fires with short fire seasons archibald et al 2013 the high fire hazard of mediterranean regions is partially a consequence of favourable conditions for vegetation growth during the rainy season followed by warm dry summers keeley et al 2012 which is particularly relevant to the fire regime in mediterranean mountains fréjaville and curt 2015 in addition socio economic factors particularly relevant in mountainous areas lasanta et al 2017 and contemporary land management and fire suppression policies across the mediterranean rigolot et al 2009 further increased landscape homogenization and fuel accumulation and connectivity moreira et al 2011 the study was conducted in the sabor river upper basin a fire prone mountain area located in northeast portugal at the southwestern end of the cantabrian mountain range körner et al 2016 fig 2 the area is approximately 30650 ha and elevation ranges from 484 to 1487 m fig 2 climate is mediterranean beck et al 2018 with average annual precipitation ranging from 806 to 1262 mm and average annual temperature ranging from 8 5 to 12 8 c sil et al 2017 seminatural habitats cistus spp cytisus spp and erica spp dominate the landscape although the areas of native forests quercus pyrenaica and q rotundifolia forest stands of maritime pine pinus pinaster and agroforestry systems castanea sativa have been increasing over the last decades sil et al 2016 on the other hand demographic and socioeconomic factors contributed to farmland abandonment and the conversion of these areas over time which modified landscape composition and configuration and favoured more hazardous fuels and fuel continuity azevedo et al 2011 which potentially decreased the fire regulating capacity over time sil et al 2019b 2 3 bfolds frm model description boreal forest landscape dynamics simulator fire regime module bfolds frm simulates fire mechanistically i e computing fire on the landscape at hourly time steps from ﬁre weather fuel type and slope inputs providing spatially explicit information about the location of the ignition the pixels burned in each simulated fire event and the intensity at which each pixel burned bfolds frm uses the cffwis canadian forest fire weather index system van wagner and pickett 1985 to describe fuel moisture conditions and provide weather related inputs to fire simulation the weather parameters wind speed and wind direction and the cffwis components computed from consecutive sequences of daily weather conditions measured at noon temperature relative humidity and wind speed and the 24 h cumulative rainfall are supplied to bfolds frm to interpolate weather data across the simulated area the cffwis indexes are numeric ratings for the moisture content of litter and other dead fine fuels ffmc the moisture content of loosely compacted organic layers of moderate depth dmc the amount of fuel available for combustion bui each land cover category is assigned to one of the 16 fuel types described in the canadian forest fire behaviour prediction fbp system forestry canada fire danger group 1992 to predict fire behaviour in bfolds frm standard fuel types in bfolds frm are partially user modifiable allowing incorporation into the simulations of vegetation type features for a given area or region fire ignition information is supplied daily and its placement on the landscape is spatially explicit but whether it spreads or not is determined by the combination of daily fire weather fuel type at the location point and a user defined dmc threshold fire ignition location is seeded randomly either weighted or unweighted the weighted mode allows the user to define the exact location of the ignition to start a fire event or to supply a surface that spatially weights the probability of ignition on the landscape while the unweighted ignition seeding is spatially random fire extinguishment is caused by absence of burnable cells adjoining the fire perimeter weather conditions below the dmc threshold for fire spread or when the fire season ends meaning that no fire management activities are implemented and used to control the process 2 4 model evaluation 2 4 1 baseline simulations baseline simulations comprised the application of bfolds frm to the sabor river upper basin taking 2007 as the reference year appendix a and appendix b overall model parameterization included the reclassification of land use and land cover lulc spatial data for 2007 dgt 2019 as burnable i e natural grasslands agroforestry systems broadleaved forests coniferous forests mixed forests shrublands and non burnable i e urban areas agricultural areas open areas with either scarce or no vegetation and water bodies then burnable land covers were assigned to five standard fuel types from the canadian fire behaviour prediction system forestry canada fire danger group 1992 wotton et al 2009 c6 conifer plantation for the coniferous forests and the conifer dominated mixed forest m2 boreal mixewood green 25 conifers for the broadleaved forest and the broadleaved dominated mixed forest m2 boreal mixewood green 50 conifers for the broadleaved and the coniferous mixed forest o1 grass for grasslands and shrublands and o1b grass for agroforestry systems weather data used in baseline simulations were retrieved from the bragança weather station for the year 2007 a complete fire season i e 1 year containing noon daily records was used to compute the previously mentioned canadian forest fire weather index fwi system codes information on daily fire ignitions for the year 2007 was retrieved from the portuguese fire database icnf 2021 and used to feed the bfolds frm model data entries registered as false alarms as well as data entries for ignitions that originated fires smaller than 0 06 ha were excluded for model evaluation we constrained the model to simulate one fire season at a time i e time step 1 using the ignition and fire weather data of the correspondent year while the random number seeds parameter of the landis ii core model was kept constant thus preventing the model from determining ignition success in a fully stochastic manner appendix b as such we run the model one time one simulation assuming that the results represent the model reference behaviour under the defined set of conditions 2 4 2 model sensitivity assessment the one at a time oat sensitivity analysis method was applied varying one input factor at a time while keeping all other constant pianosi et al 2016 bfolds frm previously parameterized in the baseline simulation step was run n 31 to assess model sensitivity to variation in the quality of spatial data i e spatial resolution 25 m vs 100 and 300 m cell size and thematic resolution 1 ha vs 25 ha minimum map unit and fire weather data i e local weather station vs era5 land database as well as in the user assumptions regarding the spatial distribution of fire ignitions i e unweighted vs weighted fire ignition seed probability surface the spatialization of the fire weather data i e weather point data vs splined weather surface land cover to fuel type conversion i e standard vs custom fuel types and model built in parameters i e ignition and fire spread dmc thresholds smouldering fire interval crown base height of forest fuel types and fuel load and curing of grass fuel types appendix c the outputs analysed annual number of fires annual mean fire size and annual mean fire intensity appendix c were compared against the outputs of the baseline simulation we applied a statistical analysis and an index based analysis to evaluate model sensitivity the statistical approach evaluated the sensitivity of model outputs i e annual mean fire size and annual mean fire intensity to changes in the spatial and non spatial components of the model the non parametric wilcoxon mann whitney test neuhäuser 2011 was applied to test for difference in medians between baseline and sensitivity simulations outputs and the stochastic superiority measure a vargha and delaney 2000 a nonparametric statistic of effect size that informs about the magnitude of the differences between outputs from tested t and baseline d simulations by computing the probability that a random observation of t is higher than a random observation of d the a measure statistic was ranked in four classes vargha and delaney 2000 no difference 0 44 a 0 56 small 0 36 a 0 44 or 0 56 a 0 64 medium 0 29 a 0 36 or 0 64 a 0 71 large a 0 29 or a 0 71 all statistical analyses were conducted using the software r r core team 2020 the sensitivity index approach was applied to evaluate the sensitivity of the model outputs i e annual number of fires annual mean fire size and annual mean fire intensity to changes in the model built in parameters a dimensionless sensitivity index i lenhart et al 2002 was computed through variation of the parameter range by 20 and 40 from their default value tested in the baseline simulation step the sensitivity of each parameter was assessed by ranking the sensitivity index in four classes lenhart et al 2002 class i small to negligible 0 00 i 0 05 class ii medium 0 05 i 0 20 class iii high 0 20 i 1 00 and class iv very high i 1 00 2 4 3 model calibration we carried out the calibration procedure by manually adjusting model parameters used in the baseline simulation appendix d either one parameter at a time or multiple parameters at once n 318 the calibration was informed by the results of the sensitivity analysis in a trial and error exercise jorgensen and fath 2011 waveren et al 1999 an evaluation of calibration results was applied to minimize the error differences between simulated and observed data by comparing outputs from the calibration exercise against fire regime attributes krebs et al 2010 of annual number of fires annual total burned area and annual mean fire size observed in the sabor river upper basin for 2007 using these as reference criteria appendix d for that we calculated the relative error of simulated data in the study area for the selected attributes of the fire regime using equations 1 3 equation 1 renf nfsim nfobs nfobs x 100 equation 2 reba basim baobs baobs x 100 equation 3 remfs mfssim mfsobs mfsobs x 100 where renf is the relative error for the annual number of fires reba is the relative error for the annual total burned area and remfs is the relative error for the annual mean fire size nfsim basim and mfssim are the simulated annual number of fires total burned area and mean fire size respectively for year 2007 and nfobs baobs and mfsobs are the observed annual number of fires burned area and mean fire size respectively for the same year simulations with different parameter settings whose outputs showed a relative error ranging between 35 were assumed as acceptable for model prediction cruz and alexander 2013 and selected for the following model evaluation step 2 4 4 model validation we carried out model validation by applying bfolds frm to the sabor river upper basin to years 2015 and 2018 appendix e we used the portuguese fire database icnf 2021 to retrieve observed data on the annual number of fires and the annual burned area for the sabor river upper basin in these years based on results of model calibration tests in 2007 we identified different model parameters settings n 13 whose outputs resulted in a relative error within 35 and applied them to the 2015 and 2018 intermediate simulations therefore we run bfolds frm n 1 independently for each year analysed and model settings appendix e then we compared three selected attributes of the fire regime in the study area for 2015 and 2018 against simulated outputs appendix e we evaluated model accuracy by applying equations 1 3 to quantify model deviation from observed data assuming as acceptable an error of up to 35 in model predictions cruz and alexander 2013 finally from the intermediate simulations we kept the model parameter configuration that generated the outputs with the lowest relative error 2 5 assessment of the fire regulation capacity and of the climate regulation ecosystem service we modelled frc based on five fire regime attributes proxies derived from simulations with bfolds frm total annual burned area annual mean fire size annual mean fire intensity and number of fires with size above 100 ha and mean fire intensity above 500 kw m 1 in years 2007 reference year 2015 and 2018 we used two main criteria to assess frc 1 landscape capacity to regulate overall fire regime attributes and 2 landscape capacity to regulate large and potentially severe fires i e fires larger than 100 ha the oﬃcial size threshold for large fires in portugal icnf 2019 and mean fire intensity above 500 kw m 1 which is generally accepted as the limit for direct fire control by firefighting crews using hand tools hirsch and martell 1996 and the threshold for tree injury by fire van wagner 1973 we assumed that the sabor river upper basin landscape was able to regulate the spatiotemporal attributes of fire if 1 the total annual burned area the annual mean fire size and the mean fire intensity decreased compared to the year 2007 and or 2 the number of fires larger than 100 ha and the mean fire intensity above 500 kw m 1 remained at 2007 levels besides we investigated the effect of frc dynamics on the supply of the climate regulation ecosystem service cres is the avoided release of carbon to the atmosphere resulting from the capacity of terrestrial ecosystems to act as carbon sinks haines young and potschin 2018 keith et al 2021 sil et al 2017 and was modelled by applying the invest integrated valuation of ecosystem services and tradeoffs carbon storage and sequestration module sharp et al 2020 to the sabor river upper basin in 2007 2015 and 2018 using data from previous work in the area sil et al 2017 appendix f we assumed the carbon stored in aboveground biomass agb as a proxy for the climate regulation es supply haines young and potschin 2018 keith et al 2021 then we used the total annual burned area simulated with bfolds frm in each year to estimate the potential losses of carbon stored in the landscape assuming that all carbon stored was released into the atmosphere therefore we expressed the effect of frc on cres as the difference between the estimated maximum potential supply of the cres and its potential loss resulting from simulated fires in the study area we used 2007 as the reference year from which we compared the corresponding carbon losses due to fire additionally to improve results interpretation we carried out an ancillary analysis on land use and land cover changes for the study area and the distribution of the number of days per fire weather index fwi class for each of the years analysed appendix g 3 results 3 1 bfolds frm model evaluation 3 1 1 sensitivity analysis statistical analysis showed that the distribution of bfolds frm outputs when tested for sensitivity to changes in the quality and assumptions of spatial fuel type and weather inputs only differed significantly for changes to spatial grid resolution cell size from 25 to 300 m and thematic resolution 1 25 ha table 1 and appendix c still all these inputs had meaningful effects on the annual mean fire size and mean fire intensity showing differences that ranged from small to large except when tested for changes to fire weather data spatialization and fuel types parameters table 1 and appendix c regarding the relative influence of the tested built in parameters on output variables significant differences only were found when varied fuel curing parameters decreased by 40 and 20 showing a small to large effect on both the annual mean fire size and mean fire intensity outputs also both the ignition dmc threshold and smoulder interval parameters showed a small effect on model outputs table 1 and appendix c nevertheless sensitivity index based analysis indicates that all output variables were sensitive to changes in spread dmc threshold and smoulder interval parameters also all output variables showed very high sensitivity to changes in ignition dmc threshold parameter and fuel curing parameters table 1 and appendix c in turn changes made to crown base height and fuel load parameters showed a more pronounced sensitivity for the annual mean fire intensity output than for the annual number of fires and mean fire size table 1 and appendix c 3 1 2 model calibration results from the baseline simulation indicate that the fire regime attributes simulated with bfolds frm deviated substantially from the data observed in the study area in 2007 overestimating total burned area and mean fire size and underestimating the number of fires table 2 conversely the estimated relative error re of the fire regime attributes simulated after model calibration fig 3 appendix d became within the range of 35 indicating satisfactory model predictions table 2 3 1 3 model validation overall bfolds frm was able to emulate observed trends of selected attributes of fire regime for the three years analysed with an estimated mean relative error within the 35 interval considered as reasonable for model predictions in the validation step table 3 on average bfolds frm overpredicted annual number of fires and annual total burned area and underpredicted annual mean fire size table 3 despite the satisfactory overall model performance the area burned in 2018 was overpredicted with a relative error outside the range accepted as reasonable although predictions for both the number of fires and mean fire size were within that interval table 3 the predicted burned area for 2015 had a substantially lower relative error although the simulated annual number of fires was greater than the number of fires observed which increased the associated relative error in turn underpredicting the mean fire size table 3 3 2 assessment of modelled fire related functions and services 3 2 1 fire regulation capacity overall most of the simulated fires in the sabor river upper basin were small and of low intensity although differences were observed among the three dates analysed fig 4 simulated fire regime attributes increased in 2015 compared to 2007 with fires exceeding 100 ha and the 500 kw m 1 mean fire intensity table 4 on the other hand overall simulated attributes in 2018 decreased compared to 2007 table 4 in 2018 simulated fires were smaller and less intense than in 2007 but one attained more than 100 ha with mean fire intensity 500 kw m 1 fig 4 and table 4 3 2 2 impacts of fire on the carbon storage balance and the climate regulation ecosystem service the maximum potential for carbon storage in the landscape increased from 2007 on compared to 2007 our estimates indicate that carbon stocks in 2015 increased by 40 while in 2018 increased by 70 table 5 on the other hand the losses in the maximum potential carbon stored in the landscape due to the simulated fires increased over time fig 5 and table 5 in 2015 losses represented 1 2 of the maximum potential for carbon storage in the landscape indicating an increase of 80 compared to 2007 table 5 in 2018 losses represented 0 7 of the maximum potential for carbon storage in the landscape increasing 20 compared to 2007 4 discussion 4 1 model evaluation changes made to the model spatial inputs did affect its outputs see table 1 which is in line with findings reported for other modelling platforms that simulate fire mechanistically cary et al 2006 as an ecological process fire creates landscape heterogeneity and responds to the spatial patterns of fuel composition and configuration which are crucial for its regulation turner 2010 turner et al 2012 fire spread is simulated in bfolds frm as a mechanistic process where among other factors spatial patterns of pixel based fuel types and topographic layers are crucial perera et al 2008 in this regard it is important to ensure that the data used in the parameterization of model spatial inputs has the best quality possible in terms of spatial and thematic resolution to describe dynamics accurately since they affect modelling and simulation of landscape fire processes saura 2002 taneja et al 2021 turner and gardner 2015 on the other hand although our indicators did not show a clear effect of the spatial weighting pattern based on historical fire ignition records on model outputs see table 1 similar to other studies bar massada et al 2011 perera et al 2009 we consider that applying this information to fire simulations may be relevant particularly in the context of mountains in the mediterranean region since fire ignition patterns in these areas are influenced by the use of fire as a management tool catry et al 2009 sequeira et al 2020 our results suggest that the quality of spatial data inputs is important since it affects the model outputs as such the user must be aware of the trade offs that may arise when selecting spatial inputs for model parameterization for example calibrating the model with the most detailed data available benefits the accuracy of model predictions but may also increase the computational costs to perform the simulations taneja et al 2021 besides integrating the fire ignition patterns in the simulations is useful to represent real world dynamics in mountainous landscapes and thereby to characterize the fire regime in these areas fernandes et al 2014 keeley et al 2012 as expected bfolds frm output variables were also sensitive to changes made to weather inputs see table 1 since these comprise the most important information for simulating fire patterns perera et al 2008 our results agree with findings from other fire modelling platforms where climate variables were very important for fire simulation cary et al 2006 hummel et al 2013 inconsistencies in the rainfall variable between observed bragança weather station and estimated era5 land dataset appendix a can partially explain the variation found in our results with the latter underestimating the fire weather indices in the study area and thereby decreasing both burned area and fire intensity the availability of fire weather data is a critical point in the application of fire simulation models riley and thompson 2016 despite recent advances in the supply of ready to use fwi and subindices data at the global scale e g era5 reanalysis products vitolo et al 2020 their application at the local scale remains limited due to their spatial resolution e g 28 56 km grid cell size as for prediction of the future fire danger conditions using meteorologically based indices e g the canadian fire weather index uncertainties regarding the use of weather data e g regional climate models have been described in the scientific literature indicating either a potential negative bias in fire predictions herrera et al 2013 or acceptable agreement between weather information and fire predictions amatulli et al 2013 ultimately our results stress that the weather data quality is important for estimates of fire weather conditions since those are critical for fire simulations in bfolds frm regarding the influence of modifications in the fbp standard fuel types although bfolds frm has responded to those changes the indicators did not show a relevant influence on model outputs see table 1 which partially supports the results reported by sturtevant et al 2009 the recalculation of the fire spread rate ros for the standard fbp o1 fuel type reduced the potential fire spread rate in turn restricting the size of the simulated fires however more substantial ros differences would have been observed if fire weather conditions used in simulations were more severe especially in terms of wind speed as it affects the computation of the initial spread index isi forestry canada fire danger group 1992 wotton et al 2009 on the other hand outputs were highly sensitive to changes made to the degree of grass curing see table 1 since ros is greatly enhanced by fully cured grass forestry canada fire danger group 1992 wotton et al 2009 the replacement of fbp o1 fuel subtypes with custom fuel types allowed to improve the representation of the vegetation found in the study area namely semi natural and agroforestry areas since o1 fuel subtypes are limited to grasslands either matted o1a or standing dead o1b forestry canada fire danger group 1992 also in shrublands which are abundant in the study area ros is mostly controlled by vegetation height anderson et al 2015 whereas grassland ros varies mainly with the degree of grass curing cruz et al 2015 and fuel load cruz et al 2017 thus underlining the need for adjustments see tables 2 and 3 studies carried out elsewhere have emphasized the need to either develop or adapt fuel models to improve fire behaviour estimates for local vegetation types clark et al 2008 cruz and fernandes 2008 fogarty et al 1998 number of fires and mean fire intensity and size variables were very sensitive to changes made to parameters based on the duff moisture code dmc see table 1 this behaviour was expected since the dmc is an indicator of fuel consumption in boreal forests wotton 2008 hence our results reflect how bfolds frm applies the principles underlying the canadian forest fire danger rating system to simulate fire ignition spread and extinguishment perera et al 2008 calibration of the dmc based parameters indicated improvements in model ability to emulate the fire regime patterns observed in the study area see tables 2 and 4 which agrees with other studies carried out in the mediterranean region dimitrakopoulos et al 2011 that suggest the need to adapt and or modify assumptions related to the dmc to improve the predictions of both fuels moisture and burned area on the other hand the need for adjusting dmc based parameters may reflect in part the variability associated with the prediction of burned area and number of fires when the fwi indices are applied across the mediterranean basin amatulli et al 2013 such differences may be partially related to the type of dominating vegetation i e forest or shrubland cruz et al 2003a dimitrakopoulos et al 2011 fernandes 2016 or to the different characteristics of the duff layer in mediterranean ecosystems and boreal areas in terms of quantity depth and moisture dimitrakopoulos et al 2011 particularly when related to shrubland fernandes 2005 2016 also strategies of full fire suppression in mediterranean countries rigolot et al 2009 may hamper the fwi based simulation of fire nevertheless the dmc is a useful predictor of fuel consumption in maritime pine stands fernandes and loureiro 2013 while the moisture content of the duff layer variable used in the calculation of the dmc code has been correlated to re ignition events and the occurrence of smouldering in aleppo pine stands xifré salvadó et al 2020 given the importance of these parameters to simulate fire in bfolds frm and the limitations and uncertainties regarding the use of fwi indices in the mediterranean region particularly the role of dmc in shrubland fires careful calibration of the dmc based parameters in bfolds frm should be considered 4 2 patterns in fire related functions and services overall our results indicate that frc in 2015 decreased compared to 2007 while it increased in 2018 although the capacity to regulate potential large and intense fires decreased from 2007 onward see table 4 and figs 3 and 4 land cover changes that took place in the sabor river upper basin landscape over time see appendix g together with the bfolds frm outputs can explain in part our results forest expansion between 2007 and 2015 mostly at the expenses of seminatural areas but also of some non burnable areas e g agriculture led to increasing fuel continuity and fuel hazard in the landscape allowing fire to spread more easily over larger areas and with high intensity see table 4 and figs 3 and 4 on the other hand between 2015 and 2018 although the vegetated area continues to grow the transition from vegetated to non burnable areas may have enabled in some cases the disruption of fire propagation in the landscape also the conversion of coniferous forests mainly to deciduous forests may have balanced the general fire intensity although such changes have not prevented the occurrence of a large fire in the simulations see table 4 and figs 3 and 4 similar patterns were observed in previous studies carried out in the study area concerning the effect of lulc changes on the dynamics in the capacity of the landscape to regulate fire azevedo et al 2011 sil et al 2019b as well as for other areas the mediterranean basin regarding the effects of changes in lulc on fire hazard and fire regime moreira et al 2011 san miguel ayanz et al 2012 and fire regulating functions depietri and orenstein 2019 landscape changes between 2007 and 2018 promoted carbon storage in the sabor river upper basin landscape see table 5 particularly through the expansion of forest areas together with vegetation growth and increase of carbon stocks in aboveground biomass see appendices f and g our results are in line with previous work assessing the cres carried out in the area sil et al 2017 as well as for other mountainous areas where similar patterns of landscape changes tend to increase the supply of cres locatelli et al 2017 pais et al 2020 on the other hand the observed changes have modified the landscape capacity to regulate fire in the years analysed threatening the supply of cres differently as suggested by the variation in carbon losses see table 5 and fig 5 for example in 2015 the growth of carbon losses by 80 compared to 2007 can be partially related to the decrease in frc due to important changes in landscape structure and composition that were reflected in the increase in the simulated total burned area in 2018 although frc increased as suggested by the reduction in the burned area carbon losses due to fires increased by 20 compared to 2007 such an increase reflects a large fire partially driven by increasing forest fuels continuity in the landscape together with an increase of carbon stocks over time which resulted in greater carbon losses our results are in line with findings reported by thom and seidl 2016 concerning the impact of fire disturbances on carbon storage as well as the potential risks associated with the increase of forest areas e g to balance carbon emission envisioned in the european green deal european comission 2019 discussed in hermoso et al 2021 in addition to the effects of landscape changes on the frc dynamics and the supply of cres our results also reflect the influence of annual fire weather conditions for example although 2015 showed the lowest number of fire ignitions among dates it accounted for more days with very high and extreme fire danger fwi 45 compared to 2007 and 2018 see appendix g in turn the growing fuel continuity and fuel hazard in the landscape overtime along with more severe weather conditions may have increased susceptibility for large and intense fires in 2015 simulations increasing the impact of fires on cres supply see fig 5 conversely 2018 accounted for more days with low fire danger fwi 23 which may have limited the conditions for fire spread and the reduction of the overall fire size and fire intensity see table 4 and figs 3 and 4 despite the number of simulated ignitions exceeded 2015 however when weather conditions worsened during the summer season and fuel continuity and fuel hazard in the landscape were relatively high a large and intense fire was simulated that impacted high carbon stock area see fig 5 our results agree with findings that show a relationship between fwi thresholds and fire size fernandes 2019 however a more detailed analysis should be carried out to reduce uncertainty regarding the effect of landscape structure and fire weather conditions in driving fire regime attributes fernandes et al 2016 4 3 modelling strengths limitations and recommendations for future research overall bfolds frm is a valuable tool for informing sustainable planning and management of mountainous areas prone to fires in the mediterranean wherein similarly to the sabor river upper basin increasing fire hazard in the landscape is an ongoing process driven by rural depopulation and vegetation encroachment in abandoned areas azevedo et al 2011 lack of effective forest management pérez rodríguez et al 2018 and potential intensification of afforestation activities due to demand for natural resources e g bioenergy pérez rodríguez and azevedo 2020 or actions to cope with climate change hermoso et al 2021 the high predictive capacity of the model allowed to estimate the selected fire regime attributes of the sabor river upper basin within an acceptable range of error regarding the observed data see table 3 which in turn enabled characterizing the dynamics of the landscape capacity to regulate fire see table 4 and fig 4 as well as to estimate potential impacts on the supply of the climate regulation ecosystem service see table 5 and fig 5 ultimately bfolds frm is particularly useful for testing fire smart management alternatives that aim fire hazard mitigation while benefiting the supply of ecosystem services at the landscape scale damianidis et al 2021 fernandes 2013 pais et al 2020 enabling mediterranean mountains to cope with challenges related to their high vulnerability to global changes schroter et al 2005 although bfolds frm was able to emulate the observed pattern of the relative dominance of small fires in the study area see fig 4 after comparing simulated outputs with fire records it was clear that the model was not able to accurately capture the occurrence of large fires except in year 2015 appendix e such deviation may be related to oversimplification of fuel types used in the simulations after aggregation of land cover data in major lulc classes as well as to simplification of the fire weather conditions based on daily rather than hourly estimates for the whole study area since these are of particular relevance concerning large fires fernandes et al 2016 in addition the conceptualization and mechanics of the model itself can explain these differences because unlike other models e g medfire brotons et al 2013 the size and extinction of the simulated fires in bfolds frm are neither predefined nor depend on modelled fire suppression efforts but rather are emergent properties of the simulation process arising from the interaction between the theoretical assumptions of the model the assumptions of the users and the inputs provided perera et al 2014 notwithstanding our effort to assess and apply bfolds frm we acknowledge some limitations in our modelling framework model evaluation results were obtained based on model runs without replicates as we set the parameter random number seeds as a constant that is preventing the model from having a stochastic behaviour this approach is common in this type of assessment and useful when testing the sensitivity of several model parameters grant and swannack 2008 because it allows evaluating the relative effect of each parameter avoiding the noise that comes from the model stochasticity besides it decreases the number of simulations and the computational costs and time spent to complete the model evaluation steps grant and swannack 2008 however we acknowledge that this approach fails to simulate and assess the uncertainty in model predictions that can derive from variability in the frequency and the location of fire ignitions riley and thompson 2016 therefore we recommend that future work should consider model output variability as a way to optimize and improve model predictions for its implementation as a management and decision support tool uusitalo et al 2015 sensitivity analysis allowed a deeper understanding of the relative influence of bfolds frm inputs and parameters on model outputs see table 1 however we recognize that the approach used i e one at a time oat method has some limitations for instance it does not allow to effectively explore the interaction of inputs and parameters and thus quantifying their combined influence on model outputs saltelli et al 2019 although more comprehensive approaches do exist e g all at a time aat methods pianosi et al 2016 these were not considered due to their implementation complexity which particularly increases when the model relies heavily on spatial inputs pianosi et al 2016 as well as due to the high number of simulations and time required to process model results and proceed with model evaluations uusitalo et al 2015 moreover we carried out our analysis considering only three years 2007 2015 and 2018 we acknowledge that these years may not represent the full range of conditions existing in the study area which may restrain model response during the calibration step and raise uncertainties regarding model behaviour validation on the other hand complete and compatible data to parameterize the model and perform the calibration and validation steps are limited for the study area although statistical methods available in the scientific literature e g cross validation refaeilzadeh et al 2009 can be useful in model training and validation when available data is scarce their application would be unfeasible due to the model mechanics to overcome these issues at least partially we used data from the same system but for different years waveren et al 1999 which introduced some variability in the environmental conditions such as the daily weather conditions as well as in the spatial composition and configuration of land cover classes fuel types or the daily fire ignition data although our approach allowed us to evaluate the behaviour and capabilities of the model we acknowledge that improving the model evaluation process with more model applications is needed either testing the model in the study area using independent data or applying it to similar systems outside the study area will strengthen the applicability and usefulness of this tool in fire related research in mountain mediterranean landscapes our modelling approach captured the influence of landscape dynamics on fire behaviour see table 4 and figs 4 and 5 but it was not able to capture the effect of fire on the landscape simulating such interaction is essential to represent feedbacks between disturbances and landscape dynamics turner 2010 as such we acknowledge that uncertainties may persist regarding whether the model can effectively characterize the fire regime and the frc in the study area despite its ability to emulate the selected fire regime attributes in the years evaluated therefore future application of bfolds frm may benefit from coupling the model with e g a succession extension of the landis ii platform scheller et al 2007 allowing to simulate interactions and feedbacks between landscape and fire besides it would enable continuous spatiotemporal outputs to explore future changes in fire regime lastly our approach to assess the impacts of fire on carbon stored simplifies the potential interactions between fire and carbon dynamics as such carbon stocks represent mean values for generic lulc classes and those are only for the aerial biomass appendix f while disregarding other carbon pools in terrestrial ecosystems such as litter and soil lorenz 2013 that can also be affected by fires garcia hurtado et al 2013 we assumed that fire fully consumed aerial biomass in turn releasing all the stored carbon into the atmosphere which is seldom the case since part of the carbon resulting from biomass burning can incorporate dead organic matter and soil pools as pyrogenic carbon jones et al 2019 therefore future work on the effects of fire on carbon balance should consider a more comprehensive assessment of carbon stocks assigned to each lulc class and sub processes that are part of carbon dynamics as a result of fire activity thom and seidl 2016 5 conclusions the evaluation of bfolds frm contributed to a more in depth understanding of the relative influence of inputs on model behaviour and to improve the identification of key parameters for fire simulation moreover adjustments made to model inputs and parameters improved bfolds frm accuracy and emulated fire regime attributes in the sabor river upper basin with a relative error within acceptable bounds in this regard we emphasize that the application of bfolds frm in the context of the fire regime of mediterranean mountainous areas must consider i supplying the model with accurate and precise data to properly characterize the spatial patterns of fuels and fire weather conditions ii carefully calibrating parameters based on fire weather conditions due to the high model sensitivity to these parameters iii customizing standard fuel types to meet the characteristics of the existing vegetation and iv including ignition patterns associated with human activities our work underlined that bfolds frm can provide useful outputs to support the characterization of fire related ecosystem functions and services in a mediterranean mountainous area still we have identified potential limitations that may arise when applying the model to different areas which should thus be considered in future applications software name boreal forest landscape dynamics simulator fire regime module bfolds frm for landis ii programming language c developers ajith h perera ontario forest research institute marc r ouellette ontario forest research institute den boychuk ontario fire science and technology program marc ouellette ontario ca available at https www landis ii org extensions funding â sil received support from the portuguese foundation for science and technology fct through ph d grant sfrh bd 132838 2017 funded by the ministry of science technology and higher education and by the european social fund operational program human capital within the 2014 2020 eu strategic framework p m fernandes contributed in the framework of the uidb 04033 2020 project funded by the portuguese foundation for science and technology fct declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge marc ouellette for his technical support on issues related to the bfolds frm software author contributions â s j c a p m f j a and j p h designed the research â s performed the simulations and analysed the data â s j c a p m f j a j p h contributed to the final version of the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105464 
25573,in situ water quality sensors yield continuous high frequency measurements that are essential for long term monitoring optical sensors such as turbidity sensors are prone to lens obstruction and ultimately anomalous measurements in this study we developed a novel approach to detect remove and replace these anomalies a free open source software package implementing this approach is provided anomaly detection and removal is completely automated though manual verification is recommended sections of missing data are filled by one of two methods depending on the size of each section this approach was tested on turbidity measurements from sensors deployed across southern ontario automated detection performance was consistent across all test datasets with most 95 synthetically introduced anomalies removed in all but one dataset gap filling provided accurate estimates for smaller gaps while performance on larger gaps varied overall this approach as implemented in the provided software can greatly assist with data management in long term monitoring programs keywords turbidity sensors data correction surface water monitoring anomalous data multiple imputations 1 introduction in situ sensors are practical and common for surface water monitoring these sensors can measure various water conditions without the need to collect transport and analyze water samples sensors also allow for high temporal resolution measurements that would otherwise be impractical for a sample based monitoring approach rode et al 2016 younos and heyer 2015 as sensor technology has improved measurement capabilities and frequencies of these devices have increased this had led to a substantial increase in the amount of data to manage correct and quality assure wagner et al 2006 the deployment of in situ sensors occurs in natural environments where various interferences can affect the accuracy and quality of the collected data the frequency and magnitude of these problems are often specific to the sensor utilized for monitoring younos and heyer 2015 environmental factors can obstruct the lens of optical sensors reducing the accuracy of the measurements alferes et al 2013 for example ice formation around the sensor during the winter is a common interference a year round issue is storm runoff carrying debris twigs leaves small branches and garbage into the stream which gradually collects and obstructs the sensor anomaly detection and correction methods for high frequency data are generally laborious and time consuming with little to no automation and many rely on operator subjectivity to determine faults in the data wagner et al 2006 some recent work has examined whether machine learning techniques can assist in sensor data corrections lucius et al 2020 but there has been a lack of focus on turbidity a key parameter used to estimate nutrient and sediment fluxes in novel monitoring approaches e g robertson et al 2018 in this study a novel approach is developed and applied to correct data from in situ turbidity sensors turbidity is a measure of the haziness of a fluid due to suspended particles grobbelaar 2009 in natural surface waters turbidity is a significant indicator of water quality as rivers and lakes become increasingly turbid less light can reach aquatic organisms on the sediment surface leading to adverse effects on the local ecosystem in addition high concentrations of suspended sediment can lead to many physiological effects on fish species and can even be fatal if concentrations are high enough for sustained periods bash and berman 2001 turbidity can also be used as a proxy to estimate other important parameters in the stream such as total phosphorus concentrations horsburgh et al 2010 lannergård et al 2019 ryberg 2006 robertson et al 2018 sensor data must be reviewed for interferences and anomalies e g fouling significant efforts in environmental science literature have been dedicated to the study detection and correction of anomalies many early publications recommended manual verification approaches which were tedious subjective error prone as well as labour and time intensive wagner et al 2006 kotamäki et al 2009 found that it is especially challenging to ensure data quality from large monitoring networks highlighting the need for data correction approaches that consider the physical properties of each monitored system e g turbidity increases with water level for streams located in natural environments horsburgh et al 2015 demonstrated a manual system for correcting sensor anomalies where the user interface made correcting common anomalies such as outliers or sensor drift straightforward and graphical however this method relied on expert interpretation regarding classification and correction of these anomalies thus there is a need for automated cross platform and cross discipline identification of anomalies in environmental sensor data cook et al 2020 approaches based on statistical identification of anomalies have been used in the environmental sciences for instance hill and minsker 2010 compared a variety of data driven time series model approaches nearest neighbor single layer linear network multi layer perceptron to identify and mitigate anomalies in high frequency wind speed measurements their method showed that even a univariate approach could be effective in identifying and mitigating anomalies taylor and loescher 2013 presented an approach to anomaly detection using a suite of plausibility tests based on observed statistical properties of the sensor data for instance possible anomalies were detected using extreme values of a variable s first and second moments or of subsequent pairs this approach was proposed for the united states national ecological observatory network other approaches use complex statistical models to identify anomalies alferes et al 2013 suggested a multivariate processing method using principal component analysis pca and partial least square pls for sensor data assessments this pca pls based approach involved developing a model to predict turbidity using multiple reference sensors e g water level ph temperature precipitation which helped automate the data correction process while effective in certain circumstances this does require multiple sensors and may lead to unclear results when there are anomalies between multiple sensors the accuracy of the model may also limit the anomaly detection still other approaches are aimed at individual research projects though are often beyond the abilities of many environmental researchers to work with russo et al 2020 showed that integrating an expert system of domain experts with multiple machine learning algorithms was able to detect anomalies accurately with a minimal number of anomalies identified by experts as training data leigh et al 2019 showed in an application to turbidity conductivity and stream level that the relationships between these variables were quite complex making models as the basis for anomaly identification inefficient rather they recommended using feature detection approaches which examine the distributions of multiple data streams in a transformed feature space and found these to outperform time series models while the field of anomaly detection is a thriving line of inquiry there is a need to make freely accessible open source tools that are available to field researchers using sensors in their research with a minimum of additional overhead and additional skill sets in this study we aimed to develop a novel approach to detect correct and replace anomalies within high frequency turbidity sensor measurements specifically anomalies caused by external interference which cause unrealistic spikes in turbidity measurements are targeted the effectiveness of each step in this new approach is tested and discussed tests were done where a variety of analysts were encouraged to find a number of synthetic errors to illustrate both the effectiveness of the software and quantify the degree of user specificity of it in addition to developing and testing this new approach its implementation as an open source software package that can be used by analysts with little or no programming experience was a primary objective this software can assist environmental scientists and engineers with managing turbidity sensor data which is especially useful in long term monitoring studies 2 methods 2 1 study area and data collection as a part of the multi watershed nutrient study mwns mohamed et al 2019 led by the ontario ministry of the environment conservation and parks mecp in situ sensors measuring turbidity and discharge alongside rain gauges were deployed at several monitoring locations situated in highly agricultural areas across southern ontario see rosamond et al 2018 for site selection process continuous measurements 15 min intervals were collected between 2015 and 2019 from 10 mwns monitoring stations a map of all monitoring locations is shown in the supplementary information fig s1 data availability of turbidity varies as stations were deployed at different times throughout the study but all locations had at least two years of continuous measurements observed anomalous measurements in the turbidity data were generally caused by debris interfering with the optical sensor and or ice formation around the sensor during winter months these anomalies are evident from the large individual or clustered spikes in the data fig 1 while this study was designed to target anomalies caused by these external environmental factors other types of anomalies may be present such as sensor drift but were not targeted in this study a more detailed description of these anomalies is provided in section 2 3 2 2 data correction workflow the overall correction approach consists of three consecutive components automated anomaly detection and removal manual verification and missing data modelling first automated anomaly detection and removal is accomplished through a univariate algorithm that also fills small gaps in the data regardless of whether the data were originally missing or removed by the algorithm next manual verification is recommended to correct false negatives or false positives i e data that were incorrectly classified by the algorithm lastly large sections of missing data are filled with estimates from multivariate imputation models 2 2 1 automated detection and removal algorithm first automated detection and removal of anomalies is done through an algorithm that can process years of high frequency sensor data in a relatively short amount of time e g a few minutes this algorithm exploits autocorrelation a common property of time series data that represents correlation between data to previous time steps it is assumed that measurement frequencies are sufficient such that autocorrelation exists due to the high frequency measurements of the sensors used in this study and lag times associated with typical water quality changes it is assumed that any drastic changes between measurements are caused by external interference and are thus anomalous note that we do not consider the effect of limits of detection detection limits were judged unimportant because low turbidity levels occurred at low flows consequently any data near the limits of detection in agricultural or urban streams would be lumped into the baseline data and unaltered from the original values the complete anomaly detection and removal algorithm alongside descriptions of the parameters and criteria is as follows step 1 estimate site specific parameters using all available measurements baseline threshold a the default value is set to the 75th percentile of the raw input data data below this threshold are baseline data upper transition threshold b the default value is set to the 90th percentile of the raw input data data below this threshold and above a should be transitioning from baseline levels to a weather related event peak or returning to baselines levels from a peak maximum acceptable absolute change in turbidity c the default value is set to the 99th percentile of the first differences difference between consecutive data points this parameter can be tuned to balance false positives with false negatives e g increasing the value of this parameter may reduce false positives but increase false negatives step 2 process data points x sequentially beginning with the x 6 as forecasted estimates are based on prior five measurements in the time series i x t is assigned to a section depending on the values identified in step 1 a baseline x t a b transition a x t b c event x t b ii if x t is in the baseline section x t is kept as the anomalies targeted in this study will not be present in this section otherwise three additional parameters are calculated a absolute change between the current and previous point ac 1 a c x t x t 1 b absolute percent change between the current and previous point pc 2 p c x t x t 1 x t 1 100 c absolute difference between current percent change and previous percent change dpc 3 d p c x t x t 1 x t 1 x t 1 x t 2 x t 2 100 iii if x t is in the transition section the following checks are made if any condition is true x t is retained a ac 5 ntu b dpc 30 c pc 50 iv if x is in the event section the following checks are made if any condition is false x t is flagged as anomalous and removed from the corrected series a ac c b dpc 50 v if x t was originally missing or removed by the algorithm it is filled with an estimated value a if x t was originally missing from the dataset i a forecasted estimate from a local linear regression modelled to x t 6 through x t 1 is made if this estimate is a positive value fill using this estimate ii otherwise fill by replicating x t 1 b if x t was removed by the algorithm i a forecasted estimate from a local linear regression modelled to x t 6 through x t 1 is made if this estimate is a negative value fill by replicating the previous measurement ii if the estimate is a positive value and less than the original measurement fill using the estimated value iii otherwise the original value is reintroduced and accepted anomalies caused by external interference will result in unusually high measurements and so modelled estimates that are larger than measurements are assumed to be inaccurate step 3 sections containing three or more consecutive estimated data points are removed the local linear models are used to fill small gaps one or two data points e g those shown in fig 1a and are not trusted to provide accurate estimates for larger gaps as local linearity is only a valid assumption for small sections of non linear data the sections described in the algorithm baseline transition event have different conditions to determine if a measurement is anomalous because realistic percent changes in turbidity at lower levels e g 1 ntu to 4 ntu 400 increase may be anomalous at higher levels e g 100 ntu to 400 ntu 400 increase similarly an absolute change in turbidity between measurements may be acceptable at higher levels but may be considered anomalous at lower levels an example of turbidity data measured at the little ausable river alongside the two calculated thresholds parameters a and b can be seen in fig 2 these thresholds help us understand the site specific characteristics of the little ausable river turbidity reaches relatively high levels during the first peak while the second peak is a relatively minor event this example will be revisited to qualitatively assess the performance of the entire data correction approach 2 2 2 manual verification next manual inspection and verification are recommended to correct misclassifications of data the provided software allows users to overrule the automated anomaly detection algorithm for any data points that are believed to be misclassified the default automated algorithm settings were chosen to limit false negatives undetected anomalous data therefore there may be a relatively high number of false positives non anomalous data that was removed given the nature of operating in situ optical sensors a fully automated data correction process is unlikely to provide completely reliable data therefore some manual effort is recommended to approve changes made by the algorithm and ensure reliable results 2 2 3 missing data modelling lastly large sections of missing data are filled following manual verification the dataset should only contain non anomalous measurements and large gaps of missing data in this study we utilized the multivariate imputation by chained equations mice package in r to estimate turbidity data using timestamps and supplemental parameters as independent variables mice involves running a series of imputation models in which each variable with missing data is modelled conditionally on the other variables in the dataset the r mice package buuren and groothuis oudshoorn 2011 includes several imputation model choices which may be selected by the user please refer to https cran r project org web packages mice mice pdf for more information regarding imputation options for mice additionally supplemental parameters related to the variable of interest may be utilized in this study water level and precipitation data were used as additional parameters for predicting turbidity mice begins by generating simple mean imputations that temporarily fill any missing values in for all parameters these mean imputations act as placeholder values next the placeholder values for a selected parameter such as turbidity are removed then the selected parameter is set as the dependent variable all other parameters then act as independent variables to train the imputation model many imputation methods can be used including predictive mean matching linear regression and classification and regression trees cart for this study a cart model was used with a maximum of 50 iterations to prevent overtraining of the regression trees a cart model is sequentially generated for each parameter that has missing data at this point all missing values have been replaced by imputed estimates from cart models though to account for imputation uncertainty and avoid dependence on the order in which the variables are imputed this entire process is repeated across five cycles while the placeholder values were replaced during the first cycle the generated cart models used placeholder values for the independent variables therefore the cart models in subsequent cycles are built on more accurate estimates from cart models in the previous cycle rather than those placeholder values cart was chosen as it is a multivariate tree building and parsing technique lewis 2000 it is versatile enough to fit interactions nonlinear relations and complex distributions without parametric assumptions or data transformations burgette and reiter 2010 he et al 2009 it operates by seeking to approximate the conditional distribution of a univariate outcome from multiple predictors the cart algorithm partitions the predictor space such that subsets of units generated by the partitions have relatively homogeneous groups it then iteratively subdivides the data at each possible split selecting the most suitable partition that produces the most homogenous subgroups the series of the splits can be effectively represented by a tree structure with the leaves corresponding to the subsets of the sample group burgette and reiter 2010 butalia et al 2008 hastie et al 2017 lewis 2000 once the missing data is filled a single exponential smoothing process is applied to the modelled data to compensate for sharp discontinuities bhattacharya and solomatine 2006 wang and witten 1996 single exponential smoothing is used for short range forecasting where the model assumes that there is no trend or consistent pattern of growth within the dataset the formula for single exponential smoothing is 4 s t α x t 1 1 α where st is the smoothed estimate at the current time t x is the measured observation and α is the smoothing constant the α values range from zero to one and the closer the α is to zero the more heavily weighted the older values are when smoothing charles 2004 through trial and error the α parameter was set to 0 05 to minimize prediction error equation 4 was applied recursively to the modelled data for the entire time period thus computation of each new smoothed value represents the weighted average of the current observation and the previous smoothed observations in effect each smoothed value is the weighted average of the past values with the single exponential smoothing assigning decreasing weights as the values get older in other words recent observations are given relatively more weight in forecasting than the older observations gooch 2011 kalekar and bernard 2004 2 2 4 software implementation all three components of the data correction approach were implemented in a web based application named turbidity cleaner using the r programming software environment r core team 2019 this application allows users to correct turbidity data without requiring programming expertise as a user friendly interface is provided to control and visualize the data corrections process each step described in this section automated detection and removal manual verification and gap filling can be done through the software the software is available on github https github com mayousif turbidity cleaner with release version 3 1 being the one used for the analyses described in this study a static version of this specific release has also been published on zenodo yousif and burdett 2022 r is the only prerequisite software needed to run the application all 200 required r packages are included in the github release and specifically r 3 5 3 is recommended as this is the version that was used in our testing the software itself is an r shiny application https shiny rstudio com which allows users to run it as a web based application i e through the user s default web browser this software was tested on windows machines and at least 16 gb of ram is recommended to handle large amounts of high frequency data e g multiple years of 15 min measurements additional information about the software is provided in the software availability section and information regarding how to set up and use the software can be found on the provided github page running the program will prompt the user to select a data level to work on the software is split into three stages each stage corresponding to a step in the correction workflow level 1 data is raw unedited measurements if level 1 is selected the user will be prompted to upload the data and after clicking start can move forward with running the automated algorithm once the algorithm is finished the results can be seen in the plot and saved to a csv file after the program is restarted or refreshed level 2 must be selected to begin the next step manual verification level 2 data refers to the resulting dataset that is saved from the automated algorithm both this dataset and the raw data level 1 are uploaded and after clicking start users can edit the adjustments made by the algorithm by visually comparing level 1 and level 2 data once complete the user again saves this data now considered to be level 3 data to a csv file again after restarting or refreshing the user can move forward with the modelling step by selecting level 3 and uploading the required files video tutorials showing the workflow of each step are also provided on the github page 2 3 method validation 2 3 1 operator subjectivity during manual verification to assess the impact of operator subjectivity during manual verification we devised an experiment that added synthetic anomalies to a previously corrected little ausable dataset three types of synthetic anomalies described in uhrich et al 2015 were added to the data table 1 these are representative of unexpected and unrealistic changes that require correction to continuous sensor data younos and heyer 2015 the data correction approach presented in this study was designed to detect remove and replace anomalies of these types as they were the most common types associated with the in situ turbidity sensors used in the mwns the matlab code used to generate the synthetic anomalies is provided in the supporting documentation si2 three users users 1 3 were challenged to find remove and correct the anomaly filled little ausable dataset using the approach presented in this study each user had varying experience correcting water quality sensor data user 2 had the most experience working with turbidity data and water quality data in general user 1 also had experience correcting turbidity data but not to the same extent as user 2 and user 3 had no experience correcting turbidity data the results of each user were compared to the results from three reference cases the first referred to as no correction is simply the anomaly filled dataset before any corrections are made the second referred to as algorithm alone is the case in which no manual verification was done i e results from the automated algorithm were fully accepted the last reference case referred to as ideal scenario represents the ideal case in which all anomalies were detected and replaced with modelled estimates while all non anomalous data was kept pearson s correlation coefficient or pearson s r pearson 1896 was used to assess the correlation between the clean little ausable dataset before adding synthetic anomalies to the final output from each user case pearson s r can range from 1 to 1 where 1 represents perfect inverse correlation 0 represents no correlation and 1 represents perfect correlation i e all anomalies removed and replaced with the true value 2 3 2 algorithm sensitivity and performance the generation of synthetic anomalies is random so it was important to ensure the automated detection and removal algorithm was not sensitive to this randomness it was also important to better understand how the number of generated anomalies influenced the results i e is the algorithm performance consistent across varying numbers of anomalies therefore the same approach to generate anomalies was repeated 10 times to the corrected little ausable dataset for each of these 10 datasets the automated detection and removal algorithm was applied this test strictly examined performance of the automated detection and removal algorithm and so manual verification and imputation based gap filling was not done for these 10 datasets performance metrics used to assess the algorithm s performance include the false discovery rate fdr false negative rate fnr and the matthews correlation coefficient mcc matthews 1975 the fdr is the ratio of false positives to total positives i e the fraction of time the algorithm is incorrect when flagging and removing a data point the fnr is the ratio of false negatives to total negatives i e the fraction of all non anomalous data that was removed by the algorithm the mcc is a measure of the correlation between two binary classifications the value of the mcc can be interpreted similarly to pearson s r e g mcc ranges from 1 to 1 in this case the mcc represents the correlation between anomaly classification made by the automated algorithm to the true classification of the data synthetic anomalies are assumed to be the only true anomalies as the mwns involved monitoring turbidity at many locations the generalizability of the automated detection and removal algorithm across locations was also assessed these locations although all located in highly agricultural areas have unique characteristics in terms of hydrology soil land use etc rosamond et al 2018 these factors can ultimately impact the behaviour of turbidity in a stream therefore in addition to the 10 little ausable datasets described earlier a single dataset containing synthetic anomalies was generated for the other nine locations algorithm performance was again primarily assessed using the mcc 3 results 3 1 data correction example revisiting the turbidity data presented in fig 2 the output from the automated detection and removal algorithm is presented in fig 3 a all anomalous spikes particularly near the end of this example were detected and removed by the algorithm two larger sections of data were also removed and both correspond to a rainstorm event indicated by the increase in water level the raw turbidity data during the first rainstorm event had many rapid changes this is unexpected behaviour of turbidity in the la river and is more likely an indication that debris interfered with the sensor during manual inspection of these results many of the identified anomalies on september 5th were determined to be false positives a few true anomalous values near the beginning of this event caused the algorithm to remove the entire section fig 3b presents the corrected turbidity data after manual verification was completed a significantly cleaner dataset was then used to model the removed sections of this turbidity data as described previously the model develops a multivariate relationship between turbidity water level and precipitation while incorporating temporal patterns found within all these parameters fig 3c presents the final corrected turbidity data after the larger missing sections were filled with the multiple imputations model 3 2 method validation results 3 2 1 synthetic anomaly generation examples of real and synthetic anomalies in turbidity measurements are shown in fig 4 the synthetic anomalies closely resemble true anomalies often found in turbidity data fig 4a shows unmodified raw turbidity measurements with no anomalous data highlighting how changes between measurements are relatively small and predictable due to the high frequency of measurements 3 2 2 user performance a total of 3913 synthetic anomalies were introduced to the little ausable dataset that was provided to each user for correction these synthetic anomalies account for roughly 5 of all little ausable data 91 993 total data points overall performance of each user is shown in table 2 users 1 3 the operators had consistent performance in detecting and removing anomalies high number of true positives however user 3 had significantly more false negatives anomalies that were not detected than the other two operators and the algorithm alone case indicating this user s intervention lowered performance the number of false positives non anomalous data classified as anomalous was variable across operators but users 2 and 3 had significantly less than the algorithm alone pearson s r also varied across operators but all improved the dataset as their values were higher than the no correction case a more detailed comparison of user performance for the three types of anomalies introduced in the experiment is provided in table 3 the previous results examined overall correlation which included non anomalous data while here calculations were done on the subset of data where synthetic anomalies were introduced i e comparing the original 3913 data points to the resulting 3913 data points from each user synthetic spikes that were detected and removed were replaced by the automated detection and removal algorithm as these were small single point gaps synthetic clustered spikes and winter errors were replaced using the cart model if properly detected and removed each operator had varying performance across the three types of anomalies though users 1 and 2 had comparable performance to the ideal scenario users 1 and 2 had a low number of false negatives 99 of synthetic anomalies were detected and removed yet their performance for spikes and clustered spikes greatly differed the false negatives for both operators were almost always spike anomalies except for 2 winter error data points that user 1 missed all clustered spikes were detected and removed by both users 3 2 3 random anomaly generation tests as described earlier 10 additional little ausable datasets containing synthetic anomalies were generated to assess the impact of random anomaly generation on the classification performance of the automated detection and removal algorithm fig 5 a shows the distributions of various statistics across the 10 tests the automated algorithm had consistent overall performance indicated by the mcc in all tests the fnr was very low indicating most anomalies were detected and removed by the algorithm the fdr was slightly more variable than the other metrics shown here but the worst case was an fdr of about 0 3 of the data that was flagged and removed by the algorithm 30 were non anomalous fig 5b shows the relationship between the number of synthetic anomalies to the calculated mcc it appears that the variability in the mcc across the 10 tests can be partially explained by the variability in the number of synthetic anomalies that are generated 3 2 4 performance across locations the generated datasets containing synthetic anomalies for the other nine locations were also processed using the automated algorithm table 4 summarizes the performance of the algorithm across all locations as 10 little ausable datasets were generated previously the average mcc two standard deviations are shown for this location mcc ranged from 0 72 to 0 94 with seven of the ten locations resulting in a mcc greater than 0 8 additional performance metrics for each individual test are shown in the supplementary information table s1 4 discussion 4 1 impact of operator subjectivity and gap filling approach the synthetic anomalies significantly degraded the dataset as evidenced by the low pearson s r value for the no correction case users 1 3 the operators were all successful in significantly improving the dataset and getting it close to the original clean dataset however this improvement can mostly be attributed to the automated algorithm as user intervention during manual verification can have a wide range of effects user 1 s manual intervention gave minimal improvement over the automated algorithm algorithm alone user 2 s intervention gave a significant improvement and user 3 s intervention degraded performance significantly these results show that running the automated algorithm without user intervention can give acceptable results but user intervention may improve or degrade these results these mixed manual intervention outcomes appear to be dependent on operator experience especially when examining the results from the untrained operator user 3 however broad conclusions cannot be drawn from with such a small sample it is still recommended that manual verification is done by experienced individuals familiar with turbidity data and or data correction most of the improvement from manual verification comes from reducing the number of false positives rather than finding missed anomalies again the automated algorithm using default settings was designed to detect and remove virtually all anomalies at the cost of removing some non anomalous data this is exactly what we observed as evidenced by algorithm alone case results relatively high false positives very low false negatives therefore users of this approach should be aware of this behaviour from the automated algorithm when modifying its output for existing approaches that rely heavily on manual intervention these results show that a range of operators and synthetic anomaly testing can help quantify the uncertainty around operator subjectiveness other ai approaches have been proposed to train a system to simulate and scale the ability of skilled analysts to locate anomalies e g russo et al 2020 such approaches are subject to the caveat that there are a range of operator accuracies for instance jones et al 2018 found that variability among operators can significantly impact the finalized data and recommended a consistent and detailed data correction approach to minimize the impact of operator subjectivity additional work is required to develop a common manual verification procedure that operators can reference when modifying the data when examining specific anomaly types there is variability across user performance the results in table 3 can be interpreted as gap filling performance especially for the ideal scenario since almost all 3913 anomalies were detected and removed across all cases differences in spike performance across users is likely due to sensitivity to outliers in the pearson s r calculation kim et al 2015 even a few missed spikes depending on their magnitude could reduce pearson s r all three operators missed spike anomalies therefore this variability across spike performance is likely connected to the magnitude of the missed data however when examining clustered spike performance users 1 and 2 removed all clustered spike anomalies yet pearson s r was significantly different for each user 0 02 and 0 88 respectively as clustered spikes were replaced using the cart model as opposed to the local linear models used to replace lone spikes this indicates that the generated estimates from the cart models provided significantly different estimates between these two operators user 2 s model was much more accurate which is likely due to the low number of false positives that user 2 achieved as this provided additional non anomalous data for the model to build upon due to the nondeterministic nature of this model a different estimate may be generated each time it is used even if the input data is always the same but the winter error results suggest that this does not greatly impact performance winter error pearson s r is consistent across all users except for user 3 as this operator reintroduced many winter errors to the data after they were removed by the automated algorithm the consistent performance across the other users indicates that the modelled estimates from the mice approach did not vary greatly across users for these winter sections however when examining the ideal scenario results the cart model was relatively inaccurate in its winter estimates compared to clustered spikes overall the approach used to fill large gaps provided acceptable results but in some specific cases provided variable estimates across users e g clustered spikes which can be explained by differences in the input data between users it should be emphasized that the mice approach relies on the relationships between sensor data and other covariates to function effectively it does require covariates with meaningful relationships with the sensor data it is common for turbidity data to be collected at locations with water level and precipitation so this is not a particularly onerous requirement but may limit applicability somewhat additional sensors can be incorporated into the cart models should they be available subsequent work could quantify the uncertainty bounds of the imputed values to ensure consistent reproducible estimates of course filling large gaps is optional and can be ignored entirely using the provided software though all modelled estimates are labelled in the software with a unique identifier to differentiate between measured and modelled data the software can also be modified to utilize other imputation methods approaches if cart is not applicable by default mice generally uses predictive mean matching ppm or linear regression to generate modelled estimates these methods may be effective choices for imputing missing data that do vary seasonally however turbidity data in this study were highly variable specifically during winter months and so cart was a suitable method as it relies on interactions between variables in these domains rather than on main effects independently burgette and reiter 2010 4 2 automated algorithm performance the automated detection and removal algorithm always improved the dataset it processed regardless of the number of anomalies or location though there appears to be a correlation between the number of anomalies and the algorithm performance the algorithm was designed with the assumption that a significant percentage of input data 1 are anomalies in all tests presented here roughly 4 7 of the data were synthetic anomalies therefore if anomalies are expected to be very rare 1 of the data adjustments to the algorithm s parameters may be required to avoid a high fraction of false positives across locations the algorithm was consistent with its classification performance some locations such as big and north have relatively low flow rates during the summer these streams often dry up completely exposing the turbidity sensor to the atmosphere therefore sensors at these locations are not only susceptible to additional problems not directly targeted here but also the low flowrate allows for relatively rapid changes in turbidity that is not observed in larger streams these real rapid changes are likely misidentified as anomalous by the algorithm and may require tuning of the model parameters to improve performance alternatively higher measurement frequencies could significantly improve the algorithm s performance at these locations therefore it is recommended that data is collected at a timescale fine enough to resolve storm event dynamics this timescale may differ by location as evidenced by the results 4 3 overall approach implications while environmental data science literature continues to produce advanced anomaly detection methods e g hill and minsker 2010 taylor and loescher 2013 russo et al 2020 simple and intuitive tools that implement these modern approaches are rare existing tools often rely on manual correction and the expertise of local analysts wagner et al 2006 horsburgh et al 2015 in this paper we present a novel approach for anomaly detection and correction for turbidity sensor data implemented in a freely available software package it is important to emphasize that while the automated detection and removal algorithm was effective for processing turbidity measurements from the sensors deployed in highly agricultural areas adjustments may be required to improve performance for measurements made in different land use environments e g urban and other water quality parameters as these were not tested in this study the behaviour of turbidity is not guaranteed to be consistent across locations and with other parameters so caution is advised if applying this tool to untested scenarios for example ph and nitrate sensors commonly experience sensor drift horsburgh et al 2015 which is not an anomaly targeted by this approach subsequent work is required to broaden the applicability of this approach to these additional anomaly types and to additional environmental parameters 5 conclusions this study proposes a novel approach of managing and correcting high frequency turbidity data this approach consists of an algorithm designed to automatically detect and remove anomalies manual verification of the flagged anomalies and two data gap filling approaches depending on the severity i e size of the gap to assess performance of this approach it was applied to datasets from 10 locations that contained random synthetic anomalies results suggest that the algorithm on its own can significantly improve turbidity datasets with manual verification recommended to reduce the number of false positives i e non anomalous data misidentified by the algorithm as anomalous performance of the algorithm was also consistent across all test datasets and performed slightly better when a higher fraction of the data was anomalous though performance may be improved by tuning the algorithm s parameters as the algorithm assumes a high degree of autocorrelation in the data measurements must be collected at a high enough frequency to resolve rapid changes in turbidity while the 15 min measurement frequency was sufficient for most of the tested locations a higher measurement frequency would have improved results for two locations where turbidity can significantly change in 15 min to assess the impact of operator subjectivity during manual verification three operators with a range of data correction experience were tasked with applying the entire approach to one of the datasets that contained synthetic anomalies results varied but the operators that had prior experience with data correction and working with turbidity data performed much better than the operator that had no prior experience manual verification is only recommended if the operator has prior experience with data correction and or knowledge of the data that is being corrected the entire approach has been implemented in a free open source software package this software features a simple user interface to navigate through the entire approach this is especially useful during the manual verification step where users can quickly assess and modify the results from the automated detection and removal algorithm additional information on how to set up and use this software can be found on the page included in the software availability section while the current approach was tested using turbidity sensor data generalizing adapting this approach to assist in correcting data from other sensors would be valuable in theory this approach can be applied to characterizing real and unrealistic changes from many different sensors as most environmental parameters share similar data qualities when sampling frequency is sufficiently high there are also additional anomaly types e g sensor drift that were not targeted in this approach as they were not common problems with turbidity sensors however expanding the current approach to handle additional anomaly types would make it much more robust future work is needed to assess the applicability and adaptability of this approach on other environmental parameters author contributions as per the contributor roles taxonomy credit meguel yousif conceptualization methodology validation software writing original draft visualization hannah burdett conceptualization methodology software writing original draft dr christopher wellen validation formal analysis writing original draft supervision dr sohom mandal validation writing original draft supervision grace arabian data curation writing review editing derek smith resources supervision dr ryan j sorichetti conceptualization writing review editing project administration software availability name of software turbidity cleaner developers meguel yousif hannah burdett email meguel yousif mail utoronto ca hburdett ryerson ca availability https github com mayousif turbidity cleaner yousif and burdett 2022 year first available 2019 software required r v3 5 3 software license mit license program language english program size 700 mb a video demonstration of the software is also provided declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the canada ontario agreement project id 1009c and 1010d 16 through the ontario ministry of the environment conservation and parks omecp with ryerson university additional funding was provided through the natural sciences and engineering research council of canada nserc discovery grant we thank kelly biagi dr cody ross and the anonymous reviewers whose comments and suggestions helped improve the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105461 
25573,in situ water quality sensors yield continuous high frequency measurements that are essential for long term monitoring optical sensors such as turbidity sensors are prone to lens obstruction and ultimately anomalous measurements in this study we developed a novel approach to detect remove and replace these anomalies a free open source software package implementing this approach is provided anomaly detection and removal is completely automated though manual verification is recommended sections of missing data are filled by one of two methods depending on the size of each section this approach was tested on turbidity measurements from sensors deployed across southern ontario automated detection performance was consistent across all test datasets with most 95 synthetically introduced anomalies removed in all but one dataset gap filling provided accurate estimates for smaller gaps while performance on larger gaps varied overall this approach as implemented in the provided software can greatly assist with data management in long term monitoring programs keywords turbidity sensors data correction surface water monitoring anomalous data multiple imputations 1 introduction in situ sensors are practical and common for surface water monitoring these sensors can measure various water conditions without the need to collect transport and analyze water samples sensors also allow for high temporal resolution measurements that would otherwise be impractical for a sample based monitoring approach rode et al 2016 younos and heyer 2015 as sensor technology has improved measurement capabilities and frequencies of these devices have increased this had led to a substantial increase in the amount of data to manage correct and quality assure wagner et al 2006 the deployment of in situ sensors occurs in natural environments where various interferences can affect the accuracy and quality of the collected data the frequency and magnitude of these problems are often specific to the sensor utilized for monitoring younos and heyer 2015 environmental factors can obstruct the lens of optical sensors reducing the accuracy of the measurements alferes et al 2013 for example ice formation around the sensor during the winter is a common interference a year round issue is storm runoff carrying debris twigs leaves small branches and garbage into the stream which gradually collects and obstructs the sensor anomaly detection and correction methods for high frequency data are generally laborious and time consuming with little to no automation and many rely on operator subjectivity to determine faults in the data wagner et al 2006 some recent work has examined whether machine learning techniques can assist in sensor data corrections lucius et al 2020 but there has been a lack of focus on turbidity a key parameter used to estimate nutrient and sediment fluxes in novel monitoring approaches e g robertson et al 2018 in this study a novel approach is developed and applied to correct data from in situ turbidity sensors turbidity is a measure of the haziness of a fluid due to suspended particles grobbelaar 2009 in natural surface waters turbidity is a significant indicator of water quality as rivers and lakes become increasingly turbid less light can reach aquatic organisms on the sediment surface leading to adverse effects on the local ecosystem in addition high concentrations of suspended sediment can lead to many physiological effects on fish species and can even be fatal if concentrations are high enough for sustained periods bash and berman 2001 turbidity can also be used as a proxy to estimate other important parameters in the stream such as total phosphorus concentrations horsburgh et al 2010 lannergård et al 2019 ryberg 2006 robertson et al 2018 sensor data must be reviewed for interferences and anomalies e g fouling significant efforts in environmental science literature have been dedicated to the study detection and correction of anomalies many early publications recommended manual verification approaches which were tedious subjective error prone as well as labour and time intensive wagner et al 2006 kotamäki et al 2009 found that it is especially challenging to ensure data quality from large monitoring networks highlighting the need for data correction approaches that consider the physical properties of each monitored system e g turbidity increases with water level for streams located in natural environments horsburgh et al 2015 demonstrated a manual system for correcting sensor anomalies where the user interface made correcting common anomalies such as outliers or sensor drift straightforward and graphical however this method relied on expert interpretation regarding classification and correction of these anomalies thus there is a need for automated cross platform and cross discipline identification of anomalies in environmental sensor data cook et al 2020 approaches based on statistical identification of anomalies have been used in the environmental sciences for instance hill and minsker 2010 compared a variety of data driven time series model approaches nearest neighbor single layer linear network multi layer perceptron to identify and mitigate anomalies in high frequency wind speed measurements their method showed that even a univariate approach could be effective in identifying and mitigating anomalies taylor and loescher 2013 presented an approach to anomaly detection using a suite of plausibility tests based on observed statistical properties of the sensor data for instance possible anomalies were detected using extreme values of a variable s first and second moments or of subsequent pairs this approach was proposed for the united states national ecological observatory network other approaches use complex statistical models to identify anomalies alferes et al 2013 suggested a multivariate processing method using principal component analysis pca and partial least square pls for sensor data assessments this pca pls based approach involved developing a model to predict turbidity using multiple reference sensors e g water level ph temperature precipitation which helped automate the data correction process while effective in certain circumstances this does require multiple sensors and may lead to unclear results when there are anomalies between multiple sensors the accuracy of the model may also limit the anomaly detection still other approaches are aimed at individual research projects though are often beyond the abilities of many environmental researchers to work with russo et al 2020 showed that integrating an expert system of domain experts with multiple machine learning algorithms was able to detect anomalies accurately with a minimal number of anomalies identified by experts as training data leigh et al 2019 showed in an application to turbidity conductivity and stream level that the relationships between these variables were quite complex making models as the basis for anomaly identification inefficient rather they recommended using feature detection approaches which examine the distributions of multiple data streams in a transformed feature space and found these to outperform time series models while the field of anomaly detection is a thriving line of inquiry there is a need to make freely accessible open source tools that are available to field researchers using sensors in their research with a minimum of additional overhead and additional skill sets in this study we aimed to develop a novel approach to detect correct and replace anomalies within high frequency turbidity sensor measurements specifically anomalies caused by external interference which cause unrealistic spikes in turbidity measurements are targeted the effectiveness of each step in this new approach is tested and discussed tests were done where a variety of analysts were encouraged to find a number of synthetic errors to illustrate both the effectiveness of the software and quantify the degree of user specificity of it in addition to developing and testing this new approach its implementation as an open source software package that can be used by analysts with little or no programming experience was a primary objective this software can assist environmental scientists and engineers with managing turbidity sensor data which is especially useful in long term monitoring studies 2 methods 2 1 study area and data collection as a part of the multi watershed nutrient study mwns mohamed et al 2019 led by the ontario ministry of the environment conservation and parks mecp in situ sensors measuring turbidity and discharge alongside rain gauges were deployed at several monitoring locations situated in highly agricultural areas across southern ontario see rosamond et al 2018 for site selection process continuous measurements 15 min intervals were collected between 2015 and 2019 from 10 mwns monitoring stations a map of all monitoring locations is shown in the supplementary information fig s1 data availability of turbidity varies as stations were deployed at different times throughout the study but all locations had at least two years of continuous measurements observed anomalous measurements in the turbidity data were generally caused by debris interfering with the optical sensor and or ice formation around the sensor during winter months these anomalies are evident from the large individual or clustered spikes in the data fig 1 while this study was designed to target anomalies caused by these external environmental factors other types of anomalies may be present such as sensor drift but were not targeted in this study a more detailed description of these anomalies is provided in section 2 3 2 2 data correction workflow the overall correction approach consists of three consecutive components automated anomaly detection and removal manual verification and missing data modelling first automated anomaly detection and removal is accomplished through a univariate algorithm that also fills small gaps in the data regardless of whether the data were originally missing or removed by the algorithm next manual verification is recommended to correct false negatives or false positives i e data that were incorrectly classified by the algorithm lastly large sections of missing data are filled with estimates from multivariate imputation models 2 2 1 automated detection and removal algorithm first automated detection and removal of anomalies is done through an algorithm that can process years of high frequency sensor data in a relatively short amount of time e g a few minutes this algorithm exploits autocorrelation a common property of time series data that represents correlation between data to previous time steps it is assumed that measurement frequencies are sufficient such that autocorrelation exists due to the high frequency measurements of the sensors used in this study and lag times associated with typical water quality changes it is assumed that any drastic changes between measurements are caused by external interference and are thus anomalous note that we do not consider the effect of limits of detection detection limits were judged unimportant because low turbidity levels occurred at low flows consequently any data near the limits of detection in agricultural or urban streams would be lumped into the baseline data and unaltered from the original values the complete anomaly detection and removal algorithm alongside descriptions of the parameters and criteria is as follows step 1 estimate site specific parameters using all available measurements baseline threshold a the default value is set to the 75th percentile of the raw input data data below this threshold are baseline data upper transition threshold b the default value is set to the 90th percentile of the raw input data data below this threshold and above a should be transitioning from baseline levels to a weather related event peak or returning to baselines levels from a peak maximum acceptable absolute change in turbidity c the default value is set to the 99th percentile of the first differences difference between consecutive data points this parameter can be tuned to balance false positives with false negatives e g increasing the value of this parameter may reduce false positives but increase false negatives step 2 process data points x sequentially beginning with the x 6 as forecasted estimates are based on prior five measurements in the time series i x t is assigned to a section depending on the values identified in step 1 a baseline x t a b transition a x t b c event x t b ii if x t is in the baseline section x t is kept as the anomalies targeted in this study will not be present in this section otherwise three additional parameters are calculated a absolute change between the current and previous point ac 1 a c x t x t 1 b absolute percent change between the current and previous point pc 2 p c x t x t 1 x t 1 100 c absolute difference between current percent change and previous percent change dpc 3 d p c x t x t 1 x t 1 x t 1 x t 2 x t 2 100 iii if x t is in the transition section the following checks are made if any condition is true x t is retained a ac 5 ntu b dpc 30 c pc 50 iv if x is in the event section the following checks are made if any condition is false x t is flagged as anomalous and removed from the corrected series a ac c b dpc 50 v if x t was originally missing or removed by the algorithm it is filled with an estimated value a if x t was originally missing from the dataset i a forecasted estimate from a local linear regression modelled to x t 6 through x t 1 is made if this estimate is a positive value fill using this estimate ii otherwise fill by replicating x t 1 b if x t was removed by the algorithm i a forecasted estimate from a local linear regression modelled to x t 6 through x t 1 is made if this estimate is a negative value fill by replicating the previous measurement ii if the estimate is a positive value and less than the original measurement fill using the estimated value iii otherwise the original value is reintroduced and accepted anomalies caused by external interference will result in unusually high measurements and so modelled estimates that are larger than measurements are assumed to be inaccurate step 3 sections containing three or more consecutive estimated data points are removed the local linear models are used to fill small gaps one or two data points e g those shown in fig 1a and are not trusted to provide accurate estimates for larger gaps as local linearity is only a valid assumption for small sections of non linear data the sections described in the algorithm baseline transition event have different conditions to determine if a measurement is anomalous because realistic percent changes in turbidity at lower levels e g 1 ntu to 4 ntu 400 increase may be anomalous at higher levels e g 100 ntu to 400 ntu 400 increase similarly an absolute change in turbidity between measurements may be acceptable at higher levels but may be considered anomalous at lower levels an example of turbidity data measured at the little ausable river alongside the two calculated thresholds parameters a and b can be seen in fig 2 these thresholds help us understand the site specific characteristics of the little ausable river turbidity reaches relatively high levels during the first peak while the second peak is a relatively minor event this example will be revisited to qualitatively assess the performance of the entire data correction approach 2 2 2 manual verification next manual inspection and verification are recommended to correct misclassifications of data the provided software allows users to overrule the automated anomaly detection algorithm for any data points that are believed to be misclassified the default automated algorithm settings were chosen to limit false negatives undetected anomalous data therefore there may be a relatively high number of false positives non anomalous data that was removed given the nature of operating in situ optical sensors a fully automated data correction process is unlikely to provide completely reliable data therefore some manual effort is recommended to approve changes made by the algorithm and ensure reliable results 2 2 3 missing data modelling lastly large sections of missing data are filled following manual verification the dataset should only contain non anomalous measurements and large gaps of missing data in this study we utilized the multivariate imputation by chained equations mice package in r to estimate turbidity data using timestamps and supplemental parameters as independent variables mice involves running a series of imputation models in which each variable with missing data is modelled conditionally on the other variables in the dataset the r mice package buuren and groothuis oudshoorn 2011 includes several imputation model choices which may be selected by the user please refer to https cran r project org web packages mice mice pdf for more information regarding imputation options for mice additionally supplemental parameters related to the variable of interest may be utilized in this study water level and precipitation data were used as additional parameters for predicting turbidity mice begins by generating simple mean imputations that temporarily fill any missing values in for all parameters these mean imputations act as placeholder values next the placeholder values for a selected parameter such as turbidity are removed then the selected parameter is set as the dependent variable all other parameters then act as independent variables to train the imputation model many imputation methods can be used including predictive mean matching linear regression and classification and regression trees cart for this study a cart model was used with a maximum of 50 iterations to prevent overtraining of the regression trees a cart model is sequentially generated for each parameter that has missing data at this point all missing values have been replaced by imputed estimates from cart models though to account for imputation uncertainty and avoid dependence on the order in which the variables are imputed this entire process is repeated across five cycles while the placeholder values were replaced during the first cycle the generated cart models used placeholder values for the independent variables therefore the cart models in subsequent cycles are built on more accurate estimates from cart models in the previous cycle rather than those placeholder values cart was chosen as it is a multivariate tree building and parsing technique lewis 2000 it is versatile enough to fit interactions nonlinear relations and complex distributions without parametric assumptions or data transformations burgette and reiter 2010 he et al 2009 it operates by seeking to approximate the conditional distribution of a univariate outcome from multiple predictors the cart algorithm partitions the predictor space such that subsets of units generated by the partitions have relatively homogeneous groups it then iteratively subdivides the data at each possible split selecting the most suitable partition that produces the most homogenous subgroups the series of the splits can be effectively represented by a tree structure with the leaves corresponding to the subsets of the sample group burgette and reiter 2010 butalia et al 2008 hastie et al 2017 lewis 2000 once the missing data is filled a single exponential smoothing process is applied to the modelled data to compensate for sharp discontinuities bhattacharya and solomatine 2006 wang and witten 1996 single exponential smoothing is used for short range forecasting where the model assumes that there is no trend or consistent pattern of growth within the dataset the formula for single exponential smoothing is 4 s t α x t 1 1 α where st is the smoothed estimate at the current time t x is the measured observation and α is the smoothing constant the α values range from zero to one and the closer the α is to zero the more heavily weighted the older values are when smoothing charles 2004 through trial and error the α parameter was set to 0 05 to minimize prediction error equation 4 was applied recursively to the modelled data for the entire time period thus computation of each new smoothed value represents the weighted average of the current observation and the previous smoothed observations in effect each smoothed value is the weighted average of the past values with the single exponential smoothing assigning decreasing weights as the values get older in other words recent observations are given relatively more weight in forecasting than the older observations gooch 2011 kalekar and bernard 2004 2 2 4 software implementation all three components of the data correction approach were implemented in a web based application named turbidity cleaner using the r programming software environment r core team 2019 this application allows users to correct turbidity data without requiring programming expertise as a user friendly interface is provided to control and visualize the data corrections process each step described in this section automated detection and removal manual verification and gap filling can be done through the software the software is available on github https github com mayousif turbidity cleaner with release version 3 1 being the one used for the analyses described in this study a static version of this specific release has also been published on zenodo yousif and burdett 2022 r is the only prerequisite software needed to run the application all 200 required r packages are included in the github release and specifically r 3 5 3 is recommended as this is the version that was used in our testing the software itself is an r shiny application https shiny rstudio com which allows users to run it as a web based application i e through the user s default web browser this software was tested on windows machines and at least 16 gb of ram is recommended to handle large amounts of high frequency data e g multiple years of 15 min measurements additional information about the software is provided in the software availability section and information regarding how to set up and use the software can be found on the provided github page running the program will prompt the user to select a data level to work on the software is split into three stages each stage corresponding to a step in the correction workflow level 1 data is raw unedited measurements if level 1 is selected the user will be prompted to upload the data and after clicking start can move forward with running the automated algorithm once the algorithm is finished the results can be seen in the plot and saved to a csv file after the program is restarted or refreshed level 2 must be selected to begin the next step manual verification level 2 data refers to the resulting dataset that is saved from the automated algorithm both this dataset and the raw data level 1 are uploaded and after clicking start users can edit the adjustments made by the algorithm by visually comparing level 1 and level 2 data once complete the user again saves this data now considered to be level 3 data to a csv file again after restarting or refreshing the user can move forward with the modelling step by selecting level 3 and uploading the required files video tutorials showing the workflow of each step are also provided on the github page 2 3 method validation 2 3 1 operator subjectivity during manual verification to assess the impact of operator subjectivity during manual verification we devised an experiment that added synthetic anomalies to a previously corrected little ausable dataset three types of synthetic anomalies described in uhrich et al 2015 were added to the data table 1 these are representative of unexpected and unrealistic changes that require correction to continuous sensor data younos and heyer 2015 the data correction approach presented in this study was designed to detect remove and replace anomalies of these types as they were the most common types associated with the in situ turbidity sensors used in the mwns the matlab code used to generate the synthetic anomalies is provided in the supporting documentation si2 three users users 1 3 were challenged to find remove and correct the anomaly filled little ausable dataset using the approach presented in this study each user had varying experience correcting water quality sensor data user 2 had the most experience working with turbidity data and water quality data in general user 1 also had experience correcting turbidity data but not to the same extent as user 2 and user 3 had no experience correcting turbidity data the results of each user were compared to the results from three reference cases the first referred to as no correction is simply the anomaly filled dataset before any corrections are made the second referred to as algorithm alone is the case in which no manual verification was done i e results from the automated algorithm were fully accepted the last reference case referred to as ideal scenario represents the ideal case in which all anomalies were detected and replaced with modelled estimates while all non anomalous data was kept pearson s correlation coefficient or pearson s r pearson 1896 was used to assess the correlation between the clean little ausable dataset before adding synthetic anomalies to the final output from each user case pearson s r can range from 1 to 1 where 1 represents perfect inverse correlation 0 represents no correlation and 1 represents perfect correlation i e all anomalies removed and replaced with the true value 2 3 2 algorithm sensitivity and performance the generation of synthetic anomalies is random so it was important to ensure the automated detection and removal algorithm was not sensitive to this randomness it was also important to better understand how the number of generated anomalies influenced the results i e is the algorithm performance consistent across varying numbers of anomalies therefore the same approach to generate anomalies was repeated 10 times to the corrected little ausable dataset for each of these 10 datasets the automated detection and removal algorithm was applied this test strictly examined performance of the automated detection and removal algorithm and so manual verification and imputation based gap filling was not done for these 10 datasets performance metrics used to assess the algorithm s performance include the false discovery rate fdr false negative rate fnr and the matthews correlation coefficient mcc matthews 1975 the fdr is the ratio of false positives to total positives i e the fraction of time the algorithm is incorrect when flagging and removing a data point the fnr is the ratio of false negatives to total negatives i e the fraction of all non anomalous data that was removed by the algorithm the mcc is a measure of the correlation between two binary classifications the value of the mcc can be interpreted similarly to pearson s r e g mcc ranges from 1 to 1 in this case the mcc represents the correlation between anomaly classification made by the automated algorithm to the true classification of the data synthetic anomalies are assumed to be the only true anomalies as the mwns involved monitoring turbidity at many locations the generalizability of the automated detection and removal algorithm across locations was also assessed these locations although all located in highly agricultural areas have unique characteristics in terms of hydrology soil land use etc rosamond et al 2018 these factors can ultimately impact the behaviour of turbidity in a stream therefore in addition to the 10 little ausable datasets described earlier a single dataset containing synthetic anomalies was generated for the other nine locations algorithm performance was again primarily assessed using the mcc 3 results 3 1 data correction example revisiting the turbidity data presented in fig 2 the output from the automated detection and removal algorithm is presented in fig 3 a all anomalous spikes particularly near the end of this example were detected and removed by the algorithm two larger sections of data were also removed and both correspond to a rainstorm event indicated by the increase in water level the raw turbidity data during the first rainstorm event had many rapid changes this is unexpected behaviour of turbidity in the la river and is more likely an indication that debris interfered with the sensor during manual inspection of these results many of the identified anomalies on september 5th were determined to be false positives a few true anomalous values near the beginning of this event caused the algorithm to remove the entire section fig 3b presents the corrected turbidity data after manual verification was completed a significantly cleaner dataset was then used to model the removed sections of this turbidity data as described previously the model develops a multivariate relationship between turbidity water level and precipitation while incorporating temporal patterns found within all these parameters fig 3c presents the final corrected turbidity data after the larger missing sections were filled with the multiple imputations model 3 2 method validation results 3 2 1 synthetic anomaly generation examples of real and synthetic anomalies in turbidity measurements are shown in fig 4 the synthetic anomalies closely resemble true anomalies often found in turbidity data fig 4a shows unmodified raw turbidity measurements with no anomalous data highlighting how changes between measurements are relatively small and predictable due to the high frequency of measurements 3 2 2 user performance a total of 3913 synthetic anomalies were introduced to the little ausable dataset that was provided to each user for correction these synthetic anomalies account for roughly 5 of all little ausable data 91 993 total data points overall performance of each user is shown in table 2 users 1 3 the operators had consistent performance in detecting and removing anomalies high number of true positives however user 3 had significantly more false negatives anomalies that were not detected than the other two operators and the algorithm alone case indicating this user s intervention lowered performance the number of false positives non anomalous data classified as anomalous was variable across operators but users 2 and 3 had significantly less than the algorithm alone pearson s r also varied across operators but all improved the dataset as their values were higher than the no correction case a more detailed comparison of user performance for the three types of anomalies introduced in the experiment is provided in table 3 the previous results examined overall correlation which included non anomalous data while here calculations were done on the subset of data where synthetic anomalies were introduced i e comparing the original 3913 data points to the resulting 3913 data points from each user synthetic spikes that were detected and removed were replaced by the automated detection and removal algorithm as these were small single point gaps synthetic clustered spikes and winter errors were replaced using the cart model if properly detected and removed each operator had varying performance across the three types of anomalies though users 1 and 2 had comparable performance to the ideal scenario users 1 and 2 had a low number of false negatives 99 of synthetic anomalies were detected and removed yet their performance for spikes and clustered spikes greatly differed the false negatives for both operators were almost always spike anomalies except for 2 winter error data points that user 1 missed all clustered spikes were detected and removed by both users 3 2 3 random anomaly generation tests as described earlier 10 additional little ausable datasets containing synthetic anomalies were generated to assess the impact of random anomaly generation on the classification performance of the automated detection and removal algorithm fig 5 a shows the distributions of various statistics across the 10 tests the automated algorithm had consistent overall performance indicated by the mcc in all tests the fnr was very low indicating most anomalies were detected and removed by the algorithm the fdr was slightly more variable than the other metrics shown here but the worst case was an fdr of about 0 3 of the data that was flagged and removed by the algorithm 30 were non anomalous fig 5b shows the relationship between the number of synthetic anomalies to the calculated mcc it appears that the variability in the mcc across the 10 tests can be partially explained by the variability in the number of synthetic anomalies that are generated 3 2 4 performance across locations the generated datasets containing synthetic anomalies for the other nine locations were also processed using the automated algorithm table 4 summarizes the performance of the algorithm across all locations as 10 little ausable datasets were generated previously the average mcc two standard deviations are shown for this location mcc ranged from 0 72 to 0 94 with seven of the ten locations resulting in a mcc greater than 0 8 additional performance metrics for each individual test are shown in the supplementary information table s1 4 discussion 4 1 impact of operator subjectivity and gap filling approach the synthetic anomalies significantly degraded the dataset as evidenced by the low pearson s r value for the no correction case users 1 3 the operators were all successful in significantly improving the dataset and getting it close to the original clean dataset however this improvement can mostly be attributed to the automated algorithm as user intervention during manual verification can have a wide range of effects user 1 s manual intervention gave minimal improvement over the automated algorithm algorithm alone user 2 s intervention gave a significant improvement and user 3 s intervention degraded performance significantly these results show that running the automated algorithm without user intervention can give acceptable results but user intervention may improve or degrade these results these mixed manual intervention outcomes appear to be dependent on operator experience especially when examining the results from the untrained operator user 3 however broad conclusions cannot be drawn from with such a small sample it is still recommended that manual verification is done by experienced individuals familiar with turbidity data and or data correction most of the improvement from manual verification comes from reducing the number of false positives rather than finding missed anomalies again the automated algorithm using default settings was designed to detect and remove virtually all anomalies at the cost of removing some non anomalous data this is exactly what we observed as evidenced by algorithm alone case results relatively high false positives very low false negatives therefore users of this approach should be aware of this behaviour from the automated algorithm when modifying its output for existing approaches that rely heavily on manual intervention these results show that a range of operators and synthetic anomaly testing can help quantify the uncertainty around operator subjectiveness other ai approaches have been proposed to train a system to simulate and scale the ability of skilled analysts to locate anomalies e g russo et al 2020 such approaches are subject to the caveat that there are a range of operator accuracies for instance jones et al 2018 found that variability among operators can significantly impact the finalized data and recommended a consistent and detailed data correction approach to minimize the impact of operator subjectivity additional work is required to develop a common manual verification procedure that operators can reference when modifying the data when examining specific anomaly types there is variability across user performance the results in table 3 can be interpreted as gap filling performance especially for the ideal scenario since almost all 3913 anomalies were detected and removed across all cases differences in spike performance across users is likely due to sensitivity to outliers in the pearson s r calculation kim et al 2015 even a few missed spikes depending on their magnitude could reduce pearson s r all three operators missed spike anomalies therefore this variability across spike performance is likely connected to the magnitude of the missed data however when examining clustered spike performance users 1 and 2 removed all clustered spike anomalies yet pearson s r was significantly different for each user 0 02 and 0 88 respectively as clustered spikes were replaced using the cart model as opposed to the local linear models used to replace lone spikes this indicates that the generated estimates from the cart models provided significantly different estimates between these two operators user 2 s model was much more accurate which is likely due to the low number of false positives that user 2 achieved as this provided additional non anomalous data for the model to build upon due to the nondeterministic nature of this model a different estimate may be generated each time it is used even if the input data is always the same but the winter error results suggest that this does not greatly impact performance winter error pearson s r is consistent across all users except for user 3 as this operator reintroduced many winter errors to the data after they were removed by the automated algorithm the consistent performance across the other users indicates that the modelled estimates from the mice approach did not vary greatly across users for these winter sections however when examining the ideal scenario results the cart model was relatively inaccurate in its winter estimates compared to clustered spikes overall the approach used to fill large gaps provided acceptable results but in some specific cases provided variable estimates across users e g clustered spikes which can be explained by differences in the input data between users it should be emphasized that the mice approach relies on the relationships between sensor data and other covariates to function effectively it does require covariates with meaningful relationships with the sensor data it is common for turbidity data to be collected at locations with water level and precipitation so this is not a particularly onerous requirement but may limit applicability somewhat additional sensors can be incorporated into the cart models should they be available subsequent work could quantify the uncertainty bounds of the imputed values to ensure consistent reproducible estimates of course filling large gaps is optional and can be ignored entirely using the provided software though all modelled estimates are labelled in the software with a unique identifier to differentiate between measured and modelled data the software can also be modified to utilize other imputation methods approaches if cart is not applicable by default mice generally uses predictive mean matching ppm or linear regression to generate modelled estimates these methods may be effective choices for imputing missing data that do vary seasonally however turbidity data in this study were highly variable specifically during winter months and so cart was a suitable method as it relies on interactions between variables in these domains rather than on main effects independently burgette and reiter 2010 4 2 automated algorithm performance the automated detection and removal algorithm always improved the dataset it processed regardless of the number of anomalies or location though there appears to be a correlation between the number of anomalies and the algorithm performance the algorithm was designed with the assumption that a significant percentage of input data 1 are anomalies in all tests presented here roughly 4 7 of the data were synthetic anomalies therefore if anomalies are expected to be very rare 1 of the data adjustments to the algorithm s parameters may be required to avoid a high fraction of false positives across locations the algorithm was consistent with its classification performance some locations such as big and north have relatively low flow rates during the summer these streams often dry up completely exposing the turbidity sensor to the atmosphere therefore sensors at these locations are not only susceptible to additional problems not directly targeted here but also the low flowrate allows for relatively rapid changes in turbidity that is not observed in larger streams these real rapid changes are likely misidentified as anomalous by the algorithm and may require tuning of the model parameters to improve performance alternatively higher measurement frequencies could significantly improve the algorithm s performance at these locations therefore it is recommended that data is collected at a timescale fine enough to resolve storm event dynamics this timescale may differ by location as evidenced by the results 4 3 overall approach implications while environmental data science literature continues to produce advanced anomaly detection methods e g hill and minsker 2010 taylor and loescher 2013 russo et al 2020 simple and intuitive tools that implement these modern approaches are rare existing tools often rely on manual correction and the expertise of local analysts wagner et al 2006 horsburgh et al 2015 in this paper we present a novel approach for anomaly detection and correction for turbidity sensor data implemented in a freely available software package it is important to emphasize that while the automated detection and removal algorithm was effective for processing turbidity measurements from the sensors deployed in highly agricultural areas adjustments may be required to improve performance for measurements made in different land use environments e g urban and other water quality parameters as these were not tested in this study the behaviour of turbidity is not guaranteed to be consistent across locations and with other parameters so caution is advised if applying this tool to untested scenarios for example ph and nitrate sensors commonly experience sensor drift horsburgh et al 2015 which is not an anomaly targeted by this approach subsequent work is required to broaden the applicability of this approach to these additional anomaly types and to additional environmental parameters 5 conclusions this study proposes a novel approach of managing and correcting high frequency turbidity data this approach consists of an algorithm designed to automatically detect and remove anomalies manual verification of the flagged anomalies and two data gap filling approaches depending on the severity i e size of the gap to assess performance of this approach it was applied to datasets from 10 locations that contained random synthetic anomalies results suggest that the algorithm on its own can significantly improve turbidity datasets with manual verification recommended to reduce the number of false positives i e non anomalous data misidentified by the algorithm as anomalous performance of the algorithm was also consistent across all test datasets and performed slightly better when a higher fraction of the data was anomalous though performance may be improved by tuning the algorithm s parameters as the algorithm assumes a high degree of autocorrelation in the data measurements must be collected at a high enough frequency to resolve rapid changes in turbidity while the 15 min measurement frequency was sufficient for most of the tested locations a higher measurement frequency would have improved results for two locations where turbidity can significantly change in 15 min to assess the impact of operator subjectivity during manual verification three operators with a range of data correction experience were tasked with applying the entire approach to one of the datasets that contained synthetic anomalies results varied but the operators that had prior experience with data correction and working with turbidity data performed much better than the operator that had no prior experience manual verification is only recommended if the operator has prior experience with data correction and or knowledge of the data that is being corrected the entire approach has been implemented in a free open source software package this software features a simple user interface to navigate through the entire approach this is especially useful during the manual verification step where users can quickly assess and modify the results from the automated detection and removal algorithm additional information on how to set up and use this software can be found on the page included in the software availability section while the current approach was tested using turbidity sensor data generalizing adapting this approach to assist in correcting data from other sensors would be valuable in theory this approach can be applied to characterizing real and unrealistic changes from many different sensors as most environmental parameters share similar data qualities when sampling frequency is sufficiently high there are also additional anomaly types e g sensor drift that were not targeted in this approach as they were not common problems with turbidity sensors however expanding the current approach to handle additional anomaly types would make it much more robust future work is needed to assess the applicability and adaptability of this approach on other environmental parameters author contributions as per the contributor roles taxonomy credit meguel yousif conceptualization methodology validation software writing original draft visualization hannah burdett conceptualization methodology software writing original draft dr christopher wellen validation formal analysis writing original draft supervision dr sohom mandal validation writing original draft supervision grace arabian data curation writing review editing derek smith resources supervision dr ryan j sorichetti conceptualization writing review editing project administration software availability name of software turbidity cleaner developers meguel yousif hannah burdett email meguel yousif mail utoronto ca hburdett ryerson ca availability https github com mayousif turbidity cleaner yousif and burdett 2022 year first available 2019 software required r v3 5 3 software license mit license program language english program size 700 mb a video demonstration of the software is also provided declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the canada ontario agreement project id 1009c and 1010d 16 through the ontario ministry of the environment conservation and parks omecp with ryerson university additional funding was provided through the natural sciences and engineering research council of canada nserc discovery grant we thank kelly biagi dr cody ross and the anonymous reviewers whose comments and suggestions helped improve the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105461 
25574,the identification of landscape classes facilitates the implementation of planning strategies although landscape patterns are key distinctive features of landscape classes existing unsupervised clustering techniques for clustering landscapes rely on categorical input data to quantify such patterns and consider only a limited number of pattern metrics to unlock the great potential of continuous spatial data such as remote sensing images for generating landscape typologies we adapted a novel unsupervised deep learning method deep convolutional embedded clustering dcec to generate a landscape typology for switzerland dcec encodes lower dimensional representations of input images in a hidden layer which is simultaneously used to divide the images into well distinguishable clusters we applied dcec to image tiles extracted from satellite images as well as ecological demographic and terrain layers dcec successfully distinguished 45 landscape classes in the continuous input data we conclude that dcec is a promising new method in landscape and land system research keywords unsupervised clustering image clustering landscape planning convolutional neural networks software and data availability the python code we used to perform deep convolutional embedded clustering dcec is free to download from https github com mvszuri landscape typ dcec some of the raster layers to which dcec was applied are open access but others are proprietary the switzerland wide sentinel 2 mosaics can be freely downloaded from https www npoc ch en services products html the maps of the proportion of coniferous and deciduous trees can be requested from the authors https www wsl ch de projekte tree species map of switzerland html the digital terrain model of switzerland can be downloaded from https www swisstopo admin ch en geodata height alti3d html the population density in buildings layer was created by combining the swiss topographic landscape model swisstlm3d downloadable from https www swisstopo admin ch en geodata landscape tlm3d html with a population density layer which can be requested from the swiss federal statistics office https www bfs admin ch bfs en home services geostat swiss federal statistics geodata html 1 introduction to monitor preserve and develop sustainable landscapes it is necessary to identify landscape types to which planning and management strategies can be applied simensen et al 2018 with the great heterogeneity in landscapes and land systems landscape typologies are a way to transfer and generalise knowledge from case or place based studies oberlack et al 2019 václavík et al 2016 moreover the complex varied and continuous landscape can be understood better when classified in types and spatial units antrop and van eetvelde 2017 p 265 these considerations sparked many research initiatives to define and map landscape typologies from local to global scales oberlack et al 2019 simensen et al 2018 government agencies also acknowledge the necessity to identify and classify landscapes for example the european landscape convention states that each of the ratifying countries should identify its own landscapes throughout its territory council of europe 2000 switzerland as one of the ratifying countries of this convention has established a switzerland wide landscape typology are 2011 also at a europe wide scale several landscape typologies have been developed such as the european landscape classification lanmap by mücher et al 2010 or the typology of european agricultural landscapes by van der zanden et al 2016 whereas the term typology seems to be mainly used in the context of landscapes antrop and van eetvelde 2017 simensen et al 2018 land system scientists tend to speak about the definition of archetypes oberlack et al 2019 sietz et al 2019 as there is considerable overlap between the methods used in both research areas sietz et al 2019 simensen et al 2018 we will here not elaborate on the potential differences between the two and present examples from both areas of research as long as the land system classification is performed in a spatially explicit way a first step in creating a landscape typology is usually the classification of landscapes into more or less homogeneous units antrop and van eetvelde 2017 differentiate between two general approaches for landscape classification the holistic or descending approach and the parametric or ascending approach with the holistic approach the landscape is successively subdivided into distinct units antrop and van eetvelde 2017 this approach has been applied for instance in expert based hierarchical landscape classification e g malek and verburg 2017 or in public or stakeholder participation processes e g käyhkö et al 2018 nogué et al 2016 in the parametric approach a set of basic landscape attributes or parameters are selected usually in the form of raster or vector maps which are then aggregated to larger spatial units antrop and van eetvelde 2017 due to the lack of pre existing landscape typologies this aggregation of landscape attributes usually relies on unsupervised clustering techniques in contrast to supervised classifications in which the outputs are predefined e g an existing landscape typology unsupervised clustering does not rely on any predetermined output classes or values i e the input is unlabelled examples of unsupervised landscape clustering methods are k means clustering rocha et al 2020 van eetvelde and antrop 2009b principle component analysis combined with hierarchical clustering pecher et al 2013 self organizing maps dittrich et al 2017 levers et al 2018 rocha et al 2020 václavík et al 2013 van der zanden et al 2016 and self organizing trees rocha et al 2020 the occurrence arrangement and shape of landscape elements henceforth referred to as landscape patterns are key distinctive features of landscape classes landscape patterns influence many ecological and socio economic processes e g stokes and seto 2019 turner and gardner 2015 affect the valuation of a landscape by people karasov et al 2019 and determine a landscape s character van eetvelde and antrop 2009a landscape character can be defined as a distinct recognisable and consistent pattern of elements in the landscape that makes one landscape different from another rather than better or worse antrop and van eetvelde 2017 p 299 remote sensing images provide the ideal data for detecting landscape patterns at different scales across large areas karasov et al 2019 due to the many ways in which image pixels can be combined across scales the potential number of different landscape patterns that can be detected in remote sensing images is practically endless therefore extracting landscape patterns from remote sensing images and other continuous spatial data requires an in depth analysis of the relationships between pixels at multiple scales karasov et al 2019 despite the great importance of landscape pattern analysis for the identification of landscape classes current parametric landscape classification approaches see above are not suited to exploit the immense variety of patterns that can be extracted from continuous spatial data such as remote sensing images the input data of unsupervised landscape clustering typically consists of a set of n raster layers representing variables that are deemed important characteristics of the landscape or land system each variable is aggregated to raster cells which typically represent an area of 1 1 km in landscape classification studies simensen et al 2018 the employed clustering method groups the individual raster cells based on these n values what results is a raster map in which each raster cell has been assigned one of k landscape classes with these approaches landscape patterns can be considered by quantifying the landscape in a raster cell with a selection of landscape pattern metrics e g pecher et al 2013 van eetvelde and antrop 2009b however most landscape pattern metrics require as input discrete categorical maps mcgarigal 2014 such as land use or land cover maps which are not always available and prone to classification bias lausch et al 2015 further bias can arise in the selection of pattern metrics which typically only represents a limited number of potential landscape patterns kupfer 2012 mcgarigal and cushman 2005 to identify well distinguishable landscape classes it is thus worthwhile to explore other unsupervised clustering approaches that do not rely on the pre processing of categorical input data and do not require a pre selection of pattern metrics in the past decade deep learning with convolutional neural networks cnns has rapidly become the leading method for image recognition classification and clustering tasks rawat and wang 2017 cnns have proven to be very successful in detecting complex patterns i e features for the classification or segmentation of images moreover they are capable of dealing with very high dimensional datasets such as large scale high resolution remote sensing data the inputs of cnns are usually images consisting of one i e monochromatic or three i e trichromatic colour bands and the outputs can be for instance classes to which these images are assigned at the heart of cnns are so called convolutional layers rawat and wang 2017 in which features are extracted from an input image and saved in a new image i e feature map each pixel in a feature map summarises the values in surrounding pixels in the input image using a convolutional filter of a certain size e g 5 5 pixels the dot product of the weights in the filter and the pixel values in the input image are calculated and passed on to the feature map this process is comparable to the moving or sliding window approach with kernel filters in traditional landscape pattern analyses hagen zanker 2016 however whereas the weights in traditional kernel filters are fixed the weights in convolutional filters are variable and learnt during the training of the cnn if the input of a cnn is an image with for example three colour bands then each convolutional filter will also have three layers with individual weights i e one layer for each colour band for such multi band input images the dot products of the individual filter layers are summed this way features can be extracted by aggregating the information in all input images for a visualised and detailed explanation of this process we refer to some excellent online resources li et al 2021 saha 2018 by training a multitude of filters with different weights one can extract a large range of features representing different patterns or shapes e g geometric shapes gradients or certain objects at different scales thus cnns can potentially be used to generate landscape typologies from continuous spatial data but to our knowledge no studies have experimented with deep learning for this purpose deep learning with cnns has frequently been used for the supervised classification or segmentation of remote sensing images e g carbonneau et al 2020 zhang et al 2019 unsupervised learning with cnns is also possible by setting up the network s architecture as an autoencoder which is a neural network that is trained to attempt to copy its input to its output and consists of an encoder and a decoder goodfellow et al 2016 p 499 the encoder translates input images to a low dimensional code describing these images i e the hidden layer the decoder is a mirrored version of the encoder and is used to reconstruct the original input image from the hidden layer if the output image resembles the input image the autoencoder has been trained successfully as the hidden layer has a smaller dimension than the input images autoencoders are often used for dimensionality reduction low dimensional representations of images are useful in clustering tasks goodfellow et al 2016 unsupervised deep learning with cnns could thus be a promising approach to learn features from remote sensing images reduce their dimensionality and cluster them into landscape classes in summary although a great variety of landscape patterns could thus be extracted from remote sensing images and other continuous spatial data currently used landscape metrics are not suitable to perform such an analysis in contrast deep learning with cnns is ideally suited to extract complex patterns from continuous data and may thus present a useful approach to identify landscape types in such data in this study we demonstrate the use of unsupervised deep learning with cnns to generate a landscape typology for switzerland making use of remote sensing images and other continuous spatial data more specifically we employ deep convolutional embedded clustering dcec guo et al 2017 which is a novel technique that has been designed for the unsupervised clustering of images we apply dcec to images extracted from several switzerland wide raster layers consisting of sentinel 2 satellite images a digital terrain model as well as continuous ecological and demographic layers after determining the optimal number of clusters or landscape classes for this set of images we trained the dcec network and predicted the landscape classes for the whole of switzerland we subsequently assessed the spatial and non spatial differences between the detected classes and compared these landscape classes to the existing swiss landscape typology are 2011 lastly we compare the dcec clustering quality to that of several common landscape clustering methods 2 methods 2 1 input images although most cnns are trained with input images consisting of one or several colour bands we used image stacks derived from six switzerland wide medium resolution raster layers representing different landscape characteristics table 1 most approaches to characterise landscapes focus on the assessment of landform and the composition of natural and human landscape elements simensen et al 2018 p 557 and we also considered these aspects in our selection of layers the first two layers were derived from a switzerland wide rgb mosaic of sentinel 2 satellite images for the period august to october 2018 copernicus sentinel data npoc www npoc ch and were included to detect general land use and land cover patterns we chose to take the ratio between colour bands i e band1 band2 band1 band2 because compared to raw images these tend to be less biased by shadow effects of mountains bijeesh and narasimhamurthy 2020 kao et al 2014 which are prevalent in switzerland such ratios are also recommended to eliminate illumination changes ball et al 2017 p 19 which may be present in mosaics composed of satellite images taken on different dates and times band ratios have proven useful in remote sensing image classification and segmentation tasks such as the classification of land cover chang et al 2016 gonçalves et al 2019 the identification of hydrological features bijeesh and narasimhamurthy 2020 james et al 2021 or the quantification of soil properties xu et al 2017 we only included the red green and the red blue ratios table 1 because a linear combination of these two ratios explained 99 85 of the variation in the green blue ratio i e a linear regression model in which the green blue ratio was a function of the red green and red blue ratios had an r2 of 0 9985 as the calculated ratios contained some extreme high and low values we replaced these outliers with the lowest and highest 0 01 percentile values the third layer was the altitude derived from a 2 2 m digital terrain model dtm swiss federal office of topography swissalti3d table 1 which was included as the shape of the terrain is an important landscape characteristic simensen et al 2018 and because altitude is a strong driver of swiss landscape types are 2011 we did not derive any additional terrain attributes from the dtm such as slope aspect or roughness as cnns are capable of learning such attributes directly during the training of the network heo et al 2020 kirkwood 2020 silburt et al 2019 the filters with which such traditional attributes e g slope are calculated are conceptually very similar to the convolutional filters used in cnns however whereas the weights in the filters of the traditional attributes are fixed the weights in cnn filters are tailored to provide maximal discriminative or explanatory power kirkwood 2020 in this way cnns have learnt terrain attributes from dtms that were useful variables for the detection of craters silburt et al 2019 for explaining concentrations of elements in soils kirkwood 2020 or for the prediction of potential solar energy heo et al 2020 the fourth layer depicted the population density in individual buildings table 1 and was selected to provide information on human population density building configuration and building type e g single family house multi family high rise building this layer was created by disaggregating a 100 100 m population density layer swiss federal statistics office geostat 2018 to individual buildings in the swiss topographic landscape model swisstlm3d swiss federal office of topography 2019 table 1 as the distribution of the resulting raster of population density per building was right skewed we log transformed the data the fifth and sixth layers consisted of the proportion of deciduous and coniferous trees in 10 10 m pixels respectively waser et al 2017 waser et al unpublished results table 1 the composition and configuration of trees in the landscape does not only have an important ecological function but is also a key aesthetical component of a landscape all layers were transformed to match the origin resolution 10 10 m and extent of the sentinel 2 mosaics we then divided the switzerland wide layers into non overlapping images of 64 64 pixels table 1 each image thus represents an area of 640 640 m which is comparable to the spatial resolution used in many other landscape classifications simensen et al 2018 only images without no data values were maintained each image stack of six layers thus consisted of 64 64 x 6 24576 values finally we obtained 99076 image stacks to cover the whole of switzerland these image stacks were used to train the dcec network 2 2 deep convolutional embedded clustering dcec in recent years several studies have demonstrated the effectiveness of neural networks to simultaneously perform dimensionality reduction and unsupervised clustering e g xie et al 2016 yang et al 2017 for the clustering of images novel methods employing cnn autoencoders are especially promising guo et al 2017 li et al 2018 one of the latter methods is dcec proposed by guo et al 2017 as these authors found that dcec had a higher clustering accuracy than other techniques we chose dcec to generate a landscape classification for switzerland the architecture of our dcec network is similar to the architecture proposed by guo et al 2017 and is schematically depicted in fig 1 with dcec input images are simultaneously reconstructed and clustered the reconstruction is achieved with an autoencoder whereas the clustering is done by passing the hidden layer through a clustering layer in the encoder part of the dcec autoencoder input images are passed through three convolutional layers conv1 conv2 and conv3 fig 1 in which feature maps sequentially become smaller but more numerous the size of the convolutional filters is 5 5 pixels for conv1 and conv2 and 3 3 pixels for conv3 by applying convolutional filters with a stride of 2 pooling filters are avoided which increases the capacity to transform features guo et al 2017 the output of the third convolutional layer is unfolded into a one dimensional string of values which is then fully connected to a hidden layer of a certain length fig 1 the hidden layer is then passed on to the decoder which contains the same processing steps as the encoder albeit in the opposite order the output of the decoder are the reconstructed images the hidden layer contains a low dimensional code of the input images which enables the clustering layer to cluster the input images into a predefined number of classes fig 1 the objective of training cnns is to minimise the difference i e loss between the output and a reference distribution in the dcec network these losses are the reconstruction loss and the clustering loss which are minimised simultaneously guo et al 2017 the reconstruction loss is the mean squared error between the input images and the reconstructed images the clustering loss is the kullback leibler divergence between the output of the clustering and a target distribution guo et al 2017 xie et al 2016 as we are using unsupervised learning there is no predefined target distribution of classes therefore dcec uses an innovative approach to generate new target distributions from those images that are assigned a high confidence of belonging to a certain class xie et al 2016 every couple of training iterations i e epochs the target distribution is updated by upweighting the high confidence assignments and downweighting the low confidence assignments this way the cluster purity is improved during training of the network xie et al 2016 at the beginning of training the initial target distribution is set by k means clustering of the hidden layer whereas xie et al 2016 and guo et al 2017 normalise the target distribution with cluster size to prevent large clusters from emerging we did not perform this normalisation as there is probably a strong variation in the prevalence of landscape classes as optimizer we used the amsgrad variation of the adam optimizer reddi et al 2018 for more detailed information on dcec we refer xie et al 2016 and guo et al 2017 2 3 pre training and training the dcec network the first step in training the dcec network was to pre train the autoencoder so that we could determine the optimal length of the hidden layer and speed up subsequent calculations for the pre training the set of input images was divided into a training 85 a validation 12 and a test set 3 the test set was not used during the pre training of the dcec network but only after the pre training to determine the reconstruction loss of the final model we tested hidden layer lengths of 500 1000 1500 2000 2500 and 3000 values the learning rate was initially set to 0 001 and gradually decreased the pre training was terminated when the learning rate reached 0 000001 and there had been no improvement of the validation reconstruction loss for six consecutive epochs to select an optimal hidden layer length we determined the final reconstruction loss on the training validation and test sets after the pre training the complete dcec network was trained on the complete set of input images guo et al 2017 as it is not possible to perform an independent validation of the clusters with unlabelled images as with many unsupervised clustering methods used to generate landscape typologies the number of clusters had to be set a priori sietz et al 2019 therefore we trained dcec networks with 10 15 20 25 30 35 40 45 50 55 60 and 65 clusters to determine the optimal number of clusters for our dataset for every number of clusters we trained the dcec network four times and we determined the quality of clustering by calculating three well known cluster evaluation metrics ünlü and xanthopoulos 2019 the davies bouldin index davies and bouldin 1979 the silhouette index rousseeuw 1987 and the calinski harabasz index caliński and harabasz 1974 these indices were calculated from the hidden layer code of all input images furthermore to get an impression of the learning process at several points during the training we performed a principle component analysis pca and calculated the aforementioned metrics from the hidden layer in guo et al 2017 the training is terminated once the change in target distribution between two consecutive updates is less than 0 001 i e 0 1 of the images are assigned to a different class however to increase comparability between different runs we stopped training after the first update after 1600 epochs or 1800 epochs for those runs with 50 clusters with the final trained dcec network we predicted the landscape typology of all 99076 input image stacks so that we obtained a switzerland wide coverage all calculations were performed in python 3 6 python software foundations 2020 and with keras chollet 2015 with tensorflow abadi et al 2015 backend pre processing of the input images was performing in arcgis pro environmental systems research institute redlands usa the networks were trained on a desktop pc with an nvidia geforce gtx 1080 gpu and 64 gb of ram our code is adapted from the code of guo et al 2017 and available on https github com mvszuri landscape typ dcec 2 4 exploration of the dcec landscape typology we performed two additional analyses on the dcec results to get a better understanding of the detected classes first we aimed to characterise each identified class based on a range of indicators mean population density mean slope mean altitude and the mean proportions of coniferous and deciduous forest we depicted these values in radarplots for each class in the second analysis we plotted ten randomly selected images from each dcec class enabling a visual analysis of the within class similarities and between class differences the above post processing steps were performed in python 3 6 software foundations 2020 and in r 4 0 2 r development core teams 2020 2 5 comparison dcec classes with swiss landscape typology we compared the dcec landscape classification to the existing swiss landscape typology from the federal office for spatial development are henceforth referred to as the are typology are 2011 to do this we calculated a contingency table which contained the count of the number of co occurring raster cells between classes from either classification to determine whether the two classifications were associated with one another we performed a fischer s test on the contingency table with 10000 replicates in the monte carlo test fisher 1922 as the order of the classes in the dcec clustering is random the highest co occurrences might be scattered all throughout the table which could hamper the visual assessment of the relationship between the two typologies therefore we reorder the rows and columns in the contingency table by performing a detrended correspondence analysis dca hill and gauch 1980 on the contingency table we then reordered the rows and columns based on the loadings on the first and second dca axes 2 6 comparison to traditional landscape clustering methods to determine whether the dcec approach was able to learn features that resulted in a better clustering quality than existing landscape clustering methods we compared the dcec clustering to the following frequently used clustering methods k means clustering rocha et al 2020 van eetvelde and antrop 2009b self organizing maps dittrich et al 2017 levers et al 2018 rocha et al 2020 václavík et al 2013 van der zanden et al 2016 and principle component analysis combined with hierarchical clustering pecher et al 2013 as mentioned in the introduction these methods are usually applied to data that has been aggregated in fairly large raster cells e g 1 km2 however in our approach the input data for each landscape unit consisted of disaggregated data of 24576 values see section 2 1 to allow comparison with the dcec approach we also used this input data for the above clustering methods only for the hierarchical clustering instead of using the raw values as input we calculated the first 50 principle components of the images and used the scores of each observation as input for each method we calculated an equal number of clusters as the final number of dcec clusters see section 2 3 to compare the clustering quality of these traditional methods with the dcec approach we computed the davies bouldin index for each of the clustering results davies and bouldin 1979 these analysis were performed with the python packages scikit learn pedregosa et al 2011 and sklearn som smith and montali 2021 3 results 3 1 pre training of dcec autoencoder during pre training of the autoencoder we found that the network with a hidden layer length of 1500 had the lowest test 0 00225 and validation loss 0 00222 fig 2 this length was thus used in all subsequent analyses the dimension of the hidden layer was thus 6 10 of the dimension of the input images i e 24576 values despite this considerable dimensionality reduction the quality of the reconstructed images is good fig 3 there are some reductions in details compared to the original images but the general patterns are still clearly visible only for the images of the population density in buildings the reconstructed images show a lower similarity to the original images fig 3 3 2 dcec training we trained four dcec networks for each of the tested number of clusters i e 10 15 20 and 65 classes the mean proportion of change in class allocations in the last three updates of all performed runs was 0 0012 sd 0 0010 thus on average only 0 12 of the input images i e approx 128 images were allocated to another class after the final updates indicating good convergence of the training of the networks i e this value is very similar to that used in guo et al 2017 during the training of the networks we found that the davies bouldin db index of the hidden layer decreased and the proportion of explained variance by the first two principle components pc increased fig 4 indicating an improvement in clustering quality a visual inspection of the clusters also showed that the cluster purity improves during training fig 4 the average training time for one dcec network was 21 6 h comparing the different numbers of clusters we found that there was no apparent improvement of the mean db index of the hidden layer beyond 45 classes fig 5 here we only present the results of the db index as the silhouette and calinski harabasz indices showed very similar results interestingly 45 detected classes is quite similar to the 39 landscape classes in the existing are landscape typology are 2011 for any number of classes we observed quite some variation in db index between separate training runs fig 5 to assess whether even lower db index values could be obtained we also trained networks with 41 42 43 44 46 47 48 and 49 classes but none of them produced better results than the network with 45 classes results not shown from the dcec networks with 45 classes we used the network with the lowest db score 0 6172 to predict the landscape classes for this network the average reconstruction loss of the last 10 epochs was 0 00304 whereas after pre training the reconstruction loss of the training set was 0 00214 fig 2 this indicates that there is a trade off between minimising the reconstruction loss and minimising the clustering loss and during training some of the reconstruction loss was sacrificed in favour of a low clustering loss 3 3 exploration of the dcec classes a map showing all dcec classes in switzerland is included in the supplementary material fig a1 the maps of the individual landscape classes combined with radarplots of the mean landscape indicators showed that some of the detected landscape classes only occurred in a certain biogeographical region fig 6 for example classes 13 5 7 and 31 mainly occurred in the inner alps whereby these classes seem to differ mainly in mean elevation and slope fig 6 in contrast other classes occurred across several regions and showed a much more scattered distribution such as classed 34 or 42 fig 6 in the swiss plateau and other populated areas in switzerland fig 6 there seems to be several landscape classes representing a gradient of population density 17 16 and 11 other landscape classes such as 30 and 39 appear very similar with respect to the mean values of the five indicators radarplots in fig 6 but have differing spatial distributions maps in fig 6 the example landscapes from each of the identified dcec classes show within class variation but also clear between class gradients ranging from alpine landscapes to forested landscapes to agricultural and more populated landscapes fig a2 it is important to remember that these example images do not represent all the layers that were input to the dcec and therefore that some of the images with a seemingly high within class heterogeneity may actually be quite homogeneous in the other input layers 3 4 comparison dcec classes with swiss landscape typology we found that there was a highly significant association between the existing are typology are 2011 and our dcec landscape clustering fisher s exact test p 0 0001 the dca reordered contingency table also showed a pattern of association between the two classifications fig 7 some of the are classes contain only few dcec classes such as vineyard landscape settlement landscape or valley landscape of the inner alps fig 7 also some of the dcec classes are quite specific to a certain are class such as class 13 which only overlaps with the high mountain landscapes of the alps or class 10 and 27 which mainly overlap with the lakes class fig 7 however most are classes contain multiple dcec classes and vice versa fig 7 nevertheless the combination and frequency of dcec classes that overlap with a certain are class is unique for each are class 3 5 comparison to traditional landscape clustering methods a visual comparison of the dcec clustering quality fig 4 to that of three traditional landscape clustering methods k means clustering self organizing maps and principle component analysis with hierarchical clustering fig 8 showed that the cluster purity for the dcec method was much higher than for the three traditional methods this was also apparent in the higher db scores for the traditional methods db 2 905 fig 8 than for the dcec clustering db 0 616 fig 4 4 discussion in this study we have used deep convolutional embedded clustering dcec guo et al 2017 to generate a landscape typology for switzerland from remote sensing images and other continuous spatial data although several methods for unsupervised clustering using cnns have recently been developed e g guo et al 2017 ji et al 2019 yang et al 2017 to our knowledge the present study is the first application of such methods for the clustering of landscapes we have shown that dcec allows us to employ the unprecedented abilities of state of the art deep learning techniques for the unsupervised clustering of continuous spatial data the good quality of the reproduced images indicates that the dcec autoencoder successfully learned features and performed dimensionality reduction of the input images during training of the dcec network the low dimensional representation of the input images was not only adapted to reproduce the images as well as possible but also to provide well distinguishable clusters of images the low davies bouldin index values and the visual representation of the clusters in principle component space fig 4 indicated that the algorithm also succeeded in the latter task some of the detected landscape classes were clearly distinguishable from one another with respect to their geographic location maps in fig 6 thematic signature radarplots in fig 6 or visual appearance images in fig a2 whereas others were much harder to distinguish this indicates that the dcec network learned distinguishing landscape features from the remote sensing ecological demographic and terrain layers for instance although slope was not an input layer table 1 it was found to be a distinguishing characteristic between some of the landscape classes e g class 7 and 31 in fig 6 this could imply that the dcec algorithm leaned to calculate slope from the dtm to distinguish between landscapes with the dcec approach it is thus not only possible to identify landscape classes with a high cluster purity but also to detect new landscape features that were previously unknown or not apparent we found that the clustering quality of the dcec approach is considerably higher than that of traditional landscape clustering methods antrop and van eetvelde 2017 rocha et al 2020 sietz et al 2019 simensen et al 2018 figs 4 and 8 this difference in performance could be because these traditional methods are not ideally suited to cluster very high dimensional data assent 2012 but also because the functioning of the presented dcec approach is fundamentally different to that of the traditional methods with the traditional methods individual raster cells are assigned to clusters therefore prior to the clustering all spatial data has to be aggregated to the individual raster cells as these cells are typically 1 1 km simensen et al 2018 aggregation can lead to a considerable loss of information to quantify landscape patterns this aggregation is usually performed with landscape metrics however landscape metrics rely on potentially biased land use or land cover maps mcgarigal 2014 and do not cover the full breadth of potential landscape patterns kupfer 2012 mcgarigal and cushman 2005 furthermore as all the input data is fixed prior to the clustering the existing methods are not able to learn new landscape patterns that differentiate well between clusters with the dcec method these potential drawbacks of these traditional methods are overcome the input of the dcec method can be continuous or classified raster layers and no a priori aggregation or calculation of landscape metrics is required instead the algorithm learns to identify landscape features that are useful to assign a landscape image to a certain class by using a large number of convolutional filters that are learned during the training of the networks the number of patterns that can be identified with dcec far exceeds that of traditional landscape pattern metrics we conclude that the dcec approach adds a powerful new method to the suite of approaches for the unsupervised generation of landscape typologies antrop and van eetvelde 2017 rocha et al 2020 sietz et al 2019 simensen et al 2018 we found that there was a significant spatial association between our dcec landscape clustering and the existing are typology are 2011 which suggests that our automated dcec approach is able to generate meaningful landscape classes nevertheless only very few dcec classes were exclusively associated with a single are class or vice versa this latter result is not surprising as our aim was not to reproduce the are typology on the one hand the difference between the typologies could be explained by the fact that certain factors considered in the are typology such as precipitation levels and agricultural land use fig 7 are 2011 were not explicitly considered in our study on the other hand the difference may be resulting from a mismatch in the spatial scales of both typologies the goal of the are typology was to define landscape classes so that the patches of a certain class were minimally 10 km2 with some exceptions possible which resulted in patches that were dominated by a certain landscape class but still contained pockets of other classes are 2011 in our dcec typology the minimal size of a landscape unit was 0 41 km2 i e the input images were 640 640 m and each image was classified independent of its neighbouring cells to deal with this mismatch in scales a possible further analysis could be to perform a second larger scale clustering analysis on the landscape class map from the current dcec classification fig a1 the fact that we found unique combinations of dcec classes in each are class suggests that certain compositions and configurations of dcec classes could be specific for an are class such a two scale analysis was performed by van eetvelde and antrop 2009b who first performed a k means clustering on several raster layers to define fine scale landscape classes these classes were then further aggregated into courser scale units by manually delineating areas with unique combinations of classes in principle the dcec approach could be applied at both scales to get large scale landscape units which are potentially more comparable to the are typology despite the excellent performance of cnns in image recognition and classification tasks rawat and wang 2017 they are often referred to as a black box as their internal logic is unknown and it is a challenge to get an idea of the learned features heinrich et al 2019 although we compared landscape classes based on their spatial distribution and thematic signatures with our post processing steps it also remained unclear what specific landscape features distinguished the different classes from each other several advances in the analysis of cnns could provide better insights in their internal logic for example zeiler and fergus 2014 present an approach to depict the feature activities in convolutional layers at the same resolution as the input images with this approach these authors showed that their cnn learned understandable features and they observed desirable properties such as compositionality increasing invariance and class discrimination as they ascend the layers zeiler and fergus 2014 p 831 in van molle et al 2018 a similar technique was used to depict where the activation of a certain feature was highest in the original image a current overview of such visualisation techniques is given in heinrich et al 2019 implementing such techniques would enable determining what features were learned and where these features are located on the input images this would not only aid in identifying problems in the cnns architecture zeiler and fergus 2014 but also help determine characteristic landscape configurations that were not considered before although the practical implementation of some of the current visualisation techniques can be a challenge heinrich et al 2019 their fast development will probably make them more widely applicable in the near future in our study we used six medium resolution switzerland wide raster layers to characterise the landscape table 1 although these layers comprised important social ecological and topographic characteristics of the landscape they may not represent all factors that characterise a landscape in a comprehensive analysis groom 2005 p 38 identified four dimensions of landscape characteristics the biophysical dimension relating to the form and functioning of the landscape the socio economic technical dimension quantifying the human influence on the landscape form the human aesthetic dimension addressing the human experience of the landscape and the policy dimension relating to opinions and rights of stakeholders for each dimension groom 2005 gives multiple examples of spatial datasets implementing all these datasets would require a considerable expansion of the number of raster layers technically there are no limitations to the number of input layers that dcec can handle however from a practical perspective it could become complicated to collect such amounts of data at a standardised resolution for example it is probably difficult to assemble detailed historical spatial datasets which are necessary to assess the historical development of a landscape i e an important factor in landscape characterisation groom 2005 one technical challenge that does remain is how to deal with zero inflated data which may be prevalent in spatial layers in our demonstration of dcec we observed a relatively low reconstruction success of the layer quantifying the population density in buildings fig 3 this can potentially be explained by the fact that the values in this layer were zero inflated as the vast majority of land area has no buildings and thus a population density of zero fortunately techniques are being developed to deal with zero inflated input data in neural networks e g diaz and joseph 2019 several other developments in machine and deep learning could be interesting alternatives or additions to dcec one of these popular novel approaches is geometric or graph deep learning which attempts to use graph theory to apply deep learning to non euclidean geometric data bronstein et al 2017 as mentioned above a drawback of our approach is that we did not consider neighbouring images when classifying the landscape in an image geometric deep learning could potentially be used to capture the spatial arrangement of landscape classes for example ma et al 2019 successfully used graphs expressing the correlations between regions to guide the segmentation of synthetic aperture radar images another interesting recent development are self organizing maps combined with convolutional layers elend and kramer 2020 similar to our approach these authors used convolutional layers to learn features from the input images which were subsequently used to improve the clustering of these images with self organizing maps given the frequent use of self organizing maps to generate landscape typologies dittrich et al 2017 levers et al 2018 rocha et al 2020 václavík et al 2013 van der zanden et al 2016 this new approach could potentially be implemented with relative ease furthermore although it may seem challenging to select an appropriate method to generate landscape typologies it may actually be an advantage to perform clusterings with several models and combine their output a process called ensemble modelling jurek et al 2014 ensemble modelling can be applied to the output of different clustering methods to increase the clustering accuracy but can also be used to average the variability in outputs of repeated runs of a single method for example as we found quite some variation in the quality of the clustering of repeated trainings of our dcec network fig 5 combining these outputs could be an approach to generate robust results computer generated landscape typologies can either be a final product or an intermediate product to support a decision making process or expert based landscape classification for example the landscape typology in the swiss canton of schwyz was established in a participatory stakeholder process which was also supported by outputs of unsupervised clustering methods rodewald et al 2019 the computer generated typologies can lead to new ideas and realisations among the involved parties the presented dcec methodology is an improvement to other methods antrop and van eetvelde 2017 rocha et al 2020 sietz et al 2019 simensen et al 2018 especially because it utilises the excellent abilities of cnns to detect patterns in images this could lead to the identification of recurrent landscape patterns that are not directly apparent to the human eye but could present an important distinguishing feature in landscapes the fact that we did not find a perfect match between the are and dcec landscape classes see above could actually be an opportunity to consider other landscape patterns and characteristics and fine tune the swiss landscape typology to our knowledge the current study is the first study to use deep learning to generate unsupervised landscape typologies directly from continuous spatial data we are confident that with further improvements of the network s architecture and interpretability the use of such methods will open many new possibilities in landscape and land system research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank four anonymous reviewers for their constructive comments on earlier versions of this manuscript this research was supported by the european union s horizon 2020 research and innovation programme ecopotential project grant agreement no 641762 and by the swiss national science foundation emphases project grant number 200021 192018 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105462 
25574,the identification of landscape classes facilitates the implementation of planning strategies although landscape patterns are key distinctive features of landscape classes existing unsupervised clustering techniques for clustering landscapes rely on categorical input data to quantify such patterns and consider only a limited number of pattern metrics to unlock the great potential of continuous spatial data such as remote sensing images for generating landscape typologies we adapted a novel unsupervised deep learning method deep convolutional embedded clustering dcec to generate a landscape typology for switzerland dcec encodes lower dimensional representations of input images in a hidden layer which is simultaneously used to divide the images into well distinguishable clusters we applied dcec to image tiles extracted from satellite images as well as ecological demographic and terrain layers dcec successfully distinguished 45 landscape classes in the continuous input data we conclude that dcec is a promising new method in landscape and land system research keywords unsupervised clustering image clustering landscape planning convolutional neural networks software and data availability the python code we used to perform deep convolutional embedded clustering dcec is free to download from https github com mvszuri landscape typ dcec some of the raster layers to which dcec was applied are open access but others are proprietary the switzerland wide sentinel 2 mosaics can be freely downloaded from https www npoc ch en services products html the maps of the proportion of coniferous and deciduous trees can be requested from the authors https www wsl ch de projekte tree species map of switzerland html the digital terrain model of switzerland can be downloaded from https www swisstopo admin ch en geodata height alti3d html the population density in buildings layer was created by combining the swiss topographic landscape model swisstlm3d downloadable from https www swisstopo admin ch en geodata landscape tlm3d html with a population density layer which can be requested from the swiss federal statistics office https www bfs admin ch bfs en home services geostat swiss federal statistics geodata html 1 introduction to monitor preserve and develop sustainable landscapes it is necessary to identify landscape types to which planning and management strategies can be applied simensen et al 2018 with the great heterogeneity in landscapes and land systems landscape typologies are a way to transfer and generalise knowledge from case or place based studies oberlack et al 2019 václavík et al 2016 moreover the complex varied and continuous landscape can be understood better when classified in types and spatial units antrop and van eetvelde 2017 p 265 these considerations sparked many research initiatives to define and map landscape typologies from local to global scales oberlack et al 2019 simensen et al 2018 government agencies also acknowledge the necessity to identify and classify landscapes for example the european landscape convention states that each of the ratifying countries should identify its own landscapes throughout its territory council of europe 2000 switzerland as one of the ratifying countries of this convention has established a switzerland wide landscape typology are 2011 also at a europe wide scale several landscape typologies have been developed such as the european landscape classification lanmap by mücher et al 2010 or the typology of european agricultural landscapes by van der zanden et al 2016 whereas the term typology seems to be mainly used in the context of landscapes antrop and van eetvelde 2017 simensen et al 2018 land system scientists tend to speak about the definition of archetypes oberlack et al 2019 sietz et al 2019 as there is considerable overlap between the methods used in both research areas sietz et al 2019 simensen et al 2018 we will here not elaborate on the potential differences between the two and present examples from both areas of research as long as the land system classification is performed in a spatially explicit way a first step in creating a landscape typology is usually the classification of landscapes into more or less homogeneous units antrop and van eetvelde 2017 differentiate between two general approaches for landscape classification the holistic or descending approach and the parametric or ascending approach with the holistic approach the landscape is successively subdivided into distinct units antrop and van eetvelde 2017 this approach has been applied for instance in expert based hierarchical landscape classification e g malek and verburg 2017 or in public or stakeholder participation processes e g käyhkö et al 2018 nogué et al 2016 in the parametric approach a set of basic landscape attributes or parameters are selected usually in the form of raster or vector maps which are then aggregated to larger spatial units antrop and van eetvelde 2017 due to the lack of pre existing landscape typologies this aggregation of landscape attributes usually relies on unsupervised clustering techniques in contrast to supervised classifications in which the outputs are predefined e g an existing landscape typology unsupervised clustering does not rely on any predetermined output classes or values i e the input is unlabelled examples of unsupervised landscape clustering methods are k means clustering rocha et al 2020 van eetvelde and antrop 2009b principle component analysis combined with hierarchical clustering pecher et al 2013 self organizing maps dittrich et al 2017 levers et al 2018 rocha et al 2020 václavík et al 2013 van der zanden et al 2016 and self organizing trees rocha et al 2020 the occurrence arrangement and shape of landscape elements henceforth referred to as landscape patterns are key distinctive features of landscape classes landscape patterns influence many ecological and socio economic processes e g stokes and seto 2019 turner and gardner 2015 affect the valuation of a landscape by people karasov et al 2019 and determine a landscape s character van eetvelde and antrop 2009a landscape character can be defined as a distinct recognisable and consistent pattern of elements in the landscape that makes one landscape different from another rather than better or worse antrop and van eetvelde 2017 p 299 remote sensing images provide the ideal data for detecting landscape patterns at different scales across large areas karasov et al 2019 due to the many ways in which image pixels can be combined across scales the potential number of different landscape patterns that can be detected in remote sensing images is practically endless therefore extracting landscape patterns from remote sensing images and other continuous spatial data requires an in depth analysis of the relationships between pixels at multiple scales karasov et al 2019 despite the great importance of landscape pattern analysis for the identification of landscape classes current parametric landscape classification approaches see above are not suited to exploit the immense variety of patterns that can be extracted from continuous spatial data such as remote sensing images the input data of unsupervised landscape clustering typically consists of a set of n raster layers representing variables that are deemed important characteristics of the landscape or land system each variable is aggregated to raster cells which typically represent an area of 1 1 km in landscape classification studies simensen et al 2018 the employed clustering method groups the individual raster cells based on these n values what results is a raster map in which each raster cell has been assigned one of k landscape classes with these approaches landscape patterns can be considered by quantifying the landscape in a raster cell with a selection of landscape pattern metrics e g pecher et al 2013 van eetvelde and antrop 2009b however most landscape pattern metrics require as input discrete categorical maps mcgarigal 2014 such as land use or land cover maps which are not always available and prone to classification bias lausch et al 2015 further bias can arise in the selection of pattern metrics which typically only represents a limited number of potential landscape patterns kupfer 2012 mcgarigal and cushman 2005 to identify well distinguishable landscape classes it is thus worthwhile to explore other unsupervised clustering approaches that do not rely on the pre processing of categorical input data and do not require a pre selection of pattern metrics in the past decade deep learning with convolutional neural networks cnns has rapidly become the leading method for image recognition classification and clustering tasks rawat and wang 2017 cnns have proven to be very successful in detecting complex patterns i e features for the classification or segmentation of images moreover they are capable of dealing with very high dimensional datasets such as large scale high resolution remote sensing data the inputs of cnns are usually images consisting of one i e monochromatic or three i e trichromatic colour bands and the outputs can be for instance classes to which these images are assigned at the heart of cnns are so called convolutional layers rawat and wang 2017 in which features are extracted from an input image and saved in a new image i e feature map each pixel in a feature map summarises the values in surrounding pixels in the input image using a convolutional filter of a certain size e g 5 5 pixels the dot product of the weights in the filter and the pixel values in the input image are calculated and passed on to the feature map this process is comparable to the moving or sliding window approach with kernel filters in traditional landscape pattern analyses hagen zanker 2016 however whereas the weights in traditional kernel filters are fixed the weights in convolutional filters are variable and learnt during the training of the cnn if the input of a cnn is an image with for example three colour bands then each convolutional filter will also have three layers with individual weights i e one layer for each colour band for such multi band input images the dot products of the individual filter layers are summed this way features can be extracted by aggregating the information in all input images for a visualised and detailed explanation of this process we refer to some excellent online resources li et al 2021 saha 2018 by training a multitude of filters with different weights one can extract a large range of features representing different patterns or shapes e g geometric shapes gradients or certain objects at different scales thus cnns can potentially be used to generate landscape typologies from continuous spatial data but to our knowledge no studies have experimented with deep learning for this purpose deep learning with cnns has frequently been used for the supervised classification or segmentation of remote sensing images e g carbonneau et al 2020 zhang et al 2019 unsupervised learning with cnns is also possible by setting up the network s architecture as an autoencoder which is a neural network that is trained to attempt to copy its input to its output and consists of an encoder and a decoder goodfellow et al 2016 p 499 the encoder translates input images to a low dimensional code describing these images i e the hidden layer the decoder is a mirrored version of the encoder and is used to reconstruct the original input image from the hidden layer if the output image resembles the input image the autoencoder has been trained successfully as the hidden layer has a smaller dimension than the input images autoencoders are often used for dimensionality reduction low dimensional representations of images are useful in clustering tasks goodfellow et al 2016 unsupervised deep learning with cnns could thus be a promising approach to learn features from remote sensing images reduce their dimensionality and cluster them into landscape classes in summary although a great variety of landscape patterns could thus be extracted from remote sensing images and other continuous spatial data currently used landscape metrics are not suitable to perform such an analysis in contrast deep learning with cnns is ideally suited to extract complex patterns from continuous data and may thus present a useful approach to identify landscape types in such data in this study we demonstrate the use of unsupervised deep learning with cnns to generate a landscape typology for switzerland making use of remote sensing images and other continuous spatial data more specifically we employ deep convolutional embedded clustering dcec guo et al 2017 which is a novel technique that has been designed for the unsupervised clustering of images we apply dcec to images extracted from several switzerland wide raster layers consisting of sentinel 2 satellite images a digital terrain model as well as continuous ecological and demographic layers after determining the optimal number of clusters or landscape classes for this set of images we trained the dcec network and predicted the landscape classes for the whole of switzerland we subsequently assessed the spatial and non spatial differences between the detected classes and compared these landscape classes to the existing swiss landscape typology are 2011 lastly we compare the dcec clustering quality to that of several common landscape clustering methods 2 methods 2 1 input images although most cnns are trained with input images consisting of one or several colour bands we used image stacks derived from six switzerland wide medium resolution raster layers representing different landscape characteristics table 1 most approaches to characterise landscapes focus on the assessment of landform and the composition of natural and human landscape elements simensen et al 2018 p 557 and we also considered these aspects in our selection of layers the first two layers were derived from a switzerland wide rgb mosaic of sentinel 2 satellite images for the period august to october 2018 copernicus sentinel data npoc www npoc ch and were included to detect general land use and land cover patterns we chose to take the ratio between colour bands i e band1 band2 band1 band2 because compared to raw images these tend to be less biased by shadow effects of mountains bijeesh and narasimhamurthy 2020 kao et al 2014 which are prevalent in switzerland such ratios are also recommended to eliminate illumination changes ball et al 2017 p 19 which may be present in mosaics composed of satellite images taken on different dates and times band ratios have proven useful in remote sensing image classification and segmentation tasks such as the classification of land cover chang et al 2016 gonçalves et al 2019 the identification of hydrological features bijeesh and narasimhamurthy 2020 james et al 2021 or the quantification of soil properties xu et al 2017 we only included the red green and the red blue ratios table 1 because a linear combination of these two ratios explained 99 85 of the variation in the green blue ratio i e a linear regression model in which the green blue ratio was a function of the red green and red blue ratios had an r2 of 0 9985 as the calculated ratios contained some extreme high and low values we replaced these outliers with the lowest and highest 0 01 percentile values the third layer was the altitude derived from a 2 2 m digital terrain model dtm swiss federal office of topography swissalti3d table 1 which was included as the shape of the terrain is an important landscape characteristic simensen et al 2018 and because altitude is a strong driver of swiss landscape types are 2011 we did not derive any additional terrain attributes from the dtm such as slope aspect or roughness as cnns are capable of learning such attributes directly during the training of the network heo et al 2020 kirkwood 2020 silburt et al 2019 the filters with which such traditional attributes e g slope are calculated are conceptually very similar to the convolutional filters used in cnns however whereas the weights in the filters of the traditional attributes are fixed the weights in cnn filters are tailored to provide maximal discriminative or explanatory power kirkwood 2020 in this way cnns have learnt terrain attributes from dtms that were useful variables for the detection of craters silburt et al 2019 for explaining concentrations of elements in soils kirkwood 2020 or for the prediction of potential solar energy heo et al 2020 the fourth layer depicted the population density in individual buildings table 1 and was selected to provide information on human population density building configuration and building type e g single family house multi family high rise building this layer was created by disaggregating a 100 100 m population density layer swiss federal statistics office geostat 2018 to individual buildings in the swiss topographic landscape model swisstlm3d swiss federal office of topography 2019 table 1 as the distribution of the resulting raster of population density per building was right skewed we log transformed the data the fifth and sixth layers consisted of the proportion of deciduous and coniferous trees in 10 10 m pixels respectively waser et al 2017 waser et al unpublished results table 1 the composition and configuration of trees in the landscape does not only have an important ecological function but is also a key aesthetical component of a landscape all layers were transformed to match the origin resolution 10 10 m and extent of the sentinel 2 mosaics we then divided the switzerland wide layers into non overlapping images of 64 64 pixels table 1 each image thus represents an area of 640 640 m which is comparable to the spatial resolution used in many other landscape classifications simensen et al 2018 only images without no data values were maintained each image stack of six layers thus consisted of 64 64 x 6 24576 values finally we obtained 99076 image stacks to cover the whole of switzerland these image stacks were used to train the dcec network 2 2 deep convolutional embedded clustering dcec in recent years several studies have demonstrated the effectiveness of neural networks to simultaneously perform dimensionality reduction and unsupervised clustering e g xie et al 2016 yang et al 2017 for the clustering of images novel methods employing cnn autoencoders are especially promising guo et al 2017 li et al 2018 one of the latter methods is dcec proposed by guo et al 2017 as these authors found that dcec had a higher clustering accuracy than other techniques we chose dcec to generate a landscape classification for switzerland the architecture of our dcec network is similar to the architecture proposed by guo et al 2017 and is schematically depicted in fig 1 with dcec input images are simultaneously reconstructed and clustered the reconstruction is achieved with an autoencoder whereas the clustering is done by passing the hidden layer through a clustering layer in the encoder part of the dcec autoencoder input images are passed through three convolutional layers conv1 conv2 and conv3 fig 1 in which feature maps sequentially become smaller but more numerous the size of the convolutional filters is 5 5 pixels for conv1 and conv2 and 3 3 pixels for conv3 by applying convolutional filters with a stride of 2 pooling filters are avoided which increases the capacity to transform features guo et al 2017 the output of the third convolutional layer is unfolded into a one dimensional string of values which is then fully connected to a hidden layer of a certain length fig 1 the hidden layer is then passed on to the decoder which contains the same processing steps as the encoder albeit in the opposite order the output of the decoder are the reconstructed images the hidden layer contains a low dimensional code of the input images which enables the clustering layer to cluster the input images into a predefined number of classes fig 1 the objective of training cnns is to minimise the difference i e loss between the output and a reference distribution in the dcec network these losses are the reconstruction loss and the clustering loss which are minimised simultaneously guo et al 2017 the reconstruction loss is the mean squared error between the input images and the reconstructed images the clustering loss is the kullback leibler divergence between the output of the clustering and a target distribution guo et al 2017 xie et al 2016 as we are using unsupervised learning there is no predefined target distribution of classes therefore dcec uses an innovative approach to generate new target distributions from those images that are assigned a high confidence of belonging to a certain class xie et al 2016 every couple of training iterations i e epochs the target distribution is updated by upweighting the high confidence assignments and downweighting the low confidence assignments this way the cluster purity is improved during training of the network xie et al 2016 at the beginning of training the initial target distribution is set by k means clustering of the hidden layer whereas xie et al 2016 and guo et al 2017 normalise the target distribution with cluster size to prevent large clusters from emerging we did not perform this normalisation as there is probably a strong variation in the prevalence of landscape classes as optimizer we used the amsgrad variation of the adam optimizer reddi et al 2018 for more detailed information on dcec we refer xie et al 2016 and guo et al 2017 2 3 pre training and training the dcec network the first step in training the dcec network was to pre train the autoencoder so that we could determine the optimal length of the hidden layer and speed up subsequent calculations for the pre training the set of input images was divided into a training 85 a validation 12 and a test set 3 the test set was not used during the pre training of the dcec network but only after the pre training to determine the reconstruction loss of the final model we tested hidden layer lengths of 500 1000 1500 2000 2500 and 3000 values the learning rate was initially set to 0 001 and gradually decreased the pre training was terminated when the learning rate reached 0 000001 and there had been no improvement of the validation reconstruction loss for six consecutive epochs to select an optimal hidden layer length we determined the final reconstruction loss on the training validation and test sets after the pre training the complete dcec network was trained on the complete set of input images guo et al 2017 as it is not possible to perform an independent validation of the clusters with unlabelled images as with many unsupervised clustering methods used to generate landscape typologies the number of clusters had to be set a priori sietz et al 2019 therefore we trained dcec networks with 10 15 20 25 30 35 40 45 50 55 60 and 65 clusters to determine the optimal number of clusters for our dataset for every number of clusters we trained the dcec network four times and we determined the quality of clustering by calculating three well known cluster evaluation metrics ünlü and xanthopoulos 2019 the davies bouldin index davies and bouldin 1979 the silhouette index rousseeuw 1987 and the calinski harabasz index caliński and harabasz 1974 these indices were calculated from the hidden layer code of all input images furthermore to get an impression of the learning process at several points during the training we performed a principle component analysis pca and calculated the aforementioned metrics from the hidden layer in guo et al 2017 the training is terminated once the change in target distribution between two consecutive updates is less than 0 001 i e 0 1 of the images are assigned to a different class however to increase comparability between different runs we stopped training after the first update after 1600 epochs or 1800 epochs for those runs with 50 clusters with the final trained dcec network we predicted the landscape typology of all 99076 input image stacks so that we obtained a switzerland wide coverage all calculations were performed in python 3 6 python software foundations 2020 and with keras chollet 2015 with tensorflow abadi et al 2015 backend pre processing of the input images was performing in arcgis pro environmental systems research institute redlands usa the networks were trained on a desktop pc with an nvidia geforce gtx 1080 gpu and 64 gb of ram our code is adapted from the code of guo et al 2017 and available on https github com mvszuri landscape typ dcec 2 4 exploration of the dcec landscape typology we performed two additional analyses on the dcec results to get a better understanding of the detected classes first we aimed to characterise each identified class based on a range of indicators mean population density mean slope mean altitude and the mean proportions of coniferous and deciduous forest we depicted these values in radarplots for each class in the second analysis we plotted ten randomly selected images from each dcec class enabling a visual analysis of the within class similarities and between class differences the above post processing steps were performed in python 3 6 software foundations 2020 and in r 4 0 2 r development core teams 2020 2 5 comparison dcec classes with swiss landscape typology we compared the dcec landscape classification to the existing swiss landscape typology from the federal office for spatial development are henceforth referred to as the are typology are 2011 to do this we calculated a contingency table which contained the count of the number of co occurring raster cells between classes from either classification to determine whether the two classifications were associated with one another we performed a fischer s test on the contingency table with 10000 replicates in the monte carlo test fisher 1922 as the order of the classes in the dcec clustering is random the highest co occurrences might be scattered all throughout the table which could hamper the visual assessment of the relationship between the two typologies therefore we reorder the rows and columns in the contingency table by performing a detrended correspondence analysis dca hill and gauch 1980 on the contingency table we then reordered the rows and columns based on the loadings on the first and second dca axes 2 6 comparison to traditional landscape clustering methods to determine whether the dcec approach was able to learn features that resulted in a better clustering quality than existing landscape clustering methods we compared the dcec clustering to the following frequently used clustering methods k means clustering rocha et al 2020 van eetvelde and antrop 2009b self organizing maps dittrich et al 2017 levers et al 2018 rocha et al 2020 václavík et al 2013 van der zanden et al 2016 and principle component analysis combined with hierarchical clustering pecher et al 2013 as mentioned in the introduction these methods are usually applied to data that has been aggregated in fairly large raster cells e g 1 km2 however in our approach the input data for each landscape unit consisted of disaggregated data of 24576 values see section 2 1 to allow comparison with the dcec approach we also used this input data for the above clustering methods only for the hierarchical clustering instead of using the raw values as input we calculated the first 50 principle components of the images and used the scores of each observation as input for each method we calculated an equal number of clusters as the final number of dcec clusters see section 2 3 to compare the clustering quality of these traditional methods with the dcec approach we computed the davies bouldin index for each of the clustering results davies and bouldin 1979 these analysis were performed with the python packages scikit learn pedregosa et al 2011 and sklearn som smith and montali 2021 3 results 3 1 pre training of dcec autoencoder during pre training of the autoencoder we found that the network with a hidden layer length of 1500 had the lowest test 0 00225 and validation loss 0 00222 fig 2 this length was thus used in all subsequent analyses the dimension of the hidden layer was thus 6 10 of the dimension of the input images i e 24576 values despite this considerable dimensionality reduction the quality of the reconstructed images is good fig 3 there are some reductions in details compared to the original images but the general patterns are still clearly visible only for the images of the population density in buildings the reconstructed images show a lower similarity to the original images fig 3 3 2 dcec training we trained four dcec networks for each of the tested number of clusters i e 10 15 20 and 65 classes the mean proportion of change in class allocations in the last three updates of all performed runs was 0 0012 sd 0 0010 thus on average only 0 12 of the input images i e approx 128 images were allocated to another class after the final updates indicating good convergence of the training of the networks i e this value is very similar to that used in guo et al 2017 during the training of the networks we found that the davies bouldin db index of the hidden layer decreased and the proportion of explained variance by the first two principle components pc increased fig 4 indicating an improvement in clustering quality a visual inspection of the clusters also showed that the cluster purity improves during training fig 4 the average training time for one dcec network was 21 6 h comparing the different numbers of clusters we found that there was no apparent improvement of the mean db index of the hidden layer beyond 45 classes fig 5 here we only present the results of the db index as the silhouette and calinski harabasz indices showed very similar results interestingly 45 detected classes is quite similar to the 39 landscape classes in the existing are landscape typology are 2011 for any number of classes we observed quite some variation in db index between separate training runs fig 5 to assess whether even lower db index values could be obtained we also trained networks with 41 42 43 44 46 47 48 and 49 classes but none of them produced better results than the network with 45 classes results not shown from the dcec networks with 45 classes we used the network with the lowest db score 0 6172 to predict the landscape classes for this network the average reconstruction loss of the last 10 epochs was 0 00304 whereas after pre training the reconstruction loss of the training set was 0 00214 fig 2 this indicates that there is a trade off between minimising the reconstruction loss and minimising the clustering loss and during training some of the reconstruction loss was sacrificed in favour of a low clustering loss 3 3 exploration of the dcec classes a map showing all dcec classes in switzerland is included in the supplementary material fig a1 the maps of the individual landscape classes combined with radarplots of the mean landscape indicators showed that some of the detected landscape classes only occurred in a certain biogeographical region fig 6 for example classes 13 5 7 and 31 mainly occurred in the inner alps whereby these classes seem to differ mainly in mean elevation and slope fig 6 in contrast other classes occurred across several regions and showed a much more scattered distribution such as classed 34 or 42 fig 6 in the swiss plateau and other populated areas in switzerland fig 6 there seems to be several landscape classes representing a gradient of population density 17 16 and 11 other landscape classes such as 30 and 39 appear very similar with respect to the mean values of the five indicators radarplots in fig 6 but have differing spatial distributions maps in fig 6 the example landscapes from each of the identified dcec classes show within class variation but also clear between class gradients ranging from alpine landscapes to forested landscapes to agricultural and more populated landscapes fig a2 it is important to remember that these example images do not represent all the layers that were input to the dcec and therefore that some of the images with a seemingly high within class heterogeneity may actually be quite homogeneous in the other input layers 3 4 comparison dcec classes with swiss landscape typology we found that there was a highly significant association between the existing are typology are 2011 and our dcec landscape clustering fisher s exact test p 0 0001 the dca reordered contingency table also showed a pattern of association between the two classifications fig 7 some of the are classes contain only few dcec classes such as vineyard landscape settlement landscape or valley landscape of the inner alps fig 7 also some of the dcec classes are quite specific to a certain are class such as class 13 which only overlaps with the high mountain landscapes of the alps or class 10 and 27 which mainly overlap with the lakes class fig 7 however most are classes contain multiple dcec classes and vice versa fig 7 nevertheless the combination and frequency of dcec classes that overlap with a certain are class is unique for each are class 3 5 comparison to traditional landscape clustering methods a visual comparison of the dcec clustering quality fig 4 to that of three traditional landscape clustering methods k means clustering self organizing maps and principle component analysis with hierarchical clustering fig 8 showed that the cluster purity for the dcec method was much higher than for the three traditional methods this was also apparent in the higher db scores for the traditional methods db 2 905 fig 8 than for the dcec clustering db 0 616 fig 4 4 discussion in this study we have used deep convolutional embedded clustering dcec guo et al 2017 to generate a landscape typology for switzerland from remote sensing images and other continuous spatial data although several methods for unsupervised clustering using cnns have recently been developed e g guo et al 2017 ji et al 2019 yang et al 2017 to our knowledge the present study is the first application of such methods for the clustering of landscapes we have shown that dcec allows us to employ the unprecedented abilities of state of the art deep learning techniques for the unsupervised clustering of continuous spatial data the good quality of the reproduced images indicates that the dcec autoencoder successfully learned features and performed dimensionality reduction of the input images during training of the dcec network the low dimensional representation of the input images was not only adapted to reproduce the images as well as possible but also to provide well distinguishable clusters of images the low davies bouldin index values and the visual representation of the clusters in principle component space fig 4 indicated that the algorithm also succeeded in the latter task some of the detected landscape classes were clearly distinguishable from one another with respect to their geographic location maps in fig 6 thematic signature radarplots in fig 6 or visual appearance images in fig a2 whereas others were much harder to distinguish this indicates that the dcec network learned distinguishing landscape features from the remote sensing ecological demographic and terrain layers for instance although slope was not an input layer table 1 it was found to be a distinguishing characteristic between some of the landscape classes e g class 7 and 31 in fig 6 this could imply that the dcec algorithm leaned to calculate slope from the dtm to distinguish between landscapes with the dcec approach it is thus not only possible to identify landscape classes with a high cluster purity but also to detect new landscape features that were previously unknown or not apparent we found that the clustering quality of the dcec approach is considerably higher than that of traditional landscape clustering methods antrop and van eetvelde 2017 rocha et al 2020 sietz et al 2019 simensen et al 2018 figs 4 and 8 this difference in performance could be because these traditional methods are not ideally suited to cluster very high dimensional data assent 2012 but also because the functioning of the presented dcec approach is fundamentally different to that of the traditional methods with the traditional methods individual raster cells are assigned to clusters therefore prior to the clustering all spatial data has to be aggregated to the individual raster cells as these cells are typically 1 1 km simensen et al 2018 aggregation can lead to a considerable loss of information to quantify landscape patterns this aggregation is usually performed with landscape metrics however landscape metrics rely on potentially biased land use or land cover maps mcgarigal 2014 and do not cover the full breadth of potential landscape patterns kupfer 2012 mcgarigal and cushman 2005 furthermore as all the input data is fixed prior to the clustering the existing methods are not able to learn new landscape patterns that differentiate well between clusters with the dcec method these potential drawbacks of these traditional methods are overcome the input of the dcec method can be continuous or classified raster layers and no a priori aggregation or calculation of landscape metrics is required instead the algorithm learns to identify landscape features that are useful to assign a landscape image to a certain class by using a large number of convolutional filters that are learned during the training of the networks the number of patterns that can be identified with dcec far exceeds that of traditional landscape pattern metrics we conclude that the dcec approach adds a powerful new method to the suite of approaches for the unsupervised generation of landscape typologies antrop and van eetvelde 2017 rocha et al 2020 sietz et al 2019 simensen et al 2018 we found that there was a significant spatial association between our dcec landscape clustering and the existing are typology are 2011 which suggests that our automated dcec approach is able to generate meaningful landscape classes nevertheless only very few dcec classes were exclusively associated with a single are class or vice versa this latter result is not surprising as our aim was not to reproduce the are typology on the one hand the difference between the typologies could be explained by the fact that certain factors considered in the are typology such as precipitation levels and agricultural land use fig 7 are 2011 were not explicitly considered in our study on the other hand the difference may be resulting from a mismatch in the spatial scales of both typologies the goal of the are typology was to define landscape classes so that the patches of a certain class were minimally 10 km2 with some exceptions possible which resulted in patches that were dominated by a certain landscape class but still contained pockets of other classes are 2011 in our dcec typology the minimal size of a landscape unit was 0 41 km2 i e the input images were 640 640 m and each image was classified independent of its neighbouring cells to deal with this mismatch in scales a possible further analysis could be to perform a second larger scale clustering analysis on the landscape class map from the current dcec classification fig a1 the fact that we found unique combinations of dcec classes in each are class suggests that certain compositions and configurations of dcec classes could be specific for an are class such a two scale analysis was performed by van eetvelde and antrop 2009b who first performed a k means clustering on several raster layers to define fine scale landscape classes these classes were then further aggregated into courser scale units by manually delineating areas with unique combinations of classes in principle the dcec approach could be applied at both scales to get large scale landscape units which are potentially more comparable to the are typology despite the excellent performance of cnns in image recognition and classification tasks rawat and wang 2017 they are often referred to as a black box as their internal logic is unknown and it is a challenge to get an idea of the learned features heinrich et al 2019 although we compared landscape classes based on their spatial distribution and thematic signatures with our post processing steps it also remained unclear what specific landscape features distinguished the different classes from each other several advances in the analysis of cnns could provide better insights in their internal logic for example zeiler and fergus 2014 present an approach to depict the feature activities in convolutional layers at the same resolution as the input images with this approach these authors showed that their cnn learned understandable features and they observed desirable properties such as compositionality increasing invariance and class discrimination as they ascend the layers zeiler and fergus 2014 p 831 in van molle et al 2018 a similar technique was used to depict where the activation of a certain feature was highest in the original image a current overview of such visualisation techniques is given in heinrich et al 2019 implementing such techniques would enable determining what features were learned and where these features are located on the input images this would not only aid in identifying problems in the cnns architecture zeiler and fergus 2014 but also help determine characteristic landscape configurations that were not considered before although the practical implementation of some of the current visualisation techniques can be a challenge heinrich et al 2019 their fast development will probably make them more widely applicable in the near future in our study we used six medium resolution switzerland wide raster layers to characterise the landscape table 1 although these layers comprised important social ecological and topographic characteristics of the landscape they may not represent all factors that characterise a landscape in a comprehensive analysis groom 2005 p 38 identified four dimensions of landscape characteristics the biophysical dimension relating to the form and functioning of the landscape the socio economic technical dimension quantifying the human influence on the landscape form the human aesthetic dimension addressing the human experience of the landscape and the policy dimension relating to opinions and rights of stakeholders for each dimension groom 2005 gives multiple examples of spatial datasets implementing all these datasets would require a considerable expansion of the number of raster layers technically there are no limitations to the number of input layers that dcec can handle however from a practical perspective it could become complicated to collect such amounts of data at a standardised resolution for example it is probably difficult to assemble detailed historical spatial datasets which are necessary to assess the historical development of a landscape i e an important factor in landscape characterisation groom 2005 one technical challenge that does remain is how to deal with zero inflated data which may be prevalent in spatial layers in our demonstration of dcec we observed a relatively low reconstruction success of the layer quantifying the population density in buildings fig 3 this can potentially be explained by the fact that the values in this layer were zero inflated as the vast majority of land area has no buildings and thus a population density of zero fortunately techniques are being developed to deal with zero inflated input data in neural networks e g diaz and joseph 2019 several other developments in machine and deep learning could be interesting alternatives or additions to dcec one of these popular novel approaches is geometric or graph deep learning which attempts to use graph theory to apply deep learning to non euclidean geometric data bronstein et al 2017 as mentioned above a drawback of our approach is that we did not consider neighbouring images when classifying the landscape in an image geometric deep learning could potentially be used to capture the spatial arrangement of landscape classes for example ma et al 2019 successfully used graphs expressing the correlations between regions to guide the segmentation of synthetic aperture radar images another interesting recent development are self organizing maps combined with convolutional layers elend and kramer 2020 similar to our approach these authors used convolutional layers to learn features from the input images which were subsequently used to improve the clustering of these images with self organizing maps given the frequent use of self organizing maps to generate landscape typologies dittrich et al 2017 levers et al 2018 rocha et al 2020 václavík et al 2013 van der zanden et al 2016 this new approach could potentially be implemented with relative ease furthermore although it may seem challenging to select an appropriate method to generate landscape typologies it may actually be an advantage to perform clusterings with several models and combine their output a process called ensemble modelling jurek et al 2014 ensemble modelling can be applied to the output of different clustering methods to increase the clustering accuracy but can also be used to average the variability in outputs of repeated runs of a single method for example as we found quite some variation in the quality of the clustering of repeated trainings of our dcec network fig 5 combining these outputs could be an approach to generate robust results computer generated landscape typologies can either be a final product or an intermediate product to support a decision making process or expert based landscape classification for example the landscape typology in the swiss canton of schwyz was established in a participatory stakeholder process which was also supported by outputs of unsupervised clustering methods rodewald et al 2019 the computer generated typologies can lead to new ideas and realisations among the involved parties the presented dcec methodology is an improvement to other methods antrop and van eetvelde 2017 rocha et al 2020 sietz et al 2019 simensen et al 2018 especially because it utilises the excellent abilities of cnns to detect patterns in images this could lead to the identification of recurrent landscape patterns that are not directly apparent to the human eye but could present an important distinguishing feature in landscapes the fact that we did not find a perfect match between the are and dcec landscape classes see above could actually be an opportunity to consider other landscape patterns and characteristics and fine tune the swiss landscape typology to our knowledge the current study is the first study to use deep learning to generate unsupervised landscape typologies directly from continuous spatial data we are confident that with further improvements of the network s architecture and interpretability the use of such methods will open many new possibilities in landscape and land system research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank four anonymous reviewers for their constructive comments on earlier versions of this manuscript this research was supported by the european union s horizon 2020 research and innovation programme ecopotential project grant agreement no 641762 and by the swiss national science foundation emphases project grant number 200021 192018 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105462 
