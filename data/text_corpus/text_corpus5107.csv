index,text
25535,water level prediction is an essential task in inland water transportation and infrastructure operation in recent years the level of uncertainty in the water level variation has increased significantly due to the climate change therefore the need to develop more robust and accurate models for multi station daily water level prediction along the long and volatile inland rivers has greatly increased this research proposes a two stage modelling method to enhance the accuracy and efficiency in simultaneous prediction of daily water levels for multiple stations in inland rivers furthermore taking the yangtze river trunk line as case study the daily water data of 19 stations are collected and utilised to verify the performance of the models first we divide the 19 stations along the yangtze river trunk line into 6 clusters by dynamic time warping dtw and hierarchical clustering algorithm hca then the long short term memory lstm network and seasonal autoregressive integrated moving average sarima model are tailored to construct a multi station daily water level prediction msdwlp model for each cluster finally to validate the proposed method the daily water level data of 912 consecutive days from the 19 stations are employed the results demonstrate that the proposed approach can yield more reliable forecasts than traditional deterministic models insight from the models can be used to predict daily water levels to better inform decision making about waterborne transportation water resources management and water emergency response graphical abstract this research proposes a two stage modelling method to enhance the accuracy and efficiency in simultaneous prediction of daily water levels for multiple stations in inland rivers in the first stage multiple stations were clustered into some clusters according to the similar characteristics of daily water level which reduced the influence of elevation and waterway topographic changes on daily water level change thereby reducing the complexity of simultaneous multiple stations daily water level prediction in the second stage a prediction model was constructed for each cluster stations according to the periodic characteristics of daily water level which reduced the number of prediction models and the hyperparameters that need to be determined thereby improving the usability and accuracy of the proposed method the results of the case study of daily water level prediction at 19 stations along the yangtze river trunk line show that the method proposed in this paper can yield more reliable and efficient forecasting image 1 keywords water level prediction multi station clustering deep learning yangtze river 1 introduction water level prediction related to rivers and coastal waters is highly valuable for waterborne transportation water resources management flood mitigation and water emergency response kasiviswanathan et al 2016 wang and zhang 2018 gabela and sarmiento 2020 liu et al 2021 in particular high and low water levels are a major concern to the safe and efficient operation of inland shipping therefore accurate and efficient water level modelling and prediction is essential for inland shipping and infrastructure operation coraddu et al 2017 xu et al 2017 li et al 2020a yuan et al 2021a as shown in fig 1 water levels prediction plays an important supporting role in ship speed control trajectory prediction fuel consumption analysis cargo loading navigation planning hydroelectric power agricultural irrigation and flood mitigation which has become an important research topic in the fields of ship operation navigation planning and infrastructure operation accurate prediction of stations daily water level along the long and volatile inland rivers has become a vital challenge since it is affected by many complex factors such as climate waterway topography and periodic characteristics yuan et al 2021b by analyzing historical data of daily water level effective model can be established to predict the future daily water level quilty and adamowski 2020 yaseen et al 2020 zhu et al 2020 ehteram et al 2021 in some existing literature regression analysis methods and base models such as method of lines auto regressive etc have been widely used in water level correlation analysis and prediction for instance wei 2015 introduced the locally weighted regression and the k nearest neighbor models and developed a methodology for formulating water level models to forecast river stages during typhoons paul et al 2018 adapted the method of lines in addition with a newly embedded rkarms 4 4 rkam 4 4 runge kutta arithmetic mean and rkrms 4 4 runge kutta root mean square technique for numerical prediction of water levels considering the effect of tide and surge related to a cyclone ebtehaja et al 2019 presented a novel linear based model for lake level time series forecasting and evaluated the performance of the methodology using two case studies of the van lake in turkey and the michigan huron lake in north america chen et al 2020 developed a hybrid model combining the auto regressive ar analysis and the non stationary tidal harmonic analysis to improve short term with time scale of days water level predictions in the tide affected estuaries the above models achieved good prediction results under certain circumstances however the water levels at inland river stations are all characterized by strong nonlinearities which are difficult to be captured by current models on the other hand as a new technology that utilizes computers as a tool to simulate human learning abilities machine learning can perform structural division knowledge discovery feature extraction information mining and predictive analysis on existing content it mainly includes the following algorithms supervised learning cluster analysis intelligent decision making deep learning reinforcement learning etc machine learning techniques can accurately capture complex relationships and make predictions jordan and mitchell 2015 and have been recently developed and implemented to water level prediction research zhong et al 2017 established a hybrid ann artificial neural network kalman filtering model for forecasting the water level of wuhan station which locates at the middle section of the yangtze river sahoo et al 2019a analysed the suitability of support vector regression for modelling monthly low flows time series for three stations in mahanadi river basin india zhu et al 2020 used the feed forward neural network and deep learning technique to predict monthly lake water level yang et al 2020 proposed an edge computing based sensory network for water level monitoring and prediction li et al 2020b proposed to use the kernel extreme learning machine algorithm to achieve the forecasting of xiangjiang river and yuanjiang river water level zhou et al 2020 employed deep learning technique and multilayer perceptron to perform forecast of nanjing navigable river s water level fluctuation liu et al 2021 proposed a hybrid bayesian vine copula model for daily and monthly water level prediction lstm hochreiter and schmidhuber 1997 is an efficient neural network that is suitable for processing multi dimensional time series samples due to the superior learning and memory ability of the lstm network it has shown great advantages in time series modelling and predictive analysis zhang et al 2018 yuan et al 2020 adikari et al 2021 which can provide valuable reference for water level series prediction however the above existing works mainly focus on the water level of a few stations or a certain segment which can hardly be directly used for the simultaneous prediction of the daily water level of multiple station for long and volatile inland rivers such as the yangtze river trunk line which contains 19 stations as shown in fig 2 recently a new idea in the field of water levels prediction is a combination of linear models and non linear models moeeni and bonakdari 2017 combined the linear sarima model with the non linear ann model to develop a hybrid sarima ann model and used the model to improve the prediction accuracy of the monthly inflow to the jamishan dam reservoir in western iran subsequently by considering the different deterministic terms jump trend and period of monthly inflow time series moeeni et al 2017 proposed a hybrid method based on the combination of sarima and adaptive neuro fuzzy inference system anfis the prediction results showed a better performance of the proposed sarima anfis method in comparison with individual ones xu et al 2019 proposed a combined auto regressive integrated moving average recurrent neural network arima rnn model for water level prediction the method solved the problem that a single forecasting model can t take into account both linear and nonlinear components in data sahoo et al 2019b explored the suitability of the lstm rnn and artificial intelligence ai method for low flow time series forecasting the results showed that the lstm rnn model outperformed rnn model in hydrological time series prediction of the basantapur gauging station and it was concluded that lstm rnn can be used as new reliable ai technique for low flow forecasting phan and nguyen 2020 took advantage of linear and nonlinear models and proposed a hybrid approach combining statistical machine learning algorithms and arima for red river water level forecasting the effectiveness of the hybrid models has been verified through performance evaluation of the prediction water level in summary some research on water level prediction have been conducted in existing literature and some conclusions have been presented including 1 artificial intelligence methods generally have better performance than linear models in water level prediction due to its ability to capture the nonlinear relationships 2 the hybrid model combining linear and nonlinear methods can effectively improve the accuracy of water level prediction however inland rivers multi station water levels predictability still remains challenging due to the complex interaction between waterways conditions and river dynamics the current water level prediction models rarely consider both the spatial and temporal changes of the water area and it is difficult to be used for the daily water level prediction of multi station along inland rivers on the other hand during a complete transportation voyage in inland rivers it is necessary to grasp the changing trend of the daily water level of multiple stations in real time therefore to achieve simultaneous prediction of daily water level in multiple stations along inland rivers the following two problems need to be explored and solved 1 build and train a single model for each station which increases the calculation cost of multi station daily water level prediction 2 build and train a single model for all stations which reduces the accuracy of daily water level prediction for each station to address the above mentioned two problems this study makes two main contributions including 1 clustering stations with similar characteristics which reduces the calculation cost of modelling for multiple stations 2 tailor a single prediction model for each cluster stations which improve the accuracy of simultaneous prediction of daily water level in multiple stations to achieve this goal a two stage divide and conquer method for daily water level of multi station analysis and prediction is proposed and a real world case study of daily water level forecasting for 19 stations along the yangtze river trunk line is presented the hope is that the research framework modelling strategy and experimental design presented herein can be informing for other researchers or practitioners to explore simultaneous predictions of various hydrology and water resources indicators from multiple related stations the remaining of this paper is organised as follows in section 2 the collected daily water level data of 19 stations are presented and analysed in section 3 the modelling strategy and methods are described section 4 provides experimental details concerning the case study the detailed experiments and discussion about the water level modelling are presented in section 5 finally conclusions and perspectives are drawn in section 6 table 1 summarises the abbreviations used in the paper 2 study area and data 2 1 study area the yangtze river is the longest inland river in china and it is known as the golden waterway peng et al 2010 the annual freight volume of this river ranks first among the world s inland rivers notteboom et al 2020 reaching 2 69 billion tons in 2019 the yangtze river trunk line has become the key area of shipping and water research where the prediction of daily water level along the line is one of the key problems that need to be solved urgently at present the yangtze river is 6387 km long and it is the longest river in asia the main stream of the yangtze river traverses central china from west to east between 90 33 122 25 east longitude and 24 30 35 45 north latitude it flows through 11 provincial administrative regions including qinghai tibet sichuan yunnan chongqing hubei hunan jiangxi anhui jiangsu and shanghai and finally empties into the east china sea the trunk line of the yangtze river is characterized by winding and uneven terrain in some sections the mountains are high the valleys are deep and the current flows fast while in other sections the slopes are gentler and the currents are calmer therefore according to the characteristics of different channel sections the entire yangtze river is divided into three water areas upper reach with many canyons and small water flow middle reach with curved river and many tributaries and lower reach with wide channel and gentle current from the source to the yangtze river estuary the elevation a s l above sea level of the yangtze river mainstream gradually decreases as shown in fig 3 among them the section from yibin station to the estuary is the yangtze river trunk line which is the water area with the most navigable ships and the largest cargo throughput mainly including 19 stations as shown in fig 2 2 2 water level data in order to study the daily water level variation trend of the entire yangtze river trunk line this research collected the daily water level data from 19 stations including all stations where the yangtze river maritime safety administration announces the daily water level with data records including yibin luzhou chongqing cuntan fuling wanzhou maoping yichang zhijiang shashi jianli chenglingji hankou huangshi jiujiang anqing wuhu nanjing and zhenjiang as shown in fig 2 in this paper the 19 stations are indexed sequentially from station 1 to station 19 where station 1 is yibin and station 19 is zhenjiang the summary statistics for the 19 stations as shown in table 2 from the topographical point of view the 19 stations are distributed along three different parts of the yangtze river trunk line stations 1 8 located in the upper reach stations 9 13 located in the middle reach and stations 14 19 located in the lower reach it is worth noting that the terrain topography and water flows of each station in different water areas have different characteristics specifically in some waters of the upper reaches the terrain is high and steep and the elevation a s l is up to 3000 4000 m while the elevation a s l suddenly drops to 200 600 m at the edge of the sichuan basin such as yibin station the river enters the hilly area in the middle reach with densely covered beaches and branching water flows the valleys of certain river sections are several kilometres wide and the river surface is 155 500 m wide the lower reaches of the river have gentle water and wide river surface and are with low mountains and wide valleys the above mentioned different characteristics have a great impact on the variation of water level resulting in different value ranges for the daily water level data at different stations for example the water levels of stations 3 7 are all above 145 m because they are in the reservoir area of the three gorges reservoir area as shown in fig 2 with wusong zero level as the reference plane as shown in fig 4 a at other stations the water level measurement considers the local navigation datum level as the reference plane as shown in fig 4 b therefore the highest water level is less than 6 m in station 19 the water level data come from the daily records issued by the ministry of transport of the people s republic of china 1 1 http www mot gov cn shuiluchuxing changjiangshuiweigonggao which include the daily water level for two and a half years 912 days form january 1 2018 to june 30 2020 the daily water level data of each station are shown in fig 5 it can be seen from fig 5 that the water level data can be regarded as time series data because they present two obvious characteristics the data of water level changes with time and the data are interrelated therefore the problem of daily water levels prediction at 19 stations can be transformed into the problem of analysis and prediction for 19 different time series to improve the efficiency in analysis and prediction of multi station daily water level and improve the applicability of the proposed prediction method it is considered to build one model for several stations with similar water level sequence rather than build one model for each station 3 methodology the research framework is as shown in fig 6 first the daily water level data of 19 stations along the yangtze river trunk line are collected to avoid building separate models for all stations a strategy of divide and conquer is proposed the dtw algorithm is employed to measure the similarity of the water level data of multiple stations and a hca is utilised to group the stations with similar characteristics according to the similarity matrix then the msdwlp models are constructed based on the lstm network and the sarima method for different clusters to improve the prediction accuracy of each station the python library statsmodels lemenkova 2019 is employed to decompose the water level series training set into trend period and residual for the complex water levels with shorter periodicity 2 20 days because a voyage time of the ship in the yangtze river trunk line is about 20 days yuan et al 2021a lstm is used to approximate the trend and sarima is used to approximate the residual term sum of period and residual finally the daily water levels of the 19 stations in the next 7 days are predicted and analysed in detail using the testing data 3 1 modelling strategy in this paper a two stage modelling strategy is proposed as shown in fig 7 at the first stage the water level data from multiple stations are clustered based on similarity measurement using a hierarchical clustering algorithm stations with a similar trend in water level data would be integrated into a single prediction model at the second stage daily water level of each cluster will be firstly decomposed into three parts including a long term trend a period change trend and residual for the clusters whose data have a long periodicity such as yearly or longer or have no obvious periodicity the lstm network is directly used to process the water levels contrarily for the clusters with shorter periodicity the lstm sarima model is employed on the water levels in particular the long term trend is modelled using the lstm network and the residuals including period and residual is modelled with sarima the prediction results of each part are combined to obtain the integral water level prediction 3 2 clustering method first the similarity matrix between the water levels of the 19 stations needs to be calculated and obtained it is worth noting that the number of the water level data is the same for all stations i e the water level data of the 19 stations are 19 time series of equal length dynamic time warping dtw is a dynamic programming algorithm suitable for accurately calculating the similarity between multiple series dürrenmatt et al 2013 yu et al 2018 in addition dtw has no parameter restrictions and is robust to time in this work we employ the dtw algorithm to calculate the similarity distance matrix assuming that s 1 and s 2 are the water level sequences of station 1 and station 2 the similarity distance between s 1 and s 2 is calculated as follows d p i j 2 s 1 0 s 2 0 2 i 0 j 0 s 1 0 s 2 j 2 d p 0 j 1 i 0 s 1 i s 2 0 2 d p i 1 0 j 0 s 1 i s 2 j 2 min d p i 1 j d p j 1 i d p i 1 j 1 i j 0 where i and j are the indices of s 1 and s 2 d p i j is the similar distance between s 1 i and s 2 j then a clustering algorithm is utilised to analyze the similarity matrix to obtain the clustering results of 19 stations hierarchical clustering algorithm hca can create a hierarchical nested clustering tree based on similar distances between data points of different categories zhou et al 2017 it is a clustering technique that does not need to consider the selection of the number and positions of initial clusters hca is very intuitive i e clustering layer by layer its core is to start with individual clusters and merge the two closest clusters at each step the clustering algorithm used for grouping stations is designed based on the hierarchical clustering principle and is shown below algorithm 1 the clustering algorithm for 19 stations image 1 3 3 predictive modelling methods 3 3 1 water level time series decomposition in general time series prediction is to capture the variation characteristic and trend of the target series by learning the historical data so as to predict the data of the future time assuming that t t represents the long term trend item p t represents the item of periodic change trend and r t represents the random interference item then for a water level series there are three common decomposition models wang et al 2017 as follows 1 addition model s t t t p t r t 2 multiplication model s t t t p t r t 3 mixed model s t t t p t r t or s t t t p t r t in this work the addition model and seasonal decompose algorithm were employed for water level time series decomposition the seasonal decompose is an algorithm for time series decomposition using the moving average method which can be called in python module statsmodels 2 https www statsmodels org stable index 2 finally the water level of each station was decomposed into trend period and residual the specific decomposition steps are as follows step 1 decompose trend items by using the centralized moving mean method are shown in equations 1 and 2 1 t t s t f 1 2 s t f 1 2 1 s t f 1 2 f t i s o d d n u m b e r and t f 1 2 l f 1 2 2 t t 0 5 s t f 2 s t f 2 1 s t f 2 1 0 5 s t f 2 f t i s e v e n n u m b e r and t f 2 1 l f 2 where f is frequency of the series s t and l is the length step 2 subtract the trend term from the original water level as shown in equation 3 average the values at the same frequency in each period to obtain the periodic term as shown in equation 4 and further centralize to obtain the period term of the water level as shown in equation 5 3 s t s t t t 4 p t i 0 n s t i f f n max n n f l t 1 f 5 p t p t mean p t further the shorter period s of the p t is determined by the graphical observation method and the numerical calculation method i e p t s p t step 3 calculate the residuals r t s t t t p t 3 3 2 lstm the lstm network is one type of recurrent neural network rnn which was designed to tackle the problem of gradient dispersion existing in the conventional rnns elman 1990 the structure of rnn is shown in fig 8 where x is the sequence of input with length t h is the sequence of the hidden layer o is the output sequence l is the total loss and y is the sequence of target u is the parameter matrix from the input layer to the hidden layer w is a self looping parameter matrix in the hidden layer and v is the parameter matrix from the hidden layer to the output layer the biggest difference between rnn and traditional neural network is that the neurons also namely nodes between the hidden layers are connected so the rnn can memorize the previous information and apply it to the calculation of the current output moreover a self loop feedback is added between the hidden nodes through weight sharing which enables the network to process the data of indefinite length therefore the rnn has strong ability of learning and memory and can efficiently process the sample data of time series fang et al 2020 yuan et al 2020 the difference is that in addition to the common network layers such as input output and hidden the lstm network has a cell state at the top which is used to memorize store and transmit information the cell state can also be understood as an information conveyor belt which is actually the memory space of the lstm changing over time however the information conveyor belt itself cannot control what information is remembered and what is forgotten it is the three control gates that really control the memory information i e input gate forget gate and output gate therefore the core of the lstm network is the cell state and the control gates as shown in fig 9 t is time i g a t e t represents the input gate at time t which is a control gate from the previous long state information to the current long state information it is used to control how much new information is saved f g a t e t represents the forget gate at time t which is also a control gate from the previous long state information to the current long state information it is used to control how much history information is forgotten o g a t e t represents the output gate at time t which is a control gate from the current information to the output state information the summation of the long state information and the short state information is the current information c t is named cell state which memorises information and c t 1 is the cell memory at the time point t 1 h i d t is named hidden state which represents the output of the hidden node i n t represents the input sequence that can be a series with one dimension or multiple dimensions f is a sigmoid activation function and h is a hyperbolic tangent activation function the calculation of control gates and transmission states are shown in equations 6 10 gers et al 2000 6 i g a t e t f w i i n t u i h i d t 1 b i 7 f g a t e t f w f i n t u f h i d t 1 b f 8 o g a t e t f w o i n t u o h i d t 1 b o 9 c t f g a t e t c t 1 i g a t e t h w c i n t u c h i d t 1 b c 10 h i d t o g a t e t h c t where w i f o c are the weight matrices linking the input layer with the hidden layer b i f o c are bias terms and u i f o c are the self looping weight matrices of the hidden layer yuan et al 2021a the specific modelling steps for daily water level using the lstm network are designed as follows step 1 extracting the data of the daily water level of each station according to the station index step 2 normalising data scale the training sample data to the same range between 0 and 1 so that the network converges quickly and avoids predicting numerical errors step 3 generating time series in this step the time series are generated according to time time steps and batch size where time times represents the length of the input variable in the time dimension and batch size represents the size of the samples data for each processing the format of the time series is samples time steps n dim which the lstm network adapts to step 4 setting the parameters of the lstm network including the number of neurons the number of hidden layers epochs training iterations activation function training function and loss function step 5 training and optimising the initial lstm network until termination criteria are satisfied e g the training error is less than a set threshold step 6 predicting the future data in a single step or multiple steps using the trained lstm network it should be noted that the prediction results need to be denormalised step 7 results evaluation and analysis the prediction accuracy is analysed by calculating some performance measures 3 3 3 sarima the sarima model is an evolution model of seasonal or periodic data based on the arima model shumway and stoffer 2017 the arima model refers to the model established by transforming non stationary time series into a stationary time series and then regressing the lag value of the dependent variable phan and nguyen 2020 and the present value and lag value of the random error term including ar process moving average ma process auto regressive moving average arma process and arima process velasco and lazakis 2020 here the lag value represents the order of the lag operator which is involved in the difference operation that transforms a non stationary sequence into a stationary sequence the expression of the sarima model can be written as s a r i m a p d q p d q s where p p q and q represent the maximum lag order of non seasonal seasonal autoregressive and moving average respectively d and d represent the order of the differentiate and seasonal difference respectively s denotes the period of the seasonal time series it is worth noting that in the process of modelling and analysis using the sarima method the values of parameters p p d d q and q are not very large and d and d usually take 0 and 1 to meet the requirements the specific modelling steps for time series using the sarima model are designed as follows step 1 visual analysis of time series data the time series diagram of the data is drawn to visualise the trend of sequence changes over time step 2 test the stationarity of the time series in this approach the augmented dickey fuller adf test is used to test the stationarity of the series which is a common and effective method for testing sequence stationarity step 3 sequence stabilisation the stabilisation of the time series is to eliminate the trend effect and seasonal effect of the sequence and the differencing method is the most common method to achieve sequence stabilisation step 4 model order determination that is determining the parameters of the sarima model p p d d q and q this is a very critical step this paper uses the network search method to systematically select the optimal values of the parameters specifically the akaike information criterion aic is selected as the criterion to search for the best model parameters from the possible candidate set the aic not only improves the degree of model fitting but also introduces a penalty term to make the model parameters as few as possible which is helpful to reduce the possibility of overfitting step 5 building of the sarima model according to the optimal parameters determined in step 4 step 6 model testing verify if the residuals of the model are correlated and they are normally distributed with zero mean if not the model can be further improved step 7 data prediction use the constructed sarima model to predict the future data of the time series step 8 results evaluation and analysis 3 4 data partitioning in this study the daily water level data of 19 stations along the yangtze river trunk line were collected for 912 consecutive days from january 1 2018 to june 30 2020 forming the water level time series with 912 rows and 19 columns in our two stage modelling method the daily water level time series was first divided into two parts the 80 data from the first 2 years january 1 2018 to december 31 2019 was training data and the 20 data from the next half year january 1 2020 to june 30 2020 was test data in the proposed modelling methodology the training data is used for models training and validation and the test data is used for models prediction performance testing moreover in the process of hyperparameter tuning and model selection the training data was further divided into training set and validation set according to the ratio of 80 and 20 the variables presented to the prediction models are vectors composed of the consecutive days daily water level time series from the 19 stations specifically the daily water levels of training data are cyclically extracted in chronological order and constructed into vectors with length t i m e s t e p n d i m and presented to the basic lstm network where the first t i m e s t e p variables are the input feature and the last n d i m variables are the output feature in this work we adopt a cyclic rolling method to predict the multiple days daily water level that is using one step ahead prediction as part of the new input in addition to verify the prediction performance of the models the 7 step ahead prediction were conducted for all testing data specifically the daily water levels of all testing set were divided into 26 groups and presented to the trained models for statistical prediction errors where the length of each group of variables is t i m e s t e p and the water level prediction values of 26 groups for 7 consecutive days are obtained through loop prediction the frameworks of prediction models training and testing are shown in fig 10 and fig 11 where w l represents water level t s represents t i m e s t e p 3 5 performance measures to assess the accuracy and reliability of the proposed msdwlp models the prediction results are analysed by several performance measures ebtehaja et al 2019 including ae absolute error mae mean absolute error mape mean absolute percentage error rmse root mean square error and nse nash sutcliffe efficiency coefficient as shown in equations 11 15 11 a e z t z t ˆ 12 m a e 1 n t 1 n z t z t ˆ 13 m a p e 1 n t 1 n z t z t ˆ z t 100 14 r m s e 1 n t 1 n z t z t ˆ 2 15 n s e 1 t 1 n z t z t ˆ 2 t 1 n z t z t 2 where t represents the time index of a datum t 1 2 n n represents the length of the prediction sequence z t ˆ and z t are the predicted value and the measured value of the t th datum and z t is the mean of z t in addition to supplement these performance evaluation scatter plots and time series plots are also inserted for a graphical demonstration 4 experimental details in this section details concerning the experiment study are presented mainly including stations clustering model settings and experimental design the data analysis and modelling experiments were conducted using a desktop pc with intel core i7 7700 cpu and 16 gb ram its operating system was 64 bit windows 10 and the programming language employed was python 3 7 where a python ide integrated development environment spyder and the open source library keras sklearn and statsmodels were employed 4 1 stations clustering first of all the dtw based hierarchical clustering method was employed for the clustering of daily water level sequences from 19 stations for the daily water level of training set 730 consecutive days from the 19 stations which from january 1 2018 to december 31 2019 using the dtw and clustering algorithms introduced in section 3 2 the hierarchical clustering tree of the 19 stations obtained from the similarity distance matrix is shown in fig 12 a different clustering results can be obtained by moving a cross cut line up and down for example moving the cross cut line to the top as shown by the black line in the figure the 19 stations are divided into two rough clusters stations 3 4 5 6 and 7 as one cluster and the other stations as another cluster in fact stations of 3 4 5 6 and 7 are all located in the three gorges reservoir area which have much higher daily water levels 145 m than the other stations moreover the overall water levels at stations 3 and 4 are higher and more complex than that of stations 5 6 and 7 which can be seen from fig 5 therefore stations 3 4 5 6 and 7 are further divided into two clusters stations 3 and 4 as one cluster and stations 5 6 and 7 are with another cluster at this time the 19 stations were divided into 3 clusters moreover moving the cross cut line down to the position of the red line 6 clusters can be obtained fig 12 b shows 7 different clustering results for 19 stations including 2 3 4 5 6 7 and 9 clusters as can be seen from figs 12 and 19 stations can be clustered into at least 2 clusters and at most 19 clusters moving the cross cut line to the bottom of fig 12 a however for multi station daily water level prediction the fewer the clusters the less distinct the water level features of the stations are the more clusters the more expensive the modelling and computation for example with 9 clusters stations 5 6 and 7 are divided into two clusters however their water level ranges and trends did not change significantly as shown in fig 5 and one model could be constructed for all 3 stations as well as stations 8 9 and 10 therefore considering the performance and computational cost the 19 stations were clustered into 6 classes in the first stage of clustering in additional experiments it is also verified that 6 cluster is an ideal clustering results 4 2 model settings according to the proposed divide and conquer modelling strategy 4 types of multi station daily water level prediction msdwlp models are constructed and tested to verify the effectiveness of the proposed method which use lstm for the trend components and sarima for the residual component in our strategy different mswlp models were set up for comparison including msdwlp s msdwlp cs msdwlp ch1 and msdwlp ch2 which are described as follows msdwlp s denotes the msdwlp model that constructed by using lstm alone for all 19 stations including one daily water level prediction model msdwlp cs denotes the msdwlp models that constructed by using lstm alone for each cluster based on stations clustering results including 6 daily water level prediction models msdwlp ch1 denotes the msdwlp models that constructed by combining lstm and sarima for each cluster based on stations clustering results where the lstm is used for the trend components and sarima is used for the residual term components period and residual including 6 daily water level prediction models msdwlp ch2 denotes the msdwlp models that constructed by combining lstm and sarima for each cluster based on stations clustering results where the sarima is used for the trend components and lstm is used for the residual components period and residual including 6 daily water level prediction models in the next sub section the model parameters settings of the models are introduced 4 3 parameter settings the optimal parameters are crucial to the performance improvement of the model for a high performing lstm network there are multiple hyperparameters and activation functions that need to debugged and optimized yuan et al 2020 in the experiments the parameters and their candidate values are described as follows neurons the number of neurons nodes in the hidden layer of the lstm network time step the length of the input time series of the lstm network n dim the length of the output time series of the lstm network batch size the size of sample data which are presented the lstm network each time training epochs the number of iterations of the lstm network learning rate control the rapid convergence of the lstm network model to the optimum generally ranging from 0 001 to 0 1 activation functions the purpose of activation function is to provide the non linear modelling capabilities for the network the common activation functions are piecewise linear functions and non linear functions with exponential shapes such as relu a rectified linear unit function linear a linear activation function tanh a hyperbolic tangent function softsign similar to tanh but smoother sigmoid a common s type function loss functions the purpose of loss function is to evaluate the severity of the neural network performance and the network uses the loss function as the clue to find the optimal weight parameters the common loss functions include mae mean absolute error mse mean squared error mape mean absolute percentage error msle mean squared logarithmic error hinge and squared hinge optimization algorithms the purpose of optimization algorithms is to minimize or maximize the loss function by improving the training method to obtain optimal network parameters the common optimization algorithms include rmsprop root mean square propagation optimiser adam adaptive moment estimation adamax a variant of adam with infinity norm nadam nesterov accelerated adaptive moment estimation adagrad adaptive gradient algorithm adadelta extension of adagrad with smaller learning rate in addition to avoid overfitting of the lstm network the early stopping method is adopted the specific steps are as follows 1 divide the original training data set into training set and validation set 2 calculate the error of the validation set in each batch size period and stop training if the error increases 3 use the parameters from the previous iteration result as the final parameters of the lstm the parameters settings of the model are divided into different groups and a network search algorithm nsa is designed based on the minimum error criterion and aic to systematically select the optimal values of the parameters moreover in order to overcome the limitations of the data two and a half years in this paper parameter transfer learning is on the sample data that is the training data of all stations are extracted to conduct experiments on the hyperparameters tuning of the lstm network and then applied the determined parameters to the water level prediction model of each cluster stations moreover during the experiments the training data set is divided into training set and validation set to ensure the accuracy of parameter settings table 3 records the nse of the lstm network parameter setting experiments for the daily water level prediction of all stations where activation function optimiser function time step batch size and neurons number are determined in group 1 2 3 4 and 5 as shown in table 3 the following parameters settings are found to be appropriate the neurons were set to 150 the batch size was set to 36 the time step was set to be 6 n dim was set to 1 cyclic rolling method is used to predict the daily water level in future days i e use one step ahead prediction as part of the new input the activation function was the rectified linear unit function relu the loss function was set to mse and the optimization algorithm was the nesterov accelerated adaptive moment estimation optimiser nadam similarly s a r i m a p d q p d q s model has 7 key parameters that need to be set namely the maximum lag order of non seasonal p the maximum lag order of autoregressive q the maximum lag order of seasonal p the maximum lag order of moving average q the order of the differentiate d the order of seasonal difference d and the period of the time series s the best parameters p p d d q and q are also determined by a network search method which is based on a variant of hyndman khandakar algorithm hyndman and khandakar 2008 in addition the python library of statsmodels was employed for building and training the sarima models 5 results and discussion 5 1 the results of stations clustering firstly through the proposed algorithm 1 19 stations are clustered into 6 clusters with cluster 1 stations 1 and 2 cluster 2 stations 3 and 4 cluster 3 stations 5 6 and 7 cluster 4 stations 8 9 and 10 cluster 5 stations 11 12 13 14 15 and 16 and cluster 6 stations 17 18 and 19 fig 13 demonstrates the daily water levels of 6 clusters of 19 stations as shown in fig 13 based on the 6 cluster result the daily water level sequences within the same cluster show very similar characteristics in the data distribution and change trend 5 2 the results of daily water level prediction in 19 stations according to the results of parameter settings table 3 and the stations clustering 6 clusters fig 13 the configuration of msdwlp models was further optimized as shown in table 4 in table 4 acti represents activation function opti represents optimization algorithm the structural form of the lstm network is lstm time step batch size neurons training ecpochs and the structural form of the sarima model is sarima p d q p d q s for example the meaning of lstm 7 36 150 1000 sarima 2 1 1 3 1 0 7 is as follows for the lstm network the time step is 7 the batch size is 36 the neurons is 150 the training ecpochs is 1000 for the sarima model p 2 p 3 d 1 d 1 q 1 q 0 and s 7 table 5 records the daily water level prediction performance of different models for the testing set in which the parameters of each model are set according to table 4 from the values averaged over all stations of performance measures in table 5 the following conclusions can be drawn 1 for each lead time the models of msdwlp cs msdwlp ch1 and msdwlp ch2 which were constructed based on the clustering results of 6 clusters exhibited more accurate predictions in terms of the mae mape rmse and nse than msdwlp s therefore the multi station clustering was an important factor that resulted in a more accurate prediction for our case study 2 for each lead time the combined models msdwlp ch1 and msdwlp ch2 both provided low errors mae mape and rmse and high nse it can be seen that the combined models present improvements in the daily water level prediction accuracy 3 for each lead time the msdwlp ch1 model which is proposed in this paper further improved the performance of daily water level prediction for multi stations for example the mae and rmse on the 1st day were only 0 06 m and 0 07 m and the nse reached 0 999 on the 7th day they were 0 41 m 0 45 m and 0 946 respectively the results in table 5 provide support to the proposed two stage divide and conquer method msdwlp ch1 for multi station daily water level analysis and prediction to further verify this claim we select a special example the first 7 days of testing data which are from january 1 2020 to january 7 2020 and summarize the detailed prediction results of the stations in each cluster as follows 5 2 1 cluster 1 a lstm sarima model cluster 1 contains stations 1 and 2 it can be seen from fig 5 that the water level data of station 1 and station 2 are not changing smoothly but the daily water level appears to have a 7 days period firstly using the addition model the daily water level training set of station 1 is decomposed into trend and residual where the residual term contains the period of small amplitude and residual then the lstm neural network was tailored and implemented to build a model for the trend prediction and the sarima method was used for predictive modelling for the residual part the parameters settings of lstm sarima were from table 4 finally the trend predicted by lstm and the residual predicted by sarima were added to obtain the predicted daily water level the water level prediction results of stations 1 and 2 for certain 7 days january 1 2020 to january 7 2020 are shown in fig 14 a it can be seen that the prediction accuracy is very good for the future 7 days where the absolute error for the future 1st day is less than 0 07 m and the one for the future 2nd day is less than 0 13 m in station 1 moreover the absolute error are only 0 03 m and 0 04 m in station 2 5 2 2 cluster 2 a lstm sarima model cluster 2 includes stations 3 and 4 which are located in the three gorges reservoir area as with cluster 1 the water level after decomposition also has an obvious period 8 days in the same way the lstm network was used for forecasting the trend and sarima was used for forecasting the residual except for time step was set to 8 and s 8 all the parameters of lstm network and sarima model are consistent with those in cluster 1 the predicted results of the daily water level of the future 7 days january 1 2020 to january 7 2020 for stations 3 and 4 are shown in fig 14 b it can be seen that the prediction is very accurate for the first two days where the absolute error for the future 1st day is less than 0 14 m and the absolute error for the future 2nd day is less than 0 13 m 5 2 3 cluster 3 a lstm model cluster 3 contains three stations 5 6 and 7 which are also located in the three gorges reservoir area unlike clusters 1 and 2 after the water level was decomposed no short period was found however good water level predictions can still be obtained using only the lstm network the predicted results of certain future 7 days january 1 2020 to january 7 2020 for stations 5 6 and 7 are as shown in fig 14 c it is worth noting that the parameters are consistent with the lstm network for clusters 1 and 2 except for the time step being 5 although the absolute errors of stations 5 6 and 7 are larger than those of stations 1 2 3 and 4 their absolute percentage errors are very small because the daily water level of the three stations has always been above 145 m it can be found the prediction accuracy of the developed model is very good in forecasting the future several days especially for stations 6 and 7 5 2 4 cluster 4 a lstm model cluster 4 has three stations no 8 no 9 and no 10 similar to cluster 3 the decomposed water levels have no short period when the lstm network is used for the daily water level prediction of cluster 4 of stations the prediction results for the next 7 days january 1 2020 to january 7 2020 are shown in fig 14 d in the experiments all the parameters of the lstm network are the same with those in the previous clusters except that the time step is 10 it can be observed the developed model can well predict the water level for future several days especially for the future 1st and 2nd days 5 2 5 cluster 5 a lstm model cluster 5 includes six stations no 11 12 13 14 15 and 16 most of these six stations are located in the lower reach of the yangtze river trunk line although the water level data of the six stations have no obvious periodic characteristics they are all relatively smooth the lstm network was set to have the same parameters as those for previous clusters except for the time step being 8 the prediction results of the six stations in the future 7 days january 1 2020 to january 7 2020 are shown in fig 14 e from fig 14 e we can see that the absolute errors of the six stations in the future 1st and 2nd days are all less than 0 09 m more importantly the absolute errors of each station in the next 3 days are no more than 0 18 m and in the next 7 days are no more than 0 59 m 5 2 6 cluster 6 a lstm model cluster 6 contains stations 17 18 and 19 after the decomposition of the water level data an obvious period of 15 days was found similarly the lstm network for forecasting the trend it was set to the same parameters as those for previous clusters except for the time step being 15 and the sarima was used for forecasting the residual term including period and residual it settings were found as follows p 2 p 2 d 0 d 1 q 1 q 1 and s 15 the absolute errors of the six stations do not exceed 0 03 m in the future 1st day and 0 06 m in the future 2nd day the prediction results of the three stations of cluster 6 for the future 7 days january 1 2020 to january 7 2020 are shown in fig 14 f as a specific example the first 7 days of testing set from january 1 2020 to january 7 2020 the mae and mape of 19 stations for a certain prediction case are also tested and analysed in general as the number of days increases the prediction errors mae of each station also increase however in the stations of each cluster good water level prediction results have been achieved in addition to verify advantages of the proposed method it is compared with two models msdwlp s and msdwlp cs and the performances mean of 10 experiments of different models can be shown in table 6 compared with the proposed method both msdwlp s and msdwlp cs have relatively high prediction errors when msdwlp s is used to predict the case the mae and mape have further increased in all stations when msdwlp cs is employed the mae and mape have greatly increased in stations 1 4 and 17 19 when msdwlp ch2 is adopted the mae and mape in cluster 1 2 and 6 are still larger than msdwlp ch1 to sum up the method msdwlp ch1 proposed in this paper has better accuracy in 19 stations daily water level prediction more importantly fewer model parameters are required for multi station daily water level prediction in our method in that the clustering in the first stage reduces the number of objects that modelling in the second stage 5 3 discussion and insights the predictive performance nse of the developed msdwlp ch1 against all the training data ranging from january 1 2018 to december 31 2019 730 days and testing data ranging from january 1 2020 to june 30 2020 182 days is shown in fig 15 among them the testing data containing 182 days of daily water level is divided into 26 groups for prediction and analysis and the output of each group includes 7 days as can be seen from fig 15 the nse of all training and testing are over 0 97 in the future 3 days moreover the nse of all stations in the future 7 days are basically above 0 90 except for 0 887 and 0 863 in the future 6th and 7th days at station 2 and most of them exceed 0 95 fig 16 illustrates the prediction rmse and mape of 19 stations in all testing data as can be seen from fig 16 a at all stations the rmse becomes greater if the prediction time is longer this means that the models always perform the best in predicting the near future such as the future 1st day and become less accurate when predicting the far future such as the future 7th day the bigger rmses appear in stations 4 5 6 and 7 where the stations are located in the three gorges reservoir area and their daily water levels are above 145 m all year round the smaller rmses appear in stations 8 9 and 10 which belong to the fourth cluster except for the stations in cluster 2 and cluster 3 the rmses of other stations for the future 1st day are less than 0 12 m and the rmses for the future 3rd day are less than 0 20 m from fig 16 b it can be seen the smallest mape appears in stations 3 4 5 6 and 7 and the large mapes appear in stations 1 2 8 9 and 10 stations 1 and 2 belong to the first cluster and are located in the upper reach of the yangtze river stations 8 9 and 10 belong to the fourth cluster and are located in the middle reach in fact the stations of first cluster are the typical mountain river channels and the stations of fourth cluster are located in the three gorges reservoir area therefore the water regime in these two river sections is the most complicated the variation of mape in other stations is relatively small especially in the lower reach of the yangtze river such as stations 14 19 in general as the number of days increases the prediction errors of each station also increase the method proposed in this paper showed decent accuracy in 19 stations daily water level prediction and more importantly fewer model parameters are required in that the clustering in the first stage reduces the number of modelling objects in the second stage 6 conclusions in this paper the daily water level data of multiple stations on the yangtze river trunk line have been collected analysed and predicted in order to better inform decision making about safe and effective waterborne transportation water management and emergency response a new two stage divide and conquer modelling strategy has been proposed in the first stage the dtw algorithm is employed to calculate the similarity distance of water level series and the hierarchical clustering algorithm is used to divide 19 stations into 6 clusters according to the similarity matrix in the second stage the lstm network and sarima model are tailored and implemented to build the msdwlp models for each cluster in particular for the clusters with short periodicity and obvious seasonal change trend the daily water level is decomposed into long term trend periodic change trend and residual then the msdwlp ch1 model is employed and selected for better results in which the long term trend part is approximated by the lstm network and the residual term part is approximated by the sarima model in the validation experiments the daily water levels of 19 stations in the future 7 days are predicted the results show that the proposed analysis and modelling method can be well applied to the case of the yangtze river trunk line the rmse of the prediction is no more than 0 12 m for the future 1st day and is no more than 0 26 m for the future 3rd day the average mape across 19 stations is 2 03 for the future 1st day and is 6 91 for the future 7 days the water levels of inland rivers is affected by many complex factors such as elevation a s l waterway topography periodic characteristics and flood control and drought resistance strategies which make it difficult to elicit conventional predictive models in the proposed two stage method firstly 19 stations were clustered into 6 categories according to the similar characteristics of daily water level which reduced the influence of elevation a s l and waterway topographic changes on daily water level change thereby reducing the complexity of multiple stations daily water level prediction secondly a prediction model was constructed for each cluster stations according to the periodic characteristics of daily water level which reduced the number of prediction models and the parameters that need to be determined thereby improving the accuracy of daily water level prediction moreover the anticipation of the water level variation will support decision making in planning and operation of waterborne transportation water management and emergency response meanwhile a potential improvement in the following study is to employ more factors in the proposed method which is flexible to use other data as predictors if available such as precipitation the inland reginal and oceanic climate variables play a critical role in providing valuable information for the multi station river water level prediction additionally the performance of the proposed approach at smaller timescales for example hourly forecasting is worth exploring declaration of competing interest the authors declare that they have no know competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the independent innovation research fund of wuhan university of technology grant no 2022iva048 the national natural science foundation of china nsfc grant no 51709219 the national key research and development program of china grant no 2018yfc1407400 the qingdao research institute of wuhan university of technology grant no 2019a02 and the china scholarship council csc grant no 201906950086 
25535,water level prediction is an essential task in inland water transportation and infrastructure operation in recent years the level of uncertainty in the water level variation has increased significantly due to the climate change therefore the need to develop more robust and accurate models for multi station daily water level prediction along the long and volatile inland rivers has greatly increased this research proposes a two stage modelling method to enhance the accuracy and efficiency in simultaneous prediction of daily water levels for multiple stations in inland rivers furthermore taking the yangtze river trunk line as case study the daily water data of 19 stations are collected and utilised to verify the performance of the models first we divide the 19 stations along the yangtze river trunk line into 6 clusters by dynamic time warping dtw and hierarchical clustering algorithm hca then the long short term memory lstm network and seasonal autoregressive integrated moving average sarima model are tailored to construct a multi station daily water level prediction msdwlp model for each cluster finally to validate the proposed method the daily water level data of 912 consecutive days from the 19 stations are employed the results demonstrate that the proposed approach can yield more reliable forecasts than traditional deterministic models insight from the models can be used to predict daily water levels to better inform decision making about waterborne transportation water resources management and water emergency response graphical abstract this research proposes a two stage modelling method to enhance the accuracy and efficiency in simultaneous prediction of daily water levels for multiple stations in inland rivers in the first stage multiple stations were clustered into some clusters according to the similar characteristics of daily water level which reduced the influence of elevation and waterway topographic changes on daily water level change thereby reducing the complexity of simultaneous multiple stations daily water level prediction in the second stage a prediction model was constructed for each cluster stations according to the periodic characteristics of daily water level which reduced the number of prediction models and the hyperparameters that need to be determined thereby improving the usability and accuracy of the proposed method the results of the case study of daily water level prediction at 19 stations along the yangtze river trunk line show that the method proposed in this paper can yield more reliable and efficient forecasting image 1 keywords water level prediction multi station clustering deep learning yangtze river 1 introduction water level prediction related to rivers and coastal waters is highly valuable for waterborne transportation water resources management flood mitigation and water emergency response kasiviswanathan et al 2016 wang and zhang 2018 gabela and sarmiento 2020 liu et al 2021 in particular high and low water levels are a major concern to the safe and efficient operation of inland shipping therefore accurate and efficient water level modelling and prediction is essential for inland shipping and infrastructure operation coraddu et al 2017 xu et al 2017 li et al 2020a yuan et al 2021a as shown in fig 1 water levels prediction plays an important supporting role in ship speed control trajectory prediction fuel consumption analysis cargo loading navigation planning hydroelectric power agricultural irrigation and flood mitigation which has become an important research topic in the fields of ship operation navigation planning and infrastructure operation accurate prediction of stations daily water level along the long and volatile inland rivers has become a vital challenge since it is affected by many complex factors such as climate waterway topography and periodic characteristics yuan et al 2021b by analyzing historical data of daily water level effective model can be established to predict the future daily water level quilty and adamowski 2020 yaseen et al 2020 zhu et al 2020 ehteram et al 2021 in some existing literature regression analysis methods and base models such as method of lines auto regressive etc have been widely used in water level correlation analysis and prediction for instance wei 2015 introduced the locally weighted regression and the k nearest neighbor models and developed a methodology for formulating water level models to forecast river stages during typhoons paul et al 2018 adapted the method of lines in addition with a newly embedded rkarms 4 4 rkam 4 4 runge kutta arithmetic mean and rkrms 4 4 runge kutta root mean square technique for numerical prediction of water levels considering the effect of tide and surge related to a cyclone ebtehaja et al 2019 presented a novel linear based model for lake level time series forecasting and evaluated the performance of the methodology using two case studies of the van lake in turkey and the michigan huron lake in north america chen et al 2020 developed a hybrid model combining the auto regressive ar analysis and the non stationary tidal harmonic analysis to improve short term with time scale of days water level predictions in the tide affected estuaries the above models achieved good prediction results under certain circumstances however the water levels at inland river stations are all characterized by strong nonlinearities which are difficult to be captured by current models on the other hand as a new technology that utilizes computers as a tool to simulate human learning abilities machine learning can perform structural division knowledge discovery feature extraction information mining and predictive analysis on existing content it mainly includes the following algorithms supervised learning cluster analysis intelligent decision making deep learning reinforcement learning etc machine learning techniques can accurately capture complex relationships and make predictions jordan and mitchell 2015 and have been recently developed and implemented to water level prediction research zhong et al 2017 established a hybrid ann artificial neural network kalman filtering model for forecasting the water level of wuhan station which locates at the middle section of the yangtze river sahoo et al 2019a analysed the suitability of support vector regression for modelling monthly low flows time series for three stations in mahanadi river basin india zhu et al 2020 used the feed forward neural network and deep learning technique to predict monthly lake water level yang et al 2020 proposed an edge computing based sensory network for water level monitoring and prediction li et al 2020b proposed to use the kernel extreme learning machine algorithm to achieve the forecasting of xiangjiang river and yuanjiang river water level zhou et al 2020 employed deep learning technique and multilayer perceptron to perform forecast of nanjing navigable river s water level fluctuation liu et al 2021 proposed a hybrid bayesian vine copula model for daily and monthly water level prediction lstm hochreiter and schmidhuber 1997 is an efficient neural network that is suitable for processing multi dimensional time series samples due to the superior learning and memory ability of the lstm network it has shown great advantages in time series modelling and predictive analysis zhang et al 2018 yuan et al 2020 adikari et al 2021 which can provide valuable reference for water level series prediction however the above existing works mainly focus on the water level of a few stations or a certain segment which can hardly be directly used for the simultaneous prediction of the daily water level of multiple station for long and volatile inland rivers such as the yangtze river trunk line which contains 19 stations as shown in fig 2 recently a new idea in the field of water levels prediction is a combination of linear models and non linear models moeeni and bonakdari 2017 combined the linear sarima model with the non linear ann model to develop a hybrid sarima ann model and used the model to improve the prediction accuracy of the monthly inflow to the jamishan dam reservoir in western iran subsequently by considering the different deterministic terms jump trend and period of monthly inflow time series moeeni et al 2017 proposed a hybrid method based on the combination of sarima and adaptive neuro fuzzy inference system anfis the prediction results showed a better performance of the proposed sarima anfis method in comparison with individual ones xu et al 2019 proposed a combined auto regressive integrated moving average recurrent neural network arima rnn model for water level prediction the method solved the problem that a single forecasting model can t take into account both linear and nonlinear components in data sahoo et al 2019b explored the suitability of the lstm rnn and artificial intelligence ai method for low flow time series forecasting the results showed that the lstm rnn model outperformed rnn model in hydrological time series prediction of the basantapur gauging station and it was concluded that lstm rnn can be used as new reliable ai technique for low flow forecasting phan and nguyen 2020 took advantage of linear and nonlinear models and proposed a hybrid approach combining statistical machine learning algorithms and arima for red river water level forecasting the effectiveness of the hybrid models has been verified through performance evaluation of the prediction water level in summary some research on water level prediction have been conducted in existing literature and some conclusions have been presented including 1 artificial intelligence methods generally have better performance than linear models in water level prediction due to its ability to capture the nonlinear relationships 2 the hybrid model combining linear and nonlinear methods can effectively improve the accuracy of water level prediction however inland rivers multi station water levels predictability still remains challenging due to the complex interaction between waterways conditions and river dynamics the current water level prediction models rarely consider both the spatial and temporal changes of the water area and it is difficult to be used for the daily water level prediction of multi station along inland rivers on the other hand during a complete transportation voyage in inland rivers it is necessary to grasp the changing trend of the daily water level of multiple stations in real time therefore to achieve simultaneous prediction of daily water level in multiple stations along inland rivers the following two problems need to be explored and solved 1 build and train a single model for each station which increases the calculation cost of multi station daily water level prediction 2 build and train a single model for all stations which reduces the accuracy of daily water level prediction for each station to address the above mentioned two problems this study makes two main contributions including 1 clustering stations with similar characteristics which reduces the calculation cost of modelling for multiple stations 2 tailor a single prediction model for each cluster stations which improve the accuracy of simultaneous prediction of daily water level in multiple stations to achieve this goal a two stage divide and conquer method for daily water level of multi station analysis and prediction is proposed and a real world case study of daily water level forecasting for 19 stations along the yangtze river trunk line is presented the hope is that the research framework modelling strategy and experimental design presented herein can be informing for other researchers or practitioners to explore simultaneous predictions of various hydrology and water resources indicators from multiple related stations the remaining of this paper is organised as follows in section 2 the collected daily water level data of 19 stations are presented and analysed in section 3 the modelling strategy and methods are described section 4 provides experimental details concerning the case study the detailed experiments and discussion about the water level modelling are presented in section 5 finally conclusions and perspectives are drawn in section 6 table 1 summarises the abbreviations used in the paper 2 study area and data 2 1 study area the yangtze river is the longest inland river in china and it is known as the golden waterway peng et al 2010 the annual freight volume of this river ranks first among the world s inland rivers notteboom et al 2020 reaching 2 69 billion tons in 2019 the yangtze river trunk line has become the key area of shipping and water research where the prediction of daily water level along the line is one of the key problems that need to be solved urgently at present the yangtze river is 6387 km long and it is the longest river in asia the main stream of the yangtze river traverses central china from west to east between 90 33 122 25 east longitude and 24 30 35 45 north latitude it flows through 11 provincial administrative regions including qinghai tibet sichuan yunnan chongqing hubei hunan jiangxi anhui jiangsu and shanghai and finally empties into the east china sea the trunk line of the yangtze river is characterized by winding and uneven terrain in some sections the mountains are high the valleys are deep and the current flows fast while in other sections the slopes are gentler and the currents are calmer therefore according to the characteristics of different channel sections the entire yangtze river is divided into three water areas upper reach with many canyons and small water flow middle reach with curved river and many tributaries and lower reach with wide channel and gentle current from the source to the yangtze river estuary the elevation a s l above sea level of the yangtze river mainstream gradually decreases as shown in fig 3 among them the section from yibin station to the estuary is the yangtze river trunk line which is the water area with the most navigable ships and the largest cargo throughput mainly including 19 stations as shown in fig 2 2 2 water level data in order to study the daily water level variation trend of the entire yangtze river trunk line this research collected the daily water level data from 19 stations including all stations where the yangtze river maritime safety administration announces the daily water level with data records including yibin luzhou chongqing cuntan fuling wanzhou maoping yichang zhijiang shashi jianli chenglingji hankou huangshi jiujiang anqing wuhu nanjing and zhenjiang as shown in fig 2 in this paper the 19 stations are indexed sequentially from station 1 to station 19 where station 1 is yibin and station 19 is zhenjiang the summary statistics for the 19 stations as shown in table 2 from the topographical point of view the 19 stations are distributed along three different parts of the yangtze river trunk line stations 1 8 located in the upper reach stations 9 13 located in the middle reach and stations 14 19 located in the lower reach it is worth noting that the terrain topography and water flows of each station in different water areas have different characteristics specifically in some waters of the upper reaches the terrain is high and steep and the elevation a s l is up to 3000 4000 m while the elevation a s l suddenly drops to 200 600 m at the edge of the sichuan basin such as yibin station the river enters the hilly area in the middle reach with densely covered beaches and branching water flows the valleys of certain river sections are several kilometres wide and the river surface is 155 500 m wide the lower reaches of the river have gentle water and wide river surface and are with low mountains and wide valleys the above mentioned different characteristics have a great impact on the variation of water level resulting in different value ranges for the daily water level data at different stations for example the water levels of stations 3 7 are all above 145 m because they are in the reservoir area of the three gorges reservoir area as shown in fig 2 with wusong zero level as the reference plane as shown in fig 4 a at other stations the water level measurement considers the local navigation datum level as the reference plane as shown in fig 4 b therefore the highest water level is less than 6 m in station 19 the water level data come from the daily records issued by the ministry of transport of the people s republic of china 1 1 http www mot gov cn shuiluchuxing changjiangshuiweigonggao which include the daily water level for two and a half years 912 days form january 1 2018 to june 30 2020 the daily water level data of each station are shown in fig 5 it can be seen from fig 5 that the water level data can be regarded as time series data because they present two obvious characteristics the data of water level changes with time and the data are interrelated therefore the problem of daily water levels prediction at 19 stations can be transformed into the problem of analysis and prediction for 19 different time series to improve the efficiency in analysis and prediction of multi station daily water level and improve the applicability of the proposed prediction method it is considered to build one model for several stations with similar water level sequence rather than build one model for each station 3 methodology the research framework is as shown in fig 6 first the daily water level data of 19 stations along the yangtze river trunk line are collected to avoid building separate models for all stations a strategy of divide and conquer is proposed the dtw algorithm is employed to measure the similarity of the water level data of multiple stations and a hca is utilised to group the stations with similar characteristics according to the similarity matrix then the msdwlp models are constructed based on the lstm network and the sarima method for different clusters to improve the prediction accuracy of each station the python library statsmodels lemenkova 2019 is employed to decompose the water level series training set into trend period and residual for the complex water levels with shorter periodicity 2 20 days because a voyage time of the ship in the yangtze river trunk line is about 20 days yuan et al 2021a lstm is used to approximate the trend and sarima is used to approximate the residual term sum of period and residual finally the daily water levels of the 19 stations in the next 7 days are predicted and analysed in detail using the testing data 3 1 modelling strategy in this paper a two stage modelling strategy is proposed as shown in fig 7 at the first stage the water level data from multiple stations are clustered based on similarity measurement using a hierarchical clustering algorithm stations with a similar trend in water level data would be integrated into a single prediction model at the second stage daily water level of each cluster will be firstly decomposed into three parts including a long term trend a period change trend and residual for the clusters whose data have a long periodicity such as yearly or longer or have no obvious periodicity the lstm network is directly used to process the water levels contrarily for the clusters with shorter periodicity the lstm sarima model is employed on the water levels in particular the long term trend is modelled using the lstm network and the residuals including period and residual is modelled with sarima the prediction results of each part are combined to obtain the integral water level prediction 3 2 clustering method first the similarity matrix between the water levels of the 19 stations needs to be calculated and obtained it is worth noting that the number of the water level data is the same for all stations i e the water level data of the 19 stations are 19 time series of equal length dynamic time warping dtw is a dynamic programming algorithm suitable for accurately calculating the similarity between multiple series dürrenmatt et al 2013 yu et al 2018 in addition dtw has no parameter restrictions and is robust to time in this work we employ the dtw algorithm to calculate the similarity distance matrix assuming that s 1 and s 2 are the water level sequences of station 1 and station 2 the similarity distance between s 1 and s 2 is calculated as follows d p i j 2 s 1 0 s 2 0 2 i 0 j 0 s 1 0 s 2 j 2 d p 0 j 1 i 0 s 1 i s 2 0 2 d p i 1 0 j 0 s 1 i s 2 j 2 min d p i 1 j d p j 1 i d p i 1 j 1 i j 0 where i and j are the indices of s 1 and s 2 d p i j is the similar distance between s 1 i and s 2 j then a clustering algorithm is utilised to analyze the similarity matrix to obtain the clustering results of 19 stations hierarchical clustering algorithm hca can create a hierarchical nested clustering tree based on similar distances between data points of different categories zhou et al 2017 it is a clustering technique that does not need to consider the selection of the number and positions of initial clusters hca is very intuitive i e clustering layer by layer its core is to start with individual clusters and merge the two closest clusters at each step the clustering algorithm used for grouping stations is designed based on the hierarchical clustering principle and is shown below algorithm 1 the clustering algorithm for 19 stations image 1 3 3 predictive modelling methods 3 3 1 water level time series decomposition in general time series prediction is to capture the variation characteristic and trend of the target series by learning the historical data so as to predict the data of the future time assuming that t t represents the long term trend item p t represents the item of periodic change trend and r t represents the random interference item then for a water level series there are three common decomposition models wang et al 2017 as follows 1 addition model s t t t p t r t 2 multiplication model s t t t p t r t 3 mixed model s t t t p t r t or s t t t p t r t in this work the addition model and seasonal decompose algorithm were employed for water level time series decomposition the seasonal decompose is an algorithm for time series decomposition using the moving average method which can be called in python module statsmodels 2 https www statsmodels org stable index 2 finally the water level of each station was decomposed into trend period and residual the specific decomposition steps are as follows step 1 decompose trend items by using the centralized moving mean method are shown in equations 1 and 2 1 t t s t f 1 2 s t f 1 2 1 s t f 1 2 f t i s o d d n u m b e r and t f 1 2 l f 1 2 2 t t 0 5 s t f 2 s t f 2 1 s t f 2 1 0 5 s t f 2 f t i s e v e n n u m b e r and t f 2 1 l f 2 where f is frequency of the series s t and l is the length step 2 subtract the trend term from the original water level as shown in equation 3 average the values at the same frequency in each period to obtain the periodic term as shown in equation 4 and further centralize to obtain the period term of the water level as shown in equation 5 3 s t s t t t 4 p t i 0 n s t i f f n max n n f l t 1 f 5 p t p t mean p t further the shorter period s of the p t is determined by the graphical observation method and the numerical calculation method i e p t s p t step 3 calculate the residuals r t s t t t p t 3 3 2 lstm the lstm network is one type of recurrent neural network rnn which was designed to tackle the problem of gradient dispersion existing in the conventional rnns elman 1990 the structure of rnn is shown in fig 8 where x is the sequence of input with length t h is the sequence of the hidden layer o is the output sequence l is the total loss and y is the sequence of target u is the parameter matrix from the input layer to the hidden layer w is a self looping parameter matrix in the hidden layer and v is the parameter matrix from the hidden layer to the output layer the biggest difference between rnn and traditional neural network is that the neurons also namely nodes between the hidden layers are connected so the rnn can memorize the previous information and apply it to the calculation of the current output moreover a self loop feedback is added between the hidden nodes through weight sharing which enables the network to process the data of indefinite length therefore the rnn has strong ability of learning and memory and can efficiently process the sample data of time series fang et al 2020 yuan et al 2020 the difference is that in addition to the common network layers such as input output and hidden the lstm network has a cell state at the top which is used to memorize store and transmit information the cell state can also be understood as an information conveyor belt which is actually the memory space of the lstm changing over time however the information conveyor belt itself cannot control what information is remembered and what is forgotten it is the three control gates that really control the memory information i e input gate forget gate and output gate therefore the core of the lstm network is the cell state and the control gates as shown in fig 9 t is time i g a t e t represents the input gate at time t which is a control gate from the previous long state information to the current long state information it is used to control how much new information is saved f g a t e t represents the forget gate at time t which is also a control gate from the previous long state information to the current long state information it is used to control how much history information is forgotten o g a t e t represents the output gate at time t which is a control gate from the current information to the output state information the summation of the long state information and the short state information is the current information c t is named cell state which memorises information and c t 1 is the cell memory at the time point t 1 h i d t is named hidden state which represents the output of the hidden node i n t represents the input sequence that can be a series with one dimension or multiple dimensions f is a sigmoid activation function and h is a hyperbolic tangent activation function the calculation of control gates and transmission states are shown in equations 6 10 gers et al 2000 6 i g a t e t f w i i n t u i h i d t 1 b i 7 f g a t e t f w f i n t u f h i d t 1 b f 8 o g a t e t f w o i n t u o h i d t 1 b o 9 c t f g a t e t c t 1 i g a t e t h w c i n t u c h i d t 1 b c 10 h i d t o g a t e t h c t where w i f o c are the weight matrices linking the input layer with the hidden layer b i f o c are bias terms and u i f o c are the self looping weight matrices of the hidden layer yuan et al 2021a the specific modelling steps for daily water level using the lstm network are designed as follows step 1 extracting the data of the daily water level of each station according to the station index step 2 normalising data scale the training sample data to the same range between 0 and 1 so that the network converges quickly and avoids predicting numerical errors step 3 generating time series in this step the time series are generated according to time time steps and batch size where time times represents the length of the input variable in the time dimension and batch size represents the size of the samples data for each processing the format of the time series is samples time steps n dim which the lstm network adapts to step 4 setting the parameters of the lstm network including the number of neurons the number of hidden layers epochs training iterations activation function training function and loss function step 5 training and optimising the initial lstm network until termination criteria are satisfied e g the training error is less than a set threshold step 6 predicting the future data in a single step or multiple steps using the trained lstm network it should be noted that the prediction results need to be denormalised step 7 results evaluation and analysis the prediction accuracy is analysed by calculating some performance measures 3 3 3 sarima the sarima model is an evolution model of seasonal or periodic data based on the arima model shumway and stoffer 2017 the arima model refers to the model established by transforming non stationary time series into a stationary time series and then regressing the lag value of the dependent variable phan and nguyen 2020 and the present value and lag value of the random error term including ar process moving average ma process auto regressive moving average arma process and arima process velasco and lazakis 2020 here the lag value represents the order of the lag operator which is involved in the difference operation that transforms a non stationary sequence into a stationary sequence the expression of the sarima model can be written as s a r i m a p d q p d q s where p p q and q represent the maximum lag order of non seasonal seasonal autoregressive and moving average respectively d and d represent the order of the differentiate and seasonal difference respectively s denotes the period of the seasonal time series it is worth noting that in the process of modelling and analysis using the sarima method the values of parameters p p d d q and q are not very large and d and d usually take 0 and 1 to meet the requirements the specific modelling steps for time series using the sarima model are designed as follows step 1 visual analysis of time series data the time series diagram of the data is drawn to visualise the trend of sequence changes over time step 2 test the stationarity of the time series in this approach the augmented dickey fuller adf test is used to test the stationarity of the series which is a common and effective method for testing sequence stationarity step 3 sequence stabilisation the stabilisation of the time series is to eliminate the trend effect and seasonal effect of the sequence and the differencing method is the most common method to achieve sequence stabilisation step 4 model order determination that is determining the parameters of the sarima model p p d d q and q this is a very critical step this paper uses the network search method to systematically select the optimal values of the parameters specifically the akaike information criterion aic is selected as the criterion to search for the best model parameters from the possible candidate set the aic not only improves the degree of model fitting but also introduces a penalty term to make the model parameters as few as possible which is helpful to reduce the possibility of overfitting step 5 building of the sarima model according to the optimal parameters determined in step 4 step 6 model testing verify if the residuals of the model are correlated and they are normally distributed with zero mean if not the model can be further improved step 7 data prediction use the constructed sarima model to predict the future data of the time series step 8 results evaluation and analysis 3 4 data partitioning in this study the daily water level data of 19 stations along the yangtze river trunk line were collected for 912 consecutive days from january 1 2018 to june 30 2020 forming the water level time series with 912 rows and 19 columns in our two stage modelling method the daily water level time series was first divided into two parts the 80 data from the first 2 years january 1 2018 to december 31 2019 was training data and the 20 data from the next half year january 1 2020 to june 30 2020 was test data in the proposed modelling methodology the training data is used for models training and validation and the test data is used for models prediction performance testing moreover in the process of hyperparameter tuning and model selection the training data was further divided into training set and validation set according to the ratio of 80 and 20 the variables presented to the prediction models are vectors composed of the consecutive days daily water level time series from the 19 stations specifically the daily water levels of training data are cyclically extracted in chronological order and constructed into vectors with length t i m e s t e p n d i m and presented to the basic lstm network where the first t i m e s t e p variables are the input feature and the last n d i m variables are the output feature in this work we adopt a cyclic rolling method to predict the multiple days daily water level that is using one step ahead prediction as part of the new input in addition to verify the prediction performance of the models the 7 step ahead prediction were conducted for all testing data specifically the daily water levels of all testing set were divided into 26 groups and presented to the trained models for statistical prediction errors where the length of each group of variables is t i m e s t e p and the water level prediction values of 26 groups for 7 consecutive days are obtained through loop prediction the frameworks of prediction models training and testing are shown in fig 10 and fig 11 where w l represents water level t s represents t i m e s t e p 3 5 performance measures to assess the accuracy and reliability of the proposed msdwlp models the prediction results are analysed by several performance measures ebtehaja et al 2019 including ae absolute error mae mean absolute error mape mean absolute percentage error rmse root mean square error and nse nash sutcliffe efficiency coefficient as shown in equations 11 15 11 a e z t z t ˆ 12 m a e 1 n t 1 n z t z t ˆ 13 m a p e 1 n t 1 n z t z t ˆ z t 100 14 r m s e 1 n t 1 n z t z t ˆ 2 15 n s e 1 t 1 n z t z t ˆ 2 t 1 n z t z t 2 where t represents the time index of a datum t 1 2 n n represents the length of the prediction sequence z t ˆ and z t are the predicted value and the measured value of the t th datum and z t is the mean of z t in addition to supplement these performance evaluation scatter plots and time series plots are also inserted for a graphical demonstration 4 experimental details in this section details concerning the experiment study are presented mainly including stations clustering model settings and experimental design the data analysis and modelling experiments were conducted using a desktop pc with intel core i7 7700 cpu and 16 gb ram its operating system was 64 bit windows 10 and the programming language employed was python 3 7 where a python ide integrated development environment spyder and the open source library keras sklearn and statsmodels were employed 4 1 stations clustering first of all the dtw based hierarchical clustering method was employed for the clustering of daily water level sequences from 19 stations for the daily water level of training set 730 consecutive days from the 19 stations which from january 1 2018 to december 31 2019 using the dtw and clustering algorithms introduced in section 3 2 the hierarchical clustering tree of the 19 stations obtained from the similarity distance matrix is shown in fig 12 a different clustering results can be obtained by moving a cross cut line up and down for example moving the cross cut line to the top as shown by the black line in the figure the 19 stations are divided into two rough clusters stations 3 4 5 6 and 7 as one cluster and the other stations as another cluster in fact stations of 3 4 5 6 and 7 are all located in the three gorges reservoir area which have much higher daily water levels 145 m than the other stations moreover the overall water levels at stations 3 and 4 are higher and more complex than that of stations 5 6 and 7 which can be seen from fig 5 therefore stations 3 4 5 6 and 7 are further divided into two clusters stations 3 and 4 as one cluster and stations 5 6 and 7 are with another cluster at this time the 19 stations were divided into 3 clusters moreover moving the cross cut line down to the position of the red line 6 clusters can be obtained fig 12 b shows 7 different clustering results for 19 stations including 2 3 4 5 6 7 and 9 clusters as can be seen from figs 12 and 19 stations can be clustered into at least 2 clusters and at most 19 clusters moving the cross cut line to the bottom of fig 12 a however for multi station daily water level prediction the fewer the clusters the less distinct the water level features of the stations are the more clusters the more expensive the modelling and computation for example with 9 clusters stations 5 6 and 7 are divided into two clusters however their water level ranges and trends did not change significantly as shown in fig 5 and one model could be constructed for all 3 stations as well as stations 8 9 and 10 therefore considering the performance and computational cost the 19 stations were clustered into 6 classes in the first stage of clustering in additional experiments it is also verified that 6 cluster is an ideal clustering results 4 2 model settings according to the proposed divide and conquer modelling strategy 4 types of multi station daily water level prediction msdwlp models are constructed and tested to verify the effectiveness of the proposed method which use lstm for the trend components and sarima for the residual component in our strategy different mswlp models were set up for comparison including msdwlp s msdwlp cs msdwlp ch1 and msdwlp ch2 which are described as follows msdwlp s denotes the msdwlp model that constructed by using lstm alone for all 19 stations including one daily water level prediction model msdwlp cs denotes the msdwlp models that constructed by using lstm alone for each cluster based on stations clustering results including 6 daily water level prediction models msdwlp ch1 denotes the msdwlp models that constructed by combining lstm and sarima for each cluster based on stations clustering results where the lstm is used for the trend components and sarima is used for the residual term components period and residual including 6 daily water level prediction models msdwlp ch2 denotes the msdwlp models that constructed by combining lstm and sarima for each cluster based on stations clustering results where the sarima is used for the trend components and lstm is used for the residual components period and residual including 6 daily water level prediction models in the next sub section the model parameters settings of the models are introduced 4 3 parameter settings the optimal parameters are crucial to the performance improvement of the model for a high performing lstm network there are multiple hyperparameters and activation functions that need to debugged and optimized yuan et al 2020 in the experiments the parameters and their candidate values are described as follows neurons the number of neurons nodes in the hidden layer of the lstm network time step the length of the input time series of the lstm network n dim the length of the output time series of the lstm network batch size the size of sample data which are presented the lstm network each time training epochs the number of iterations of the lstm network learning rate control the rapid convergence of the lstm network model to the optimum generally ranging from 0 001 to 0 1 activation functions the purpose of activation function is to provide the non linear modelling capabilities for the network the common activation functions are piecewise linear functions and non linear functions with exponential shapes such as relu a rectified linear unit function linear a linear activation function tanh a hyperbolic tangent function softsign similar to tanh but smoother sigmoid a common s type function loss functions the purpose of loss function is to evaluate the severity of the neural network performance and the network uses the loss function as the clue to find the optimal weight parameters the common loss functions include mae mean absolute error mse mean squared error mape mean absolute percentage error msle mean squared logarithmic error hinge and squared hinge optimization algorithms the purpose of optimization algorithms is to minimize or maximize the loss function by improving the training method to obtain optimal network parameters the common optimization algorithms include rmsprop root mean square propagation optimiser adam adaptive moment estimation adamax a variant of adam with infinity norm nadam nesterov accelerated adaptive moment estimation adagrad adaptive gradient algorithm adadelta extension of adagrad with smaller learning rate in addition to avoid overfitting of the lstm network the early stopping method is adopted the specific steps are as follows 1 divide the original training data set into training set and validation set 2 calculate the error of the validation set in each batch size period and stop training if the error increases 3 use the parameters from the previous iteration result as the final parameters of the lstm the parameters settings of the model are divided into different groups and a network search algorithm nsa is designed based on the minimum error criterion and aic to systematically select the optimal values of the parameters moreover in order to overcome the limitations of the data two and a half years in this paper parameter transfer learning is on the sample data that is the training data of all stations are extracted to conduct experiments on the hyperparameters tuning of the lstm network and then applied the determined parameters to the water level prediction model of each cluster stations moreover during the experiments the training data set is divided into training set and validation set to ensure the accuracy of parameter settings table 3 records the nse of the lstm network parameter setting experiments for the daily water level prediction of all stations where activation function optimiser function time step batch size and neurons number are determined in group 1 2 3 4 and 5 as shown in table 3 the following parameters settings are found to be appropriate the neurons were set to 150 the batch size was set to 36 the time step was set to be 6 n dim was set to 1 cyclic rolling method is used to predict the daily water level in future days i e use one step ahead prediction as part of the new input the activation function was the rectified linear unit function relu the loss function was set to mse and the optimization algorithm was the nesterov accelerated adaptive moment estimation optimiser nadam similarly s a r i m a p d q p d q s model has 7 key parameters that need to be set namely the maximum lag order of non seasonal p the maximum lag order of autoregressive q the maximum lag order of seasonal p the maximum lag order of moving average q the order of the differentiate d the order of seasonal difference d and the period of the time series s the best parameters p p d d q and q are also determined by a network search method which is based on a variant of hyndman khandakar algorithm hyndman and khandakar 2008 in addition the python library of statsmodels was employed for building and training the sarima models 5 results and discussion 5 1 the results of stations clustering firstly through the proposed algorithm 1 19 stations are clustered into 6 clusters with cluster 1 stations 1 and 2 cluster 2 stations 3 and 4 cluster 3 stations 5 6 and 7 cluster 4 stations 8 9 and 10 cluster 5 stations 11 12 13 14 15 and 16 and cluster 6 stations 17 18 and 19 fig 13 demonstrates the daily water levels of 6 clusters of 19 stations as shown in fig 13 based on the 6 cluster result the daily water level sequences within the same cluster show very similar characteristics in the data distribution and change trend 5 2 the results of daily water level prediction in 19 stations according to the results of parameter settings table 3 and the stations clustering 6 clusters fig 13 the configuration of msdwlp models was further optimized as shown in table 4 in table 4 acti represents activation function opti represents optimization algorithm the structural form of the lstm network is lstm time step batch size neurons training ecpochs and the structural form of the sarima model is sarima p d q p d q s for example the meaning of lstm 7 36 150 1000 sarima 2 1 1 3 1 0 7 is as follows for the lstm network the time step is 7 the batch size is 36 the neurons is 150 the training ecpochs is 1000 for the sarima model p 2 p 3 d 1 d 1 q 1 q 0 and s 7 table 5 records the daily water level prediction performance of different models for the testing set in which the parameters of each model are set according to table 4 from the values averaged over all stations of performance measures in table 5 the following conclusions can be drawn 1 for each lead time the models of msdwlp cs msdwlp ch1 and msdwlp ch2 which were constructed based on the clustering results of 6 clusters exhibited more accurate predictions in terms of the mae mape rmse and nse than msdwlp s therefore the multi station clustering was an important factor that resulted in a more accurate prediction for our case study 2 for each lead time the combined models msdwlp ch1 and msdwlp ch2 both provided low errors mae mape and rmse and high nse it can be seen that the combined models present improvements in the daily water level prediction accuracy 3 for each lead time the msdwlp ch1 model which is proposed in this paper further improved the performance of daily water level prediction for multi stations for example the mae and rmse on the 1st day were only 0 06 m and 0 07 m and the nse reached 0 999 on the 7th day they were 0 41 m 0 45 m and 0 946 respectively the results in table 5 provide support to the proposed two stage divide and conquer method msdwlp ch1 for multi station daily water level analysis and prediction to further verify this claim we select a special example the first 7 days of testing data which are from january 1 2020 to january 7 2020 and summarize the detailed prediction results of the stations in each cluster as follows 5 2 1 cluster 1 a lstm sarima model cluster 1 contains stations 1 and 2 it can be seen from fig 5 that the water level data of station 1 and station 2 are not changing smoothly but the daily water level appears to have a 7 days period firstly using the addition model the daily water level training set of station 1 is decomposed into trend and residual where the residual term contains the period of small amplitude and residual then the lstm neural network was tailored and implemented to build a model for the trend prediction and the sarima method was used for predictive modelling for the residual part the parameters settings of lstm sarima were from table 4 finally the trend predicted by lstm and the residual predicted by sarima were added to obtain the predicted daily water level the water level prediction results of stations 1 and 2 for certain 7 days january 1 2020 to january 7 2020 are shown in fig 14 a it can be seen that the prediction accuracy is very good for the future 7 days where the absolute error for the future 1st day is less than 0 07 m and the one for the future 2nd day is less than 0 13 m in station 1 moreover the absolute error are only 0 03 m and 0 04 m in station 2 5 2 2 cluster 2 a lstm sarima model cluster 2 includes stations 3 and 4 which are located in the three gorges reservoir area as with cluster 1 the water level after decomposition also has an obvious period 8 days in the same way the lstm network was used for forecasting the trend and sarima was used for forecasting the residual except for time step was set to 8 and s 8 all the parameters of lstm network and sarima model are consistent with those in cluster 1 the predicted results of the daily water level of the future 7 days january 1 2020 to january 7 2020 for stations 3 and 4 are shown in fig 14 b it can be seen that the prediction is very accurate for the first two days where the absolute error for the future 1st day is less than 0 14 m and the absolute error for the future 2nd day is less than 0 13 m 5 2 3 cluster 3 a lstm model cluster 3 contains three stations 5 6 and 7 which are also located in the three gorges reservoir area unlike clusters 1 and 2 after the water level was decomposed no short period was found however good water level predictions can still be obtained using only the lstm network the predicted results of certain future 7 days january 1 2020 to january 7 2020 for stations 5 6 and 7 are as shown in fig 14 c it is worth noting that the parameters are consistent with the lstm network for clusters 1 and 2 except for the time step being 5 although the absolute errors of stations 5 6 and 7 are larger than those of stations 1 2 3 and 4 their absolute percentage errors are very small because the daily water level of the three stations has always been above 145 m it can be found the prediction accuracy of the developed model is very good in forecasting the future several days especially for stations 6 and 7 5 2 4 cluster 4 a lstm model cluster 4 has three stations no 8 no 9 and no 10 similar to cluster 3 the decomposed water levels have no short period when the lstm network is used for the daily water level prediction of cluster 4 of stations the prediction results for the next 7 days january 1 2020 to january 7 2020 are shown in fig 14 d in the experiments all the parameters of the lstm network are the same with those in the previous clusters except that the time step is 10 it can be observed the developed model can well predict the water level for future several days especially for the future 1st and 2nd days 5 2 5 cluster 5 a lstm model cluster 5 includes six stations no 11 12 13 14 15 and 16 most of these six stations are located in the lower reach of the yangtze river trunk line although the water level data of the six stations have no obvious periodic characteristics they are all relatively smooth the lstm network was set to have the same parameters as those for previous clusters except for the time step being 8 the prediction results of the six stations in the future 7 days january 1 2020 to january 7 2020 are shown in fig 14 e from fig 14 e we can see that the absolute errors of the six stations in the future 1st and 2nd days are all less than 0 09 m more importantly the absolute errors of each station in the next 3 days are no more than 0 18 m and in the next 7 days are no more than 0 59 m 5 2 6 cluster 6 a lstm model cluster 6 contains stations 17 18 and 19 after the decomposition of the water level data an obvious period of 15 days was found similarly the lstm network for forecasting the trend it was set to the same parameters as those for previous clusters except for the time step being 15 and the sarima was used for forecasting the residual term including period and residual it settings were found as follows p 2 p 2 d 0 d 1 q 1 q 1 and s 15 the absolute errors of the six stations do not exceed 0 03 m in the future 1st day and 0 06 m in the future 2nd day the prediction results of the three stations of cluster 6 for the future 7 days january 1 2020 to january 7 2020 are shown in fig 14 f as a specific example the first 7 days of testing set from january 1 2020 to january 7 2020 the mae and mape of 19 stations for a certain prediction case are also tested and analysed in general as the number of days increases the prediction errors mae of each station also increase however in the stations of each cluster good water level prediction results have been achieved in addition to verify advantages of the proposed method it is compared with two models msdwlp s and msdwlp cs and the performances mean of 10 experiments of different models can be shown in table 6 compared with the proposed method both msdwlp s and msdwlp cs have relatively high prediction errors when msdwlp s is used to predict the case the mae and mape have further increased in all stations when msdwlp cs is employed the mae and mape have greatly increased in stations 1 4 and 17 19 when msdwlp ch2 is adopted the mae and mape in cluster 1 2 and 6 are still larger than msdwlp ch1 to sum up the method msdwlp ch1 proposed in this paper has better accuracy in 19 stations daily water level prediction more importantly fewer model parameters are required for multi station daily water level prediction in our method in that the clustering in the first stage reduces the number of objects that modelling in the second stage 5 3 discussion and insights the predictive performance nse of the developed msdwlp ch1 against all the training data ranging from january 1 2018 to december 31 2019 730 days and testing data ranging from january 1 2020 to june 30 2020 182 days is shown in fig 15 among them the testing data containing 182 days of daily water level is divided into 26 groups for prediction and analysis and the output of each group includes 7 days as can be seen from fig 15 the nse of all training and testing are over 0 97 in the future 3 days moreover the nse of all stations in the future 7 days are basically above 0 90 except for 0 887 and 0 863 in the future 6th and 7th days at station 2 and most of them exceed 0 95 fig 16 illustrates the prediction rmse and mape of 19 stations in all testing data as can be seen from fig 16 a at all stations the rmse becomes greater if the prediction time is longer this means that the models always perform the best in predicting the near future such as the future 1st day and become less accurate when predicting the far future such as the future 7th day the bigger rmses appear in stations 4 5 6 and 7 where the stations are located in the three gorges reservoir area and their daily water levels are above 145 m all year round the smaller rmses appear in stations 8 9 and 10 which belong to the fourth cluster except for the stations in cluster 2 and cluster 3 the rmses of other stations for the future 1st day are less than 0 12 m and the rmses for the future 3rd day are less than 0 20 m from fig 16 b it can be seen the smallest mape appears in stations 3 4 5 6 and 7 and the large mapes appear in stations 1 2 8 9 and 10 stations 1 and 2 belong to the first cluster and are located in the upper reach of the yangtze river stations 8 9 and 10 belong to the fourth cluster and are located in the middle reach in fact the stations of first cluster are the typical mountain river channels and the stations of fourth cluster are located in the three gorges reservoir area therefore the water regime in these two river sections is the most complicated the variation of mape in other stations is relatively small especially in the lower reach of the yangtze river such as stations 14 19 in general as the number of days increases the prediction errors of each station also increase the method proposed in this paper showed decent accuracy in 19 stations daily water level prediction and more importantly fewer model parameters are required in that the clustering in the first stage reduces the number of modelling objects in the second stage 6 conclusions in this paper the daily water level data of multiple stations on the yangtze river trunk line have been collected analysed and predicted in order to better inform decision making about safe and effective waterborne transportation water management and emergency response a new two stage divide and conquer modelling strategy has been proposed in the first stage the dtw algorithm is employed to calculate the similarity distance of water level series and the hierarchical clustering algorithm is used to divide 19 stations into 6 clusters according to the similarity matrix in the second stage the lstm network and sarima model are tailored and implemented to build the msdwlp models for each cluster in particular for the clusters with short periodicity and obvious seasonal change trend the daily water level is decomposed into long term trend periodic change trend and residual then the msdwlp ch1 model is employed and selected for better results in which the long term trend part is approximated by the lstm network and the residual term part is approximated by the sarima model in the validation experiments the daily water levels of 19 stations in the future 7 days are predicted the results show that the proposed analysis and modelling method can be well applied to the case of the yangtze river trunk line the rmse of the prediction is no more than 0 12 m for the future 1st day and is no more than 0 26 m for the future 3rd day the average mape across 19 stations is 2 03 for the future 1st day and is 6 91 for the future 7 days the water levels of inland rivers is affected by many complex factors such as elevation a s l waterway topography periodic characteristics and flood control and drought resistance strategies which make it difficult to elicit conventional predictive models in the proposed two stage method firstly 19 stations were clustered into 6 categories according to the similar characteristics of daily water level which reduced the influence of elevation a s l and waterway topographic changes on daily water level change thereby reducing the complexity of multiple stations daily water level prediction secondly a prediction model was constructed for each cluster stations according to the periodic characteristics of daily water level which reduced the number of prediction models and the parameters that need to be determined thereby improving the accuracy of daily water level prediction moreover the anticipation of the water level variation will support decision making in planning and operation of waterborne transportation water management and emergency response meanwhile a potential improvement in the following study is to employ more factors in the proposed method which is flexible to use other data as predictors if available such as precipitation the inland reginal and oceanic climate variables play a critical role in providing valuable information for the multi station river water level prediction additionally the performance of the proposed approach at smaller timescales for example hourly forecasting is worth exploring declaration of competing interest the authors declare that they have no know competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the independent innovation research fund of wuhan university of technology grant no 2022iva048 the national natural science foundation of china nsfc grant no 51709219 the national key research and development program of china grant no 2018yfc1407400 the qingdao research institute of wuhan university of technology grant no 2019a02 and the china scholarship council csc grant no 201906950086 
25536,fire is a dominant disturbance in temperate and boreal biomes and increasing burned area with climate change may fundamentally alter forests improved information about how fire induced changes to forests may feedback to affect subsequent burning at regional scales could inform forest management and climate mitigation strategies however fire is simplistically represented in earth system models and regional statistical fire models often assume sufficient fuels contributing to uncertainty in future projections to address this challenge we developed the dynamic temperate and boreal fire and forest ecosystem simulator dynafforest dynafforest represents the hierarchical structuring of forests from individual cohorts to continental extents making it possible to simulate feedbacks between fire and forests at broad scales over decades to centuries we parameterized dynafforest for the western united states of america and benchmarked simulations with observations dynafforest recreated patterns of forest cover structure and downed fuels and was capable of capturing average 20th century fire activity keywords climate change ecosystem modeling forest resilience natural disturbance scaling wildfire 1 introduction forest fire is a prevalent natural disturbance in terrestrial ecosystems and is sensitive to climate particularly in temperate and boreal forest biomes seidl et al 2020 trends toward warmer more arid conditions are causing fire frequency size and severity to rapidly increase in many places abatzoglou and williams 2016 kelly et al 2013 westerling 2016 for example annual burned forest area has grown 1 100 since 1984 in the western conterminous united states of america hereafter western us williams et al 2022 current climate fire trends will almost certainly continue over the next few decades abatzoglou et al 2021 with large consequences for people and ecosystems coop et al 2020 mcwethy et al 2019 schoennagel et al 2017 while climate is a dominant cause of increasing fire activity other factors also contribute to recent trends including spatiotemporal variability in fuel loads sometimes related to legacies of fire suppression and human caused ignitions balch et al 2017 calef et al 2008 hagmann et al 2021 many of these drivers are not well accounted for in disturbance succession models of forests particularly at broader scales nor are the complex feedbacks between fire and its drivers therefore new quantitative tools that incorporate multiple drivers and feedbacks could yield important insights into the trends causes and consequences of forest fire one of the most important feedbacks that models must better capture is how fires can alter forests in ways that affect the likelihood of subsequent burning two pathways exist through which these fire forest feedbacks play out first fire combusts fuels which decreases the probability of another fire for a period in the western us reduced fire probability can last 5 30 years after the initial burn depending on climate and extreme weather events parks et al 2018 second increased burning can initiate transitions from forests to alternate vegetation communities that differ in their flammability johnstone et al 2016 tepley et al 2018 for example less flammable deciduous tree species now commonly replace black spruce after unusually severe fire in the north american boreal forest scientists and practitioners need improved information about where when and why fire induced changes to forests may feedback to affect subsequent burning otherwise known as re burns prichard et al 2017 schoennagel et al 2017 this need for improved modeling is particularly true across regional to continental scales becknell et al 2015 turetsky et al 2017 walker et al 2018 the spatial scales at which policy and management are commonly targeted and impending ecological change could alter broader earth system functions heffernan et al 2014 rose et al 2017 most models capable of simulating broad spatial domains 106 km2 ignore or simplistically represent fire and or do not explicitly represent forests and their dynamics for example earth system models esms were designed to simulate the role of vegetation in regional to global carbon and water cycling but have a coarse spatial resolution 0 5 and often do not include fire hantson et al 2020 rabin et al 2017 of the few esms with a dynamic fire module most only simulate aggregate burned area and do not include the demographic processes that underpin forest recovery like tree seed dispersal and seedling establishment fisher et al 2018 hantson et al 2020 sanderson and fisher 2020 at regional scales statistical models of fire are often used that operate at a finer spatial grain 6 12 km and represent the multi step process of burning more realistically than esms by relating observed climate to individual fire characteristics such as occurrence size and severity keyser and westerling 2019 westerling et al 2011 however statistical fire models rarely simulate forest change and assume sufficient fuel and unchanging forest cover when projecting future fire use rules to define the duration after a fire in which a cell cannot burn employ empirical functions to implicitly represent dynamic patterns of postfire fuel limitation or implement simple statistical vegetation climate relationships to represent potential change in fuels with climate abatzoglou et al 2021 kitzberger et al 2017 littell et al 2018 numerous forest landscape models include both fire and detailed representations of forest dynamics such models capture the necessary demographic processes and simulate feedbacks to fire albrich et al 2020 hansen et al 2020 hurteau et al 2019 seidl et al 2014 serra diaz et al 2018 but are computationally expensive and most cannot be run for broad domains though see rammer and seidl 2019 to address the dearth of quantitative tools capable of modeling temperate and boreal fire and forests across regions to continents we present a new model called the dynamic temperate and boreal fire and forest ecosystem simulator dynafforest we designed dynafforest to simulate complex and interacting causes of fire and to represent the demographic processes that underpin postfire forest recovery thus the model is capable of simulating dynamic feedbacks between fire and forests dynafforest is computationally efficient such that it can be run with a sufficiently fine spatial resolution 1 km to capture heterogeneity in vegetation size classes structure and stand ages in topographically complex landscapes the model is capable of simulating broad domains of similar scope to the forested area of the western us 850 000 km2 and operates at an annual time step in this paper we describe dynafforest and evaluate model skill by comparing simulations of western us forests under mid 20th century climate conditions with multiple independent benchmarking datasets 2 materials and methods 2 1 model overview our objective was to represent how forest fires and climate affect temperate and boreal forest ecosystems across regions to continents and how fire induced changes to forests feedback to alter subsequent fires dynafforest is broken into two modules a forest ecosystem module and a fire module fig 1 we constructed dynafforest with a hierarchical structure where the forest ecosystem module operates at a 1 km spatial resolution and the fire module operates at a 12 km resolution the time step of both modules is annual the forest ecosystem module probabilistically represents key processes through which fire and climate affect forests and uses deterministic equations to represent other forest processes based on age successional stage fig 1c forest types can be parameterized at the level of specificity necessary for the application from individual species to coarse plant functional types pfts we simulate broad spatial domains with a fine grain size using a cohort based approach that tracks a single cohort for each 1 km grid cell the cohort has an age dependent size and density based on pft specific traits the approach is computationally efficient as only one cohort per grid cell must be tracked it also still allows for representation of heterogeneity in forest cover composition and structure at a 1 km resolution which is finer than other models capable of simulating forests across comparable domains buotte et al 2018 fisher et al 2018 however because one pft cohort per grid cell is tracked the model does not explicitly represent relay successional trajectories where transitions from the initial fast growing pioneer tree species or shrubs to slow growing late successional shade tolerant species may alter stand flammability with time since disturbance e g tepley et al 2018 probabilistic process based components are inspired by and closely follow the forest model iland seidl et al 2012a which has been widely applied in temperate and boreal forests braziunas et al 2018 hansen et al 2018 2020 2021 seidl et al 2014 deterministic components are derived from several sources e g dixon 2015 rammig et al 2007 a technical description of the forest ecosystem module is provided in sections 2 2 2 5 section 2 9 and appendix s1 we paired the forest ecosystem module with a statistically based fire module that predicts the occurrence number and sizes of fires 100 ha as a function of variables including the amount and connectivity of fuels from the forest module spatial variation in climate aridity natural and human caused ignitions and topography appendix s1 the module is designed so users can parameterize it based on an a priori understanding of their own system fire occurrence number and maximum size are simulated on a grid with a spatial resolution of 12 km fires are then randomly ignited in a forested 1 km grid cell within the 12 km fire cell and iteratively spread to other forested 1 km grid cells until no more forested cells remain or the maximum size predicted by the fire module is reached fire effects such as cohort mortality and combustion of live biomass and fuel loads are calculated at the 1 km resolution of the forest ecosystem module because our goal in this initial development and presentation of dynafforest was to establish whether the approach could accurately simulate forest dynamics when dynamically coupled with a module that produced 20th century fire seasons the fire module implemented here does not include fire response to temporal climate variability instead it represents responses of fire to spatiotemporal changes in forest biomass and spatial variations in long term mean aridity and ignition sources as such it is inevitable that this relatively simple baseline fire module will under represent the frequency of very large wildfires which are strongly promoted by the extreme climate anomalies that have occurred more frequently in recent years with human caused warming a technical description of the fire module is provided in sections 2 2 6 2 2 8 and appendix s1 along with next steps for future development together the forest ecosystem and fire modules provide a powerful framework for addressing previously intractable questions about how feedbacks emerge to shape temperate and boreal forest and fire outcomes the flexibility of the fire module means it can be adjusted to fit a wide range of applications the forest ecosystem module takes a unique approach explicitly representing some key forest processes with great detail e g tree regeneration and mortality and deterministically representing others e g tree growth and stand density parameters for the model table 1 can be attained from the literature and public databases for most study areas globally dynafforest currently only represents fire but other natural and human caused disturbances such as bark beetle outbreaks restoration treatments and logging are also important in temperate and boreal forests berner et al 2017 morris et al 2016 ruess et al 2021 the modular design of dynafforest makes it easy to develop or plug in future modules representing other disturbance agents and management activities honkaniemi et al 2021 rammer and seidl 2015 2 2 detailed model description 2 2 1 technical implementation we aimed to develop a model that is adaptable for a wide range of applications and useable by researchers with varying expertise in programming thus we wrote dynafforest in the r language and environment for statistical computing v 4 0 4 core team 2021 while other programming languages offer superior computational efficiency e g c many ecologists and environmental scientists are experienced with r and the software is open source making it an ideal choice for encouraging widespread use and rapid community based evolution of the model full source code and documentation is available under a gnu general public license gnu gpl www gnu org licenses gpl 3 0 html 2 2 2 tree regeneration stem density and species composition of tree seedlings that establish after severe fire shape successional trajectories for decades to centuries kashian et al 2005 postfire tree regeneration can be compromised if fires recur before trees reach reproductive maturity brown and johnstone 2012 buma et al 2013 if the size of severely burned patches exceeds dispersal distance from the unburned edge or fire refugia gill et al 2022 if winter snowpack or cold temperatures reduce germination rates e g at high elevation treeline brown et al 2019 kueppers et al 2017 or if postfire drought kills tree seedlings davis et al 2019 hansen and turner 2019 because tree regeneration is essential for postfire recovery we took a probabilistic process based approach tree regeneration is simulated in dynafforest starting the year after a forest cohort dies from density independent mortality factors related to tree age size fire or drought and continues annually until another cohort establishes or the simulation ends fig 1c seed supply is modeled based on pft specific fecundity which is the number of potential seedlings produced per m2 of forest canopy and calculated using average seed mass germination rate and early seedling survival moles et al 2004 appendix s1 fecundity is assumed to be zero if a cohort is reproductively immature if the cohort that died in the target cell was reproductively mature we assume sufficient within cell seed supply from that pft to support regeneration in the year after cohort mortality probability of dispersal equals 1 which allows us to implicitly account for surviving trees in the target grid cell and alternate regeneration strategies such as asexual resprouting and cone serotiny even the largest and most severe 20th century fires often generated mosaics of live and burned trees that ensured sufficient seed for regeneration in the year following fire harvey et al 2016 turner et al 1997 thus we felt this assumption was warranted given our goal of simulating 20th century forests and fire however fire severity is increasing with climate change such that seed limitation is a growing concern and future projections with dynafforest will require explicitly simulating within cell seed supply as a function of percent crown kill gill et al 2022 parks and abatzoglou 2020 steel et al 2018 we also calculate the probability of dispersal from pfts in the eight directly adjacent grid cells using a two part exponential equation seidl et al 2012a appendix s1 effects of temperature and drought on tree seedling establishment are then simulated following a mechanistic approach commonly implemented in process based forest models burton and cumming 1995 hansen et al 2018 nitschke and innes 2008 seidl et al 2012a fig 1c annual establishment probability is calculated for each pft with a non zero probability of dispersal as a function of temperature thresholds annual seed stratification is assessed based on a pft specific chilling day requirement of days between 5 and 5 c from the end of the previous growing season defined as september 30th in our model the coldest daily temperature during the year and the annual number of frost days number of days below 0 c also cannot exceed pft specific thresholds if germination requirements are met climate effects on early seedling survival are assessed minimum and maximum annual growing degree day thresholds ensure appropriate growing season length for seedlings to establish frost after bud burst and unusually dry soils jointly reduce probability of establishment representing frost induced damage johnson et al 2011 and drought induced mortality hansen and turner 2019 eq 1 e s t a b p r o b a b g t f r o s t t o l f r o s t a f t e r b u d g t p s i a v e g t p s i m i n p s i f i e l d p s i m i n where e s t a b p r o b a b g t is the probability of establishment due to abiotic factors in grid cell g and year t f r o s t t o l is a pft specific frost tolerance parameter f r o s t a f t e r b u d g t is the number of frost days that occur after bud break in grid cell g and year t p s i a v e g t is the mean growing season soil water potential in grid cell g and year t p s i m i n is the minimum soil water potential in which seedlings of a given pft can establish and p s i f i e l d is the soil water potential at field capacity probability of dispersal from the target and neighboring cells and probability of establishment are multiplied to calculate a total regeneration probability for each pft the pft with the highest regeneration probability gets first priority for regeneration we compare regeneration probability to a value between 0 and 1 randomly drawn from a uniform distribution if probability of regeneration exceeds the random number the pft with highest priority regenerates if not another random number is drawn and compared to the pft with the next highest regeneration probability this continues until either any pft with a nonzero regeneration probability is established or regeneration fails for all pfts if no new cohort establishes in the year following cohort mortality grid cells are converted to a grassland shrub land meadow ecosystem state hereafter grassland and the tree regeneration algorithms are called each year until a pft regenerates or the simulation completes the model is also capable of representing inhibition or facilitation of tree seedling establishment by grasses and shrubs appendix s1 2 2 3 tree growth annual tree height increment of each forest cohort is deterministically represented based on the prior year s height using a bertalanffy growth equation von bertalanffy 1957 as adapted by rammig et al 2007 fig 1c appendix s1 height growth rate is pft specific and not responsive to climate in our model we then use a pft specific tree height to diameter ratio to derive tree diameter at a height of 1 35 m dbh 2 2 4 stand structure temperate and boreal tree density often declines with stand age due to self thinning from intraspecific competition stand density in dynafforest is estimated from dbh using empirical self thinning relationships also known as reineke s r reineke 1933 following an approach similar to the us forest service s forest vegetation simulator fvs dixon 2015 fig 1c based on tree dbh and stand density live biomass pools in stems branches and leaves are simulated with pft specific allometric equations the leaf and branch biomass that dies and falls to the forest floor is also modeled with pool specific turnover rates that can be set individually based on pft specific rates table 1 2 2 5 dead biomass falling leaves and branches are immediately added to dead forest floor biomass pools fig 1c the model tracks three pools of dead plant material in each grid cell standing snags forest floor litter and downed coarse wood when a cohort dies from density independent mortality or drought see section 2 2 9 all leaf and branch biomass is added to the forest floor litter and coarse wood pools in the following year standing snag biomass is added to the coarse wood pool over time based on a pft specific snag half life seidl et al 2012b stenzel et al 2019 when a cohort is burned live leaf and branch pools are reduced proportional to percent crown kill see section 2 2 9 of the percent crown killed we assume 90 of live leaf biomass and 50 of live branch biomass is combusted and emitted to the atmosphere seidl et al 2014 the remainder is added to the litter and downed coarse wood pools if a cohort is killed by fire 100 crown kill is assumed for the purposes of calculating live leaf and branch biomass combustion and turnover to dead forest floor pools in the future this could be expanded to also represent percent crown kill via heating and scorching snags fall and enter the coarse wood pool according to their pft specific half life parameter following a fire the portions of dead forest floor fuel pools that were available for burning are assumed to be combusted whether or not the forest cohort occupying the grid cell is killed simulating surface fire in the stand seidl et al 2014 decomposition of each pool is calculated assuming a pool specific decomposition rate that is invariant with climate 2 2 6 fire characteristics the statistical fire module simulates fire characteristics at the resolution of 12 km grid cells before simulations can be run the dynafforest fire module is parameterized in a flexible framework where regressions are constructed to separately predict fire occurrence number and size as a function of variables that can vary depending on a priori understanding of the study domain we use model selection with aic to determine the most skilled and parsimonious models appendix s1 provides an in depth description of how the fire module was parameterized for forests of the western us first dynafforest probabilistically predicts whether one or more fires has occurred where the likelihood of fire increases as a function of variables like the amount and connectivity of fuels aridity and natural and human caused ignition density fig 1c eq 2 f i r e p r o b g t 1 1 e b 0 b 1 p r e d i c t o r 1 b 2 p r e d i c t o r 2 b n p r e d i c t o r n where f i r e p r o b is the probability of at least one fire occurring in grid cell g and year t and p r e d i c t o r 1 through p r e d i c t o r n are indices of fire probability selected in a stepwise multi variable logistic regression appendix s1 if one or more fires are predicted in a grid cell the module calculates the probability of different numbers of fires occurring based on eq 3 p r o b f i r e s n g t 1 1 e a b f i r e p r o b g t where p r o b f i r e s n g t is the probability of n fires occurring in grid cell g and year t n ranges from 1 to the maximum number of fires recorded in the simulation domain during the observational record and a and b are empirical coefficients determined from logistic regression using f i r e p r o b g t as a single predictor westerling et al 2011 the number of fires is then probabilistically selected maximum predicted size of each fire is determined based on an empirical stepwise multivariable regression that relates observed fire sizes to most of the same predictors used to estimate fireprob appendix s1 rather than predicting maximum fire size directly we predict cumulative distribution function cdf quantile values we then randomly select a fire size cdf value from the 200 observations in the empirical distribution that were closest to the predicted fire size quantile and back transform to the maximum fire size by assuming that each region s fire sizes come from a generalized pareto distribution thus if multiple fires are predicted to occur in a grid cell they can have different predicted maximum sizes despite identical predictor conditions maximum fire sizes can range from 100 ha up to the largest fire in the observational record which can exceed a 12 km grid cell 2 2 7 fire geography for each simulated fire dynafforest randomly selects a forested 1 km grid cell from within the 12 km fire grid cell as the ignition point fig 1c fire shapes are grown up to the predicted fire size using an algorithm where fires spread iteratively from the ignition point burning all neighboring 1 km cells that are classified as forest ecosystem i e grid cells that are forested or that are grassland but were initialized as forest or that are nonforested lands immediately adjacent to grid cells classified as forest ecosystem this allows us to account for forest fires that commonly spread through grasslands to another forest patch and for spotting where embers are carried in the air and set new fires if sufficient fuels exist fires can reach their maximum size predicted by climate which can exceed the 12 km fire grid however complex shapes emerge because non burnable grid cells may constrain fire spread which can cause fires to not reach their maximum predicted size if there are insufficient connected grid cells to burn 2 2 8 fire effects tremendous variability exists in the severity of forest fires across temperate and boreal forest biomes ranging from surface fires where mature trees survive to severe stand replacing crown fires that kill all trees dynafforest simulates fire severity for each burned 1 km grid cell as percent crown kill percent of the tree crown consumed by fire following an approach commonly implemented in process based forest models hansen et al 2020 schumacher et al 2006 seidl et al 2014 fig 1c eq 4 c k g t min k c k 1 k c k 2 d b h g t f u e l g t 1 where c k g t is the percent crown kill in burned grid cell g and year t k c k 1 and k c k 2 are empirically derived parameters from the literature that describe how tree sizes and available fuels determine how much of the crown is killed by fire and f u e l g t is the forest floor biomass available to burn in grid cell g and year t which is determined as a function of fuel moisture d b h g t is the dbh of the cohort in grid cell g and year t which is assumed to be 40 cm if the dbh of the cohort is larger than 40 cm 2 2 9 cohort mortality mortality of trees whether due to fire density independent mortality drought or myriad other factors is a key window of opportunity for ecological reorganization in temperate and boreal forests mortality events can break legacy locks associated with the dominance of long lived trees sometimes fostering shifts to alternate vegetation types that better match current abiotic conditions johnstone et al 2016 thus we implemented a probabilistic and process based approach to cohort mortality in dynafforest where trees can die from three causes 1 fire 2 density independent mortality and 3 drought stress implicitly representing death directly from hydraulic failure or increased vulnerability to biotic disturbance cohort mortality from fire is determined as eq 5 p m o r t g t 1 1 e 1 466 1 91 b t 0 1775 b t 2 5 41 c k g t 2 where p m o r t g t is the probability of cohort mortality from fire in burned grid cell g and year t b t is bark thickness calculated from dbh and a pft specific bark thickness coefficient and c k g t is percent crown kill eq 4 this is a commonly used equation in process based models hansen et al 2020 schumacher et al 2006 seidl et al 2014 dynafforest is capable of representing a range of fire severities from low severity surface fire to stand replacing crown fire in each 1 km grid cell however it cannot simulate mixed severity fire within a grid cell the model also currently assumes that all fires combust vegetation section 2 2 5 and does not represent smoldering fires density independent mortality is probabilistically represented following seidl et al 2012a where the chances of a cohort dying increases with their age size fig 1c eq 6 d i m g t 1 p l 1 a g e m a x where d i m g t is the probability of density independent mortality in cell g and year t p l is the pft specific probability that a tree survives to its maximum age a g e m a x the maximum age a pft commonly reaches in the field probability of cohort mortality also increases with soil drying represented in the model as growing season soil water potential in the rooting zone 0 100 cm soil depth relative to average soil moisture conditions and pft specific hydraulic safety margins i e difference between minimum xylem water potential typically experienced in the field and the xylem water potential at which trees experience a 50 loss in conductivity meinzer et al 2009 eq 7 s w m g t 1 1 e a p s i a v e g t p s i 50 g h s m b where s w m g t is this year s probability of stress due to low soil moisture in cell g and year t p s i a v e g t is the mean growing season soil water potential in grid cell g and year t h s m is the pft specific hydraulic safety margin a and b are empirical parameters p s i 50 g is median average growing season soil water potential between 2000 and 2019 in grid cell g the period in which hydraulic safety margins were calculated in the dataset used to parameterize the model 2 3 model evaluation 2 3 1 study regions we evaluated dynafforest by simulating forests and fires in the western us forced with mid 20th century climate we chose the western us because forest fire has long been a prevalent disturbance and climate change is causing fire activity to rapidly increase threatening people and ecosystems westerling 2016 the western us is also data rich for example the usda forest inventory and analysis fia network bechtold and patterson 2005 appendix s1 includes approximately 160 000 permanent plots across the united states designed to provide insights into forest condition this allowed us to develop a robust parameterization for pfts in the western us the wealth of data also ensured that we started simulations from relatively high quality initial conditions described in section 2 3 2 and that we could benchmark model runs against several independent datasets described in section 2 3 3 parameters for the western us are provided in appendix 2 tables s1 and s2 we selected five study regions that collectively include 56 of all forested area in the western us the coastal and inland pacific northwest of oregon and washington 148 000 km2 of forest hereafter pnw northern idaho 97 000 km2 of forest idaho the greater yellowstone ecosystem in southern montana and northern wyoming 47 500 km2 of forest gye the southern rockies including parts of utah colorado new mexico and arizona 135 000 km2 of forest southern rockies and the sierra nevada mountains 77 000 km2 of forest sierras fig 2 study regions were selected because they represent the range of forest types and fire regimes that characterize the western us from dry pinyon and ponderosa pine woodlands where frequent low severity fires burned every 5 15 years prior to euro american settlement to wet douglas fir forests of the pacific northwest where fire return intervals could exceed 700 years with infrequent yet extensive high severity stand replacing fire events we grouped tree species in the western us into 12 pfts following buotte et al 2018 and ruefenacht et al 2008 fig 2 appendix s1 table s1 and also included a grassland pft 2 3 2 simulation design we initialized the model with the gridded pft map from buotte et al 2018 a stand age map derived from remote sensing historical fire records and forest inventory plots pan et al 2011 and information on fuel loads based on forest type prichard et al 2019 initial cohort heights were derived from the stand age map using internal model equations appendix s1 fig 1b initial dbhs live biomass and stand densities were calculated from initial cohort heights the forest ecosystem module was forced with 1965 1994 daily temperature from the topowx dataset oyler et al 2015 and average growing season volumetric soil moisture in the rooting zone 0 to 100 cm depth which was calculated following methods from williams et al 2017 temperature data was used to calculate tree seedling germination and establishment thresholds section 2 2 2 volumetric soil moisture was used in fire severity equations section 2 2 8 and was also converted to soil water potential based on sand silt and clay from soilgrids250m 2 0 hengl et al 2017 for calculations of tree regeneration section 2 2 2 and cohort mortality section 2 2 9 inputs to the fire module included the 1984 2019 climatological mean annual aridity the ratio of total annual precipitation to total annual potential evapotranspiration williams et al 2020 topography slope angle and topographic complexity hastings and dubar 1999 and factors that influence fire ignition density the 1987 2019 mean lightning strike density and 1990 human population density cummins et al 1998 radeloff et al 2018 table s3 initial fuel loads were representative of the forest types found in our study regions but did not reflect spatial heterogeneity due to the past legacies of harvest fire or drought prichard et al 2019 thus we ran a 200 year spin up simulating the coupled response of vegetation to fire and climate in the five study regions to generate spatially heterogeneous fuels conditions consistent with internal model logic following hansen et al 2020 after the spin up we ran a 100 year experiment to benchmark model performance because several processes are probabilistic in the model including fire mortality and recruitment we ran five replicates of each region to account for model based variability no forest harvest was simulated 2 3 3 analyses expected patterns we benchmarked simulated forest characteristics from model year 300 and simulated fire activity from simulation years 201 300 classical tests of statistical significance are problematic with simulated data because large sample sizes can artificially inflate significance thus we used a pattern oriented modeling approach grimm et al 2005 patterns of several simulated variables at stand 1 km grid cell to western us scales were compared to observed datasets to evaluate model skill we compared modeled and observed distributions of tree sizes and cohort densities for each pft and region and biomass pools and fire regime characteristics for each region and across the western us pooling all five study regions data limitations constrained our ability to benchmark the model with completely independent observational datasets but we prioritized independence whenever possible table 2 we represented model distributions of variables by calculating their median inter quartile range iqr skewness and minimum and maximum for each pft when appropriate study region and pooling across all study regions for each of the five replicates we then calculated the average median iqr skewness and minimum and maximum values across the replicates to compare with observed distributions observed distributions of tree sizes stand densities and biomass pools came from fia plots that were classified as forested lands and sampled since 2000 when the usfs adopted a standard fixed radius plot design bechtold and patterson 2005 fia plots were not filtered based on disturbance history for tree sizes and stand densities model evaluation was conducted with the one third of fia plots that were not used in model parameterization appendix s1 we identified all fia plots dominated by the pfts represented in dynafforest and calculated the median tree height and dbh for each plot see appendix s1 for further details we then compared simulated distributions of heights and dbhs with the fia plots dominated by the same pft to ensure accurate comparison we limited our analysis to simulated cohorts with a dbh 2 54 cm the cutoff for tree measurement in fia protocols observed distributions of biomass pools came from the most recent sampling of the fia plots that were classified as forested lands estimated using the rfia package stanke et al 2020 because live and downed biomass routines in the forest ecosystem module were parameterized independently from fia appendix s1 we used all available fia plots in benchmarking biomass simulated annual median fire size annual number of fires fire perimeter shape complexity perimeter length to patch area ratio and annual total area burned were compared to observed fire records from the period 1985 1994 from the same database used in model parameterization see appendix s1 fires came from the western us mtbs interagency wumi wildfire database juang et al 2022 which includes large fires 404 ha from the us forest service s monitoring trends in burn severity database as well as a quality controlled list of mostly smaller fires 100 ha maintained by the national wildfire coordinating group e g keeley and syphard 2017 westerling et al 2011 because we wanted to test the fire module s ability to generate fire characteristics consistent with the mid 20th century fire regime we chose 1985 1994 to exclude more recent observations during years where climate change has profoundly altered fire activity in the western us abatzoglou and williams 2016 westerling 2016 this temporal window is best suited for our current purposes because of 1 the broad spatial extent of our modeling that captures tremendous variability in fuels aridity climate ignition patterns and fire regime characteristics and 2 because our goal in this paper was to develop a model that could generate fire consistent with mid 20th century fire seasons subsequent development of the fire module for forecasting interannual effects of climate on fire will require more advanced statistical models and benchmarking methods the simulated percent of area that burned at high severity each year was independently benchmarked against a remotely sensed burn severity product parks and abatzoglou 2020 the database provides composite burn index cbi for all fires 404 ha in the observational record at a 30 m spatial resolution to ensure the remotely sensed product was comparable to model outputs we first included only portions of 1985 1994 fires that fell within our initial simulated forested areas we then masked out portions of those fires where the prefire ndvi was below 0 35 to exclude non forested vegetation potentially misclassified as forest parks and abatzoglou 2020 we aggregated the 30 m cbi estimates within each observed fire to a 1 km2 resolution we used a cbi value of 2 25 equivalent to 95 tree mortality miller et al 2009 as the cutoff to define stand replacing fire in each grid cell we then calculated the percent of annual burned area that was stand replacing for each region we report results comparing the distributions of simulated and observed variables in three ways to evaluate how well dynafforest captures the central tendency of each variable s observed distribution we compared the median value and iqr from observations to the averaged median value and iqr from the simulation replicates at the pft and or regional and western us scales to compare modeled and observed distributional shapes we compared the skewness of variable distributions to determine how well dynafforest represents the observed variability in each variable we quantified what percent of observations fell between the averaged minimum and maximum values from the simulation replicates while we assessed model skill based on the ability of dynafforest to accurately recreate observed distributions we recognize many other factors not included in our model affect real world forests such as bark beetle outbreaks forest restoration and timber harvest berner et al 2017 thus we anticipated that these unaccounted for drivers of forest dynamics would generate some divergence between simulated and observed central tendencies distributional shapes and ranges of variability benchmarking analyses were conducted in r statistical software v 4 0 4 core team 2021 using the packages rfia stanke et al 2020 tidyverse wickham et al 2019 ncdf4 pierce 2017 landscapemetrics hesselbarth et al 2019 sf pebesma 2018 dbplyr wickham and ruiz 2020 moments lukasz and novomestky 2015 and raster hijmans 2021 simulations and processing of model outputs were run on the amarel cluster at rutgers university 3 results 3 1 tree size stand density and forest cover among pfts modeled median tree heights and dbhs differed from observations of the same pft by 16 and 20 on average averaged across pfts the skew of modeled and observed tree heights was 1 3 and 1 2 and the skew of modeled and observed dbhs was 1 3 and 2 0 table 3 model skill in representing tree heights and dbhs varied across pfts fig 3 for example simulated aspen trees were shorter on average and had smaller diameters than observed aspen while modeled inland douglas fir tended to be larger than observations dynafforest captured much of the variability in observed tree heights and dbhs however on average 68 of observations fell within the ranges of simulated tree heights and dbhs of the same pft some variability also existed between regions in how well the model captured observed median tree sizes figs s1 and s2 for example modeled median heights of mixed firs closely matched the observed medians in idaho pnw and sierras but aspen trees in dynafforest were considerably taller than observations in the southern rockies fig s1 modeled median stand density was within 39 of the observed median stand density of the same pft on average the skew of modeled and observed stand densities was 4 3 and 4 8 table 3 the median simulated stands of engelmann spruce fir hemlock cedar five needle pine and lodgepole pine were moderately denser than observations while the median simulated stands of mixed conifer coastal douglas fir and inland douglas fir were less dense fig 4 an average of 62 of observations fell within the range of simulated stand densities of the same pft model skill also varied among regions fig s3 median inland douglas fir densities in the model for example were lower than observed medians in the gye idaho pnw and southern rockies but were higher than observed in the sierras fig s3 eight percent of initial forest area converted to grassland by simulation year 300 on average across the five study regions the grassland pft was most common in the sierras where it replaced 15 of initial forest area followed by the pnw 9 southern rockies 8 gye 5 and was least common in idaho where it replaced 4 of forest area after 300 years of simulation expansion of grassland was unsurprising as the pft distributions used to initialize simulations did not include grassland and it is often interspersed within western forest ecosystems 3 2 biomass pools pooling all study regions together there was strong model observation correspondence in live biomass the average median leaf and aboveground live wood biomass from the five simulation replicates were within 29 and 12 of the observed medians respectively table 3 fig 5 a the skew of modeled and observed leaf biomass was 1 4 and 2 2 and the skew of modeled and observed aboveground live wood biomass was 1 1 and 2 3 table 3 the model also represented observed variability in live biomass well across all study regions 90 and 92 of observations fell within the range of simulations for leaf and aboveground woody biomass median live aboveground tree biomass was highest in the idaho study region followed closely by the pnw gye sierras and southern rockies table s4 fig 6 when model outputs were compared with observations within regions median leaf biomass was underestimated by dynafforest in the pnw and was overestimated in the other study regions table s4 modeled median live aboveground woody biomass was lower than observations in the pnw and sierras and was higher in the southern rockies idaho and gye average median litter and coarse wood biomass were within 5 and 7 of the median observed values respectively fig 5b the skew of modeled and observed litter biomass was 1 7 and 5 3 and the skew of modeled and observed aboveground coarse wood biomass was 2 5 and 3 4 table 3 across all study regions 98 and 100 of observations fell within the range of simulations for downed litter and coarse wood pools suggesting the model could reasonably represent the range of variability in downed biomass however variability did exist in model observation correspondence when broken out by study region table s1 fig s4 dynafforest underestimated median litter biomass in the sierras and pnw and overestimated median litter biomass in the other regions modeled median coarse wood biomass was also lower than in the observational dataset in the pnw idaho and gye and larger than observed in the sierras and southern rockies 3 3 fire regime characteristics pooling all study regions together the annual median number of fires fire size perimeter to area ratio annual area burned and percent of annual burned area that was stand replacing from simulations differed from observed medians by 4 86 22 53 and 31 respectively table 3 fig 7 dynafforest overestimated the annual median fire size but also did not simulate rare extremely large observed fire years such as the 1988 yellowstone fires fig s5 which caused the model to underestimate median annual area burned fig 7b and may have contributed to the model s over representation of live biomass in the gye fig 6 approximately half of observations fell within the range of simulations for the annual number of fires and area burned while only 36 of observations fell within the range of simulations for annual median fire size 82 and 98 of observations fell within the range of simulations for the perimeter to area ratio and the percent of annual burned area that was stand replacing respectively substantial variability in model observation correspondence existed when evaluating the fire module within study regions table s4 fig s5 for example annual area burned was most strongly underestimated by the model in the gye where the iconic 1988 yellowstone fires took place reinforcing the importance of rare extremely large fire years for shaping model observation disparities in fire characteristics 4 discussion fires and forests dynamically influence one another through disturbance succession cycles parks et al 2015 prichard et al 2017 where when and why fire induced changes to temperate and boreal forests may feedback to affect subsequent burning remains poorly resolved particularly at regional to continental scales abatzoglou et al 2021 hurteau et al 2019 here we present a new approach to address this challenge dynafforest combines a hybrid probabilistic and deterministic process based forest ecosystem module with a flexible statistical fire module benchmarking results demonstrate that the forest ecosystem module reasonably recreates patterns of diverse forest characteristics across broad spatial domains 850 000 km2 of forest in the western us the fire module as currently conceptualized captures average 20th century fire characteristics in five study regions with highly variable fire regimes however the fire module misses the extremely large fire events that were relatively rare over the last century because we did not include effects of temporal climate variability on fire occurrence and size in this first version our baseline model provides proof of concept and will continue to be refined and expanded to meet the need for bottom up forest and fire projections that can inform forest management and adaptation strategies filotas et al 2014 messier et al 2015 for example dynafforest could be used in the future to evaluate where when and at what spatial scales different combinations of thinning and prescribed fire could decouple 21st century climate fire relationships a key objective of the forest ecosystem module was to explicitly include the demographic processes through which climate and fire affect forests and initiate feedbacks that can influence subsequent fire we accomplished this in a computationally efficient manner by taking a probabilistic approach for representing a few key processes such as seed dispersal seedling establishment and tree mortality while deterministically representing others based on successional stage the model generated patterns of forest cover and structure consistent with observations across five diverse study regions importantly the model also captured examples of type conversions the pft maps used to initialize simulations did not include grassland even though it is commonly interspersed among some forests of the western us however simulated forests that experienced low soil moisture and or unusually frequent and large high severity fires converted to nonforest in all study regions generating realistic forest grassland mosaics at low elevations in the sierras and southern rockies study regions and in southern and eastern oregon part of the pnw study region such patterns are consistent with recent analyses of drought induced forest conversion davis et al 2019 and highlight how dynafforest could be a powerful tool for quantifying 21st century forest resilience and transformation by identifying the climate fire tipping points that initiate wide spread conversion to grassland coop et al 2020 hansen et al 2020 hansen and turner 2019 we set out to generate a parameter set for the forest ecosystem module that was robust across all of the western us and dynafforest was particularly skilled at capturing patterns of forest structure when outputs from the five study regions were pooled to the western us scale however variation in model observation correspondence within study regions highlights inevitable tradeoffs between maximizing model realism and distilling immense ecological complexity into a tractable system representation for example tree size and stand density were deterministic in dynafforest based solely on the previous year s state this means that myriad abiotic and biotic factors such as climate nutrient availability and interspecific competition were not explicitly considered thus we expected dynafforest to overestimate tree growth and stand density on the trailing and leading edges of pft ranges where environmental conditions are harsh arid cold indeed the model did not capture the lowest density stands observed for most pfts fig 4 this may explain some of the variability in model observation correspondence between study regions as well as any mismatch between simulated and observed distribution shapes as quantified by skewness of pft tree size and stand density even when there was agreement for medians and iqrs of distributions trait values also vary tremendously across pft ranges anderegg et al 2018 messier et al 2010 2017 and regional variability in model performance highlights how pft parameterizations must be tailored to the scale at which questions are being asked dynafforest includes a flexible statistical fire module that can be adapted to meet the needs of many different applications here we implemented a relatively simple representation where fire occurrence and size were only sensitive to forest biomass and spatial variability in mean aridity lightning frequency topography and human population but did not respond to inter annual variability in climate or within season extreme fire weather this explains why the model was modestly skilled at capturing the average characteristics of 20th century forest fires but did not simulate as much inter annual variability in fire activity as observed in reality inter annual variability of atmospheric aridity is a dominant driver of burned area in temperate and boreal forests abatzoglou and williams 2016 higuera and abatzoglou 2020 seidl et al 2020 an essential next step in the development of dynafforest is to represent how temporal changes in climate affect fire occurrence and size even now however dynafforest includes far more realism in its representation of fire than many other models operating at this scale for example of nine esms evaluated in the fire inter model comparison project nearly half prescribed vegetation rather than dynamically coupling fire and forests only one third explicitly simulated individual fires instead of aggregate burned area and all operated at coarse spatial resolutions 0 5 grid cells meaning they were incapable of representing fine scale spatial heterogeneity within individual fire perimeters hantson et al 2020 rabin et al 2017 in contrast dynafforest explicitly couples fire and vegetation dynamics simulates individual fire occurrence and size as a function of human and natural drivers and dynamically grows individual fires at a 1 km spatial resolution based on fuel availability the model also simulates heterogeneity of fire severity within burned perimeters thus dynafforest is uniquely capable of capturing the complex finer scale patterns generated by fires across broad spatial domains as compared with many other models 5 conclusions we present a new model for simulating the dynamic interactions and feedbacks between fire and temperate and boreal forests our approach fills a unique need by 1 explicitly representing the key forest demographics through which fire affects forests 2 capturing the spatially heterogeneous effects of individual fires and 3 simulating regional to continental domains for decades to centuries such tools are desperately needed by scientists and decision makers to determine where when and why fire induced changes to forests may feedback to affect subsequent burning mcwethy et al 2019 and affect regional forest conditions better constrained projections of changing fire and forests could inform policy and management strategies designed to help people more sustainably coexist with fire now and in the future cochrane and bowman 2021 to focus on important questions of forest carbon biodiversity conservation forest economics and fire hazard software and data availability software name dynafforest dynamic temperate and boreal fire and forest ecosystem simulator developer winslow d hansen first year available 2022 hardware requirements pc mac software requirements r statistical environment and language program language r program size 121 5 kb availability http forestfutureslab org dynafforest license gpl 3 0 archive with data from benchmarking data archive can be found at 10 25390 caryinstitute 20452386 size of archive 157 mb author contributions wdh developed the model with input from apw wdh conducted the benchmarking exercises with input from all coauthors wdh wrote the paper and all coauthors contributed to revisions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements wdh acknowledges support from the gordon and betty moore foundation grant gbmf10763 the royal bank of canada the environmental defense fund and the national science foundation grant opp 2116863 apw acknowledges support from the zegar family foundation and doe grant desc0022302 att acknowledges funding from the nsf grants 2003205 and 2017949 the usda national institute of food and agriculture agricultural and food research initiative competitive programme grant no 2018 67012 31496 and the university of california laboratory fees research program award no lfr 20 652467 we thank rupert seidl and werner rammer for sharing their expertise on the development of forest simulation models appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105473 
25536,fire is a dominant disturbance in temperate and boreal biomes and increasing burned area with climate change may fundamentally alter forests improved information about how fire induced changes to forests may feedback to affect subsequent burning at regional scales could inform forest management and climate mitigation strategies however fire is simplistically represented in earth system models and regional statistical fire models often assume sufficient fuels contributing to uncertainty in future projections to address this challenge we developed the dynamic temperate and boreal fire and forest ecosystem simulator dynafforest dynafforest represents the hierarchical structuring of forests from individual cohorts to continental extents making it possible to simulate feedbacks between fire and forests at broad scales over decades to centuries we parameterized dynafforest for the western united states of america and benchmarked simulations with observations dynafforest recreated patterns of forest cover structure and downed fuels and was capable of capturing average 20th century fire activity keywords climate change ecosystem modeling forest resilience natural disturbance scaling wildfire 1 introduction forest fire is a prevalent natural disturbance in terrestrial ecosystems and is sensitive to climate particularly in temperate and boreal forest biomes seidl et al 2020 trends toward warmer more arid conditions are causing fire frequency size and severity to rapidly increase in many places abatzoglou and williams 2016 kelly et al 2013 westerling 2016 for example annual burned forest area has grown 1 100 since 1984 in the western conterminous united states of america hereafter western us williams et al 2022 current climate fire trends will almost certainly continue over the next few decades abatzoglou et al 2021 with large consequences for people and ecosystems coop et al 2020 mcwethy et al 2019 schoennagel et al 2017 while climate is a dominant cause of increasing fire activity other factors also contribute to recent trends including spatiotemporal variability in fuel loads sometimes related to legacies of fire suppression and human caused ignitions balch et al 2017 calef et al 2008 hagmann et al 2021 many of these drivers are not well accounted for in disturbance succession models of forests particularly at broader scales nor are the complex feedbacks between fire and its drivers therefore new quantitative tools that incorporate multiple drivers and feedbacks could yield important insights into the trends causes and consequences of forest fire one of the most important feedbacks that models must better capture is how fires can alter forests in ways that affect the likelihood of subsequent burning two pathways exist through which these fire forest feedbacks play out first fire combusts fuels which decreases the probability of another fire for a period in the western us reduced fire probability can last 5 30 years after the initial burn depending on climate and extreme weather events parks et al 2018 second increased burning can initiate transitions from forests to alternate vegetation communities that differ in their flammability johnstone et al 2016 tepley et al 2018 for example less flammable deciduous tree species now commonly replace black spruce after unusually severe fire in the north american boreal forest scientists and practitioners need improved information about where when and why fire induced changes to forests may feedback to affect subsequent burning otherwise known as re burns prichard et al 2017 schoennagel et al 2017 this need for improved modeling is particularly true across regional to continental scales becknell et al 2015 turetsky et al 2017 walker et al 2018 the spatial scales at which policy and management are commonly targeted and impending ecological change could alter broader earth system functions heffernan et al 2014 rose et al 2017 most models capable of simulating broad spatial domains 106 km2 ignore or simplistically represent fire and or do not explicitly represent forests and their dynamics for example earth system models esms were designed to simulate the role of vegetation in regional to global carbon and water cycling but have a coarse spatial resolution 0 5 and often do not include fire hantson et al 2020 rabin et al 2017 of the few esms with a dynamic fire module most only simulate aggregate burned area and do not include the demographic processes that underpin forest recovery like tree seed dispersal and seedling establishment fisher et al 2018 hantson et al 2020 sanderson and fisher 2020 at regional scales statistical models of fire are often used that operate at a finer spatial grain 6 12 km and represent the multi step process of burning more realistically than esms by relating observed climate to individual fire characteristics such as occurrence size and severity keyser and westerling 2019 westerling et al 2011 however statistical fire models rarely simulate forest change and assume sufficient fuel and unchanging forest cover when projecting future fire use rules to define the duration after a fire in which a cell cannot burn employ empirical functions to implicitly represent dynamic patterns of postfire fuel limitation or implement simple statistical vegetation climate relationships to represent potential change in fuels with climate abatzoglou et al 2021 kitzberger et al 2017 littell et al 2018 numerous forest landscape models include both fire and detailed representations of forest dynamics such models capture the necessary demographic processes and simulate feedbacks to fire albrich et al 2020 hansen et al 2020 hurteau et al 2019 seidl et al 2014 serra diaz et al 2018 but are computationally expensive and most cannot be run for broad domains though see rammer and seidl 2019 to address the dearth of quantitative tools capable of modeling temperate and boreal fire and forests across regions to continents we present a new model called the dynamic temperate and boreal fire and forest ecosystem simulator dynafforest we designed dynafforest to simulate complex and interacting causes of fire and to represent the demographic processes that underpin postfire forest recovery thus the model is capable of simulating dynamic feedbacks between fire and forests dynafforest is computationally efficient such that it can be run with a sufficiently fine spatial resolution 1 km to capture heterogeneity in vegetation size classes structure and stand ages in topographically complex landscapes the model is capable of simulating broad domains of similar scope to the forested area of the western us 850 000 km2 and operates at an annual time step in this paper we describe dynafforest and evaluate model skill by comparing simulations of western us forests under mid 20th century climate conditions with multiple independent benchmarking datasets 2 materials and methods 2 1 model overview our objective was to represent how forest fires and climate affect temperate and boreal forest ecosystems across regions to continents and how fire induced changes to forests feedback to alter subsequent fires dynafforest is broken into two modules a forest ecosystem module and a fire module fig 1 we constructed dynafforest with a hierarchical structure where the forest ecosystem module operates at a 1 km spatial resolution and the fire module operates at a 12 km resolution the time step of both modules is annual the forest ecosystem module probabilistically represents key processes through which fire and climate affect forests and uses deterministic equations to represent other forest processes based on age successional stage fig 1c forest types can be parameterized at the level of specificity necessary for the application from individual species to coarse plant functional types pfts we simulate broad spatial domains with a fine grain size using a cohort based approach that tracks a single cohort for each 1 km grid cell the cohort has an age dependent size and density based on pft specific traits the approach is computationally efficient as only one cohort per grid cell must be tracked it also still allows for representation of heterogeneity in forest cover composition and structure at a 1 km resolution which is finer than other models capable of simulating forests across comparable domains buotte et al 2018 fisher et al 2018 however because one pft cohort per grid cell is tracked the model does not explicitly represent relay successional trajectories where transitions from the initial fast growing pioneer tree species or shrubs to slow growing late successional shade tolerant species may alter stand flammability with time since disturbance e g tepley et al 2018 probabilistic process based components are inspired by and closely follow the forest model iland seidl et al 2012a which has been widely applied in temperate and boreal forests braziunas et al 2018 hansen et al 2018 2020 2021 seidl et al 2014 deterministic components are derived from several sources e g dixon 2015 rammig et al 2007 a technical description of the forest ecosystem module is provided in sections 2 2 2 5 section 2 9 and appendix s1 we paired the forest ecosystem module with a statistically based fire module that predicts the occurrence number and sizes of fires 100 ha as a function of variables including the amount and connectivity of fuels from the forest module spatial variation in climate aridity natural and human caused ignitions and topography appendix s1 the module is designed so users can parameterize it based on an a priori understanding of their own system fire occurrence number and maximum size are simulated on a grid with a spatial resolution of 12 km fires are then randomly ignited in a forested 1 km grid cell within the 12 km fire cell and iteratively spread to other forested 1 km grid cells until no more forested cells remain or the maximum size predicted by the fire module is reached fire effects such as cohort mortality and combustion of live biomass and fuel loads are calculated at the 1 km resolution of the forest ecosystem module because our goal in this initial development and presentation of dynafforest was to establish whether the approach could accurately simulate forest dynamics when dynamically coupled with a module that produced 20th century fire seasons the fire module implemented here does not include fire response to temporal climate variability instead it represents responses of fire to spatiotemporal changes in forest biomass and spatial variations in long term mean aridity and ignition sources as such it is inevitable that this relatively simple baseline fire module will under represent the frequency of very large wildfires which are strongly promoted by the extreme climate anomalies that have occurred more frequently in recent years with human caused warming a technical description of the fire module is provided in sections 2 2 6 2 2 8 and appendix s1 along with next steps for future development together the forest ecosystem and fire modules provide a powerful framework for addressing previously intractable questions about how feedbacks emerge to shape temperate and boreal forest and fire outcomes the flexibility of the fire module means it can be adjusted to fit a wide range of applications the forest ecosystem module takes a unique approach explicitly representing some key forest processes with great detail e g tree regeneration and mortality and deterministically representing others e g tree growth and stand density parameters for the model table 1 can be attained from the literature and public databases for most study areas globally dynafforest currently only represents fire but other natural and human caused disturbances such as bark beetle outbreaks restoration treatments and logging are also important in temperate and boreal forests berner et al 2017 morris et al 2016 ruess et al 2021 the modular design of dynafforest makes it easy to develop or plug in future modules representing other disturbance agents and management activities honkaniemi et al 2021 rammer and seidl 2015 2 2 detailed model description 2 2 1 technical implementation we aimed to develop a model that is adaptable for a wide range of applications and useable by researchers with varying expertise in programming thus we wrote dynafforest in the r language and environment for statistical computing v 4 0 4 core team 2021 while other programming languages offer superior computational efficiency e g c many ecologists and environmental scientists are experienced with r and the software is open source making it an ideal choice for encouraging widespread use and rapid community based evolution of the model full source code and documentation is available under a gnu general public license gnu gpl www gnu org licenses gpl 3 0 html 2 2 2 tree regeneration stem density and species composition of tree seedlings that establish after severe fire shape successional trajectories for decades to centuries kashian et al 2005 postfire tree regeneration can be compromised if fires recur before trees reach reproductive maturity brown and johnstone 2012 buma et al 2013 if the size of severely burned patches exceeds dispersal distance from the unburned edge or fire refugia gill et al 2022 if winter snowpack or cold temperatures reduce germination rates e g at high elevation treeline brown et al 2019 kueppers et al 2017 or if postfire drought kills tree seedlings davis et al 2019 hansen and turner 2019 because tree regeneration is essential for postfire recovery we took a probabilistic process based approach tree regeneration is simulated in dynafforest starting the year after a forest cohort dies from density independent mortality factors related to tree age size fire or drought and continues annually until another cohort establishes or the simulation ends fig 1c seed supply is modeled based on pft specific fecundity which is the number of potential seedlings produced per m2 of forest canopy and calculated using average seed mass germination rate and early seedling survival moles et al 2004 appendix s1 fecundity is assumed to be zero if a cohort is reproductively immature if the cohort that died in the target cell was reproductively mature we assume sufficient within cell seed supply from that pft to support regeneration in the year after cohort mortality probability of dispersal equals 1 which allows us to implicitly account for surviving trees in the target grid cell and alternate regeneration strategies such as asexual resprouting and cone serotiny even the largest and most severe 20th century fires often generated mosaics of live and burned trees that ensured sufficient seed for regeneration in the year following fire harvey et al 2016 turner et al 1997 thus we felt this assumption was warranted given our goal of simulating 20th century forests and fire however fire severity is increasing with climate change such that seed limitation is a growing concern and future projections with dynafforest will require explicitly simulating within cell seed supply as a function of percent crown kill gill et al 2022 parks and abatzoglou 2020 steel et al 2018 we also calculate the probability of dispersal from pfts in the eight directly adjacent grid cells using a two part exponential equation seidl et al 2012a appendix s1 effects of temperature and drought on tree seedling establishment are then simulated following a mechanistic approach commonly implemented in process based forest models burton and cumming 1995 hansen et al 2018 nitschke and innes 2008 seidl et al 2012a fig 1c annual establishment probability is calculated for each pft with a non zero probability of dispersal as a function of temperature thresholds annual seed stratification is assessed based on a pft specific chilling day requirement of days between 5 and 5 c from the end of the previous growing season defined as september 30th in our model the coldest daily temperature during the year and the annual number of frost days number of days below 0 c also cannot exceed pft specific thresholds if germination requirements are met climate effects on early seedling survival are assessed minimum and maximum annual growing degree day thresholds ensure appropriate growing season length for seedlings to establish frost after bud burst and unusually dry soils jointly reduce probability of establishment representing frost induced damage johnson et al 2011 and drought induced mortality hansen and turner 2019 eq 1 e s t a b p r o b a b g t f r o s t t o l f r o s t a f t e r b u d g t p s i a v e g t p s i m i n p s i f i e l d p s i m i n where e s t a b p r o b a b g t is the probability of establishment due to abiotic factors in grid cell g and year t f r o s t t o l is a pft specific frost tolerance parameter f r o s t a f t e r b u d g t is the number of frost days that occur after bud break in grid cell g and year t p s i a v e g t is the mean growing season soil water potential in grid cell g and year t p s i m i n is the minimum soil water potential in which seedlings of a given pft can establish and p s i f i e l d is the soil water potential at field capacity probability of dispersal from the target and neighboring cells and probability of establishment are multiplied to calculate a total regeneration probability for each pft the pft with the highest regeneration probability gets first priority for regeneration we compare regeneration probability to a value between 0 and 1 randomly drawn from a uniform distribution if probability of regeneration exceeds the random number the pft with highest priority regenerates if not another random number is drawn and compared to the pft with the next highest regeneration probability this continues until either any pft with a nonzero regeneration probability is established or regeneration fails for all pfts if no new cohort establishes in the year following cohort mortality grid cells are converted to a grassland shrub land meadow ecosystem state hereafter grassland and the tree regeneration algorithms are called each year until a pft regenerates or the simulation completes the model is also capable of representing inhibition or facilitation of tree seedling establishment by grasses and shrubs appendix s1 2 2 3 tree growth annual tree height increment of each forest cohort is deterministically represented based on the prior year s height using a bertalanffy growth equation von bertalanffy 1957 as adapted by rammig et al 2007 fig 1c appendix s1 height growth rate is pft specific and not responsive to climate in our model we then use a pft specific tree height to diameter ratio to derive tree diameter at a height of 1 35 m dbh 2 2 4 stand structure temperate and boreal tree density often declines with stand age due to self thinning from intraspecific competition stand density in dynafforest is estimated from dbh using empirical self thinning relationships also known as reineke s r reineke 1933 following an approach similar to the us forest service s forest vegetation simulator fvs dixon 2015 fig 1c based on tree dbh and stand density live biomass pools in stems branches and leaves are simulated with pft specific allometric equations the leaf and branch biomass that dies and falls to the forest floor is also modeled with pool specific turnover rates that can be set individually based on pft specific rates table 1 2 2 5 dead biomass falling leaves and branches are immediately added to dead forest floor biomass pools fig 1c the model tracks three pools of dead plant material in each grid cell standing snags forest floor litter and downed coarse wood when a cohort dies from density independent mortality or drought see section 2 2 9 all leaf and branch biomass is added to the forest floor litter and coarse wood pools in the following year standing snag biomass is added to the coarse wood pool over time based on a pft specific snag half life seidl et al 2012b stenzel et al 2019 when a cohort is burned live leaf and branch pools are reduced proportional to percent crown kill see section 2 2 9 of the percent crown killed we assume 90 of live leaf biomass and 50 of live branch biomass is combusted and emitted to the atmosphere seidl et al 2014 the remainder is added to the litter and downed coarse wood pools if a cohort is killed by fire 100 crown kill is assumed for the purposes of calculating live leaf and branch biomass combustion and turnover to dead forest floor pools in the future this could be expanded to also represent percent crown kill via heating and scorching snags fall and enter the coarse wood pool according to their pft specific half life parameter following a fire the portions of dead forest floor fuel pools that were available for burning are assumed to be combusted whether or not the forest cohort occupying the grid cell is killed simulating surface fire in the stand seidl et al 2014 decomposition of each pool is calculated assuming a pool specific decomposition rate that is invariant with climate 2 2 6 fire characteristics the statistical fire module simulates fire characteristics at the resolution of 12 km grid cells before simulations can be run the dynafforest fire module is parameterized in a flexible framework where regressions are constructed to separately predict fire occurrence number and size as a function of variables that can vary depending on a priori understanding of the study domain we use model selection with aic to determine the most skilled and parsimonious models appendix s1 provides an in depth description of how the fire module was parameterized for forests of the western us first dynafforest probabilistically predicts whether one or more fires has occurred where the likelihood of fire increases as a function of variables like the amount and connectivity of fuels aridity and natural and human caused ignition density fig 1c eq 2 f i r e p r o b g t 1 1 e b 0 b 1 p r e d i c t o r 1 b 2 p r e d i c t o r 2 b n p r e d i c t o r n where f i r e p r o b is the probability of at least one fire occurring in grid cell g and year t and p r e d i c t o r 1 through p r e d i c t o r n are indices of fire probability selected in a stepwise multi variable logistic regression appendix s1 if one or more fires are predicted in a grid cell the module calculates the probability of different numbers of fires occurring based on eq 3 p r o b f i r e s n g t 1 1 e a b f i r e p r o b g t where p r o b f i r e s n g t is the probability of n fires occurring in grid cell g and year t n ranges from 1 to the maximum number of fires recorded in the simulation domain during the observational record and a and b are empirical coefficients determined from logistic regression using f i r e p r o b g t as a single predictor westerling et al 2011 the number of fires is then probabilistically selected maximum predicted size of each fire is determined based on an empirical stepwise multivariable regression that relates observed fire sizes to most of the same predictors used to estimate fireprob appendix s1 rather than predicting maximum fire size directly we predict cumulative distribution function cdf quantile values we then randomly select a fire size cdf value from the 200 observations in the empirical distribution that were closest to the predicted fire size quantile and back transform to the maximum fire size by assuming that each region s fire sizes come from a generalized pareto distribution thus if multiple fires are predicted to occur in a grid cell they can have different predicted maximum sizes despite identical predictor conditions maximum fire sizes can range from 100 ha up to the largest fire in the observational record which can exceed a 12 km grid cell 2 2 7 fire geography for each simulated fire dynafforest randomly selects a forested 1 km grid cell from within the 12 km fire grid cell as the ignition point fig 1c fire shapes are grown up to the predicted fire size using an algorithm where fires spread iteratively from the ignition point burning all neighboring 1 km cells that are classified as forest ecosystem i e grid cells that are forested or that are grassland but were initialized as forest or that are nonforested lands immediately adjacent to grid cells classified as forest ecosystem this allows us to account for forest fires that commonly spread through grasslands to another forest patch and for spotting where embers are carried in the air and set new fires if sufficient fuels exist fires can reach their maximum size predicted by climate which can exceed the 12 km fire grid however complex shapes emerge because non burnable grid cells may constrain fire spread which can cause fires to not reach their maximum predicted size if there are insufficient connected grid cells to burn 2 2 8 fire effects tremendous variability exists in the severity of forest fires across temperate and boreal forest biomes ranging from surface fires where mature trees survive to severe stand replacing crown fires that kill all trees dynafforest simulates fire severity for each burned 1 km grid cell as percent crown kill percent of the tree crown consumed by fire following an approach commonly implemented in process based forest models hansen et al 2020 schumacher et al 2006 seidl et al 2014 fig 1c eq 4 c k g t min k c k 1 k c k 2 d b h g t f u e l g t 1 where c k g t is the percent crown kill in burned grid cell g and year t k c k 1 and k c k 2 are empirically derived parameters from the literature that describe how tree sizes and available fuels determine how much of the crown is killed by fire and f u e l g t is the forest floor biomass available to burn in grid cell g and year t which is determined as a function of fuel moisture d b h g t is the dbh of the cohort in grid cell g and year t which is assumed to be 40 cm if the dbh of the cohort is larger than 40 cm 2 2 9 cohort mortality mortality of trees whether due to fire density independent mortality drought or myriad other factors is a key window of opportunity for ecological reorganization in temperate and boreal forests mortality events can break legacy locks associated with the dominance of long lived trees sometimes fostering shifts to alternate vegetation types that better match current abiotic conditions johnstone et al 2016 thus we implemented a probabilistic and process based approach to cohort mortality in dynafforest where trees can die from three causes 1 fire 2 density independent mortality and 3 drought stress implicitly representing death directly from hydraulic failure or increased vulnerability to biotic disturbance cohort mortality from fire is determined as eq 5 p m o r t g t 1 1 e 1 466 1 91 b t 0 1775 b t 2 5 41 c k g t 2 where p m o r t g t is the probability of cohort mortality from fire in burned grid cell g and year t b t is bark thickness calculated from dbh and a pft specific bark thickness coefficient and c k g t is percent crown kill eq 4 this is a commonly used equation in process based models hansen et al 2020 schumacher et al 2006 seidl et al 2014 dynafforest is capable of representing a range of fire severities from low severity surface fire to stand replacing crown fire in each 1 km grid cell however it cannot simulate mixed severity fire within a grid cell the model also currently assumes that all fires combust vegetation section 2 2 5 and does not represent smoldering fires density independent mortality is probabilistically represented following seidl et al 2012a where the chances of a cohort dying increases with their age size fig 1c eq 6 d i m g t 1 p l 1 a g e m a x where d i m g t is the probability of density independent mortality in cell g and year t p l is the pft specific probability that a tree survives to its maximum age a g e m a x the maximum age a pft commonly reaches in the field probability of cohort mortality also increases with soil drying represented in the model as growing season soil water potential in the rooting zone 0 100 cm soil depth relative to average soil moisture conditions and pft specific hydraulic safety margins i e difference between minimum xylem water potential typically experienced in the field and the xylem water potential at which trees experience a 50 loss in conductivity meinzer et al 2009 eq 7 s w m g t 1 1 e a p s i a v e g t p s i 50 g h s m b where s w m g t is this year s probability of stress due to low soil moisture in cell g and year t p s i a v e g t is the mean growing season soil water potential in grid cell g and year t h s m is the pft specific hydraulic safety margin a and b are empirical parameters p s i 50 g is median average growing season soil water potential between 2000 and 2019 in grid cell g the period in which hydraulic safety margins were calculated in the dataset used to parameterize the model 2 3 model evaluation 2 3 1 study regions we evaluated dynafforest by simulating forests and fires in the western us forced with mid 20th century climate we chose the western us because forest fire has long been a prevalent disturbance and climate change is causing fire activity to rapidly increase threatening people and ecosystems westerling 2016 the western us is also data rich for example the usda forest inventory and analysis fia network bechtold and patterson 2005 appendix s1 includes approximately 160 000 permanent plots across the united states designed to provide insights into forest condition this allowed us to develop a robust parameterization for pfts in the western us the wealth of data also ensured that we started simulations from relatively high quality initial conditions described in section 2 3 2 and that we could benchmark model runs against several independent datasets described in section 2 3 3 parameters for the western us are provided in appendix 2 tables s1 and s2 we selected five study regions that collectively include 56 of all forested area in the western us the coastal and inland pacific northwest of oregon and washington 148 000 km2 of forest hereafter pnw northern idaho 97 000 km2 of forest idaho the greater yellowstone ecosystem in southern montana and northern wyoming 47 500 km2 of forest gye the southern rockies including parts of utah colorado new mexico and arizona 135 000 km2 of forest southern rockies and the sierra nevada mountains 77 000 km2 of forest sierras fig 2 study regions were selected because they represent the range of forest types and fire regimes that characterize the western us from dry pinyon and ponderosa pine woodlands where frequent low severity fires burned every 5 15 years prior to euro american settlement to wet douglas fir forests of the pacific northwest where fire return intervals could exceed 700 years with infrequent yet extensive high severity stand replacing fire events we grouped tree species in the western us into 12 pfts following buotte et al 2018 and ruefenacht et al 2008 fig 2 appendix s1 table s1 and also included a grassland pft 2 3 2 simulation design we initialized the model with the gridded pft map from buotte et al 2018 a stand age map derived from remote sensing historical fire records and forest inventory plots pan et al 2011 and information on fuel loads based on forest type prichard et al 2019 initial cohort heights were derived from the stand age map using internal model equations appendix s1 fig 1b initial dbhs live biomass and stand densities were calculated from initial cohort heights the forest ecosystem module was forced with 1965 1994 daily temperature from the topowx dataset oyler et al 2015 and average growing season volumetric soil moisture in the rooting zone 0 to 100 cm depth which was calculated following methods from williams et al 2017 temperature data was used to calculate tree seedling germination and establishment thresholds section 2 2 2 volumetric soil moisture was used in fire severity equations section 2 2 8 and was also converted to soil water potential based on sand silt and clay from soilgrids250m 2 0 hengl et al 2017 for calculations of tree regeneration section 2 2 2 and cohort mortality section 2 2 9 inputs to the fire module included the 1984 2019 climatological mean annual aridity the ratio of total annual precipitation to total annual potential evapotranspiration williams et al 2020 topography slope angle and topographic complexity hastings and dubar 1999 and factors that influence fire ignition density the 1987 2019 mean lightning strike density and 1990 human population density cummins et al 1998 radeloff et al 2018 table s3 initial fuel loads were representative of the forest types found in our study regions but did not reflect spatial heterogeneity due to the past legacies of harvest fire or drought prichard et al 2019 thus we ran a 200 year spin up simulating the coupled response of vegetation to fire and climate in the five study regions to generate spatially heterogeneous fuels conditions consistent with internal model logic following hansen et al 2020 after the spin up we ran a 100 year experiment to benchmark model performance because several processes are probabilistic in the model including fire mortality and recruitment we ran five replicates of each region to account for model based variability no forest harvest was simulated 2 3 3 analyses expected patterns we benchmarked simulated forest characteristics from model year 300 and simulated fire activity from simulation years 201 300 classical tests of statistical significance are problematic with simulated data because large sample sizes can artificially inflate significance thus we used a pattern oriented modeling approach grimm et al 2005 patterns of several simulated variables at stand 1 km grid cell to western us scales were compared to observed datasets to evaluate model skill we compared modeled and observed distributions of tree sizes and cohort densities for each pft and region and biomass pools and fire regime characteristics for each region and across the western us pooling all five study regions data limitations constrained our ability to benchmark the model with completely independent observational datasets but we prioritized independence whenever possible table 2 we represented model distributions of variables by calculating their median inter quartile range iqr skewness and minimum and maximum for each pft when appropriate study region and pooling across all study regions for each of the five replicates we then calculated the average median iqr skewness and minimum and maximum values across the replicates to compare with observed distributions observed distributions of tree sizes stand densities and biomass pools came from fia plots that were classified as forested lands and sampled since 2000 when the usfs adopted a standard fixed radius plot design bechtold and patterson 2005 fia plots were not filtered based on disturbance history for tree sizes and stand densities model evaluation was conducted with the one third of fia plots that were not used in model parameterization appendix s1 we identified all fia plots dominated by the pfts represented in dynafforest and calculated the median tree height and dbh for each plot see appendix s1 for further details we then compared simulated distributions of heights and dbhs with the fia plots dominated by the same pft to ensure accurate comparison we limited our analysis to simulated cohorts with a dbh 2 54 cm the cutoff for tree measurement in fia protocols observed distributions of biomass pools came from the most recent sampling of the fia plots that were classified as forested lands estimated using the rfia package stanke et al 2020 because live and downed biomass routines in the forest ecosystem module were parameterized independently from fia appendix s1 we used all available fia plots in benchmarking biomass simulated annual median fire size annual number of fires fire perimeter shape complexity perimeter length to patch area ratio and annual total area burned were compared to observed fire records from the period 1985 1994 from the same database used in model parameterization see appendix s1 fires came from the western us mtbs interagency wumi wildfire database juang et al 2022 which includes large fires 404 ha from the us forest service s monitoring trends in burn severity database as well as a quality controlled list of mostly smaller fires 100 ha maintained by the national wildfire coordinating group e g keeley and syphard 2017 westerling et al 2011 because we wanted to test the fire module s ability to generate fire characteristics consistent with the mid 20th century fire regime we chose 1985 1994 to exclude more recent observations during years where climate change has profoundly altered fire activity in the western us abatzoglou and williams 2016 westerling 2016 this temporal window is best suited for our current purposes because of 1 the broad spatial extent of our modeling that captures tremendous variability in fuels aridity climate ignition patterns and fire regime characteristics and 2 because our goal in this paper was to develop a model that could generate fire consistent with mid 20th century fire seasons subsequent development of the fire module for forecasting interannual effects of climate on fire will require more advanced statistical models and benchmarking methods the simulated percent of area that burned at high severity each year was independently benchmarked against a remotely sensed burn severity product parks and abatzoglou 2020 the database provides composite burn index cbi for all fires 404 ha in the observational record at a 30 m spatial resolution to ensure the remotely sensed product was comparable to model outputs we first included only portions of 1985 1994 fires that fell within our initial simulated forested areas we then masked out portions of those fires where the prefire ndvi was below 0 35 to exclude non forested vegetation potentially misclassified as forest parks and abatzoglou 2020 we aggregated the 30 m cbi estimates within each observed fire to a 1 km2 resolution we used a cbi value of 2 25 equivalent to 95 tree mortality miller et al 2009 as the cutoff to define stand replacing fire in each grid cell we then calculated the percent of annual burned area that was stand replacing for each region we report results comparing the distributions of simulated and observed variables in three ways to evaluate how well dynafforest captures the central tendency of each variable s observed distribution we compared the median value and iqr from observations to the averaged median value and iqr from the simulation replicates at the pft and or regional and western us scales to compare modeled and observed distributional shapes we compared the skewness of variable distributions to determine how well dynafforest represents the observed variability in each variable we quantified what percent of observations fell between the averaged minimum and maximum values from the simulation replicates while we assessed model skill based on the ability of dynafforest to accurately recreate observed distributions we recognize many other factors not included in our model affect real world forests such as bark beetle outbreaks forest restoration and timber harvest berner et al 2017 thus we anticipated that these unaccounted for drivers of forest dynamics would generate some divergence between simulated and observed central tendencies distributional shapes and ranges of variability benchmarking analyses were conducted in r statistical software v 4 0 4 core team 2021 using the packages rfia stanke et al 2020 tidyverse wickham et al 2019 ncdf4 pierce 2017 landscapemetrics hesselbarth et al 2019 sf pebesma 2018 dbplyr wickham and ruiz 2020 moments lukasz and novomestky 2015 and raster hijmans 2021 simulations and processing of model outputs were run on the amarel cluster at rutgers university 3 results 3 1 tree size stand density and forest cover among pfts modeled median tree heights and dbhs differed from observations of the same pft by 16 and 20 on average averaged across pfts the skew of modeled and observed tree heights was 1 3 and 1 2 and the skew of modeled and observed dbhs was 1 3 and 2 0 table 3 model skill in representing tree heights and dbhs varied across pfts fig 3 for example simulated aspen trees were shorter on average and had smaller diameters than observed aspen while modeled inland douglas fir tended to be larger than observations dynafforest captured much of the variability in observed tree heights and dbhs however on average 68 of observations fell within the ranges of simulated tree heights and dbhs of the same pft some variability also existed between regions in how well the model captured observed median tree sizes figs s1 and s2 for example modeled median heights of mixed firs closely matched the observed medians in idaho pnw and sierras but aspen trees in dynafforest were considerably taller than observations in the southern rockies fig s1 modeled median stand density was within 39 of the observed median stand density of the same pft on average the skew of modeled and observed stand densities was 4 3 and 4 8 table 3 the median simulated stands of engelmann spruce fir hemlock cedar five needle pine and lodgepole pine were moderately denser than observations while the median simulated stands of mixed conifer coastal douglas fir and inland douglas fir were less dense fig 4 an average of 62 of observations fell within the range of simulated stand densities of the same pft model skill also varied among regions fig s3 median inland douglas fir densities in the model for example were lower than observed medians in the gye idaho pnw and southern rockies but were higher than observed in the sierras fig s3 eight percent of initial forest area converted to grassland by simulation year 300 on average across the five study regions the grassland pft was most common in the sierras where it replaced 15 of initial forest area followed by the pnw 9 southern rockies 8 gye 5 and was least common in idaho where it replaced 4 of forest area after 300 years of simulation expansion of grassland was unsurprising as the pft distributions used to initialize simulations did not include grassland and it is often interspersed within western forest ecosystems 3 2 biomass pools pooling all study regions together there was strong model observation correspondence in live biomass the average median leaf and aboveground live wood biomass from the five simulation replicates were within 29 and 12 of the observed medians respectively table 3 fig 5 a the skew of modeled and observed leaf biomass was 1 4 and 2 2 and the skew of modeled and observed aboveground live wood biomass was 1 1 and 2 3 table 3 the model also represented observed variability in live biomass well across all study regions 90 and 92 of observations fell within the range of simulations for leaf and aboveground woody biomass median live aboveground tree biomass was highest in the idaho study region followed closely by the pnw gye sierras and southern rockies table s4 fig 6 when model outputs were compared with observations within regions median leaf biomass was underestimated by dynafforest in the pnw and was overestimated in the other study regions table s4 modeled median live aboveground woody biomass was lower than observations in the pnw and sierras and was higher in the southern rockies idaho and gye average median litter and coarse wood biomass were within 5 and 7 of the median observed values respectively fig 5b the skew of modeled and observed litter biomass was 1 7 and 5 3 and the skew of modeled and observed aboveground coarse wood biomass was 2 5 and 3 4 table 3 across all study regions 98 and 100 of observations fell within the range of simulations for downed litter and coarse wood pools suggesting the model could reasonably represent the range of variability in downed biomass however variability did exist in model observation correspondence when broken out by study region table s1 fig s4 dynafforest underestimated median litter biomass in the sierras and pnw and overestimated median litter biomass in the other regions modeled median coarse wood biomass was also lower than in the observational dataset in the pnw idaho and gye and larger than observed in the sierras and southern rockies 3 3 fire regime characteristics pooling all study regions together the annual median number of fires fire size perimeter to area ratio annual area burned and percent of annual burned area that was stand replacing from simulations differed from observed medians by 4 86 22 53 and 31 respectively table 3 fig 7 dynafforest overestimated the annual median fire size but also did not simulate rare extremely large observed fire years such as the 1988 yellowstone fires fig s5 which caused the model to underestimate median annual area burned fig 7b and may have contributed to the model s over representation of live biomass in the gye fig 6 approximately half of observations fell within the range of simulations for the annual number of fires and area burned while only 36 of observations fell within the range of simulations for annual median fire size 82 and 98 of observations fell within the range of simulations for the perimeter to area ratio and the percent of annual burned area that was stand replacing respectively substantial variability in model observation correspondence existed when evaluating the fire module within study regions table s4 fig s5 for example annual area burned was most strongly underestimated by the model in the gye where the iconic 1988 yellowstone fires took place reinforcing the importance of rare extremely large fire years for shaping model observation disparities in fire characteristics 4 discussion fires and forests dynamically influence one another through disturbance succession cycles parks et al 2015 prichard et al 2017 where when and why fire induced changes to temperate and boreal forests may feedback to affect subsequent burning remains poorly resolved particularly at regional to continental scales abatzoglou et al 2021 hurteau et al 2019 here we present a new approach to address this challenge dynafforest combines a hybrid probabilistic and deterministic process based forest ecosystem module with a flexible statistical fire module benchmarking results demonstrate that the forest ecosystem module reasonably recreates patterns of diverse forest characteristics across broad spatial domains 850 000 km2 of forest in the western us the fire module as currently conceptualized captures average 20th century fire characteristics in five study regions with highly variable fire regimes however the fire module misses the extremely large fire events that were relatively rare over the last century because we did not include effects of temporal climate variability on fire occurrence and size in this first version our baseline model provides proof of concept and will continue to be refined and expanded to meet the need for bottom up forest and fire projections that can inform forest management and adaptation strategies filotas et al 2014 messier et al 2015 for example dynafforest could be used in the future to evaluate where when and at what spatial scales different combinations of thinning and prescribed fire could decouple 21st century climate fire relationships a key objective of the forest ecosystem module was to explicitly include the demographic processes through which climate and fire affect forests and initiate feedbacks that can influence subsequent fire we accomplished this in a computationally efficient manner by taking a probabilistic approach for representing a few key processes such as seed dispersal seedling establishment and tree mortality while deterministically representing others based on successional stage the model generated patterns of forest cover and structure consistent with observations across five diverse study regions importantly the model also captured examples of type conversions the pft maps used to initialize simulations did not include grassland even though it is commonly interspersed among some forests of the western us however simulated forests that experienced low soil moisture and or unusually frequent and large high severity fires converted to nonforest in all study regions generating realistic forest grassland mosaics at low elevations in the sierras and southern rockies study regions and in southern and eastern oregon part of the pnw study region such patterns are consistent with recent analyses of drought induced forest conversion davis et al 2019 and highlight how dynafforest could be a powerful tool for quantifying 21st century forest resilience and transformation by identifying the climate fire tipping points that initiate wide spread conversion to grassland coop et al 2020 hansen et al 2020 hansen and turner 2019 we set out to generate a parameter set for the forest ecosystem module that was robust across all of the western us and dynafforest was particularly skilled at capturing patterns of forest structure when outputs from the five study regions were pooled to the western us scale however variation in model observation correspondence within study regions highlights inevitable tradeoffs between maximizing model realism and distilling immense ecological complexity into a tractable system representation for example tree size and stand density were deterministic in dynafforest based solely on the previous year s state this means that myriad abiotic and biotic factors such as climate nutrient availability and interspecific competition were not explicitly considered thus we expected dynafforest to overestimate tree growth and stand density on the trailing and leading edges of pft ranges where environmental conditions are harsh arid cold indeed the model did not capture the lowest density stands observed for most pfts fig 4 this may explain some of the variability in model observation correspondence between study regions as well as any mismatch between simulated and observed distribution shapes as quantified by skewness of pft tree size and stand density even when there was agreement for medians and iqrs of distributions trait values also vary tremendously across pft ranges anderegg et al 2018 messier et al 2010 2017 and regional variability in model performance highlights how pft parameterizations must be tailored to the scale at which questions are being asked dynafforest includes a flexible statistical fire module that can be adapted to meet the needs of many different applications here we implemented a relatively simple representation where fire occurrence and size were only sensitive to forest biomass and spatial variability in mean aridity lightning frequency topography and human population but did not respond to inter annual variability in climate or within season extreme fire weather this explains why the model was modestly skilled at capturing the average characteristics of 20th century forest fires but did not simulate as much inter annual variability in fire activity as observed in reality inter annual variability of atmospheric aridity is a dominant driver of burned area in temperate and boreal forests abatzoglou and williams 2016 higuera and abatzoglou 2020 seidl et al 2020 an essential next step in the development of dynafforest is to represent how temporal changes in climate affect fire occurrence and size even now however dynafforest includes far more realism in its representation of fire than many other models operating at this scale for example of nine esms evaluated in the fire inter model comparison project nearly half prescribed vegetation rather than dynamically coupling fire and forests only one third explicitly simulated individual fires instead of aggregate burned area and all operated at coarse spatial resolutions 0 5 grid cells meaning they were incapable of representing fine scale spatial heterogeneity within individual fire perimeters hantson et al 2020 rabin et al 2017 in contrast dynafforest explicitly couples fire and vegetation dynamics simulates individual fire occurrence and size as a function of human and natural drivers and dynamically grows individual fires at a 1 km spatial resolution based on fuel availability the model also simulates heterogeneity of fire severity within burned perimeters thus dynafforest is uniquely capable of capturing the complex finer scale patterns generated by fires across broad spatial domains as compared with many other models 5 conclusions we present a new model for simulating the dynamic interactions and feedbacks between fire and temperate and boreal forests our approach fills a unique need by 1 explicitly representing the key forest demographics through which fire affects forests 2 capturing the spatially heterogeneous effects of individual fires and 3 simulating regional to continental domains for decades to centuries such tools are desperately needed by scientists and decision makers to determine where when and why fire induced changes to forests may feedback to affect subsequent burning mcwethy et al 2019 and affect regional forest conditions better constrained projections of changing fire and forests could inform policy and management strategies designed to help people more sustainably coexist with fire now and in the future cochrane and bowman 2021 to focus on important questions of forest carbon biodiversity conservation forest economics and fire hazard software and data availability software name dynafforest dynamic temperate and boreal fire and forest ecosystem simulator developer winslow d hansen first year available 2022 hardware requirements pc mac software requirements r statistical environment and language program language r program size 121 5 kb availability http forestfutureslab org dynafforest license gpl 3 0 archive with data from benchmarking data archive can be found at 10 25390 caryinstitute 20452386 size of archive 157 mb author contributions wdh developed the model with input from apw wdh conducted the benchmarking exercises with input from all coauthors wdh wrote the paper and all coauthors contributed to revisions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements wdh acknowledges support from the gordon and betty moore foundation grant gbmf10763 the royal bank of canada the environmental defense fund and the national science foundation grant opp 2116863 apw acknowledges support from the zegar family foundation and doe grant desc0022302 att acknowledges funding from the nsf grants 2003205 and 2017949 the usda national institute of food and agriculture agricultural and food research initiative competitive programme grant no 2018 67012 31496 and the university of california laboratory fees research program award no lfr 20 652467 we thank rupert seidl and werner rammer for sharing their expertise on the development of forest simulation models appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105473 
25537,the advent of detailed hydrodynamic model simulations of urban flooding has not been matched by improved capabilities in flood exposure analysis which rely on validation against observed data this work introduces a generic building level flood exposure analysis tool applying high resolution flood data and building geometries derived from hydrodynamic simulations performed with the 2d hydrodynamic flood modelling software citycat validation data were obtained from a survey of affected residents following a large pluvial flood event in newcastle upon tyne uk sensitivity testing was carried out for different hydrodynamic model and exposure tool settings and between 68 and 75 of the surveyed buildings were correctly modelled as either flooded or not flooded the tool tends to underrepresent flooding with a better performance in identifying true negatives i e no flooding observed with no flooding modelled compared to true positives as higher true positive rates were accompanied by higher false positive rates no single scenario could be identified as the optimal solution however the results suggest a greater sensitivity of the results to the classification scheme than to the buffer distance applied overall if applied to high resolution flood depth maps the method is efficient and suitable for application to large urban areas for flood risk management and insurance analysis purposes keywords urban flooding flood exposure model validation open source code hydrodynamic flood modelling data availability data will be made available on request 1 introduction people and properties worldwide are increasingly exposed to floods as more extreme rainfall events occur as a result of climate change mailhot and duchesne 2010 chan et al 2018 and additional permeable surface area due to urban growth are causing more pluvial and sewer flooding nie et al 2009 skougaard kaspersen et al 2017 a case study by skougaard kaspersen et al 2017 concluded that a 1 increase in impermeable surface area increases pluvial flooding by up to 10 at the same time urban population is rising with more than half of the world s population already living in major cities brown et al 2009 consequently the number of people and buildings exposure to the risk of flooding is expected to increase responsible authorities are therefore looking to assess current and future flood risk in order to identify potential adaptation measures to protect people and properties to identify buildings at risk from flooding exposure analysis is commonly applied relating building location to flood depth information from observation or modelling exposure analysis tools are available for different purposes and scales de moel et al 2015 on a macro scale level exposure analysis focus on countries röthlisberger et al 2017 or large flood plains pluvial exposure assessment on a macro scale and regional level are presented in bhattarai et al 2016 and grahn and nyberg 2017 respectively the methodologies applied for macro scale analysis usually involve a simplification of information this can be through aggregating hazard grids to coarser resolutions or polygons as well as representing buildings as single points while macro scale analyses assists in strategic level decision making they are not suitable for the purpose of a detailed micro scale building level exposure analysis particularly in the context of pluvial and sewer flooding in densely built up areas small scale terrain e g kerbs and features of the urban topography influence runoff pathways and consequently the locations and magnitude of pluvial floods furthermore as shown in bertsch et al 2017 the drainage rate of storm drain inlets is highly sensitive to surrounding elevations impacting the surface water depth it is therefore necessary to conduct urban flood models at fine grid resolution i e better than 5m to correctly represent flow pathways between buildings along roads and into storm drainage networks furthermore modelling and testing the impact of flood adaptation measures such as green roofs or permeable pavements requires a detailed representation of building geometries a detailed building level flood exposure analysis tool is therefore required to process high resolution data of hazard and exposed buildings most importantly any exposure tool should be able to correctly identify the buildings which are most likely to be at risk from flooding small scale exposure assessments presented in ernst et al 2010 arrighi et al 2013 and garrote et al 2016 apply damage functions to analyse fluvial flood events damage functions are however difficult to transfer and apply in other areas and different flood events merz et al 2010 wagenaar et al 2016 furthermore detailed damage functions and complex exposure tools e g custer and nishijima 2015 van ootegem et al 2015 rely heavily on detailed datasets such as water levels inside the property building characteristics e g doors and windows value of the building fabric as well as costs to cover damages and repairs those data may not always be available or accessible and thus limit a wider and more generic application of damage functions focusing on pluvial flooding szewrański et al 2018 combine a hydrological model with a terrain based sink analysis to assess the risk of flooding for buildings the approach in szewrański et al 2018 however ignores the hydrodynamic component of a 2d overland flood routing a similar methodology relying on terrain data instead of hydrodynamic flood model results for assessing exposure to pluvial flooding can be found in torgersen et al 2017 another problem which often concerns flood exposure tools and exposure analysis are missing data for validation as surveyed or questionnaire information is often unavailable because of confidentiality or commercial value gerl et al 2016 molinari et al 2019 particularly when combining hazard exposure damage and vulnerability models it is important to analyse the performance of those models separately in order to understand the potential sources and magnitude of error see also apel et al 2009 an exposure analysis conducted in zischg et al 2018 applies insurance claim data and flood area information to assess the performance of the fluvial flood simulation software basement www basement ethz ch the results indicate a relatively good model fit with hit rates ranging between 0 61 and 0 92 the method applied in zischg et al 2018 however is not feasible for pluvial events this is because zischg et al 2018 consider buildings as exposed if the water level of at least one node of the depth mesh intersecting the building footprint area is 0 m in case of a pluvial flood event this approach would see an unrealistically large number of buildings classified as exposed to flooding this work aims to close the missing gap of having a validation and sensitivity analysis of a generic flood exposure analysis tool for pluvial flood models capable of handling large datasets of high resolution hydrodynamic model results and exact building geometries available as a jupyter notebook there are no compatibility restrictions regarding the hydrodynamic model results or any gis package furthermore the tool is operating with standard data formats adding to its usability the source code can be adapted for specific user purposes and additional functions e g depth damage functions may be added the validation is focusing on the capability of the exposure tool to correctly identify exposed and non exposed buildings for this purpose a set of 12 scenarios were simulated to test different hydrodynamic modelling and exposure tool settings validation data were obtained following a large pluvial flood event that took place on june 28 2012 in the north east of england uk a study of the same event validating simulated flood depths at specific locations using crowd sourced photographical information can be found in kutija et al 2014 due to data protection reasons the exact locations of the study area in this work cannot be disclosed the hydrodynamic modelling was conducted with the citycat city catchment analysis tool glenis et al 2018 2 methods and data 2 1 flood exposure tool the flood exposure tool initially developed by bertsch 2019 is written in python https www python org and available as a jupyter notebook https jupyter org index html which can be downloaded under the bsd 2 clause license download link the tool runs independently from the flood model or gis tool and uses standard data formats the exposure tool algorithm fig 1 requires two sets of input data 1 a surface water depth grid in regularly spaced coordinates e g as csv or txt file format and 2 a shapefile shp containing the building geometry polygons fig 2 as the tool can process multiple depth grids all grids need to be the same horizontal resolution compared to the initial version the updated flood exposure tool developed and tested in this work benefits from a more efficient algorithm which no longer requires any temporary files to be created furthermore the spatial intersection of depth points and building buffer polygons is only done once calculation steps 1 6 in fig 1 this particularly benefits an analysis consisting of multiple depth grids thus the run times of the new version are smaller than the previous version bertsch 2019 the exposure assessment is based on the principle of the internal flooding likelihood obtained from the uk water industry bertsch 2019 generally internal flooding is considered the situation of flood water entering a building from any possible source i e from the outside or the internal drainage system of the property for the purpose of a generic flood exposure tool the internal flooding component is ignored as this would require a separate modelling entity for the internal drainage system of each property in this work the probability of exposure reflects the potential of a building being internally flooded from the outside i e the water level on the outside building footprint reaches a level at which it potentially enters the building potentially because the exact location and elevation of the point of water entrance e g door window or air brick would require additional data which go beyond the purpose of a generic and flood exposure tool the actual exposure analysis is based on the spatial intersection of the surface water depth grid and the building geometry fig 2 using the python rtree module butler 2016 initially a buffer is created around the building geometry in order to extract the depth information from cells adjacent to the building outline fig 2 based on the extracted information the maximum and mean depth are calculated in order to assess the likelihood of internal flooding of a building according to table 1 the combination of mean and max depth is applied in order to make the assessment more robust against outliers of the maximum depth which may be caused by erroneous cells of the terrain model spurious cell elevations can be caused by measurement issues as well as sudden changes in elevation near ground features such as buildings or walls for instance meng et al 2010 the sensitivity of the buffer distance and the classification scheme table 1 are assessed as part of the validation analysis in section 3 2 2 hydrodynamic flood model the hydrodynamic simulations were carried out with the hydrodynamic flood model citycat city catchment analysis tool glenis et al 2018 as a fully distributed model citycat allows for a realistic simulation of the urban environment as buildings and green areas are explicitly represented capturing roof runoff and infiltration respectively the model building processes is supported by efficient algorithms which automatically generate the computational mesh which includes the allocation of buildings and green areas the capacity to run citycat on the cloud enables users to simulate entire cities at high resolutions glenis et al 2013 2 3 data 2 3 1 modelling data and parameters the surface catchments with horizontal resolutions of δx δy 1 m and δx δy 2 m were delineated separately in arcgis from lidar light detecting and ranging data the final computational domains are formed of 1 421 828 and 341 531 cells for the 1 m and 2 m resolution grids respectively see also table 2 manning s n surface roughness coefficients of 0 02 and 0 035 for impermeable and permeable surfaces were applied respectively building and green areas were obtained from mastermap data in total 4562 buildings are located in the study catchment the rainfall data were obtained from a tipping bucket gauge located on drummond building newcastle university campus with measurements available at 15 min intervals as shown in fig 3 the rainfall event had two peaks with the more intense one occurring at 30 min for a comprehensive analysis of the hydrological characteristics of the storm event the reader is referred to a report published by the environment agency ea environment agency 2012 the distance between the location of the raingauge and the study area is around 3 km to account for potentially different rainfall intensities in the study area compared to the measured two additional rainfall scenarios were considered as shown in fig 3 those scenarios represent 110 and 90 of the measured rainfall intensity respectively 2 3 2 validation data following the event on june 26 2012 newcastle city council issued questionnaires to residents in places which were known or anticipated to have been flooded during the event newcastle city council 2013 in total more than 12 000 questionnaires formed of 18 questions were issued based on roughly 3000 replies 1200 properties were affected from flooding with 500 houses internally flooded for the 4562 buildings in the study catchment 248 questionnaire responses were available for the validation against the defined internal flooding criteria described above any building which fulfilled one or more of the following criteria was considered to be exposed to flooding part of the property affected 1 house 2 water level on the outside of the property was at the height of 1 air bricks 2 1st quarter of front door 3 half of front door 4 3rd quarter of front door 5 loft 3 depth of water inside the property was 1 under the floor boards 2 above the floor boards 3 above skirting boards of the 248 buildings 119 were finally considered to be exposed to internal flooding and applied for the validation as either 1 water was reported to be inside the house or 2 the water level outside of the house was at a height at which it could potentially enter the house 2 4 validation metric the roc receiver operating characteristics methodology including a contingency table also referred to as confusion matrix fig 4 was applied to validate the simulated results against the observed data see fawcett 2006 bennett et al 2013 for a detailed explaination of this methodolgy according to fawcett 2006 the observed set of data or instances are p n while y n are the classifiers i e the modelled set of data in this work p exposure observed 119 buildings and n no exposure observed 129 buildings representing the total sample size of 248 for the modelled data y exposure simulated and n no exposure simulated based on the confusion matrix shown in fig 4 each instance in y n is mapped against a classifier in p n thus there are four possible outcomes which are tp true positives tn true negatives fp false positives and fn false negatives based on a contingency table or confusion matrix shown in fig 4 five validation metrics after fawcett 2006 and bennett et al 2013 are calculated to avoid being misled by a single and potentially skewed metric which may occur when working with contingency tables stephens et al 2014 the tp rate 0 1 or hit rate assesses the rate of correctly identified classifiers the fp rate 0 1 or false alarm rate represents the fraction of classifiers incorrectly identified the tp rate and fp rate are plotted in the roc graph shown in fig 5 allowing for a comparison of all scenarios tested the accuracy 0 1 informs about the fraction of correctly identified classifiers the critical success index csi 0 1 assesses whether the model can successfully identify tp at the cost of both fn and fp the bias score informs on the tendency of an under bias 1 or overestimation bias 1 of the number of buildings observed as exposed to flooding 3 results and discussion 3 1 scenarios different hydrodynamic model settings were simulated in citycat reflecting different horizontal grid resolutions of 1 m and 2 m respectively as well as different rainfall intensities table 3 those hydrodynamic model setting were combined with different exposure tool settings resulting in 12 scenarios in total table 3 the exposure tool settings consider different buffer distances of 150 and 200 respectively and different classification schemes for the internal flooding threshold tables 1 and 2 the computing times on a standard 8 00 gb ram 2 60 ghz laptop were approximately 3 5 min for three 1 m resolution grids formed of 1 421 828 cells e g scenarios a c and 1 min for three 2 m grids formed of 341 531 cells scenarios d f respectively the generation of the spatial index requires the majority of the computing time 3 2 validation and sensitivity analysis results fig 5 shows the calculated tp rates and fp rates for the 12 scenarios plotted on a roc graph results located towards the top left corner with higher tp rates and low fp rates indicate a better performance compared to results located towards the bottom right corner table 4 summarises the confusion matrix results and the five validation metrics tp rate fp rate accuracy csi and bias calculated for all 12 scenarios overall the accuracy rates for the 12 scenarios range between 0 68 and 0 75 table 4 in other words between 68 and 75 of the buildings are correctly identified as exposed tp or not exposed tn to flooding for the different scenarios the csi values however are relatively low ranging between 0 36 and 0 56 this indicates a better performance in identifying tn compared to tp which can also be seen by the high numbers of tn compared to tp in table 4 the relatively large number of fn and consequently the bias scores between 0 44 and 0 85 mean that each scenario underrepresents the risk of flooding compared to the observed data the tp rates for the 12 scenarios range between 0 4 and 0 66 with fp rates between 0 05 and 0 17 table 4 looking at the tp rates and fp rates plotted on the roc graph in fig 5 suggest that a higher tp rate is generally accompanied with a higher fp rate using the mean flood depth of a building only scenario l resulted in the highest tp rate and csi value together with the lowest degree of bias compared to using the full classifications presented in tables 1 and 2 applying only the maximum value scenario k had less of an impact the intensity of the storm event used in this work was relatively high leading to a number of inundated hot spot areas fig 6 thus the mean values are expected to have a higher significance compared to a low intensity rainfall event during a rainfall event the lowest lying cells are the ones to be inundated first hence for the exposure calculation the maximum depth is more significant for a rainfall event of a small magnitude in case of a more intense rainfall event and widespread flooding the significance of the mean value for the exposure calculation increases this emphasises the need to conduct further validation for rainfall events of different magnitudes and durations to test this hypothesis compared to using the mean or maximum depth only changing the classification criteria from table 1 to table 2 scenario a b and d e had a relatively little impact on the results changing the dem resolution led to relatively little change in the results scenario d with a dem resolution of 2 m performed slightly better compared to scenario a with a dem resolution of 1 m however scenario d also has a higher fp rate compared to scenario a altering the rainfall intensities to 110 and 90 of the recorded rainfall only shows little changes in the results comparing scenarios g and i as well as scenarios h and j with scenarios a and b respectively considering the impact of the buffer distance the results presented in table 4 are almost identical for scenarios b and c as well as e and f respectively wider buffer distances 200 of dem resolution were not considered in this work as this would extract water depth information from a distance too far from the building to be considered relevant in comparison to the accuracy values presented in table 4 ranging between 0 68 and 0 75 the validation presented in zischg et al 2018 analysing fluvial flooding in four different locations tends to deliver higher accuracies between 0 77 and 0 91 both the tp rates and fp rates in zischg et al 2018 are larger ranging between 0 61 and 0 91 as well as 0 07 and 0 55 respectively pluvial and fluvial flooding not only differ in terms of their hydrodynamic characteristics but also in terms of the implications on calculating flood risk and thus the assessment of exposure the range of the performance metrics presented in zischg et al 2018 across four different locations also highlights the need to analyse areas with different degrees of urbanisation in future validation analysis the metrics presented in table 4 also underline the importance to consider multiple metrics for a validation analysis as relying on only the csi or accuracy index for instance may lead to a misinterpretation of results see also stephens et al 2014 3 3 mapping of results fig 6 shows four different locations demonstrating the capacity of the exposure tool to correctly identify cases of tp and tn while also highlighting issues resulting in situation of fn and fp in fig 6a the model correctly identifies a number of buildings as tp and tn but also underrepresents the exposure indicated by fn in this case the flooded area in the centre does not reach the exposed buildings in the north those findings underline the need to consider the sub surface drainage network in future work as fully coupled 1d 2d flood models dynamically capture both drainage and surcharging flooded hot spot areas across the domain may change thus a potentially surcharging sewer near the location of fig 6a for instance adding more water may result in a greater number of buildings exposed and consequently decrease the number of fn and improve the tp rate on the other hand additional drainage in other areas with buildings classified as fp may see a reduction in flooding and consequently increasing the number of tn the case of the single building classified as fp in fig 6b shows the challenge of correctly identifying individual buildings amongst a larger number of buildings classified differently tp in this case conventional analysis for insurance purposes to detect false claims may use homogeneity of exposure which is generally high for fluvial events but as can be seen here may be low during pluvial events affecting built up areas with complex flow paths near or around buildings the examples in fig 6c and d demonstrate the capacity of the flood exposure tool to correctly identify cases of tp and tn those results also underline the need to have high resolution hydrodynamic flood model results together with detailed building geometries as shown by the examples in fig 6 in particular c and d there are many unclassified buildings located in flooded areas while many of those buildings were classified as internally flooded by the exposure tool no observation records were available hence those buildings were not considered in the analysis 4 conclusions this paper presents a validation and sensitivity analysis of a generic flood exposure analysis tool developed for the analysis of pluvial flooding the exposure analysis tool was developed in python and is available as a jupyter notebook which can easily be transferred to colab the exposure analysis takes into account water depth information and detailed building geometries in order to estimate flood exposure based on a buffer analysis validation data were obtained after a large pluvial flood event providing flooding information for 248 individual buildings hydrodynamic simulations were performed with citycat glenis et al 2018 in total 12 scenarios were simulated the sensitivity analysis reflected on the hydrodynamic modelling through different computational grid resolution and rainfall intensities and the exposure tool parameters using different buffer distances and classification criteria the main conclusions drawn from the analysis and the work conducted in this paper can be summarised as follows with accuracy rates ranging between 0 67 and 0 75 the different scenarios correctly identify approximately 2 3 3 4 of the buildings as flooded or not flooded compared with observed data the relatively small range of the accuracy rates suggest that the results are not very sensitive to the different scenarios tested higher tp rates are accompanied with higher fp rates hence no scenario setting could be identified as the optimal solution the results are slightly more sensitive to the classification scheme compared to the buffer distance which had almost no impact at all the accuracy levels combined with the relatively low csi scores between 0 36 and 0 56 suggest that the exposure tool performs better in identifying true negatives compared to true positives with bias scores between 0 44 and 0 85 combined with relatively large numbers of fn the exposure tool tends to underrepresent the number of buildings exposed to flooding there was a considerably large number of buildings modelled as exposed which had no validation data available it is difficult to assign the potential source of uncertainty to either the hydrodynamic model results or the results from the exposure tool the application of multiple metrics enhanced the robustness of the validation analysis crucially the impact of the sub surface drainage network on the performance of the exposure tool needs to be tested in future work hence fully coupled 1d 2d hydrodynamic simulations ought to be carried out to assess the performance of the flood exposure analysis tool further work is also required to validate the exposure tool against rainfall events of different magnitudes and areas with different degrees of urbanisation the overhauled algorithm of the flood exposure tool offers greater efficiency and reduced run times the open source code of the flood exposure tool enables users to adapt it towards specific needs this may include different classification schemes to the ones presented in this work an adaptation for the application of irregular depth grids e g tin triangular irregular networks or an expansion to include detailed depth damage functions software availability section name of application flood exposure calculator v2 0 developer robert bertsch at school of engineering newcastle university ne1 7ru newcastle upon tyne contact vassilis glenis school of engineering newcastle university ne1 7ru newcastle upon tyne year first available 2018 software required jupyter for python 3 5 and rtree and geopandas modules programming language python 3 5 implementation flood exposure calculator ipynb for jupyter availability on github https github com hydrob flood exposure calculator license bsd 2 clause flood exposure calculator v2 0 ipynb author contributions r b v g and c k conceived the problem and established the concept behind the flood exposure tool r b developed the flood exposure tool v g performed the modelling in citycat 2d r b analysed the data r b v g and c k wrote the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been funded by scottish water and the engineering and physical science research council epsrc as part of grant 1368347 of centre for doctoral training in engineering for the water sector stream ep g037094 1 funding was also provided by two epsrc projects itrc mistral multi scale infrastructure systems analytics ep n017064 1 and future urban flood risk management ep p004334 1 c k was partly supported by the willis towers watson research network the authors gratefully acknowledge darren varley from newcastle city council for providing the questionnaire data 
25537,the advent of detailed hydrodynamic model simulations of urban flooding has not been matched by improved capabilities in flood exposure analysis which rely on validation against observed data this work introduces a generic building level flood exposure analysis tool applying high resolution flood data and building geometries derived from hydrodynamic simulations performed with the 2d hydrodynamic flood modelling software citycat validation data were obtained from a survey of affected residents following a large pluvial flood event in newcastle upon tyne uk sensitivity testing was carried out for different hydrodynamic model and exposure tool settings and between 68 and 75 of the surveyed buildings were correctly modelled as either flooded or not flooded the tool tends to underrepresent flooding with a better performance in identifying true negatives i e no flooding observed with no flooding modelled compared to true positives as higher true positive rates were accompanied by higher false positive rates no single scenario could be identified as the optimal solution however the results suggest a greater sensitivity of the results to the classification scheme than to the buffer distance applied overall if applied to high resolution flood depth maps the method is efficient and suitable for application to large urban areas for flood risk management and insurance analysis purposes keywords urban flooding flood exposure model validation open source code hydrodynamic flood modelling data availability data will be made available on request 1 introduction people and properties worldwide are increasingly exposed to floods as more extreme rainfall events occur as a result of climate change mailhot and duchesne 2010 chan et al 2018 and additional permeable surface area due to urban growth are causing more pluvial and sewer flooding nie et al 2009 skougaard kaspersen et al 2017 a case study by skougaard kaspersen et al 2017 concluded that a 1 increase in impermeable surface area increases pluvial flooding by up to 10 at the same time urban population is rising with more than half of the world s population already living in major cities brown et al 2009 consequently the number of people and buildings exposure to the risk of flooding is expected to increase responsible authorities are therefore looking to assess current and future flood risk in order to identify potential adaptation measures to protect people and properties to identify buildings at risk from flooding exposure analysis is commonly applied relating building location to flood depth information from observation or modelling exposure analysis tools are available for different purposes and scales de moel et al 2015 on a macro scale level exposure analysis focus on countries röthlisberger et al 2017 or large flood plains pluvial exposure assessment on a macro scale and regional level are presented in bhattarai et al 2016 and grahn and nyberg 2017 respectively the methodologies applied for macro scale analysis usually involve a simplification of information this can be through aggregating hazard grids to coarser resolutions or polygons as well as representing buildings as single points while macro scale analyses assists in strategic level decision making they are not suitable for the purpose of a detailed micro scale building level exposure analysis particularly in the context of pluvial and sewer flooding in densely built up areas small scale terrain e g kerbs and features of the urban topography influence runoff pathways and consequently the locations and magnitude of pluvial floods furthermore as shown in bertsch et al 2017 the drainage rate of storm drain inlets is highly sensitive to surrounding elevations impacting the surface water depth it is therefore necessary to conduct urban flood models at fine grid resolution i e better than 5m to correctly represent flow pathways between buildings along roads and into storm drainage networks furthermore modelling and testing the impact of flood adaptation measures such as green roofs or permeable pavements requires a detailed representation of building geometries a detailed building level flood exposure analysis tool is therefore required to process high resolution data of hazard and exposed buildings most importantly any exposure tool should be able to correctly identify the buildings which are most likely to be at risk from flooding small scale exposure assessments presented in ernst et al 2010 arrighi et al 2013 and garrote et al 2016 apply damage functions to analyse fluvial flood events damage functions are however difficult to transfer and apply in other areas and different flood events merz et al 2010 wagenaar et al 2016 furthermore detailed damage functions and complex exposure tools e g custer and nishijima 2015 van ootegem et al 2015 rely heavily on detailed datasets such as water levels inside the property building characteristics e g doors and windows value of the building fabric as well as costs to cover damages and repairs those data may not always be available or accessible and thus limit a wider and more generic application of damage functions focusing on pluvial flooding szewrański et al 2018 combine a hydrological model with a terrain based sink analysis to assess the risk of flooding for buildings the approach in szewrański et al 2018 however ignores the hydrodynamic component of a 2d overland flood routing a similar methodology relying on terrain data instead of hydrodynamic flood model results for assessing exposure to pluvial flooding can be found in torgersen et al 2017 another problem which often concerns flood exposure tools and exposure analysis are missing data for validation as surveyed or questionnaire information is often unavailable because of confidentiality or commercial value gerl et al 2016 molinari et al 2019 particularly when combining hazard exposure damage and vulnerability models it is important to analyse the performance of those models separately in order to understand the potential sources and magnitude of error see also apel et al 2009 an exposure analysis conducted in zischg et al 2018 applies insurance claim data and flood area information to assess the performance of the fluvial flood simulation software basement www basement ethz ch the results indicate a relatively good model fit with hit rates ranging between 0 61 and 0 92 the method applied in zischg et al 2018 however is not feasible for pluvial events this is because zischg et al 2018 consider buildings as exposed if the water level of at least one node of the depth mesh intersecting the building footprint area is 0 m in case of a pluvial flood event this approach would see an unrealistically large number of buildings classified as exposed to flooding this work aims to close the missing gap of having a validation and sensitivity analysis of a generic flood exposure analysis tool for pluvial flood models capable of handling large datasets of high resolution hydrodynamic model results and exact building geometries available as a jupyter notebook there are no compatibility restrictions regarding the hydrodynamic model results or any gis package furthermore the tool is operating with standard data formats adding to its usability the source code can be adapted for specific user purposes and additional functions e g depth damage functions may be added the validation is focusing on the capability of the exposure tool to correctly identify exposed and non exposed buildings for this purpose a set of 12 scenarios were simulated to test different hydrodynamic modelling and exposure tool settings validation data were obtained following a large pluvial flood event that took place on june 28 2012 in the north east of england uk a study of the same event validating simulated flood depths at specific locations using crowd sourced photographical information can be found in kutija et al 2014 due to data protection reasons the exact locations of the study area in this work cannot be disclosed the hydrodynamic modelling was conducted with the citycat city catchment analysis tool glenis et al 2018 2 methods and data 2 1 flood exposure tool the flood exposure tool initially developed by bertsch 2019 is written in python https www python org and available as a jupyter notebook https jupyter org index html which can be downloaded under the bsd 2 clause license download link the tool runs independently from the flood model or gis tool and uses standard data formats the exposure tool algorithm fig 1 requires two sets of input data 1 a surface water depth grid in regularly spaced coordinates e g as csv or txt file format and 2 a shapefile shp containing the building geometry polygons fig 2 as the tool can process multiple depth grids all grids need to be the same horizontal resolution compared to the initial version the updated flood exposure tool developed and tested in this work benefits from a more efficient algorithm which no longer requires any temporary files to be created furthermore the spatial intersection of depth points and building buffer polygons is only done once calculation steps 1 6 in fig 1 this particularly benefits an analysis consisting of multiple depth grids thus the run times of the new version are smaller than the previous version bertsch 2019 the exposure assessment is based on the principle of the internal flooding likelihood obtained from the uk water industry bertsch 2019 generally internal flooding is considered the situation of flood water entering a building from any possible source i e from the outside or the internal drainage system of the property for the purpose of a generic flood exposure tool the internal flooding component is ignored as this would require a separate modelling entity for the internal drainage system of each property in this work the probability of exposure reflects the potential of a building being internally flooded from the outside i e the water level on the outside building footprint reaches a level at which it potentially enters the building potentially because the exact location and elevation of the point of water entrance e g door window or air brick would require additional data which go beyond the purpose of a generic and flood exposure tool the actual exposure analysis is based on the spatial intersection of the surface water depth grid and the building geometry fig 2 using the python rtree module butler 2016 initially a buffer is created around the building geometry in order to extract the depth information from cells adjacent to the building outline fig 2 based on the extracted information the maximum and mean depth are calculated in order to assess the likelihood of internal flooding of a building according to table 1 the combination of mean and max depth is applied in order to make the assessment more robust against outliers of the maximum depth which may be caused by erroneous cells of the terrain model spurious cell elevations can be caused by measurement issues as well as sudden changes in elevation near ground features such as buildings or walls for instance meng et al 2010 the sensitivity of the buffer distance and the classification scheme table 1 are assessed as part of the validation analysis in section 3 2 2 hydrodynamic flood model the hydrodynamic simulations were carried out with the hydrodynamic flood model citycat city catchment analysis tool glenis et al 2018 as a fully distributed model citycat allows for a realistic simulation of the urban environment as buildings and green areas are explicitly represented capturing roof runoff and infiltration respectively the model building processes is supported by efficient algorithms which automatically generate the computational mesh which includes the allocation of buildings and green areas the capacity to run citycat on the cloud enables users to simulate entire cities at high resolutions glenis et al 2013 2 3 data 2 3 1 modelling data and parameters the surface catchments with horizontal resolutions of δx δy 1 m and δx δy 2 m were delineated separately in arcgis from lidar light detecting and ranging data the final computational domains are formed of 1 421 828 and 341 531 cells for the 1 m and 2 m resolution grids respectively see also table 2 manning s n surface roughness coefficients of 0 02 and 0 035 for impermeable and permeable surfaces were applied respectively building and green areas were obtained from mastermap data in total 4562 buildings are located in the study catchment the rainfall data were obtained from a tipping bucket gauge located on drummond building newcastle university campus with measurements available at 15 min intervals as shown in fig 3 the rainfall event had two peaks with the more intense one occurring at 30 min for a comprehensive analysis of the hydrological characteristics of the storm event the reader is referred to a report published by the environment agency ea environment agency 2012 the distance between the location of the raingauge and the study area is around 3 km to account for potentially different rainfall intensities in the study area compared to the measured two additional rainfall scenarios were considered as shown in fig 3 those scenarios represent 110 and 90 of the measured rainfall intensity respectively 2 3 2 validation data following the event on june 26 2012 newcastle city council issued questionnaires to residents in places which were known or anticipated to have been flooded during the event newcastle city council 2013 in total more than 12 000 questionnaires formed of 18 questions were issued based on roughly 3000 replies 1200 properties were affected from flooding with 500 houses internally flooded for the 4562 buildings in the study catchment 248 questionnaire responses were available for the validation against the defined internal flooding criteria described above any building which fulfilled one or more of the following criteria was considered to be exposed to flooding part of the property affected 1 house 2 water level on the outside of the property was at the height of 1 air bricks 2 1st quarter of front door 3 half of front door 4 3rd quarter of front door 5 loft 3 depth of water inside the property was 1 under the floor boards 2 above the floor boards 3 above skirting boards of the 248 buildings 119 were finally considered to be exposed to internal flooding and applied for the validation as either 1 water was reported to be inside the house or 2 the water level outside of the house was at a height at which it could potentially enter the house 2 4 validation metric the roc receiver operating characteristics methodology including a contingency table also referred to as confusion matrix fig 4 was applied to validate the simulated results against the observed data see fawcett 2006 bennett et al 2013 for a detailed explaination of this methodolgy according to fawcett 2006 the observed set of data or instances are p n while y n are the classifiers i e the modelled set of data in this work p exposure observed 119 buildings and n no exposure observed 129 buildings representing the total sample size of 248 for the modelled data y exposure simulated and n no exposure simulated based on the confusion matrix shown in fig 4 each instance in y n is mapped against a classifier in p n thus there are four possible outcomes which are tp true positives tn true negatives fp false positives and fn false negatives based on a contingency table or confusion matrix shown in fig 4 five validation metrics after fawcett 2006 and bennett et al 2013 are calculated to avoid being misled by a single and potentially skewed metric which may occur when working with contingency tables stephens et al 2014 the tp rate 0 1 or hit rate assesses the rate of correctly identified classifiers the fp rate 0 1 or false alarm rate represents the fraction of classifiers incorrectly identified the tp rate and fp rate are plotted in the roc graph shown in fig 5 allowing for a comparison of all scenarios tested the accuracy 0 1 informs about the fraction of correctly identified classifiers the critical success index csi 0 1 assesses whether the model can successfully identify tp at the cost of both fn and fp the bias score informs on the tendency of an under bias 1 or overestimation bias 1 of the number of buildings observed as exposed to flooding 3 results and discussion 3 1 scenarios different hydrodynamic model settings were simulated in citycat reflecting different horizontal grid resolutions of 1 m and 2 m respectively as well as different rainfall intensities table 3 those hydrodynamic model setting were combined with different exposure tool settings resulting in 12 scenarios in total table 3 the exposure tool settings consider different buffer distances of 150 and 200 respectively and different classification schemes for the internal flooding threshold tables 1 and 2 the computing times on a standard 8 00 gb ram 2 60 ghz laptop were approximately 3 5 min for three 1 m resolution grids formed of 1 421 828 cells e g scenarios a c and 1 min for three 2 m grids formed of 341 531 cells scenarios d f respectively the generation of the spatial index requires the majority of the computing time 3 2 validation and sensitivity analysis results fig 5 shows the calculated tp rates and fp rates for the 12 scenarios plotted on a roc graph results located towards the top left corner with higher tp rates and low fp rates indicate a better performance compared to results located towards the bottom right corner table 4 summarises the confusion matrix results and the five validation metrics tp rate fp rate accuracy csi and bias calculated for all 12 scenarios overall the accuracy rates for the 12 scenarios range between 0 68 and 0 75 table 4 in other words between 68 and 75 of the buildings are correctly identified as exposed tp or not exposed tn to flooding for the different scenarios the csi values however are relatively low ranging between 0 36 and 0 56 this indicates a better performance in identifying tn compared to tp which can also be seen by the high numbers of tn compared to tp in table 4 the relatively large number of fn and consequently the bias scores between 0 44 and 0 85 mean that each scenario underrepresents the risk of flooding compared to the observed data the tp rates for the 12 scenarios range between 0 4 and 0 66 with fp rates between 0 05 and 0 17 table 4 looking at the tp rates and fp rates plotted on the roc graph in fig 5 suggest that a higher tp rate is generally accompanied with a higher fp rate using the mean flood depth of a building only scenario l resulted in the highest tp rate and csi value together with the lowest degree of bias compared to using the full classifications presented in tables 1 and 2 applying only the maximum value scenario k had less of an impact the intensity of the storm event used in this work was relatively high leading to a number of inundated hot spot areas fig 6 thus the mean values are expected to have a higher significance compared to a low intensity rainfall event during a rainfall event the lowest lying cells are the ones to be inundated first hence for the exposure calculation the maximum depth is more significant for a rainfall event of a small magnitude in case of a more intense rainfall event and widespread flooding the significance of the mean value for the exposure calculation increases this emphasises the need to conduct further validation for rainfall events of different magnitudes and durations to test this hypothesis compared to using the mean or maximum depth only changing the classification criteria from table 1 to table 2 scenario a b and d e had a relatively little impact on the results changing the dem resolution led to relatively little change in the results scenario d with a dem resolution of 2 m performed slightly better compared to scenario a with a dem resolution of 1 m however scenario d also has a higher fp rate compared to scenario a altering the rainfall intensities to 110 and 90 of the recorded rainfall only shows little changes in the results comparing scenarios g and i as well as scenarios h and j with scenarios a and b respectively considering the impact of the buffer distance the results presented in table 4 are almost identical for scenarios b and c as well as e and f respectively wider buffer distances 200 of dem resolution were not considered in this work as this would extract water depth information from a distance too far from the building to be considered relevant in comparison to the accuracy values presented in table 4 ranging between 0 68 and 0 75 the validation presented in zischg et al 2018 analysing fluvial flooding in four different locations tends to deliver higher accuracies between 0 77 and 0 91 both the tp rates and fp rates in zischg et al 2018 are larger ranging between 0 61 and 0 91 as well as 0 07 and 0 55 respectively pluvial and fluvial flooding not only differ in terms of their hydrodynamic characteristics but also in terms of the implications on calculating flood risk and thus the assessment of exposure the range of the performance metrics presented in zischg et al 2018 across four different locations also highlights the need to analyse areas with different degrees of urbanisation in future validation analysis the metrics presented in table 4 also underline the importance to consider multiple metrics for a validation analysis as relying on only the csi or accuracy index for instance may lead to a misinterpretation of results see also stephens et al 2014 3 3 mapping of results fig 6 shows four different locations demonstrating the capacity of the exposure tool to correctly identify cases of tp and tn while also highlighting issues resulting in situation of fn and fp in fig 6a the model correctly identifies a number of buildings as tp and tn but also underrepresents the exposure indicated by fn in this case the flooded area in the centre does not reach the exposed buildings in the north those findings underline the need to consider the sub surface drainage network in future work as fully coupled 1d 2d flood models dynamically capture both drainage and surcharging flooded hot spot areas across the domain may change thus a potentially surcharging sewer near the location of fig 6a for instance adding more water may result in a greater number of buildings exposed and consequently decrease the number of fn and improve the tp rate on the other hand additional drainage in other areas with buildings classified as fp may see a reduction in flooding and consequently increasing the number of tn the case of the single building classified as fp in fig 6b shows the challenge of correctly identifying individual buildings amongst a larger number of buildings classified differently tp in this case conventional analysis for insurance purposes to detect false claims may use homogeneity of exposure which is generally high for fluvial events but as can be seen here may be low during pluvial events affecting built up areas with complex flow paths near or around buildings the examples in fig 6c and d demonstrate the capacity of the flood exposure tool to correctly identify cases of tp and tn those results also underline the need to have high resolution hydrodynamic flood model results together with detailed building geometries as shown by the examples in fig 6 in particular c and d there are many unclassified buildings located in flooded areas while many of those buildings were classified as internally flooded by the exposure tool no observation records were available hence those buildings were not considered in the analysis 4 conclusions this paper presents a validation and sensitivity analysis of a generic flood exposure analysis tool developed for the analysis of pluvial flooding the exposure analysis tool was developed in python and is available as a jupyter notebook which can easily be transferred to colab the exposure analysis takes into account water depth information and detailed building geometries in order to estimate flood exposure based on a buffer analysis validation data were obtained after a large pluvial flood event providing flooding information for 248 individual buildings hydrodynamic simulations were performed with citycat glenis et al 2018 in total 12 scenarios were simulated the sensitivity analysis reflected on the hydrodynamic modelling through different computational grid resolution and rainfall intensities and the exposure tool parameters using different buffer distances and classification criteria the main conclusions drawn from the analysis and the work conducted in this paper can be summarised as follows with accuracy rates ranging between 0 67 and 0 75 the different scenarios correctly identify approximately 2 3 3 4 of the buildings as flooded or not flooded compared with observed data the relatively small range of the accuracy rates suggest that the results are not very sensitive to the different scenarios tested higher tp rates are accompanied with higher fp rates hence no scenario setting could be identified as the optimal solution the results are slightly more sensitive to the classification scheme compared to the buffer distance which had almost no impact at all the accuracy levels combined with the relatively low csi scores between 0 36 and 0 56 suggest that the exposure tool performs better in identifying true negatives compared to true positives with bias scores between 0 44 and 0 85 combined with relatively large numbers of fn the exposure tool tends to underrepresent the number of buildings exposed to flooding there was a considerably large number of buildings modelled as exposed which had no validation data available it is difficult to assign the potential source of uncertainty to either the hydrodynamic model results or the results from the exposure tool the application of multiple metrics enhanced the robustness of the validation analysis crucially the impact of the sub surface drainage network on the performance of the exposure tool needs to be tested in future work hence fully coupled 1d 2d hydrodynamic simulations ought to be carried out to assess the performance of the flood exposure analysis tool further work is also required to validate the exposure tool against rainfall events of different magnitudes and areas with different degrees of urbanisation the overhauled algorithm of the flood exposure tool offers greater efficiency and reduced run times the open source code of the flood exposure tool enables users to adapt it towards specific needs this may include different classification schemes to the ones presented in this work an adaptation for the application of irregular depth grids e g tin triangular irregular networks or an expansion to include detailed depth damage functions software availability section name of application flood exposure calculator v2 0 developer robert bertsch at school of engineering newcastle university ne1 7ru newcastle upon tyne contact vassilis glenis school of engineering newcastle university ne1 7ru newcastle upon tyne year first available 2018 software required jupyter for python 3 5 and rtree and geopandas modules programming language python 3 5 implementation flood exposure calculator ipynb for jupyter availability on github https github com hydrob flood exposure calculator license bsd 2 clause flood exposure calculator v2 0 ipynb author contributions r b v g and c k conceived the problem and established the concept behind the flood exposure tool r b developed the flood exposure tool v g performed the modelling in citycat 2d r b analysed the data r b v g and c k wrote the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been funded by scottish water and the engineering and physical science research council epsrc as part of grant 1368347 of centre for doctoral training in engineering for the water sector stream ep g037094 1 funding was also provided by two epsrc projects itrc mistral multi scale infrastructure systems analytics ep n017064 1 and future urban flood risk management ep p004334 1 c k was partly supported by the willis towers watson research network the authors gratefully acknowledge darren varley from newcastle city council for providing the questionnaire data 
25538,this paper presents an integrated modeling framework aiming at accurate predictions of flood hazard from heavy rainfalls the accuracy of such predictions generally depends on the complexity and resolution of the employed model components we propose an integration of complementary models in one framework that facilitates gpus to improve accuracy and simulation time the spatially distributed runoff model integrates surface flow routing based on the full shallow water equations infiltration based on the green ampt equation and interception in urban areas the runoff model is coupled with the storm water management model swmm the integrated model is validated and tested on laboratory rural and urban scenarios with regards to accuracy and computational efficiency the gpu acceleration yields speedups of 1000 times compared to a cpu implementation and enables the coupled simulation of flash floods at 1 m resolution for an urban area of 200 km 2 in realtime keywords spatially distributed rainfall runoff model gpu acceleration dual drainage surface sewer coupling flash flood hazard modeling data availability data will be made available on request 1 introduction floods are increasing in many parts of the world due to climate and land use change chen et al 2018 blöschl et al 2019 causing disproportionally high damage in urban regions jongman 2018 to mitigate future flood damage detailed models that assist in assessing the flood hazard spatially are crucial rosenzweig et al 2021 in contrast to lumped models spatially distributed models allow for an explicit representation of spatial variations and inhomogeneities in input data such as topography vegetation soil characteristics and urban features notwithstanding scale issues grayson and blöschl 2001 there is a lot of value in spatially distributed high resolution modeling for management purposes however higher resolutions lead to slower simulations in addition ensemble simulations that quantify the uncertainty of the predictions and provide insights into the effects of parameter variation increase the computational burden even further thus the challenge is to advance the capabilities of numerical modeling while balancing simulation performance and model accuracy to accurately represent the topography the resolution of the simulation grid should be chosen accordingly as a rule of thumb terrain features should be covered by at least 3 cells to be represented explicitly gallegos et al 2009 fewtrell et al 2011 horváth et al 2020 for urban areas a resolution of 2 m or less is considered necessary to accurately represent buildings curbstones and other features fewtrell et al 2011 dottori et al 2013 xing et al 2018 moreover simulations at submeter resolutions are useful for assessing the benefits of small scale alteration of street topography for flood risk management purposes at low costs de almeida et al 2016 the simulation grid is typically either a structured cartesian grid costabile et al 2013 horváth et al 2015 buttinger kreuzhuber et al 2019 or an unstructured triangular mesh bermúdez et al 1998 hou et al 2015 fernández pato et al 2016 unstructured meshes are able to incorporate complex geometries however they require time consuming mesh generation in contrast structured grids lack the pre processing step at the expense of poor resolution of topographic features not aligned with the grid thus they usually require a higher resolution in comparison to unstructured meshes to enable the same level of accuracy the shallow water equations swes are typically used for describing surface flow due to the courant friedrichs lewy cfl condition a high spatial resolution requires a fine temporal discretization therefore the total amount of computational work increases and in turn slows down simulation runs to accelerate the simulations one possibility is to simplify the shallow water model for example using diffusive wave sometimes also called zero inertia or kinematic wave approximations neal et al 2012 le et al 2015 fry and maxwell 2018 yang et al 2020 for urban regions the full or dynamic swes in combination with shock capturing schemes are able to reproduce observed hydraulic behavior and velocities more accurately than simplified models kvočka et al 2015 costabile et al 2020 cozzolino et al 2019 conclude that the preferred model for floodplain simulations should be the full 2d swes as simplified models often suffer from a poor representation of receding flows and bed discontinuities still issues such as wetting and drying over complex terrain pose a numerical challenge and constitute an active area of research chen and noelle 2017 xia et al 2017 buttinger kreuzhuber et al 2019 if not treated properly numerical instabilities occur and lead to slow simulations the full swe offer a model to simulate both complex open channel hydrodynamics and overland flow processes costabile et al 2013 fernández pato et al 2016 in particular at high resolutions caviedes voullième et al 2020 recent studies costabile et al 2017 aricò and nasello 2018 caviedes voullième et al 2020 point out that solvers for the full swes might in fact require less computational time than their zero inertia counterparts popular scheme choices for the full swes include discontinuous galerkin dg kesserwani et al 2008 vater et al 2017 ayog et al 2021 and finite volume fv audusse et al 2004 horváth et al 2015 hou et al 2014 buttinger kreuzhuber et al 2019 dong and li 2021 methods second or higher order fv schemes are more accurate than their first order counterparts which are prone to numerical diffusion audusse and bristeau 2005 noelle et al 2006 2007 li et al 2014 navas montilla and murillo 2015 hou et al 2015 most second order schemes follow a monotonic upstream centered scheme for conservation laws muscl approach van leer 1979 resulting in a shock capturing scheme with reduced numerical diffusion yet without unphysical oscillations however compared to first order schemes the higher accuracy of second order schemes comes at the price of higher runtimes due to a reduced cfl constant and second order time integration thus in large scale flood modeling first order schemes are still commonly used xia et al 2019 echeverribar et al 2019 morales hernández et al 2021 in complex flash flood scenarios it is not immediately clear whether the application of a second order scheme pays off in terms of accuracy and required computational work a promising way to achieve computational speedups is the execution in a massively parallel fashion on supercomputers noh et al 2018 kuffour et al 2020 or on graphics processing units gpus lastra et al 2009 brodtkorb et al 2012 vacondio et al 2014 lacasta et al 2015 le et al 2015 horváth et al 2016 xing et al 2018 xia et al 2019 morales hernández et al 2021 to achieve a high computational performance on gpus regular grids are a convenient choice due to the structured arrangement of cores morales hernández et al 2020 unstructured meshes should be reordered for efficient memory access patterns allowing for coalescent transactions lacasta et al 2015 cutting edge flash flood models are on the verge of handling resolutions of 5 m for large regions of up to 2500 km 2 or 100 million cells xia et al 2019 traditionally cities are split into multiple smaller simulation regions that tend to underestimate inundation xing et al 2018 thus high resolution simulations at large scales e g spanning entire cities are needed a variety of infiltration models exist including the empirical soil conservation service scs curve number method chow et al 1988 aureli et al 2020 the empirical horton model fernández pato et al 2016 fernández pato and garcía navarro 2018 the semi empirical green ampt model fiedler and ramirez 2000 simons et al 2013 delestre et al 2017 fernández pato et al 2016 and more complex models such as richards equation maxwell 2013 le et al 2015 kuffour et al 2020 although capturing macropore flow zehe et al 2007 remains a challenge the model s ability to capture the local effects of green infrastructure gi such as green roofs rain gardens or bioswales is important in urban flood resilience planning berland et al 2017 fry and maxwell 2018 rosenzweig et al 2021 for urban flood hazard modeling the flow in sewer systems and its interaction with the overland flow may be relevant the storm water management model swmm is an established tool for routing stormwater in sewer systems it is developed by the environmental protection agency epa as an open source software package rossman 2017 a widely used approach to bidirectionally couple urban drainage networks to overland flow are dual drainage models leandro and martins 2016 yang et al 2020 li et al 2020 rosenzweig et al 2021 the interaction terms are commonly based on the water level differences between the sewer nodes and the surface water djordjević et al 2005 chen et al 2016 rubinato et al 2017 fernández pato and garcía navarro 2018 previous coupled models featuring a dynamic sewer network simulation e g leandro and martins 2016 fernández pato and garcía navarro 2018 noh et al 2018 yang et al 2020 were run on central processing units cpus and not exploiting recent leaps in model acceleration facilitated by gpus as the coupled models runtimes are typically governed by the surface simulation noh et al 2018 fast runoff models are crucial in this paper we present a coupled modeling framework for fast simulations in urban and rural settings the framework includes several components considered relevant in rainfall runoff modeling and flash flood hazard assessment that is spatially distributed interception and infiltration an accurate representation of overland flow and subsurface flow in sewer networks we go beyond current modeling practice by using both a spatially distributed rainfall runoff model and a fully bidirectional coupling of the sewer network accounting for drains and overflows at large scales and very high resolutions we propose a novel hybrid coupling approach of a gpu accelerated runoff simulation with an established cpu sewer network simulation we validate and test the framework in laboratory rural and urban scenarios we answer the question whether first order or second order schemes in the surface flow discretization of the full 2d swes should be favored in terms of the workload accuracy tradeoff moreover we highlight the influence of resolution and of the individual model components finally we address the extent of computational acceleration on a modern gpu for high resolution simulations of entire cities 2 methods 2 1 surface flow model the full shallow water equations swes are used to describe the surface flow and may be written in vector form as 1 t u x f u y g u s b u b s f u where u h h u h v t is the vector of conserved variables h represents the water height h u is the discharge along the x axis and h v is the discharge along the y axis f and g are the flux functions 2 f h u h u 2 1 2 g h 2 h u v g h v h u v h v 2 1 2 g h 2 the bed slope term s b 3 s b 0 g h x b g h y b models the fluid s acceleration due to the gravitational forces the friction term s f 4 s f 0 g n 2 h 1 3 u u 2 v 2 g n 2 h 1 3 v u 2 v 2 accounts for the bed friction here u and v are the average flow velocities in x and y directions respectively g is the gravitational constant b is the bed level assumed to be time independent and n is the manning friction coefficient to integrate the interception and infiltration processes of the runoff model and the sewer model with the surface flow coupling terms s r and s s respectively are added on the right hand side of eq 1 the coupling term for the sewers s s accounts for the specific sewer exchange discharge q e m s the source terms s r and s s are given by 5 s r r e t 0 0 and s s q e u 0 0 the runoff rate r e m s is the difference between the effective precipitation rate and the effective infiltration rate its precise application is defined in section 2 4 2 2 spatio temporal discretization of the surface flow for the spatial discretization of the swes the finite volume method fvm was chosen on a uniform cartesian grid although all runoff components can also be discretized on unstructured grids structured grids require less pre processing steps and enable fast memory access patterns on the gpu morales hernández et al 2020 moreover there is no need to store or optimize mesh connectivity the fvm discretizes the conserved variables u as cell averages yielding a system of ordinary differential equations for the cell averages u j k t for the simulation of the overland flow we employ either a first order accurate or a second order accurate scheme the first order scheme of chen and noelle 2017 to which we refer as cn scheme enables a better handling of flow states across bed discontinuities than the original hydrostatic reconstruction hr scheme proposed by audusse et al 2004 a second order accurate extension of the first order cn scheme is presented in buttinger kreuzhuber et al 2019 to which we refer as bh scheme the second order accuracy in space is achieved through a minmod limiter the minmod parameter is set to 1 in order to ensure robust and fast simulations horváth et al 2020 at wet dry boundaries only the velocities are set to zero below a cut off water depth threshold in the simulations this threshold is set to 0 1 mm the surface flow is discretized in time by the explicit euler s method for the first order cn scheme with a courant friedrichs lewy cfl constant of 0 5 to guarantee numerical stability and non negativity of the water depths the second order bh scheme is integrated in time with heun s method and the cfl constant is set to 0 25 the cfl condition restricts the time step δ t 6 δ t cfl δ x σ where δ x is the uniform grid resolution and σ is the maximum of the absolute values of the numerical wave speeds the numerical wave speeds at the cell interfaces are computed from the eigenvalues of the jacobian of the flux functions f and g buttinger kreuzhuber et al 2019 both schemes are mass conserving and preserve lake at rest steady states the friction source term s f is evaluated in a semi implicit manner by splitting it into a coefficient wise product of an implicitly evaluated state and an explicitly evaluated friction term s f brodtkorb et al 2012 buttinger kreuzhuber et al 2019 2 3 runoff model the spatially distributed runoff simulation integrates the surface flow routing component with an interception and an infiltration component determining the effective surface runoff first part of the rain is stored in the canopy of vegetation through interception second infiltration occurs as surface water percolates into permeable soils the remaining water effectively materializing during a rain event runs off the surface as overland flow the rainfall intensity is given by a time and space dependent precipitation rate p the integrated interception component reduces the effective precipitation and accounts for micro topographic depressions and losses due to vegetation the interception storage capacity i s is typically roughly around 1 mm robinson and ward 2017 except for rural forests where values as high as 5 mm were found xiao et al 1998 the cumulative interception i t until time t is modeled with a constant non negative rate i until a predefined storage capacity i s is reached thus the spatially distributed effective precipitation rate is given by 7 p e t p t i if i t i s p t else the infiltration process is modeled by the green ampt equation the cumulative infiltration f up to time t is 8 0 f t f f ψ h δ θ d f 0 t k s d t where k s is the saturated hydraulic conductivity the difference δ θ between the initial water content and the saturated water content of the soil is usually called effective porosity the suction head ψ represents the capillary attraction of the water towards the soil voids solving eq 8 for the infiltration rate f the time derivative of f we obtain 9 f t d f d t k s ψ h δ θ f t 1 the proposed dynamic infiltration model accounts for the surface water pressure via the surface water height h a shortcoming of the presented green ampt model is the inability to account for multi layered soils limited storage capacity soil water redistribution in dry phases and macropores extensions to overcome these limitations have been proposed corradini et al 2000 gowdish and muñoz carpena 2009 mohammadzadeh habili and heidarpour 2015 leandro et al 2016 2 4 temporal discretization of the runoff model the green ampt ga model is discretized in time with the implicit euler method solving eq 9 at every cell for every time step the infiltration depths f at time step t n 1 is given by 10 f n 1 1 2 f n k s δ t 1 2 f n k s δ t 2 4 k s δ t δ θ h 0 n ψ we note that even though the infiltration rate is undefined for f 0 the implicit euler method yields a well defined infiltration depth close to zero in contrast to the explicit euler method if the infiltration depth increment δ f defined by δ f n f n 1 f n exceeds the available surface water depth it is restricted to ensure a nonnegative surface water depth the effective infiltration rate is then given by 11 f e n 1 δ t n min δ f n h n where δ t n is the cfl limited timestep of the overland flow for timestep n for the runoff model it is enough to perform a simple integration for both the effective precipitation and infiltration rate we combine the precipitation and infiltration increment into a single effective runoff increment δ r e n 12 δ r e n δ t n p e n f e n the overland flow and runoff models are tightly coupled every step in the surface flow simulation is synchronized with the computation and application of the runoff update 2 5 gpu implementation of the runoff model the spatial discretization of the surface flow with the fvm enables straightforward parallelization on structured grids as only neighboring cells need to be considered when computing the next time step the gpu implementation of the runoff model uses the cuda platform of nvidia in cuda parallel tasks are organized into thread blocks and typically each parallel task each so called thread is associated with one cell a block size of 16 by 16 cells ensures a high utilization of the gpu horváth et al 2016 the number and positions of the neighboring cells required to compute the fluxes of a particular cell are called the computation stencil for the first order cn scheme only the direct neighbors are accessed for the second order bh scheme the computation of the minmod limited gradient requires a neighborhood of two cells to be accessed in the four axis aligned directions halo cells are required to exchange the data between adjacent blocks effectively fluxes are computed only for the inner block of 14 by 14 cells for the first order cn scheme or for 12 by 12 cells for the second order bh scheme due to the different computational stencils and hence the different number of halo cells required the gpu implementation handles data dependencies with on chip memory for fast processing of the computational stencil e g shuffles introduced with the kepler microarchitecture for implementation details the reader is referred to horváth et al 2016 gpu implementations of the swes operate either on single precision or on double precision floating point variables for single precision state variables the memory burden is lower and floating point operations are faster as gpus were originally optimized for single precision computations and therefore speed ups of up to eight times over double precision are reported morales hernández et al 2020 to alleviate the computational burden in the proposed runoff model both the surface state and the infiltration state are stored in single precision floating point variables in previous publications buttinger kreuzhuber et al 2019 horváth et al 2020 we verified that the employed surface flow model relying on single precision state variables captures lake at rest steady states displays the expected convergence order and is capable of handling wet dry transitions still truncation errors might accumulate over time which may have an effect on the mass balance liang and smith 2015 dazzi et al 2021 therefore we verify that the runoff model s mass balance is within machine precision in the presented test cases on a modern gpu with 24 gb of video memory the domain size in our computational model is limited to around 175 million active wet cells for the second order bh scheme with dynamic runoff and single precision floating point variables for the first order cn scheme the domain size is limited to around 225 million active cells as the first order time integration does not require the storage of an intermediate state 2 6 sewer network model in the storm water management model swmm a sewer network is represented by a set of nodes connected by links rossman 2017 links transmit pipe discharges q from one node to another a so called node assembly consists of the node and all connected half links these are the halves of the link that are connected to the node at each node assembly the change in the hydraulic head is modeled by the continuity equation the pipe flow is governed by the transient 1d swe i e 13 t q 2 u t a u 2 x a g a x h g a s f where u is the pipe flow velocity a is the cross sectional flow area h is the hydraulic head and s f is the friction slope in the pipes in the swmm eq 13 is solved with a finite difference scheme thus at each link momentum and continuity are conserved in contrast to the nodes where only continuity is conserved the continuous state variables in the time differences are approximated with their average values over the conduit length swmm 5 1 uses an implicit backwards euler method for the time discretization which is solved iteratively with picard s method we use the preissmann slot model implemented in the latest version of swmm 5 1 013 which is integrated in our coupled model setup unfortunately swmm does not exactly preserve water volumes continuity errors are typically within a few percent rossman 2006 and may be reduced by artificially introducing a finer spatial discretization pachaly et al 2020 swmm 5 1 is written in c c and is easily incorporated into existing c software such as the proposed modeling framework 2 7 bidirectional surface sewer coupling the surface sewer discharge exchange term depends on the water level at the surface w the hydraulic head at the manhole h and the bed surface elevation b in the following a m and d m are the manhole s area and diameter respectively d i is the distance between the surface and the invert level of the pipes entering the node the invert level refers to the lowest elevation admitting water flow following djordjević et al 2005 chen et al 2016 rubinato et al 2017 fernández pato and garcía navarro 2018 we distinguish between four cases 1 inflow into a non pressurized node 2 inflow into a pressurized node where the surface flow depth is small when compared to the node width 3 inflow into a pressurized node where the surface flow depth is large when compared to the node width 4 outflow onto the floodplain if the head in the pipe network is lower than the surface elevation i e h d i the discharge exchange q e m 3 s is given by the free weir equation 14 q e 2 3 c d w π d m 2 g 1 2 h 3 2 if the head in the pipe network exceeds the surface elevation i e h d i the discharge exchange is either given by the submerged weir equation 15 q e c d s w π d m 2 g 1 2 h h d i h 1 2 as long as h a m π d m if h a m π d m the node is considered fully submerged the submerged orifice equation 16 q e c d o a m 2 g 1 2 h d i h 1 2 is considered a more appropriate description for example for circular manholes the orifice equation is applied if h d m 4 the discharge coefficients for the free weir the submerged weir and the orifice equations are set to c d w 0 56 c d s w 0 11 and c d o 0 2 respectively rubinato et al 2017 if the head in the pipe system exceeds the water level of the surface flow an orifice equation is used djordjević et al 2005 assuming that the surface velocity is negligible the discharge exchange is given by 17 q e c d o a m 2 g 1 2 h h d i 1 2 this equation also holds for dry surface cells i e when h 0 with these four cases all exchange flow conditions are properly handled a negative exchange discharge q e indicates flow into the sewer network from the surface a positive value indicates sewer overflow sewer overflow also occurs if water flow from the roofs of the surrounding buildings exceeds the sewer inflow capacity consequently water spills over at this node in this case the roof water is directly added to the sewer overflow as excess discharges and not reduced by the surface sewer exchange equations eqs 14 17 the surface sewer coupling takes place only at the cells where manholes and inlets are connected to the surface to this end the geometry of the manholes and inlets is rasterized on the simulation grid effectively at each cell the perimeters and areas of all intersecting manholes and inlets are collected furthermore we also collect the contributions of each cell to each sewer node we are able to include cases where multiple manholes intersect the same cell as well as cases where multiple cells contribute to the same node for each rasterized cell we keep track of the corresponding node head by averaging over all nodes connected to the specified cell the sewer node exchange discharge eqs 14 17 are solved on a per cell basis where we account for the relative contributions 18 q e a j k q e j k the specific sewer exchange term for the surface model is limited by the water availability in the case of sewer inflow i e 19 q e j k min h j k δ t q e j k in time the sewer network model is interleaved with the surface and runoff model with an a priori defined coupling timestep δ t c at multiples of the coupling timestep information between the runoff model and the sewer model is exchanged the coupling time step size also represents an upper limit to the time step sizes of the individual simulators if not stated otherwise it is set to 1 s as a rule of thumb the coupling time step size should be roughly of the same order as the time step sizes of the individual solvers a time step of the coupled model is illustrated in fig 1 and subdivided into the following steps 1 exchange sewer surface coupling data i e provide node heads and excess discharges to the surface flow simulation and provide exchange discharges q e to the sewer model 2 advance the simulators in parallel from time t i to t i δ t c 3 compute the exchanged discharges in this coupling time step in the respective simulator each simulator performs multiple routines at each of these steps see fig 1 when coupling data need to be exchanged the simulators are required to wait for the other simulator at synchronization barriers in the proposed c framework the synchronization barriers are implemented with the functionality offered by std thread and boost barrier synchronization barriers are set only at the beginning of a coupled simulation time step but not in the individual simulator s advance methods the loop for the time steps in the advance step is executed independently of the other simulator to advance from t i to t i δ t c each simulator only needs the minimal amount of time steps required for its own numerical stability for the sewer simulation the exchange discharges are obtained from the surface simulation and applied to the sewer network additional inflows from external sources e g roofs are applied and compared with the sewer network s inflow capacities these excess discharges contribute to node overflow swmm routes the water flow in the sewer network then the sewer network state node depths and heads link discharges and volumes inflow volumes and excess volumes is updated the excess volumes accumulated during one coupling time step in the sewer network simulation are provided as excess discharges to the surface runoff simulation in the next coupling time step more precisely we compute the excess discharges prescribed in the surface simulation at time step t i 1 from the excess volumes accumulated in the sewer network simulation from t i to t i 1 t i δ t c as a consequence of the parallel execution during the coupling time step the state of the surface variables remains fixed for the sewer simulation and vice versa for the simulation of the surface flow and runoff node heads from the sewer simulation are necessary for the computation of the surface sewer exchange discharges once acquired the surface runoff simulation advances independently of the sewer simulation the routines in the surface runoff advance step correspond to typical routines in fvms for overland flow simulation these routines are performed on the gpu and looped until the next coupling time step t i δ t c is reached the surface state variables water depth and level surface flow discharges and the infiltration state variables are updated after timesteps δ t n according to section 2 4 after the loop the applied exchange discharges during the coupling time step are computed and are provided to the sewer simulation in the next coupling time step the coupled simulation time steps are executed until the simulation end time is reached as the sewer simulation and the surface runoff simulation are executed in parallel the execution time of the coupled surface sewer model is determined by the execution time of the slower simulator and not by the sum of the execution times as it would be the case for a sequential coupled simulation usually the one dimensional sewer network simulation is faster than the two dimensional surface flow simulation noh et al 2018 3 results and discussion we demonstrate the capabilities of the coupled model on laboratory and real world test cases the scenarios include a small scale rainfall runoff plot experiment a rural catchment and a full scale urban test case including sewer coupling more specifically we simulate the thiès plot experiment section 3 1 the hoal catchment at petzenkirchen austria section 3 3 and the city of cologne germany section 3 4 furthermore we validate the sewer surface coupling approach on a laboratory experiment presented in rubinato et al 2017 in section 3 2 the numerical simulations were performed on a desktop pc equipped with 10 intel i9 9820x cores at 3 3 ghz and 128 gb ram the gpu utilized for the test cases was an nvidia titan rtx in tesla compute cluster tcc mode it features 4608 cuda cores and has 24 gb memory in the following the term runtime describes the cumulative execution time of the simulation measured via wall clock timing the runtime neither includes the initialization process such as reading input data nor postprocessing steps such as writing results to the disk however the gpu runtime includes data transfer between the gpu and the cpu during simulation 3 1 thiès plot experiment we validate the model with measurements performed at in thiès senegal by tatard et al 2008 the experiment was carried out on a 10 4 m 2 plot the plot has an average slope of 1 and the resolution of the digital terrain model dtm is 0 1 m rainfall was simulated with a constant rate of 70 mm h for a duration of 1 h on the sandy soil in the reference data set of mügler et al 2011 measurements of mean flow velocities are available at 62 locations across the plot fig 2a following simons et al 2013 manning s roughness coefficient was set to a constant value of 0 014 m 1 3 s throughout the entire plot we compare the results from the first order cn and second order bh scheme for the steady state after 1 h the simulated water depths show a slightly clearer depiction of the flow paths in the second order scheme compare fig 2b c the simulated velocities are shown as arrows in fig 2b c for the cn and bh scheme in fig 3a we compare the simulated velocities with the measured velocities second order schemes are computationally more involved than first order schemes but are supposed to yield superior results due to the improved accuracy the root mean square error rmse of the velocities is defined by 20 rmse 1 n i v s i v o i 2 where n is the total number of all observed velocities v o i the rmses of the velocities are consistently lower for the second order scheme for all resolutions from 0 05 to 0 25 m as is shown in fig 3b and the achieved rmse of 0 026 m s is in line to results in the literature tatard et al 2008 mügler et al 2011 simons et al 2013 caviedes voullième et al 2020 however we emphasize that a proper discretization of the source term is important for the simulation of runoff processes even more so in the case of first order accurate schemes the superiority of the first order cn scheme above the popular hr scheme audusse et al 2004 is noticeable from the velocity errors in fig 3a and b the simulated velocities of the hr scheme are consistently lower than for the cn scheme as the hr scheme is not able to fully account for the bed slope in the case of shallow flow delestre et al 2012 switching to second order accuracy in the hr scheme fixes this issue albeit at the cost of a higher computational workload the velocities of the second order hr scheme are only slightly deviating from the bh scheme therefore they were excluded from the plots in fig 3 in line with numerical theory the rmse decreases with grid refinement in general as shown in fig 3b however the rmses of the second order scheme do not exhibit such a clear trend as the rmses of the first order scheme for the second order scheme convergence with regard to mesh refinement is limited for cell sizes below 0 1 m due to the resolution of the underlying dtm 0 1 m typically for smooth solution surfaces e g in river floods the second order accurate scheme is expected to be more efficient when considering the accuracy versus runtime tradeoff horváth et al 2020 when the corresponding runtimes of the cn and the bh schemes are compared in fig 3c the first order cn scheme produces better results for the same amount of computational time spent in terms of the tradeoff between computational workload and accuracy this suggests the use of finer grids together with first order schemes for the surface flow in rainfall runoff simulations summarizing we conclude that fast first order schemes which properly resolve the source term are sufficiently accurate for rainfall runoff simulations both the first order cn and the second order bh scheme are robust as no unphysical high numerical speeds develop over time fig 4 the time step δ t is inversely proportional to the maximum numerical speed σ by the cfl condition see 6 thus if the maximum numerical speeds are small large time steps are possible there is a small spike at the beginning of the wetting but its magnitude is reasonable in fig 5 we compare the effect of using single precision against double precision floating point numbers for the first order cn scheme on the cpu and the gpu in terms of the velocity rmses the differences are noticeable for a cell size of 20 cm but they are still very small 0 1 mm s the mass balance error for the 10 cm runoff model is below 0 0001 the rain volume is exactly 2 8 m 3 1 074432 m 3 are infiltrated 1 697895 m 3 are flowing out at the open southern boundary during the simulation and 0 027672 m 3 remain at the surface at the end of the simulation in terms of the runtimes the parallel single precision gpu implementation is more than 30 times faster than the sequential single precision cpu implementation for a resolution of 5 cm fig 5b in this case due to the low number of cells in the domain that is 16000 cells the gpu cannot fully exploit its parallel capabilities the modest speedup in this small scale experiment is therefore not representative of large real world test cases 3 2 surface sewer coupling in this section we present simulation results to verify the surface sewer coupling against experimental data provided by rubinato et al 2017 the setup consists of a flume and a single manhole connected to a pipe the surface bed is 4 m wide and 8 m long with a slope of 1 m per 1 km at the upper end a hydrograph with a constant discharge of 11 l s is specified at the outlet critical flow conditions are imposed the manhole is located 2 5 m downstream of the inlet and has a diameter of 0 24 m the invert level of the pipe is 0 478 m below the flume bed manning s roughness coefficient is set to 0 009 s m 1 3 for both the pipe and the flume as both are pvc in the experiment the pipe pressure and the surface level was measured 0 34 m and 0 35 m away from the manhole fig 6 first we tested steady state inflow from the surface into the sewer system for various prescribed surface discharges ranging from 5 l s to 11 l s the simulation reaches a steady state after 300 s the simulation is able to accurately reproduce the measured water depths and exchange flows fig 7 with a mean absolute error mae of 0 19 mm and relative differences ranging from 0 1 to 4 3 the accuracy is excellent in the second test case we simulated overflow from the sewer onto the wet flume the surface inflow is fixed at 11 l s and the pipe inflow ranges from 2 2 l s to 7 6 l s again the simulated water depths agree well with the measured water depths with a mae of 0 72 mm the relative differences range from 3 to 4 8 in this case unfortunately swmm cannot extract the pressure head at an arbitrary location along the pipe so instead we extracted the pressure head directly at the manhole the pressure head at the node is supposed to be lower than in the pipe as energy is dissipated in the transition from the pipe into the manhole this discrepancy is visible in fig 8b overall the coupled simulation correctly exchanges flows from the surface to the sewer system and vice versa as simulated and observed values agree the small differences are comparable to results in the literature rubinato et al 2017 fernández pato and garcía navarro 2018 3 3 hoal petzenkirchen this scenario analyzes a rainfall event in june 2013 in the hydrological open air laboratory hoal catchment in petzenkirchen lower austria the hoal catchment is used to test hydrological hypotheses under natural conditions the catchment is 0 66 km 2 in size and is mainly covered by arable land 87 and grassland 10 blöschl et al 2016 a high resolution 0 5 m dtm of 2012 was used the topographic elevation of the catchment ranges from 255 to 325 m a s l manning s roughness coefficient is set to 0 1 s m 1 3 for the riparian forest to 0 05 s m 1 3 for grassland and arable land and to 0 03 s m 1 3 everywhere else the saturated conductivities range from 1 to 32 mm h and are set according to literature values rawls et al 1983 carsel and parrish 1988 smith et al 2002 and measured values picciafuoco et al 2019 streets and the river bed are assumed to be impermeable we specify an interception storage capacity of 5 mm for the riparian forests around the outflow colored in light green in fig 9a and of 2 mm for the arable land and grassland the investigated heavy rain event starts on june 23 at 21 00 and is simulated for 1 5 days there are two distinct blocks of intense rainfall the first occurs after 1 h and the second after 28 h the main flow paths of the surface runoff for the second rain block are clearly visible fig 9a for the 1 m simulation 0 75 million cells are wet at the peak of the second rain block which corresponds to the simulated region of 0 75 km 2 including a small buffer zone around the catchment regarding the runtimes of the sequential cpu and the parallel gpu implementation speedups of three orders of magnitude are achieved table 1 the speedup increases with higher resolutions and higher workloads as the gpu is not fully utilized for a low resolution for a fully occupied gpu doubling the resolution causes a theoretical increase of the amount of work by eight times as the number of cells quadruples and due to the cfl condition twice the number of time steps are required for the 1 m simulation 90 of the computation time on the gpu is spent in the following routines the reconstruction and flux computation 32 the time integration and time step reduction 30 and the computation and integration of the runoff 28 in the latter routines it is not the amount of floating point operations but the memory transfers that prevent the gpu from achieving faster runtimes on the cpu the distribution is slightly different with 60 11 and 9 respectively due to faster memory access rates one drawback of the gpu implementation is the comparably longer development time parallel cpu implementations are possible neal et al 2010 noh et al 2018 morales hernández et al 2021 but even if the implementation achieves full parallelization speedups over thousand cpu cores would be needed to match the computational advantage of the gpu from an economical and ecological perspective the gpu simulation still performs better with regards to power consumption than a parallel cpu simulation running on a supercomputer the fast gpu simulation opens up new possibilities for this small catchment such as calibration tasks within reasonable time spans 3 4 urban flooding in cologne we study two urban scenarios in the city of cologne germany first we present a dual drainage model at the central part of the city at the eastern bank of the rhine river to which we refer as cologne center east second we present a city scale simulation encompassing the entire city of cologne with an area of 23 73 27 14 km 2 without sewer coupling in the first scenario the region simulated with the coupled model lies at the eastern bank of the rhine river and covers an area of 5 41 9 86 km 2 the terrain model is obtained from light detection and ranging lidar data where solid urban features such as buildings or bridges were removed in a pre processing step so that the dtm represents a so called bare earth dtm the resolution of the dtm is 1 m with a typical vertical accuracy around a decimeter kraus 2011 dottori et al 2013 the terrain is relatively flat mostly ranging between 40 and 60 m a s l fig 10a the simulation domain exhibits modest average slopes of 0 3 m km along the rhine river from south to north and of around 1 m km from east to west buildings and land use data are extracted from the official alkis data set of 2021 caffier et al 2017 buildings cover 13 of the area they are impermeable for the surface flow and water from roofs is routed to sewer nodes in the coupled model thus building cells remain dry during simulation roughness coefficients are mapped from the land use a detailed overview of the spatial distribution is shown in fig 10b the interception parameters are assumed to correlate with land use woods and gardens are assigned a storage capacity of 5 mm for public recreational areas and residential areas it is set to 2 mm rivers streets and parking lots are assumed to not retain rain thus their storage capacity is set to zero the infiltration parameters are derived from a soil map compare the saturated hydraulic conductivity in fig 10c streets and squares as well as rivers and lakes are considered impermeable we preprocessed sewer data for swmm for the eastern bank of the rhine river therefore cells west of the river are excluded from the simulation the active simulation region is thus restricted to 39 km 2 the sewer network consists of 6392 junction nodes and 7206 conduits linking them with a total length of 245 km moreover there are 16 pumps 16 outfalls and 18 weirs in the simulation domain which are included in the model the sewer network and the invalidated region are shown in fig 11a in yellow and pink respectively exchange between the sewer network and the surface is assumed to occur at the nodes with the parameters specified in section 2 7 each node has a maximum inflow capacity of 0 1 m 3 s rain that falls on buildings is directly routed to an assigned sewer junction node if the roof water discharge exceeds the capacity it spills over at the nodes we simulate a hypothetical uniform one hour rainfall of 53 mm h corresponding to approximately a 100 year event according to the kostra 2010r data set junghänel et al 2017 with a cell size of 1 1 m 2 the grid has nearly 40 million cells valid for simulation the water depths are aggregated in time during the first order 1 m simulation resulting in a maximum water depth for each cell at the end of the simulation fig 11b to illustrate the effects of the sewers the resolution and the order of accuracy of the surface flow scheme we focus on the region marked with a red frame in fig 11b the water depths of the coupled simulation for the specified region are aggregated in time resulting in maximum water depths fig 12a this heightfield serves as the reference for the difference heightfields where positive values indicate higher maximum water depths in the reference simulation than in the corresponding alternative simulations the differences between the simulation with sewer coupling and the one without are spatially restricted to the vicinity of the sewer network fig 12b for the simulation without sewer coupling we assume that water falling onto roofs can be drained by the sewer network in regions with positive values red the maximum water depths of the coupled simulation fig 12a are higher than the maximum water depths of the simulation without sewer coupling in terms of the mae the differences amount to 7 37 mm the mean signed error mse where the results of the runoff simulation without sewers are subtracted from those of the coupled simulation amounts to 6 60 mm this indicates that surface water levels do not rise as high in the coupled simulation due to sewer drainage in fact more water is drained from the streets than what is spilling onto the streets as excess roof water which exceeds node inflow capacities the sewer simulation also induces a water redistribution and causes minor floodings at a few streets due to sewer overflows overall the sewer simulation drains around 250000 m 3 of surface water in fig 12c the difference field resulting from the subtraction of the maximum water depths computed by a 4 m simulation from the 1 m simulation is displayed in regions with negative values blue the maximum water depths of the 4 m simulation are higher than the corresponding maximum water depths of the 1 m simulation the difference between the maximum water depths of the 4 m and the 1 m grids are spatially concentrated at certain locations and appear mostly where the dtm shows strong variations at the scale of the employed cell size for example major differences occur in the vicinity of underpasses and garage entrances or at the edge of elevated plateaus at the edges of buildings differences appear as boundary cells are rasterized as wall cells in one grid but not in the other overall the mae between the 4 m and the 1 m simulations is 8 12 mm for the mae computation the results of the 1 m grid are downsampled onto the 4 m grid in fig 12d we compare the first order accurate cn and the second order accurate bh scheme in general the differences in the maximum water depths between the two surface flow discretizations are marginal noticeable deviations are concentrated on a few spots and occur where relatively high velocities up to 1 m s develop this happens for example at sloped entrances to inner courtyards in the entire simulation domain the mae between the two schemes is 1 2 mm and the mse amounts to 0 1 mm as we consider differences between the maximum water depths occurring during the event a non nil mse does not indicate a volume error but rather indicates that the surface water travels a greater distance in fact the negative mse reveals that maximum water depths are slightly higher in the second order bh scheme in this comparison the errors are considerably smaller than in the previous comparisons the relatively small differences have to be considered in light of the mild terrain slopes to verify the validity of the proposed parallelized coupling approach we perform a comparison of the proposed coupling method with a tightly coupled simulation as in leandro and martins 2016 in the tightly coupled simulation the individual solvers execute one time step after each other so that each solvers advances exactly one time step with a fixed time step size of 0 1 s the continuity errors in the tightly coupled simulation amount to 1047 9 m 3 due to flow routing errors 942 3 m 3 in swmm the overall error can be considered acceptable when compared to the total sewer inflow volume 426802 8 m 3 furthermore we compare the maximum water depths for a cell size of 4 m across the entire simulation domain the mse is 0 32 mm and the mae is 0 38 mm differences between the maximum water depths of the proposed model and the tightly coupled model are predominantly located around overflowing sewer nodes see fig 13b which shows the region marked with a red frame in fig 11b still the mae between the two coupling approaches is small it is lower than the mae between first and second order scheme and considerably lower than the difference between the simulations on the 1 m and the 4 m grid we investigate the effect of the coupling time step size δ t c on the accuracy of the simulation results by considering continuity errors in the water volumes as well as differences in the maximum water depths with respect to the tightly coupled simulation see table 2 the overall inlet volume amounts to 1935514 8 m 3 the water volume on the surface and in the sewers ranges from 698052 3 m 3 to 762353 6 m 3 and the outlet volumes from 1238814 4 m 3 to 1243885 8 m 3 for the different coupling time step sizes the mass balance errors are dominated by the flow routing error of swmm table 2 overall the total mass balance errors is below 1 per mille for coupling time step sizes under 2 s and around 1 per mille for 4 s and 8 s the runoff simulation s time step δ t varies between 0 18 s and 0 32 s for the 4 m grid during the coupling time step 1 s around 4 steps of the surface runoff simulation and around 10 sewer routing steps are performed in average a coupling time step size of 1 s seems to offer a good compromise between computational performance and accuracy to thoroughly assess the quality of the predictive capabilities of the coupled model further validation is required the validation of coupled models on large scale scenarios is challenging as in most cases the collected data is sparse possible strategies to tackle this problem are the collection of crowd sourced data yu et al 2016 wang et al 2018 xing et al 2018 imagery from unmanned aerial vehicle sensing perks et al 2016 or insurance claims zischg et al 2018 a comparison of runtimes of the coupled simulator for resolutions of 1 m 2 m and 4 m in table 3 shows that the gpu accelerated 2d surface flow simulation is faster than the sequential 1d cpu sewer simulation for low resolutions this emphasizes once more the massive gain in computing power for the surface flow simulation due to the gpu acceleration usually solving the continuity and momentum equations for the sewer flow only requires around 0 1 of the total coupled simulation runtime for a sequential implementation noh et al 2018 in the proposed implementation as the simulations advance in parallel the runtime per coupling time step is determined by the slower coupling component which is either the gpu runoff simulation or the cpu sewer simulation thus in order to improve the model performance further effective parallelization strategies burger et al 2014 of the sewer module are necessary for the derivation of pluvial flood hazard maps we perform benchmark tests regarding large simulation domains with a size of 23 73 27 14 km 2 spanning the entire city of cologne we simulate uniform rainfall of 53 mm h that lasts for 1 h in order to account for surface flow routing after the rainfall ends the total simulated duration is extended to 2 h in this city scale scenario shown in fig 14a infiltration and interception are considered and are set up analogously to the previously studied smaller domain the 1 5 m simulation grid has a total number of 220 million cells valid for simulation of which 17 million cells are rasterized buildings in this scenario we do not explicitly simulate the sewer network of the entire city but we assume that water falling onto roofs can be drained by the sewer network therefore building cells are excluded from simulation all other input parameters remain unchanged in fig 14b we display the maximum water depths that occur during the simulated event the computational speed is faster than physical time with a total runtime of 1 62 h for a total simulated duration of 2 h the simulation uses up to 23 4 gb of memory on the nvidia titan rtx gpu close to its limit of 24 gb in previous studies urban regions of 40 km 2 were modeled with an efficient hybrid parallelization strategy on an adaptive grid with a minimum cell size of 1 m noh et al 2018 their model uses runoff coefficients depending on land use instead of a dynamic infiltration model as in this work but dynamic sewer coupling is also integrated in xing et al 2018 the city of fuzhou china is modeled with a resolution of 2 m and a constant drainage loss at streets neglecting bidirectional sewer interaction and water routing in sewers the simulation involves 66 million cells and runs almost in realtime on a rack of 8 nvidia tesla k80 gpus the proposed computational model is efficient in terms of memory consumption and performance it is able to simulate over 200 million cells on a single titan rtx gpu with 24 gb memory processing almost 10 million cells per gb of gpu memory it is useful to extend the capabilities of single gpu implementations to large regions as they typically run faster than comparable multi gpu implementations requiring inter gpu communication which introduces an additional bottleneck morales hernández et al 2021 in order to support larger simulation regions and higher spatial resolutions an indispensable extension to multiple gpus is possible 4 conclusions and perspectives in this study we present an integrated modeling framework for the simulation of rainfall runoff processes and urban flash floods the introduced modeling framework accounts for all major processes needed for an accurate description of flash floods while still accomplishing very fast runtimes infiltration is modeled with the green ampt equations in a fully dynamic and spatially distributed way an interception module accounts for initial rain abstractions due to vegetation instead of applying simple surface flow approximations we discretize the full 2d shallow water equations swes in the context of rainfall runoff simulations the first order accurate cn scheme chen and noelle 2017 was shown to be able to reproduce velocities and discharges accurately in particular using higher resolutions in the first order scheme proved to be more beneficial than using the second order accurate bh scheme buttinger kreuzhuber et al 2019 when considering the tradeoff between accuracy and required computational work we remark that an appropriate bed source term discretization of the surface flow is essential for providing correct velocity estimates for urban flash floods the rainfall runoff simulation is coupled with the sewer network simulation from the storm water management model swmm an effective approach for the bidirectional coupling of the sewer simulation to the surface runoff simulation is developed where the two simulators advance in parallel in each coupling timestep the runoff simulation is validated in the thiès irrigation experiment and the surface sewer coupling is validated in a laboratory experiment both showing good agreement between simulations and observations regarding the validation of the integrated model we point out that more detailed spatial observations are needed in order to assess the model s predictive performance in a more exhaustive way we demonstrate the benefits of using graphics processing units gpus as computational devices to speed up the rainfall runoff simulations the gpu accelerated model enables high resolution simulations for entire cities with simulation domains involving up to 225 million cells on a single gpu with 24 gb of memory in other words we enable simulation of regions up to 220 km 2 with a resolution of 1 m in realtime this removes the need for multiple localized small scale simulations speed ups of up to three orders of magnitude are achieved for simulated regions with around 10 million cells if compared against a serial cpu implementation the speedup increases with the number of cells thus large simulation regions profit even more from gpu acceleration the efficiency of the approach opens up new possibilities regarding ensemble simulations and high resolution environmental modeling the coupling of the surface flow with the spatially distributed infiltration and interception component allows the direct inclusion of green infrastructure by varying parameters accordingly detailed results help raise public awareness for flash floods by providing straightforward impact analysis at the scale of individual buildings and enable the analysis of efforts to mitigate the effects of climate change in rural and urban settings software availability the presented modeling framework visdom waser et al 2011 is developed in c the gpu implementation is written in the cuda language the visdom framework is closed source licensing information as well as information on cooperation in a joint research project with visdom is available on request from vrvis www vrvis at declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are supported by the vrvis zentrum für virtual reality und visualisierung forschungs gmbh and by the austrian science funds fwf as part of the vienna doctoral programme on water resource systems dk w1219 n28 vrvis is funded by bmk bmdw styria sfg tyrol and vienna business agency in the scope of comet competence centers for excellent technologies 879730 which is managed by ffg austria the authors thank d hesslow for contributing to the implementation of the runoff model the authors thank the bundesamt für wasserwirtschaft institut für kulturtechnik und bodenwasserhaushalt the stadtentwässerungsbetriebe steb köln www steb koeln de and the open data platform of nordrhein westfalen https open nrw for providing us with data necessary for setting up the petzenkirchen and cologne scenarios respectively 
25538,this paper presents an integrated modeling framework aiming at accurate predictions of flood hazard from heavy rainfalls the accuracy of such predictions generally depends on the complexity and resolution of the employed model components we propose an integration of complementary models in one framework that facilitates gpus to improve accuracy and simulation time the spatially distributed runoff model integrates surface flow routing based on the full shallow water equations infiltration based on the green ampt equation and interception in urban areas the runoff model is coupled with the storm water management model swmm the integrated model is validated and tested on laboratory rural and urban scenarios with regards to accuracy and computational efficiency the gpu acceleration yields speedups of 1000 times compared to a cpu implementation and enables the coupled simulation of flash floods at 1 m resolution for an urban area of 200 km 2 in realtime keywords spatially distributed rainfall runoff model gpu acceleration dual drainage surface sewer coupling flash flood hazard modeling data availability data will be made available on request 1 introduction floods are increasing in many parts of the world due to climate and land use change chen et al 2018 blöschl et al 2019 causing disproportionally high damage in urban regions jongman 2018 to mitigate future flood damage detailed models that assist in assessing the flood hazard spatially are crucial rosenzweig et al 2021 in contrast to lumped models spatially distributed models allow for an explicit representation of spatial variations and inhomogeneities in input data such as topography vegetation soil characteristics and urban features notwithstanding scale issues grayson and blöschl 2001 there is a lot of value in spatially distributed high resolution modeling for management purposes however higher resolutions lead to slower simulations in addition ensemble simulations that quantify the uncertainty of the predictions and provide insights into the effects of parameter variation increase the computational burden even further thus the challenge is to advance the capabilities of numerical modeling while balancing simulation performance and model accuracy to accurately represent the topography the resolution of the simulation grid should be chosen accordingly as a rule of thumb terrain features should be covered by at least 3 cells to be represented explicitly gallegos et al 2009 fewtrell et al 2011 horváth et al 2020 for urban areas a resolution of 2 m or less is considered necessary to accurately represent buildings curbstones and other features fewtrell et al 2011 dottori et al 2013 xing et al 2018 moreover simulations at submeter resolutions are useful for assessing the benefits of small scale alteration of street topography for flood risk management purposes at low costs de almeida et al 2016 the simulation grid is typically either a structured cartesian grid costabile et al 2013 horváth et al 2015 buttinger kreuzhuber et al 2019 or an unstructured triangular mesh bermúdez et al 1998 hou et al 2015 fernández pato et al 2016 unstructured meshes are able to incorporate complex geometries however they require time consuming mesh generation in contrast structured grids lack the pre processing step at the expense of poor resolution of topographic features not aligned with the grid thus they usually require a higher resolution in comparison to unstructured meshes to enable the same level of accuracy the shallow water equations swes are typically used for describing surface flow due to the courant friedrichs lewy cfl condition a high spatial resolution requires a fine temporal discretization therefore the total amount of computational work increases and in turn slows down simulation runs to accelerate the simulations one possibility is to simplify the shallow water model for example using diffusive wave sometimes also called zero inertia or kinematic wave approximations neal et al 2012 le et al 2015 fry and maxwell 2018 yang et al 2020 for urban regions the full or dynamic swes in combination with shock capturing schemes are able to reproduce observed hydraulic behavior and velocities more accurately than simplified models kvočka et al 2015 costabile et al 2020 cozzolino et al 2019 conclude that the preferred model for floodplain simulations should be the full 2d swes as simplified models often suffer from a poor representation of receding flows and bed discontinuities still issues such as wetting and drying over complex terrain pose a numerical challenge and constitute an active area of research chen and noelle 2017 xia et al 2017 buttinger kreuzhuber et al 2019 if not treated properly numerical instabilities occur and lead to slow simulations the full swe offer a model to simulate both complex open channel hydrodynamics and overland flow processes costabile et al 2013 fernández pato et al 2016 in particular at high resolutions caviedes voullième et al 2020 recent studies costabile et al 2017 aricò and nasello 2018 caviedes voullième et al 2020 point out that solvers for the full swes might in fact require less computational time than their zero inertia counterparts popular scheme choices for the full swes include discontinuous galerkin dg kesserwani et al 2008 vater et al 2017 ayog et al 2021 and finite volume fv audusse et al 2004 horváth et al 2015 hou et al 2014 buttinger kreuzhuber et al 2019 dong and li 2021 methods second or higher order fv schemes are more accurate than their first order counterparts which are prone to numerical diffusion audusse and bristeau 2005 noelle et al 2006 2007 li et al 2014 navas montilla and murillo 2015 hou et al 2015 most second order schemes follow a monotonic upstream centered scheme for conservation laws muscl approach van leer 1979 resulting in a shock capturing scheme with reduced numerical diffusion yet without unphysical oscillations however compared to first order schemes the higher accuracy of second order schemes comes at the price of higher runtimes due to a reduced cfl constant and second order time integration thus in large scale flood modeling first order schemes are still commonly used xia et al 2019 echeverribar et al 2019 morales hernández et al 2021 in complex flash flood scenarios it is not immediately clear whether the application of a second order scheme pays off in terms of accuracy and required computational work a promising way to achieve computational speedups is the execution in a massively parallel fashion on supercomputers noh et al 2018 kuffour et al 2020 or on graphics processing units gpus lastra et al 2009 brodtkorb et al 2012 vacondio et al 2014 lacasta et al 2015 le et al 2015 horváth et al 2016 xing et al 2018 xia et al 2019 morales hernández et al 2021 to achieve a high computational performance on gpus regular grids are a convenient choice due to the structured arrangement of cores morales hernández et al 2020 unstructured meshes should be reordered for efficient memory access patterns allowing for coalescent transactions lacasta et al 2015 cutting edge flash flood models are on the verge of handling resolutions of 5 m for large regions of up to 2500 km 2 or 100 million cells xia et al 2019 traditionally cities are split into multiple smaller simulation regions that tend to underestimate inundation xing et al 2018 thus high resolution simulations at large scales e g spanning entire cities are needed a variety of infiltration models exist including the empirical soil conservation service scs curve number method chow et al 1988 aureli et al 2020 the empirical horton model fernández pato et al 2016 fernández pato and garcía navarro 2018 the semi empirical green ampt model fiedler and ramirez 2000 simons et al 2013 delestre et al 2017 fernández pato et al 2016 and more complex models such as richards equation maxwell 2013 le et al 2015 kuffour et al 2020 although capturing macropore flow zehe et al 2007 remains a challenge the model s ability to capture the local effects of green infrastructure gi such as green roofs rain gardens or bioswales is important in urban flood resilience planning berland et al 2017 fry and maxwell 2018 rosenzweig et al 2021 for urban flood hazard modeling the flow in sewer systems and its interaction with the overland flow may be relevant the storm water management model swmm is an established tool for routing stormwater in sewer systems it is developed by the environmental protection agency epa as an open source software package rossman 2017 a widely used approach to bidirectionally couple urban drainage networks to overland flow are dual drainage models leandro and martins 2016 yang et al 2020 li et al 2020 rosenzweig et al 2021 the interaction terms are commonly based on the water level differences between the sewer nodes and the surface water djordjević et al 2005 chen et al 2016 rubinato et al 2017 fernández pato and garcía navarro 2018 previous coupled models featuring a dynamic sewer network simulation e g leandro and martins 2016 fernández pato and garcía navarro 2018 noh et al 2018 yang et al 2020 were run on central processing units cpus and not exploiting recent leaps in model acceleration facilitated by gpus as the coupled models runtimes are typically governed by the surface simulation noh et al 2018 fast runoff models are crucial in this paper we present a coupled modeling framework for fast simulations in urban and rural settings the framework includes several components considered relevant in rainfall runoff modeling and flash flood hazard assessment that is spatially distributed interception and infiltration an accurate representation of overland flow and subsurface flow in sewer networks we go beyond current modeling practice by using both a spatially distributed rainfall runoff model and a fully bidirectional coupling of the sewer network accounting for drains and overflows at large scales and very high resolutions we propose a novel hybrid coupling approach of a gpu accelerated runoff simulation with an established cpu sewer network simulation we validate and test the framework in laboratory rural and urban scenarios we answer the question whether first order or second order schemes in the surface flow discretization of the full 2d swes should be favored in terms of the workload accuracy tradeoff moreover we highlight the influence of resolution and of the individual model components finally we address the extent of computational acceleration on a modern gpu for high resolution simulations of entire cities 2 methods 2 1 surface flow model the full shallow water equations swes are used to describe the surface flow and may be written in vector form as 1 t u x f u y g u s b u b s f u where u h h u h v t is the vector of conserved variables h represents the water height h u is the discharge along the x axis and h v is the discharge along the y axis f and g are the flux functions 2 f h u h u 2 1 2 g h 2 h u v g h v h u v h v 2 1 2 g h 2 the bed slope term s b 3 s b 0 g h x b g h y b models the fluid s acceleration due to the gravitational forces the friction term s f 4 s f 0 g n 2 h 1 3 u u 2 v 2 g n 2 h 1 3 v u 2 v 2 accounts for the bed friction here u and v are the average flow velocities in x and y directions respectively g is the gravitational constant b is the bed level assumed to be time independent and n is the manning friction coefficient to integrate the interception and infiltration processes of the runoff model and the sewer model with the surface flow coupling terms s r and s s respectively are added on the right hand side of eq 1 the coupling term for the sewers s s accounts for the specific sewer exchange discharge q e m s the source terms s r and s s are given by 5 s r r e t 0 0 and s s q e u 0 0 the runoff rate r e m s is the difference between the effective precipitation rate and the effective infiltration rate its precise application is defined in section 2 4 2 2 spatio temporal discretization of the surface flow for the spatial discretization of the swes the finite volume method fvm was chosen on a uniform cartesian grid although all runoff components can also be discretized on unstructured grids structured grids require less pre processing steps and enable fast memory access patterns on the gpu morales hernández et al 2020 moreover there is no need to store or optimize mesh connectivity the fvm discretizes the conserved variables u as cell averages yielding a system of ordinary differential equations for the cell averages u j k t for the simulation of the overland flow we employ either a first order accurate or a second order accurate scheme the first order scheme of chen and noelle 2017 to which we refer as cn scheme enables a better handling of flow states across bed discontinuities than the original hydrostatic reconstruction hr scheme proposed by audusse et al 2004 a second order accurate extension of the first order cn scheme is presented in buttinger kreuzhuber et al 2019 to which we refer as bh scheme the second order accuracy in space is achieved through a minmod limiter the minmod parameter is set to 1 in order to ensure robust and fast simulations horváth et al 2020 at wet dry boundaries only the velocities are set to zero below a cut off water depth threshold in the simulations this threshold is set to 0 1 mm the surface flow is discretized in time by the explicit euler s method for the first order cn scheme with a courant friedrichs lewy cfl constant of 0 5 to guarantee numerical stability and non negativity of the water depths the second order bh scheme is integrated in time with heun s method and the cfl constant is set to 0 25 the cfl condition restricts the time step δ t 6 δ t cfl δ x σ where δ x is the uniform grid resolution and σ is the maximum of the absolute values of the numerical wave speeds the numerical wave speeds at the cell interfaces are computed from the eigenvalues of the jacobian of the flux functions f and g buttinger kreuzhuber et al 2019 both schemes are mass conserving and preserve lake at rest steady states the friction source term s f is evaluated in a semi implicit manner by splitting it into a coefficient wise product of an implicitly evaluated state and an explicitly evaluated friction term s f brodtkorb et al 2012 buttinger kreuzhuber et al 2019 2 3 runoff model the spatially distributed runoff simulation integrates the surface flow routing component with an interception and an infiltration component determining the effective surface runoff first part of the rain is stored in the canopy of vegetation through interception second infiltration occurs as surface water percolates into permeable soils the remaining water effectively materializing during a rain event runs off the surface as overland flow the rainfall intensity is given by a time and space dependent precipitation rate p the integrated interception component reduces the effective precipitation and accounts for micro topographic depressions and losses due to vegetation the interception storage capacity i s is typically roughly around 1 mm robinson and ward 2017 except for rural forests where values as high as 5 mm were found xiao et al 1998 the cumulative interception i t until time t is modeled with a constant non negative rate i until a predefined storage capacity i s is reached thus the spatially distributed effective precipitation rate is given by 7 p e t p t i if i t i s p t else the infiltration process is modeled by the green ampt equation the cumulative infiltration f up to time t is 8 0 f t f f ψ h δ θ d f 0 t k s d t where k s is the saturated hydraulic conductivity the difference δ θ between the initial water content and the saturated water content of the soil is usually called effective porosity the suction head ψ represents the capillary attraction of the water towards the soil voids solving eq 8 for the infiltration rate f the time derivative of f we obtain 9 f t d f d t k s ψ h δ θ f t 1 the proposed dynamic infiltration model accounts for the surface water pressure via the surface water height h a shortcoming of the presented green ampt model is the inability to account for multi layered soils limited storage capacity soil water redistribution in dry phases and macropores extensions to overcome these limitations have been proposed corradini et al 2000 gowdish and muñoz carpena 2009 mohammadzadeh habili and heidarpour 2015 leandro et al 2016 2 4 temporal discretization of the runoff model the green ampt ga model is discretized in time with the implicit euler method solving eq 9 at every cell for every time step the infiltration depths f at time step t n 1 is given by 10 f n 1 1 2 f n k s δ t 1 2 f n k s δ t 2 4 k s δ t δ θ h 0 n ψ we note that even though the infiltration rate is undefined for f 0 the implicit euler method yields a well defined infiltration depth close to zero in contrast to the explicit euler method if the infiltration depth increment δ f defined by δ f n f n 1 f n exceeds the available surface water depth it is restricted to ensure a nonnegative surface water depth the effective infiltration rate is then given by 11 f e n 1 δ t n min δ f n h n where δ t n is the cfl limited timestep of the overland flow for timestep n for the runoff model it is enough to perform a simple integration for both the effective precipitation and infiltration rate we combine the precipitation and infiltration increment into a single effective runoff increment δ r e n 12 δ r e n δ t n p e n f e n the overland flow and runoff models are tightly coupled every step in the surface flow simulation is synchronized with the computation and application of the runoff update 2 5 gpu implementation of the runoff model the spatial discretization of the surface flow with the fvm enables straightforward parallelization on structured grids as only neighboring cells need to be considered when computing the next time step the gpu implementation of the runoff model uses the cuda platform of nvidia in cuda parallel tasks are organized into thread blocks and typically each parallel task each so called thread is associated with one cell a block size of 16 by 16 cells ensures a high utilization of the gpu horváth et al 2016 the number and positions of the neighboring cells required to compute the fluxes of a particular cell are called the computation stencil for the first order cn scheme only the direct neighbors are accessed for the second order bh scheme the computation of the minmod limited gradient requires a neighborhood of two cells to be accessed in the four axis aligned directions halo cells are required to exchange the data between adjacent blocks effectively fluxes are computed only for the inner block of 14 by 14 cells for the first order cn scheme or for 12 by 12 cells for the second order bh scheme due to the different computational stencils and hence the different number of halo cells required the gpu implementation handles data dependencies with on chip memory for fast processing of the computational stencil e g shuffles introduced with the kepler microarchitecture for implementation details the reader is referred to horváth et al 2016 gpu implementations of the swes operate either on single precision or on double precision floating point variables for single precision state variables the memory burden is lower and floating point operations are faster as gpus were originally optimized for single precision computations and therefore speed ups of up to eight times over double precision are reported morales hernández et al 2020 to alleviate the computational burden in the proposed runoff model both the surface state and the infiltration state are stored in single precision floating point variables in previous publications buttinger kreuzhuber et al 2019 horváth et al 2020 we verified that the employed surface flow model relying on single precision state variables captures lake at rest steady states displays the expected convergence order and is capable of handling wet dry transitions still truncation errors might accumulate over time which may have an effect on the mass balance liang and smith 2015 dazzi et al 2021 therefore we verify that the runoff model s mass balance is within machine precision in the presented test cases on a modern gpu with 24 gb of video memory the domain size in our computational model is limited to around 175 million active wet cells for the second order bh scheme with dynamic runoff and single precision floating point variables for the first order cn scheme the domain size is limited to around 225 million active cells as the first order time integration does not require the storage of an intermediate state 2 6 sewer network model in the storm water management model swmm a sewer network is represented by a set of nodes connected by links rossman 2017 links transmit pipe discharges q from one node to another a so called node assembly consists of the node and all connected half links these are the halves of the link that are connected to the node at each node assembly the change in the hydraulic head is modeled by the continuity equation the pipe flow is governed by the transient 1d swe i e 13 t q 2 u t a u 2 x a g a x h g a s f where u is the pipe flow velocity a is the cross sectional flow area h is the hydraulic head and s f is the friction slope in the pipes in the swmm eq 13 is solved with a finite difference scheme thus at each link momentum and continuity are conserved in contrast to the nodes where only continuity is conserved the continuous state variables in the time differences are approximated with their average values over the conduit length swmm 5 1 uses an implicit backwards euler method for the time discretization which is solved iteratively with picard s method we use the preissmann slot model implemented in the latest version of swmm 5 1 013 which is integrated in our coupled model setup unfortunately swmm does not exactly preserve water volumes continuity errors are typically within a few percent rossman 2006 and may be reduced by artificially introducing a finer spatial discretization pachaly et al 2020 swmm 5 1 is written in c c and is easily incorporated into existing c software such as the proposed modeling framework 2 7 bidirectional surface sewer coupling the surface sewer discharge exchange term depends on the water level at the surface w the hydraulic head at the manhole h and the bed surface elevation b in the following a m and d m are the manhole s area and diameter respectively d i is the distance between the surface and the invert level of the pipes entering the node the invert level refers to the lowest elevation admitting water flow following djordjević et al 2005 chen et al 2016 rubinato et al 2017 fernández pato and garcía navarro 2018 we distinguish between four cases 1 inflow into a non pressurized node 2 inflow into a pressurized node where the surface flow depth is small when compared to the node width 3 inflow into a pressurized node where the surface flow depth is large when compared to the node width 4 outflow onto the floodplain if the head in the pipe network is lower than the surface elevation i e h d i the discharge exchange q e m 3 s is given by the free weir equation 14 q e 2 3 c d w π d m 2 g 1 2 h 3 2 if the head in the pipe network exceeds the surface elevation i e h d i the discharge exchange is either given by the submerged weir equation 15 q e c d s w π d m 2 g 1 2 h h d i h 1 2 as long as h a m π d m if h a m π d m the node is considered fully submerged the submerged orifice equation 16 q e c d o a m 2 g 1 2 h d i h 1 2 is considered a more appropriate description for example for circular manholes the orifice equation is applied if h d m 4 the discharge coefficients for the free weir the submerged weir and the orifice equations are set to c d w 0 56 c d s w 0 11 and c d o 0 2 respectively rubinato et al 2017 if the head in the pipe system exceeds the water level of the surface flow an orifice equation is used djordjević et al 2005 assuming that the surface velocity is negligible the discharge exchange is given by 17 q e c d o a m 2 g 1 2 h h d i 1 2 this equation also holds for dry surface cells i e when h 0 with these four cases all exchange flow conditions are properly handled a negative exchange discharge q e indicates flow into the sewer network from the surface a positive value indicates sewer overflow sewer overflow also occurs if water flow from the roofs of the surrounding buildings exceeds the sewer inflow capacity consequently water spills over at this node in this case the roof water is directly added to the sewer overflow as excess discharges and not reduced by the surface sewer exchange equations eqs 14 17 the surface sewer coupling takes place only at the cells where manholes and inlets are connected to the surface to this end the geometry of the manholes and inlets is rasterized on the simulation grid effectively at each cell the perimeters and areas of all intersecting manholes and inlets are collected furthermore we also collect the contributions of each cell to each sewer node we are able to include cases where multiple manholes intersect the same cell as well as cases where multiple cells contribute to the same node for each rasterized cell we keep track of the corresponding node head by averaging over all nodes connected to the specified cell the sewer node exchange discharge eqs 14 17 are solved on a per cell basis where we account for the relative contributions 18 q e a j k q e j k the specific sewer exchange term for the surface model is limited by the water availability in the case of sewer inflow i e 19 q e j k min h j k δ t q e j k in time the sewer network model is interleaved with the surface and runoff model with an a priori defined coupling timestep δ t c at multiples of the coupling timestep information between the runoff model and the sewer model is exchanged the coupling time step size also represents an upper limit to the time step sizes of the individual simulators if not stated otherwise it is set to 1 s as a rule of thumb the coupling time step size should be roughly of the same order as the time step sizes of the individual solvers a time step of the coupled model is illustrated in fig 1 and subdivided into the following steps 1 exchange sewer surface coupling data i e provide node heads and excess discharges to the surface flow simulation and provide exchange discharges q e to the sewer model 2 advance the simulators in parallel from time t i to t i δ t c 3 compute the exchanged discharges in this coupling time step in the respective simulator each simulator performs multiple routines at each of these steps see fig 1 when coupling data need to be exchanged the simulators are required to wait for the other simulator at synchronization barriers in the proposed c framework the synchronization barriers are implemented with the functionality offered by std thread and boost barrier synchronization barriers are set only at the beginning of a coupled simulation time step but not in the individual simulator s advance methods the loop for the time steps in the advance step is executed independently of the other simulator to advance from t i to t i δ t c each simulator only needs the minimal amount of time steps required for its own numerical stability for the sewer simulation the exchange discharges are obtained from the surface simulation and applied to the sewer network additional inflows from external sources e g roofs are applied and compared with the sewer network s inflow capacities these excess discharges contribute to node overflow swmm routes the water flow in the sewer network then the sewer network state node depths and heads link discharges and volumes inflow volumes and excess volumes is updated the excess volumes accumulated during one coupling time step in the sewer network simulation are provided as excess discharges to the surface runoff simulation in the next coupling time step more precisely we compute the excess discharges prescribed in the surface simulation at time step t i 1 from the excess volumes accumulated in the sewer network simulation from t i to t i 1 t i δ t c as a consequence of the parallel execution during the coupling time step the state of the surface variables remains fixed for the sewer simulation and vice versa for the simulation of the surface flow and runoff node heads from the sewer simulation are necessary for the computation of the surface sewer exchange discharges once acquired the surface runoff simulation advances independently of the sewer simulation the routines in the surface runoff advance step correspond to typical routines in fvms for overland flow simulation these routines are performed on the gpu and looped until the next coupling time step t i δ t c is reached the surface state variables water depth and level surface flow discharges and the infiltration state variables are updated after timesteps δ t n according to section 2 4 after the loop the applied exchange discharges during the coupling time step are computed and are provided to the sewer simulation in the next coupling time step the coupled simulation time steps are executed until the simulation end time is reached as the sewer simulation and the surface runoff simulation are executed in parallel the execution time of the coupled surface sewer model is determined by the execution time of the slower simulator and not by the sum of the execution times as it would be the case for a sequential coupled simulation usually the one dimensional sewer network simulation is faster than the two dimensional surface flow simulation noh et al 2018 3 results and discussion we demonstrate the capabilities of the coupled model on laboratory and real world test cases the scenarios include a small scale rainfall runoff plot experiment a rural catchment and a full scale urban test case including sewer coupling more specifically we simulate the thiès plot experiment section 3 1 the hoal catchment at petzenkirchen austria section 3 3 and the city of cologne germany section 3 4 furthermore we validate the sewer surface coupling approach on a laboratory experiment presented in rubinato et al 2017 in section 3 2 the numerical simulations were performed on a desktop pc equipped with 10 intel i9 9820x cores at 3 3 ghz and 128 gb ram the gpu utilized for the test cases was an nvidia titan rtx in tesla compute cluster tcc mode it features 4608 cuda cores and has 24 gb memory in the following the term runtime describes the cumulative execution time of the simulation measured via wall clock timing the runtime neither includes the initialization process such as reading input data nor postprocessing steps such as writing results to the disk however the gpu runtime includes data transfer between the gpu and the cpu during simulation 3 1 thiès plot experiment we validate the model with measurements performed at in thiès senegal by tatard et al 2008 the experiment was carried out on a 10 4 m 2 plot the plot has an average slope of 1 and the resolution of the digital terrain model dtm is 0 1 m rainfall was simulated with a constant rate of 70 mm h for a duration of 1 h on the sandy soil in the reference data set of mügler et al 2011 measurements of mean flow velocities are available at 62 locations across the plot fig 2a following simons et al 2013 manning s roughness coefficient was set to a constant value of 0 014 m 1 3 s throughout the entire plot we compare the results from the first order cn and second order bh scheme for the steady state after 1 h the simulated water depths show a slightly clearer depiction of the flow paths in the second order scheme compare fig 2b c the simulated velocities are shown as arrows in fig 2b c for the cn and bh scheme in fig 3a we compare the simulated velocities with the measured velocities second order schemes are computationally more involved than first order schemes but are supposed to yield superior results due to the improved accuracy the root mean square error rmse of the velocities is defined by 20 rmse 1 n i v s i v o i 2 where n is the total number of all observed velocities v o i the rmses of the velocities are consistently lower for the second order scheme for all resolutions from 0 05 to 0 25 m as is shown in fig 3b and the achieved rmse of 0 026 m s is in line to results in the literature tatard et al 2008 mügler et al 2011 simons et al 2013 caviedes voullième et al 2020 however we emphasize that a proper discretization of the source term is important for the simulation of runoff processes even more so in the case of first order accurate schemes the superiority of the first order cn scheme above the popular hr scheme audusse et al 2004 is noticeable from the velocity errors in fig 3a and b the simulated velocities of the hr scheme are consistently lower than for the cn scheme as the hr scheme is not able to fully account for the bed slope in the case of shallow flow delestre et al 2012 switching to second order accuracy in the hr scheme fixes this issue albeit at the cost of a higher computational workload the velocities of the second order hr scheme are only slightly deviating from the bh scheme therefore they were excluded from the plots in fig 3 in line with numerical theory the rmse decreases with grid refinement in general as shown in fig 3b however the rmses of the second order scheme do not exhibit such a clear trend as the rmses of the first order scheme for the second order scheme convergence with regard to mesh refinement is limited for cell sizes below 0 1 m due to the resolution of the underlying dtm 0 1 m typically for smooth solution surfaces e g in river floods the second order accurate scheme is expected to be more efficient when considering the accuracy versus runtime tradeoff horváth et al 2020 when the corresponding runtimes of the cn and the bh schemes are compared in fig 3c the first order cn scheme produces better results for the same amount of computational time spent in terms of the tradeoff between computational workload and accuracy this suggests the use of finer grids together with first order schemes for the surface flow in rainfall runoff simulations summarizing we conclude that fast first order schemes which properly resolve the source term are sufficiently accurate for rainfall runoff simulations both the first order cn and the second order bh scheme are robust as no unphysical high numerical speeds develop over time fig 4 the time step δ t is inversely proportional to the maximum numerical speed σ by the cfl condition see 6 thus if the maximum numerical speeds are small large time steps are possible there is a small spike at the beginning of the wetting but its magnitude is reasonable in fig 5 we compare the effect of using single precision against double precision floating point numbers for the first order cn scheme on the cpu and the gpu in terms of the velocity rmses the differences are noticeable for a cell size of 20 cm but they are still very small 0 1 mm s the mass balance error for the 10 cm runoff model is below 0 0001 the rain volume is exactly 2 8 m 3 1 074432 m 3 are infiltrated 1 697895 m 3 are flowing out at the open southern boundary during the simulation and 0 027672 m 3 remain at the surface at the end of the simulation in terms of the runtimes the parallel single precision gpu implementation is more than 30 times faster than the sequential single precision cpu implementation for a resolution of 5 cm fig 5b in this case due to the low number of cells in the domain that is 16000 cells the gpu cannot fully exploit its parallel capabilities the modest speedup in this small scale experiment is therefore not representative of large real world test cases 3 2 surface sewer coupling in this section we present simulation results to verify the surface sewer coupling against experimental data provided by rubinato et al 2017 the setup consists of a flume and a single manhole connected to a pipe the surface bed is 4 m wide and 8 m long with a slope of 1 m per 1 km at the upper end a hydrograph with a constant discharge of 11 l s is specified at the outlet critical flow conditions are imposed the manhole is located 2 5 m downstream of the inlet and has a diameter of 0 24 m the invert level of the pipe is 0 478 m below the flume bed manning s roughness coefficient is set to 0 009 s m 1 3 for both the pipe and the flume as both are pvc in the experiment the pipe pressure and the surface level was measured 0 34 m and 0 35 m away from the manhole fig 6 first we tested steady state inflow from the surface into the sewer system for various prescribed surface discharges ranging from 5 l s to 11 l s the simulation reaches a steady state after 300 s the simulation is able to accurately reproduce the measured water depths and exchange flows fig 7 with a mean absolute error mae of 0 19 mm and relative differences ranging from 0 1 to 4 3 the accuracy is excellent in the second test case we simulated overflow from the sewer onto the wet flume the surface inflow is fixed at 11 l s and the pipe inflow ranges from 2 2 l s to 7 6 l s again the simulated water depths agree well with the measured water depths with a mae of 0 72 mm the relative differences range from 3 to 4 8 in this case unfortunately swmm cannot extract the pressure head at an arbitrary location along the pipe so instead we extracted the pressure head directly at the manhole the pressure head at the node is supposed to be lower than in the pipe as energy is dissipated in the transition from the pipe into the manhole this discrepancy is visible in fig 8b overall the coupled simulation correctly exchanges flows from the surface to the sewer system and vice versa as simulated and observed values agree the small differences are comparable to results in the literature rubinato et al 2017 fernández pato and garcía navarro 2018 3 3 hoal petzenkirchen this scenario analyzes a rainfall event in june 2013 in the hydrological open air laboratory hoal catchment in petzenkirchen lower austria the hoal catchment is used to test hydrological hypotheses under natural conditions the catchment is 0 66 km 2 in size and is mainly covered by arable land 87 and grassland 10 blöschl et al 2016 a high resolution 0 5 m dtm of 2012 was used the topographic elevation of the catchment ranges from 255 to 325 m a s l manning s roughness coefficient is set to 0 1 s m 1 3 for the riparian forest to 0 05 s m 1 3 for grassland and arable land and to 0 03 s m 1 3 everywhere else the saturated conductivities range from 1 to 32 mm h and are set according to literature values rawls et al 1983 carsel and parrish 1988 smith et al 2002 and measured values picciafuoco et al 2019 streets and the river bed are assumed to be impermeable we specify an interception storage capacity of 5 mm for the riparian forests around the outflow colored in light green in fig 9a and of 2 mm for the arable land and grassland the investigated heavy rain event starts on june 23 at 21 00 and is simulated for 1 5 days there are two distinct blocks of intense rainfall the first occurs after 1 h and the second after 28 h the main flow paths of the surface runoff for the second rain block are clearly visible fig 9a for the 1 m simulation 0 75 million cells are wet at the peak of the second rain block which corresponds to the simulated region of 0 75 km 2 including a small buffer zone around the catchment regarding the runtimes of the sequential cpu and the parallel gpu implementation speedups of three orders of magnitude are achieved table 1 the speedup increases with higher resolutions and higher workloads as the gpu is not fully utilized for a low resolution for a fully occupied gpu doubling the resolution causes a theoretical increase of the amount of work by eight times as the number of cells quadruples and due to the cfl condition twice the number of time steps are required for the 1 m simulation 90 of the computation time on the gpu is spent in the following routines the reconstruction and flux computation 32 the time integration and time step reduction 30 and the computation and integration of the runoff 28 in the latter routines it is not the amount of floating point operations but the memory transfers that prevent the gpu from achieving faster runtimes on the cpu the distribution is slightly different with 60 11 and 9 respectively due to faster memory access rates one drawback of the gpu implementation is the comparably longer development time parallel cpu implementations are possible neal et al 2010 noh et al 2018 morales hernández et al 2021 but even if the implementation achieves full parallelization speedups over thousand cpu cores would be needed to match the computational advantage of the gpu from an economical and ecological perspective the gpu simulation still performs better with regards to power consumption than a parallel cpu simulation running on a supercomputer the fast gpu simulation opens up new possibilities for this small catchment such as calibration tasks within reasonable time spans 3 4 urban flooding in cologne we study two urban scenarios in the city of cologne germany first we present a dual drainage model at the central part of the city at the eastern bank of the rhine river to which we refer as cologne center east second we present a city scale simulation encompassing the entire city of cologne with an area of 23 73 27 14 km 2 without sewer coupling in the first scenario the region simulated with the coupled model lies at the eastern bank of the rhine river and covers an area of 5 41 9 86 km 2 the terrain model is obtained from light detection and ranging lidar data where solid urban features such as buildings or bridges were removed in a pre processing step so that the dtm represents a so called bare earth dtm the resolution of the dtm is 1 m with a typical vertical accuracy around a decimeter kraus 2011 dottori et al 2013 the terrain is relatively flat mostly ranging between 40 and 60 m a s l fig 10a the simulation domain exhibits modest average slopes of 0 3 m km along the rhine river from south to north and of around 1 m km from east to west buildings and land use data are extracted from the official alkis data set of 2021 caffier et al 2017 buildings cover 13 of the area they are impermeable for the surface flow and water from roofs is routed to sewer nodes in the coupled model thus building cells remain dry during simulation roughness coefficients are mapped from the land use a detailed overview of the spatial distribution is shown in fig 10b the interception parameters are assumed to correlate with land use woods and gardens are assigned a storage capacity of 5 mm for public recreational areas and residential areas it is set to 2 mm rivers streets and parking lots are assumed to not retain rain thus their storage capacity is set to zero the infiltration parameters are derived from a soil map compare the saturated hydraulic conductivity in fig 10c streets and squares as well as rivers and lakes are considered impermeable we preprocessed sewer data for swmm for the eastern bank of the rhine river therefore cells west of the river are excluded from the simulation the active simulation region is thus restricted to 39 km 2 the sewer network consists of 6392 junction nodes and 7206 conduits linking them with a total length of 245 km moreover there are 16 pumps 16 outfalls and 18 weirs in the simulation domain which are included in the model the sewer network and the invalidated region are shown in fig 11a in yellow and pink respectively exchange between the sewer network and the surface is assumed to occur at the nodes with the parameters specified in section 2 7 each node has a maximum inflow capacity of 0 1 m 3 s rain that falls on buildings is directly routed to an assigned sewer junction node if the roof water discharge exceeds the capacity it spills over at the nodes we simulate a hypothetical uniform one hour rainfall of 53 mm h corresponding to approximately a 100 year event according to the kostra 2010r data set junghänel et al 2017 with a cell size of 1 1 m 2 the grid has nearly 40 million cells valid for simulation the water depths are aggregated in time during the first order 1 m simulation resulting in a maximum water depth for each cell at the end of the simulation fig 11b to illustrate the effects of the sewers the resolution and the order of accuracy of the surface flow scheme we focus on the region marked with a red frame in fig 11b the water depths of the coupled simulation for the specified region are aggregated in time resulting in maximum water depths fig 12a this heightfield serves as the reference for the difference heightfields where positive values indicate higher maximum water depths in the reference simulation than in the corresponding alternative simulations the differences between the simulation with sewer coupling and the one without are spatially restricted to the vicinity of the sewer network fig 12b for the simulation without sewer coupling we assume that water falling onto roofs can be drained by the sewer network in regions with positive values red the maximum water depths of the coupled simulation fig 12a are higher than the maximum water depths of the simulation without sewer coupling in terms of the mae the differences amount to 7 37 mm the mean signed error mse where the results of the runoff simulation without sewers are subtracted from those of the coupled simulation amounts to 6 60 mm this indicates that surface water levels do not rise as high in the coupled simulation due to sewer drainage in fact more water is drained from the streets than what is spilling onto the streets as excess roof water which exceeds node inflow capacities the sewer simulation also induces a water redistribution and causes minor floodings at a few streets due to sewer overflows overall the sewer simulation drains around 250000 m 3 of surface water in fig 12c the difference field resulting from the subtraction of the maximum water depths computed by a 4 m simulation from the 1 m simulation is displayed in regions with negative values blue the maximum water depths of the 4 m simulation are higher than the corresponding maximum water depths of the 1 m simulation the difference between the maximum water depths of the 4 m and the 1 m grids are spatially concentrated at certain locations and appear mostly where the dtm shows strong variations at the scale of the employed cell size for example major differences occur in the vicinity of underpasses and garage entrances or at the edge of elevated plateaus at the edges of buildings differences appear as boundary cells are rasterized as wall cells in one grid but not in the other overall the mae between the 4 m and the 1 m simulations is 8 12 mm for the mae computation the results of the 1 m grid are downsampled onto the 4 m grid in fig 12d we compare the first order accurate cn and the second order accurate bh scheme in general the differences in the maximum water depths between the two surface flow discretizations are marginal noticeable deviations are concentrated on a few spots and occur where relatively high velocities up to 1 m s develop this happens for example at sloped entrances to inner courtyards in the entire simulation domain the mae between the two schemes is 1 2 mm and the mse amounts to 0 1 mm as we consider differences between the maximum water depths occurring during the event a non nil mse does not indicate a volume error but rather indicates that the surface water travels a greater distance in fact the negative mse reveals that maximum water depths are slightly higher in the second order bh scheme in this comparison the errors are considerably smaller than in the previous comparisons the relatively small differences have to be considered in light of the mild terrain slopes to verify the validity of the proposed parallelized coupling approach we perform a comparison of the proposed coupling method with a tightly coupled simulation as in leandro and martins 2016 in the tightly coupled simulation the individual solvers execute one time step after each other so that each solvers advances exactly one time step with a fixed time step size of 0 1 s the continuity errors in the tightly coupled simulation amount to 1047 9 m 3 due to flow routing errors 942 3 m 3 in swmm the overall error can be considered acceptable when compared to the total sewer inflow volume 426802 8 m 3 furthermore we compare the maximum water depths for a cell size of 4 m across the entire simulation domain the mse is 0 32 mm and the mae is 0 38 mm differences between the maximum water depths of the proposed model and the tightly coupled model are predominantly located around overflowing sewer nodes see fig 13b which shows the region marked with a red frame in fig 11b still the mae between the two coupling approaches is small it is lower than the mae between first and second order scheme and considerably lower than the difference between the simulations on the 1 m and the 4 m grid we investigate the effect of the coupling time step size δ t c on the accuracy of the simulation results by considering continuity errors in the water volumes as well as differences in the maximum water depths with respect to the tightly coupled simulation see table 2 the overall inlet volume amounts to 1935514 8 m 3 the water volume on the surface and in the sewers ranges from 698052 3 m 3 to 762353 6 m 3 and the outlet volumes from 1238814 4 m 3 to 1243885 8 m 3 for the different coupling time step sizes the mass balance errors are dominated by the flow routing error of swmm table 2 overall the total mass balance errors is below 1 per mille for coupling time step sizes under 2 s and around 1 per mille for 4 s and 8 s the runoff simulation s time step δ t varies between 0 18 s and 0 32 s for the 4 m grid during the coupling time step 1 s around 4 steps of the surface runoff simulation and around 10 sewer routing steps are performed in average a coupling time step size of 1 s seems to offer a good compromise between computational performance and accuracy to thoroughly assess the quality of the predictive capabilities of the coupled model further validation is required the validation of coupled models on large scale scenarios is challenging as in most cases the collected data is sparse possible strategies to tackle this problem are the collection of crowd sourced data yu et al 2016 wang et al 2018 xing et al 2018 imagery from unmanned aerial vehicle sensing perks et al 2016 or insurance claims zischg et al 2018 a comparison of runtimes of the coupled simulator for resolutions of 1 m 2 m and 4 m in table 3 shows that the gpu accelerated 2d surface flow simulation is faster than the sequential 1d cpu sewer simulation for low resolutions this emphasizes once more the massive gain in computing power for the surface flow simulation due to the gpu acceleration usually solving the continuity and momentum equations for the sewer flow only requires around 0 1 of the total coupled simulation runtime for a sequential implementation noh et al 2018 in the proposed implementation as the simulations advance in parallel the runtime per coupling time step is determined by the slower coupling component which is either the gpu runoff simulation or the cpu sewer simulation thus in order to improve the model performance further effective parallelization strategies burger et al 2014 of the sewer module are necessary for the derivation of pluvial flood hazard maps we perform benchmark tests regarding large simulation domains with a size of 23 73 27 14 km 2 spanning the entire city of cologne we simulate uniform rainfall of 53 mm h that lasts for 1 h in order to account for surface flow routing after the rainfall ends the total simulated duration is extended to 2 h in this city scale scenario shown in fig 14a infiltration and interception are considered and are set up analogously to the previously studied smaller domain the 1 5 m simulation grid has a total number of 220 million cells valid for simulation of which 17 million cells are rasterized buildings in this scenario we do not explicitly simulate the sewer network of the entire city but we assume that water falling onto roofs can be drained by the sewer network therefore building cells are excluded from simulation all other input parameters remain unchanged in fig 14b we display the maximum water depths that occur during the simulated event the computational speed is faster than physical time with a total runtime of 1 62 h for a total simulated duration of 2 h the simulation uses up to 23 4 gb of memory on the nvidia titan rtx gpu close to its limit of 24 gb in previous studies urban regions of 40 km 2 were modeled with an efficient hybrid parallelization strategy on an adaptive grid with a minimum cell size of 1 m noh et al 2018 their model uses runoff coefficients depending on land use instead of a dynamic infiltration model as in this work but dynamic sewer coupling is also integrated in xing et al 2018 the city of fuzhou china is modeled with a resolution of 2 m and a constant drainage loss at streets neglecting bidirectional sewer interaction and water routing in sewers the simulation involves 66 million cells and runs almost in realtime on a rack of 8 nvidia tesla k80 gpus the proposed computational model is efficient in terms of memory consumption and performance it is able to simulate over 200 million cells on a single titan rtx gpu with 24 gb memory processing almost 10 million cells per gb of gpu memory it is useful to extend the capabilities of single gpu implementations to large regions as they typically run faster than comparable multi gpu implementations requiring inter gpu communication which introduces an additional bottleneck morales hernández et al 2021 in order to support larger simulation regions and higher spatial resolutions an indispensable extension to multiple gpus is possible 4 conclusions and perspectives in this study we present an integrated modeling framework for the simulation of rainfall runoff processes and urban flash floods the introduced modeling framework accounts for all major processes needed for an accurate description of flash floods while still accomplishing very fast runtimes infiltration is modeled with the green ampt equations in a fully dynamic and spatially distributed way an interception module accounts for initial rain abstractions due to vegetation instead of applying simple surface flow approximations we discretize the full 2d shallow water equations swes in the context of rainfall runoff simulations the first order accurate cn scheme chen and noelle 2017 was shown to be able to reproduce velocities and discharges accurately in particular using higher resolutions in the first order scheme proved to be more beneficial than using the second order accurate bh scheme buttinger kreuzhuber et al 2019 when considering the tradeoff between accuracy and required computational work we remark that an appropriate bed source term discretization of the surface flow is essential for providing correct velocity estimates for urban flash floods the rainfall runoff simulation is coupled with the sewer network simulation from the storm water management model swmm an effective approach for the bidirectional coupling of the sewer simulation to the surface runoff simulation is developed where the two simulators advance in parallel in each coupling timestep the runoff simulation is validated in the thiès irrigation experiment and the surface sewer coupling is validated in a laboratory experiment both showing good agreement between simulations and observations regarding the validation of the integrated model we point out that more detailed spatial observations are needed in order to assess the model s predictive performance in a more exhaustive way we demonstrate the benefits of using graphics processing units gpus as computational devices to speed up the rainfall runoff simulations the gpu accelerated model enables high resolution simulations for entire cities with simulation domains involving up to 225 million cells on a single gpu with 24 gb of memory in other words we enable simulation of regions up to 220 km 2 with a resolution of 1 m in realtime this removes the need for multiple localized small scale simulations speed ups of up to three orders of magnitude are achieved for simulated regions with around 10 million cells if compared against a serial cpu implementation the speedup increases with the number of cells thus large simulation regions profit even more from gpu acceleration the efficiency of the approach opens up new possibilities regarding ensemble simulations and high resolution environmental modeling the coupling of the surface flow with the spatially distributed infiltration and interception component allows the direct inclusion of green infrastructure by varying parameters accordingly detailed results help raise public awareness for flash floods by providing straightforward impact analysis at the scale of individual buildings and enable the analysis of efforts to mitigate the effects of climate change in rural and urban settings software availability the presented modeling framework visdom waser et al 2011 is developed in c the gpu implementation is written in the cuda language the visdom framework is closed source licensing information as well as information on cooperation in a joint research project with visdom is available on request from vrvis www vrvis at declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are supported by the vrvis zentrum für virtual reality und visualisierung forschungs gmbh and by the austrian science funds fwf as part of the vienna doctoral programme on water resource systems dk w1219 n28 vrvis is funded by bmk bmdw styria sfg tyrol and vienna business agency in the scope of comet competence centers for excellent technologies 879730 which is managed by ffg austria the authors thank d hesslow for contributing to the implementation of the runoff model the authors thank the bundesamt für wasserwirtschaft institut für kulturtechnik und bodenwasserhaushalt the stadtentwässerungsbetriebe steb köln www steb koeln de and the open data platform of nordrhein westfalen https open nrw for providing us with data necessary for setting up the petzenkirchen and cologne scenarios respectively 
25539,efficient simulation of water flow processes in the vadose zone is crucial to increase agricultural productivity within environmental limits this requires deriving detailed soil hydraulic parameters of the soil profile that is highly challenging particularly for heterogeneous soils we therefore developed an alternative indirect methodology to calibrate the hydraulic parameters from soil water content time series measured at multiple depths by using the new physically based hydrological model hypix we propose a novel efficient multistep optimization algorithm for layered soils that derives an optimal set of hydraulic parameters for a desired number of soil layers for each selected soil layer hypix derives five physical bimodal kosugi hydraulic parameters that describe the soil water retention and hydraulic conductivity by using a novel algorithm that reduces the degree of sensitivity and freedom of the parameters the optimization algorithm upscales the soil hydraulic parameters by gradually incorporating the soil heterogeneity this method overcomes the problems associated with optimization of the hydraulic parameters of each layer individually which leads to poor results because it does not represent the cohesive soil water dynamics across the unsaturated zone we tested the method using soil water content measurements at different depths at five heterogeneous experimental sites in new zealand we show how the accuracy of the simulated water balance components increases with the number of soil layers the multistep optimization upscales a detailed layered profile of soil hydraulic parameters into a model with fewer layers the methodology developed provides an estimate of the uncertainty of using a reduced number of soil layers we also show that a pedological description can provide an indication of the minimum soil layers of vertical discretization required to accurately compute the soil water balance components graphical abstract image 1 keywords variably saturated flow vertical discretization multistep optimization inverse modelling soil hydraulic properties non uniqueness julia language 1 introduction the digital soil database s map https smap landcareresearch co nz lilburne et al 2004 mcneill et al 2018 with a coverage of 37 of new zealand in 2021 provides the kosugi 1994 1996 soil water retention θ ψ and the unsaturated hydraulic conductivity k θ pollacco et al 2013b 2017 for up to six functional horizons to a maximum depth of 1 m on a 150 m 150 m spatial grid this soil hydrological property database is termed smap hydro however most hydrological models are not designed to accommodate multiple layers and therefore require soil hydrological characterization at a coarser vertical resolution e g one two or three layers here we determine how to upscale hydraulic parameters to multiple soil layers replacing a heterogeneous domain by a less heterogenous domain vereecken et al 2007 for which both parameterizations produce similar hydrological responses drainage evapotranspiration root zone soil water content etc under certain scaled boundary conditions using inverse modelling to derive soil hydraulic properties has become increasingly popular in the last few decades e g graham et al 2018 wöhling and vrugt 2011 making it possible to obtain more representative estimates of soil hydraulic parameters compared with laboratory methods and estimates from pedotransfer methods e g al ashwal et al 2021 hydraulic parameters derived from laboratory methods are not always representative of field conditions and direct methods for determining soil hydraulic parameters require steady state conditions as well as restrictive initial and boundary conditions alternatively inverse methods combine forward soil water flow models with appropriate optimization algorithms to find the optimal parameter set that minimizes an objective function advanced inverse methods combine a physically based numerical model with an algorithm for automatic parameter estimation e g pollacco 2005 pollacco et al 2013a pollacco and mohanty 2012 but this becomes more challenging for heterogeneous soils deriving hydraulic parameters of highly heterogeneous soils by inverting solely time series of soil water content θ l3 l 3 by using the numerical solution of the richardson richard s equation rre richardson 1922 richards 1931 as a forward model was attempted by ritter et al 2003 to optimize three layers but they found that to be a challenging ill posedness problem pollacco et al 2008b ill posedness problems can be overcome by measuring θ and soil water pressure ψ l simultaneously from multiple depths schelle et al 2012 scharnagl et al 2011 derived the hydraulic parameters by inverting θ and using the rosetta pedotransfer function schaap et al 2001 to provide additional information about the correlation structure of the predicted parameters that was found to be essential for the effectiveness and robustness of the methodology ines and droogers 2002 successfully inverted the hydraulic parameters by inverting soil water content from two layers and evapotranspiration derived from remote sensing rezaei et al 2016 optimized the hydraulic parameters using θ monitoring together with a crop growth model and a soil hydrological model on a 2 layer soil qanza et al 2019 also successfully used inverse estimation of soil hydraulic parameters for four layers but considering null residual θ θ r l3 l 3 over et al 2015 introduced a hierarchical simulation and modelling framework that allows for inference and validation of the likelihood function in bayesian inversion of vadose zone hydraulic properties they inverted multilayer soil hydrological properties using θ observations collected in the uppermost four layers the main drawback of this work however is the increased computational expense of the inversion to the best of our knowledge no optimization has been performed considering up to five soil layers the only work optimizing 25 hydraulic parameters simultaneously is that of wöhling and vrugt 2011 but this combines observed θ and ψ from multiple depths a variety of upscaling procedures are available in the literature that allow migration of point scale variables into coarser models e g abbaspour et al 1997 ward et al 2006 zhang et al 2004 a detailed review of the most commonly used upscaling methods can be found in vereecken et al 2007 as illustrated in previous methods it is important to apply an approach that is suitable for the specific needs the widespread adoption of soil water content sensors in agricultural regions in new zealand the availability of site specific time series data on soil wetting and drying patterns and evapotranspiration processes provide an opportunity to develop a method for upscaling high resolution multilayer smap hydro parameters to assist with better parameterisation of coarser models to address the challenge of simultaneously optimizing many hydraulic parameters in highly heterogeneous soils or in the presence of macroporosity we develop a novel multistep optimization algorithm which first models the overall flow in the profile assuming a homogeneous soil and then gradually introduces heterogeneities to the level required by the hydrological model this approach is needed because optimizing the hydraulic parameters of each layer separately for example by matching time series θ individually produces poor results kamali and zand parsa 2016 since it does not represent the overall soil water dynamics across the unsaturated zone pollacco 2005 optimization is generally performed so that the simulated output from a hydrological model correspond to the observed time series θ at different depths e g from time or frequency domain reflectometer hence the soil hydraulic parameters are derived by minimizing an objective function formed from the observed and simulated data pollacco 2005 pollacco et al 2008b 2013a pollacco and mohanty 2012 experiments have shown that the objective function has a unique solution but suffers from excessive sensitivity to some parameters pollacco et al 2008a 2008b pollacco and angulo jaramillo 2009 the methodology proposed by fernández gálvez et al 2021 and further validated in vogeler et al 2021 reduces the sensitivity of the bimodal soil kosugi hydraulic parameters by preventing non physical combinations of hydraulic parameters pollacco et al 2008b the kosugi 1994 1996 hydraulic functions are used because their parameters are physically related to the pore size distribution of the soil the multistep optimization method is implemented into the new performant hydrological pixel hypix model hypix pollacco et al 2022 is a physically based hydrological model and solves the mixed form of the richardson richard s equation ree by using the newton raphson method the non linear rre is solved with an efficient heuristic and physical time stepping strategy using a reduced number of control parameters hypix also incorporates a novel algorithm to avoid overshooting by controlling the newton raphson step hypix can a process a large number of soil layers b simulate kosugi unimodal and bimodal hydraulic parameters for each soil layer c simulate realistic water ponding at the soil surface by using a novel approach for the computation of sorptivity lassabatere et al 2021 2022 d compute rainfall interception from leaf area index e derive transpiration from root water uptake with a compensation mechanism for deeper layers where root density is limited f compute evaporation and g compute drainage through the bottom of the soil profile under different boundary conditions this paper is organized as follows section 2 presents the theory describing the bimodal kosugi hydraulic functions an overview of the hypix model and the novel vertical multistep optimization scheme used to upscale the hydraulic parameters section 3 describes the experimental data and methodology used to illustrate and validate the proposed scaling method section 4 shows the results of the derived soil hydraulic and vegetation parameters from inverse modelling section 5 illustrates some of the direct applications in future research and section 6 summarizes the main conclusions 2 theory 2 1 soil hydraulic functions 2 1 1 bimodal kosugi soil hydraulic functions hypix uses the bimodal kosugi 1994 1996 soil hydraulic functions the choice of the kosugi soil hydraulic functions is based on the physical interpretation of the parameters in relation to the soil pore size distribution and the fact that these parameters can be constrained by exploiting the relationship between them fernández gálvez et al 2021 pollacco et al 2013b moreover the selection of bimodal functions is based on the prevalence of soils with a bimodal pore system e g jarvis 2007 mcleod et al 2008 where macropores and micropores lead to a two stage drainage fast flow macropore flow can occur when the water pressure head exceeds the threshold needed to activate the macropore network adding to the matrix flow below this threshold only the matrix participates in the flow fernández gálvez et al 2021 the representation of the soil water retention curve θ ψ l3 l 3 and the unsaturated hydraulic conductivity k ψ l t 1 functions is based on the dual porosity model of pollacco et al 2017 1 θ ψ θ mat ψ θ mac ψ θ mat ψ 1 2 θ s macmat θ r erfc ln ψ ψ m 2 σ θ r θ mac ψ 1 2 θ s θ s macmat erfc ln ψ ψ m mac 2 σ mac 2 k ψ k mat s e ψ k mac s e ψ s e ψ θ θ r θ s θ r 1 2 θ s macmat θ r θ s θ r erfc ln ψ ψ m 2 σ θ s θ s macmat θ s θ r erfc ln ψ ψ m mac 2 σ mac k mat s e ψ k s θ s macmat θ r θ s θ r s e ψ 1 2 erfc ln ψ ψ m 2 σ σ 2 2 k mac s e ψ k s θ s θ s macmat θ s θ r s e ψ 1 2 erfc ln ψ ψ m mac 2 σ mac σ mac 2 2 where erfc is the complementary error function θ l3 l 3 represents the volumetric soil water content and ψ l the soil water pressure considering ψ 0 for unsaturated soils i e matrix suction θ s l3 l 3 and θ r l3 l 3 are the saturated and residual volumetric soil water content respectively ln ψ m and σ denote the mean and standard deviation of ln ψ respectively in the soil matrix domain ln ψ mmac and σ mac denote the mean and standard deviation of ln ψ respectively in the macropore soil domain with the argument of ln in units of length i e ψ m ψ and ψ mmac in l θ smacmat l3 l 3 is the volumetric saturated water content that theoretically differentiates inter aggregate pores structural macropores and matrix domains intra aggregate micropores defining the corresponding soil water pressure threshold between macropore and matrix ψ macmat l s e ψ denotes the effective saturation as a function of ψ with values between 0 and 1 k s l t 1 is the saturated hydraulic conductivity and k s e ψ l t 1 refers to the unsaturated hydraulic conductivity written as a function of s e ψ for the case where θ smacmat θ s eq 1 and eq 2 reduce to the unimodal kosugi soil hydraulic functions 2 1 2 constraining bimodal kosugi soil hydraulic parameters estimating the bimodal kosugi soil hydraulic parameters from observed θ requires the simultaneous estimation of eight parameters θ s θ r σ ψ m k s θ smacmat σ mac and ψ mmac using limited measurement data here we use the term optimize in the estimation of the parameters since the task is to produce a set of values for the parameters that optimizes typically minimizes an objective function therefore optimization is used in the sense of parameter estimation in certain cases full inversion of the data to produce parametric estimates of the bimodal kosugi soil hydraulic model is not possible for example where measurements have a restricted range pollacco et al 2008b leading to highly sensitive parameters pollacco et al 2008b 2013a pollacco and mohanty 2012 reducing the sensitivity of the parameters requires either adding novel independent measurements or incorporating constraints to reduce the effective complexity of the model since gathering new measurements is in this case not feasible here we use the set of constraints proposed by fernández gálvez et al 2021 and further validated in vogeler et al 2021 which reduces the number of parameters to be optimized without compromising the fit of the hydraulic functions while the estimated hydraulic parameters still have physical meaning this set of constraints can be summarized as follows θ r is derived from σ ψ mmac and σ mac which are considered constant for a fixed value of ψ macmat equal to 100 mm and ψ m and σ are dynamically constrained based on the assumption that θ ψ and k θ are lognormally distributed therefore the number of hydraulic parameters to be optimized is reduced from eight to five using the principles of soil physics although θ s can be derived from total porosity when soil bulk density and particle density data are available in this case it is an optimized parameter with a feasible range determined from the maximum observed θ in the corresponding layer the soil hydraulic parameters are optimized using the bimodal θ ψ and k θ model described in section 2 1 1 this is performed by matching simulated θ time series derived from the hypix model with observed θ time series at the corresponding depth the dynamic physically feasible range of the five optimized bimodal kosugi soil hydraulic parameters using the full set of constraints derived from fernández gálvez et al 2021 is indicated in table 1 note that ψ macmat is a constant with a value of 100 mm and ln ψ macmat is higher than ln ψ mmac by three times σ mac p σ 3 eq 3 in fernández gálvez et al 2021 here θ r is derived from σ in eq 3 and table 1 and is given by 3 σ σ σ min σ max σ min θ r σ θ r max 1 e α 1 σ α 2 1 e α 1 where θ rmax is set at 0 2 the maximum value for θ r that was found to be satisfactory pollacco et al 2013b α1 15 and α2 4 are two optimized empirical parameters σ is the normalized σ and σ min and σ max are set at 0 75 and 4 from table 1 therefore with all these simplifications and additional constraints we define a model that requires only five parameters θ s σ ψ m k s and θ smacmat 2 2 summary of hypix model modelling unsaturated flow in highly heterogeneous soils can accurately be performed by solving the rre which is commonly adopted by hydrological and soil vegetation atmosphere transfer models the hypix model implements improvements to solve the rre by including a dynamic physical smoothing criterion for controlling the newton raphson step and a novel time stepping management scheme based on ψ without introducing further parameters pollacco et al 2022 the solution of the rre is based on maina and ackerer 2017 for which the rre partial differential equation is solved using a cell centred finite volume implicit finite differences scheme for the spatial discretization with an implicit euler scheme for the temporal discretization by using the weighted average inter cell hydraulic conductivity assuming a rigid solid matrix the mixed form of the rre is written as 4 θ i ψ i t θ i ψ i t 1 δ t t s o θ i ψ i t θ s i ψ i t 1 ψ i t δ t t q i 1 2 t q i 1 2 t δ z i s i n k i ψ i t 1 where δt t t is the time step at time t δz i l is the mesh size of cell i with the vertical coordinate positive downwards θ i l3 l 3 is the volumetric soil water content of cell i θ s i l3 l 3 is the saturated volumetric soil water content of cell i s 0 l 1 is a parameter that accounts for fluid compressibility which is assumed to be constant with depth ψ i l is the soil water pressure of cell i considering ψ 0 for unsaturated soils q l t 1 is the soil water flux based on the extended darcy buckingham s law which is positive downward and negative when water moves upwards q i 1 2 t l t 1 is the flux entering cell i and q i 1 2 t l t 1 is the flux exiting cell i and sink i l3 l 3 t 1 taken as positive is the sink term defined as the volume of water per unit time removed from cell i by soil evaporation and root water uptake additional details of the hypix model can be found in pollacco et al 2022 2 3 novel vertical multistep optimization the aim of this study is to derive a strategy to optimize the five hydraulic parameters θ s σ ψ m k s θ smacmat of the parsimonious bimodal θ ψ and k θ functions described in section 2 1 1 for each layer of the soil profile by using the hypix model each layer of the profile corresponds to the depths where θ is experimentally measured for example in a site where θ values are measured at five depths 25 5 5 hydraulic parameters need to be optimized it is challenging to optimize 25 parameters simultaneously however pollacco 2005 found that optimizing each layer separately produces poor results particularly for a highly heterogeneous soil profile in the presence of lenses of clay pebbles or macropore flow this is because an optimization method that isolates each layer without considering the overall water flow in the soil profile causes unrepresentative parameters and thus poor representation of the soil water fluxes kamali and zand parsa 2016 therefore we present an inverse modelling algorithm for layered soils when optimizing the hydraulic parameters the soil profile is initially considered homogeneous then a stepwise grouping of local layers zones defined by the end user allows heterogeneous patterns to be addressed the optimization of the different layers in a specified order and pattern is presented in table 2 table 2a for odd and table 2b for even number of layers where 0 or 1 indicates which soil layers are optimized simultaneously at each specific step for example layers containing the number 1 in table 2 show the grouping of different layers zones in which the soil hydraulic parameters have the same optimal value i e homogeneous layer in the first step opt 1 it is assumed that the soil is homogeneous and therefore the whole profile is modelled with five optimized soil hydraulic parameters and the same values were given for each parameter in the different layers of the soil profile the derived effective optimal hydraulic parameters will be used for hydrological models requiring only one homogeneous layer in the second step opt 2 optimal hydraulic parameters for models requiring two layers only the parameters of the upper half of the profile are optimized maintaining the bottom half below the root zone with the value of optimized parameters derived from the previous step and then the third step opt 3 optimal hydraulic parameters for models requiring three layers etc operates on the deeper zone keeping the upper half of the profile with the values of the parameters with values of the previously optimized parameters optimizing from top to bottom was observed to produce better results than from bottom to top because water percolates downwards so a change of the hydraulic parameters of the top layer will affect the lower layer note that the top layer is also the layer where the water content has larger variations with time each zone of the profile is successively split into two zones from top to bottom and the optimization is repeated by copying the values of the optimal parameters from the previous optimization step the vertical multistep optimization assumes that the number of soil layers il corresponds to the number of θ observations the centre of each soil layer is also given by the corresponding θ observation 2 3 1 evaluation of improvements in accuracy by increasing the number of layers the procedure for identifying the relative errors between optimization performed with the greatest number of layers and simulated with a reduced number of layers is presented in table 3 it is assumed that outputs from the hypix model computed with the greatest number of layers il is the reference free of error ref and simulations computed with a reduced number of layers are simulated sim the reduction of error in different steps of computation a drainage eq 5 b evapotranspiration eq 6 and c root zone soil water content top 600 mm eq 7 is possible by increasing the number of layers il as presented in table 2 2 3 2 weighted objective functions of the multistep optimization the global optimizer searches for the optimal hydraulic parameters by minimizing a weighted objective function wof we selected the robust global optimizer blackboxoptim v0 6 1 https github com robertfeldt blackboxoptim jl written in julia bezanson et al 2017 and selected the adaptive de rand 1 bin radiuslimited method for every optimization step the maximum evaluation functions that are allowed to be optimized are controlled by maxfuncevals which was set to 100 we designed the novel wof to address the issue that observed θ θ obs l3 l 3 in deeper layers below the root zone contains greater uncertainties compared to measurements in the root zone it is also important that simulated θ θ sim l3 l 3 gives more accurate predictions in the root zone than below the root zone this is because most of the dynamism of soil evaporation and root water uptake takes place in the root zone prioritizing the root zone by optimizing from top to bottom is performed in the algorithm used by the vertical multistep optimization section 2 3 this is performed by introducing a weighting w in the wof by assuming that the measurements placed at different depths are equally spaced the wof is computed as follows 8 w i l 2 n i l 1 i l n i l n i l 1 i l 1 n i l w i l 1 w o f θ t n t i l n i l w i l θ o b s i l t θ s i m i l t 2 n i l n t where n il is the total number of layers il where θ is measured n t is the total number of time steps and θ obs and θ sim are observed and simulated θ respectively 3 material and methods 3 1 data 3 1 1 monitoring periods for calibration and validation the monitoring periods of the θ at each depth for the five experimental sites were split in two using one period for model calibration and another for model validation the dates of calibration and validation for each site were selected to consider the different installation periods sensor stabilization after installation erroneous data data gaps and the occasional presence of the groundwater table in deeper layers at some sites the calibration and validation periods were chosen to include a wide variety of dry and wet periods and simulations were always preceded by a 2 month warm up period there were not enough data for validation for one of the sites waihou due to malfunctioning of the sensors 3 1 2 climate daily values of precipitation δpr l were measured adjacent to the soil water content probes using a tipping bucket electronic rain gauge recording at 0 2 mm resolution potential evapotranspiration δpet l was derived from the new zealand virtual climate stations network vcs tait and woods 2007 with estimates based on the spatial interpolation of actual data observations made at climate stations located around the country tait 2010 for a given variable x we used the symbol δx to indicate the change in the variable x in a given time step as follows t 1 t x t t 1 t 1 x t e g δpr is the cumulative precipitation between two time steps the yearly average precipitation and potential evapotranspiration using representative wet and dry periods are shown in the results section table 5 by using hypix model 3 1 3 soils the sites used in this study are located in the waikato region of new zealand fig 1 all soils have formed from airfall volcanic tephra but vary in their soil physical properties and heterogeneity particularly their texture and profile drainage characteristics a brief description of the soils as follows well drained soils the taupō soil is a sandy textured soil formed from volcanic airfall pumice material new zealand classification podzolic orthic pumice soil usda soil taxonomy classification orthod hewitt 2010 the otorohanga and waihou soils are also formed from airfall volcanic material but with finer tephra material compared with taupō resulting in silty loam topsoil textures grading to silty clay in the subsoil they are classified in new zealand as typic orthic allophanic soils and in soil taxonomy as a haplohumult hewitt 2010 all soils are characterized by well drained morphology having yellow brown coloured subsoils with no redox mottles indicating that the frequency and duration of internal waterlogging is minimal and oxidation processes predominate based on the soil morphology the otorohanga and waihou soils would be considered to have the least heterogeneous soil profiles of those used in this study since they have a reduced number of distinct layers the taupō soil although free draining is expected to show more heterogeneity in water movement due to the stone content cichota et al 2016 imperfectly and poorly drained soils the hamilton soil has a silt loam topsoil overlying clayey textured subsoils having formed into strongly weathered volcanic tephra it is classified in new zealand as a typic orthic granular soil and in soil taxonomy as a haplohumult hewitt 2010 the drainage class indicates a degree of subsoil drainage impediment that may result in short term waterlogging during wet periods although oxidation processes predominate most of the time the waitoa soil is a silty textured soil it is classified in new zealand as a typic orthic gley soil and in soil taxonomy as a haplohumult hewitt 2010 the drainage class indicates a slow draining subsoil with subsoil saturation occurring most years in winter and early spring the soil morphology of the hamilton and waitoa shows the greatest heterogeneity of the soils in this study as reflected in previous studies of soils with impeded drainage features mcleod et al 2008 vogeler et al 2019 3 1 3 1 time series of soil water content θ θ measurements were recorded at 15 min intervals with decagon 5tm frequency domain reflectometers located at up to five depths for each of the experimental sites the monitoring depths for the different sites differ corresponding to the soil morphological features at each individual site both the rain gauges and the θ sensors were calibrated following the new zealand standard duncan et al 2013 in particular the θ were calibrated against neutron probe measurements taken at particular times throughout the monitoring period the neutron probe data were calibrated with θ measurements from sample weighting in situ and after oven drying for each depth and site 3 1 4 vegetation experimental sites are five mixed non irrigated pasture grass sites the trapezoidal water stress response function feddes et al 1978 was used with parameters for mixed pasture grass in new zealand derived from van housen 2015 for all sites we used default values at four soil water pressures ψ feddes1 100 mm ψ feddes2 250 mm ψ feddes3 5000 mm and ψ feddes4 80 000 mm these parameters are not as sensitive as the crop coefficient k c which we optimized pollacco et al 2008a the k c and leaf area index lai vary throughout the growing season due to the lack of k c and lai data it was assumed that there is a positive linear relationship between lai and pasture growth as already suggested by many authors e g kaur and singh 2013 punalekar et al 2018 and a positive linear relationship between k c and pasture growth allen and pereira 2009 showed that k c is a function of the fraction of ground cover and crop height which is also related to pasture growth pasture growth kg drymatter ha day data were provided by dairynz for the waikato region https www dairynz co nz media 5793237 average pasture growth data waikato 2020 v1 pdf with the following normalized monthly values 0 1 from january to december 0 48 0 24 0 15 0 23 0 34 0 14 0 00 0 23 0 65 1 00 0 79 and 0 72 the range of k c 0 8 to 0 95 was taken from rotated grazing pasture according to fao irrigation paper 56 allen et al 1998 and the range of the lai 0 19 to 5 10 was taken from van housen 2015 normalized lai and k c are calculated as 9 normalised v a l u e v a l u e v a l u e min v a l u e max v a l u e min where the corresponding value of the vegetation parameters lai min lai max k cmin and k cmax as well as the maximum saturated storage capacity of a wet canopy sint sat l are optimized after multistep opt 1 and reoptimized after multistep opt 9 or opt 7 depending on whether the number of soil layers is five or four then the multistep optimization is rerun with the updated vegetation parameters for all sites maximum root depth z nroot was averaged to 800 mm vogeler and cichota 2019 and the percentage of roots in the top 300 mm rdf top was 90 evans 1978 3 2 hypix model parameters the vertical discretization of the soil profile has 37 cells for soils with five layers and 25 cells for soils with four layers depth between 0 and 500 mm is discretized with cells of mesh size δz 20 mm and depth between 500 and 1200 mm is discretized with cells of mesh size δz 50 mm the vertical discretization is the same as the one used in pollacco et al 2022 simulations use the standard values of the hypix model parameters as shown in table 4 3 3 goodness of fit the goodness of fit between the model outputs θ sim and the corresponding observations θ obs was assessed using the best concordance correlation coefficient ccc and the refined index of agreement dr proposed by willmott et al 2012 the ccc is defined as follows 10 c c c 2 ρ σ θ o b s σ θ s i m σ θ o b s 2 σ θ s i m 2 μ θ o b s μ θ s i m 2 where μ θ o b s and μ θ s i m are the observed and simulated means of θ at each depth where θ is measured σ θ o b s 2 and σ θ s i m 2 are the observed and simulated variances of θ at each depth where θ is measured and ρ is the pearson correlation coefficient between observed and simulated values ccc is equal to 1 for a perfect fit the dr index with values between 1 0 and 1 0 indicates the sum of the magnitudes of the differences between the model predicted and observed deviations about the observed mean relative to the sum of the magnitudes of the perfect model θ obs θ sim for all i and observed deviations about the observed mean this index is used because in general it is more rationally related to model accuracy than other existing indices willmott et al 2012 the dr index can be written as follows with c 2 11 d r 1 i 1 n t θ s i m i θ o b s i c i 1 n t θ o b s i θ o b s when i 1 n t θ s i m i θ o b s i c i 1 n t θ o b s i θ o b s c i 1 n t θ o b s i θ o b s i 1 n t θ s i m i θ o b s i 1 when i 1 n t θ s i m i θ o b s i c i 1 n t θ o b s i θ o b s a value for dr of 0 5 for example indicates that the sum of the error magnitudes is one half of the sum of the perfect model deviation and observed deviation magnitudes when dr 0 0 it signifies that the sum of the magnitudes of the errors and the sum of the perfect model deviation and observed deviation magnitudes are equivalent when dr 0 5 it indicates that the sum of the error magnitudes is twice the sum of the perfect model deviation and observed deviation magnitudes values of dr near 1 0 can mean that the model estimated deviations about θ o b s are poor estimates of the observed deviations but they can also mean that the observed variability is insignificant when approaching the lower limit of dr interpretation should be made cautiously 4 results 4 1 hydraulic and vegetation parameters derived from inverting observed θ 4 1 1 water balance components the rainfall interception algorithm in hypix pollacco et al 2022 uses vegetation parameters described in section 3 1 4 the interception loss is computed for the five pasture grass sites in table 5 ranging from 8 to 20 with annual throughfall precipitation values corresponding to 2298 and 823 mm respectively as expected the interception loss is greater for sites with smaller annual rainfall waitoa and waihou compared to sites with a larger annual rainfall otorohanga and taupō because the saturated storage capacity sint sat of the interception model has greater impact on smaller rainfall events than on large ones simulated yearly drainage values are lower at sites with lower precipitation while simulated yearly evaporation values show a distinctive behaviour at each site the closure of the simulated water balance is computed by the normalized soil water balance to the infiltration error wb in table 5 with very small discrepancies between 4 4 10 6 and 0 2 10 6 the results of interception loss between 8 and 20 are similar to values derived by thurow et al 1987 for curly mesquite short grass and sideoats mid grass dominated sites with 10 8 and 18 1 interception loss respectively note that interception varies monthly with lai and k c section 3 1 4 the goodness of fit between θ obs and θ sim at all depths for each site is represented by the wof θ values in table 5 with satisfactory results between 0 009 and 0 014 table 6 shows the optimal values of vegetation parameters described in hypix pollacco et al 2022 for the five experimental sites due to the limited vegetation information available it was not possible to fully validate the rainfall interception model and its vegetation parameters as an excellent fit between observed and simulated θ does not guarantee that the vegetation parameters be physical plausible pollacco et al 2008a showed that when the vegetation parameters are optimized against observed θ assuming the soil hydraulic parameters are known which is not the case here the outputs of the drainage transpiration and interception are particularly sensitive to small uncertainties of the θ data and therefore the optimized vegetation parameters suffer from correlated parameters the reason for having correlated parameters pollacco et al 2008a 2008b pollacco and angulo jaramillo 2009 is that the rainfall interception and the evapotranspiration parameters are linearly linked which means that interception compensates for over under predicting evapotranspiration and vice versa if the water balance between interception and evapotranspiration is inadequate then the drainage will compensate without affecting the global water balance 4 1 2 model calibration and validation a monitoring period long enough to include a variety of wet and dry periods at each site was used for calibrating the soil hydraulic properties for each layer derived from inverse modelling by using the multistep optimization algorithm implemented in hypix section 2 3 the optimal bimodal kosugi soil hydraulic parameters from inverse modelling by using the multistep optimization algorithm with observed θ at the five experimental sites are shown in table 7 the selection of the calibration period is crucial to provide an accurate estimate of the hydraulic parameters and due to malfunctioning of the waihou site there were not enough data left for validating this site after successfully automatically calibrating the hydraulic and vegetation parameters at each site hypix was run for a validation period using the optimal hydraulic and vegetation parameters obtained from calibration simulations corresponding to the calibration and validation periods are shown in fig 2 where time series of precipitation δpr throughfall precipitation δprthrough ponding δhpond drainage δq potential evapotranspiration δpet sink term δsink and evaporation δevap are presented above the θ values at depths corresponding to measurements for the five experimental sites note that the intercepted precipitation varies monthly with lai and k c section 3 1 4 simulated values of θ at different measuring depths were found to be close to the observations and follow the general trend according to the forcing input data with larger changes in θ near the surface and less responsive values deeper in the soil profile rainfall events are followed by a sudden increase in θ that gets reflected progressively down into the soil profile after these episodes θ decreases progressively with an approximately simultaneous increase in both simulated drainage and evapotranspiration a sudden increase in evapotranspiration values occurs because after each rainfall event the amount of water available next to the soil surface is greater drainage does not take place immediately after a rainfall event there is a delay in the drainage pulse with differences related to the amount of water reaching the surface during each rainfall event and the hydraulic characteristics of each experimental site the goodness of fit between measured and simulated θ for the calibration and validation periods at each of the experimental sites using the dynamically constrained set of hydraulic parameters is shown in table 8 a better fit is obtained for the first four sites waitoa waihou taupō and otorohanga with higher ccc 0 743 and dr 0 666 values compared to the ones for the hamilton site with ccc 0 596 and dr 0 533 the goodness of fit for validation is only marginally lower than for calibration providing confidence in the retrieved hydraulic and vegetation parameters implemented in hypix for each site only for the hamilton site the dr index is reduced by half for the validation period due to both the higher soil heterogeneity and the shorter monitoring period with available data in fact the hamilton site with its slow draining subsoil and a high soil heterogeneity is the most challenging site studied which also be due to errors in the climate data one of the challenges addressed by the automatic multistep optimization algorithm section 2 3 and the weighted objective function eq 8 is to predict the root zone θ more accurately and ensure the predictions below the root zone remain acceptable this is because the water balance is mainly determined in the root zone where the sink term is active this is challenging since greater uncertainties are found in measuring θ in the root zone due to presence of roots and macropores which can be seen in the otorohanga site fig 2 where θ sim modelled at 100 mm is more accurate during the validation period compared with the calibration period this may be because the time required for the top sensor to settle down is greater than for the other sensors contact between the sensor and the surrounding soil 4 1 3 multistep optimization for vertical scaling the improvement of the fit between observed and simulated θ with increasing splitting of the soil profile layers as described in table 2a five depths of measuring θ and table 2b four depths of measuring θ depends on the a heterogeneity of the soil profile b aptitude of the model to represent the hydrological processes which is quantified by the success of the rre to reproduce θ obs c accuracy of θ obs the maximum number of splits of the soil profile required by a hydrological model to obtain the desired accuracy in a particular output is difficult to establish since it does not depend solely on the severity of heterogeneity in the soil but also on the required accuracy of the outputs of the model table 9 shows the minimum number of layers required to achieve a particular accuracy in drainage evapotranspiration and soil water content of the root zone for each of the experimental sites by assuming that outputs from the hypix model computed with the greatest number of layers il is the reference free of error as indicated in table 3 the selected accuracy indicated in table 9 for each model output is chosen as an example and the measure of the accuracy is evaluated by the relative errors described in table 3 fig 3 a shows the decrease of goodness of fit between observed and simulated θ represented by the wof θ eq 8 with the increasing amount of splitting of the soil profile using the multistep optimization scheme described in section 2 3 for each site as can be observed the decrease is less pronounced for less heterogenous soils with the higher impact for the most heterogeneous site fig 3b d illustrate the results corresponding to the algorithms which compute the percentage errors table 3 as a function of the amount of splitting for a drainage ζ q b evapotranspiration ζ e t and c soil water content of the root zone top 600 mm ζ s w c in the evaluation of the errors the discretization with the greatest amount of splitting of the profile greatest number of layers was considered as the reference the minimum splitting needed to achieve a particular goal is described in table 9 as previously indicated table 9 shows that the minimum number of required layers depends on the output of interest clearly accurate computation of evapotranspiration requires the greatest number of layers compared to drainage and soil water content in the root zone this is explained by the fact that evapotranspiration is influenced by the movements of water from deeper layers to the surface and therefore requires accurate layering along the profile the dynamics of evapotranspiration and soil water content occur only in the root zone and therefore the impact of having accurate hydraulic information at greater depths is less important the fact that small errors in θ cause large errors in drainage and evapotranspiration was already reported by pollacco et al 2008a and pollacco and mohanty 2012 which is why in table 9 the threshold of ζ s w c is 2 5 and not 5 whilst these results are indicative of the effects of reducing the number of layers providing guidance on the minimum number of layers to be used for modelling at the catchment or region level would require a larger number of experimental sites 4 1 3 1 time of execution of multistep optimization the yearly average execution time for each site of performing the multistep optimization by running hypix model is described in table 10 hypix is run with julia 1 7 3 on a laptop equipped with 64 gb ram and intel xeon e 2186m cpu 2 9 ghz processor to facilitate inter comparison the simulations are scaled to a period of 1 year table 10 shows that there are differences in execution time which are largely due to the heterogeneity of the soil layers pollacco et al 2022 which influences the efforts of the rre to converge 4 1 4 modelled heterogeneity compared to pedological description table 11 classifies the result reported in table 9 and shows the minimum number of layers to accurately model each site the classification derived from inverse modelling is compared to the soil characteristics derived from the soil database s map despite the limited number of experimental sites included in this study table 11 shows there is a clear relationship between the number of layers required for adequate representation of the soil hydraulic parameters and the heterogeneous soil description derived from s map both the soil heterogeneity and the drainage class from s map can provide an indication of the vertical discretization required to accurately compute the soil water balance components for example well drained soils can generally be adequately described by a relatively small number of layers by contrast poor or imperfectly drained soils require more complex descriptions with more layers 5 future research next steps in implementing this approach to scaling smap hydro information include a sensitivity analysis of deriving hydraulic parameters by using the vertical multistep optimization assuming fixed vegetation parameters assessing the value of remotely sensed sources of surface soil water content and transpiration and testing the scaled hydrological parameterizations in the various hydrological models used in new zealand another application of the multistep optimization is that observed θ derived at multiple depths e g from time or frequency domain reflectometers is becoming readily available in precision agriculture in new zealand to control irrigation rates drewry et al 2019 ekanayake and hedley 2018 el naggar et al 2020 therefore the proposed methodology of deriving unique sets of hydraulic parameters from observed θ time series would enable physically based models to optimize the application of irrigation and minimize leaching through excess drainage 6 conclusions we derived a novel vertical multistep algorithm that enables us to invert observed θ at multiple depths by fitting only five bimodal kosugi hydraulic parameters to every layer and optimizing the hydraulic parameters in a particular pattern from top to bottom by successively splitting the profile the multistep optimization algorithm puts more weight into optimizing the top layers because most of the dynamism of the soil evaporation and root water uptake takes place in the root zone the optimization algorithm upscales the soil hydraulic parameters gradually introducing heterogeneity because optimizing the hydraulic parameters of each layer individually produces poor results since it does not represent the overall soil water dynamics across the unsaturated zone it has been shown that the minimum number of layers to accurately estimate drainage evapotranspiration and soil water content of the root zone computed from the hypix model depends on the heterogeneity of the soil and the required accuracy for each output of the model it was found that a more detailed layered model is required when computing evapotranspiration compared to drainage the multistep optimization is useful to upscale a detailed layered profile of soil hydraulic parameters into a model with fewer layers and provide an estimate of the uncertainty due to the reduced number of layers moreover the soil description from s map provides an indication of the vertical discretization required to accurately compute the soil water balance components it was found that the richards richardson equation converges faster in hypix when the hydraulic parameters describing θ ψ and k θ are plausible this is performed during the optimization process by dynamically constraining the relationship between ψ m and σ the hypix model efficiently solves the mixed form of the richards equation using a cell centred finite volume scheme for the spatial discretization with an implicit euler scheme for the temporal discretization by using the weighted average inter cell hydraulic conductivity hypix includes rainfall interception soil evaporation root water uptake with compensation algorithm and ponding using the novel computation of sorptivity software availability the hypix model can be downloaded from https github com manaakiwhenua soilwater toolbox tree master src hypix and is open source under the gp 3 0 license this software is part of a set of interlinked modules implemented into the soilwater toolbox ecosystem led by j a p pollacco from manaaki whenua landcare research in new zealand and j fernández gálvez from the university of granada in spain the preliminary objectives of the soilwater toolbox are to derive the soil hydraulic parameters by using a wide range of cost effective methods the estimated hydraulic parameters can be directly implemented into hypix to compute the soil water budget the soilwater toolbox enables performing inter comparison and sensitivity analyses of the hydraulic parameters computed from different methods on the soil water fluxes the following modules are currently included in the soilwater toolbox intergranular mixing particle size distribution module derives unimodal hydraulic parameters by using particle size distribution pollacco et al 2020 general beerkan estimation of soil transfer parameters module derives the unimodal hydraulic parameters from single ring infiltration experiments fernández gálvez et al 2019 sorptivity module a novel computation of sorptivity used in the general beerkan estimation of soil transfer parameters method lassabatere et al 2021 2022 saturated hydraulic conductivity module derived from unimodal and bimodal θ ψ pollacco et al 2013b 2017 inverse module which inverts hydraulic parameters from θ time series as in this paper reduce uniqueness module of a physical bimodal soil kosugi hydraulic parameters from inverse modelling fernández gálvez et al 2021 using water retention and or unsaturated hydraulic conductivity data directly measured in the laboratory or indirectly obtained from inverting θ time series as described in this paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the new zealand ministry of business innovation and employment throught the winning against wildings c09x1611 and next generation s map c09x1612 research programmes mathieu sellier from mechanical engineering of the university of canterbury new zealand is thanked for reviewing the manuscript funding for open access was provided by university of granada cbua spain 
25539,efficient simulation of water flow processes in the vadose zone is crucial to increase agricultural productivity within environmental limits this requires deriving detailed soil hydraulic parameters of the soil profile that is highly challenging particularly for heterogeneous soils we therefore developed an alternative indirect methodology to calibrate the hydraulic parameters from soil water content time series measured at multiple depths by using the new physically based hydrological model hypix we propose a novel efficient multistep optimization algorithm for layered soils that derives an optimal set of hydraulic parameters for a desired number of soil layers for each selected soil layer hypix derives five physical bimodal kosugi hydraulic parameters that describe the soil water retention and hydraulic conductivity by using a novel algorithm that reduces the degree of sensitivity and freedom of the parameters the optimization algorithm upscales the soil hydraulic parameters by gradually incorporating the soil heterogeneity this method overcomes the problems associated with optimization of the hydraulic parameters of each layer individually which leads to poor results because it does not represent the cohesive soil water dynamics across the unsaturated zone we tested the method using soil water content measurements at different depths at five heterogeneous experimental sites in new zealand we show how the accuracy of the simulated water balance components increases with the number of soil layers the multistep optimization upscales a detailed layered profile of soil hydraulic parameters into a model with fewer layers the methodology developed provides an estimate of the uncertainty of using a reduced number of soil layers we also show that a pedological description can provide an indication of the minimum soil layers of vertical discretization required to accurately compute the soil water balance components graphical abstract image 1 keywords variably saturated flow vertical discretization multistep optimization inverse modelling soil hydraulic properties non uniqueness julia language 1 introduction the digital soil database s map https smap landcareresearch co nz lilburne et al 2004 mcneill et al 2018 with a coverage of 37 of new zealand in 2021 provides the kosugi 1994 1996 soil water retention θ ψ and the unsaturated hydraulic conductivity k θ pollacco et al 2013b 2017 for up to six functional horizons to a maximum depth of 1 m on a 150 m 150 m spatial grid this soil hydrological property database is termed smap hydro however most hydrological models are not designed to accommodate multiple layers and therefore require soil hydrological characterization at a coarser vertical resolution e g one two or three layers here we determine how to upscale hydraulic parameters to multiple soil layers replacing a heterogeneous domain by a less heterogenous domain vereecken et al 2007 for which both parameterizations produce similar hydrological responses drainage evapotranspiration root zone soil water content etc under certain scaled boundary conditions using inverse modelling to derive soil hydraulic properties has become increasingly popular in the last few decades e g graham et al 2018 wöhling and vrugt 2011 making it possible to obtain more representative estimates of soil hydraulic parameters compared with laboratory methods and estimates from pedotransfer methods e g al ashwal et al 2021 hydraulic parameters derived from laboratory methods are not always representative of field conditions and direct methods for determining soil hydraulic parameters require steady state conditions as well as restrictive initial and boundary conditions alternatively inverse methods combine forward soil water flow models with appropriate optimization algorithms to find the optimal parameter set that minimizes an objective function advanced inverse methods combine a physically based numerical model with an algorithm for automatic parameter estimation e g pollacco 2005 pollacco et al 2013a pollacco and mohanty 2012 but this becomes more challenging for heterogeneous soils deriving hydraulic parameters of highly heterogeneous soils by inverting solely time series of soil water content θ l3 l 3 by using the numerical solution of the richardson richard s equation rre richardson 1922 richards 1931 as a forward model was attempted by ritter et al 2003 to optimize three layers but they found that to be a challenging ill posedness problem pollacco et al 2008b ill posedness problems can be overcome by measuring θ and soil water pressure ψ l simultaneously from multiple depths schelle et al 2012 scharnagl et al 2011 derived the hydraulic parameters by inverting θ and using the rosetta pedotransfer function schaap et al 2001 to provide additional information about the correlation structure of the predicted parameters that was found to be essential for the effectiveness and robustness of the methodology ines and droogers 2002 successfully inverted the hydraulic parameters by inverting soil water content from two layers and evapotranspiration derived from remote sensing rezaei et al 2016 optimized the hydraulic parameters using θ monitoring together with a crop growth model and a soil hydrological model on a 2 layer soil qanza et al 2019 also successfully used inverse estimation of soil hydraulic parameters for four layers but considering null residual θ θ r l3 l 3 over et al 2015 introduced a hierarchical simulation and modelling framework that allows for inference and validation of the likelihood function in bayesian inversion of vadose zone hydraulic properties they inverted multilayer soil hydrological properties using θ observations collected in the uppermost four layers the main drawback of this work however is the increased computational expense of the inversion to the best of our knowledge no optimization has been performed considering up to five soil layers the only work optimizing 25 hydraulic parameters simultaneously is that of wöhling and vrugt 2011 but this combines observed θ and ψ from multiple depths a variety of upscaling procedures are available in the literature that allow migration of point scale variables into coarser models e g abbaspour et al 1997 ward et al 2006 zhang et al 2004 a detailed review of the most commonly used upscaling methods can be found in vereecken et al 2007 as illustrated in previous methods it is important to apply an approach that is suitable for the specific needs the widespread adoption of soil water content sensors in agricultural regions in new zealand the availability of site specific time series data on soil wetting and drying patterns and evapotranspiration processes provide an opportunity to develop a method for upscaling high resolution multilayer smap hydro parameters to assist with better parameterisation of coarser models to address the challenge of simultaneously optimizing many hydraulic parameters in highly heterogeneous soils or in the presence of macroporosity we develop a novel multistep optimization algorithm which first models the overall flow in the profile assuming a homogeneous soil and then gradually introduces heterogeneities to the level required by the hydrological model this approach is needed because optimizing the hydraulic parameters of each layer separately for example by matching time series θ individually produces poor results kamali and zand parsa 2016 since it does not represent the overall soil water dynamics across the unsaturated zone pollacco 2005 optimization is generally performed so that the simulated output from a hydrological model correspond to the observed time series θ at different depths e g from time or frequency domain reflectometer hence the soil hydraulic parameters are derived by minimizing an objective function formed from the observed and simulated data pollacco 2005 pollacco et al 2008b 2013a pollacco and mohanty 2012 experiments have shown that the objective function has a unique solution but suffers from excessive sensitivity to some parameters pollacco et al 2008a 2008b pollacco and angulo jaramillo 2009 the methodology proposed by fernández gálvez et al 2021 and further validated in vogeler et al 2021 reduces the sensitivity of the bimodal soil kosugi hydraulic parameters by preventing non physical combinations of hydraulic parameters pollacco et al 2008b the kosugi 1994 1996 hydraulic functions are used because their parameters are physically related to the pore size distribution of the soil the multistep optimization method is implemented into the new performant hydrological pixel hypix model hypix pollacco et al 2022 is a physically based hydrological model and solves the mixed form of the richardson richard s equation ree by using the newton raphson method the non linear rre is solved with an efficient heuristic and physical time stepping strategy using a reduced number of control parameters hypix also incorporates a novel algorithm to avoid overshooting by controlling the newton raphson step hypix can a process a large number of soil layers b simulate kosugi unimodal and bimodal hydraulic parameters for each soil layer c simulate realistic water ponding at the soil surface by using a novel approach for the computation of sorptivity lassabatere et al 2021 2022 d compute rainfall interception from leaf area index e derive transpiration from root water uptake with a compensation mechanism for deeper layers where root density is limited f compute evaporation and g compute drainage through the bottom of the soil profile under different boundary conditions this paper is organized as follows section 2 presents the theory describing the bimodal kosugi hydraulic functions an overview of the hypix model and the novel vertical multistep optimization scheme used to upscale the hydraulic parameters section 3 describes the experimental data and methodology used to illustrate and validate the proposed scaling method section 4 shows the results of the derived soil hydraulic and vegetation parameters from inverse modelling section 5 illustrates some of the direct applications in future research and section 6 summarizes the main conclusions 2 theory 2 1 soil hydraulic functions 2 1 1 bimodal kosugi soil hydraulic functions hypix uses the bimodal kosugi 1994 1996 soil hydraulic functions the choice of the kosugi soil hydraulic functions is based on the physical interpretation of the parameters in relation to the soil pore size distribution and the fact that these parameters can be constrained by exploiting the relationship between them fernández gálvez et al 2021 pollacco et al 2013b moreover the selection of bimodal functions is based on the prevalence of soils with a bimodal pore system e g jarvis 2007 mcleod et al 2008 where macropores and micropores lead to a two stage drainage fast flow macropore flow can occur when the water pressure head exceeds the threshold needed to activate the macropore network adding to the matrix flow below this threshold only the matrix participates in the flow fernández gálvez et al 2021 the representation of the soil water retention curve θ ψ l3 l 3 and the unsaturated hydraulic conductivity k ψ l t 1 functions is based on the dual porosity model of pollacco et al 2017 1 θ ψ θ mat ψ θ mac ψ θ mat ψ 1 2 θ s macmat θ r erfc ln ψ ψ m 2 σ θ r θ mac ψ 1 2 θ s θ s macmat erfc ln ψ ψ m mac 2 σ mac 2 k ψ k mat s e ψ k mac s e ψ s e ψ θ θ r θ s θ r 1 2 θ s macmat θ r θ s θ r erfc ln ψ ψ m 2 σ θ s θ s macmat θ s θ r erfc ln ψ ψ m mac 2 σ mac k mat s e ψ k s θ s macmat θ r θ s θ r s e ψ 1 2 erfc ln ψ ψ m 2 σ σ 2 2 k mac s e ψ k s θ s θ s macmat θ s θ r s e ψ 1 2 erfc ln ψ ψ m mac 2 σ mac σ mac 2 2 where erfc is the complementary error function θ l3 l 3 represents the volumetric soil water content and ψ l the soil water pressure considering ψ 0 for unsaturated soils i e matrix suction θ s l3 l 3 and θ r l3 l 3 are the saturated and residual volumetric soil water content respectively ln ψ m and σ denote the mean and standard deviation of ln ψ respectively in the soil matrix domain ln ψ mmac and σ mac denote the mean and standard deviation of ln ψ respectively in the macropore soil domain with the argument of ln in units of length i e ψ m ψ and ψ mmac in l θ smacmat l3 l 3 is the volumetric saturated water content that theoretically differentiates inter aggregate pores structural macropores and matrix domains intra aggregate micropores defining the corresponding soil water pressure threshold between macropore and matrix ψ macmat l s e ψ denotes the effective saturation as a function of ψ with values between 0 and 1 k s l t 1 is the saturated hydraulic conductivity and k s e ψ l t 1 refers to the unsaturated hydraulic conductivity written as a function of s e ψ for the case where θ smacmat θ s eq 1 and eq 2 reduce to the unimodal kosugi soil hydraulic functions 2 1 2 constraining bimodal kosugi soil hydraulic parameters estimating the bimodal kosugi soil hydraulic parameters from observed θ requires the simultaneous estimation of eight parameters θ s θ r σ ψ m k s θ smacmat σ mac and ψ mmac using limited measurement data here we use the term optimize in the estimation of the parameters since the task is to produce a set of values for the parameters that optimizes typically minimizes an objective function therefore optimization is used in the sense of parameter estimation in certain cases full inversion of the data to produce parametric estimates of the bimodal kosugi soil hydraulic model is not possible for example where measurements have a restricted range pollacco et al 2008b leading to highly sensitive parameters pollacco et al 2008b 2013a pollacco and mohanty 2012 reducing the sensitivity of the parameters requires either adding novel independent measurements or incorporating constraints to reduce the effective complexity of the model since gathering new measurements is in this case not feasible here we use the set of constraints proposed by fernández gálvez et al 2021 and further validated in vogeler et al 2021 which reduces the number of parameters to be optimized without compromising the fit of the hydraulic functions while the estimated hydraulic parameters still have physical meaning this set of constraints can be summarized as follows θ r is derived from σ ψ mmac and σ mac which are considered constant for a fixed value of ψ macmat equal to 100 mm and ψ m and σ are dynamically constrained based on the assumption that θ ψ and k θ are lognormally distributed therefore the number of hydraulic parameters to be optimized is reduced from eight to five using the principles of soil physics although θ s can be derived from total porosity when soil bulk density and particle density data are available in this case it is an optimized parameter with a feasible range determined from the maximum observed θ in the corresponding layer the soil hydraulic parameters are optimized using the bimodal θ ψ and k θ model described in section 2 1 1 this is performed by matching simulated θ time series derived from the hypix model with observed θ time series at the corresponding depth the dynamic physically feasible range of the five optimized bimodal kosugi soil hydraulic parameters using the full set of constraints derived from fernández gálvez et al 2021 is indicated in table 1 note that ψ macmat is a constant with a value of 100 mm and ln ψ macmat is higher than ln ψ mmac by three times σ mac p σ 3 eq 3 in fernández gálvez et al 2021 here θ r is derived from σ in eq 3 and table 1 and is given by 3 σ σ σ min σ max σ min θ r σ θ r max 1 e α 1 σ α 2 1 e α 1 where θ rmax is set at 0 2 the maximum value for θ r that was found to be satisfactory pollacco et al 2013b α1 15 and α2 4 are two optimized empirical parameters σ is the normalized σ and σ min and σ max are set at 0 75 and 4 from table 1 therefore with all these simplifications and additional constraints we define a model that requires only five parameters θ s σ ψ m k s and θ smacmat 2 2 summary of hypix model modelling unsaturated flow in highly heterogeneous soils can accurately be performed by solving the rre which is commonly adopted by hydrological and soil vegetation atmosphere transfer models the hypix model implements improvements to solve the rre by including a dynamic physical smoothing criterion for controlling the newton raphson step and a novel time stepping management scheme based on ψ without introducing further parameters pollacco et al 2022 the solution of the rre is based on maina and ackerer 2017 for which the rre partial differential equation is solved using a cell centred finite volume implicit finite differences scheme for the spatial discretization with an implicit euler scheme for the temporal discretization by using the weighted average inter cell hydraulic conductivity assuming a rigid solid matrix the mixed form of the rre is written as 4 θ i ψ i t θ i ψ i t 1 δ t t s o θ i ψ i t θ s i ψ i t 1 ψ i t δ t t q i 1 2 t q i 1 2 t δ z i s i n k i ψ i t 1 where δt t t is the time step at time t δz i l is the mesh size of cell i with the vertical coordinate positive downwards θ i l3 l 3 is the volumetric soil water content of cell i θ s i l3 l 3 is the saturated volumetric soil water content of cell i s 0 l 1 is a parameter that accounts for fluid compressibility which is assumed to be constant with depth ψ i l is the soil water pressure of cell i considering ψ 0 for unsaturated soils q l t 1 is the soil water flux based on the extended darcy buckingham s law which is positive downward and negative when water moves upwards q i 1 2 t l t 1 is the flux entering cell i and q i 1 2 t l t 1 is the flux exiting cell i and sink i l3 l 3 t 1 taken as positive is the sink term defined as the volume of water per unit time removed from cell i by soil evaporation and root water uptake additional details of the hypix model can be found in pollacco et al 2022 2 3 novel vertical multistep optimization the aim of this study is to derive a strategy to optimize the five hydraulic parameters θ s σ ψ m k s θ smacmat of the parsimonious bimodal θ ψ and k θ functions described in section 2 1 1 for each layer of the soil profile by using the hypix model each layer of the profile corresponds to the depths where θ is experimentally measured for example in a site where θ values are measured at five depths 25 5 5 hydraulic parameters need to be optimized it is challenging to optimize 25 parameters simultaneously however pollacco 2005 found that optimizing each layer separately produces poor results particularly for a highly heterogeneous soil profile in the presence of lenses of clay pebbles or macropore flow this is because an optimization method that isolates each layer without considering the overall water flow in the soil profile causes unrepresentative parameters and thus poor representation of the soil water fluxes kamali and zand parsa 2016 therefore we present an inverse modelling algorithm for layered soils when optimizing the hydraulic parameters the soil profile is initially considered homogeneous then a stepwise grouping of local layers zones defined by the end user allows heterogeneous patterns to be addressed the optimization of the different layers in a specified order and pattern is presented in table 2 table 2a for odd and table 2b for even number of layers where 0 or 1 indicates which soil layers are optimized simultaneously at each specific step for example layers containing the number 1 in table 2 show the grouping of different layers zones in which the soil hydraulic parameters have the same optimal value i e homogeneous layer in the first step opt 1 it is assumed that the soil is homogeneous and therefore the whole profile is modelled with five optimized soil hydraulic parameters and the same values were given for each parameter in the different layers of the soil profile the derived effective optimal hydraulic parameters will be used for hydrological models requiring only one homogeneous layer in the second step opt 2 optimal hydraulic parameters for models requiring two layers only the parameters of the upper half of the profile are optimized maintaining the bottom half below the root zone with the value of optimized parameters derived from the previous step and then the third step opt 3 optimal hydraulic parameters for models requiring three layers etc operates on the deeper zone keeping the upper half of the profile with the values of the parameters with values of the previously optimized parameters optimizing from top to bottom was observed to produce better results than from bottom to top because water percolates downwards so a change of the hydraulic parameters of the top layer will affect the lower layer note that the top layer is also the layer where the water content has larger variations with time each zone of the profile is successively split into two zones from top to bottom and the optimization is repeated by copying the values of the optimal parameters from the previous optimization step the vertical multistep optimization assumes that the number of soil layers il corresponds to the number of θ observations the centre of each soil layer is also given by the corresponding θ observation 2 3 1 evaluation of improvements in accuracy by increasing the number of layers the procedure for identifying the relative errors between optimization performed with the greatest number of layers and simulated with a reduced number of layers is presented in table 3 it is assumed that outputs from the hypix model computed with the greatest number of layers il is the reference free of error ref and simulations computed with a reduced number of layers are simulated sim the reduction of error in different steps of computation a drainage eq 5 b evapotranspiration eq 6 and c root zone soil water content top 600 mm eq 7 is possible by increasing the number of layers il as presented in table 2 2 3 2 weighted objective functions of the multistep optimization the global optimizer searches for the optimal hydraulic parameters by minimizing a weighted objective function wof we selected the robust global optimizer blackboxoptim v0 6 1 https github com robertfeldt blackboxoptim jl written in julia bezanson et al 2017 and selected the adaptive de rand 1 bin radiuslimited method for every optimization step the maximum evaluation functions that are allowed to be optimized are controlled by maxfuncevals which was set to 100 we designed the novel wof to address the issue that observed θ θ obs l3 l 3 in deeper layers below the root zone contains greater uncertainties compared to measurements in the root zone it is also important that simulated θ θ sim l3 l 3 gives more accurate predictions in the root zone than below the root zone this is because most of the dynamism of soil evaporation and root water uptake takes place in the root zone prioritizing the root zone by optimizing from top to bottom is performed in the algorithm used by the vertical multistep optimization section 2 3 this is performed by introducing a weighting w in the wof by assuming that the measurements placed at different depths are equally spaced the wof is computed as follows 8 w i l 2 n i l 1 i l n i l n i l 1 i l 1 n i l w i l 1 w o f θ t n t i l n i l w i l θ o b s i l t θ s i m i l t 2 n i l n t where n il is the total number of layers il where θ is measured n t is the total number of time steps and θ obs and θ sim are observed and simulated θ respectively 3 material and methods 3 1 data 3 1 1 monitoring periods for calibration and validation the monitoring periods of the θ at each depth for the five experimental sites were split in two using one period for model calibration and another for model validation the dates of calibration and validation for each site were selected to consider the different installation periods sensor stabilization after installation erroneous data data gaps and the occasional presence of the groundwater table in deeper layers at some sites the calibration and validation periods were chosen to include a wide variety of dry and wet periods and simulations were always preceded by a 2 month warm up period there were not enough data for validation for one of the sites waihou due to malfunctioning of the sensors 3 1 2 climate daily values of precipitation δpr l were measured adjacent to the soil water content probes using a tipping bucket electronic rain gauge recording at 0 2 mm resolution potential evapotranspiration δpet l was derived from the new zealand virtual climate stations network vcs tait and woods 2007 with estimates based on the spatial interpolation of actual data observations made at climate stations located around the country tait 2010 for a given variable x we used the symbol δx to indicate the change in the variable x in a given time step as follows t 1 t x t t 1 t 1 x t e g δpr is the cumulative precipitation between two time steps the yearly average precipitation and potential evapotranspiration using representative wet and dry periods are shown in the results section table 5 by using hypix model 3 1 3 soils the sites used in this study are located in the waikato region of new zealand fig 1 all soils have formed from airfall volcanic tephra but vary in their soil physical properties and heterogeneity particularly their texture and profile drainage characteristics a brief description of the soils as follows well drained soils the taupō soil is a sandy textured soil formed from volcanic airfall pumice material new zealand classification podzolic orthic pumice soil usda soil taxonomy classification orthod hewitt 2010 the otorohanga and waihou soils are also formed from airfall volcanic material but with finer tephra material compared with taupō resulting in silty loam topsoil textures grading to silty clay in the subsoil they are classified in new zealand as typic orthic allophanic soils and in soil taxonomy as a haplohumult hewitt 2010 all soils are characterized by well drained morphology having yellow brown coloured subsoils with no redox mottles indicating that the frequency and duration of internal waterlogging is minimal and oxidation processes predominate based on the soil morphology the otorohanga and waihou soils would be considered to have the least heterogeneous soil profiles of those used in this study since they have a reduced number of distinct layers the taupō soil although free draining is expected to show more heterogeneity in water movement due to the stone content cichota et al 2016 imperfectly and poorly drained soils the hamilton soil has a silt loam topsoil overlying clayey textured subsoils having formed into strongly weathered volcanic tephra it is classified in new zealand as a typic orthic granular soil and in soil taxonomy as a haplohumult hewitt 2010 the drainage class indicates a degree of subsoil drainage impediment that may result in short term waterlogging during wet periods although oxidation processes predominate most of the time the waitoa soil is a silty textured soil it is classified in new zealand as a typic orthic gley soil and in soil taxonomy as a haplohumult hewitt 2010 the drainage class indicates a slow draining subsoil with subsoil saturation occurring most years in winter and early spring the soil morphology of the hamilton and waitoa shows the greatest heterogeneity of the soils in this study as reflected in previous studies of soils with impeded drainage features mcleod et al 2008 vogeler et al 2019 3 1 3 1 time series of soil water content θ θ measurements were recorded at 15 min intervals with decagon 5tm frequency domain reflectometers located at up to five depths for each of the experimental sites the monitoring depths for the different sites differ corresponding to the soil morphological features at each individual site both the rain gauges and the θ sensors were calibrated following the new zealand standard duncan et al 2013 in particular the θ were calibrated against neutron probe measurements taken at particular times throughout the monitoring period the neutron probe data were calibrated with θ measurements from sample weighting in situ and after oven drying for each depth and site 3 1 4 vegetation experimental sites are five mixed non irrigated pasture grass sites the trapezoidal water stress response function feddes et al 1978 was used with parameters for mixed pasture grass in new zealand derived from van housen 2015 for all sites we used default values at four soil water pressures ψ feddes1 100 mm ψ feddes2 250 mm ψ feddes3 5000 mm and ψ feddes4 80 000 mm these parameters are not as sensitive as the crop coefficient k c which we optimized pollacco et al 2008a the k c and leaf area index lai vary throughout the growing season due to the lack of k c and lai data it was assumed that there is a positive linear relationship between lai and pasture growth as already suggested by many authors e g kaur and singh 2013 punalekar et al 2018 and a positive linear relationship between k c and pasture growth allen and pereira 2009 showed that k c is a function of the fraction of ground cover and crop height which is also related to pasture growth pasture growth kg drymatter ha day data were provided by dairynz for the waikato region https www dairynz co nz media 5793237 average pasture growth data waikato 2020 v1 pdf with the following normalized monthly values 0 1 from january to december 0 48 0 24 0 15 0 23 0 34 0 14 0 00 0 23 0 65 1 00 0 79 and 0 72 the range of k c 0 8 to 0 95 was taken from rotated grazing pasture according to fao irrigation paper 56 allen et al 1998 and the range of the lai 0 19 to 5 10 was taken from van housen 2015 normalized lai and k c are calculated as 9 normalised v a l u e v a l u e v a l u e min v a l u e max v a l u e min where the corresponding value of the vegetation parameters lai min lai max k cmin and k cmax as well as the maximum saturated storage capacity of a wet canopy sint sat l are optimized after multistep opt 1 and reoptimized after multistep opt 9 or opt 7 depending on whether the number of soil layers is five or four then the multistep optimization is rerun with the updated vegetation parameters for all sites maximum root depth z nroot was averaged to 800 mm vogeler and cichota 2019 and the percentage of roots in the top 300 mm rdf top was 90 evans 1978 3 2 hypix model parameters the vertical discretization of the soil profile has 37 cells for soils with five layers and 25 cells for soils with four layers depth between 0 and 500 mm is discretized with cells of mesh size δz 20 mm and depth between 500 and 1200 mm is discretized with cells of mesh size δz 50 mm the vertical discretization is the same as the one used in pollacco et al 2022 simulations use the standard values of the hypix model parameters as shown in table 4 3 3 goodness of fit the goodness of fit between the model outputs θ sim and the corresponding observations θ obs was assessed using the best concordance correlation coefficient ccc and the refined index of agreement dr proposed by willmott et al 2012 the ccc is defined as follows 10 c c c 2 ρ σ θ o b s σ θ s i m σ θ o b s 2 σ θ s i m 2 μ θ o b s μ θ s i m 2 where μ θ o b s and μ θ s i m are the observed and simulated means of θ at each depth where θ is measured σ θ o b s 2 and σ θ s i m 2 are the observed and simulated variances of θ at each depth where θ is measured and ρ is the pearson correlation coefficient between observed and simulated values ccc is equal to 1 for a perfect fit the dr index with values between 1 0 and 1 0 indicates the sum of the magnitudes of the differences between the model predicted and observed deviations about the observed mean relative to the sum of the magnitudes of the perfect model θ obs θ sim for all i and observed deviations about the observed mean this index is used because in general it is more rationally related to model accuracy than other existing indices willmott et al 2012 the dr index can be written as follows with c 2 11 d r 1 i 1 n t θ s i m i θ o b s i c i 1 n t θ o b s i θ o b s when i 1 n t θ s i m i θ o b s i c i 1 n t θ o b s i θ o b s c i 1 n t θ o b s i θ o b s i 1 n t θ s i m i θ o b s i 1 when i 1 n t θ s i m i θ o b s i c i 1 n t θ o b s i θ o b s a value for dr of 0 5 for example indicates that the sum of the error magnitudes is one half of the sum of the perfect model deviation and observed deviation magnitudes when dr 0 0 it signifies that the sum of the magnitudes of the errors and the sum of the perfect model deviation and observed deviation magnitudes are equivalent when dr 0 5 it indicates that the sum of the error magnitudes is twice the sum of the perfect model deviation and observed deviation magnitudes values of dr near 1 0 can mean that the model estimated deviations about θ o b s are poor estimates of the observed deviations but they can also mean that the observed variability is insignificant when approaching the lower limit of dr interpretation should be made cautiously 4 results 4 1 hydraulic and vegetation parameters derived from inverting observed θ 4 1 1 water balance components the rainfall interception algorithm in hypix pollacco et al 2022 uses vegetation parameters described in section 3 1 4 the interception loss is computed for the five pasture grass sites in table 5 ranging from 8 to 20 with annual throughfall precipitation values corresponding to 2298 and 823 mm respectively as expected the interception loss is greater for sites with smaller annual rainfall waitoa and waihou compared to sites with a larger annual rainfall otorohanga and taupō because the saturated storage capacity sint sat of the interception model has greater impact on smaller rainfall events than on large ones simulated yearly drainage values are lower at sites with lower precipitation while simulated yearly evaporation values show a distinctive behaviour at each site the closure of the simulated water balance is computed by the normalized soil water balance to the infiltration error wb in table 5 with very small discrepancies between 4 4 10 6 and 0 2 10 6 the results of interception loss between 8 and 20 are similar to values derived by thurow et al 1987 for curly mesquite short grass and sideoats mid grass dominated sites with 10 8 and 18 1 interception loss respectively note that interception varies monthly with lai and k c section 3 1 4 the goodness of fit between θ obs and θ sim at all depths for each site is represented by the wof θ values in table 5 with satisfactory results between 0 009 and 0 014 table 6 shows the optimal values of vegetation parameters described in hypix pollacco et al 2022 for the five experimental sites due to the limited vegetation information available it was not possible to fully validate the rainfall interception model and its vegetation parameters as an excellent fit between observed and simulated θ does not guarantee that the vegetation parameters be physical plausible pollacco et al 2008a showed that when the vegetation parameters are optimized against observed θ assuming the soil hydraulic parameters are known which is not the case here the outputs of the drainage transpiration and interception are particularly sensitive to small uncertainties of the θ data and therefore the optimized vegetation parameters suffer from correlated parameters the reason for having correlated parameters pollacco et al 2008a 2008b pollacco and angulo jaramillo 2009 is that the rainfall interception and the evapotranspiration parameters are linearly linked which means that interception compensates for over under predicting evapotranspiration and vice versa if the water balance between interception and evapotranspiration is inadequate then the drainage will compensate without affecting the global water balance 4 1 2 model calibration and validation a monitoring period long enough to include a variety of wet and dry periods at each site was used for calibrating the soil hydraulic properties for each layer derived from inverse modelling by using the multistep optimization algorithm implemented in hypix section 2 3 the optimal bimodal kosugi soil hydraulic parameters from inverse modelling by using the multistep optimization algorithm with observed θ at the five experimental sites are shown in table 7 the selection of the calibration period is crucial to provide an accurate estimate of the hydraulic parameters and due to malfunctioning of the waihou site there were not enough data left for validating this site after successfully automatically calibrating the hydraulic and vegetation parameters at each site hypix was run for a validation period using the optimal hydraulic and vegetation parameters obtained from calibration simulations corresponding to the calibration and validation periods are shown in fig 2 where time series of precipitation δpr throughfall precipitation δprthrough ponding δhpond drainage δq potential evapotranspiration δpet sink term δsink and evaporation δevap are presented above the θ values at depths corresponding to measurements for the five experimental sites note that the intercepted precipitation varies monthly with lai and k c section 3 1 4 simulated values of θ at different measuring depths were found to be close to the observations and follow the general trend according to the forcing input data with larger changes in θ near the surface and less responsive values deeper in the soil profile rainfall events are followed by a sudden increase in θ that gets reflected progressively down into the soil profile after these episodes θ decreases progressively with an approximately simultaneous increase in both simulated drainage and evapotranspiration a sudden increase in evapotranspiration values occurs because after each rainfall event the amount of water available next to the soil surface is greater drainage does not take place immediately after a rainfall event there is a delay in the drainage pulse with differences related to the amount of water reaching the surface during each rainfall event and the hydraulic characteristics of each experimental site the goodness of fit between measured and simulated θ for the calibration and validation periods at each of the experimental sites using the dynamically constrained set of hydraulic parameters is shown in table 8 a better fit is obtained for the first four sites waitoa waihou taupō and otorohanga with higher ccc 0 743 and dr 0 666 values compared to the ones for the hamilton site with ccc 0 596 and dr 0 533 the goodness of fit for validation is only marginally lower than for calibration providing confidence in the retrieved hydraulic and vegetation parameters implemented in hypix for each site only for the hamilton site the dr index is reduced by half for the validation period due to both the higher soil heterogeneity and the shorter monitoring period with available data in fact the hamilton site with its slow draining subsoil and a high soil heterogeneity is the most challenging site studied which also be due to errors in the climate data one of the challenges addressed by the automatic multistep optimization algorithm section 2 3 and the weighted objective function eq 8 is to predict the root zone θ more accurately and ensure the predictions below the root zone remain acceptable this is because the water balance is mainly determined in the root zone where the sink term is active this is challenging since greater uncertainties are found in measuring θ in the root zone due to presence of roots and macropores which can be seen in the otorohanga site fig 2 where θ sim modelled at 100 mm is more accurate during the validation period compared with the calibration period this may be because the time required for the top sensor to settle down is greater than for the other sensors contact between the sensor and the surrounding soil 4 1 3 multistep optimization for vertical scaling the improvement of the fit between observed and simulated θ with increasing splitting of the soil profile layers as described in table 2a five depths of measuring θ and table 2b four depths of measuring θ depends on the a heterogeneity of the soil profile b aptitude of the model to represent the hydrological processes which is quantified by the success of the rre to reproduce θ obs c accuracy of θ obs the maximum number of splits of the soil profile required by a hydrological model to obtain the desired accuracy in a particular output is difficult to establish since it does not depend solely on the severity of heterogeneity in the soil but also on the required accuracy of the outputs of the model table 9 shows the minimum number of layers required to achieve a particular accuracy in drainage evapotranspiration and soil water content of the root zone for each of the experimental sites by assuming that outputs from the hypix model computed with the greatest number of layers il is the reference free of error as indicated in table 3 the selected accuracy indicated in table 9 for each model output is chosen as an example and the measure of the accuracy is evaluated by the relative errors described in table 3 fig 3 a shows the decrease of goodness of fit between observed and simulated θ represented by the wof θ eq 8 with the increasing amount of splitting of the soil profile using the multistep optimization scheme described in section 2 3 for each site as can be observed the decrease is less pronounced for less heterogenous soils with the higher impact for the most heterogeneous site fig 3b d illustrate the results corresponding to the algorithms which compute the percentage errors table 3 as a function of the amount of splitting for a drainage ζ q b evapotranspiration ζ e t and c soil water content of the root zone top 600 mm ζ s w c in the evaluation of the errors the discretization with the greatest amount of splitting of the profile greatest number of layers was considered as the reference the minimum splitting needed to achieve a particular goal is described in table 9 as previously indicated table 9 shows that the minimum number of required layers depends on the output of interest clearly accurate computation of evapotranspiration requires the greatest number of layers compared to drainage and soil water content in the root zone this is explained by the fact that evapotranspiration is influenced by the movements of water from deeper layers to the surface and therefore requires accurate layering along the profile the dynamics of evapotranspiration and soil water content occur only in the root zone and therefore the impact of having accurate hydraulic information at greater depths is less important the fact that small errors in θ cause large errors in drainage and evapotranspiration was already reported by pollacco et al 2008a and pollacco and mohanty 2012 which is why in table 9 the threshold of ζ s w c is 2 5 and not 5 whilst these results are indicative of the effects of reducing the number of layers providing guidance on the minimum number of layers to be used for modelling at the catchment or region level would require a larger number of experimental sites 4 1 3 1 time of execution of multistep optimization the yearly average execution time for each site of performing the multistep optimization by running hypix model is described in table 10 hypix is run with julia 1 7 3 on a laptop equipped with 64 gb ram and intel xeon e 2186m cpu 2 9 ghz processor to facilitate inter comparison the simulations are scaled to a period of 1 year table 10 shows that there are differences in execution time which are largely due to the heterogeneity of the soil layers pollacco et al 2022 which influences the efforts of the rre to converge 4 1 4 modelled heterogeneity compared to pedological description table 11 classifies the result reported in table 9 and shows the minimum number of layers to accurately model each site the classification derived from inverse modelling is compared to the soil characteristics derived from the soil database s map despite the limited number of experimental sites included in this study table 11 shows there is a clear relationship between the number of layers required for adequate representation of the soil hydraulic parameters and the heterogeneous soil description derived from s map both the soil heterogeneity and the drainage class from s map can provide an indication of the vertical discretization required to accurately compute the soil water balance components for example well drained soils can generally be adequately described by a relatively small number of layers by contrast poor or imperfectly drained soils require more complex descriptions with more layers 5 future research next steps in implementing this approach to scaling smap hydro information include a sensitivity analysis of deriving hydraulic parameters by using the vertical multistep optimization assuming fixed vegetation parameters assessing the value of remotely sensed sources of surface soil water content and transpiration and testing the scaled hydrological parameterizations in the various hydrological models used in new zealand another application of the multistep optimization is that observed θ derived at multiple depths e g from time or frequency domain reflectometers is becoming readily available in precision agriculture in new zealand to control irrigation rates drewry et al 2019 ekanayake and hedley 2018 el naggar et al 2020 therefore the proposed methodology of deriving unique sets of hydraulic parameters from observed θ time series would enable physically based models to optimize the application of irrigation and minimize leaching through excess drainage 6 conclusions we derived a novel vertical multistep algorithm that enables us to invert observed θ at multiple depths by fitting only five bimodal kosugi hydraulic parameters to every layer and optimizing the hydraulic parameters in a particular pattern from top to bottom by successively splitting the profile the multistep optimization algorithm puts more weight into optimizing the top layers because most of the dynamism of the soil evaporation and root water uptake takes place in the root zone the optimization algorithm upscales the soil hydraulic parameters gradually introducing heterogeneity because optimizing the hydraulic parameters of each layer individually produces poor results since it does not represent the overall soil water dynamics across the unsaturated zone it has been shown that the minimum number of layers to accurately estimate drainage evapotranspiration and soil water content of the root zone computed from the hypix model depends on the heterogeneity of the soil and the required accuracy for each output of the model it was found that a more detailed layered model is required when computing evapotranspiration compared to drainage the multistep optimization is useful to upscale a detailed layered profile of soil hydraulic parameters into a model with fewer layers and provide an estimate of the uncertainty due to the reduced number of layers moreover the soil description from s map provides an indication of the vertical discretization required to accurately compute the soil water balance components it was found that the richards richardson equation converges faster in hypix when the hydraulic parameters describing θ ψ and k θ are plausible this is performed during the optimization process by dynamically constraining the relationship between ψ m and σ the hypix model efficiently solves the mixed form of the richards equation using a cell centred finite volume scheme for the spatial discretization with an implicit euler scheme for the temporal discretization by using the weighted average inter cell hydraulic conductivity hypix includes rainfall interception soil evaporation root water uptake with compensation algorithm and ponding using the novel computation of sorptivity software availability the hypix model can be downloaded from https github com manaakiwhenua soilwater toolbox tree master src hypix and is open source under the gp 3 0 license this software is part of a set of interlinked modules implemented into the soilwater toolbox ecosystem led by j a p pollacco from manaaki whenua landcare research in new zealand and j fernández gálvez from the university of granada in spain the preliminary objectives of the soilwater toolbox are to derive the soil hydraulic parameters by using a wide range of cost effective methods the estimated hydraulic parameters can be directly implemented into hypix to compute the soil water budget the soilwater toolbox enables performing inter comparison and sensitivity analyses of the hydraulic parameters computed from different methods on the soil water fluxes the following modules are currently included in the soilwater toolbox intergranular mixing particle size distribution module derives unimodal hydraulic parameters by using particle size distribution pollacco et al 2020 general beerkan estimation of soil transfer parameters module derives the unimodal hydraulic parameters from single ring infiltration experiments fernández gálvez et al 2019 sorptivity module a novel computation of sorptivity used in the general beerkan estimation of soil transfer parameters method lassabatere et al 2021 2022 saturated hydraulic conductivity module derived from unimodal and bimodal θ ψ pollacco et al 2013b 2017 inverse module which inverts hydraulic parameters from θ time series as in this paper reduce uniqueness module of a physical bimodal soil kosugi hydraulic parameters from inverse modelling fernández gálvez et al 2021 using water retention and or unsaturated hydraulic conductivity data directly measured in the laboratory or indirectly obtained from inverting θ time series as described in this paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the new zealand ministry of business innovation and employment throught the winning against wildings c09x1611 and next generation s map c09x1612 research programmes mathieu sellier from mechanical engineering of the university of canterbury new zealand is thanked for reviewing the manuscript funding for open access was provided by university of granada cbua spain 
