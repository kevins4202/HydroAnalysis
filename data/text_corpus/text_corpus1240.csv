index,text
6200,the author read with much interest the publication by mize et al 2018 on the decrease in suspended sediment concentrations and loads in the lower mississippi and atchafalaya rivers between 1980 and 2015 researchers interested in sediment dynamics in the lower mississippi and atchafalaya rivers also should be aware that sediment data previously reported for the lower mississippi atchafalaya river basin by the u s geological survey usgs and u s army corps of engineers usace were affected by a systematic error in the fines fraction particle sizes less than 0 063 millimeter of the suspended sediment concentration this error affected results reported from october 1973 through february 2015 for two sites on the main stem of the mississippi river three sites on the atchafalaya river one site on the old river outflow channel and one site on the red river the usgs and usace identified the error and have revised the results for samples collected from october 1989 through february 2015 as reported by norton et al 2019a and norton et al 2019b the significant conclusions of mize et al 2018 and sediment loads modeled using the weighted regressions on time discharge and season wrtds method were unaffected by the systematic error because the models used data from unaffected sites however the background information provided in tables 1 and 2 of mize et al 2018 included references to several previous studies that contained results affected by the error this comment paper identifies which references in mize et al 2018 were affected by the error and alerts readers to implications of the data revision keywords sediment water quality trend analysis wrtds coastal restoration data revision the author of this comment paper found the conclusions of mize et al 2018 on suspended sediment concentrations and loads in the lower mississippi and atchafalaya rivers to be an important contribution to the understanding of sediment transport in the lower mississippi atchafalaya river basin the author has also been aware of an effort to identify and correct a systematic error in the fines fraction particle sizes 0 063 mm of the suspended sediment concentrations previously reported by the u s geological survey usgs and u s army corps of engineers usace from october 1973 to february 2015 for seven sites in the lower mississippi atchafalaya river basin two of the affected sites are on the mainstem of the mississippi river the mississippi river at tarbert landing ms usgs site no 07295100 and the mississippi river at union point mile 326 usgs site no 07295025 which was moved from coochie la usgs site no 310552091361200 in 1991 three sites are on the atchafalaya river the atchafalaya river at simmesport la usgs site no 07381490 wax lake outlet at calumet la usgs site no 07381590 and the lower atchafalaya river at morgan city la usgs site no 07381600 one site is on the old river outflow channel old river outflow channel below hydropower channel usgs site no 07381482 which was moved from the old river outflow channel near knox landing usgs site no 310355091411500 in 2010 one site is on the red river red river above old river outflow channel above simmesport usgs site no 310408091424500 the nature of the correction to the data is that 94 percent of the revised values for suspended sediment concentration for these sites are lower than their corresponding originally reported values indicating that less suspended sediment moves through the river system than was previously reported norton et al 2019a b following revision the median instantaneous suspended sediment discharge at the atchafalaya river at simmesport was 105 000 short tons per day for october 1989 through february 2015 compared to 143 000 short tons per day as originally reported and the median for the mississippi river at tarbert landing was 315 000 short tons per day compared to 378 000 short tons per day as originally reported the author of this comment paper communicated with authors mize murphy and demcheck after the publication of mize et al 2018 and verified that the sediment load modeling did not involve any of the aforementioned sites for which the data were known to be in need of revision the weighted regressions on time discharge and season wrtds models in mize et al 2018 used results exclusively from two unaffected sites mississippi river near st francisville la usgs site no 07373420 and atchafalaya river at melville la usgs site no 07381495 the significant conclusion that suspended sediment concentrations and loads decreased by half between 1980 and 2015 remains valid however some of the publications that were cited in table 1 and table 2 for comparison purposes included suspended sediment results that were affected by the systematic error identified documented and subsequently revised by norton et al 2019a b the authors of this comment paper with the concurrence of the authors of mize et al 2018 identified the affected publications publications cited in mize et al 2018 that reported and or interpreted results that were affected by the systematic error in sediment concentrations and thus are affected by the data revision are allison and meselhe 2010 allison et al 2012 allison et al 2014 blum and roberts 2009 horowitz 2010 heimann et al 2011 kemp et al 2016 keown et al 1986 khalil and freeman 2015 kim et al 2008 kim et al 2009 little and biedenharn 2014 meade and moody 2010 morang et al 2013 mossa 1996 mossa 1988 nittrouer and viparelli 2014 rosen 2013 smith and winkley 1996 thorne et al 2008 welch et al 2014 winer 2011 and xu and rosen 2012 these publications and others that used the affected data including those not cited by mize et al 2018 likely reflect a positive upward bias in the suspended sediment concentrations and loads the magnitude of which varies depending on which sites and sample results were used declaration of competing interest none 
6200,the author read with much interest the publication by mize et al 2018 on the decrease in suspended sediment concentrations and loads in the lower mississippi and atchafalaya rivers between 1980 and 2015 researchers interested in sediment dynamics in the lower mississippi and atchafalaya rivers also should be aware that sediment data previously reported for the lower mississippi atchafalaya river basin by the u s geological survey usgs and u s army corps of engineers usace were affected by a systematic error in the fines fraction particle sizes less than 0 063 millimeter of the suspended sediment concentration this error affected results reported from october 1973 through february 2015 for two sites on the main stem of the mississippi river three sites on the atchafalaya river one site on the old river outflow channel and one site on the red river the usgs and usace identified the error and have revised the results for samples collected from october 1989 through february 2015 as reported by norton et al 2019a and norton et al 2019b the significant conclusions of mize et al 2018 and sediment loads modeled using the weighted regressions on time discharge and season wrtds method were unaffected by the systematic error because the models used data from unaffected sites however the background information provided in tables 1 and 2 of mize et al 2018 included references to several previous studies that contained results affected by the error this comment paper identifies which references in mize et al 2018 were affected by the error and alerts readers to implications of the data revision keywords sediment water quality trend analysis wrtds coastal restoration data revision the author of this comment paper found the conclusions of mize et al 2018 on suspended sediment concentrations and loads in the lower mississippi and atchafalaya rivers to be an important contribution to the understanding of sediment transport in the lower mississippi atchafalaya river basin the author has also been aware of an effort to identify and correct a systematic error in the fines fraction particle sizes 0 063 mm of the suspended sediment concentrations previously reported by the u s geological survey usgs and u s army corps of engineers usace from october 1973 to february 2015 for seven sites in the lower mississippi atchafalaya river basin two of the affected sites are on the mainstem of the mississippi river the mississippi river at tarbert landing ms usgs site no 07295100 and the mississippi river at union point mile 326 usgs site no 07295025 which was moved from coochie la usgs site no 310552091361200 in 1991 three sites are on the atchafalaya river the atchafalaya river at simmesport la usgs site no 07381490 wax lake outlet at calumet la usgs site no 07381590 and the lower atchafalaya river at morgan city la usgs site no 07381600 one site is on the old river outflow channel old river outflow channel below hydropower channel usgs site no 07381482 which was moved from the old river outflow channel near knox landing usgs site no 310355091411500 in 2010 one site is on the red river red river above old river outflow channel above simmesport usgs site no 310408091424500 the nature of the correction to the data is that 94 percent of the revised values for suspended sediment concentration for these sites are lower than their corresponding originally reported values indicating that less suspended sediment moves through the river system than was previously reported norton et al 2019a b following revision the median instantaneous suspended sediment discharge at the atchafalaya river at simmesport was 105 000 short tons per day for october 1989 through february 2015 compared to 143 000 short tons per day as originally reported and the median for the mississippi river at tarbert landing was 315 000 short tons per day compared to 378 000 short tons per day as originally reported the author of this comment paper communicated with authors mize murphy and demcheck after the publication of mize et al 2018 and verified that the sediment load modeling did not involve any of the aforementioned sites for which the data were known to be in need of revision the weighted regressions on time discharge and season wrtds models in mize et al 2018 used results exclusively from two unaffected sites mississippi river near st francisville la usgs site no 07373420 and atchafalaya river at melville la usgs site no 07381495 the significant conclusion that suspended sediment concentrations and loads decreased by half between 1980 and 2015 remains valid however some of the publications that were cited in table 1 and table 2 for comparison purposes included suspended sediment results that were affected by the systematic error identified documented and subsequently revised by norton et al 2019a b the authors of this comment paper with the concurrence of the authors of mize et al 2018 identified the affected publications publications cited in mize et al 2018 that reported and or interpreted results that were affected by the systematic error in sediment concentrations and thus are affected by the data revision are allison and meselhe 2010 allison et al 2012 allison et al 2014 blum and roberts 2009 horowitz 2010 heimann et al 2011 kemp et al 2016 keown et al 1986 khalil and freeman 2015 kim et al 2008 kim et al 2009 little and biedenharn 2014 meade and moody 2010 morang et al 2013 mossa 1996 mossa 1988 nittrouer and viparelli 2014 rosen 2013 smith and winkley 1996 thorne et al 2008 welch et al 2014 winer 2011 and xu and rosen 2012 these publications and others that used the affected data including those not cited by mize et al 2018 likely reflect a positive upward bias in the suspended sediment concentrations and loads the magnitude of which varies depending on which sites and sample results were used declaration of competing interest none 
6201,river ice plays an important role in biophysical and socio economic systems in northern regions and is related to climate variability at both regional and global scales the objective of this study was to estimate river ice thickness using several new machine learning methods extreme learning machine elm least squares support vector machine lssvm and their bootstrap versions belm and blssvm respectively to explore the usefulness of these methods we chose two stations mackenzie river and yellowknife river in the mackenzie river basin in the northwest territories canada the variables used to develop the river ice thickness cm estimation models included water flow m3 s snow depth cm and mean air temperature c with a time period of 1981 2016 two techniques namely the kendall theil robust line ktrl and the regularized expectation maximization regem methods were utilized to fill in the missing values in the time series records the performance of the developed models was measured using several statistical indicators correlation coefficient r root mean square error rmse mean absolute error mae their normalized equivalents expressed as a percentage root mean square percentage error rmspe and mean absolute percentage error rmpe bias error bias and willmott s index wi results indicated that the bootstrap elm model outperformed the elm lssvm and blssvm models in the testing phases based on a number of statistical measures for the belm using regem at the mackenzie river station the results were r 0 826 rmse 19 756 cm rmspe 33 011 mae 15 364 cm mape 35 988 bias 1 199 cm and wi 0 818 for the belm using ktrl at the yellowknife river station the results were r 0 856 rmse 15 444 cm rmspe 19 468 mae 12 006 cm mape 19 045 bias 0 966 cm and wi 0 868 the findings of this study indicate that the belm machine learning approach using meteorological variables is a promising new tool for river ice thickness estimation keywords bootstrap extreme learning machine least squares support vector machine ice thickness 1 introduction ice formation on lakes and rivers has an important role in shaping water resources management and engineering in cold regions as noted in earlier studies brown and duguay 2010 prowse et al 2011 arp et al 2015 ice formations can dominate the annual cycle of lakes and rivers this can isolate water bodies not only from moisture exchanges but from gas fluxes with the atmosphere for 60 80 of the year as well thus the presence of freshwater ice may result in ice jams ultimately leading to widespread spring flooding that often accounts for the largest hydrologic event of the year in northern latitudes prowse and carter 2002 moreover it can result in reduced hydroelectric power generation and navigation and transportation disruption as well as damage to infrastructure the environment and ecosystems shen and liu 2003 duguay et al 2015 however investigating different aspects of ice conditions requires extensive data on break up date freeze up date and ice thickness among other parameters moreover in northern regions such as canada the collection of data on freshwater ice is often challenging due to the site specific nature of such processes chu and lindenschmidt 2016 thus different forecasting and simulation models are required ice thickness is particularly useful in river ice hydraulic models chokmani et al 2007 flood routing models she and hicks 2006 hydrodynamic modeling of freezing rivers shen and liu 2003 and flow control in regulated rivers tuthill 1999 given the lack or incompleteness of ice thickness measurements different analytical mathematical and numerical models have been used to address this inadequacy of data while all numerical models developed for ice thickness simulation consider energy budget components they differ from each other by the level of detail used to compute the ice energy budget hicks et al 1997 andres and van der vinne 2001 ma and fukushima 2002 the one dimensional numerical model for river ice rice processes can simulate the growth and decay of the ice cover along with other associated parameters with a close agreement between observed and simulated data shen et al 1991 the updated river ice network ricen model provides substantial improvements in ice process simulation including consideration of the effects of wind artificial icebreaking by icebreakers and flow resistance due to moving ice shen et al 1995 however these kinds of models require extensive input data e g wind speed vapor pressure cloud cover which are not always accessible in addition indices that are needed for estimating energy fluxes due to shortwave radiation evaporation and convection at the air ice or snow interface cannot be accurately determined as of yet shen and yapa 1985 consequently simplified versions of energy budget models can be developed using stefan s law sl or the revised stefan s law rsl detailed descriptions of these equations can be found in the theoretical background section of this paper the date of onset of ice cover is a primary parameter in stefan s law equation as it determines the date of onset of the accumulation of freezing degree days for any given winter since in any one season this date is unknown for most sites stefan s law cannot be used however rsl based on the accumulation of freezing degree days from the first day of below freezing air temperatures can work reasonably well for ice thicknesses exceeding 0 10 m for ice thicknesses less than 0 10 m rsl overestimates ice thickness ashton 1989 based on a theoretical analysis shen and yapa 1985 introduced a modified degree day method capable of continuously simulating the variation of the thickness of river ice cover from formation to break up in their study the method s application to the upper st lawrence river in canada showed that simulated ice thicknesses compared well with field observations although an empirical coefficient was still required to judge the ice thickness growth rates at different types of river sites geographic information system gis and remote sensing techniques offer alternative means for monitoring the processes and patterns of ice cover there are different remote sensing systems that can be used to map ice cover extent and ice phenology chu and lindenschmidt 2016 optical sensors with low to medium spatial resolution and high temporal resolution such as the advanced very high resolution radiometer avhrr 1 km to 4 km spatial resolution and moderate resolution imaging spectrometer modis 250 m to 1 km spatial resolution are two types of ice information collection methods used for large freshwater ice surfaces even though high temporal resolution data such as modis and avhrr can provide timely information about river ice conditions the low spatial resolution and dependence on atmospheric conditions to gather these data can pose significant challenges in monitoring the details of river ice cover conditions microwave remote sensing e g synthetic aperture radar sar has proven to be another useful tool in monitoring and studying freshwater ice processes given its higher spatial resolution and capacity to acquire images under all weather and atmospheric conditions e g clouds as well as its sensitivity to water ice surfaces ice structure and ice thickness nghiem and leshkevich 2007 unterschultz et al 2009 duguay et al 2015 numerous studies have used sar imagery to monitor river ice thickness and reported promising simulation results unterschultz et al 2009 lindenschmidt et al 2010 mermoz et al 2014 however the main drawback of the numerical physically based models is their complexity and the fact that they require large amounts of precise data in order to estimate the physical properties of the problem domain furthermore a careful selection of model parameters sometimes based on field measurements is required to effectively calibrate the model yoon et al 2011 additionally many assumptions regarding the governing equations barzegar et al 2017 as well as some degree of user experience and expertise with the model are required to generate realistic simulations through numerical models inam et al 2017 data driven models require suitable data and their application also requires some degree of experience and expertise their forecasts however are based on statistical relationships between model inputs rather than physical laws or conceptualizations this affords data driven models some degree of flexibility over numerical models as model inputs can only comprise historical records of the process to be simulated many numerical models however may require particular inputs that are unavailable or costly to collect motivated by the limitations of numerical models as mentioned above researchers have sought to develop statistical data driven models with an acceptable level of accuracy and simple model development and application procedures such as the machine learning methods used in this study to estimate river ice thickness chokmani et al 2007 zaier et al 2010 for example parameters that influence ice thickness have been identified through single variable regression and factor analysis and the results combined in a multivariable linear regression analysis to address the complexity of ice growth and decay williams et al 2004 in their study accumulated freezing degree days and mean air temperature over a fixed period provided the best forecast of maximum ice thickness dornan 2005 developed a multiple linear regression model to determine the best model for river ice thickness by considering the effect of climate variables such as snow cover snow density and solar radiation cumulated solar radiation was not found to be a useful variable in the ice thickness models given its correlation with the most relevant model variable cumulative freezing degree days it is worth noting that by coupling data driven models with physically based models new approaches have been developed and used for the successful modeling of different hydrological phenomena e g runoff modeling modeling flows within wastewater pipe networks rainfall runoff modeling this offers an interesting method of coupling the strengths of both model types as a way of improving process understanding and prediction accuracy ayzel and izhitskiy 2018 young et al 2017 babovic 2009 yu et al 2004 vojinovic et al 2003 babovic et al 2001 another useful tool for river and lake ice thickness modeling is the artificial neural network ann in a comparison of the accuracy of lake ice thickness simulation with an ann model multiple linear regression mlr model revised stefan s law rsl model and the canadian lake ice model climo all relying on data for daily mean air temperature daily rainfall daily total solar radiation and daily snow depth seidou et al 2006 reported that the ann model followed data variations more closely than the rsl chokmani et al 2007 assessed ann based river ice thickness estimation methods drawing upon cumulative climate input variables such as freezing degree day cfdd solar radiation csr and snow cums finding that for sites in alberta these models provided good estimates with 90 mm rmse 130 mm however as the models failed to estimate low and high values correctly they were not considered ideal zaier et al 2010 tested six ann models five using ensemble modeling techniques e g randomization bagging and boosting for creating ensemble members and averaging and stacking for combining ensemble members and a single ann model for estimating ice thickness on 17 canadian lakes and demonstrated the superior performance of ensemble ann methods compared to the single ann model besides ann and mlr support vector machine svm and fuzzy logic have been successfully used in the estimation of other ice characteristics e g ice affected stream flow chokmani et al 2008 ice jam occurrence massie et al 2002 water level and ice jam thickness wang et al 2010 and date of ice break up and freeze up mahabir et al 2005 shouyu and honglan 2005 tao et al 2008 zhou et al 2009 zhao et al 2012 sun and trevor 2017 2018a 2018b despite the extensive application of ann in modeling hydrological processes and freshwater ice forecasts still suffer from deficiencies such as over fitting slow learning speed and local minima regression models that suffer from over fitting generally perform very well with training data sometimes forecasting the target with zero error however they tend to perform rather poorly with testing data this is because the resulting model parameters have been calibrated almost exactly for the training data but do not properly generalize examples outside of the training set the low speed of the training process is usually related to models that require iterative training approaches such as backpropagation and is most problematic in very large networks with a large amount of potentially redundant data furthermore the local minima issue is mostly related to iterative training approaches e g backpropagation where the model error parameter surface gets trapped in a localized area and where further iterations of model training cannot escape toward the global minimum of the objective function hamid et al 2012 kirar 2015 to overcome these limitations relatively new machine learning models e g extreme learning machine elm and least squares support vector machine lssvm have been developed the faster learning algorithms and improved generalization performance of the elm models have led to their extensive use in different environmental modeling processes e g drought deo and şahin 2015 water quality barzegar et al 2018b fijani et al 2019 groundwater level barzegar et al 2017 groundwater vulnerability barzegar et al 2018a water demand quilty and adamowski 2018 water flow yaseen et al 2016 atiquzzaman and kandasamy 2018 wave height ali and prasad 2019 and soil temperature feng et al 2019 in addition advanced design features of elm models such as analytical output determination by a least squares problem and random generation of the parameters of hidden nodes without the need for tuning the algorithm renders elm an interesting and new alternative to traditional forms of machine learning methods yaseen et al 2016 likewise lssvm models a simplified and more computationally efficient version of svm that uses kernel methods have the capacity to estimate output parameters more precisely than traditional techniques such as arima ann anfis or neuro fuzzy systems hong and pai 2006 wang et al 2009 in addition to the practice of newer forms of machine learning techniques researchers have started to investigate particular procedures such as bootstrapping in their modeling processes to improve model accuracy and or reliability belayneh et al 2016 jia and culver 2006 srivastav et al 2007 the bootstrap efron 1979 which uses resampling with replacement for a given input output dataset has been employed in hydrological applications to reduce uncertainty in model forecasts tiwari and chatterjee 2010b this method has been successfully applied to hydrological modeling given its ability to reduce uncertainty and provide accurate results especially in circumstances where relatively small training data sets are available erdal and karakurt 2013 to the best of our knowledge no studies have explored the potential of elm and lssvm coupled with the bootstrap technique for river ice thickness estimation this study is also the first attempt to model river ice thickness in the mackenzie river basin in the northwest territories canada performances of elm and lssvm models were compared with ann a widely used machine learning method to evaluate their relative estimation accuracy several quantitative performance indicators were employed for model comparison in addition the inherent accuracy of two imputation methods kendall theil robust line ktrl and the regularized expectation maximization regem were evaluated the imputation of missing values was considered in this study since historical river ice thickness records are sparse and ignoring observations with missing variables would result in a significant loss of information 2 study area the mackenzie system drains an area of about 1 805 200 km2 snowfall is the major form of precipitation and there is snow cover on the ground for more than six months a year over a substantial part of the basin aziz and burn 2006 the mackenzie river is the largest north flowing river in north america its basin is the largest in canada and is exceeded on the continent only by the mississippi missouri system the mackenzie river flows from great slave lake to the arctic and is entirely within the northwest territories the yellowknife river another important river in the mackenzie river basin flows south and empties into yellowknife bay part of great slave lake at the city of yellowknife due to various impacts such as frequent ice jams this area was chosen to test the feasibility of machine learning techniques in the estimation of ice thickness in the present study direct measurements of ice thickness cm and snow depth cm were collected from the environment canada winter gauges on the mackenzie river and yellowknife river meteorological stations formerly the water survey of canada collected this data river water flow m3 s was obtained from the mackenzie river at inuvik 10lc002 and the yellowknife river at the outlet of prosperous lake 07sb002 hydrometric stations the location of each station is shown in fig 1 the mean monthly air temperature c data for the stations were extracted from the nasa website https power larc nasa gov the experimental data was checked for quality by the various authorities that provided the data e g environment canada and nasa measurement of ice thickness was limited to one to seven measurements per season water flow data were recorded in digital form using a water flow sensor connected to an automated electronic recorder in total 166 and 156 available records of ice thickness snow depth and water flow for the mackenzie river and yellowknife river stations respectively were collected from 1981 to 2016 these records along with the mean monthly air temperature were used to develop test and evaluate the different models e g bootstrap elm bootstrap lssvm elm lssvm and ann an important step in machine learning modeling is the selection of appropriate input variables irrelevant and or redundant inputs can significantly influence model accuracy or add unnecessary complexity which impacts model reliability hejazi and cai 2009 quilty et al 2016 careful trial and error was applied to find the best set of input variables among the available data for river ice thickness estimation and three predictors namely water flow snow depth and mean air temperature were selected as inputs for the developed models for estimating the river ice thickness in the study area 3 theoretical background 3 1 stefan s law and revised stefan s law stefan s law or the degree day method is the most widely used formula for estimating ice thickness which is derived by simplifying the energy balance equations lock 1990 1 h a 0 d d where a 0 is an empirical constant that varies with the site d d is the sum of the degree days below the freezing point and h is the ice thickness in practice a 0 is used as an adjustable parameter with a value that is lower than the theoretical value to account for varying conditions of exposure and insulation michel 1971 gives a range of values adapted for a variety of lakes and rivers as explained in the introduction the date of onset of ice cover a basic parameter in stefan s equation is not known for the majority of sites the revised stefan s law rsl is a substitute equation which is based on d g the accumulation of freezing degree days starting with the first day of below freezing air temperatures in any given season the rsl has one more adjustable parameter c the effective number of degree days to be subtracted from d g in order to obtain d d seidou et al 2006 2 h a 0 d g c if d g c 0 if d g c since the occurrence of the first day of the freezing daily mean air temperature usually arrives several days or weeks before the date of ice onset c will normally take a positive value seidou et al 2006 3 2 imputation of missing values missing or incomplete datasets are a challenge to hydrologists attempting to model processes since machine learning models rely solely on data to learn the underlying input output relationships imputation techniques are used for missing values in the available dataset the total number of missing values for the mackenzie river and yellowknife river stations for ice thickness measurement dates were one and two respectively there were also some missing values for the snow depth sixteen and three for yellowknife river and mackenzie river stations respectively parameter the imputation techniques used in this study were the kendall theil robust line ktrl and the regularized expectation maximization regem these two methods for the imputation of scattered missing values substitute missing records differently the ktrl employs the correlation between one of the complete variables and the variable with missing values whereas regem considers the covariance between all variables to impute missing values in the dataset as a result both these methods were considered for substitution of missing records in the dataset of this study the ktrl was initially described by theil 1950 and is based on the kendall rank correlation coefficient τ theil 1992 the ktrl robust slope estimator or sen s slope is computed by comparing each pair of records to all others in a pair wise manner considering a dataset of x y of size n will result in n n 1 2 pair wise comparisons for each set of comparisons a slope δ y δ x is computed the slope estimate b k is the median of all slopes computed theil 1992 3 b k m e d i a n y j y t x j x t t j t 1 2 n 1 a n d j 2 3 n the ktrl intercept a k which is a function of its slope and the median values is defined as follows 4 a k m e d i a n y c b k m e d i a n x c the ktrl technique can be considered as an analogue to ordinary least squares ols with the main difference being that the ktrl regression line passes through the point representing the median values of the response and predictor median x median y whereas for ols it represents the mean values mean x mean y helsel and hirsch 2002 based on the ktrl method s robustness in the presence of outliers this method for substitution of scattered missing values is highly recommended khalil and adamowski 2014 khalil et al 2012 and was therefore implemented for the present study s dataset regem is based on an iterative analysis of the linear regression of missing and available values along with estimated regression coefficients obtained by ridge regression a regularized regression method in which the filtering of the noise in the dataset is controlled by a continuous regularization parameter schneider 2001 the conventional em algorithm and the regem are only applicable to datasets with scattered missing values and share the advantage of being easy to program having a low cost per iteration and providing an economy of storage li et al 2005 the regem for a dataset of size n is formulated as schneider 2001 5 x m μ m x a μ a b e where e is the assumed residual with unknown covariance matrix b is a matrix of regression coefficients x a is the vector of available values x m is the vector of missing values μ a is the vector of means of the available values and μ m is the vector of means of the missing values the conditional maximum likelihood estimate of the regression coefficients can be written as schneider 2001 6 b aa 1 am where aa 1 is the estimated covariance matrix of the available values and am is the estimated cross covariance of the available and missing values the regularization of the regression model in ridge regression is achieved by adjusting the inverse matrix aa 1 in eq 6 to be 7 aa h 2 d 1 where d is the diagonal matrix consisting of the diagonal elements of the covariance matrix aa and h is a positive number termed the ridge parameter a detailed derivation and discussion of these equations can be found in schneider 2001 3 3 extreme learning machine the extreme learning machine elm was first proposed by huang et al 2006b in order to overcome the limitations and shortcomings of its counterparts e g ann and svm huang et al 2006a the novel single layer feed forward network slfn in elm operates differently than the traditional feed forward backpropagation ffbp ann this confers on elm a shorter modeling time which is the main advantage of elm algorithms barzegar et al 2017 this arises because the input weights and biases are randomized and the output weights have a unique least squares solution solved by a moore penrose generalized inverse function huang et al 2006a huang et al 2006b huang and xiang 2015 this process provides an advantage over iterative training techniques that tend to collapse to local rather than global minima deo and şahin 2015 accordingly three simple steps are employed in elm algorithms i random generation of hidden layer weights and biases ii generation of the hidden layer output matrix by the input variables that are passed through the hidden layer parameters and iii estimation of elm output weights by inverting the hidden layer output matrix as well as computation of its product with the response variable the number of hidden neuron nodes has typically been identified by trial and error processes using a validation dataset randomization of the hidden layer weights and biases by a continuous probability distribution such as the uniform distribution normal distribution or triangular distribution must be generated in order to determine the output weights via least squares deo et al 2017 fig 2 illustrates the basic structure of an elm model considering a dataset with x t as the d dimensional predictor variables and y t as the predictand comprised of n training data records where t 1 2 n and x t r d and y t r the slfn with l hidden neurons is mathematically expressed as huang et al 2006b 8 i 1 h β i g i a i x t b i z t where ai rd ai rd β r h are the output weights between the hidden layers with h nodes and the model output g i a b x is the hidden layer activation function and z z t r is the model output there are different activation functions adopted in elm models e g tangent sigmoid logarithmic sigmoid hard limit triangular basis and radial basis the logarithmic sigmoid is the most commonly used activation function in the field of hydrological forecasting adamowski et al 2012 kişi 2008 and is denoted as follows huang et al 2006a huang et al 2006b 9 g a i b i x 1 1 e a x b according to huang et al 2006b eq 8 can approximate n training set samples with zero error as follows huang et al 2006b 10 t 1 n o t y t 0 this equation illustrates the point that network parameters a b β can be obtained analytically for a given set of training records in this context β can be estimated directly from the n input output records as a linear system of equations denoted as huang et al 2006b g β y β β 1 β h h 1 a n d y y 1 y n n 1 11 where g is the hidden layer output matrix in order to compute the output weights of the elm network the hidden layer matrix is inverted using the moore penrose generalized inverse function huang et al 2006b 12 β g y where g is the inverted hidden layer output matrix represents the moore penrose generalized inverse function and β is the estimated output weights from n data records in the end the predicted values can be generated by a new input vector testing set akusok et al 2015 with all variables having been identified earlier 13 z i 1 h β i g a i x new b i where z is the model output and x new is the new set of model inputs 3 4 least squares support vector machine originating from support vector machines svm and first proposed by suykens and vandewalle 1999 a useful aspect of the least squares support vector machine lssvm is its ability to solve problems of non linear classification and function estimation kumar and kar 2009 the lssvm formulation computes a linear system in dual space under a least squares cost function whereas svm uses a quadratic optimization problem approach the lssvm has been employed in a variety of areas e g pattern recognition signal processing and non linear regression estimation due to its more limited computation time compared to conventional models such as back propagation neural networks bpnn and partial least squares regression pls kisi and parmar 2016 fig 3 illustrates the basic structure of an lssvm model a dataset comprised of x t as the predictor s y t as the predictand and w as the d dimentional weight vectors where x t r d and y t r is considered the lssvm non linear function is written as suykens and vandewalle 1999 suykens and vandewalle 2000 cao et al 2008 vapnik 2013 14 f x w t x b where b is the bias term and is the mapping function that maps x into a d dimensional feature vector t is the transposed vector the regression problem can be expressed according to the structural minimization principle considering the complexity of function and fitting error as 15 m i n j w e 1 2 w t w γ 2 t 1 d e t 2 that has the following constraints 16 y t w t x t b e t t 1 2 d e t is the slack variable for x t and γ is the margin parameter introducing the lagrange multipliers α t and changing the constrained problem into an unconstrained one the objective function can be obtained in order to solve the optimization problems in eq 15 as 17 l w b e α j w e t 1 d t w t x t b e t y t by taking the partial derivatives of eq 17 the optimal condition can be obtained according to karush kuhn tucker kkt as follows cao et al 2008 suykens and vandewalle 1999 suykens and vandewalle 2000 vapnik 2013 18 w t 1 d t x t t 1 d t t γ e t w t x t b e t y t 0 thus the linear equations are generated as 19 0 y t y zz t i γ b 0 1 where y y 1 y d z x 1 t y t x d t y d and 1 i by defining the kernel function k x x t x t x i i 1 d the lssvm regression for new points becomes 20 f x t 1 d t k x new x i b many kernel functions such as linear polynomial radial basis and sigmoidal have been proposed for lssvm this study adopts the radial basis function rbf kernel since it is the most commonly used function in regression problems kisi 2012 the rbf kernel is denoted as cao et al 2008 21 k x x t exp x x t 2 2 σ 2 σ 2 is the bandwidth of the radial basis kernel function 3 5 bootstrap technique for each set of bootstrap samples t s elm and lssvm models were developed and trained the model outputs were then evaluated using the set of a s observation pairs t t x t y t that were not a part of t s the generalization error for the models then represented the performances of the models with the validation sets which were subsequently averaged the generalization error e 0 can then be estimated as tiwari and chatterjee 2010a 22 e 0 s 1 s i a s y i f model x t w s t s 2 s s a s where f model x t w s t s is the output of the model elm and lssvm developed from bootstrap sample t s in which w s is the weight vector x t is a particular input vector and a s is the number of observation pair indices in a s the bootstrap model estimate y x for the developed models is given by the mean of the s bootstrap estimates jia and culver 2006 wan et al 2014 23 y x 1 s s 1 s f model x w s and the variance is computed as 24 σ 2 x 1 s 1 s 1 s f model x w s y x 2 4 methods 4 1 model development the river ice thickness data spanned a period of 35 years 1981 2016 for all the applied models the dataset for both the mackenzie river and yellowknife river stations was split into training 70 validation 15 and testing 15 subsets it is important to note that there is no set rule for data division for a predictive dataset and it varies with the problem of interest deo et al 2016 however some researchers e g palani et al 2008 barzegar et al 2016 believe that the validation subset should include data never used in the training sets and that these data should constitute approximately 10 40 of the size of the training set table 1 shows the univariate statistics for the variables used in this study for both stations snow depth water flow and mean air temperature were selected as the input variables of the models since these variables provided the best models as identified through trial and error to prevent variables with large numerical ranges dominating those with smaller numerical ranges in the models and also to speed up the model training process and convergence during training the input variables were scaled prior to model calibration validation and test data were scaled using the maximum and minimum values of the different explanatory variables from the calibration data the equation used for scaling the input variables is as follows hastie et al 2002 25 x norm x i x min x max x min where x i is the original value of input variable that is to be normalized x min is the minimum value of variable x x max is the maximum value of variable x and x norm is the normalized value between 0 and 1 of the input variable in this study a back propagation levenberg marquardt lm algorithm trainlm was used for training the three layered ann model in function approximation problems when training moderate sized networks with less than a few hundred weights the lm algorithm has been shown to be fast and accurate generating lower mean square errors than any other training algorithms tested demuth and beale 2009 the ann models were developped using three inputs in the first layer i e based on the three input variables noted in table 1 and one output in the output layer i e river ice thickness the number of hidden neurons in the hidden layer was obtained through careful trial and error as discussed below the tangent sigmoid and purelin activation functions were used for the hidden and output layers respectively for the ann models for the anns a learning rate and momentum factor were set at 0 01 and 0 1 respectively the optimum number of hidden neurons based on the lowest mean square error mse in the validation phase was selected for both the mackenzie river and yellowknife river station datasets the optimum number of hidden neurons in the ann models for the regem and ktrl imputation techniques datasets was 6 the maximum number of training epochs for anns was set to 100 since this was sufficient to train the anns to convergence since traditional ann models e g ffbp ann are based on iterative training solutions a careful selection of momentum and learning rate factors is required to reduce over fitting the elm differs from the ann by choosing the input weights and hidden neuron biases at random and afterward estimating the output weights through a linear least squares solution thus there is no need to use iterative training approaches i e gradient descent based algorithms such as back propagation or global search methods e g particle swarm optimization genetic algorithms etc in the elm model tavares et al 2014 random initialization of hidden layer parameters i e weights and biases is important for an elm model huang et al 2006b therefore the elm models were executed 1000 times and their estimates averaged in order to reduce the effect of randomization on identifying the optimal number of hidden neurons the rmse was calculated for each hidden neuron number to identify the optimal number of hidden neurons a range of hidden neurons from 1 to 150 in increments of one was set for the elm models and the optimal model structure was chosen based on the lowest rmse on the validation phase the optimum number of hidden neurons for regem and ktrl techniques for elm modeling of both stations was found to be 6 the optimal random initialization for the regem and ktrl techniques for both stations for elm modeling was 1000 for bootstrap elm belm in addition to the number of hidden neurons the bootstrap ensemble size must be determined ensemble sizes from 10 to 100 in increments of 10 were tested for the bootstrapped based models the optimum ensemble size for all the developed belm models was 50 except for the regem technique at the mackenzie river station which was 10 for both stations the optimum number of hidden neurons for the regem and ktrl techniques for belm modeling was found to be 6 except for the regem technique at the yellowknife river station which was found to be 4 similarly 1000 randomized trials of the elm weights and biases resulted in the best performance for both the regem and ktrl techniques for belm modeling at both stations both the elm and belm models adopted the sigmoid activation function for the hidden layer designing the lssvm requires the selection of two parameters γ and σ 2 in which γ is a regulization constant and σ 2 is the bandwidth of the radial basis kernel function for all the lssvm based models the rbf was used as the selected kernel function which required a lower number of hyper parameters that influenced the complexity of the model barzegar et al 2018b for both standalone and bootstrapped lssvm models a grid search approach with a 10 fold cross validation process was applied to determine the optimal parameter values and to prevent overfitting of the final lssvm based models for the mackenzie river station γ 3 24 and σ 2 2 44 for the regem technique dataset and γ 3 19 and σ 2 2 36 for the ktrl technique dataset for lssvm modeling were obtained in contrast γ 1 46 and σ 2 9 92 for the regem technique dataset and γ 3 00 and σ 2 19 19 for the ktrl technique dataset for lssvm modeling were obtained at the yellowknife station for the bootstrap lssvm blssvm the optimal number of bootstraps was 10 for both regem and ktrl technique datasets the grid search for blssvm models at the mackenzie river station resulted in values of γ 9 19 and σ 2 3 65 for the regem technique dataset and γ 15 99 and σ 2 4 97 for the ktrl technique dataset for the yellowknife river station optimum values of γ 1 86 and σ 2 1 31 for the regem technique dataset and γ 1 30 and σ 2 3 73 for the ktrl technique dataset were obtained all the predictive models algorithms were developed in the matlab programming environment 4 2 performance assessment in order to assess the proposed models several quantitative performance indicators were employed a combination of different metrics is often required for an extensive evaluation of a model s performance chai and draxler 2014 the correlation coefficient r root mean square error rmse mean absolute error mae their normalized equivalents expressed as a percentage root mean square percentage error rmspe and mean absolute percentage error rmpe bias error bias and willmott s index wi were considered in assessing the accuracy of river ice thickness rit estimation these are defined as follows 26 r t 1 t n r i t t obs rit obs r i t t est rit est t 1 t n r i t t obs rit obs 2 t 1 t n r i t t est rit est 2 1 0 r 1 0 27 rmse 1 n t 1 t n rit t obs rit t est 2 28 rmspe 100 rmse rit obs 29 mae 1 n t 1 t n rit t obs rit t est 30 mape 100 mae rit obs 31 bias 1 n t 1 t n rit t obs rit t est 32 wi 1 i 1 i n rit i obs rit i est 2 i 1 i n rit i est rit obs rit i obs rit est 2 where n is the sample size rit i obs is the i th observed rit value rit obs is the mean observed rit value rit i est is the ith estimated rit value and rit est is the mean estimated rit value in general the correlation coefficient r represents the strength of the linear regression between observed and estimated river ice thickness and compares them directly however for the observed vs estimated linear relationship an ideal r 1 0 can occur even if the slope and ordinate intercept differ from 1 0 and zero respectively so scatter plots and other metrics must be used the estimation accuracy of the models via goodness of fit was assessed by the rmse and mae indicators the mae gives the same weight to all errors whilst the rmse squares errors thereby giving greater weights to errors with larger absolute values the smaller the mae and rmse the better the estimation accuracy the bias represents the mean error which shows whether the model systematically under or overestimates the observed values all the above mentioned indicators showed the linear agreement between observed and estimated values in the modeling also they can be overly sensitive to extreme values outliers in the observed data and somewhat insensitive to the additive or proportional differences between estimations and observations willmott 1981 to overcome these limitations wi was also considered 5 results and discussion the goals of this study were to investigate the feasibility of using elm and lssvm and their bootstrap enhanced equivalents for river ice thickness estimation in comparison to an ann model benchmark also the performance of applied imputation techniques were compared by various performance metrics which were computed to assess model accuracy a direct comparison of the developed models for the mackenzie river and yellowknife river stations is presented in tables 2 and 3 respectively for each imputation method i e ktrl and regem based on tables 2 and 3 the ann model performed acceptably in the training and validation phases for both stations the performance of ann models in the testing phase for the regem and ktrl imputation datasets showed that this model had the worst performance among the other algorithms however the performance of ann models at the yellowknife river station for both the regem r 0 703 rmse 21 247 cm rmspe 26 783 mae 17 905 cm mape 32 181 bias 1 049 cm and wi 0 685 and ktrl r 0 609 rmse 24 298 cm rmspe 30 629 mae 19 760 cm mape 38 869 bias 1 090 cm and wi 0 536 datasets were better than at the mackenzie river station for regem r 0 559 rmse 26 961 cm rmspe 45 099 mae 21 662 cm mape 46 620 bias 1 221 cm and wi 0 708 and ktrl r 0 560 rmse 26 502 cm rmspe 44 283 mae 21 902 cm mape 46 527 bias 1 224 cm and wi 0 643 datasets in comparison to the standalone ann and lssvm models the performance of the elm models were far better with regem r 0 789 rmse 19 686 cm rmspe 31 338 mae 16 079 cm mape 34 970 bias 1 172 cm and wi 0 763 and with ktrl r 0 792 rmse 19 569 cm rmspe 31 150 mae 16 015 cm mape 34 860 bias 1 171 cm and wi 0 765 for the mackenzie river station and with regem r 0 865 rmse 16 001 cm rmspe 20 170 mae 12 242 cm mape 21 570 bias 1 011 cm and wi 0 822 and ktrl r 0 841 rmse 16 811 cm rmspe 21 191 mae 12 658 cm mape 24 011 bias 1 035 cm and wi 0 819 for the yellowknife river station during the testing phase although the performance of lssvm was better than ann the lssvm and blssvm models performed worse in comparison to both elm and belm models during the training validation and testing phases for both stations however the bootstrap blssvm models outperformed the standalone lssvm it was observed that in comparison to the standalone lssvm model the bootstrap counterpart blssvm model using the regem and ktrl datasets at the mackenzie river station decreased the rmspe 17 42 and 7 77 respectively for the regem and ktrl datasets at the yellowknife river station reductions were 2 76 and 2 09 respectively the elm and belm model performances with ktrl and regem behaved similarly for the mackenzie river station 1 67 to 0 97 difference in rmspe for the regem and ktrl datasets respectively for the yellowknife river station 0 9 to 1 36 difference in rmspe for the regem and ktrl datasets respectively during the testing phase although the bootstrap based elm models improved the performance of the elm models slightly they also reduced uncertainty and provided more accurate results the bootstrap method was more effective in improving the performance of the lssvm model compared with the elm model but the elm based models performed much better than the lssvm based models considering model performance based on the evaluation metrics for the testing phase the belm outperformed the elm lssvm and blssvm in river ice thickness estimation indicating that the belm model had a slightly better ability to estimate river ice thickness for out of sample records the better performance of the belm model over the lssvm and blssvm can be attributed to the fact that for this study i the parameterization of belm better fit the given dataset and ii the trained network in belm was more robust to a training dataset that contained input patterns drastically different from the bulk of the input records comparing the ktrl and regem imputation techniques the performance of the regem imputation was better than the ktrl for the mackenzie river station whereas the ktrl method outperformed the regem at the yellowknife river station therefore it can be concluded that different imputation methods may have different performances in the modeling process time series graphs provided a closer examination of model performance with respect to their ability to capture minimum and maximum values of river ice thickness it is important to note that the time series graphs were plotted so that they omitted any gaps between observations to improve presentation due to the sparsely sampled ice thickness measurements taken each winter season for the mackenzie river station and all models developed using regem and ktrl techniques comparative plots of observed vs estimated rit over time and scatter plots of estimated vs observed rit were developed figs 4 and 5 rit est values for the mackenzie river station obtained using the regem technique based belm model were much closer to the rit obs values than for other models with the highest r 0 826 and a closely fitted linear regression equation ritest 0 3682 rit est 40 823 it was also observed that the belm model captured the maximum values of rit better than the other developed models the elm model better estimated peak values with the regem imputation technique compared to the ktrl technique also although the performance of elm in catching the peak value was poor with both imputation techniques the belm with the regem technique generated better results lssvm and blssvm generated similar results in capturing the maximum and minimum value with both imputation techniques comparative and scatter plots of observed vs estimated rit over time are shown in figs 6 and 7 for regem and ktrl datasets at the yellowknife river station respectively rit est values for the yellowknife river station obtained using the ktrl technique and the belm model were much closer to the rit obs values than for other models the belm model obtained the highest r 0 866 and was well fitted by the linear regression equation ritest 0 6247 rit est 27 055 this elm based model clearly demonstrated its ability to capture the majority of minimum and maximum values of river ice thickness 6 conclusions and recommendations the primary objective of this study was to demonstrate the use of machine learning algorithms extreme learning machine elm and least squares support vector machine lssvm and their bootstrap versions belm and blssvm for the estimation of river ice thickness at the mackenzie river and yellowknife river stations in the northwest territories canada the performance of the applied models was compared to that of an artificial neural network ann the secondary objective was to evaluate the performance accuracy of two imputation methods ktrl and regem implemented in the machine learning models when applied to the same scenario using 35 years of data the input predictor variables for estimating river ice thickness cm were snow depth cm water flow m3 s and mean air temperature c the datasets were divided into 70 training 15 validation and 15 testing subsets while the ann model was not sufficiently accurate in estimating river ice thickness in this study area the newer form of machine learning techniques elm lssvm and their bootstrap versions provided acceptable performances the model testing phase indicated that the belm model could potentially improve the estimation accuracy of the modeling process compared to the elm lssvm and blssvm models at both stations results showed that the regem and ktrl imputation techniques were more efficient for modeling the ice thickness at the mackenzie river and yellowknife river stations respectively according to the model evaluation metrics rmspe and mape decreased by about 0 07 3 8 and 1 1 5 respectively for the mackenzie river station when the belm model with regem imputation technique was used in comparison with the elm lssvm and blssvm models moreover for the belm model employing the ktrl imputation technique for the yellowknife river station rmspe and mape were reduced by about 1 7 6 2 and 4 9 12 3 respectively compared to the other applied models the results showed that the elm model with the regem imputation technique estimated peak values with better accuracy than with the ktrl imputation technique overall it can be concluded that the machine learning models developed in this study e g elm and lssvm based models and their bootstrapped versions are promising techniques for the estimation of river ice thickness a study conducted by zaier et al 2010 that estimated ice thickness in a number of selected canadian lakes found that ann ensemble models were more accurate than single ann models especially when boosting was used to combine ensemble members and when stacking was used to combine the outputs from individual members the current study also demonstrated the capability of the bootstrapped based machine learning models to estimate river ice thickness the authors of this study recommend that further investigations be conducted by applying the multi model ensemble based approach to integrate the merit of each standalone model coupled with imputation and bootstrap techniques to improve the generalization performance of the models declaration of competing interest none acknowledgements the authors would like to thank the water survey of canada for the data used in this study this study was funded by an nserc discovery grant held by jan adamowski 
6201,river ice plays an important role in biophysical and socio economic systems in northern regions and is related to climate variability at both regional and global scales the objective of this study was to estimate river ice thickness using several new machine learning methods extreme learning machine elm least squares support vector machine lssvm and their bootstrap versions belm and blssvm respectively to explore the usefulness of these methods we chose two stations mackenzie river and yellowknife river in the mackenzie river basin in the northwest territories canada the variables used to develop the river ice thickness cm estimation models included water flow m3 s snow depth cm and mean air temperature c with a time period of 1981 2016 two techniques namely the kendall theil robust line ktrl and the regularized expectation maximization regem methods were utilized to fill in the missing values in the time series records the performance of the developed models was measured using several statistical indicators correlation coefficient r root mean square error rmse mean absolute error mae their normalized equivalents expressed as a percentage root mean square percentage error rmspe and mean absolute percentage error rmpe bias error bias and willmott s index wi results indicated that the bootstrap elm model outperformed the elm lssvm and blssvm models in the testing phases based on a number of statistical measures for the belm using regem at the mackenzie river station the results were r 0 826 rmse 19 756 cm rmspe 33 011 mae 15 364 cm mape 35 988 bias 1 199 cm and wi 0 818 for the belm using ktrl at the yellowknife river station the results were r 0 856 rmse 15 444 cm rmspe 19 468 mae 12 006 cm mape 19 045 bias 0 966 cm and wi 0 868 the findings of this study indicate that the belm machine learning approach using meteorological variables is a promising new tool for river ice thickness estimation keywords bootstrap extreme learning machine least squares support vector machine ice thickness 1 introduction ice formation on lakes and rivers has an important role in shaping water resources management and engineering in cold regions as noted in earlier studies brown and duguay 2010 prowse et al 2011 arp et al 2015 ice formations can dominate the annual cycle of lakes and rivers this can isolate water bodies not only from moisture exchanges but from gas fluxes with the atmosphere for 60 80 of the year as well thus the presence of freshwater ice may result in ice jams ultimately leading to widespread spring flooding that often accounts for the largest hydrologic event of the year in northern latitudes prowse and carter 2002 moreover it can result in reduced hydroelectric power generation and navigation and transportation disruption as well as damage to infrastructure the environment and ecosystems shen and liu 2003 duguay et al 2015 however investigating different aspects of ice conditions requires extensive data on break up date freeze up date and ice thickness among other parameters moreover in northern regions such as canada the collection of data on freshwater ice is often challenging due to the site specific nature of such processes chu and lindenschmidt 2016 thus different forecasting and simulation models are required ice thickness is particularly useful in river ice hydraulic models chokmani et al 2007 flood routing models she and hicks 2006 hydrodynamic modeling of freezing rivers shen and liu 2003 and flow control in regulated rivers tuthill 1999 given the lack or incompleteness of ice thickness measurements different analytical mathematical and numerical models have been used to address this inadequacy of data while all numerical models developed for ice thickness simulation consider energy budget components they differ from each other by the level of detail used to compute the ice energy budget hicks et al 1997 andres and van der vinne 2001 ma and fukushima 2002 the one dimensional numerical model for river ice rice processes can simulate the growth and decay of the ice cover along with other associated parameters with a close agreement between observed and simulated data shen et al 1991 the updated river ice network ricen model provides substantial improvements in ice process simulation including consideration of the effects of wind artificial icebreaking by icebreakers and flow resistance due to moving ice shen et al 1995 however these kinds of models require extensive input data e g wind speed vapor pressure cloud cover which are not always accessible in addition indices that are needed for estimating energy fluxes due to shortwave radiation evaporation and convection at the air ice or snow interface cannot be accurately determined as of yet shen and yapa 1985 consequently simplified versions of energy budget models can be developed using stefan s law sl or the revised stefan s law rsl detailed descriptions of these equations can be found in the theoretical background section of this paper the date of onset of ice cover is a primary parameter in stefan s law equation as it determines the date of onset of the accumulation of freezing degree days for any given winter since in any one season this date is unknown for most sites stefan s law cannot be used however rsl based on the accumulation of freezing degree days from the first day of below freezing air temperatures can work reasonably well for ice thicknesses exceeding 0 10 m for ice thicknesses less than 0 10 m rsl overestimates ice thickness ashton 1989 based on a theoretical analysis shen and yapa 1985 introduced a modified degree day method capable of continuously simulating the variation of the thickness of river ice cover from formation to break up in their study the method s application to the upper st lawrence river in canada showed that simulated ice thicknesses compared well with field observations although an empirical coefficient was still required to judge the ice thickness growth rates at different types of river sites geographic information system gis and remote sensing techniques offer alternative means for monitoring the processes and patterns of ice cover there are different remote sensing systems that can be used to map ice cover extent and ice phenology chu and lindenschmidt 2016 optical sensors with low to medium spatial resolution and high temporal resolution such as the advanced very high resolution radiometer avhrr 1 km to 4 km spatial resolution and moderate resolution imaging spectrometer modis 250 m to 1 km spatial resolution are two types of ice information collection methods used for large freshwater ice surfaces even though high temporal resolution data such as modis and avhrr can provide timely information about river ice conditions the low spatial resolution and dependence on atmospheric conditions to gather these data can pose significant challenges in monitoring the details of river ice cover conditions microwave remote sensing e g synthetic aperture radar sar has proven to be another useful tool in monitoring and studying freshwater ice processes given its higher spatial resolution and capacity to acquire images under all weather and atmospheric conditions e g clouds as well as its sensitivity to water ice surfaces ice structure and ice thickness nghiem and leshkevich 2007 unterschultz et al 2009 duguay et al 2015 numerous studies have used sar imagery to monitor river ice thickness and reported promising simulation results unterschultz et al 2009 lindenschmidt et al 2010 mermoz et al 2014 however the main drawback of the numerical physically based models is their complexity and the fact that they require large amounts of precise data in order to estimate the physical properties of the problem domain furthermore a careful selection of model parameters sometimes based on field measurements is required to effectively calibrate the model yoon et al 2011 additionally many assumptions regarding the governing equations barzegar et al 2017 as well as some degree of user experience and expertise with the model are required to generate realistic simulations through numerical models inam et al 2017 data driven models require suitable data and their application also requires some degree of experience and expertise their forecasts however are based on statistical relationships between model inputs rather than physical laws or conceptualizations this affords data driven models some degree of flexibility over numerical models as model inputs can only comprise historical records of the process to be simulated many numerical models however may require particular inputs that are unavailable or costly to collect motivated by the limitations of numerical models as mentioned above researchers have sought to develop statistical data driven models with an acceptable level of accuracy and simple model development and application procedures such as the machine learning methods used in this study to estimate river ice thickness chokmani et al 2007 zaier et al 2010 for example parameters that influence ice thickness have been identified through single variable regression and factor analysis and the results combined in a multivariable linear regression analysis to address the complexity of ice growth and decay williams et al 2004 in their study accumulated freezing degree days and mean air temperature over a fixed period provided the best forecast of maximum ice thickness dornan 2005 developed a multiple linear regression model to determine the best model for river ice thickness by considering the effect of climate variables such as snow cover snow density and solar radiation cumulated solar radiation was not found to be a useful variable in the ice thickness models given its correlation with the most relevant model variable cumulative freezing degree days it is worth noting that by coupling data driven models with physically based models new approaches have been developed and used for the successful modeling of different hydrological phenomena e g runoff modeling modeling flows within wastewater pipe networks rainfall runoff modeling this offers an interesting method of coupling the strengths of both model types as a way of improving process understanding and prediction accuracy ayzel and izhitskiy 2018 young et al 2017 babovic 2009 yu et al 2004 vojinovic et al 2003 babovic et al 2001 another useful tool for river and lake ice thickness modeling is the artificial neural network ann in a comparison of the accuracy of lake ice thickness simulation with an ann model multiple linear regression mlr model revised stefan s law rsl model and the canadian lake ice model climo all relying on data for daily mean air temperature daily rainfall daily total solar radiation and daily snow depth seidou et al 2006 reported that the ann model followed data variations more closely than the rsl chokmani et al 2007 assessed ann based river ice thickness estimation methods drawing upon cumulative climate input variables such as freezing degree day cfdd solar radiation csr and snow cums finding that for sites in alberta these models provided good estimates with 90 mm rmse 130 mm however as the models failed to estimate low and high values correctly they were not considered ideal zaier et al 2010 tested six ann models five using ensemble modeling techniques e g randomization bagging and boosting for creating ensemble members and averaging and stacking for combining ensemble members and a single ann model for estimating ice thickness on 17 canadian lakes and demonstrated the superior performance of ensemble ann methods compared to the single ann model besides ann and mlr support vector machine svm and fuzzy logic have been successfully used in the estimation of other ice characteristics e g ice affected stream flow chokmani et al 2008 ice jam occurrence massie et al 2002 water level and ice jam thickness wang et al 2010 and date of ice break up and freeze up mahabir et al 2005 shouyu and honglan 2005 tao et al 2008 zhou et al 2009 zhao et al 2012 sun and trevor 2017 2018a 2018b despite the extensive application of ann in modeling hydrological processes and freshwater ice forecasts still suffer from deficiencies such as over fitting slow learning speed and local minima regression models that suffer from over fitting generally perform very well with training data sometimes forecasting the target with zero error however they tend to perform rather poorly with testing data this is because the resulting model parameters have been calibrated almost exactly for the training data but do not properly generalize examples outside of the training set the low speed of the training process is usually related to models that require iterative training approaches such as backpropagation and is most problematic in very large networks with a large amount of potentially redundant data furthermore the local minima issue is mostly related to iterative training approaches e g backpropagation where the model error parameter surface gets trapped in a localized area and where further iterations of model training cannot escape toward the global minimum of the objective function hamid et al 2012 kirar 2015 to overcome these limitations relatively new machine learning models e g extreme learning machine elm and least squares support vector machine lssvm have been developed the faster learning algorithms and improved generalization performance of the elm models have led to their extensive use in different environmental modeling processes e g drought deo and şahin 2015 water quality barzegar et al 2018b fijani et al 2019 groundwater level barzegar et al 2017 groundwater vulnerability barzegar et al 2018a water demand quilty and adamowski 2018 water flow yaseen et al 2016 atiquzzaman and kandasamy 2018 wave height ali and prasad 2019 and soil temperature feng et al 2019 in addition advanced design features of elm models such as analytical output determination by a least squares problem and random generation of the parameters of hidden nodes without the need for tuning the algorithm renders elm an interesting and new alternative to traditional forms of machine learning methods yaseen et al 2016 likewise lssvm models a simplified and more computationally efficient version of svm that uses kernel methods have the capacity to estimate output parameters more precisely than traditional techniques such as arima ann anfis or neuro fuzzy systems hong and pai 2006 wang et al 2009 in addition to the practice of newer forms of machine learning techniques researchers have started to investigate particular procedures such as bootstrapping in their modeling processes to improve model accuracy and or reliability belayneh et al 2016 jia and culver 2006 srivastav et al 2007 the bootstrap efron 1979 which uses resampling with replacement for a given input output dataset has been employed in hydrological applications to reduce uncertainty in model forecasts tiwari and chatterjee 2010b this method has been successfully applied to hydrological modeling given its ability to reduce uncertainty and provide accurate results especially in circumstances where relatively small training data sets are available erdal and karakurt 2013 to the best of our knowledge no studies have explored the potential of elm and lssvm coupled with the bootstrap technique for river ice thickness estimation this study is also the first attempt to model river ice thickness in the mackenzie river basin in the northwest territories canada performances of elm and lssvm models were compared with ann a widely used machine learning method to evaluate their relative estimation accuracy several quantitative performance indicators were employed for model comparison in addition the inherent accuracy of two imputation methods kendall theil robust line ktrl and the regularized expectation maximization regem were evaluated the imputation of missing values was considered in this study since historical river ice thickness records are sparse and ignoring observations with missing variables would result in a significant loss of information 2 study area the mackenzie system drains an area of about 1 805 200 km2 snowfall is the major form of precipitation and there is snow cover on the ground for more than six months a year over a substantial part of the basin aziz and burn 2006 the mackenzie river is the largest north flowing river in north america its basin is the largest in canada and is exceeded on the continent only by the mississippi missouri system the mackenzie river flows from great slave lake to the arctic and is entirely within the northwest territories the yellowknife river another important river in the mackenzie river basin flows south and empties into yellowknife bay part of great slave lake at the city of yellowknife due to various impacts such as frequent ice jams this area was chosen to test the feasibility of machine learning techniques in the estimation of ice thickness in the present study direct measurements of ice thickness cm and snow depth cm were collected from the environment canada winter gauges on the mackenzie river and yellowknife river meteorological stations formerly the water survey of canada collected this data river water flow m3 s was obtained from the mackenzie river at inuvik 10lc002 and the yellowknife river at the outlet of prosperous lake 07sb002 hydrometric stations the location of each station is shown in fig 1 the mean monthly air temperature c data for the stations were extracted from the nasa website https power larc nasa gov the experimental data was checked for quality by the various authorities that provided the data e g environment canada and nasa measurement of ice thickness was limited to one to seven measurements per season water flow data were recorded in digital form using a water flow sensor connected to an automated electronic recorder in total 166 and 156 available records of ice thickness snow depth and water flow for the mackenzie river and yellowknife river stations respectively were collected from 1981 to 2016 these records along with the mean monthly air temperature were used to develop test and evaluate the different models e g bootstrap elm bootstrap lssvm elm lssvm and ann an important step in machine learning modeling is the selection of appropriate input variables irrelevant and or redundant inputs can significantly influence model accuracy or add unnecessary complexity which impacts model reliability hejazi and cai 2009 quilty et al 2016 careful trial and error was applied to find the best set of input variables among the available data for river ice thickness estimation and three predictors namely water flow snow depth and mean air temperature were selected as inputs for the developed models for estimating the river ice thickness in the study area 3 theoretical background 3 1 stefan s law and revised stefan s law stefan s law or the degree day method is the most widely used formula for estimating ice thickness which is derived by simplifying the energy balance equations lock 1990 1 h a 0 d d where a 0 is an empirical constant that varies with the site d d is the sum of the degree days below the freezing point and h is the ice thickness in practice a 0 is used as an adjustable parameter with a value that is lower than the theoretical value to account for varying conditions of exposure and insulation michel 1971 gives a range of values adapted for a variety of lakes and rivers as explained in the introduction the date of onset of ice cover a basic parameter in stefan s equation is not known for the majority of sites the revised stefan s law rsl is a substitute equation which is based on d g the accumulation of freezing degree days starting with the first day of below freezing air temperatures in any given season the rsl has one more adjustable parameter c the effective number of degree days to be subtracted from d g in order to obtain d d seidou et al 2006 2 h a 0 d g c if d g c 0 if d g c since the occurrence of the first day of the freezing daily mean air temperature usually arrives several days or weeks before the date of ice onset c will normally take a positive value seidou et al 2006 3 2 imputation of missing values missing or incomplete datasets are a challenge to hydrologists attempting to model processes since machine learning models rely solely on data to learn the underlying input output relationships imputation techniques are used for missing values in the available dataset the total number of missing values for the mackenzie river and yellowknife river stations for ice thickness measurement dates were one and two respectively there were also some missing values for the snow depth sixteen and three for yellowknife river and mackenzie river stations respectively parameter the imputation techniques used in this study were the kendall theil robust line ktrl and the regularized expectation maximization regem these two methods for the imputation of scattered missing values substitute missing records differently the ktrl employs the correlation between one of the complete variables and the variable with missing values whereas regem considers the covariance between all variables to impute missing values in the dataset as a result both these methods were considered for substitution of missing records in the dataset of this study the ktrl was initially described by theil 1950 and is based on the kendall rank correlation coefficient τ theil 1992 the ktrl robust slope estimator or sen s slope is computed by comparing each pair of records to all others in a pair wise manner considering a dataset of x y of size n will result in n n 1 2 pair wise comparisons for each set of comparisons a slope δ y δ x is computed the slope estimate b k is the median of all slopes computed theil 1992 3 b k m e d i a n y j y t x j x t t j t 1 2 n 1 a n d j 2 3 n the ktrl intercept a k which is a function of its slope and the median values is defined as follows 4 a k m e d i a n y c b k m e d i a n x c the ktrl technique can be considered as an analogue to ordinary least squares ols with the main difference being that the ktrl regression line passes through the point representing the median values of the response and predictor median x median y whereas for ols it represents the mean values mean x mean y helsel and hirsch 2002 based on the ktrl method s robustness in the presence of outliers this method for substitution of scattered missing values is highly recommended khalil and adamowski 2014 khalil et al 2012 and was therefore implemented for the present study s dataset regem is based on an iterative analysis of the linear regression of missing and available values along with estimated regression coefficients obtained by ridge regression a regularized regression method in which the filtering of the noise in the dataset is controlled by a continuous regularization parameter schneider 2001 the conventional em algorithm and the regem are only applicable to datasets with scattered missing values and share the advantage of being easy to program having a low cost per iteration and providing an economy of storage li et al 2005 the regem for a dataset of size n is formulated as schneider 2001 5 x m μ m x a μ a b e where e is the assumed residual with unknown covariance matrix b is a matrix of regression coefficients x a is the vector of available values x m is the vector of missing values μ a is the vector of means of the available values and μ m is the vector of means of the missing values the conditional maximum likelihood estimate of the regression coefficients can be written as schneider 2001 6 b aa 1 am where aa 1 is the estimated covariance matrix of the available values and am is the estimated cross covariance of the available and missing values the regularization of the regression model in ridge regression is achieved by adjusting the inverse matrix aa 1 in eq 6 to be 7 aa h 2 d 1 where d is the diagonal matrix consisting of the diagonal elements of the covariance matrix aa and h is a positive number termed the ridge parameter a detailed derivation and discussion of these equations can be found in schneider 2001 3 3 extreme learning machine the extreme learning machine elm was first proposed by huang et al 2006b in order to overcome the limitations and shortcomings of its counterparts e g ann and svm huang et al 2006a the novel single layer feed forward network slfn in elm operates differently than the traditional feed forward backpropagation ffbp ann this confers on elm a shorter modeling time which is the main advantage of elm algorithms barzegar et al 2017 this arises because the input weights and biases are randomized and the output weights have a unique least squares solution solved by a moore penrose generalized inverse function huang et al 2006a huang et al 2006b huang and xiang 2015 this process provides an advantage over iterative training techniques that tend to collapse to local rather than global minima deo and şahin 2015 accordingly three simple steps are employed in elm algorithms i random generation of hidden layer weights and biases ii generation of the hidden layer output matrix by the input variables that are passed through the hidden layer parameters and iii estimation of elm output weights by inverting the hidden layer output matrix as well as computation of its product with the response variable the number of hidden neuron nodes has typically been identified by trial and error processes using a validation dataset randomization of the hidden layer weights and biases by a continuous probability distribution such as the uniform distribution normal distribution or triangular distribution must be generated in order to determine the output weights via least squares deo et al 2017 fig 2 illustrates the basic structure of an elm model considering a dataset with x t as the d dimensional predictor variables and y t as the predictand comprised of n training data records where t 1 2 n and x t r d and y t r the slfn with l hidden neurons is mathematically expressed as huang et al 2006b 8 i 1 h β i g i a i x t b i z t where ai rd ai rd β r h are the output weights between the hidden layers with h nodes and the model output g i a b x is the hidden layer activation function and z z t r is the model output there are different activation functions adopted in elm models e g tangent sigmoid logarithmic sigmoid hard limit triangular basis and radial basis the logarithmic sigmoid is the most commonly used activation function in the field of hydrological forecasting adamowski et al 2012 kişi 2008 and is denoted as follows huang et al 2006a huang et al 2006b 9 g a i b i x 1 1 e a x b according to huang et al 2006b eq 8 can approximate n training set samples with zero error as follows huang et al 2006b 10 t 1 n o t y t 0 this equation illustrates the point that network parameters a b β can be obtained analytically for a given set of training records in this context β can be estimated directly from the n input output records as a linear system of equations denoted as huang et al 2006b g β y β β 1 β h h 1 a n d y y 1 y n n 1 11 where g is the hidden layer output matrix in order to compute the output weights of the elm network the hidden layer matrix is inverted using the moore penrose generalized inverse function huang et al 2006b 12 β g y where g is the inverted hidden layer output matrix represents the moore penrose generalized inverse function and β is the estimated output weights from n data records in the end the predicted values can be generated by a new input vector testing set akusok et al 2015 with all variables having been identified earlier 13 z i 1 h β i g a i x new b i where z is the model output and x new is the new set of model inputs 3 4 least squares support vector machine originating from support vector machines svm and first proposed by suykens and vandewalle 1999 a useful aspect of the least squares support vector machine lssvm is its ability to solve problems of non linear classification and function estimation kumar and kar 2009 the lssvm formulation computes a linear system in dual space under a least squares cost function whereas svm uses a quadratic optimization problem approach the lssvm has been employed in a variety of areas e g pattern recognition signal processing and non linear regression estimation due to its more limited computation time compared to conventional models such as back propagation neural networks bpnn and partial least squares regression pls kisi and parmar 2016 fig 3 illustrates the basic structure of an lssvm model a dataset comprised of x t as the predictor s y t as the predictand and w as the d dimentional weight vectors where x t r d and y t r is considered the lssvm non linear function is written as suykens and vandewalle 1999 suykens and vandewalle 2000 cao et al 2008 vapnik 2013 14 f x w t x b where b is the bias term and is the mapping function that maps x into a d dimensional feature vector t is the transposed vector the regression problem can be expressed according to the structural minimization principle considering the complexity of function and fitting error as 15 m i n j w e 1 2 w t w γ 2 t 1 d e t 2 that has the following constraints 16 y t w t x t b e t t 1 2 d e t is the slack variable for x t and γ is the margin parameter introducing the lagrange multipliers α t and changing the constrained problem into an unconstrained one the objective function can be obtained in order to solve the optimization problems in eq 15 as 17 l w b e α j w e t 1 d t w t x t b e t y t by taking the partial derivatives of eq 17 the optimal condition can be obtained according to karush kuhn tucker kkt as follows cao et al 2008 suykens and vandewalle 1999 suykens and vandewalle 2000 vapnik 2013 18 w t 1 d t x t t 1 d t t γ e t w t x t b e t y t 0 thus the linear equations are generated as 19 0 y t y zz t i γ b 0 1 where y y 1 y d z x 1 t y t x d t y d and 1 i by defining the kernel function k x x t x t x i i 1 d the lssvm regression for new points becomes 20 f x t 1 d t k x new x i b many kernel functions such as linear polynomial radial basis and sigmoidal have been proposed for lssvm this study adopts the radial basis function rbf kernel since it is the most commonly used function in regression problems kisi 2012 the rbf kernel is denoted as cao et al 2008 21 k x x t exp x x t 2 2 σ 2 σ 2 is the bandwidth of the radial basis kernel function 3 5 bootstrap technique for each set of bootstrap samples t s elm and lssvm models were developed and trained the model outputs were then evaluated using the set of a s observation pairs t t x t y t that were not a part of t s the generalization error for the models then represented the performances of the models with the validation sets which were subsequently averaged the generalization error e 0 can then be estimated as tiwari and chatterjee 2010a 22 e 0 s 1 s i a s y i f model x t w s t s 2 s s a s where f model x t w s t s is the output of the model elm and lssvm developed from bootstrap sample t s in which w s is the weight vector x t is a particular input vector and a s is the number of observation pair indices in a s the bootstrap model estimate y x for the developed models is given by the mean of the s bootstrap estimates jia and culver 2006 wan et al 2014 23 y x 1 s s 1 s f model x w s and the variance is computed as 24 σ 2 x 1 s 1 s 1 s f model x w s y x 2 4 methods 4 1 model development the river ice thickness data spanned a period of 35 years 1981 2016 for all the applied models the dataset for both the mackenzie river and yellowknife river stations was split into training 70 validation 15 and testing 15 subsets it is important to note that there is no set rule for data division for a predictive dataset and it varies with the problem of interest deo et al 2016 however some researchers e g palani et al 2008 barzegar et al 2016 believe that the validation subset should include data never used in the training sets and that these data should constitute approximately 10 40 of the size of the training set table 1 shows the univariate statistics for the variables used in this study for both stations snow depth water flow and mean air temperature were selected as the input variables of the models since these variables provided the best models as identified through trial and error to prevent variables with large numerical ranges dominating those with smaller numerical ranges in the models and also to speed up the model training process and convergence during training the input variables were scaled prior to model calibration validation and test data were scaled using the maximum and minimum values of the different explanatory variables from the calibration data the equation used for scaling the input variables is as follows hastie et al 2002 25 x norm x i x min x max x min where x i is the original value of input variable that is to be normalized x min is the minimum value of variable x x max is the maximum value of variable x and x norm is the normalized value between 0 and 1 of the input variable in this study a back propagation levenberg marquardt lm algorithm trainlm was used for training the three layered ann model in function approximation problems when training moderate sized networks with less than a few hundred weights the lm algorithm has been shown to be fast and accurate generating lower mean square errors than any other training algorithms tested demuth and beale 2009 the ann models were developped using three inputs in the first layer i e based on the three input variables noted in table 1 and one output in the output layer i e river ice thickness the number of hidden neurons in the hidden layer was obtained through careful trial and error as discussed below the tangent sigmoid and purelin activation functions were used for the hidden and output layers respectively for the ann models for the anns a learning rate and momentum factor were set at 0 01 and 0 1 respectively the optimum number of hidden neurons based on the lowest mean square error mse in the validation phase was selected for both the mackenzie river and yellowknife river station datasets the optimum number of hidden neurons in the ann models for the regem and ktrl imputation techniques datasets was 6 the maximum number of training epochs for anns was set to 100 since this was sufficient to train the anns to convergence since traditional ann models e g ffbp ann are based on iterative training solutions a careful selection of momentum and learning rate factors is required to reduce over fitting the elm differs from the ann by choosing the input weights and hidden neuron biases at random and afterward estimating the output weights through a linear least squares solution thus there is no need to use iterative training approaches i e gradient descent based algorithms such as back propagation or global search methods e g particle swarm optimization genetic algorithms etc in the elm model tavares et al 2014 random initialization of hidden layer parameters i e weights and biases is important for an elm model huang et al 2006b therefore the elm models were executed 1000 times and their estimates averaged in order to reduce the effect of randomization on identifying the optimal number of hidden neurons the rmse was calculated for each hidden neuron number to identify the optimal number of hidden neurons a range of hidden neurons from 1 to 150 in increments of one was set for the elm models and the optimal model structure was chosen based on the lowest rmse on the validation phase the optimum number of hidden neurons for regem and ktrl techniques for elm modeling of both stations was found to be 6 the optimal random initialization for the regem and ktrl techniques for both stations for elm modeling was 1000 for bootstrap elm belm in addition to the number of hidden neurons the bootstrap ensemble size must be determined ensemble sizes from 10 to 100 in increments of 10 were tested for the bootstrapped based models the optimum ensemble size for all the developed belm models was 50 except for the regem technique at the mackenzie river station which was 10 for both stations the optimum number of hidden neurons for the regem and ktrl techniques for belm modeling was found to be 6 except for the regem technique at the yellowknife river station which was found to be 4 similarly 1000 randomized trials of the elm weights and biases resulted in the best performance for both the regem and ktrl techniques for belm modeling at both stations both the elm and belm models adopted the sigmoid activation function for the hidden layer designing the lssvm requires the selection of two parameters γ and σ 2 in which γ is a regulization constant and σ 2 is the bandwidth of the radial basis kernel function for all the lssvm based models the rbf was used as the selected kernel function which required a lower number of hyper parameters that influenced the complexity of the model barzegar et al 2018b for both standalone and bootstrapped lssvm models a grid search approach with a 10 fold cross validation process was applied to determine the optimal parameter values and to prevent overfitting of the final lssvm based models for the mackenzie river station γ 3 24 and σ 2 2 44 for the regem technique dataset and γ 3 19 and σ 2 2 36 for the ktrl technique dataset for lssvm modeling were obtained in contrast γ 1 46 and σ 2 9 92 for the regem technique dataset and γ 3 00 and σ 2 19 19 for the ktrl technique dataset for lssvm modeling were obtained at the yellowknife station for the bootstrap lssvm blssvm the optimal number of bootstraps was 10 for both regem and ktrl technique datasets the grid search for blssvm models at the mackenzie river station resulted in values of γ 9 19 and σ 2 3 65 for the regem technique dataset and γ 15 99 and σ 2 4 97 for the ktrl technique dataset for the yellowknife river station optimum values of γ 1 86 and σ 2 1 31 for the regem technique dataset and γ 1 30 and σ 2 3 73 for the ktrl technique dataset were obtained all the predictive models algorithms were developed in the matlab programming environment 4 2 performance assessment in order to assess the proposed models several quantitative performance indicators were employed a combination of different metrics is often required for an extensive evaluation of a model s performance chai and draxler 2014 the correlation coefficient r root mean square error rmse mean absolute error mae their normalized equivalents expressed as a percentage root mean square percentage error rmspe and mean absolute percentage error rmpe bias error bias and willmott s index wi were considered in assessing the accuracy of river ice thickness rit estimation these are defined as follows 26 r t 1 t n r i t t obs rit obs r i t t est rit est t 1 t n r i t t obs rit obs 2 t 1 t n r i t t est rit est 2 1 0 r 1 0 27 rmse 1 n t 1 t n rit t obs rit t est 2 28 rmspe 100 rmse rit obs 29 mae 1 n t 1 t n rit t obs rit t est 30 mape 100 mae rit obs 31 bias 1 n t 1 t n rit t obs rit t est 32 wi 1 i 1 i n rit i obs rit i est 2 i 1 i n rit i est rit obs rit i obs rit est 2 where n is the sample size rit i obs is the i th observed rit value rit obs is the mean observed rit value rit i est is the ith estimated rit value and rit est is the mean estimated rit value in general the correlation coefficient r represents the strength of the linear regression between observed and estimated river ice thickness and compares them directly however for the observed vs estimated linear relationship an ideal r 1 0 can occur even if the slope and ordinate intercept differ from 1 0 and zero respectively so scatter plots and other metrics must be used the estimation accuracy of the models via goodness of fit was assessed by the rmse and mae indicators the mae gives the same weight to all errors whilst the rmse squares errors thereby giving greater weights to errors with larger absolute values the smaller the mae and rmse the better the estimation accuracy the bias represents the mean error which shows whether the model systematically under or overestimates the observed values all the above mentioned indicators showed the linear agreement between observed and estimated values in the modeling also they can be overly sensitive to extreme values outliers in the observed data and somewhat insensitive to the additive or proportional differences between estimations and observations willmott 1981 to overcome these limitations wi was also considered 5 results and discussion the goals of this study were to investigate the feasibility of using elm and lssvm and their bootstrap enhanced equivalents for river ice thickness estimation in comparison to an ann model benchmark also the performance of applied imputation techniques were compared by various performance metrics which were computed to assess model accuracy a direct comparison of the developed models for the mackenzie river and yellowknife river stations is presented in tables 2 and 3 respectively for each imputation method i e ktrl and regem based on tables 2 and 3 the ann model performed acceptably in the training and validation phases for both stations the performance of ann models in the testing phase for the regem and ktrl imputation datasets showed that this model had the worst performance among the other algorithms however the performance of ann models at the yellowknife river station for both the regem r 0 703 rmse 21 247 cm rmspe 26 783 mae 17 905 cm mape 32 181 bias 1 049 cm and wi 0 685 and ktrl r 0 609 rmse 24 298 cm rmspe 30 629 mae 19 760 cm mape 38 869 bias 1 090 cm and wi 0 536 datasets were better than at the mackenzie river station for regem r 0 559 rmse 26 961 cm rmspe 45 099 mae 21 662 cm mape 46 620 bias 1 221 cm and wi 0 708 and ktrl r 0 560 rmse 26 502 cm rmspe 44 283 mae 21 902 cm mape 46 527 bias 1 224 cm and wi 0 643 datasets in comparison to the standalone ann and lssvm models the performance of the elm models were far better with regem r 0 789 rmse 19 686 cm rmspe 31 338 mae 16 079 cm mape 34 970 bias 1 172 cm and wi 0 763 and with ktrl r 0 792 rmse 19 569 cm rmspe 31 150 mae 16 015 cm mape 34 860 bias 1 171 cm and wi 0 765 for the mackenzie river station and with regem r 0 865 rmse 16 001 cm rmspe 20 170 mae 12 242 cm mape 21 570 bias 1 011 cm and wi 0 822 and ktrl r 0 841 rmse 16 811 cm rmspe 21 191 mae 12 658 cm mape 24 011 bias 1 035 cm and wi 0 819 for the yellowknife river station during the testing phase although the performance of lssvm was better than ann the lssvm and blssvm models performed worse in comparison to both elm and belm models during the training validation and testing phases for both stations however the bootstrap blssvm models outperformed the standalone lssvm it was observed that in comparison to the standalone lssvm model the bootstrap counterpart blssvm model using the regem and ktrl datasets at the mackenzie river station decreased the rmspe 17 42 and 7 77 respectively for the regem and ktrl datasets at the yellowknife river station reductions were 2 76 and 2 09 respectively the elm and belm model performances with ktrl and regem behaved similarly for the mackenzie river station 1 67 to 0 97 difference in rmspe for the regem and ktrl datasets respectively for the yellowknife river station 0 9 to 1 36 difference in rmspe for the regem and ktrl datasets respectively during the testing phase although the bootstrap based elm models improved the performance of the elm models slightly they also reduced uncertainty and provided more accurate results the bootstrap method was more effective in improving the performance of the lssvm model compared with the elm model but the elm based models performed much better than the lssvm based models considering model performance based on the evaluation metrics for the testing phase the belm outperformed the elm lssvm and blssvm in river ice thickness estimation indicating that the belm model had a slightly better ability to estimate river ice thickness for out of sample records the better performance of the belm model over the lssvm and blssvm can be attributed to the fact that for this study i the parameterization of belm better fit the given dataset and ii the trained network in belm was more robust to a training dataset that contained input patterns drastically different from the bulk of the input records comparing the ktrl and regem imputation techniques the performance of the regem imputation was better than the ktrl for the mackenzie river station whereas the ktrl method outperformed the regem at the yellowknife river station therefore it can be concluded that different imputation methods may have different performances in the modeling process time series graphs provided a closer examination of model performance with respect to their ability to capture minimum and maximum values of river ice thickness it is important to note that the time series graphs were plotted so that they omitted any gaps between observations to improve presentation due to the sparsely sampled ice thickness measurements taken each winter season for the mackenzie river station and all models developed using regem and ktrl techniques comparative plots of observed vs estimated rit over time and scatter plots of estimated vs observed rit were developed figs 4 and 5 rit est values for the mackenzie river station obtained using the regem technique based belm model were much closer to the rit obs values than for other models with the highest r 0 826 and a closely fitted linear regression equation ritest 0 3682 rit est 40 823 it was also observed that the belm model captured the maximum values of rit better than the other developed models the elm model better estimated peak values with the regem imputation technique compared to the ktrl technique also although the performance of elm in catching the peak value was poor with both imputation techniques the belm with the regem technique generated better results lssvm and blssvm generated similar results in capturing the maximum and minimum value with both imputation techniques comparative and scatter plots of observed vs estimated rit over time are shown in figs 6 and 7 for regem and ktrl datasets at the yellowknife river station respectively rit est values for the yellowknife river station obtained using the ktrl technique and the belm model were much closer to the rit obs values than for other models the belm model obtained the highest r 0 866 and was well fitted by the linear regression equation ritest 0 6247 rit est 27 055 this elm based model clearly demonstrated its ability to capture the majority of minimum and maximum values of river ice thickness 6 conclusions and recommendations the primary objective of this study was to demonstrate the use of machine learning algorithms extreme learning machine elm and least squares support vector machine lssvm and their bootstrap versions belm and blssvm for the estimation of river ice thickness at the mackenzie river and yellowknife river stations in the northwest territories canada the performance of the applied models was compared to that of an artificial neural network ann the secondary objective was to evaluate the performance accuracy of two imputation methods ktrl and regem implemented in the machine learning models when applied to the same scenario using 35 years of data the input predictor variables for estimating river ice thickness cm were snow depth cm water flow m3 s and mean air temperature c the datasets were divided into 70 training 15 validation and 15 testing subsets while the ann model was not sufficiently accurate in estimating river ice thickness in this study area the newer form of machine learning techniques elm lssvm and their bootstrap versions provided acceptable performances the model testing phase indicated that the belm model could potentially improve the estimation accuracy of the modeling process compared to the elm lssvm and blssvm models at both stations results showed that the regem and ktrl imputation techniques were more efficient for modeling the ice thickness at the mackenzie river and yellowknife river stations respectively according to the model evaluation metrics rmspe and mape decreased by about 0 07 3 8 and 1 1 5 respectively for the mackenzie river station when the belm model with regem imputation technique was used in comparison with the elm lssvm and blssvm models moreover for the belm model employing the ktrl imputation technique for the yellowknife river station rmspe and mape were reduced by about 1 7 6 2 and 4 9 12 3 respectively compared to the other applied models the results showed that the elm model with the regem imputation technique estimated peak values with better accuracy than with the ktrl imputation technique overall it can be concluded that the machine learning models developed in this study e g elm and lssvm based models and their bootstrapped versions are promising techniques for the estimation of river ice thickness a study conducted by zaier et al 2010 that estimated ice thickness in a number of selected canadian lakes found that ann ensemble models were more accurate than single ann models especially when boosting was used to combine ensemble members and when stacking was used to combine the outputs from individual members the current study also demonstrated the capability of the bootstrapped based machine learning models to estimate river ice thickness the authors of this study recommend that further investigations be conducted by applying the multi model ensemble based approach to integrate the merit of each standalone model coupled with imputation and bootstrap techniques to improve the generalization performance of the models declaration of competing interest none acknowledgements the authors would like to thank the water survey of canada for the data used in this study this study was funded by an nserc discovery grant held by jan adamowski 
6202,the wenchuan area has become highly susceptible to landslides and debris flows since the earthquake occurred on 2008 a detailed debris flow hazard assessment is necessary to provide information for future risk management debris flow runout on a depositional fan is an important factor for assessing debris flow hazard and its estimate becomes essential for the planning of the mitigation works that must be built we therefore developed two simple empirical relationships based both on univariate and multivariate approaches which provide the runout distance using the data for 134 channelized debris flow events in 134 catchments in the wenchuan earthquake zone we generated a correlation matrix of the debris flow runout distances and the relevant variables because of the high correlations the independent variable debris flow volume v d was selected for the univariate approach using a multicollinearity analysis and a stepwise regression technique the catchment basin internal relief h and v d were used to develop a multivariate runout relationship the coefficients of the variables were obtained using 80 of the dataset training dataset the remaining 20 was used for test of estimated performance by comparing the computed runout distances with observed values the validation demonstrated that the proposed relationships are suitable for estimating the runout distances of debris flows on depositional fans in the wenchuan earthquake zone the univariate runout relationship has the advantage of being simple whereas the multivariate runout relationship provides higher accuracy additionally we rearranged and reformulated existing relationships using the same training dataset and compared the results with those from the relationships here proposed the estimations provided by our relationships were the closest to the observed values the presented approaches may be applied to estimate debris flow runout distances in other areas after they are retrained using local datasets keywords channelized debris flow runout distance wenchuan earthquake regression analysis depositional fan nomenclature a catchment area km2 aic akaike information criterion a l landslide area km2 c channel length km h catchment internal relief km i channel gradient k l empirical coefficient l debris flow runout distance km l i observed debris flow runout in the training dataset l i estimated value of debris flow runout l i mean value of the observed debris flow runout l m maximum likelihood estimate l m uncertainty factor in the multivariate regression equation l u uncertainty factor in the univariate regression equation n number of input variable n number of the debris flow runout in the training dataset r 2 adj adjusted r 2 s mean slope of the catchment u a random error term v d debris flow volume 103 m3 v l total landslide volume 106 m3 x 1 variable x 2 variable x n variable α empirical coefficient β 1 empirical coefficient β 2 empirical coefficient β n empirical coefficient 1 introduction the earthquake that struck the wenchuan area in 2008 triggered many landslides and left enough unconsolidated materials on the slopes and in the nearby channels the unconsolidated material can be entrained by abundant runoff usually occurring at the feet of cliffs kean et al 2012 gregoretti et al 2016 wei et al 2018 both on channels berti and simoni 2005 coe et al 2008 kean et al 2013 hürlimann et al 2015 degetto et al 2015 and on slopes gregoretti 2000 hu et al 2016 many channelized debris flows occurred in this area during heavy rainfall after the wenchuan earthquake such as the beichuan debris flows on 24 september 2008 tang et al 2012a and yingxiu debris flows on 13 14 august 2010 tang et al 2011 large amounts of sediments can be transported by channelized debris flows to the depositional zone over short periods threatening infrastructure such as buildings and roads and local residents fuchs et al 2007 jacob et al 2012 stancanelli and musumeci 2018 to reduce economic losses and casualties the debris flow hazard on a depositional fan should be rigorously assessed in order to properly design the mitigation works for the protection of the inhabited areas debris flow runout distance on a depositional fan is an important factor for delineating debris flow hazard areas d agostino et al 2010 hürlimann et al 2006 over the past decades two different approaches have been used for the estimation of the debris flow runout physically based modelling of debris flow propagation 1 and empirical relationships 2 physically based models include leading edge and numerical models the former are essentially analytical expressions derived from physical relationships based on simple parameters takahashi 1981 lo 2000 the latter simulate the debris flow inundation of inhabited areas for an efficient hazard assessment they must simulate the entrainment and deposition processes chen et al 2006 medina et al 2008 armanini et al 2009 frank et al 2015 cuomo et al 2016 bernard et al 2019 using parameters calibrated through a sensitivity analysis hussin et al 2012 frank et al 2017 gregoretti et al 2019 physically based models have the advantage of estimating accurate runout lengths by using the correct parameters and can provide additional information such as debris flow discharge and velocity mcdougall 2017 however obtaining the appropriate input parameters could be difficult and data collection and analyses are complex e g iverson 1997 prochaska et al 2008 empirical relationships are based on the sediment volume to be deposited ikeya 1981 rickenmann 1999 garcía ruiz et al 2002 yu et al 2006 zhang et al 2013 von ruette et al 2016 or topography cannon 1989 fannin and wise 2001 lorente et al 2003 mcdougall 2017 prochaska et al 2008 tang et al 2012a vandre 1985 topographic methods require the channel slope prochaska et al 2008 or elevation difference vandre 1985 burton and bathurst 1998 their advantage is that they do not require an estimate of debris flow volume volume based relationships require the deposited debris flow volume and some topographical quantities as channel slope ikeya 1981 among volume based relationships that proposed by rickenmann 1999 has received considerable attention it is based on the geometric scaling and statistical analysis of historical and experimental debris flows and estimate the runout distance as dimensional function of the deposited volume similar correlations have been developed using data from the italian alps and central spanish pyrenean debris flows garcía ruiz et al 2002 the main advantage of empirical statistical approaches is their user friendliness but their limitation is that they can only be applied to conditions similar to those in the areas where the relationships were developed rickenmann 2005 additionally before their application the uncertainty of empirical relationships must be evaluated simoni et al 2011 the aim of the present work was to develop univariate and multivariate empirical statistical relationships for estimating debris flow runout based on the data of 134 past channelized debris flows in this paper we first present the study area and introduce data sources and methods next the proposed relationships are validated and compared with those of literature finally the conclusions of this study are drawn this work includes data of tang et al 2012a that also proposed an empirical relationship for the estimation of the runout the contribution of the study is two fold firstly we collected six channelized debris flow inventories and expanded the dataset the area of the study area is larger than that of tang et al 2012a secondly our approach involves the selection of candidate variables based on the correlation coefficient and multicollinearity analysis the construction of a training dataset and a testing dataset using random sampling and the determination of the best fit linear regression equations 2 material and methods 2 1 study area the study area is situated in the north of sichuan province china it has an area of 23 387 km2 fig 1 and its elevation ranges from 490 to 6203 m above sea level the area is characterized by steep slopes slopes derived from a 25 m digital elevation model dem have an average of 27 7 with 67 of the study area having slopes of 20 60 the climate of the study area is classified as humid subtropical monsoon tang et al 2012a and the mean annual temperature varies from 11 to 16 3 c the mean average precipitation ranges from 486 to 1300 mm approximately 75 85 of the annual precipitation occurs between may and september he et al 2008 the longmenshan fault zone located in the central study area consists of three subparallel thrust faults along a ne sw transect namely the jiangyou guanxian yingxiu beichuan and maoxian wenchuan faults the lithology dominantly comprises sandstone phyllite sandy slate granite and dolomite more detailed information on the specific lithological characteristics are provided by qi et al 2010 the selected debris flow catchments are distributed along the yingxiu beichuan fault which triggered the wenchuan earthquake many landslides were caused by this earthquake which generated a large amount of loose material landslide debris which was deposited on the slopes as well as in and near proximal channel beds provided the source material for debris flows debris flows represent a significant geological hazard in the study area they are triggered in the summer and early autumn by high intensity short duration rainfall or low intensity rainfall only when the amount of antecedent precipitation is high channelized debris flows are especially disastrous because of their ability to transport large amounts of materials over long distances damaging infrastructure and causing casualties gregoretti et al 2018 chen et al 2019 kean et al 2019 a channelized debris flow catchment is shown in fig 2 the channelized debris flow events that typify the region include the beichuan debris flows on 24 september 2008 tang et al 2012a qingping debris flows on 13 august 2010 tang et al 2012b yingxiu debris flows on 14 august 2010 tang et al 2011 longchi debris flows on 13 august 2010 chang et al 2014 and dabao debris flows on 18 august 2012 huang and tang 2014 hence this study concerns channelized debris flows that occurred during 2008 2012 2 2 dataset in this study a debris flow inventory for the estimation of the runout distance was constructed a debris flow inventory map was produced from 134 channelized debris flow locations in the study area fig 1 the summary of the inventory dataset is shown in table 1 the inventory dataset included data of the catchment area a channel length c catchment basin internal relief h channel gradient i the mean slope of the catchment s areas subjected to landslides a l the deposited debris flow volume v d and debris flow runout distance l these data were collected from the literature chang et al 2014 huang and tang 2014 tang et al 2011 2012a 2012b the data including a c h s and a l were estimated from the dems and aerial photographs in arcgis 9 3 l is defined as the distance from the fan apex to the lowest point of debris flow deposits along the flow direction fig 2 and can be measured in the field using a hand held laser ranger compared to a c h and s a l is more difficult and time intensive to determine because field investigations are needed the estimation of v d was approached by using the field observed thickness and depositional area the depositional area on the fan was computed after field mapping by means of a handheld gps the thickness of debris flow deposits on a fan was measured using a tape tang et al 2012b the multiplication between the obtained depositional area and the thickness of debris flow deposits provides v d the dataset was divided into a training dataset 107 debris flow catchments 80 of the dataset and a testing dataset 27 debris flow catchments 20 of the dataset using random sampling for further analyses all the values were log transformed base 10 2 3 methods our approach for estimating the debris flow runout distance on a depositional fan for the wenchuan earthquake zone comprised three steps 1 selecting runout factors acting as input variables to the relationships 2 constructing empirical relationships and 3 evaluating both the performance and the uncertainties of the relationships 2 3 1 selection of candidate variables the selection of input variables is considered the greatest current challenge for runout forecasting mcdougall 2017 the input variables should adequately capture the physical reality and the impact of the runout the factors influencing debris flows runout can be subdivided in geometric morphological and magnitude based geometric morphological factors include the morphological characteristics of the basin upstream of the deposition area a c h i and s a is defined as the horizontal projection area of the catchment surface c refers to the length along the main channel fig 2 h is defined as the difference between the maximum elevation of the catchment and the fan apex of the catchment i is the average slope angle of the channel s is defined as the average slope of the hillslopes of the catchment taken together c h i and s can provide the potential energy for debris flow initiation and movement the magnitude related factors contain debris flow volume v d and the conditioning factors for sediment supplies with loose debris from a geometric perspective the runout distance of a debris flow on a depositional fan depends on the volume of the debris flow event rickenmann 1999 in addition landslides distributed in catchments contribute to v d and impact debris flow runout distances the two quantitative indicators of the loose debris source are a l and v l referring to the sum of landslide areas and the sum of landslide volumes in the catchment respectively as v l depends on a value obtained by a regression between an approximated value of the average thickness of landslide and a l tang et al 2012a its estimate is more uncertain than a l even if v l contributes more to the debris flow magnitude it is removed therefore a l represents the sediment volume in the source areas the use of the variables relative to the characteristics of the catchment has been justified by cavalli et al 2013 and tiranti et al 2016 we developed empirical relationships using univariate and multivariate regressions for the former we computed the correlation coefficient for each variable factor and selected the variable factor with the largest correlation coefficient for the latter multicollinearity analysis was employed to determine independence among the considered variables multicollinearity refers to two or more variables in a multivariate regression equation that are highly correlated resulting in estimation distortion or inaccuracy multicollinearity analysis was performed using variance inflation factors vifs lee et al 2018 higher vif values indicate higher multicollinearity variables with vif 10 were removed from the relationship vif values should also be recalculated to ensure that no variables have been deleted and values of the input variables should be 10 2 3 2 construction of empirical relationships both univariate and multivariate regressions were used to estimate the maximum debris flow runout distance they can be expressed as 1 l α x 1 β 1 x 2 β 2 x n β n e u where x 1 x 2 and x n are variables α β 1 β 2 and β n are empirical coefficients n is the number of input variables and u is a random error term for univariate regression analysis n 1 to facilitate the nonlinear regression analysis eq 1 was transformed into 2 lg l lg α β 1 lg x 1 β 2 lg x 2 β n lg x n u regression analyses were carried out by means of the software r 3 4 4 for the multivariate regression analysis the akaike information criterion aic was employed for relationship selection because it allowed for the quality of each relationship to be compared with that of other relationships the aic is a numerical value that is meaningless by itself but it may be used to rank empirical relationships based on information loss symonds and moussalli 2011 aic values can easily be calculated through the following equation 3 aic 2 ln l m 2 n where l m is the maximum likelihood estimate for the relationship and n is the number of input variables in the relationship the relationship with the lowest aic value is considered to be the relationship that best fits the data simple and multivariate linear regression analyses were subsequently applied using the training dataset to obtain higher goodness of fit and avoid removing outliers robust linear regressions were obtained by the function rlm in the r package mass robust fitting of linear models rlm is considered as a supplementary approach to linear regression to deal with outliers in the dataset it fits a linear model by robust regression using an m estimator fitting is carried out by means of iterated re weighted least squares iwls venables and ripley 2002 additionally the regression equations had to be statistically significant and follow the major assumptions for linear regression the adjusted r 2 was used to evaluate the fitting performance of these relationships and the p value was used to investigate the statistical significance the regression equation with a higher adjusted r 2 generally had a better fitting performance the relationship with the p value 0 05 was considered statistically significant the adjusted r 2 can be computed through the following equations 4 r adj 2 1 1 r 2 n 1 n n 1 5 r 2 1 i 1 n l i l i 2 i 1 n l i l i 2 where r 2 adj is the adjusted r 2 n is the number of the debris flow runout in the training dataset l i is the observed debris flow runout in the training dataset l i is the estimated value of debris flow runout and l i is mean value of the observed debris flow runout 2 3 3 test of estimated performance and evaluation of uncertainty test of estimated performance is essential for empirical statistical relationships for runout estimation because it proves its reliability through a comparison between the estimated runout values and those observed the estimated performance was tested using the testing dataset we further used the relative error to assess the performance of the developed relationships a relationship with a lower relative error generally had better performance to evaluate the estimation uncertainty berti and simoni 2007 proposed the uncertainty factors e a and e b similarly two factors a and b were used to indicate the uncertainty in scaling relationships simoni et al 2011 berti and simoni 2014 in this study the method proposed by simoni et al 2011 was used to evaluate the uncertainty in the estimation of l 3 results 3 1 selection of candidate variables 3 1 1 univariate regression analysis fig 3 shows a simple scatter plot matrix with a correlogram for l and the selected variables the graph consists of two parts namely the upper with the scatter plot and the lower with the correlogram it combines the advantages of both the scatter plot and correlogram and illustrates the correlations and correlation coefficients among the considered variables positive correlations were found between l and the following variables in decreasing order of strength v d a a l h c i and s accordingly v d was selected as the candidate variable for univariate regression analysis because of its high positive correlation with l 3 1 2 multivariate regression analysis vif values were calculated using the function vif in the r package car a series of multicollinearity analysis was carried out eliminating the variable with vif 10 table 2 summarizes the results of vif values of the factors considered in a set of multivariate regression analyses three multicollinearity analyses i ii and iii were carried out in the first analysis i the vif values of c i and h were considerably 100 which indicated that there was severe multicollinearity after removing the variable with the largest vif the second analysis ii was carried out a was removed in the third analysis iii because its vif value was above 10 the vif values varying from 1 34 to 5 19 were below 10 table 2 hence five candidate variables h i s a l and v d were used for multivariate regression analysis 3 2 construction of empirical relationships 3 2 1 univariate regression equation robust regression analyses were performed based on the training dataset the adjusted r 2 of the robust regression was 0 61 and the estimated coefficients were statistically significant p 0 05 the best fit simple linear regression equation can be described as 6 log l 1 41 0 33 log v d where l is the runout distance on debris flow fans km and v d is the volume of a debris flow event on the fan 103 m3 v d ranged from 1 26 to 3238 4 103 m3 based on geometric scaling and statistical analyses rickenmann 1999 proposed the relationship l v d 1 3 to estimate l for a range of v d the semi empirical equation l k l v d 1 3 is physically based and was found to fit the empirical data for historical debris flows the log transformation of this formula is 7 log l log k l 1 3 log v d where 1 3 is the slope and logk l is the intercept on a log log plot of l versus v d in the specified 1 3 slope of the regression equation logk l is approximately equal to 1 42 the adjusted r 2 of eq 7 was 0 62 and the estimated coefficient was statistically significant p 0 05 eq 6 and eq 7 are nearly the same fig 4 a shows that the 1 3 slope model eq 7 is preferable as it is physically meaningful and the quality of its fitting is comparable with the best fit relationship eq 6 3 2 2 multivariate regression equation a stepwise regression approach was used for the multivariate linear regression analysis the function stepaic in the r package mass was used for the chosen criterion i e aic value the procedure automatically calculated aic values for four candidate relationships the fourth relationship was the best because it exhibited the minimum aic value table 3 a l was not selected as the input variable although it represents the sediment volume in the source areas that can be used the variable associated with landslides was not taken into account in the multivariate linear regression equation because v d contains the volume of sediments in the source areas with the volume of sediments entrained along the channel during the downstream propagation therefore two variables namely h and v d were adopted to develop the multivariate regression equation the best fit multivariate linear regression equation can be expressed as 8 log l 1 29 0 43 log h 0 28 log v d where h is the basin relief km ranging from 0 22 to 2 98 km the adjusted r 2 of eq 8 was 0 73 fig 4b and the estimated coefficients were statistically significant p 0 05 eqs 7 and 8 can be transformed into power law relationships 9 l 0 04 v d 1 3 10 l 0 05 h 0 43 v d 0 28 the apparent exponents in descending order in eq 10 are h and v d while this is inconsistent with the previous correlation analysis they cannot be compared directly because the data were not normalized to ensure fair comparison we used min max normalization approach to normalize the training dataset robust regression analysis was performed using the normalized training dataset the exponents in descending order were v d and h which is consistent with the previous correlation analysis the adjusted r 2 of eqs 9 and 10 were 0 62 and 0 73 respectively it indicates that multivariate runout relationship provides higher accuracy fig 5 illustrates the distribution of the residuals related to the observed debris flow runout distance the graph shows normality and an absence of trends in the residuals the p values of the linear regression equations were 0 05 indicating that the relationships were statistically significant 3 3 test of estimated performance and evaluation of uncertainty 3 3 1 test of estimated performance fig 6 shows a comparison of the estimated runout distances with observed debris flow runout distances using the testing dataset eqs 9 and 10 over and underestimated the runout when the measured runout was 0 13 km and 0 13 km respectively there are at least two possible explanations for the over and underestimated the runout distances l one is that this could be due to the combination of the catchments large channel gradients and the narrow main river in the study area because of these large channel gradients debris flows were deposited near the river and could block it however the fan deposits were removed or washed away by the river guthrie et al 2010 the other could be due to a different rheological behavior velocity of channelized debris flow depends on the triggering runoff discharge lanzoni et al 2017 and influences the debris flow runout these were not taken into account during construction of empirical relationships and the estimated values were therefore smaller than the observed values most empirical relationships have this limitation and cannot provide accurate estimations rickenmann 1999 because they can only estimate the magnitudes of certain debris flow parameters compared with popular empirical relationships e g ikeya 1981 rickenmann 1999 the relative errors of our relationships were within an acceptable range 3 3 2 evaluation of uncertainty the relationships expressed by eqs 9 and 10 were highly statistically significant but the data in fig 4 are scattered fig 7 shows the 95 confidence intervals for the estimate of logl based on the sample standard deviation and t statistic weisberg 2014 eqs 9 and 10 were respectively reformulated into general forms 11 l l u 0 04 v d 1 3 12 l l m 0 05 h 0 43 v d 0 28 where l u and l m are uncertainty factors in the estimation of l in the univariate and multivariate regression equations respectively these factors are functions of the confidence level of the estimations as shown in fig 8 when they are 1 l is larger than the estimated values and when they are 1 l is smaller than the estimated values using eqs 9 and 10 for a 95 confidence level the estimated values of l may be 2 21 times larger or smaller than the observed values indicating that the uncertainty of our estimations is high the choice of uncertainty factors is more difficult in actual applications as it depends on the tolerance level when the uncertainty is associated with the data a lower tolerance may be adopted due to the random errors responsible for most of the scatter simoni et al 2011 or a higher tolerance may be chosen 4 discussion 4 1 selection of input variables the input variable in the relationships of rickenmann 1999 eq 13 garcía ruiz et al 2002 eqs 14 and 15 and our relationship eq 9 only require v d the input variable in the relationship of ikeya 1981 eq 16 requires both v d and the channel gradient i hence v d is considered a primary variable for the estimation of l v d is an estimated value and is influenced by multiple factors such as availability of loose debris runoff and land surface conditions certain popular empirical statistical approaches may be used to estimate the volumes of future debris flows e g rickenmann 1999 bovis and jakob 1999 marchi and d agostino 2004 cannon et al 2010 gartner et al 2014 abancó and hürlimann 2014 although all impact factors can be used to construct a complex multivariate regression relationship multicollinearity of impact factors may not be diagnosable the results of selecting the variables indicate that all impact factors considered in the empirical statistical relationship may not improve the performance of the relationship 4 2 comparison with pre existing runout relationships 4 2 1 comparison with volume based relationships estimated values of l for the debris flow events in the testing dataset were compared to the l values estimated by three volume based runout relationships the relationships used for comparison were proposed by rickenmann 1999 and garcía ruiz et al 2002 these relationships were rearranged from the original form table 4 the runout distances for the testing dataset were calculated using eqs 13 15 and our relationship eq 9 respectively the observed values from the testing dataset were plotted against the estimated values fig 9 a comparison of the four relationships suggested that the estimated values computed by the relationship of rickenmann 1999 were greater than the observed values and those computed by the relationship of garcía ruiz et al 2002 and our relationship were nearer to the observed runout distances rickenmann s 1999 relationship is not recommended for the study area because of its large overestimations similar trends were found in our relationship and that proposed by garcía ruiz et al 2002 in which some estimations were overestimated and others were underestimated in general the estimations computed by our relationship were the closest to the observed runout distances because it is based on the local dataset 4 2 2 comparison with relationships based on multivariate approaches the relationships used for comparison were proposed by ikeya 1981 and tang et al 2012a these relationships were developed based on a multivariate approach to ensure reasonable comparison we retrained the empirical equations by using r software based on our training dataset the relationship of ikeya 1981 can be expressed as 16 l 0 04 v d i 0 12 the adjusted r 2 of eq 16 was 0 57 which is lower than the value of eq 10 the estimated coefficients were statistically significant p 0 05 the reformulated relationship of tang et al 2012a can be expressed as 17 l 0 04 a 0 10 0 132 hv l 0 02 0 05 the multiple correlation coefficient of eq 17 was 0 52 which is lower than the value of the original equation 0 66 the adjusted r 2 of eq 17 was 0 27 suggesting poor model fit the root mean square error was small 0 14 and the low p value 0 05 indicates that eq 17 is statistically significant comparisons of eqs 10 and 16 and 17 with the testing dataset revealed a similar scatter three estimated values were overestimated and 24 estimated values were underestimated using eq 17 twenty three relative errors were 40 using eq 10 16 relative errors were 40 using 16 and 12 relative errors were 40 using eq 17 fig 10 the estimation using eq 10 had a higher level of accuracy the relative errors of the three equations were acceptable the input variables were v d and h which are the characteristic parameters of debris flow catchments and are easily obtained the comparisons of our relationship and the relationships of ikeya 1981 and tang et al 2012a indicated that our multivariate relationship has better performance and is preferred for the estimation of l in the study area 5 limitations as mentioned previously the input variables were h and v d these variables were selected based on either a correlation matrix or stepwise regression approach variable h was derived from the dem and thus the dem resolution could have influenced the estimate of h multivariate regression relationships are sensitive to dem resolution in addition to numerical models input variables with higher resolutions can yield more accurate runout values mcdougall 2017 the main limitations of the multivariate regression relationships are therefore related to the resolutions of the dem limitations for the use of dem can originate from the topographic survey techniques degetto et al 2015 from gridding technique and from the dem size boreggio et al 2018 this limitation is shared by most empirical statistical relationships and does not impact the approach itself tang et al 2012a regression analysis requires the maximum runout length of a debris flow on a depositional fan due to the large channel gradients debris flows in the wenchuan earthquake zone deposited near the river debris flow sediments may have blocked the main river and were subsequently removed or washed away by the river guthrie et al 2010 resulting in an inaccurate measured runout length debris flow runout further depends on parameters related to debris flow and fan morphology jakob et al 2000 haas et al 2015 however regression analysis does not take into account debris flow parameters such as velocity discharge frequency or fan morphology these factors should be considered when accurate estimates of runout zones are required although the proposed approach had the abovementioned limitations it can be used to estimate the runout distance on a depositional fan for future debris flow events in areas similar to that in which its suitability has been validated the relationship was constructed based on limited data from the wenchuan earthquake zone and can be refined by using more debris flow event data knowledge of the history and morphology of debris flow on a depositional fan may substantially improve the accuracy of estimated runout distances 6 conclusions in this study two empirical statistical relationships were proposed to estimate debris flow runout distance on a depositional fan before the occurrence of a debris flow a dataset of 134 debris flow catchments in the wenchuan earthquake zone was generated the dataset consisted of topographical factors loose debris source conditioning factors and v d due to the high correlation between debris flow runout and v d the latter was selected as input variable to establish a univariate relationship for estimating debris flow runout distances the variables h and v d were used to develop a multivariate runout relationship to validate the reliability of our empirical relationships the relationships were tested using an independent dataset of 27 debris flow catchments 20 of the dataset both types of relationship proved to be suitable for estimating the runout distances of debris flows on depositional fans in the wenchuan earthquake zone test of estimated performance revealed that the univariate runout relationship had the advantage of being simple whereas the multivariate runout relationship was more accurate we compared our relationships with five similar existing empirical relationships to ensure reasonable comparison we reformulated the relationships of ikeya 1981 and tang et al 2012a using our training dataset the abovementioned independent dataset was also used to test the reformulated multivariate relationships and the volume based relationships the comparison of the empirical relationships and our relationships shows that a similar scatter was found and the estimations computed by our relationships were the closest to the observed runout distances this comparison demonstrated that our relationships performed better than reformulated relationships and necessary variables were easily measured or estimated thus eliminating the need to extract more variables before the application of our relationships it is necessary to take into account the uncertainty of the empirical relationships moreover the choice of uncertainty factors is a challenge due to the lack of reasonable methods for determination of the degree of tolerance due to its simplicity our proposed approach can be applied to estimate the debris flow runout in other zone sharing the same geological and morphological characteristics declaration of competing interest none acknowledgements this study was supported by the national key research and development program of china 2017yfc1501004 national natural science foundation of china no 41572300 and research fund of the state key laboratory of geohazard prevention and geoenvironment protection of china no 2013z006 we performed all statistical analyses with the r software environment www r project org particularly the car and mass using code by john fox and sanford weisberg venables and ripley we thank five anonymous referees for their helpful suggestions to improve our paper finally we thank elsevier language editing services webshop elsevier com for its linguistic assistance 
6202,the wenchuan area has become highly susceptible to landslides and debris flows since the earthquake occurred on 2008 a detailed debris flow hazard assessment is necessary to provide information for future risk management debris flow runout on a depositional fan is an important factor for assessing debris flow hazard and its estimate becomes essential for the planning of the mitigation works that must be built we therefore developed two simple empirical relationships based both on univariate and multivariate approaches which provide the runout distance using the data for 134 channelized debris flow events in 134 catchments in the wenchuan earthquake zone we generated a correlation matrix of the debris flow runout distances and the relevant variables because of the high correlations the independent variable debris flow volume v d was selected for the univariate approach using a multicollinearity analysis and a stepwise regression technique the catchment basin internal relief h and v d were used to develop a multivariate runout relationship the coefficients of the variables were obtained using 80 of the dataset training dataset the remaining 20 was used for test of estimated performance by comparing the computed runout distances with observed values the validation demonstrated that the proposed relationships are suitable for estimating the runout distances of debris flows on depositional fans in the wenchuan earthquake zone the univariate runout relationship has the advantage of being simple whereas the multivariate runout relationship provides higher accuracy additionally we rearranged and reformulated existing relationships using the same training dataset and compared the results with those from the relationships here proposed the estimations provided by our relationships were the closest to the observed values the presented approaches may be applied to estimate debris flow runout distances in other areas after they are retrained using local datasets keywords channelized debris flow runout distance wenchuan earthquake regression analysis depositional fan nomenclature a catchment area km2 aic akaike information criterion a l landslide area km2 c channel length km h catchment internal relief km i channel gradient k l empirical coefficient l debris flow runout distance km l i observed debris flow runout in the training dataset l i estimated value of debris flow runout l i mean value of the observed debris flow runout l m maximum likelihood estimate l m uncertainty factor in the multivariate regression equation l u uncertainty factor in the univariate regression equation n number of input variable n number of the debris flow runout in the training dataset r 2 adj adjusted r 2 s mean slope of the catchment u a random error term v d debris flow volume 103 m3 v l total landslide volume 106 m3 x 1 variable x 2 variable x n variable α empirical coefficient β 1 empirical coefficient β 2 empirical coefficient β n empirical coefficient 1 introduction the earthquake that struck the wenchuan area in 2008 triggered many landslides and left enough unconsolidated materials on the slopes and in the nearby channels the unconsolidated material can be entrained by abundant runoff usually occurring at the feet of cliffs kean et al 2012 gregoretti et al 2016 wei et al 2018 both on channels berti and simoni 2005 coe et al 2008 kean et al 2013 hürlimann et al 2015 degetto et al 2015 and on slopes gregoretti 2000 hu et al 2016 many channelized debris flows occurred in this area during heavy rainfall after the wenchuan earthquake such as the beichuan debris flows on 24 september 2008 tang et al 2012a and yingxiu debris flows on 13 14 august 2010 tang et al 2011 large amounts of sediments can be transported by channelized debris flows to the depositional zone over short periods threatening infrastructure such as buildings and roads and local residents fuchs et al 2007 jacob et al 2012 stancanelli and musumeci 2018 to reduce economic losses and casualties the debris flow hazard on a depositional fan should be rigorously assessed in order to properly design the mitigation works for the protection of the inhabited areas debris flow runout distance on a depositional fan is an important factor for delineating debris flow hazard areas d agostino et al 2010 hürlimann et al 2006 over the past decades two different approaches have been used for the estimation of the debris flow runout physically based modelling of debris flow propagation 1 and empirical relationships 2 physically based models include leading edge and numerical models the former are essentially analytical expressions derived from physical relationships based on simple parameters takahashi 1981 lo 2000 the latter simulate the debris flow inundation of inhabited areas for an efficient hazard assessment they must simulate the entrainment and deposition processes chen et al 2006 medina et al 2008 armanini et al 2009 frank et al 2015 cuomo et al 2016 bernard et al 2019 using parameters calibrated through a sensitivity analysis hussin et al 2012 frank et al 2017 gregoretti et al 2019 physically based models have the advantage of estimating accurate runout lengths by using the correct parameters and can provide additional information such as debris flow discharge and velocity mcdougall 2017 however obtaining the appropriate input parameters could be difficult and data collection and analyses are complex e g iverson 1997 prochaska et al 2008 empirical relationships are based on the sediment volume to be deposited ikeya 1981 rickenmann 1999 garcía ruiz et al 2002 yu et al 2006 zhang et al 2013 von ruette et al 2016 or topography cannon 1989 fannin and wise 2001 lorente et al 2003 mcdougall 2017 prochaska et al 2008 tang et al 2012a vandre 1985 topographic methods require the channel slope prochaska et al 2008 or elevation difference vandre 1985 burton and bathurst 1998 their advantage is that they do not require an estimate of debris flow volume volume based relationships require the deposited debris flow volume and some topographical quantities as channel slope ikeya 1981 among volume based relationships that proposed by rickenmann 1999 has received considerable attention it is based on the geometric scaling and statistical analysis of historical and experimental debris flows and estimate the runout distance as dimensional function of the deposited volume similar correlations have been developed using data from the italian alps and central spanish pyrenean debris flows garcía ruiz et al 2002 the main advantage of empirical statistical approaches is their user friendliness but their limitation is that they can only be applied to conditions similar to those in the areas where the relationships were developed rickenmann 2005 additionally before their application the uncertainty of empirical relationships must be evaluated simoni et al 2011 the aim of the present work was to develop univariate and multivariate empirical statistical relationships for estimating debris flow runout based on the data of 134 past channelized debris flows in this paper we first present the study area and introduce data sources and methods next the proposed relationships are validated and compared with those of literature finally the conclusions of this study are drawn this work includes data of tang et al 2012a that also proposed an empirical relationship for the estimation of the runout the contribution of the study is two fold firstly we collected six channelized debris flow inventories and expanded the dataset the area of the study area is larger than that of tang et al 2012a secondly our approach involves the selection of candidate variables based on the correlation coefficient and multicollinearity analysis the construction of a training dataset and a testing dataset using random sampling and the determination of the best fit linear regression equations 2 material and methods 2 1 study area the study area is situated in the north of sichuan province china it has an area of 23 387 km2 fig 1 and its elevation ranges from 490 to 6203 m above sea level the area is characterized by steep slopes slopes derived from a 25 m digital elevation model dem have an average of 27 7 with 67 of the study area having slopes of 20 60 the climate of the study area is classified as humid subtropical monsoon tang et al 2012a and the mean annual temperature varies from 11 to 16 3 c the mean average precipitation ranges from 486 to 1300 mm approximately 75 85 of the annual precipitation occurs between may and september he et al 2008 the longmenshan fault zone located in the central study area consists of three subparallel thrust faults along a ne sw transect namely the jiangyou guanxian yingxiu beichuan and maoxian wenchuan faults the lithology dominantly comprises sandstone phyllite sandy slate granite and dolomite more detailed information on the specific lithological characteristics are provided by qi et al 2010 the selected debris flow catchments are distributed along the yingxiu beichuan fault which triggered the wenchuan earthquake many landslides were caused by this earthquake which generated a large amount of loose material landslide debris which was deposited on the slopes as well as in and near proximal channel beds provided the source material for debris flows debris flows represent a significant geological hazard in the study area they are triggered in the summer and early autumn by high intensity short duration rainfall or low intensity rainfall only when the amount of antecedent precipitation is high channelized debris flows are especially disastrous because of their ability to transport large amounts of materials over long distances damaging infrastructure and causing casualties gregoretti et al 2018 chen et al 2019 kean et al 2019 a channelized debris flow catchment is shown in fig 2 the channelized debris flow events that typify the region include the beichuan debris flows on 24 september 2008 tang et al 2012a qingping debris flows on 13 august 2010 tang et al 2012b yingxiu debris flows on 14 august 2010 tang et al 2011 longchi debris flows on 13 august 2010 chang et al 2014 and dabao debris flows on 18 august 2012 huang and tang 2014 hence this study concerns channelized debris flows that occurred during 2008 2012 2 2 dataset in this study a debris flow inventory for the estimation of the runout distance was constructed a debris flow inventory map was produced from 134 channelized debris flow locations in the study area fig 1 the summary of the inventory dataset is shown in table 1 the inventory dataset included data of the catchment area a channel length c catchment basin internal relief h channel gradient i the mean slope of the catchment s areas subjected to landslides a l the deposited debris flow volume v d and debris flow runout distance l these data were collected from the literature chang et al 2014 huang and tang 2014 tang et al 2011 2012a 2012b the data including a c h s and a l were estimated from the dems and aerial photographs in arcgis 9 3 l is defined as the distance from the fan apex to the lowest point of debris flow deposits along the flow direction fig 2 and can be measured in the field using a hand held laser ranger compared to a c h and s a l is more difficult and time intensive to determine because field investigations are needed the estimation of v d was approached by using the field observed thickness and depositional area the depositional area on the fan was computed after field mapping by means of a handheld gps the thickness of debris flow deposits on a fan was measured using a tape tang et al 2012b the multiplication between the obtained depositional area and the thickness of debris flow deposits provides v d the dataset was divided into a training dataset 107 debris flow catchments 80 of the dataset and a testing dataset 27 debris flow catchments 20 of the dataset using random sampling for further analyses all the values were log transformed base 10 2 3 methods our approach for estimating the debris flow runout distance on a depositional fan for the wenchuan earthquake zone comprised three steps 1 selecting runout factors acting as input variables to the relationships 2 constructing empirical relationships and 3 evaluating both the performance and the uncertainties of the relationships 2 3 1 selection of candidate variables the selection of input variables is considered the greatest current challenge for runout forecasting mcdougall 2017 the input variables should adequately capture the physical reality and the impact of the runout the factors influencing debris flows runout can be subdivided in geometric morphological and magnitude based geometric morphological factors include the morphological characteristics of the basin upstream of the deposition area a c h i and s a is defined as the horizontal projection area of the catchment surface c refers to the length along the main channel fig 2 h is defined as the difference between the maximum elevation of the catchment and the fan apex of the catchment i is the average slope angle of the channel s is defined as the average slope of the hillslopes of the catchment taken together c h i and s can provide the potential energy for debris flow initiation and movement the magnitude related factors contain debris flow volume v d and the conditioning factors for sediment supplies with loose debris from a geometric perspective the runout distance of a debris flow on a depositional fan depends on the volume of the debris flow event rickenmann 1999 in addition landslides distributed in catchments contribute to v d and impact debris flow runout distances the two quantitative indicators of the loose debris source are a l and v l referring to the sum of landslide areas and the sum of landslide volumes in the catchment respectively as v l depends on a value obtained by a regression between an approximated value of the average thickness of landslide and a l tang et al 2012a its estimate is more uncertain than a l even if v l contributes more to the debris flow magnitude it is removed therefore a l represents the sediment volume in the source areas the use of the variables relative to the characteristics of the catchment has been justified by cavalli et al 2013 and tiranti et al 2016 we developed empirical relationships using univariate and multivariate regressions for the former we computed the correlation coefficient for each variable factor and selected the variable factor with the largest correlation coefficient for the latter multicollinearity analysis was employed to determine independence among the considered variables multicollinearity refers to two or more variables in a multivariate regression equation that are highly correlated resulting in estimation distortion or inaccuracy multicollinearity analysis was performed using variance inflation factors vifs lee et al 2018 higher vif values indicate higher multicollinearity variables with vif 10 were removed from the relationship vif values should also be recalculated to ensure that no variables have been deleted and values of the input variables should be 10 2 3 2 construction of empirical relationships both univariate and multivariate regressions were used to estimate the maximum debris flow runout distance they can be expressed as 1 l α x 1 β 1 x 2 β 2 x n β n e u where x 1 x 2 and x n are variables α β 1 β 2 and β n are empirical coefficients n is the number of input variables and u is a random error term for univariate regression analysis n 1 to facilitate the nonlinear regression analysis eq 1 was transformed into 2 lg l lg α β 1 lg x 1 β 2 lg x 2 β n lg x n u regression analyses were carried out by means of the software r 3 4 4 for the multivariate regression analysis the akaike information criterion aic was employed for relationship selection because it allowed for the quality of each relationship to be compared with that of other relationships the aic is a numerical value that is meaningless by itself but it may be used to rank empirical relationships based on information loss symonds and moussalli 2011 aic values can easily be calculated through the following equation 3 aic 2 ln l m 2 n where l m is the maximum likelihood estimate for the relationship and n is the number of input variables in the relationship the relationship with the lowest aic value is considered to be the relationship that best fits the data simple and multivariate linear regression analyses were subsequently applied using the training dataset to obtain higher goodness of fit and avoid removing outliers robust linear regressions were obtained by the function rlm in the r package mass robust fitting of linear models rlm is considered as a supplementary approach to linear regression to deal with outliers in the dataset it fits a linear model by robust regression using an m estimator fitting is carried out by means of iterated re weighted least squares iwls venables and ripley 2002 additionally the regression equations had to be statistically significant and follow the major assumptions for linear regression the adjusted r 2 was used to evaluate the fitting performance of these relationships and the p value was used to investigate the statistical significance the regression equation with a higher adjusted r 2 generally had a better fitting performance the relationship with the p value 0 05 was considered statistically significant the adjusted r 2 can be computed through the following equations 4 r adj 2 1 1 r 2 n 1 n n 1 5 r 2 1 i 1 n l i l i 2 i 1 n l i l i 2 where r 2 adj is the adjusted r 2 n is the number of the debris flow runout in the training dataset l i is the observed debris flow runout in the training dataset l i is the estimated value of debris flow runout and l i is mean value of the observed debris flow runout 2 3 3 test of estimated performance and evaluation of uncertainty test of estimated performance is essential for empirical statistical relationships for runout estimation because it proves its reliability through a comparison between the estimated runout values and those observed the estimated performance was tested using the testing dataset we further used the relative error to assess the performance of the developed relationships a relationship with a lower relative error generally had better performance to evaluate the estimation uncertainty berti and simoni 2007 proposed the uncertainty factors e a and e b similarly two factors a and b were used to indicate the uncertainty in scaling relationships simoni et al 2011 berti and simoni 2014 in this study the method proposed by simoni et al 2011 was used to evaluate the uncertainty in the estimation of l 3 results 3 1 selection of candidate variables 3 1 1 univariate regression analysis fig 3 shows a simple scatter plot matrix with a correlogram for l and the selected variables the graph consists of two parts namely the upper with the scatter plot and the lower with the correlogram it combines the advantages of both the scatter plot and correlogram and illustrates the correlations and correlation coefficients among the considered variables positive correlations were found between l and the following variables in decreasing order of strength v d a a l h c i and s accordingly v d was selected as the candidate variable for univariate regression analysis because of its high positive correlation with l 3 1 2 multivariate regression analysis vif values were calculated using the function vif in the r package car a series of multicollinearity analysis was carried out eliminating the variable with vif 10 table 2 summarizes the results of vif values of the factors considered in a set of multivariate regression analyses three multicollinearity analyses i ii and iii were carried out in the first analysis i the vif values of c i and h were considerably 100 which indicated that there was severe multicollinearity after removing the variable with the largest vif the second analysis ii was carried out a was removed in the third analysis iii because its vif value was above 10 the vif values varying from 1 34 to 5 19 were below 10 table 2 hence five candidate variables h i s a l and v d were used for multivariate regression analysis 3 2 construction of empirical relationships 3 2 1 univariate regression equation robust regression analyses were performed based on the training dataset the adjusted r 2 of the robust regression was 0 61 and the estimated coefficients were statistically significant p 0 05 the best fit simple linear regression equation can be described as 6 log l 1 41 0 33 log v d where l is the runout distance on debris flow fans km and v d is the volume of a debris flow event on the fan 103 m3 v d ranged from 1 26 to 3238 4 103 m3 based on geometric scaling and statistical analyses rickenmann 1999 proposed the relationship l v d 1 3 to estimate l for a range of v d the semi empirical equation l k l v d 1 3 is physically based and was found to fit the empirical data for historical debris flows the log transformation of this formula is 7 log l log k l 1 3 log v d where 1 3 is the slope and logk l is the intercept on a log log plot of l versus v d in the specified 1 3 slope of the regression equation logk l is approximately equal to 1 42 the adjusted r 2 of eq 7 was 0 62 and the estimated coefficient was statistically significant p 0 05 eq 6 and eq 7 are nearly the same fig 4 a shows that the 1 3 slope model eq 7 is preferable as it is physically meaningful and the quality of its fitting is comparable with the best fit relationship eq 6 3 2 2 multivariate regression equation a stepwise regression approach was used for the multivariate linear regression analysis the function stepaic in the r package mass was used for the chosen criterion i e aic value the procedure automatically calculated aic values for four candidate relationships the fourth relationship was the best because it exhibited the minimum aic value table 3 a l was not selected as the input variable although it represents the sediment volume in the source areas that can be used the variable associated with landslides was not taken into account in the multivariate linear regression equation because v d contains the volume of sediments in the source areas with the volume of sediments entrained along the channel during the downstream propagation therefore two variables namely h and v d were adopted to develop the multivariate regression equation the best fit multivariate linear regression equation can be expressed as 8 log l 1 29 0 43 log h 0 28 log v d where h is the basin relief km ranging from 0 22 to 2 98 km the adjusted r 2 of eq 8 was 0 73 fig 4b and the estimated coefficients were statistically significant p 0 05 eqs 7 and 8 can be transformed into power law relationships 9 l 0 04 v d 1 3 10 l 0 05 h 0 43 v d 0 28 the apparent exponents in descending order in eq 10 are h and v d while this is inconsistent with the previous correlation analysis they cannot be compared directly because the data were not normalized to ensure fair comparison we used min max normalization approach to normalize the training dataset robust regression analysis was performed using the normalized training dataset the exponents in descending order were v d and h which is consistent with the previous correlation analysis the adjusted r 2 of eqs 9 and 10 were 0 62 and 0 73 respectively it indicates that multivariate runout relationship provides higher accuracy fig 5 illustrates the distribution of the residuals related to the observed debris flow runout distance the graph shows normality and an absence of trends in the residuals the p values of the linear regression equations were 0 05 indicating that the relationships were statistically significant 3 3 test of estimated performance and evaluation of uncertainty 3 3 1 test of estimated performance fig 6 shows a comparison of the estimated runout distances with observed debris flow runout distances using the testing dataset eqs 9 and 10 over and underestimated the runout when the measured runout was 0 13 km and 0 13 km respectively there are at least two possible explanations for the over and underestimated the runout distances l one is that this could be due to the combination of the catchments large channel gradients and the narrow main river in the study area because of these large channel gradients debris flows were deposited near the river and could block it however the fan deposits were removed or washed away by the river guthrie et al 2010 the other could be due to a different rheological behavior velocity of channelized debris flow depends on the triggering runoff discharge lanzoni et al 2017 and influences the debris flow runout these were not taken into account during construction of empirical relationships and the estimated values were therefore smaller than the observed values most empirical relationships have this limitation and cannot provide accurate estimations rickenmann 1999 because they can only estimate the magnitudes of certain debris flow parameters compared with popular empirical relationships e g ikeya 1981 rickenmann 1999 the relative errors of our relationships were within an acceptable range 3 3 2 evaluation of uncertainty the relationships expressed by eqs 9 and 10 were highly statistically significant but the data in fig 4 are scattered fig 7 shows the 95 confidence intervals for the estimate of logl based on the sample standard deviation and t statistic weisberg 2014 eqs 9 and 10 were respectively reformulated into general forms 11 l l u 0 04 v d 1 3 12 l l m 0 05 h 0 43 v d 0 28 where l u and l m are uncertainty factors in the estimation of l in the univariate and multivariate regression equations respectively these factors are functions of the confidence level of the estimations as shown in fig 8 when they are 1 l is larger than the estimated values and when they are 1 l is smaller than the estimated values using eqs 9 and 10 for a 95 confidence level the estimated values of l may be 2 21 times larger or smaller than the observed values indicating that the uncertainty of our estimations is high the choice of uncertainty factors is more difficult in actual applications as it depends on the tolerance level when the uncertainty is associated with the data a lower tolerance may be adopted due to the random errors responsible for most of the scatter simoni et al 2011 or a higher tolerance may be chosen 4 discussion 4 1 selection of input variables the input variable in the relationships of rickenmann 1999 eq 13 garcía ruiz et al 2002 eqs 14 and 15 and our relationship eq 9 only require v d the input variable in the relationship of ikeya 1981 eq 16 requires both v d and the channel gradient i hence v d is considered a primary variable for the estimation of l v d is an estimated value and is influenced by multiple factors such as availability of loose debris runoff and land surface conditions certain popular empirical statistical approaches may be used to estimate the volumes of future debris flows e g rickenmann 1999 bovis and jakob 1999 marchi and d agostino 2004 cannon et al 2010 gartner et al 2014 abancó and hürlimann 2014 although all impact factors can be used to construct a complex multivariate regression relationship multicollinearity of impact factors may not be diagnosable the results of selecting the variables indicate that all impact factors considered in the empirical statistical relationship may not improve the performance of the relationship 4 2 comparison with pre existing runout relationships 4 2 1 comparison with volume based relationships estimated values of l for the debris flow events in the testing dataset were compared to the l values estimated by three volume based runout relationships the relationships used for comparison were proposed by rickenmann 1999 and garcía ruiz et al 2002 these relationships were rearranged from the original form table 4 the runout distances for the testing dataset were calculated using eqs 13 15 and our relationship eq 9 respectively the observed values from the testing dataset were plotted against the estimated values fig 9 a comparison of the four relationships suggested that the estimated values computed by the relationship of rickenmann 1999 were greater than the observed values and those computed by the relationship of garcía ruiz et al 2002 and our relationship were nearer to the observed runout distances rickenmann s 1999 relationship is not recommended for the study area because of its large overestimations similar trends were found in our relationship and that proposed by garcía ruiz et al 2002 in which some estimations were overestimated and others were underestimated in general the estimations computed by our relationship were the closest to the observed runout distances because it is based on the local dataset 4 2 2 comparison with relationships based on multivariate approaches the relationships used for comparison were proposed by ikeya 1981 and tang et al 2012a these relationships were developed based on a multivariate approach to ensure reasonable comparison we retrained the empirical equations by using r software based on our training dataset the relationship of ikeya 1981 can be expressed as 16 l 0 04 v d i 0 12 the adjusted r 2 of eq 16 was 0 57 which is lower than the value of eq 10 the estimated coefficients were statistically significant p 0 05 the reformulated relationship of tang et al 2012a can be expressed as 17 l 0 04 a 0 10 0 132 hv l 0 02 0 05 the multiple correlation coefficient of eq 17 was 0 52 which is lower than the value of the original equation 0 66 the adjusted r 2 of eq 17 was 0 27 suggesting poor model fit the root mean square error was small 0 14 and the low p value 0 05 indicates that eq 17 is statistically significant comparisons of eqs 10 and 16 and 17 with the testing dataset revealed a similar scatter three estimated values were overestimated and 24 estimated values were underestimated using eq 17 twenty three relative errors were 40 using eq 10 16 relative errors were 40 using 16 and 12 relative errors were 40 using eq 17 fig 10 the estimation using eq 10 had a higher level of accuracy the relative errors of the three equations were acceptable the input variables were v d and h which are the characteristic parameters of debris flow catchments and are easily obtained the comparisons of our relationship and the relationships of ikeya 1981 and tang et al 2012a indicated that our multivariate relationship has better performance and is preferred for the estimation of l in the study area 5 limitations as mentioned previously the input variables were h and v d these variables were selected based on either a correlation matrix or stepwise regression approach variable h was derived from the dem and thus the dem resolution could have influenced the estimate of h multivariate regression relationships are sensitive to dem resolution in addition to numerical models input variables with higher resolutions can yield more accurate runout values mcdougall 2017 the main limitations of the multivariate regression relationships are therefore related to the resolutions of the dem limitations for the use of dem can originate from the topographic survey techniques degetto et al 2015 from gridding technique and from the dem size boreggio et al 2018 this limitation is shared by most empirical statistical relationships and does not impact the approach itself tang et al 2012a regression analysis requires the maximum runout length of a debris flow on a depositional fan due to the large channel gradients debris flows in the wenchuan earthquake zone deposited near the river debris flow sediments may have blocked the main river and were subsequently removed or washed away by the river guthrie et al 2010 resulting in an inaccurate measured runout length debris flow runout further depends on parameters related to debris flow and fan morphology jakob et al 2000 haas et al 2015 however regression analysis does not take into account debris flow parameters such as velocity discharge frequency or fan morphology these factors should be considered when accurate estimates of runout zones are required although the proposed approach had the abovementioned limitations it can be used to estimate the runout distance on a depositional fan for future debris flow events in areas similar to that in which its suitability has been validated the relationship was constructed based on limited data from the wenchuan earthquake zone and can be refined by using more debris flow event data knowledge of the history and morphology of debris flow on a depositional fan may substantially improve the accuracy of estimated runout distances 6 conclusions in this study two empirical statistical relationships were proposed to estimate debris flow runout distance on a depositional fan before the occurrence of a debris flow a dataset of 134 debris flow catchments in the wenchuan earthquake zone was generated the dataset consisted of topographical factors loose debris source conditioning factors and v d due to the high correlation between debris flow runout and v d the latter was selected as input variable to establish a univariate relationship for estimating debris flow runout distances the variables h and v d were used to develop a multivariate runout relationship to validate the reliability of our empirical relationships the relationships were tested using an independent dataset of 27 debris flow catchments 20 of the dataset both types of relationship proved to be suitable for estimating the runout distances of debris flows on depositional fans in the wenchuan earthquake zone test of estimated performance revealed that the univariate runout relationship had the advantage of being simple whereas the multivariate runout relationship was more accurate we compared our relationships with five similar existing empirical relationships to ensure reasonable comparison we reformulated the relationships of ikeya 1981 and tang et al 2012a using our training dataset the abovementioned independent dataset was also used to test the reformulated multivariate relationships and the volume based relationships the comparison of the empirical relationships and our relationships shows that a similar scatter was found and the estimations computed by our relationships were the closest to the observed runout distances this comparison demonstrated that our relationships performed better than reformulated relationships and necessary variables were easily measured or estimated thus eliminating the need to extract more variables before the application of our relationships it is necessary to take into account the uncertainty of the empirical relationships moreover the choice of uncertainty factors is a challenge due to the lack of reasonable methods for determination of the degree of tolerance due to its simplicity our proposed approach can be applied to estimate the debris flow runout in other zone sharing the same geological and morphological characteristics declaration of competing interest none acknowledgements this study was supported by the national key research and development program of china 2017yfc1501004 national natural science foundation of china no 41572300 and research fund of the state key laboratory of geohazard prevention and geoenvironment protection of china no 2013z006 we performed all statistical analyses with the r software environment www r project org particularly the car and mass using code by john fox and sanford weisberg venables and ripley we thank five anonymous referees for their helpful suggestions to improve our paper finally we thank elsevier language editing services webshop elsevier com for its linguistic assistance 
6203,this study describes immiscible fluid displacement near a transitional boundary at which inertial effect occurs we simulated two phase fluid flow in a 2d micromodel using the lattice boltzmann method with varying reynolds numbers re log re 2 to 2 and viscosity ratios m log m 2 to 0 to highlight the role of inertial force rather than capillary and viscous forces the capillary number ca was kept constant at log ca 5 as re and m increased development in the preferential flow became more pronounced with tortuous paths we found the transitional boundary exists near log re 1 to 1 at which residual saturation of the displaced fluid starts to fluctuate and changes 5 14 in the transitional regime in particular this boundary describes the effect of inertia on immiscible fluid displacement before an increase in the work done due to inertial force occurs keywords immiscible fluid displacement lattice boltzmann method micromodel reynolds number residual saturation 1 introduction immiscible fluid flow incorporated with interface dynamics at the pore scale exhibits complex and non linear behavior the pore geometry physical properties of both the invading and displaced fluids and the injection and discharging conditions at the boundary are critical to the migration and displacement of immiscible fluids in soil remediation production of methane gas from hydrate bearing sediments enhanced oil recovery and geological disposal of greenhouse gases agaoglu et al 2015 akbarabadi and piri 2013 dai et al 2016 kang et al 2016 koh et al 2016 the 2d micromodel experiments often give an insight into immiscible fluid displacement by emphasizing the role of two dimensionless numbers viz capillary number ca and viscosity ratio m as primary factors cao et al 2016 lenormand et al 1988 wang et al 2013 zheng et al 2017 1 ca μ i q σ and m μ i μ d here μ i and μ d are the viscosities of the invading and displaced fluids respectively q is the darcy velocity of the invading fluid and σ is the interfacial tension the capillary number is a ratio between the viscous and capillary forces meanwhile the viscosity ratio is a ratio between the viscous forces of the invading and displaced fluids before the point of breakthrough mapping the displacement pattern and saturation of the invading fluid in a log ca log m space not only highlights that saturation tends to increase with increasing ca and m but also delineates the viscous and capillary force dominant regimes wang et al 2013 zheng et al 2017 however displacement in an immiscible fluid flow may be affected by the absolute pore size or fluid density further it may be subjected to an inertial effect which can be defined by the reynolds number re which is given by the ratio between inertial and viscous forces 2 re ρ i q d μ i ϕ ρ i σ d μ i 2 ϕ c a here ρ i is the density of the invading fluid d is the characteristic length equivalent to the hydraulic diameter of the mean pore throat and ϕ is porosity from a scientific and engineering perspective the median pore size of porous media e g sandstone sand pervious block ranges from 10 nm to 1 mm bachu and bennion 2008 nelson 2009 seo et al 2017 and the fluid density under consideration has a hundredfold difference kim et al 2004 okamura et al 2011 zhang et al 2011 eq 2 therefore implies that changes in the order of magnitude of either the fluid density ρ i or the pore size d or both values affects re darcy flow showing linear behavior is dominant at low re conditions while an increase in re imposes additional inertia terms therefore the darcy forchheimer equation and ergun equation are often used to assess the effect of inertia on single phase fluid flow in porous media chen et al 2015 ergun 1952 liu et al 1994 moutsopoulos and tsihrintzis 2005 zeng and grigg 2006 further numerical studies have also been recently performed to understand single phase fluid flow under non darcy condition at the pore scale across a wide range of re values chukwudozie et al 2012 muljadi et al 2016 sukop et al 2013 a previous study proposed buckley leverett type analytical solutions for immiscible fluids flow incorporated with the non darcy effect of single phase fluid flow and reported that a non darcy effect helps improve displacement efficiency wu et al 2010 however sufficient numerical and experimental evidence is not provided for the macro scale models yet meanwhile many researchers have widely conducted numerical studies simulating two phase fluid flow at the pore scale by using numerical approaches such as pore network model volume of fluid method smoothed particles hydrodynamics and lattice boltzmann method huang et al 2014 kang and yun 2018 keehm 2003 liu et al 2013 raeini et al 2015 sivanesapillai et al 2016 song et al 2015 suh and yun 2018 tsuji et al 2016 the numerical approaches facilitate modulating domain geometry input parameters and injection condition compared to the experimental methods despite its limitations and simplified assumptions especially the lattice boltzmann lb method has received attention owing to its capability of complex flow simulation in complex pore geometry and most of those studies have focused on the numerical methodology on two phase fluid flow the effects of interfacial tension velocity pressure gradient viscosity ratio contact angle and pore geometry also it was showed that capillary and viscous forces expressed through log ca log m space significantly affected the displacement patterns and fluid saturations similar to experimental observations huang et al 2014 kang and yun 2018 liu et al 2015 2013 tsuji et al 2016 however it is still hard to identify the independent role of inertial force because both capillary and viscous forces prevail concurrently therefore modulating injection velocity or fluid viscosity as carried out for single phase fluid flow is insufficient to individually control inertial force recently a few studies have attempted to understand the role of inertial force in immiscible fluid displacement at the pore scale chen et al 2018 ferrari and lunati 2014 moebius and or 2014 in pore network simulation of cases with and without an inertial force inertial force in the drainage condition strongly influences the invasion pattern rather than the residual saturation of the invading fluid moebius and or 2014 meanwhile the pore network model of two phase fluid flow based on artificial filling rules cannot accurately represent interface dynamics in complex pore geometries adopting a volume of fluid simulation with imbibition condition showed that spontaneous reconfiguration of the meniscus by damped oscillations is irreversible ferrari and lunati 2014 this pressure oscillation may help in overcoming the minimum entry capillary pressure required by a non wetting fluid to penetrate the neighboring pores saturated by a wetting fluid a recent study using lb simulation showed that the number of co2 fingers increased as the re value increased chen et al 2018 these results emphasize that inertia can influence non wetting fluid behavior but how the inertial force consequently affects the residual saturation and the displacement pattern in the immiscible fluid displacement are still unanswered and the interplay between viscosity ratio and inertia is also not completely understood in this study we investigate the effect of inertia by changing the re values for different m values in order to not only observe changes in the invasion pattern and residual saturation values but also to outline the boundary of the darcy flow numerical simulations were performed for the drainage condition in a 2d homogeneous micromodel using the lb method it allows identification of the boundary that emerges due to inertia in terms of the residual saturation of the displaced fluid at different m values 2 materials and methods 2 1 pore domain in 2d micromodel the 2d micromodel domain used in this study contained hexagonally patterned circular particles with a particle radius d p of 47 lu lu is a length unit and mean spacing d s of 14 lu at the pore throat with a small random perturbation fig 1 the hydraulic diameter of the mean pore throat was 28 lu the inlet on the left side included a small buffer zone with a width of 47 lu and the outlet on the right side was located at the pore throat the top and bottom sides were sealed by a solid wall the width w and height h of the domain were 2439 lu and 1478 lu respectively the porosity ϕ of the zone outlined by dotted lines in fig 1 was 0 325 the geometric tortuosity computed from the medial axis in the porous region was t g 2 1 cos π 3 1 333 the specific surface area s surface area total volume was 0 0303 l u 1 2 2 lattice boltzmann method the lb method mimics the statistical behavior of fluid particle assembly at meso scale according to kinetic theory so that the fluid behavior at macro scale is recovered chen et al 1992 it has great advantages to simulate the fluid flow in porous media due to the bounce back scheme at the solid boundary various multi phase fluid models and inherent parallelism by local computation at each node habich et al 2011 keehm 2003 krüger et al 2017 liu et al 2016 the lb equation commonly consists of two parts for a propagation of fluid particles viz streaming step and a moment redistribution of fluid particles viz collision step as 3 f i x e i δ t t δ t f i x t streaming f i x t ω i collision where f i is the particle distribution function that represents the density of fluid particles having discrete particle velocity e i at position x and time t the interval of the time step δt is unity and ω i is the collision operator in the i th direction the bhatnagar gross krook bgk model by using single relaxation time is widely used for the collision operator as 4 ω i 1 τ f i f i eq where the single relaxation time τ is directly related to the kinematic viscosity ν as ν c s 2 τ 0 5 δ t f i eq is the equilibrium distribution function computed by local density and local momentum at each node before the collision step and after the streaming step c s is the speed of sound the local density and the local momentum are defined by the zeroth and the first order moment of particle distribution function as 5 ρ i f i and ρ u i f i e i in the lb method the local pressure p can be expressed by the ideal gas law as 6 p ρ ρ 0 c s 2 where ρ 0 is the reference fluid density although the bgk model is simple fast and is widely used in many studies it suffers from low numerical accuracy and stability this drawback is caused due to that collision process for all moment components having different relaxation speeds uses a single relaxation time thus the multi relaxation time mrt model was proposed to enhance numerical accuracy and stability d humières et al 2002 lallemand and luo 2000 as 7 f f m 1 s m m eq where f and f are sets of particle distribution function f i and f i respectively m is an orthogonal matrix to transform a particle distribution function in a discrete particle velocity space into a moment space s is a diagonal collision matrix m and m eq represent the local fluid moment and local equilibrium moment respectively and are obtained by 8 m mf and m eq mf eq where f and f eq are sets of particle distribution function and equilibrium distribution function respectively in the lb method several models are available to treat multi phase fluid flow in porous media including the rothman keller rk pseudopotential free energy and lee models huang et al 2011 keehm 2003 liu et al 2016 pan et al 2004 although the free energy and lee models seem theoretically advanced the rk model is more advantageous for the current study owing to its low computational cost ability to implement a sharp interface mass conservation at the phase interface and numerical stability in high interfacial tension conditions when compared to other models therefore this study used the mrt model and the rk model for simulating the single phase fluid flow and immiscible fluid displacement in the 2d domain for all lb simulations a mid grid bounce back scheme was applied for the non slip boundary condition at a solid surface and constant flow rate and pressure conditions were applied at the inlet and outlet boundaries in the case of two phase fluid flow the wetting boundary condition based on geometrical formulation with weighted averages of the directional derivative which was developed in our previous study was implemented to achieve an accurate contact angle on non flat solid surfaces kang and yun 2018 for the natural discharging of invading and displaced fluids through the outlet boundary a convective outflow boundary was applied for assigning the local fractions of both fluids and the bulk fluid velocity at the outlet boundary after the streaming step in particular at the point of breakthrough the lack of outlet pressure in the non wetting phase causes a capillary end effect to reduce this effect in our previous study we demonstrated that the outlet pressure of the non wetting phase fluid should be considered a sum of the outlet pressure of the wetting phase fluid and mean capillary pressure kang and yun 2018 the mean capillary pressure was evaluated from the mean curvature of the phase interface between the invading and displaced fluids details of the lb method used in this study can be found in the supporting material 3 results and discussion 3 1 single phase fluid flow to investigate the effect of inertia in the micromodel we performed lb simulations on single phase fluid flow in the re range of log re 2 to 2 the reference density ρ 0 dynamic viscosity µ and corresponding relaxation time parameter τ of the fluid were set at 1 9 288 10 4 and 0 5028 respectively note that units are frequently omitted in the lb method the initial density and velocity fields were set to the reference density and zero respectively the initial particle distribution function was calculated using the equilibrium distribution function later fluid was injected at a constant flow rate after satisfying the predetermined reynolds numbers in eq 2 a constant outlet pressure of p out 0 was maintained for the displaced fluid at the outlet boundary under these conditions the permeability k could be computed by darcy s law as 9 q k μ δ p l where δ p is the pressure difference between the outlet and inlet of the porous region i e region of interest roi in fig 1 and l is the length of the porous region along the flow direction the simulation was stopped when the permeability k computed by darcy s law at time step t satisfied the following convergence criterion 10 k t k t 1000 k t 10 10 the permeability estimated by eq 9 at different log re values is plotted in fig 2 in the range of log re 2 to 1 the permeability was constant at 3 34 lu2 this flow condition is referred to as the darcy flow regime when log re is 1 permeability begins to decrease due to the effect of inertia on fluid flow forchheimer flow regime in this regime the reynolds number is higher than that in the darcy flow regime log re 1 but lower than that in the turbulent flow regime log re 3 the flow in this regime can be expressed by the darcy forchheimer equation as liu et al 1994 11 p a q b q 2 where the pressure loss coefficient for viscous effect a is 1 85 10 4 and the pressure loss coefficient corresponding to inertia dissipation b is 0 16 note that eq 11 becomes asymptotically identical to eq 9 when re decreases i e μ k a the boundary at log re 1 outlines the darcy and forchheimer flow regimes for single phase fluid flow in the micromodel fig 2 also displays the flow streamlines for log re 2 1 25 1 5 and 2 near the middle of the domain in the steady state vorticity starts occurring behind circular particles when log re 1 25 vorticity caused by the detachment of fluid particles from the solid surface due to inertia makes the flow path more tortuous and narrows down the effective flow area despite the fact that vorticity depends on pore geometry and flow direction muljadi et al 2016 this requires a higher hydraulic pressure gradient to drive the same flow rate than small inertia condition yet in two phase fluid flow the existence of a phase interface which is responsible for discontinuous fluid distribution and capillary forces brings in more complex dynamics factors when compared to single phase fluid flow 3 2 immiscible fluid flow at different viscosity ratios before investigating the effect of inertia in two phase fluid flow we performed immiscible fluid displacement simulations with different viscosity ratios to observe the effect of viscosity ratio in a given micromodel domain first we set the three dimensionless numbers representing flow conditions i e the capillary number ca viscosity ratio m and reynolds number re the capillary number was set at a constant value of log ca 5 at which the capillary and viscous forces are almost in balance in porous media armstrong et al 2014 wang et al 2013 and this highlights the effect of inertial force two viscosity ratio values were selected in this simulation log m 2 and 0 and the reynolds number was set at log re 0 the invading and displaced fluid densities were set at unity to reduce the number of controlling parameters and enhance numerical stability the equilibrium contact angle between the wetting and non wetting phase fluids was θ eq 30 at the solid surface in all simulations while the interfacial tension σ was 0 01 initially the wetting fluid i e the displaced fluid fully fills the 2d micromodel domain except for the inlet zone and the non wetting fluid i e invading fluid occupies the inlet zone fig 1 the invading fluid is injected from the inlet boundary at a constant flow rate the pressures of both fluids discharged from the outlet boundary are individually constrained to be constant in order to maintain a constant capillary pressure which was computed from the volume averaged interface curvature at the outlet boundary kang and yun 2018 this reduces the capillary end effect at the breakthrough point of the invading fluid and provides a more authentic configuration in the steady state flow simulation was continued until there were no more changes in the configuration of the invading fluid the pressure and velocity magnitude fields of the invading and displaced fluids are illustrated in fig 3 when log m 0 and 2 at log re 0 at log m 0 capillary fingering a high pressure is generated in the invading fluid while a nominal pressure gradient occurs due to the action of the capillary force at the interface between the invading and displaced fluids two major fingerings are developed in early stage invasion and they continue pushing displaced fluids with wide side branches fig 3a a capillary barrier at the pore throat limits further development in the main flow path and pressure drop in the form of viscous dissipation is predominant in the invading fluid fig 3b on the other hand pressure gradient in the displaced fluid connected to the outlet is relatively small because fluid velocity evolves well across the entire displaced fluid region the tortuosity of preferential fingering is predominant when multiple branches expand and coalesce because the maximum pressure difference between invading and displaced fluid phases occurs at the back of preferential fingering heads note that this mechanism is slightly different from capillary fingering at an extremely low capillary number in a heterogeneous pore structure where pressure difference is maximized at the phase interface hanging over a large pore throat when log m 2 viscous fingering four major fingerings are well developed in the early stages fig 3d and their straightforward invasion continues with three dendritic branches fig 3e the invading pattern near the inlet does not significantly change even after further invasion the driving force for percolation through the displaced fluid and overcoming the capillary barrier is the local pressure difference at the phase interface the pressure drop caused by viscous dissipation of the invading fluid is negligible in the early stages and during continuous invasion the maximum pressure difference occurs at the front of the preferential fingering so that straightforward fingering develops towards the outlet and any additional fingering behind the preferential fingerings is suppressed fig 3e and f this mechanism often suppresses or delays the development of already born weak preferential fingerings when the top and bottom fingerings come close to the outlet the velocity of the middle fingering rapidly decreases and pressure increases however the maximum pressure difference is still dominated at the front of the top and bottom fingerings until shortly before breakthrough fig 3f these observations and interpretations of fingering mechanisms before the breakthrough time are similarly coincident with a previous numerical study under the fixed capillary and reynolds numbers and different viscosity ratios liu et al 2015 3 3 immiscible fluid flow at different reynolds numbers at steady state we further investigated the effect of inertia in immiscible fluid flow under drainage conditions reynolds number was varied from log re 2 to 2 to indirectly exemplify a 10 000 fold difference either in pore size or fluid density the dynamic viscosity μ i and injection darcy velocity q of the invading fluid are obtained from eqs 1 and 2 and as 12 μ i ρ i σ d s ϕ ca re and 13 q σ ϕ r e c a ρ i d s interfacial tension σ is independently selected from 0 001 to 0 1 for each case to render the simulation conditions stable under these conditions the corresponding viscosity and injection flow rate are determined table 1 summarizes the lb parameters of each flow condition used in this study with an increase in re permeability in single phase fluid flow decreased due to retardation in fluid motion by inertia as shown in fig 2 this implies that more energy may be required for fluid injection when inertia increases in two phase fluid flow to evaluate this phenomenon we calculated and compared the work done by external pressure in different cases with log m 0 1 and 2 and log re 2 1 0 1 and 2 the rate of work done in fluids in a control volume ω with an open boundary ω subjected to external pressure p is defined as follows 14 d w p dt ω p u n b d a where n b is a unit normal direction vector on the boundary with an outward direction and u is the local fluid velocity by integrating eq 14 for the roi in the 2d micromodel and time t the work done w can be calculated as 15 w t 0 t q p in p out d t where the volumetric flow rate is represented by q qa p in and p out are the pressures at the inlet and outlet boundaries respectively and a is the cross sectional area normal to the flow direction fig 4 shows the normalized work done during immiscible fluid displacement with respect to the injection volume unit pore volume pv until breakthrough regardless of the log m value the normalized work done at log re 2 increases and runs above other re cases at log m 0 and 1 the evolution of normalized work is analogous when log re 2 in particular the normalized work at log re 2 runs below other cases at log m 2 this means that the external pressure difference does a large amount of work at high inertial force conditions akin to single phase fluid flow this is consistent with previous observations on pore network simulation for imbibition conditions ferrari and lunati 2014 as log m decreases the normalized work done increases due to the high viscosity of the displaced fluid at the same time the initial slope of the work done also increases saturation of the invading fluid computed within the roi increases linearly in a manner equivalent to the injected volume until breakthrough occurs after which saturation either increases gently or converges fig 5 breakthrough occurs before 1 pv of the non wetting fluid is injected regardless of log re and log m for log m 2 red lines breakthrough occurs at relatively small injection volumes e g 0 3 0 5 pv because the viscous fingering reaches the outlet first subsequent injection results in local displacement and the steady state saturation of the invading fluid slightly increases toward asymptotic values after breakthrough on the other hand when log m 1 and 0 blue and green lines breakthrough occurs at high saturation values and high injection volumes e g 0 55 0 65 pv after which no changes occur there is no further displacement and equilibrium is instantaneously achieved after breakthrough we describe the invasion patterns and velocity magnitude maps at steady state for simulated cases with log re 2 0 and 2 and log m 0 1 and 2 in fig 6 at log m 2 the developed fingerings are relatively straighter when compared to those at log m 1 and 0 at all log m values the main flow paths are more developed at log re 2 than at log re 2 and 0 this suggests that a high inertial force may help initiate fingerings during the invasion process regardless of the viscosity ratio now we evaluated the mean tortuosity at steady state by calculating the mean flow length of main flow paths rather than the hydraulic tortuosity computed by volume averaging the spurious velocity caused by the truncation error of a discretization scheme for an interfacial tension force could hinder the accurate measurement of the hydraulic tortuosity by volume averaging under a low capillary number condition like as this study the mean tortuosity tm is the mean flow length lm divided by the domain length l and the mean flow length lm computed by followings 1 find high flux regions occupying 99 of the overall flow 2 extract high flow regions where are connected to the inlet and outlet boundaries for removing vorticities caused by the spurious velocity 3 extract medial axis of high flow regions connected to the inlet and outlet boundaries 4 calculate the mean flow length lm by eq 16 16 l m i j l i j n l where l i j is a flow path from node i at the inlet boundary to node j at the outlet boundary and the path is along the extracted medial axis l i j is a shortest path length for the flow path l i j and n l is number of flow paths in fig 7 a the mean tortuosity at log m 0 tended to increase as re increases and it became constant with decreasing log m moreover the mean tortuosity for all cases were higher than the geometric tortuosity in this study 3 4 transitional change in residual saturation due to inertia taking saturation values from the simulated cases we plotted the evolution of breakthrough saturation and residual saturation of the displaced fluid as shown in fig 7b and c respectively breakthrough saturation of the displaced fluid s d b is higher at low m red square values when compared to other cases and is relatively uniform even with a little fluctuation except when log m 1 with log re 1 5 blue triangles it seems that inertial force barely influences the breakthrough saturation of the displaced fluid when log re 2 regardless of the viscosity ratio in fig 7c it can be observed that the residual saturation of the displaced fluid remains constant from log re 2 further the convex transitional zone in which s d r slightly increases and decreases again occurs at different log re values later s d r of log m 0 and 1 converged when log re 1 but s d r of log m 2 still decreased until log re 2 when log m decreased the constant residual saturation zone unaffected by inertial force and the transitional zone affected by inertial force were extended to higher log re regions changes in the residual saturation of the transitional regime are in the range of 5 14 the change of residual saturation in the transitional zone is analogous to the change in the crossover zone between the capillary force and viscous force dominant regimes wang et al 2013 therefore it seems possible to outline the lower bound of log re where the inertial effect starts to appear as denoted by the black solid line in fig 7c the figure highlights that inertial effect in two phase fluid flow becomes predominant with an increase in the reynolds number and decrease in the viscosity ratio nonlinearity in the effect due to inertia at high log re is not uniquely defined in terms of residual saturation which exhibits inertial effect ahead of the work done by external pressure 4 conclusions immiscible fluid displacement in porous media is governed by viscous capillary and inertial forces and hence the corresponding dimensionless numbers of capillary number ca viscosity ratio m and reynolds number re were used to characterize dominant forces in the flow system in order to assess the effect of inertia on immiscible fluid displacement we performed lattice boltzmann simulations with varying reynolds numbers and viscosity ratios in a 2d micromodel domain to highlight the effect of force due to inertia the capillary number was kept constant at log ca 5 at this condition the viscous force and capillary force were in balance for both single and two phase fluid flow simulations the energy excessively consumed due to inertia in fluid flow was negligible until log re 1 and increased after log re 1 an increase in re barely affected the displacement pattern at steady state but the number of fingering paths increased more importantly we identified the existence of a transitional boundary where the residual saturation fluctuated when log re 1 to 1 and log m 0 to 2 therefore residual saturation cannot be distinctively defined with viscosity ratio and capillary number under the same pore shape because changes in the absolute pore size or fluid density i e change in the reynolds number also influences the residual saturation in conclusion this study proves the existence of transitional boundary regimes at a constant capillary number log ca 5 in a 2d homogeneous micromodel and helps understand the role of inertial force in immiscible fluid displacement in porous media the fingering is a non linear process influenced by various forces and conditions so that the further study is highly desired to figure out the fingering mechanism subjected to various inertial force declaration of competing interest none acknowledgments this work was supported by the land and housing institute lhi grant funded by the korea land and housing corporation and the national research foundation of korea nrf grant funded by the korea government msit 2016r1a2b4011292 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 123934 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6203,this study describes immiscible fluid displacement near a transitional boundary at which inertial effect occurs we simulated two phase fluid flow in a 2d micromodel using the lattice boltzmann method with varying reynolds numbers re log re 2 to 2 and viscosity ratios m log m 2 to 0 to highlight the role of inertial force rather than capillary and viscous forces the capillary number ca was kept constant at log ca 5 as re and m increased development in the preferential flow became more pronounced with tortuous paths we found the transitional boundary exists near log re 1 to 1 at which residual saturation of the displaced fluid starts to fluctuate and changes 5 14 in the transitional regime in particular this boundary describes the effect of inertia on immiscible fluid displacement before an increase in the work done due to inertial force occurs keywords immiscible fluid displacement lattice boltzmann method micromodel reynolds number residual saturation 1 introduction immiscible fluid flow incorporated with interface dynamics at the pore scale exhibits complex and non linear behavior the pore geometry physical properties of both the invading and displaced fluids and the injection and discharging conditions at the boundary are critical to the migration and displacement of immiscible fluids in soil remediation production of methane gas from hydrate bearing sediments enhanced oil recovery and geological disposal of greenhouse gases agaoglu et al 2015 akbarabadi and piri 2013 dai et al 2016 kang et al 2016 koh et al 2016 the 2d micromodel experiments often give an insight into immiscible fluid displacement by emphasizing the role of two dimensionless numbers viz capillary number ca and viscosity ratio m as primary factors cao et al 2016 lenormand et al 1988 wang et al 2013 zheng et al 2017 1 ca μ i q σ and m μ i μ d here μ i and μ d are the viscosities of the invading and displaced fluids respectively q is the darcy velocity of the invading fluid and σ is the interfacial tension the capillary number is a ratio between the viscous and capillary forces meanwhile the viscosity ratio is a ratio between the viscous forces of the invading and displaced fluids before the point of breakthrough mapping the displacement pattern and saturation of the invading fluid in a log ca log m space not only highlights that saturation tends to increase with increasing ca and m but also delineates the viscous and capillary force dominant regimes wang et al 2013 zheng et al 2017 however displacement in an immiscible fluid flow may be affected by the absolute pore size or fluid density further it may be subjected to an inertial effect which can be defined by the reynolds number re which is given by the ratio between inertial and viscous forces 2 re ρ i q d μ i ϕ ρ i σ d μ i 2 ϕ c a here ρ i is the density of the invading fluid d is the characteristic length equivalent to the hydraulic diameter of the mean pore throat and ϕ is porosity from a scientific and engineering perspective the median pore size of porous media e g sandstone sand pervious block ranges from 10 nm to 1 mm bachu and bennion 2008 nelson 2009 seo et al 2017 and the fluid density under consideration has a hundredfold difference kim et al 2004 okamura et al 2011 zhang et al 2011 eq 2 therefore implies that changes in the order of magnitude of either the fluid density ρ i or the pore size d or both values affects re darcy flow showing linear behavior is dominant at low re conditions while an increase in re imposes additional inertia terms therefore the darcy forchheimer equation and ergun equation are often used to assess the effect of inertia on single phase fluid flow in porous media chen et al 2015 ergun 1952 liu et al 1994 moutsopoulos and tsihrintzis 2005 zeng and grigg 2006 further numerical studies have also been recently performed to understand single phase fluid flow under non darcy condition at the pore scale across a wide range of re values chukwudozie et al 2012 muljadi et al 2016 sukop et al 2013 a previous study proposed buckley leverett type analytical solutions for immiscible fluids flow incorporated with the non darcy effect of single phase fluid flow and reported that a non darcy effect helps improve displacement efficiency wu et al 2010 however sufficient numerical and experimental evidence is not provided for the macro scale models yet meanwhile many researchers have widely conducted numerical studies simulating two phase fluid flow at the pore scale by using numerical approaches such as pore network model volume of fluid method smoothed particles hydrodynamics and lattice boltzmann method huang et al 2014 kang and yun 2018 keehm 2003 liu et al 2013 raeini et al 2015 sivanesapillai et al 2016 song et al 2015 suh and yun 2018 tsuji et al 2016 the numerical approaches facilitate modulating domain geometry input parameters and injection condition compared to the experimental methods despite its limitations and simplified assumptions especially the lattice boltzmann lb method has received attention owing to its capability of complex flow simulation in complex pore geometry and most of those studies have focused on the numerical methodology on two phase fluid flow the effects of interfacial tension velocity pressure gradient viscosity ratio contact angle and pore geometry also it was showed that capillary and viscous forces expressed through log ca log m space significantly affected the displacement patterns and fluid saturations similar to experimental observations huang et al 2014 kang and yun 2018 liu et al 2015 2013 tsuji et al 2016 however it is still hard to identify the independent role of inertial force because both capillary and viscous forces prevail concurrently therefore modulating injection velocity or fluid viscosity as carried out for single phase fluid flow is insufficient to individually control inertial force recently a few studies have attempted to understand the role of inertial force in immiscible fluid displacement at the pore scale chen et al 2018 ferrari and lunati 2014 moebius and or 2014 in pore network simulation of cases with and without an inertial force inertial force in the drainage condition strongly influences the invasion pattern rather than the residual saturation of the invading fluid moebius and or 2014 meanwhile the pore network model of two phase fluid flow based on artificial filling rules cannot accurately represent interface dynamics in complex pore geometries adopting a volume of fluid simulation with imbibition condition showed that spontaneous reconfiguration of the meniscus by damped oscillations is irreversible ferrari and lunati 2014 this pressure oscillation may help in overcoming the minimum entry capillary pressure required by a non wetting fluid to penetrate the neighboring pores saturated by a wetting fluid a recent study using lb simulation showed that the number of co2 fingers increased as the re value increased chen et al 2018 these results emphasize that inertia can influence non wetting fluid behavior but how the inertial force consequently affects the residual saturation and the displacement pattern in the immiscible fluid displacement are still unanswered and the interplay between viscosity ratio and inertia is also not completely understood in this study we investigate the effect of inertia by changing the re values for different m values in order to not only observe changes in the invasion pattern and residual saturation values but also to outline the boundary of the darcy flow numerical simulations were performed for the drainage condition in a 2d homogeneous micromodel using the lb method it allows identification of the boundary that emerges due to inertia in terms of the residual saturation of the displaced fluid at different m values 2 materials and methods 2 1 pore domain in 2d micromodel the 2d micromodel domain used in this study contained hexagonally patterned circular particles with a particle radius d p of 47 lu lu is a length unit and mean spacing d s of 14 lu at the pore throat with a small random perturbation fig 1 the hydraulic diameter of the mean pore throat was 28 lu the inlet on the left side included a small buffer zone with a width of 47 lu and the outlet on the right side was located at the pore throat the top and bottom sides were sealed by a solid wall the width w and height h of the domain were 2439 lu and 1478 lu respectively the porosity ϕ of the zone outlined by dotted lines in fig 1 was 0 325 the geometric tortuosity computed from the medial axis in the porous region was t g 2 1 cos π 3 1 333 the specific surface area s surface area total volume was 0 0303 l u 1 2 2 lattice boltzmann method the lb method mimics the statistical behavior of fluid particle assembly at meso scale according to kinetic theory so that the fluid behavior at macro scale is recovered chen et al 1992 it has great advantages to simulate the fluid flow in porous media due to the bounce back scheme at the solid boundary various multi phase fluid models and inherent parallelism by local computation at each node habich et al 2011 keehm 2003 krüger et al 2017 liu et al 2016 the lb equation commonly consists of two parts for a propagation of fluid particles viz streaming step and a moment redistribution of fluid particles viz collision step as 3 f i x e i δ t t δ t f i x t streaming f i x t ω i collision where f i is the particle distribution function that represents the density of fluid particles having discrete particle velocity e i at position x and time t the interval of the time step δt is unity and ω i is the collision operator in the i th direction the bhatnagar gross krook bgk model by using single relaxation time is widely used for the collision operator as 4 ω i 1 τ f i f i eq where the single relaxation time τ is directly related to the kinematic viscosity ν as ν c s 2 τ 0 5 δ t f i eq is the equilibrium distribution function computed by local density and local momentum at each node before the collision step and after the streaming step c s is the speed of sound the local density and the local momentum are defined by the zeroth and the first order moment of particle distribution function as 5 ρ i f i and ρ u i f i e i in the lb method the local pressure p can be expressed by the ideal gas law as 6 p ρ ρ 0 c s 2 where ρ 0 is the reference fluid density although the bgk model is simple fast and is widely used in many studies it suffers from low numerical accuracy and stability this drawback is caused due to that collision process for all moment components having different relaxation speeds uses a single relaxation time thus the multi relaxation time mrt model was proposed to enhance numerical accuracy and stability d humières et al 2002 lallemand and luo 2000 as 7 f f m 1 s m m eq where f and f are sets of particle distribution function f i and f i respectively m is an orthogonal matrix to transform a particle distribution function in a discrete particle velocity space into a moment space s is a diagonal collision matrix m and m eq represent the local fluid moment and local equilibrium moment respectively and are obtained by 8 m mf and m eq mf eq where f and f eq are sets of particle distribution function and equilibrium distribution function respectively in the lb method several models are available to treat multi phase fluid flow in porous media including the rothman keller rk pseudopotential free energy and lee models huang et al 2011 keehm 2003 liu et al 2016 pan et al 2004 although the free energy and lee models seem theoretically advanced the rk model is more advantageous for the current study owing to its low computational cost ability to implement a sharp interface mass conservation at the phase interface and numerical stability in high interfacial tension conditions when compared to other models therefore this study used the mrt model and the rk model for simulating the single phase fluid flow and immiscible fluid displacement in the 2d domain for all lb simulations a mid grid bounce back scheme was applied for the non slip boundary condition at a solid surface and constant flow rate and pressure conditions were applied at the inlet and outlet boundaries in the case of two phase fluid flow the wetting boundary condition based on geometrical formulation with weighted averages of the directional derivative which was developed in our previous study was implemented to achieve an accurate contact angle on non flat solid surfaces kang and yun 2018 for the natural discharging of invading and displaced fluids through the outlet boundary a convective outflow boundary was applied for assigning the local fractions of both fluids and the bulk fluid velocity at the outlet boundary after the streaming step in particular at the point of breakthrough the lack of outlet pressure in the non wetting phase causes a capillary end effect to reduce this effect in our previous study we demonstrated that the outlet pressure of the non wetting phase fluid should be considered a sum of the outlet pressure of the wetting phase fluid and mean capillary pressure kang and yun 2018 the mean capillary pressure was evaluated from the mean curvature of the phase interface between the invading and displaced fluids details of the lb method used in this study can be found in the supporting material 3 results and discussion 3 1 single phase fluid flow to investigate the effect of inertia in the micromodel we performed lb simulations on single phase fluid flow in the re range of log re 2 to 2 the reference density ρ 0 dynamic viscosity µ and corresponding relaxation time parameter τ of the fluid were set at 1 9 288 10 4 and 0 5028 respectively note that units are frequently omitted in the lb method the initial density and velocity fields were set to the reference density and zero respectively the initial particle distribution function was calculated using the equilibrium distribution function later fluid was injected at a constant flow rate after satisfying the predetermined reynolds numbers in eq 2 a constant outlet pressure of p out 0 was maintained for the displaced fluid at the outlet boundary under these conditions the permeability k could be computed by darcy s law as 9 q k μ δ p l where δ p is the pressure difference between the outlet and inlet of the porous region i e region of interest roi in fig 1 and l is the length of the porous region along the flow direction the simulation was stopped when the permeability k computed by darcy s law at time step t satisfied the following convergence criterion 10 k t k t 1000 k t 10 10 the permeability estimated by eq 9 at different log re values is plotted in fig 2 in the range of log re 2 to 1 the permeability was constant at 3 34 lu2 this flow condition is referred to as the darcy flow regime when log re is 1 permeability begins to decrease due to the effect of inertia on fluid flow forchheimer flow regime in this regime the reynolds number is higher than that in the darcy flow regime log re 1 but lower than that in the turbulent flow regime log re 3 the flow in this regime can be expressed by the darcy forchheimer equation as liu et al 1994 11 p a q b q 2 where the pressure loss coefficient for viscous effect a is 1 85 10 4 and the pressure loss coefficient corresponding to inertia dissipation b is 0 16 note that eq 11 becomes asymptotically identical to eq 9 when re decreases i e μ k a the boundary at log re 1 outlines the darcy and forchheimer flow regimes for single phase fluid flow in the micromodel fig 2 also displays the flow streamlines for log re 2 1 25 1 5 and 2 near the middle of the domain in the steady state vorticity starts occurring behind circular particles when log re 1 25 vorticity caused by the detachment of fluid particles from the solid surface due to inertia makes the flow path more tortuous and narrows down the effective flow area despite the fact that vorticity depends on pore geometry and flow direction muljadi et al 2016 this requires a higher hydraulic pressure gradient to drive the same flow rate than small inertia condition yet in two phase fluid flow the existence of a phase interface which is responsible for discontinuous fluid distribution and capillary forces brings in more complex dynamics factors when compared to single phase fluid flow 3 2 immiscible fluid flow at different viscosity ratios before investigating the effect of inertia in two phase fluid flow we performed immiscible fluid displacement simulations with different viscosity ratios to observe the effect of viscosity ratio in a given micromodel domain first we set the three dimensionless numbers representing flow conditions i e the capillary number ca viscosity ratio m and reynolds number re the capillary number was set at a constant value of log ca 5 at which the capillary and viscous forces are almost in balance in porous media armstrong et al 2014 wang et al 2013 and this highlights the effect of inertial force two viscosity ratio values were selected in this simulation log m 2 and 0 and the reynolds number was set at log re 0 the invading and displaced fluid densities were set at unity to reduce the number of controlling parameters and enhance numerical stability the equilibrium contact angle between the wetting and non wetting phase fluids was θ eq 30 at the solid surface in all simulations while the interfacial tension σ was 0 01 initially the wetting fluid i e the displaced fluid fully fills the 2d micromodel domain except for the inlet zone and the non wetting fluid i e invading fluid occupies the inlet zone fig 1 the invading fluid is injected from the inlet boundary at a constant flow rate the pressures of both fluids discharged from the outlet boundary are individually constrained to be constant in order to maintain a constant capillary pressure which was computed from the volume averaged interface curvature at the outlet boundary kang and yun 2018 this reduces the capillary end effect at the breakthrough point of the invading fluid and provides a more authentic configuration in the steady state flow simulation was continued until there were no more changes in the configuration of the invading fluid the pressure and velocity magnitude fields of the invading and displaced fluids are illustrated in fig 3 when log m 0 and 2 at log re 0 at log m 0 capillary fingering a high pressure is generated in the invading fluid while a nominal pressure gradient occurs due to the action of the capillary force at the interface between the invading and displaced fluids two major fingerings are developed in early stage invasion and they continue pushing displaced fluids with wide side branches fig 3a a capillary barrier at the pore throat limits further development in the main flow path and pressure drop in the form of viscous dissipation is predominant in the invading fluid fig 3b on the other hand pressure gradient in the displaced fluid connected to the outlet is relatively small because fluid velocity evolves well across the entire displaced fluid region the tortuosity of preferential fingering is predominant when multiple branches expand and coalesce because the maximum pressure difference between invading and displaced fluid phases occurs at the back of preferential fingering heads note that this mechanism is slightly different from capillary fingering at an extremely low capillary number in a heterogeneous pore structure where pressure difference is maximized at the phase interface hanging over a large pore throat when log m 2 viscous fingering four major fingerings are well developed in the early stages fig 3d and their straightforward invasion continues with three dendritic branches fig 3e the invading pattern near the inlet does not significantly change even after further invasion the driving force for percolation through the displaced fluid and overcoming the capillary barrier is the local pressure difference at the phase interface the pressure drop caused by viscous dissipation of the invading fluid is negligible in the early stages and during continuous invasion the maximum pressure difference occurs at the front of the preferential fingering so that straightforward fingering develops towards the outlet and any additional fingering behind the preferential fingerings is suppressed fig 3e and f this mechanism often suppresses or delays the development of already born weak preferential fingerings when the top and bottom fingerings come close to the outlet the velocity of the middle fingering rapidly decreases and pressure increases however the maximum pressure difference is still dominated at the front of the top and bottom fingerings until shortly before breakthrough fig 3f these observations and interpretations of fingering mechanisms before the breakthrough time are similarly coincident with a previous numerical study under the fixed capillary and reynolds numbers and different viscosity ratios liu et al 2015 3 3 immiscible fluid flow at different reynolds numbers at steady state we further investigated the effect of inertia in immiscible fluid flow under drainage conditions reynolds number was varied from log re 2 to 2 to indirectly exemplify a 10 000 fold difference either in pore size or fluid density the dynamic viscosity μ i and injection darcy velocity q of the invading fluid are obtained from eqs 1 and 2 and as 12 μ i ρ i σ d s ϕ ca re and 13 q σ ϕ r e c a ρ i d s interfacial tension σ is independently selected from 0 001 to 0 1 for each case to render the simulation conditions stable under these conditions the corresponding viscosity and injection flow rate are determined table 1 summarizes the lb parameters of each flow condition used in this study with an increase in re permeability in single phase fluid flow decreased due to retardation in fluid motion by inertia as shown in fig 2 this implies that more energy may be required for fluid injection when inertia increases in two phase fluid flow to evaluate this phenomenon we calculated and compared the work done by external pressure in different cases with log m 0 1 and 2 and log re 2 1 0 1 and 2 the rate of work done in fluids in a control volume ω with an open boundary ω subjected to external pressure p is defined as follows 14 d w p dt ω p u n b d a where n b is a unit normal direction vector on the boundary with an outward direction and u is the local fluid velocity by integrating eq 14 for the roi in the 2d micromodel and time t the work done w can be calculated as 15 w t 0 t q p in p out d t where the volumetric flow rate is represented by q qa p in and p out are the pressures at the inlet and outlet boundaries respectively and a is the cross sectional area normal to the flow direction fig 4 shows the normalized work done during immiscible fluid displacement with respect to the injection volume unit pore volume pv until breakthrough regardless of the log m value the normalized work done at log re 2 increases and runs above other re cases at log m 0 and 1 the evolution of normalized work is analogous when log re 2 in particular the normalized work at log re 2 runs below other cases at log m 2 this means that the external pressure difference does a large amount of work at high inertial force conditions akin to single phase fluid flow this is consistent with previous observations on pore network simulation for imbibition conditions ferrari and lunati 2014 as log m decreases the normalized work done increases due to the high viscosity of the displaced fluid at the same time the initial slope of the work done also increases saturation of the invading fluid computed within the roi increases linearly in a manner equivalent to the injected volume until breakthrough occurs after which saturation either increases gently or converges fig 5 breakthrough occurs before 1 pv of the non wetting fluid is injected regardless of log re and log m for log m 2 red lines breakthrough occurs at relatively small injection volumes e g 0 3 0 5 pv because the viscous fingering reaches the outlet first subsequent injection results in local displacement and the steady state saturation of the invading fluid slightly increases toward asymptotic values after breakthrough on the other hand when log m 1 and 0 blue and green lines breakthrough occurs at high saturation values and high injection volumes e g 0 55 0 65 pv after which no changes occur there is no further displacement and equilibrium is instantaneously achieved after breakthrough we describe the invasion patterns and velocity magnitude maps at steady state for simulated cases with log re 2 0 and 2 and log m 0 1 and 2 in fig 6 at log m 2 the developed fingerings are relatively straighter when compared to those at log m 1 and 0 at all log m values the main flow paths are more developed at log re 2 than at log re 2 and 0 this suggests that a high inertial force may help initiate fingerings during the invasion process regardless of the viscosity ratio now we evaluated the mean tortuosity at steady state by calculating the mean flow length of main flow paths rather than the hydraulic tortuosity computed by volume averaging the spurious velocity caused by the truncation error of a discretization scheme for an interfacial tension force could hinder the accurate measurement of the hydraulic tortuosity by volume averaging under a low capillary number condition like as this study the mean tortuosity tm is the mean flow length lm divided by the domain length l and the mean flow length lm computed by followings 1 find high flux regions occupying 99 of the overall flow 2 extract high flow regions where are connected to the inlet and outlet boundaries for removing vorticities caused by the spurious velocity 3 extract medial axis of high flow regions connected to the inlet and outlet boundaries 4 calculate the mean flow length lm by eq 16 16 l m i j l i j n l where l i j is a flow path from node i at the inlet boundary to node j at the outlet boundary and the path is along the extracted medial axis l i j is a shortest path length for the flow path l i j and n l is number of flow paths in fig 7 a the mean tortuosity at log m 0 tended to increase as re increases and it became constant with decreasing log m moreover the mean tortuosity for all cases were higher than the geometric tortuosity in this study 3 4 transitional change in residual saturation due to inertia taking saturation values from the simulated cases we plotted the evolution of breakthrough saturation and residual saturation of the displaced fluid as shown in fig 7b and c respectively breakthrough saturation of the displaced fluid s d b is higher at low m red square values when compared to other cases and is relatively uniform even with a little fluctuation except when log m 1 with log re 1 5 blue triangles it seems that inertial force barely influences the breakthrough saturation of the displaced fluid when log re 2 regardless of the viscosity ratio in fig 7c it can be observed that the residual saturation of the displaced fluid remains constant from log re 2 further the convex transitional zone in which s d r slightly increases and decreases again occurs at different log re values later s d r of log m 0 and 1 converged when log re 1 but s d r of log m 2 still decreased until log re 2 when log m decreased the constant residual saturation zone unaffected by inertial force and the transitional zone affected by inertial force were extended to higher log re regions changes in the residual saturation of the transitional regime are in the range of 5 14 the change of residual saturation in the transitional zone is analogous to the change in the crossover zone between the capillary force and viscous force dominant regimes wang et al 2013 therefore it seems possible to outline the lower bound of log re where the inertial effect starts to appear as denoted by the black solid line in fig 7c the figure highlights that inertial effect in two phase fluid flow becomes predominant with an increase in the reynolds number and decrease in the viscosity ratio nonlinearity in the effect due to inertia at high log re is not uniquely defined in terms of residual saturation which exhibits inertial effect ahead of the work done by external pressure 4 conclusions immiscible fluid displacement in porous media is governed by viscous capillary and inertial forces and hence the corresponding dimensionless numbers of capillary number ca viscosity ratio m and reynolds number re were used to characterize dominant forces in the flow system in order to assess the effect of inertia on immiscible fluid displacement we performed lattice boltzmann simulations with varying reynolds numbers and viscosity ratios in a 2d micromodel domain to highlight the effect of force due to inertia the capillary number was kept constant at log ca 5 at this condition the viscous force and capillary force were in balance for both single and two phase fluid flow simulations the energy excessively consumed due to inertia in fluid flow was negligible until log re 1 and increased after log re 1 an increase in re barely affected the displacement pattern at steady state but the number of fingering paths increased more importantly we identified the existence of a transitional boundary where the residual saturation fluctuated when log re 1 to 1 and log m 0 to 2 therefore residual saturation cannot be distinctively defined with viscosity ratio and capillary number under the same pore shape because changes in the absolute pore size or fluid density i e change in the reynolds number also influences the residual saturation in conclusion this study proves the existence of transitional boundary regimes at a constant capillary number log ca 5 in a 2d homogeneous micromodel and helps understand the role of inertial force in immiscible fluid displacement in porous media the fingering is a non linear process influenced by various forces and conditions so that the further study is highly desired to figure out the fingering mechanism subjected to various inertial force declaration of competing interest none acknowledgments this work was supported by the land and housing institute lhi grant funded by the korea land and housing corporation and the national research foundation of korea nrf grant funded by the korea government msit 2016r1a2b4011292 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 123934 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6204,a new solute transport inverse method is proposed for estimating plume trajectory and source release location under unknown solute transport boundary conditions in a steady state non uniform groundwater flow field solute concentration is modeled by proposing a set of local approximation solutions las of transport that are discretized over the problem domain at a given time step the inverse method imposes continuities of concentration and total solute mass flux at a set of collocation points in the inversion grid whereas the las are conditioned to measured breakthrough concentrations by enforcing transport physics at selected points in space and time the inverse problem becomes well posed and a single system of inversion equations is assembled and solved with a parallel iterative solver unlike most of the inversion techniques that minimize a model data mismatch objective function the inverse method does not require the simulation of a forward transport model for optimization thus both solute initial and boundary conditions can be recovered assuming dispersivity estimates are available the method was demonstrated using synthetic breakthrough data from various sampling densities and designs i e irregular versus uniformly spaced well networks different measurement errors and source release histories e g uniform in time single and multiple pulses were also investigated results suggest that for the source release histories tested 1 inversion is stable under increasing measurement errors up to 5 of the maximum observed concentration 2 accurate plume trajectory and source release location can be estimated from solute breakthrough concentrations 3 inversion accuracy appears the most sensitive to sampling well density and its information content keywords solute transport contaminant source identification las inverse method source release history boundary conditions 1 introduction worldwide groundwater contamination from point and non point sources is widespread in the u s cleanup of over 300 000 soil and groundwater sites is projected to cost 200 billion by 2033 national research council 2013 identification of pollutant pathways and sources is of particular importance for remediation design and the long term planning and management of contaminated sites various techniques have been proposed for identifying contaminant pathways and sources and they can be divided into two general groups 1 solute transport is solved with a forward or backward in time numerical model with which plume trajectory is recovered by optimizing a model date mismatch or objective function chadalavada et al 2012 mirghani et al 2012 yeh et al 2016 michalak and kitanidis 2004 prakash and datta 2013 sun 2007 because transport is dispersive and irreversible inverse modeling using reverse time is considered ill posed skaggs and kabala 1994 on the other hand when a forward model is used solute transport is simulated with a set of assumed initial and boundary conditions bc in order to provide simulated measurements to match with observations for aquifers where hydraulic conductivity k is heterogeneous various techniques have been extended to invert transport in non uniform flows to account for uncertainty in inversion geostatistical methods have also been developed 2 solute transport is inverted using analytical solutions and regression techniques alapati and kabala 2000 bagtzoglou and atmadja 2005 sun et al 2006 woodbury and ulrych 1996 bagtzoglou 2003 neupauer et al 2000 to develop such solutions to describe plume migration flow fields are usually considered uniform importantly both groups of techniques assume that solute transport bc are known a priori in order to develop numerical and analytical solutions for optimization at aquifer sites with limited or incomplete bc information application of these techniques can lead to non unique inverse solutions ayvaz 2016 to identify contaminant trajectory and source in a non uniform flow field this study proposes a new inverse method by developing local approximate solutions las of concentration and by conditioning them to solute breakthrough measurements key difference between this new approach and the majority of existing inversion techniques is that standard regression techniques are not employed thus a solute transport forward model does not need to be built and simulated in order to optimize a model data mismatch and as a result non uniqueness in inversion that is due to an incorrect assumption of transport bc is eliminated the same issue in groundwater flow inversion exists which has been demonstrated in irsa and zhang 2012 this article explains the new transport inverse method for estimating the state variable i e concentration and its spatial temporal evolution assuming appropriate dispersivity data are available as prior information for inversion the method was verified using synthetic concentration breakthroughs with or without measurement errors that were sampled from a true model that simulated several source release histories at a given source location note that the true model can be an analytical or numerical solution of the solute transport equation subject to a set of known initial and bc or it can be a physical system created in a laboratory in this article because transport in non uniform flows is of interest a numerical model was adopted as the true model to test the inverse method performance of the new inverse method was evaluated using various sampling densities and designs i e irregular versus uniform well networks the irregular network was based on monitoring wells from a shallow unconfined aquifer in the texas high plains where water level and core hydrogeological measurements were available this aquifer which overlies 4 texas counties parmer castro bailey lamb extends to a depth 30 m and consists of unconsolidated coarse sand gravel minor clays and carbonate minerals seni 1980 from the numerous lithology logs and limited core measurements the aquifer is moderately heterogeneous where k varies over 3 orders of magnitude http www twdb texas gov groundwater data index asp water level data came from usgs national water information system https waterdata usgs gov nwis nwis and have been quality checked by usgs with an approval status of processing and review completed from the water levels overall groundwater flow direction is from west to east in the reminder of this article the true model and the inverse method are described first followed by a section describing application of the new method to plume recovery for different source release histories e g uniform in time single and multiple pulses breakthrough data with varied measurement quality density and distribution were tested we then summarize the outcomes while pointing out future research directions 2 method a new inverse method is proposed for identifying solute plume history and source release location under unknown solute transport bc the solute is assumed to be dilute non reactive and is migrating in a steady state non uniform flow field variable density coupling with flow is ignored note that groundwater flow and concentration data can be assimilated using a sequential inversion approach 1 given hydraulic head and local k measurements flow inversion is performed to recover a steady state non uniform flow field of an unconfined aquifer jiao and zhang 2014a 2015a b 2 given the flow field solute plume history and source release location are identified by conditioning the estimated plume trajectory to observed breakthrough curves btc at wells for the above steps described both bc for flow inversion and initial and bc for transport inversion are unknown two separate grids can be used based on measurement locations e g one for flow inversion using wells with hydraulic data and one for transport inversion using wells with btc data as the flow inversion step has been previously described this work focuses on plume identification within the estimated flow field below equation describing the solute transport true model is presented first followed by a description of the inverse method groundwater is assumed to have negligible vertical flow thus transport inversion is performed in 2d on the horizontal plane moreover the physics governing solute transport is assumed known and is described by the advection dispersion equation ade to verify the inverse method numerical transport experiments true model were performed solving the ade subject to different source release scenarios to provide synthetic measurements for inversion 1 c x y t t x α l v d s c x y t x y α t v d s c x y t y x c x y t v x x y y c x y t v y x y o n ω 2 c x y 0 0 o n ω 3 c x y t c 0 x y o n γ 1 t 0 where ω is computational domain c is concentration in mass per unit pore water volume m l3 d s is porous medium diffusion coefficient v v x x y v y x y is average linear velocity l t v is related to darcy flux q q x x y q y x y via effective porosity θ v q θ which is assumed a known constant α l and α t are local longitudinal and transverse dispersivities respectively assumed known as priori information we ignore spatial variation of α l and α t because the site k field resolved by flow inversion is moderately heterogeneous finally γ 1 is dirichlet bc where zero concentration is assigned at the inflow boundary zero mass flux is assigned to the top and bottom while outflow bc are imposed on the remaining boundary so solute can migrate out of the domain for the same computational domain ω transport inversion enforces 3 sets of constraints 1 global continuity of concentration and total solute flux mass at a set of collocation points at a given discretized time 2 local conditioning of the inverse solutions of the given time to observed breakthrough concentrations at the same time 3 equation constraints that enforces the transport physics at selected points in space and time the first set of constraints is written as 4 w n p γ r c n p γ δ n p γ d γ m 0 m 1 y γ 1 v 5 w n p γ r j n p γ δ n p γ d γ m 0 m 1 y γ 1 v where n denotes a discretized inversion time p γ denotes a collocation point on mth cell interface γ m in the inversion grid γ is the number of collocation points on γ m r c n p γ and r j n p γ are residuals of solute concentration and total solute mass flux at p γ on γ m at the nth time respectively y is the number of cell interfaces in the grid δ n p γ is dirac delta function at the nth time and w n p γ is weighting function which samples the residuals at p γ on γ m at the nth time details on how the weight is implemented can be found in zhang et al 2014 the residuals can be expanded as 6 r c n p γ c i n p γ c k n p γ 7 r j n p γ j i n p γ j k n p γ 8 j n v c n d c n where c is a proposed las that varies with space and time c i n and c k n denote concentrations of cells i and k adjacent to γ m at the nth time d is the dispersion tensor j n is a proposed las for total solute mass flux j n depends on c and c thus is space and time dependent for a two dimensional problem the total mass flux can be written as bear and cheng 2010 j n j x n j y n v x x y c n α l v d s c n x v y x y c n α t v d s c n y the second set of constraints is defined by conditioning c n to measured concentrations 9 w n p a c n p a c ob n p a 0 a 1 a where pa is a measurement point c ob n is observed concentration at pa at the nth time a is total number of observations and w n p a is a weighting function at the nth time assigned to each observation equation to reflect the magnitude of measurement errors w n p a can be time dependent i e becomes larger if measurement quality improves with time the inverse method assumes that the governing transport equation is known which provides a set of equation constraints for inversion 10 w n p e r e n ε r e n c n t x α l v d s c n x y α t v d s c n y x c n v x y c n v y e e 1 y v a where pe include both collocation points and measurement locations pe can be time dependent which is not implemented here r e is the equation residual at pe at the nth time for a given time eq 10 enforces a set of physical constraints on the las of c n and j n at pe the system of inverse equations is solved using an iterative solver where ε is on the order of 10 5 or smaller in eqs 4 10 both las of c n and j n are approximate rather than exact solutions which allows a flexible handling of parameters initial and bc while eq 10 provides physics based constraints on the approximations in the solution domain v x x y and v y x y are obtained from flow inversion and are deterministic coefficients in the las for each inversion grid cell ω e polynomial functions were chosen as the las 11 c n x y t a 1 n a 2 n t n a 3 n a 4 n t n x a 5 n a 6 n t n y a 7 n a 8 n t n x y where a i n i 1 8 are unknown time dependent coefficients that together control the shape of the las of concentration following eq 11 total mass flux can be approximated as 12 j x n x y t c n v x α l v d s c n x a 1 n a 2 n t n a 3 n a 4 n t n x a 5 n a 6 n t n y a 7 n a 8 n t n x y v x α l v d s a 3 n a 4 n t n a 7 n a 8 n t n y 13 j y n x y t c n v y α t v d s c n y a 1 n a 2 n t n a 3 n a 4 n t n x a 5 n a 6 n t n y a 7 n a 8 n t n x y v y α t v d s a 5 n a 6 n t n a 7 n a 8 n t n x following eq 11 r e n i n eq 10 is rewritten as 14 a 2 n a 4 n x a 6 n y a 8 n x y v x a 3 n a 4 n t n a 7 n a 8 n t n y v y a 5 n a 6 n t n a 7 n a 8 n t n x 0 for the discretized inversion spatial and temporal domains eqs 4 10 can be assembled to form a single system of equations that can be solved using standard techniques a scalable parallel linear solver based on the message passing interface scalable parallel lsqr huang et al 2013 is implemented for its solution for which the inversion coefficient matrix is decomposed to a kernel sub matrix and a damping sub matrix to reduce overlapping information this decreases inter processor communication and achieves computational efficiency wang 2014 for a trial transport inversion problem a serial solver and the parallel solver were both employed yielding nearly identical results 3 results 3 1 computational domain a regional scale 2d computation domain 112 km e w and 79 km n s was selected to reflect realistic monitoring well designs from an unconfined aquifer in texas high plains before transport inversion was performed steady state flow was inverted to recover long term water table and darcy fluxes by conditioning to core k and water level data from 329 active wells to obtain groundwater velocity an average effective porosity from core measurements was used because field measurements are never exhaustive uncertainty exists in the inverted flow field however it is treated here as deterministic and is imported into the transport analysis for both the true forward model and the inverse analyses specifically based on observed concentrations prior dispersivity estimates and the groundwater velocity solute concentration was inverted without the knowledge of both solute initial and boundary conditions moreover based on site lithology data and mean groundwater velocity an average pe number of 1 0 was calculated assuming the molecular diffusion coefficient of common dissolved hydrocarbons thus for the modeled site solute transport is advection dominated perkins and johnston 1963 batu 2005 and diffusion is ignored in both the forward model and the inverse analysis 3 2 numerical true model four transport models were simulated in the domain solving eqs 1 3 using mt3dms zheng 2010 in each model a tracer was released in upstream flow field x 19 1 19 9 km y 55 2 56 1 km near the left boundary and migrated towards the right boundary one model simulated continuous constant in time source release for a period of 1 8 106 days which is also the total simulation time fig 1 a for a shorter simulation time of 8 105 days the second model simulated a short 104 day release of the tracer at the same location fig 1b during the release time a constant concentration of 10 mg l is specified at the source location in both models for the simulation time of 8 105 days two additional release scenarios were simulated with a gaussian and multimodal release at the source respectively fig 1c d in the last example 4 gaussian distributions were superimposed at the source to create 4 peak concentrations over time in all simulations α l and α t of 0 1 km and 0 03 km respectively were assigned to the grid cells gelhar et al 1992 for the true model mt3dms implements the finite difference scheme fdm for spatial and temporal discretization various discretizations were experimented to ensure the accuracy of transport simulations the final grid has 1120 790 cells δx δy 100 m and δt 1 day from the fdm concentrations were sampled at hypothetical well locations and then imposed with unbiased measurement errors 15 c m c fdm 1 σ r a n d where σ is absolute percent error and rand is a uniform r v from 1 1 in inversion the highest σ tested is 5 of maximum concentration in the fdm to assess the accuracy of inversion a root mean square error rms and a relative root mean square error res of concentration were computed at each well location at a given sampling time t 16 r m s t 1 n total i 1 n total c true x i y i t c x i y i t 2 17 r e s t i 1 n total c true x i y i t c x i y i t 2 i 1 n total c true x i y i t 2 where c true x i y i t is error free measured concentration from the fdm c x i y i t is inverted concentration at the same location at the same time and n total is total number of measurements at t from each synthetic well in the fdm solute btc was sampled at two discrete times t 1 t 2 the recovered plume was estimated at these times for which rms and res were computed 3 3 continuous source release for the release history in fig 1a 7 tests were carried out to evaluate the accuracy and stability of inversion under different σ well density sampling pattern and inversion domain size and discretization table 1 for most tests σ was increased from 0 to 5 test 1 employs a large number of sampling wells 2184 distributed in a gridded pattern average well spacing is 2 km for this test a relatively large inversion grid 56 40 was used coarsened from the true model 1120 790 by 20 times in each dimension true α l and α t were assigned as prior information for inversion tests 2 4 use fewer 329 sampling wells following the pattern of the texas aquifer a coarsened inversion grid 14 10 was used to test sensitivity of inversion to the assumed values of α l and α t these were modified by 1 order of magnitude introducing errors into inversion compared to test 2 test 5 further reduces sampling density although wells were placed in a gridded pattern for tests 1 5 observed concentrations were sampled at t 1 1 6 106 and t 2 1 8 106 days since release of the solute for most wells these times largely captured the late time portion of the btcs in tests 6 7 btcs at t 1 4 0 104 and t 2 4 1 104 days were sampled from the fdm which captured the early time portion of the btcs accordingly a smaller domain closer to the source location was used x 5 45 km y 35 75 km in inversion along with a smaller grid 10 10 test 6 adopts the field network while test 7 uses a gridded pattern for all tests rms and res of the recovered concentrations at t 1 are tabulated at t 2 similarly valued error measures were computed not shown accuracy of plume recovery is evaluated by rms and res and by comparing the recovered plume against that simulated by fdm fig 2 for all test cases inversion outcomes are stable and compare favorably with the true plume and are relatively insensitive to measurement error inversion grid discretization and dispersivities prior information in contrast the number of wells used to condition inversion has the largest impact on accuracy test 1 with the largest number of observation wells yields the lowest rms and res for σ 0 they vary from 6 5 10 3 and 5 5 10 3 when sampling density is reduced inversion becomes less accurate moreover even though test 5 has a lower density than test 2 its rms and res are actually lower this is attributed to the gridded sampling pattern with improved information content to test this idea two additional cases not listed in table 1 were created for the problem domain of tests 1 5 one used an irregular network from the texas aquifer with 1650 wells including both active 329 and abandoned wells 1321 from the fdm 529 of the 1650 wells were observed to provide positive btcs the other used gridded sampling with a total of 609 wells for which only 191 wells have positive btcs for both error free and increased errors in measured concentrations plume recovery is more accurate for the gridded network irregular sampling likely suffers from clustering whereby closely spaced wells with similar measurements exert undue influences on data conditioning during inversion further research will evaluate optimal weights for which de clustering methods will be investigated li et al 2018 in tests 6 and 7 the fdm plume was sampled at earlier times when the plume was smaller inversion adopts a smaller computational domain with correspondingly fewer wells that lie close to the source location for test 6 plume was sampled following the field network test 7 sampled the same plume in a gridded pattern at t 1 4 0 104 day only 11 test 6 and 16 wells test 7 have positive btcs while the remaining 54 test 6 and 84 wells test 7 have no btc as they lie outside the plume inversion is fairly accurate and robust for both tests not shown although test 7 gridded network yields a slightly better recovered plume 3 4 pulse releases uniform gaussian multimodal for a pulse release scenario fig 1b test 8 inversion was carried out using the gridded sampling of test 1 because less solute mass was released plume extent was more limited and t 1 1 0 104 days and t 2 8 0 105 days were chosen as sampling times though 2184 observation wells were used in inversion only a very small subset 4 at t 1 and 24 at t 2 has positive btcs result was stable under increasing σ fig 3 although accuracy is lower at t 1 when only 4 wells have btcs at t 1 which concurs with the end of tracer release inversion has largely identified the release location despite the inaccurate plume shape at t 2 the plume evolved to be larger and migrated further downstream more wells have positive btcs and accordingly the inverted plume was very accurate in addition using the sampling pattern of test 2 field network the pulse release was inverted again because none of the wells have positive btcs at t 1 and t 2 inversion cannot recover the plume as expected in tests 9 and 10 release history based on a single to multiple superimposed gaussian functions fig 1c d was simulated by the fdm and subsequently inverted using the gridded sampling of test 1 table 1 while true dispersivities were given as prior information σ was again increased both rms and res and plume recovery suggest that accurate inversion can be obtained for these cases for test 10 inversion was able to capture the bi modal plume shape that had evolved in both early and late times fig 4 4 conclusion a new inverse method based on approximating solute concentration and mass flux with local approximate solutions las is proposed for estimating plume trajectory and source release location under unknown solute transport bc at a given inversion time the method imposes concentration and flux continuities at a set of collocation points in space while the las are conditioned to measured breakthrough concentrations by enforcing transport physics at selected points in space and time the inverse problem becomes well posed a single system of equations is assembled and solved with a parallel linear solver unlike the majority of existing techniques plume trajectory and source location can be recovered efficiently because the new method does not require the repeated simulations of a forward transport model for an unconfined aquifer with a moderately heterogeneous flow field the new method was demonstrated by inverting synthetic concentration breakthroughs from well networks sampling a numerical true model different measurement errors and source release histories e g uniform in time single and multiple pulses were evaluated results suggest that for the source release histories tested 1 inversion is stable under increasing measurement errors up to 5 of the maximum observed concentrations 2 accurate plume recovery and source release location can be attained from the btc data 3 inversion accuracy is the most sensitive to the sampling well density and its information content for the cases investigated herein the recovered plume also appears relatively insensitive to the assumed dispersivity values this work extends our previous research inverting transient groundwater flow under unknown fluid flow initial and bc jiao and zhang 2014a b 2015b to contaminant source identification unlike flow inversion where parameters were estimated along with the hydraulic head dispersivities were not estimated dispersivities were assumed known as prior information for inversion using values considered representative at the inversion grid scale parameter estimation in transport inversion requires measured solute mass fluxes both advective and dispersive components which are not generally available from the field advancement in measurement technology could extend the capability of the current technique to joint parameter and state estimation similar to the fluid flow inversion problems future work will also investigate joint flow and transport inversion by conditioning the las to both hydraulic and concentration measurements in this work for computational efficiency solute was sampled two times at each monitoring well location which allows the recovery of the plume history over time this amount of data however is not sufficient to recover the source release history for which denser measurements in time are likely needed future work will aim to address this for which computational efficiency is key moreover uncertainty of the flow field which gives rise to uncertainty in inferring the plume history will also be investigated declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we acknowledge the financial support by the nsf hydrological sciences ear 1702078 we thank he huang and en jui lee for comments of the parallel code this is a theoretical work for which no data need be made available 
6204,a new solute transport inverse method is proposed for estimating plume trajectory and source release location under unknown solute transport boundary conditions in a steady state non uniform groundwater flow field solute concentration is modeled by proposing a set of local approximation solutions las of transport that are discretized over the problem domain at a given time step the inverse method imposes continuities of concentration and total solute mass flux at a set of collocation points in the inversion grid whereas the las are conditioned to measured breakthrough concentrations by enforcing transport physics at selected points in space and time the inverse problem becomes well posed and a single system of inversion equations is assembled and solved with a parallel iterative solver unlike most of the inversion techniques that minimize a model data mismatch objective function the inverse method does not require the simulation of a forward transport model for optimization thus both solute initial and boundary conditions can be recovered assuming dispersivity estimates are available the method was demonstrated using synthetic breakthrough data from various sampling densities and designs i e irregular versus uniformly spaced well networks different measurement errors and source release histories e g uniform in time single and multiple pulses were also investigated results suggest that for the source release histories tested 1 inversion is stable under increasing measurement errors up to 5 of the maximum observed concentration 2 accurate plume trajectory and source release location can be estimated from solute breakthrough concentrations 3 inversion accuracy appears the most sensitive to sampling well density and its information content keywords solute transport contaminant source identification las inverse method source release history boundary conditions 1 introduction worldwide groundwater contamination from point and non point sources is widespread in the u s cleanup of over 300 000 soil and groundwater sites is projected to cost 200 billion by 2033 national research council 2013 identification of pollutant pathways and sources is of particular importance for remediation design and the long term planning and management of contaminated sites various techniques have been proposed for identifying contaminant pathways and sources and they can be divided into two general groups 1 solute transport is solved with a forward or backward in time numerical model with which plume trajectory is recovered by optimizing a model date mismatch or objective function chadalavada et al 2012 mirghani et al 2012 yeh et al 2016 michalak and kitanidis 2004 prakash and datta 2013 sun 2007 because transport is dispersive and irreversible inverse modeling using reverse time is considered ill posed skaggs and kabala 1994 on the other hand when a forward model is used solute transport is simulated with a set of assumed initial and boundary conditions bc in order to provide simulated measurements to match with observations for aquifers where hydraulic conductivity k is heterogeneous various techniques have been extended to invert transport in non uniform flows to account for uncertainty in inversion geostatistical methods have also been developed 2 solute transport is inverted using analytical solutions and regression techniques alapati and kabala 2000 bagtzoglou and atmadja 2005 sun et al 2006 woodbury and ulrych 1996 bagtzoglou 2003 neupauer et al 2000 to develop such solutions to describe plume migration flow fields are usually considered uniform importantly both groups of techniques assume that solute transport bc are known a priori in order to develop numerical and analytical solutions for optimization at aquifer sites with limited or incomplete bc information application of these techniques can lead to non unique inverse solutions ayvaz 2016 to identify contaminant trajectory and source in a non uniform flow field this study proposes a new inverse method by developing local approximate solutions las of concentration and by conditioning them to solute breakthrough measurements key difference between this new approach and the majority of existing inversion techniques is that standard regression techniques are not employed thus a solute transport forward model does not need to be built and simulated in order to optimize a model data mismatch and as a result non uniqueness in inversion that is due to an incorrect assumption of transport bc is eliminated the same issue in groundwater flow inversion exists which has been demonstrated in irsa and zhang 2012 this article explains the new transport inverse method for estimating the state variable i e concentration and its spatial temporal evolution assuming appropriate dispersivity data are available as prior information for inversion the method was verified using synthetic concentration breakthroughs with or without measurement errors that were sampled from a true model that simulated several source release histories at a given source location note that the true model can be an analytical or numerical solution of the solute transport equation subject to a set of known initial and bc or it can be a physical system created in a laboratory in this article because transport in non uniform flows is of interest a numerical model was adopted as the true model to test the inverse method performance of the new inverse method was evaluated using various sampling densities and designs i e irregular versus uniform well networks the irregular network was based on monitoring wells from a shallow unconfined aquifer in the texas high plains where water level and core hydrogeological measurements were available this aquifer which overlies 4 texas counties parmer castro bailey lamb extends to a depth 30 m and consists of unconsolidated coarse sand gravel minor clays and carbonate minerals seni 1980 from the numerous lithology logs and limited core measurements the aquifer is moderately heterogeneous where k varies over 3 orders of magnitude http www twdb texas gov groundwater data index asp water level data came from usgs national water information system https waterdata usgs gov nwis nwis and have been quality checked by usgs with an approval status of processing and review completed from the water levels overall groundwater flow direction is from west to east in the reminder of this article the true model and the inverse method are described first followed by a section describing application of the new method to plume recovery for different source release histories e g uniform in time single and multiple pulses breakthrough data with varied measurement quality density and distribution were tested we then summarize the outcomes while pointing out future research directions 2 method a new inverse method is proposed for identifying solute plume history and source release location under unknown solute transport bc the solute is assumed to be dilute non reactive and is migrating in a steady state non uniform flow field variable density coupling with flow is ignored note that groundwater flow and concentration data can be assimilated using a sequential inversion approach 1 given hydraulic head and local k measurements flow inversion is performed to recover a steady state non uniform flow field of an unconfined aquifer jiao and zhang 2014a 2015a b 2 given the flow field solute plume history and source release location are identified by conditioning the estimated plume trajectory to observed breakthrough curves btc at wells for the above steps described both bc for flow inversion and initial and bc for transport inversion are unknown two separate grids can be used based on measurement locations e g one for flow inversion using wells with hydraulic data and one for transport inversion using wells with btc data as the flow inversion step has been previously described this work focuses on plume identification within the estimated flow field below equation describing the solute transport true model is presented first followed by a description of the inverse method groundwater is assumed to have negligible vertical flow thus transport inversion is performed in 2d on the horizontal plane moreover the physics governing solute transport is assumed known and is described by the advection dispersion equation ade to verify the inverse method numerical transport experiments true model were performed solving the ade subject to different source release scenarios to provide synthetic measurements for inversion 1 c x y t t x α l v d s c x y t x y α t v d s c x y t y x c x y t v x x y y c x y t v y x y o n ω 2 c x y 0 0 o n ω 3 c x y t c 0 x y o n γ 1 t 0 where ω is computational domain c is concentration in mass per unit pore water volume m l3 d s is porous medium diffusion coefficient v v x x y v y x y is average linear velocity l t v is related to darcy flux q q x x y q y x y via effective porosity θ v q θ which is assumed a known constant α l and α t are local longitudinal and transverse dispersivities respectively assumed known as priori information we ignore spatial variation of α l and α t because the site k field resolved by flow inversion is moderately heterogeneous finally γ 1 is dirichlet bc where zero concentration is assigned at the inflow boundary zero mass flux is assigned to the top and bottom while outflow bc are imposed on the remaining boundary so solute can migrate out of the domain for the same computational domain ω transport inversion enforces 3 sets of constraints 1 global continuity of concentration and total solute flux mass at a set of collocation points at a given discretized time 2 local conditioning of the inverse solutions of the given time to observed breakthrough concentrations at the same time 3 equation constraints that enforces the transport physics at selected points in space and time the first set of constraints is written as 4 w n p γ r c n p γ δ n p γ d γ m 0 m 1 y γ 1 v 5 w n p γ r j n p γ δ n p γ d γ m 0 m 1 y γ 1 v where n denotes a discretized inversion time p γ denotes a collocation point on mth cell interface γ m in the inversion grid γ is the number of collocation points on γ m r c n p γ and r j n p γ are residuals of solute concentration and total solute mass flux at p γ on γ m at the nth time respectively y is the number of cell interfaces in the grid δ n p γ is dirac delta function at the nth time and w n p γ is weighting function which samples the residuals at p γ on γ m at the nth time details on how the weight is implemented can be found in zhang et al 2014 the residuals can be expanded as 6 r c n p γ c i n p γ c k n p γ 7 r j n p γ j i n p γ j k n p γ 8 j n v c n d c n where c is a proposed las that varies with space and time c i n and c k n denote concentrations of cells i and k adjacent to γ m at the nth time d is the dispersion tensor j n is a proposed las for total solute mass flux j n depends on c and c thus is space and time dependent for a two dimensional problem the total mass flux can be written as bear and cheng 2010 j n j x n j y n v x x y c n α l v d s c n x v y x y c n α t v d s c n y the second set of constraints is defined by conditioning c n to measured concentrations 9 w n p a c n p a c ob n p a 0 a 1 a where pa is a measurement point c ob n is observed concentration at pa at the nth time a is total number of observations and w n p a is a weighting function at the nth time assigned to each observation equation to reflect the magnitude of measurement errors w n p a can be time dependent i e becomes larger if measurement quality improves with time the inverse method assumes that the governing transport equation is known which provides a set of equation constraints for inversion 10 w n p e r e n ε r e n c n t x α l v d s c n x y α t v d s c n y x c n v x y c n v y e e 1 y v a where pe include both collocation points and measurement locations pe can be time dependent which is not implemented here r e is the equation residual at pe at the nth time for a given time eq 10 enforces a set of physical constraints on the las of c n and j n at pe the system of inverse equations is solved using an iterative solver where ε is on the order of 10 5 or smaller in eqs 4 10 both las of c n and j n are approximate rather than exact solutions which allows a flexible handling of parameters initial and bc while eq 10 provides physics based constraints on the approximations in the solution domain v x x y and v y x y are obtained from flow inversion and are deterministic coefficients in the las for each inversion grid cell ω e polynomial functions were chosen as the las 11 c n x y t a 1 n a 2 n t n a 3 n a 4 n t n x a 5 n a 6 n t n y a 7 n a 8 n t n x y where a i n i 1 8 are unknown time dependent coefficients that together control the shape of the las of concentration following eq 11 total mass flux can be approximated as 12 j x n x y t c n v x α l v d s c n x a 1 n a 2 n t n a 3 n a 4 n t n x a 5 n a 6 n t n y a 7 n a 8 n t n x y v x α l v d s a 3 n a 4 n t n a 7 n a 8 n t n y 13 j y n x y t c n v y α t v d s c n y a 1 n a 2 n t n a 3 n a 4 n t n x a 5 n a 6 n t n y a 7 n a 8 n t n x y v y α t v d s a 5 n a 6 n t n a 7 n a 8 n t n x following eq 11 r e n i n eq 10 is rewritten as 14 a 2 n a 4 n x a 6 n y a 8 n x y v x a 3 n a 4 n t n a 7 n a 8 n t n y v y a 5 n a 6 n t n a 7 n a 8 n t n x 0 for the discretized inversion spatial and temporal domains eqs 4 10 can be assembled to form a single system of equations that can be solved using standard techniques a scalable parallel linear solver based on the message passing interface scalable parallel lsqr huang et al 2013 is implemented for its solution for which the inversion coefficient matrix is decomposed to a kernel sub matrix and a damping sub matrix to reduce overlapping information this decreases inter processor communication and achieves computational efficiency wang 2014 for a trial transport inversion problem a serial solver and the parallel solver were both employed yielding nearly identical results 3 results 3 1 computational domain a regional scale 2d computation domain 112 km e w and 79 km n s was selected to reflect realistic monitoring well designs from an unconfined aquifer in texas high plains before transport inversion was performed steady state flow was inverted to recover long term water table and darcy fluxes by conditioning to core k and water level data from 329 active wells to obtain groundwater velocity an average effective porosity from core measurements was used because field measurements are never exhaustive uncertainty exists in the inverted flow field however it is treated here as deterministic and is imported into the transport analysis for both the true forward model and the inverse analyses specifically based on observed concentrations prior dispersivity estimates and the groundwater velocity solute concentration was inverted without the knowledge of both solute initial and boundary conditions moreover based on site lithology data and mean groundwater velocity an average pe number of 1 0 was calculated assuming the molecular diffusion coefficient of common dissolved hydrocarbons thus for the modeled site solute transport is advection dominated perkins and johnston 1963 batu 2005 and diffusion is ignored in both the forward model and the inverse analysis 3 2 numerical true model four transport models were simulated in the domain solving eqs 1 3 using mt3dms zheng 2010 in each model a tracer was released in upstream flow field x 19 1 19 9 km y 55 2 56 1 km near the left boundary and migrated towards the right boundary one model simulated continuous constant in time source release for a period of 1 8 106 days which is also the total simulation time fig 1 a for a shorter simulation time of 8 105 days the second model simulated a short 104 day release of the tracer at the same location fig 1b during the release time a constant concentration of 10 mg l is specified at the source location in both models for the simulation time of 8 105 days two additional release scenarios were simulated with a gaussian and multimodal release at the source respectively fig 1c d in the last example 4 gaussian distributions were superimposed at the source to create 4 peak concentrations over time in all simulations α l and α t of 0 1 km and 0 03 km respectively were assigned to the grid cells gelhar et al 1992 for the true model mt3dms implements the finite difference scheme fdm for spatial and temporal discretization various discretizations were experimented to ensure the accuracy of transport simulations the final grid has 1120 790 cells δx δy 100 m and δt 1 day from the fdm concentrations were sampled at hypothetical well locations and then imposed with unbiased measurement errors 15 c m c fdm 1 σ r a n d where σ is absolute percent error and rand is a uniform r v from 1 1 in inversion the highest σ tested is 5 of maximum concentration in the fdm to assess the accuracy of inversion a root mean square error rms and a relative root mean square error res of concentration were computed at each well location at a given sampling time t 16 r m s t 1 n total i 1 n total c true x i y i t c x i y i t 2 17 r e s t i 1 n total c true x i y i t c x i y i t 2 i 1 n total c true x i y i t 2 where c true x i y i t is error free measured concentration from the fdm c x i y i t is inverted concentration at the same location at the same time and n total is total number of measurements at t from each synthetic well in the fdm solute btc was sampled at two discrete times t 1 t 2 the recovered plume was estimated at these times for which rms and res were computed 3 3 continuous source release for the release history in fig 1a 7 tests were carried out to evaluate the accuracy and stability of inversion under different σ well density sampling pattern and inversion domain size and discretization table 1 for most tests σ was increased from 0 to 5 test 1 employs a large number of sampling wells 2184 distributed in a gridded pattern average well spacing is 2 km for this test a relatively large inversion grid 56 40 was used coarsened from the true model 1120 790 by 20 times in each dimension true α l and α t were assigned as prior information for inversion tests 2 4 use fewer 329 sampling wells following the pattern of the texas aquifer a coarsened inversion grid 14 10 was used to test sensitivity of inversion to the assumed values of α l and α t these were modified by 1 order of magnitude introducing errors into inversion compared to test 2 test 5 further reduces sampling density although wells were placed in a gridded pattern for tests 1 5 observed concentrations were sampled at t 1 1 6 106 and t 2 1 8 106 days since release of the solute for most wells these times largely captured the late time portion of the btcs in tests 6 7 btcs at t 1 4 0 104 and t 2 4 1 104 days were sampled from the fdm which captured the early time portion of the btcs accordingly a smaller domain closer to the source location was used x 5 45 km y 35 75 km in inversion along with a smaller grid 10 10 test 6 adopts the field network while test 7 uses a gridded pattern for all tests rms and res of the recovered concentrations at t 1 are tabulated at t 2 similarly valued error measures were computed not shown accuracy of plume recovery is evaluated by rms and res and by comparing the recovered plume against that simulated by fdm fig 2 for all test cases inversion outcomes are stable and compare favorably with the true plume and are relatively insensitive to measurement error inversion grid discretization and dispersivities prior information in contrast the number of wells used to condition inversion has the largest impact on accuracy test 1 with the largest number of observation wells yields the lowest rms and res for σ 0 they vary from 6 5 10 3 and 5 5 10 3 when sampling density is reduced inversion becomes less accurate moreover even though test 5 has a lower density than test 2 its rms and res are actually lower this is attributed to the gridded sampling pattern with improved information content to test this idea two additional cases not listed in table 1 were created for the problem domain of tests 1 5 one used an irregular network from the texas aquifer with 1650 wells including both active 329 and abandoned wells 1321 from the fdm 529 of the 1650 wells were observed to provide positive btcs the other used gridded sampling with a total of 609 wells for which only 191 wells have positive btcs for both error free and increased errors in measured concentrations plume recovery is more accurate for the gridded network irregular sampling likely suffers from clustering whereby closely spaced wells with similar measurements exert undue influences on data conditioning during inversion further research will evaluate optimal weights for which de clustering methods will be investigated li et al 2018 in tests 6 and 7 the fdm plume was sampled at earlier times when the plume was smaller inversion adopts a smaller computational domain with correspondingly fewer wells that lie close to the source location for test 6 plume was sampled following the field network test 7 sampled the same plume in a gridded pattern at t 1 4 0 104 day only 11 test 6 and 16 wells test 7 have positive btcs while the remaining 54 test 6 and 84 wells test 7 have no btc as they lie outside the plume inversion is fairly accurate and robust for both tests not shown although test 7 gridded network yields a slightly better recovered plume 3 4 pulse releases uniform gaussian multimodal for a pulse release scenario fig 1b test 8 inversion was carried out using the gridded sampling of test 1 because less solute mass was released plume extent was more limited and t 1 1 0 104 days and t 2 8 0 105 days were chosen as sampling times though 2184 observation wells were used in inversion only a very small subset 4 at t 1 and 24 at t 2 has positive btcs result was stable under increasing σ fig 3 although accuracy is lower at t 1 when only 4 wells have btcs at t 1 which concurs with the end of tracer release inversion has largely identified the release location despite the inaccurate plume shape at t 2 the plume evolved to be larger and migrated further downstream more wells have positive btcs and accordingly the inverted plume was very accurate in addition using the sampling pattern of test 2 field network the pulse release was inverted again because none of the wells have positive btcs at t 1 and t 2 inversion cannot recover the plume as expected in tests 9 and 10 release history based on a single to multiple superimposed gaussian functions fig 1c d was simulated by the fdm and subsequently inverted using the gridded sampling of test 1 table 1 while true dispersivities were given as prior information σ was again increased both rms and res and plume recovery suggest that accurate inversion can be obtained for these cases for test 10 inversion was able to capture the bi modal plume shape that had evolved in both early and late times fig 4 4 conclusion a new inverse method based on approximating solute concentration and mass flux with local approximate solutions las is proposed for estimating plume trajectory and source release location under unknown solute transport bc at a given inversion time the method imposes concentration and flux continuities at a set of collocation points in space while the las are conditioned to measured breakthrough concentrations by enforcing transport physics at selected points in space and time the inverse problem becomes well posed a single system of equations is assembled and solved with a parallel linear solver unlike the majority of existing techniques plume trajectory and source location can be recovered efficiently because the new method does not require the repeated simulations of a forward transport model for an unconfined aquifer with a moderately heterogeneous flow field the new method was demonstrated by inverting synthetic concentration breakthroughs from well networks sampling a numerical true model different measurement errors and source release histories e g uniform in time single and multiple pulses were evaluated results suggest that for the source release histories tested 1 inversion is stable under increasing measurement errors up to 5 of the maximum observed concentrations 2 accurate plume recovery and source release location can be attained from the btc data 3 inversion accuracy is the most sensitive to the sampling well density and its information content for the cases investigated herein the recovered plume also appears relatively insensitive to the assumed dispersivity values this work extends our previous research inverting transient groundwater flow under unknown fluid flow initial and bc jiao and zhang 2014a b 2015b to contaminant source identification unlike flow inversion where parameters were estimated along with the hydraulic head dispersivities were not estimated dispersivities were assumed known as prior information for inversion using values considered representative at the inversion grid scale parameter estimation in transport inversion requires measured solute mass fluxes both advective and dispersive components which are not generally available from the field advancement in measurement technology could extend the capability of the current technique to joint parameter and state estimation similar to the fluid flow inversion problems future work will also investigate joint flow and transport inversion by conditioning the las to both hydraulic and concentration measurements in this work for computational efficiency solute was sampled two times at each monitoring well location which allows the recovery of the plume history over time this amount of data however is not sufficient to recover the source release history for which denser measurements in time are likely needed future work will aim to address this for which computational efficiency is key moreover uncertainty of the flow field which gives rise to uncertainty in inferring the plume history will also be investigated declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we acknowledge the financial support by the nsf hydrological sciences ear 1702078 we thank he huang and en jui lee for comments of the parallel code this is a theoretical work for which no data need be made available 
