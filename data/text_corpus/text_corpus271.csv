index,text
1355,thermal enhancement is known to be an efficient way to decrease the residual saturation of some common dense non aqueous phase liquids dnapls after pumping however the effect of transient heat transfer during the recovery of a high viscosity contaminant such as coal tar in highly permeable porous media is still unknown a 2d tank experimental setup allowing monitoring of temperature and saturation fields during dnapl pumping has been developed experiments were run under isothermal and non isothermal conditions at low and high flow rates we investigated the presence of viscous fingering and how that influences the shape of the cone of depression as well as the residual saturation the saturation fields show that less viscous fingering occurs in pre heated cases and that heating increases the recovery efficiency increasing the temperature increases the critical velocity and the viscosity ratio and helps to stabilize the interface between the non wetting and wetting phase observations were first made on an oil and ethanol fluid pair because its properties were known before extending the experiments to a coal tar and water fluid pair residual oil saturation after pumping was decreased by 6 16 in all pre heated conditions pumping at low flow rate in these conditions leaves the smallest oil residual saturation 20 after pumping a low flow rate increases the recovery efficiency by reducing viscous fingering and by spreading the generated heat to a larger part of the tank finally results on coal tar pumping show that the high thermal conductivity of water helps in keeping the temperature high during pumping the residual coal tar saturation was reduced from 40 at 20 c to 28 when pre heating the tank operating at a low flow rate and with a uniform temperature is the key to recovering the highest amount of a viscous dnapl such as coal tar from the soil and satisfying cleanup goals when using thermally enhanced pumping keywords dense non aqueous phase liquid two phase flow non isothermal 2d tank thermally enhanced dnapl recovery coal tar 1 introduction the remediation of sites contaminated by dense non aqueous phase liquid dnapl remains an open subject these phases are composed of substances that are generally toxic and carcinogenic among dnapls are coal tars with complex liquid phases they are composed of monocyclic and polycyclic aromatic hydrocarbons pahs as well as phenolic compounds lee et al 1992 previous characterization studies have shown that coal tars vary according to origin the coal tar used in this study is the same as the one studied recently by johansson et al 2020 iravani et al 2020 philippe et al 2020 pump and treat technology remains the most widely used to remediate high saturation coal tar sources in general remediation efficiency is very low for dnapls and this method is technologically and economically unsatisfactory usepa 1996 russell and rabideau 2000 kavanaugh and kresic 2008 navy 2008 newell et al 2011 this low efficiency flows from a combination of factors i coal tar is denser than water it flows along the most permeable local paths often in the form of fingerings to impermeable horizons and then forms deep source zones ii when recovering coal tar by pumping the unfavorable viscosity ratio causes water to infiltrate in the form of fingerings these fingers gradually trap significant volumes of coal tar until the water starts to be recovered from the pumping well at this stage only a tiny amount often mixed in water of coal tar is collected that no longer allows the soil to be decontaminated iii some coal tars have non newtonian behavior fitzer et al 1987 but the coal tar used in this study showed newtonian behavior philippe et al 2020 thermal desorption techniques have already proved their effectiveness for the remediation of dnapl contaminated soils heat treatment can be highly effective for volatile organic compounds vocs with efficiencies of up to 99 even in highly contaminated sources renoldi et al 2003 johnsen et al 2005 for example heron et al 1998 succeeded in extracting 99 8 of tce from a silty soil 50 cm deep using this method heron et al 1998 recently the same type of process was applied to pah contaminated sites with an equally efficient yield by reaching soil temperatures of 325 c baker et al 2006 nevertheless this technique has a significant financial cost because of the high energy required mcdade et al 2005 baston et al 2010 another technique quite similar consists in burning the pollutant when it is flammable which happens to be the case of coal tar the smoldering fire provoked during this combustion can incinerate coal tar source zones with yields as high as thermal desorption techniques in unsaturated zones switzer et al 2009 hasan et al 2015 nowadays this technique has already been applied to coal tar in the field with a mass concentration of pahs decreased by 97 scholes et al 2015 smoldering fire is however a type of combustion whose fundamental understanding is still limited and whose environmental impact on the soil has not yet been studied rein 2009 however these techniques generate temperatures higher than 100 c boiling temperature of water on the contrary here thermally enhanced dnapl recovery focuses on temperatures lower than 100 c to avoid contaminant evaporation the goal is to reduce the quantity of residual dnapl formed during pumping that could require further treatment or management we are interested in the pumping of coal tars and the means of recovering a larger quantity of contaminants therefore the objective is to increase the recovery rate by reducing fingerings as little coal tar as possible is trapped during pumping a similar problem has already been faced in petroleum engineering in relation to the recovery of heavy oils the most appropriate solution is to heat the medium and the liquids to reduce the fluid fluid viscosity ratios koci et al 1989 wang et al 2006 most tests in petroleum engineering demonstrate the effectiveness of heating and its feasibility at the laboratory scale whether by external heating or by injecting hot water however they represent an essentially one dimensional flow in a horizontal or vertical column and do not consider the 2d flow complexity of a real pollution under non isothermal conditions the change of dimension from 1d columns to a 2d tank scale is a recent subject of study on dnapl wilking et al 2013 followed the infiltration and dissolution of a dnapl trichloroacetic acid and obtained saturation fields using x ray tomography technology wilking et al 2013 more recently dnapl saturation has been monitored in transparent media thanks to a calibration between the optical density and the dnapl volume fraction these techniques are referred to as the light reflection method lrm and light transmission method ltm for instance alazaiza et al 2016 used lrm to monitor progressive infiltration of pce alazaiza et al 2016 darnault et al 1998 similarly followed the oil water system using a blue dye with ltm darnault et al 1998 heiderscheidt et al 2008 also used ltm to study in situ oxidation on sand impacted by pce heiderscheidt et al 2008 colombano et al 2020 demonstrated that from lrm the residual saturation of chlorinated solvents in 2d tank pumping experiments could be estimated with good accuracy r2 0 95 monitoring by imaging allows the study of transient phenomena linked to dnapl flows in porous media colombano et al 2021 for example explored the remediation of chlorinated solvents by increasing the system s temperature and chemical enhancement using surfactants in 0 1 mm and 0 5 mm glass beads colombano et al 2021 no temperature effect has been reported in this study as their dnapl s dynamic viscosity and surface tension did not vary with temperature however coal tars are much more viscous than most dnapls including chlorinated solvents moreover coal tars dynamic viscosity decreases greatly with temperature philippe et al 2020 also colombano et al 2021 only worked in isothermal conditions their recovery by pumping can therefore change the observed results pankow and cherry 1996 li and schwartz 2004 childs et al 2006 moreover thermal techniques for soil remediation have also already been studied on this scale heat transfers in a small scale two dimensional experimental tank 60 cm 45 cm 1 cm filled with 0 5 mm glass beads and a large scale tank 2 4 m 1 2 m 0 15 m filled with sandy soil have been investigated by krol et al 2011 furthermore johnson et al 2011 studied the potential of the electrical resistance heating erh technique at low temperature 30 50 c to enhance the clean up of tce and pce sources in the same large scale tank johnson et al 2011 the authors concluded that pre heating groundwater and a tce or pce source is an efficient way of promoting its recovery in highly permeable media also the generated heat can enhance in situ biodegradation or chemical reactions afterward however no existing study considers the transient temperature and saturation variations together during the pumping of a high viscosity fluid in highly permeable porous media which is the main goal of our study the coal tar used here comes from a seriously polluted site in france due to its characteristics this coal tar is not compatible with imaging techniques it sticks on the glass walls and is completely opaque to the light of the visible spectrum thus we conducted most of our studies using the canola oil ethanol model pair proposed by philippe et al 2020 its properties were recently approved in philippe et al 2020 as well as the reduced residual saturation with increased temperature the presence of viscous fingering generated due to the dynamic effect as opposed to static experiments was found to be responsible for the high residual saturation in the system increasing the temperature reduces viscous fingering and increases ethanol film presence residual oil saturation falls as a consequence philippe et al 2020 in two phase flow in porous media viscous fingering may appear when the viscosity ratio m is unfavorable 1 and the velocity in the experiment is higher than a critical velocity uc where m and uc are defined as lenormand et al 1988 glass and nicholl 1996 1 m μ inv μ dis 2 u c ρ inv ρ dis g k φ μ inv μ dis where μ inv pa s and ρ inv kg m 3 are the dynamic viscosity and density of the invading phase μ dis pa s and ρ dis kg m 3 the dynamic viscosity and density of the displaced phase k m2 is the permeability of the porous medium and φ its porosity g 9 8 m s 2 is the conventional value of gravitational acceleration we investigated the presence of viscous fingering and how that influences the shape of the cone of depression as well as the residual saturation the experimental development and most of the conclusions were obtained on the oil ethanol liquid pair the coal tar water pair was also studied as an industrial application our main objective was to study the coupling between temperature and immiscible two phase flow in pumping conditions the porous medium consists of homogeneous glass beads with high permeability to avoid any other disturbing phenomena first the experiments were carried out on the oil ethanol pair to work with simple fluids whose properties are well known and which allow optical monitoring then the pumping of dnapl in the saturated zone was carried out as an industrial application to observe similarities between two pairs of immiscible fluids in practice during these experiments the saturation s was followed by time domain reflectometry tdr local small volumes and imaging 2d fields the temperature t was followed using thermocouples we focused on i the shape of the line delimiting the two continuous phases that referred to as the interface and represented by a pumping cone cone of depression ii the saturation during pumping and the residual saturation after pumping iii the frequency and appearance of the fingerings encountered and iv the recovery rates and volumes 2 experimental setup and procedures 2 1 experimental setup the experimental setup consists of a decametric 2d tank with a pump a camera and a well instrumented monitoring system wetting and non wetting phases are injected into the same 1 mm homogeneous glass beads used in philippe et al 2020 and pumped under isothermal circulation of cold or hot water around 2d tank and non isothermal heating element conditions the mean porosity φ and permeability k of the glass beads used are 0 4 and 3 5 10 10 m2 respectively saturation and temperature were monitored during the heating and pumping steps fig 1 the 2d tank height 30 cm width 50 cm thickness 7 cm made of polyvinylidene fluoride pvdf fabricated by scodip was used to carry out all the pumping experiments fig 2 the front of the tank is transparent glass 1 cm thick so that the environment can be photographed at different times photographs are taken to obtain saturation fields during the pumping experiments the apparatus consists of a central reservoir containing the porous medium and side channels counter channels regulating the static levels of the liquids used the counter channels are connected to the central reservoir by a metal grid which allows fluids to pass but blocks the glass beads inside the central reservoir side and bottom accesses allow connection of pipes and injection or pumping of liquids the internal diameter of the pumping access points is 3 125 cm the tank is equipped with a double jacket in back part and double glazing in front allowing experiments to be carried out in isothermal conditions by circulating regulated temperature water using a thermostatic bath lauda model eco re 420 heated water circulates through the double jacket and double glazing to keep the temperature steady during isothermal experiments a heating element 350 w 0 7 w cm 2 made of stainless steel and designed by vulcanic is added inside the tank near its left wall cavity to create non isothermal heating conditions in a real case heating is not uniform due to the size of the field the heating element is used to reproduce more realistic conditions that correspond to non isothermal pumping this element is helical 30 cm long and 6 5 cm in diameter the element heats the porous medium uniformly along the x axis the heating is regulated by a proportional integral derivative controller pid controller using a control box and a pt100 probe connected to the heating element fig 1 even if the heating element is not needed for the isothermal case it is kept inside the tank for all the experiments to ensure the same conditions between isothermal and non isothermal conditions the double jacket and double glazing systems which have been filled with air for non isothermal cases can communicate freely with the air of the experiment room through the inlet and systems this reduces significantly the heat loss from the 2d tank front glass and back parts pumping used a peristaltic pump watson marlow 530 u the liquids recovered are injected directly into a beaker placed on a precision balance sartorius cubis mse 0 1 mg acquisition of the recovered mass is automatic and performed every 6 s we used lrm based on image analysis to obtain a precise relationship between light intensity and saturation field to determine the average dnapl saturation inside the tank a 34 megapixel 7360 4912 nikon d810 digital still camera with a nikkor 105 lens and two 500 w broncolor projectors were used projectors prevented light reflection and shadow and improved the brightness of photographs the imaging system was installed in a dark room to prevent outside light from influencing the brightness of the photographs the tank was also equipped with 15 measuring units consisting of tdr probes 5te meter group to measure the medium relative permittivity and type k thermocouples at the back of the tank provided by tc s a precision 0 4 sensitivity 0 150 s the position of these sensors is shown in fig 2 and represents a network of three rows and five columns a campbell cr 1000 datalogger was used to acquire temperature and relative permittivity measurements in practice tdrs 11 21 and 31 were removed as well as thermocouple 21 because they were physically blocked by the addition of the heating element the diameter of the pumping access point 3 125 cm is around half of the 2d tank thickness 7 cm the saturation fields obtained with lrm are just representative of the saturation on a 2d plane corresponding to the front glass wall of the tank we effectively observed that the wetting phase water or ethanol flows out of the tank while the apex of the cone of depression does not still reach the pumping point this is evidence of a very small 3d effect near the pumping point the difference between the recovered fluid mass using the mass scale and imaging at the end of the experiment proves also this small 3d effect therefore the saturation obtained with lrm will be compared to saturation measured from tdr probes to evaluate the two measuring methods the comparison between both techniques is used to determine potential 3d effects that cannot be captured by the lrm technique the liquid pairs used to carry out these experiments are the same as philippe et al 2020 the coal tar water pair representing in a geometrically simplified manner a layer of dnapl on an impermeable substratum in an aquifer the sticky aspect and the lack of transparency of the coal tar are nevertheless two properties that complicate its study the water used was previously degassed to avoid the presence of residual air in the tank the canola oil ethanol pair serves as a model pair sodium chloride was also dissolved in ethanol 0 65 g l to make measuring electrical resistivity easier in a parallel work iravani et al 2020 2 2 fluid properties the density and the dynamic viscosity of each fluid are specified in table 1 for the coal tar ct water w pair and table 2 for the canola oil o ethanol e pair philippe et al 2020 the measured interfacial tension and the contact angle are also presented we used the pendant drop technique to measure interfacial tension of the fluids by forming a drop of coal tar or oil in a small cuvette filled with degassed water or ethanol contact angles were measured using the sessile drop method by placing a drop of coal tar or oil on the bottom plate of the same quartz glass cuvette previously filled with degassed water or ethanol the low interfacial tension of coal tar water can be related to the presence of amphiphilic substances in the coal tar sampled from a real site barranco and dawson 1999 reported similar low values of interfacial tension of coal tar water at basic ph values ph 9 sanaiotti et al 2010 also reported interfacial tension values between 1 and 3 mn m 1 for soybean oil and water and ethanol mixtures sanaiotti et al 2010 the high 90 values reported of contact angle indicate that coal tar and oil are the non wetting fluids properties and trends of these liquid pairs are similar which supports the use of the canola oil and ethanol pair to model coal tar and water two phase flow 2 3 experimental procedures 2 3 1 setup preparation and heating the heating element was first installed on the left side of the empty tank and initially held straight with a support oil was injected through the five lower inlets using a low flow peristaltic pump simultaneously 10 ml min 1 to an initial height of 1 5 cm from this moment glass beads were progressively added while continuing the oil injection the glass beads were also stirred during injection to homogenize their distribution in the tank the injection and addition were done slowly to avoid any air trapping during this step the oil level was always kept just above the level of the glass beads to also prevent air trapping oil injection stopped when the oil reached a height of 15 cm then ethanol was injected in the same way using the lateral counter channels from above the ethanol injection stopped when the tank was fully saturated all 30 cm during the injection of the liquids precautions were taken to avoid mixing the oil and ethanol and to keep the interface between both liquids at 15 cm as straight as possible fig 1 initially in the case of oil ethanol the oil completely saturates the porous medium from 0 to 15 cm and the same for ethanol from 15 to 30 cm however in the case of the coal tar water application case the tank preparation described above was slightly changed the 2d tank was first fully saturated with water then coal tar was injected through the five lower inlets to perform primary drainage up to a height of 15 cm this was done to better represent a real pollution case which is rarely fully saturated in addition preparing the experiment with coal tar required more attention as a first step the solid particles contained in the coal tar were removed using a glass fiber filter gf d pore size 2 7 μm water present in the collected coal tar during sampling was also removed using a vacuum pump and a 10 μm hydrophobic membrane for both liquid pairs once the tank was full the initial relative permittivity measurements were carried out for 12 h to verify that the tdrs were working correctly initially the tank was at room temperature 20 c in the case of pumping under isothermal conditions a temperature controlled water bath was used to set the tank temperature to 20 c or 35 c the heating element was still added in isothermal cases so that the porosity was the same as in non isothermal experiments in non isothermal conditions the temperature of the heating element was set to 60 c with the temperature control unit pid controller the tank was heated until a steady state was reached this step lasts about 8 h during the heating phase the temperature of the tank and the permittivity values were measured with sensors directly in contact with the porous medium inside the tank 2 3 2 pumping procedures pumping used a flexible tube diameter 3 125 mm located on the lower central inlet of the tank the rotation speed was fixed at 84 3 rpm or 25 rpm the corresponding pumping rates q of these rotations for water and ethanol were 100 ml min 1 and 20 ml min 1 respectively the measured pumping rate for the same rotation rates but for canola oil was lower than water and ethanol at 84 3 rpm 70 ml min 1 and the same at 25 rpm 20 ml min 1 the mass of recovered liquid was measured continuously with an automatic acquisition mass balance every 6 s the time at which the ethanol begins to be recovered was also noted and is defined as the breakthrough time photographs of the tank were taken using the light reflection method lrm technique during the pumping step raw images and data were saved in nikon format every three minutes at 100 ml min 1 and 5 min at 20 ml min 1 the pumping step lasted until the stationary state was reached which was indicated by the exclusive recovery of the wetting phase ethanol or water and a stationary depression cone the photographs were then converted into tiff format with capture one software the images were finally processed with the open source software fiji to calibrate light intensity and saturation according to the method developed by colombano et al 2021 and philippe et al 2020 all of our experiments are summarized in table 3 in addition to the viscosity ratio m the capillary number ca and bond number bo both of which have an important role in analyzing the behavior of fluids in porous media can be calculated using the fluid velocity at the pumping point u p 2 88 10 4 m s and solid particle size ℓ to generalize our experimental conditions 3 ca u p μ inv γ bo ρ dis ρ inv g ℓ 2 γ the calculated dimensionless numbers are listed in table 4 dimensionless numbers for different isothermal pumping scenarios q 100 ml min and for two temperatures and fluid pairs heating increases the viscosity ratio by 30 but has no significant effect on capillary numbers which are low because of the low pumping flow rate the bond numbers are slightly less than one heating or changing the fluids pair do not impact significantly the bond number 3 image analysis and sensor calibrations 3 1 image analysis both the light transmission method ltm and light reflection method lrm have been used in previous studies on the visualization of two phase flow in porous media geel and sykes 1994 darnault et al 1998 wu et al 2017 the main idea is to determine a relationship between light intensity and wetting phase saturation previous works have shown that there is a logarithmic relationship between light intensity and saturation tidwell and glass 1994 gerhard and kueper 2003 colombano et al 2020 in that research the optical density od is defined such that od log i mes i 0 where i mes is the measured intensity and i 0 is the white intensity to obtain a linear relationship between optical density and wetting phase saturation in the case of coal tar and oil the optical density was measured for uniform saturation values s w 0 s r w s r nw 1 to get the line coefficients fig 3 where s w is the wetting saturation s r w and s r nw irreducible and residual saturation respectively a small drainage imbibition 2d cell was used to relate the optical density to saturation colombano et al 2020 philippe et al 2020 the 2d cell used to obtain the correlation curves is made of the same materials pvdf for the main part and the same glass at the front of the cell measurements were conducted with the same light source and camera more details on this experimental setup can be found in philippe et al 2020 the results show that a linear relationship between optical density and saturation exists for the oil ethanol pair r2 0 91 therefore it is possible from the photographs obtained during pumping to determine the saturation however the results are more dispersed for the coal tar water pair r2 0 36 due to its opacity light visualization methods do not apply to complex fluids like coal tars for the two fluid pairs the relationship between optical density and saturation was established at both 20 c and 50 c no effect of temperature on the relationship between optical density saturation has been observed therefore the same relationships can be used for non isothermal cases to summarize a pumping experiment can be separated into four stages a initial setup of the fluid interface b pumping through the central well c transformation into shades of gray and determination of the area of interest aoi and d conversion of the light intensity field into a saturation field with the relationship determined above see fig 4 3 2 relative permittivity and saturation calibration the relative permittivity measurements were carried out on 12 points by tdr sensors these sensors have recently been used successfully to determine dnapl saturation in 1d columns filled with glass beads iravani et al 2020 colombano et al 2021 these sensors measure relative permittivity values between 1 and 80 accuracy of 1 between 1 and 40 the sensors are placed so that the pins are vertical to reduce their impact on fluid flow they also measure temperature and electrical conductivity but these measurements have not been interpreted the tdr thermocouple sensor is not sensitive enough to measure fast temperature variations as observed in pumping cases the use of these sensors to measure temperature was therefore excluded and type k thermocouples were used instead sensitivity 0 1 s precision 0 1 c tdr sensors are generally used and calibrated by the manufacturer to measure water content in the unsaturated zone water air from permittivity measurements in static conditions davarzani et al 2014 gao et al 2018 here we were working with another fluid pair as well as in a dynamic regime caused by the pumping conditions the permittivity measurements should therefore be calibrated on our medium made up of glass beads and oil ethanol or coal tar water to obtain correct saturation values the licheteneker rother model eq 3 has been developed and gives a relationship between relative permittivity and saturation for a three phase medium made up of two liquid phases and one solid phase brovelli and cassiani 2008 4 ε r α 1 φ ε s α φ s w ε w α φ 1 s w ε nw α with ε r the effective relative permittivity measured by the sensor and ε s ε w and ε nw the relative permittivity of the glass beads wetting liquid water ethanol or non wetting liquid coal tar canola oil respectively α 0 5 corresponds to the complex refractive index model crim for this relationship to work for our pumping experiments we must use calibration points at known saturation in addition at constant saturation the measured permittivity can be influenced by temperature iravani et al 2020 the temperature measured by thermocouples will be used to correct the permittivity values obtained measurements were made at different temperatures for each fluid pair to determine how temperature influenced relative permittivity during all pumping experiments the only tdr sensor that can be affected significantly by a change in saturation is k33 located in the bottom row and center of the 2d tank fig 2 fig 5a shows how the two phase relative permittivity values i e glass beads and a single liquid filling the pore volume changed as a function of the temperature at this point between 20 c and 32 c for canola oil and ethanol and fig 5b shows the same for coal tar and water in fluid saturated glass beads the permittivity of ethanol decreases slightly while that of oil increases with temperature the same behavior is observed for coal tar and water this corresponds to the observations of iravani et al 2020 in both cases we have found linear relationships in the temperature range studied therefore the following relationships can be used to compare permittivity values obtained at different temperatures in glass beads 5 porous media saturated with ethanol r 2 0 91 ε r e t 1 85 10 2 t 11 4 6 porous media saturated with canola oil r 2 0 92 ε r o t 3 45 10 2 t 5 51 7 porous media saturated with coal tar r 2 0 91 ε r ct t 3 26 10 2 t 5 56 8 porous media saturated with water r 2 0 97 ε r w t 4 99 10 2 t 19 2 4 results the results will first be presented for the oil ethanol pair this way we see the temperature effect independently of other parameters related to the complexity of coal tar 4 1 isothermal two phase flow fig 6 shows photographs converted to oil saturation fields for isothermal pumping experiments at 20 c and 32 c the oil saturation fields obtained by imaging show distinctively the cone of depression i e the oil ethanol interface as well as the presence of residual oil in the form of ganglia throughout pumping ethanol initially on top of the oil gradually replaces the oil the maximum velocity that can be reached inside the tank is equal to the pumping velocity 2 88 10 4 m s 1 at all temperatures for oil ethanol at 20 c case the critical velocity is equal to uc 2 00 10 5 m s 1 which is lower than the pumping velocity in this case viscous fingerings occur around the central area due to the higher fluid velocity and low viscosity ratio between ethanol and oil m 1 64 10 2 at 20 c we observe less fingering at 32 c when pre heating the tank before pumping at 32 c the viscosity ratio and the critical velocity are higher than at 20 c m 2 18 10 2 uc 3 31 10 5 m s 1 at 32 c although the shape of the cone is established around t 9 min it is observed that the oil residual saturation continues to decrease until t 15 min in the cone of depression area when comparing saturation fields obtained at 20 c and 32 c it appears that the oil ethanol interface is smoother during pumping at 32 c the pre heating also reduced the number of viscous fingers forming thanks to an increase in the viscosity ratio 4 2 non isothermal two phase flow 4 2 1 temperature profiles pumping in isothermal conditions has demonstrated the advantages of heating the tank and the viscous dnapl before pumping non isothermal conditions which simulate local heating are closer to situations encountered on real sites temperature variations measured at the thermocouples are shown in fig 7 during the entire 100 ml min 1 pumping experiment heating and pumping steps the initial time t 0 represents the start of the heating step in fig 8 temperatures are shown only during pumping for the experiment at 100 ml min 1 with t 0 representing the start of the pumping step the temperature on the left t11 and t31 of the 2d tank near the heating element increased quickly and reached a steady temperature of 48 c the increase in temperature caused by heating is also visible on the second column of thermocouples t12 t32 and t22 reaching between 27 c and 35 c the temperature of the rest of the tank is between 20 c and 25 c and does not seem to be affected by the heating element variations in the outside temperature should also be noted and are notably responsible for the general decrease in values after 15 h of heating during pumping temperatures ultimately had small variations except on the thermocouples located near the center t22 and t23 the temperature rises from 25 c to almost 30 c due to the replacement of oil by ethanol at higher temperatures 4 2 2 saturation fields fig 9 shows the oil saturation fields during pumping under non isothermal conditions at pumping flow rate of q 100 ml min 1 compared to the 20 c isothermal case at the same flow rate some observations join those already carried out on isothermal cases viscous fingerings appear during pumping in the central part of the tank significant between t 3 min and t 9 min until a smoother cone of depression is visible at the end of pumping t 15 min in addition it also seems that the cone is slightly unsymmetrical at t 3 min with a greater cone height on the left compared to the central pumping point than on the right also we can see that the residual saturation is higher in the right of the tank than the left this is linked to the temperature field gradient during the heating step this heating gradient causes the properties of canola oil and ethanol to change due the critical velocity varies between 2 00 10 5 m s 1 at 20 c and 3 31 10 5 at 32 c inside 2d tank the viscosity ratio also increases from 1 64 10 2 at 20 c to 2 64 10 2 at 32 c thus the critical velocity and the viscosity ratio are higher in the left of the tank than the right creating a difference in fingering number in the tank in the end residual saturation is also variable inside the tank depending on the temperature 4 3 effect of flow rate on non isothermal two phase flow the presence of viscous fingers was still observed at 100 ml min 1 in non isothermal conditions the pumping velocity is still higher than the critical velocity at which the flow is unstable a pumping experiment at 20 ml min 1 was done to observe the cone of depression and the saturation field in stable flow conditions the effect of the pumping flow rate on the temperature distribution in the tank is shown in fig 10 temperature fields obtained with surfer using the kriging method are also presented in appendix a fig a2 for both flow rates investigated here 20 ml min 1 and 100 ml min 1 the temperature increases during pumping on t12 t22 t33 however this increase is greater for the higher flow rate 100 ml min 1 a higher pumping rate effectively increases thermal advection in the tank therefore the generated heat from the heating element is better transferred to the rest of the tank the calculation of the rayleigh number ra see appendix a table a 1 showed that the natural convection should be considered in the wetting phase zone the comparison between saturation fields obtained for both flow rates is presented fig 11 regarding the pre and post treatment photographs obtained at 20 ml min 1 no viscous fingering has been observed throughout the pumping in this case the pumping velocity is equal to 5 76 10 5 m s 1 much closer to the order of magnitude of the critical velocity 10 5 m s 1 thus the absence of viscous fingering at 20 ml min 1 can be explained by a much more stable interface than at 100 ml min 1 also the time needed to reach the steady state is longer 40 min and the saturation in the residual zone is much smaller than we observed at 100 ml min 1 the interface is smoother throughout pumping and better describes a cone of depression typically represented using multiphase darcy s laws however from t 25 min we notice many oil ganglia present in the residual zone this number of ganglia is much higher on the right of the cone of depression less influenced by the heating than the left the number and size of these ganglia are bigger for higher flow rate 5 discussion 5 1 evaluation of saturation and residual saturation the change in ethanol saturation near the pumping point tdr k33 is shown in fig 12 as a dotted line for different pumping cases the calculated averaged saturations around the tdr k33 radius 1 cm obtained by imaging are also presented in the form of a point on this same graph we should note that the lrm gives a pixel scale saturation field and tdr gives centimeter scale saturation the corresponding imaging technique saturation reported in fig 12 comes from averaging the pixel scale saturation over a surface to obtain a centimeter scale quantity later we will validate the imaging technique against the mass recovery in non isothermal conditions the residual saturation is lower 0 28 than that observed at the isothermal 20 c case 0 40 moreover reducing the flow rate does not seem to significantly decrease residual oil saturation in this zone finally the time required for the ethanol to be detected at the tdr sensor increases when pumping is performed at higher temperature ethanol replaces the oil initially present in a smaller pore volume because of the greater number of viscous fingers at low temperature in this case the flushing fluid ethanol will therefore reach the pumping point in a shorter time and leaves a higher volume of trapped oil in the tank this observation is confirmed from the images taken during pumping the number of fingers decreases during the experiments with heating or at high temperature and the height of the cone is the greatest at 20 c in addition the lowest residual saturation 0 25 is still obtained in the isothermal case at 32 c it is interesting to note that even though the fluid interface front passed the sensor level oil saturation keeps on decreasing afterward the rate of decrease depends on the presence and the amount of the fingering this can be explained by the dynamic effect and remobilization of the discontinuous non wetting phase ganglia by the flow of wetting phase towards the pumping well also we see that the saturation values estimated by imaging and tdr are quite close except for the experiment at 20 c under isothermal conditions the presence of more fingers and ganglia in the 20 c isothermal experiment is a possible explanation for these differences therefore it should be possible to use tdr measurements to evaluate coal tar saturation during coal tar water experiments 5 2 fingering evaluation the fingers were measured as the difference between the interpolated profile corresponding to the oil ethanol interface and the actual profile on all the pixels the method that we used to determine the interface is different from the saturation conversions and does not require such a high range of optical densities the photographs were converted to binary images 0 for coal tar 1 for water with thresholding the same threshold value and method was applied for all coal tar water photographs the processes were executed with fiji imagej with this method we can distinguish the main line separating both liquids referred as the interface the obtained interface results using imagej fit the dnapl water interface observation this method does not allow us to distinguish between viscous and capillary fingering the flattened fingers using fiji at the oil ethanol interface during pumping at 20 c and 32 c and in non isothermal conditions are shown in fig 13 the scale is the same on each fingering profile the size of the fingers initially increases as the cone of depression is formed this trend is consistent with observing the formation of viscous fingerings during pumping at the interface on experiments at 100 ml min 1 also higher fingers are formed near the center of the tank than on the left and right sides then the fingering effects decrease once the ethanol begins to be recovered until it reaches an interface with a few fingers especially visible in the non isothermal case and the case at 32 c heating the tank reduces the length of the fingers at any time the main differences are mostly towards the center near the pumping point this result is consistent with the fact that the oil ethanol viscosity ratio is lower at 32 c than that at 20 c in the non isothermal case the fingers are shorter than those observed at 20 c but still longer than for the case at 32 c the fingering patterns at 20 ml min 1 also do not have large fingers at the center a lower number of fingers indicates that more oil is recovered since it is not trapped during pumping thus pumping at low flow rate effectively increases the oil volume extracted from the tank but the pumping time is also increased accordingly the average size of the fingers is also calculated between pumping in non isothermal and isothermal conditions at 20 c separately in the left and right parts of the tank the point separating these two parts is chosen as being the lowest point of the cone of depression determined from image analysis table 5 summarizes the mean and maximum observed finger length for the pumping experiments in isothermal 20 c and non isothermal conditions at 100 ml min 1 in the case of pumping at 20 c the mean and maximum finger sizes are basically of the same order of magnitude in the left and right sides of the tank 0 20 cm and 0 80 cm in the non isothermal case however the fingers in the left of the tank are much smaller 0 08 cm and 0 33 cm than the right 0 17 cm and 0 77 cm the length of the fingers is therefore reduced when pumping in a pre heated tank non isothermal conditions rather than at isothermal 20 c 5 3 recovery efficiency the recovered oil volumes estimated by imaging from analyzing the cone of depression zone are shown in fig 14 comparison of this data with mass scale measurements are also shown we found great agreement between oil volume measured with mass scale and estimated by imaging data for all experiments before breakthrough however we noted that the mass measurements are a little overestimated compared to the quantity of oil recovered estimated by imaging this is because a small part of ethanol bubbles and oil ethanol emulsion started to be recovered near the breakthrough point and is included in the mass measurements we could not separate both liquids directly during the experiments and this results in a slight overestimation of the oil volume recovered compared to the oil volume estimated with imaging on the figure isothermal pumping at 32 c recovers more oil than at 20 c pumping under non isothermal conditions recovers more oil than isothermal pumping a lower flow rate 20 ml min 1 collects a higher volume of oil from the tank than at a high flow rate 100 ml min 1 table 6 shows the radius of influence the residual saturations in the cone at the end of pumping and for the left and right of the cone of depression zone and the remediation efficiency values obtained by imaging for each oil pumping experiment we found that an increase of only 12 c reduces the residual saturation from 0 36 to 0 20 moreover we observed improved remediation efficiency linked to a slight increase in the radius of influence and a decrease in viscous fingerings heating the left side of the tank to 60 c before pumping at 100 ml min 1 non isothermal conditions also reduced the residual saturation and increased the efficiency compared to pumping at 20 c however pumping at 32 c was still more efficient than pumping under non isothermal conditions in addition in non isothermal conditions the lower pumping rate slightly increased the radius of influence as well as the efficiency the residual saturation also decreased thanks to a reduction in the formation of viscous fingers during pumping the residual saturation estimated by imaging in the cone of depression was almost identical between the left and right parts of the pumping in isothermal cases while it was lower by 3 at 100 ml min 1 and 7 at 20 ml min 1 in non isothermal conditions finally the residual saturations estimated by imaging are still close to the ones directly measured with the tdrs for instance at 20 c the measured residual saturation is 0 40 while it is equal to 0 36 when considering the whole residual zone of the tank the masses measured using the automatic balance were converted to mass flow rates by interpolation in fig 15 for oil ethanol initially pumping only recovers oil constant flow until the breakthrough time which is different for each experiment this breakthrough time represents the point at which ethanol begins to be recovered as well after this breakthrough time the oil is no longer pumped and a new constant value is reached where only the ethanol is recovered the residual saturation is reached the breakthrough time increases when the tank is heated this is due to the reduction of viscous fingers in the tank which allows the oil to be pumped for a longer time and therefore increases the remediation efficiency the initial and final flow rates between the 20 c isothermal case and the non isothermal case are equal however the ethanol arrived later and the oil ethanol mixture is recovered for a shorter time the temperature at the pumping point is effectively 20 c but the lower viscosity ratio allows oil to be recovered for a longer time 6 real dnapl water case experiments with oil ethanol have provided a better understanding of how heating can affect the recovery of a viscous dense liquid phase in a saturated medium a temperature increase is linked to a residual saturation decrease and an increase in the pumping radius the coal tar used here has in fact more variable properties because of its chemical complexity also the coal tar adheres to the glass internal wall of the tank and is opaque to visible light so the previous imaging techniques cannot be used to estimate the water saturation even if the water coal tar interface can be distinguished the saturation calculations are generally wrong because coal tar completely absorbs visible light in this case the optical density measurements do not have a range sufficient to convert them into saturation when we converted the photographs into grayscale images we observed shades of gray corresponding to different saturations for oil ethanol however that was not the case for coal tar water the initial tank preparation and the heating phase did not present any problems however during the pumping phase the measured flow rate was much lower when recovering the coal tar instead of the 15 min used for the oil the pumping took 45 min before reaching a steady state the sticky nature of the coal tar and clogging effects in the tubing could both explain this difference grayscale images taken during coal tar pumping are shown in fig 16 6 1 temperature and saturation results the thermal properties of the oil ethanol and coal tar water fluid pairs are quite different the thermal conductivity and specific heat capacity of the oil used is indeed quite close to that of coal tar but ethanol s is much lower than water s the temperature profiles measured at the thermocouples are presented in fig 17 the temperature increases significantly when pumping on t11 t12 t22 t23 t31 these increments correspond to the arrival of heated water during pumping temperature fields obtained with surfer are also presented in appendix a fig a3 the saturation profiles have been plotted using tdr data fig 18 compares the coal tar saturation measured at the sensor k33 in isothermal conditions 20 c and non isothermal conditions heating setpoint 60 c coal tar residual saturation was estimated to be 0 39 at 20 c pre heating at 60 c the left side of the tank before pumping coal tar reduces this value to 0 27 the reduction is similar to that observed for oil ethanol from 0 40 at 20 c to 0 28 in non isothermal conditions 6 2 fingerings even if the estimation of coal tar saturation is not accurate using imaging it is still possible to interpolate the coal tar water interface with binary images 0 coal tar 1 water and thresholding thus we can still measure the fingers in the case of coal tar fig 19 shows how the fingers changed during pumping the fingers are much larger with the coal tar and water liquid pair than with the oil and ethanol model pair the average and maximum finger length observed for coal tar water is 0 39 cm and 1 44 cm long these values are 3 and 2 times higher than observed for oil ethanol respectively however the same trend was observed the number and length of the fingers increase up to t 30 min during the formation of the cone of depression then decrease until the end of the pumping the fingerings are also larger on the left than on the right the effect of heating is much more noticeable on fingerings with coal tar water than for oil ethanol the dimensionless numbers and trends with temperature are similar thus the differences in fingering patterns may come from the complexity of coal tar 7 conclusions a 2d tank experimental setup was designed to reproduce pumping cones of depression for the flow of two immiscible liquids in 1 mm glass beads the addition of a heating element similar to those used on real sites allowed pumping operations to be performed in non isothermal conditions and the observation of how a pre heating step influenced the temperature and saturation in the tank in isothermal conditions homogeneous heating improves the efficiency of oil pumping the residual saturation estimated by imaging is decreased from 0 36 to 0 20 by increasing the medium temperature from 20 c to 32 c then the study of pumping in non isothermal conditions showed how effective thermal enhancement is to improve the efficiency of oil pumping heating reduces the length and number of fingers observed during pumping and improves remediation efficiency in non isothermal conditions a heating element has been used to generate a temperature field in the 2d tank to reproduce conditions closer to those encountered in a real polluted site the left part of the cone of depression in the tank close to the heating element is thus warmer than the right part the latter remains close to room temperature that means that the dynamic viscosity of the oil varies in the tank unlike the isothermal case where the dynamic viscosity is constant when pumping in non isothermal conditions a slightly unsymmetrical cone forms initially but the final state is symmetrical we observed that the fingering effects are smaller in the left heated part of the cone of depression in the tank than the right the residual saturation estimated by imaging also follows this pattern recovery in non isothermal conditions is therefore more efficient 7 at 20 ml min 1 3 at 100 ml min 1 in the heated parts also we applied a lower flow rate of 20 ml min 1 in non isothermal conditions in this case a lower number of fingers appeared and a higher volume of oil 5 more was recovered from the tank with an increase in temperature the critical velocity and the viscosity ratio increased and helped to stabilize the interface between the non wetting and wetting phase we applied these observations to a real pollutant pumping case coal tar pollution first the temperature field is quite different due to the thermal properties of water which allow better heat transport especially during the pumping phase the observation of the fingering changes showed a higher difference in finger length between the left and right parts thanks to the action of the heating before pumping unfortunately the imaging does not allow to obtain coal tar saturation fields due to the excessively high coal tar opacity however we could analyze the evolution of the coal tar saturations inside the 2d tank using tdrs sensors these experimental measurements have improved the understanding of non isothermal flows in porous media they can be used as a basis for the development and validation of a numerical model coupling two phase flow and heat transfers this type of model should allow the simulation of the behavior of two immiscible fluids and make prospective analysis prior to remediation processes for industrial applications in the case of unfavorable viscosity ratio this study leads to the recommendation of heating the soil as homogeneously as possible and then pump at the lower flow rate possible to recover the highest amount of dnapl possible from the soil declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was performed as part of the bioxyval project the authors would like to thank ademe french environment and energy management agency for co funding the project under the ami program remea and anrt for providing the ph d grant for nicolas philippe we thank ali iravani and benjarese oniangue for their help on experiment measurements we gratefully acknowledge the financial support provided to the pivots project by the région centre val de loire and the european regional development fund appendix a temperature field during 2d tank heating temperature changes are represented in the form of 2d fields via the surfer software by the kriging method fig a1 shows the effect of flow rate on the temperature field at the start and end of pumping fig a1 comparison of the temperature fields before and after pumping in non isothermal conditions for two pumping rates 20 ml min 1 and 100 ml min 1 fig a1 the temperature is much higher on the left of the tank than on the right in non isothermal conditions due to the presence of the heating element the temperature seems to redirect slightly towards the central and lower part of the tank during pumping for both flow rates certainly linked to the infiltration of ethanol the temperature variations occurring during pumping are more easily observed in fig a2 which compares the ratio between temperature measured at the end of pumping divided by the temperature just before pumping t 0 of pumping fig a2 comparison of the ratio between temperature after pumping and initial temperature before pumping in non isothermal conditions for two pumping rates 20 ml min 1 and 100 ml min 1 fig a2 after pumping the temperature increases especially in the center of the tank this corresponds to the arrival of hot ethanol replacing the oil initially present at high flow rate the temperature ratio is substantial in the left and center parts of the tank 0 05 m x 0 35 m due to high convection effects in conclusion pumping at a lower flow rate gives a more uniform temperature field which reduces the viscosity ratio between the oil and ethanol this effect could translate into a lower residual saturation at lower flow rate the temperature fields before and after pumping are shown in fig a3 for the coal tar water fluid pair fig a3 comparison of the temperature fields before t 0 min and after t 40 min pumping in non isothermal conditions for coal tar water fluids pair fig a3 the high temperature is redirected to the central part of the tank during pumping certainly linked to the infiltration of heated water fig a4 compares temperature ratio fields between oil ethanol and coal tar water pumping cases the temperature ratio is defined as the temperature after pumping over the initial temperature after heating fig a4 comparison of the ratio between temperature after pumping and initial temperature before pumping under non isothermal conditions for pumping oil at steady state t 15 min and coal tar t 40 min q 100 ml min 1 fig a4 the temperature also increases in the upper left part of the coal tar water case unlike in the oil ethanol case where most of the pumping increases the temperature in the center of the tank the thermal conductivity and specific heat capacity of water are higher than those of ethanol this allows the water to continue heating while pumping the calculation of the reynolds number re thermal péclet number pe t and the rayleigh number ra help to understand the heat transfer regimes in non isothermal conditions these dimensionless numbers can be calculated as a1 re u p ℓ ν pe t u p ℓ α ra gβ t h t c αν k h where t h k and t c k are respectively the hot 60 c and the cold 20 c temperature in the system β k 1 is the coefficient of thermal expansion ℓ m characteristic length solid particle size h m height of liquid in 2d tank α m2s 1 thermal diffusivity ν m2s 1 kinematic viscosity and k m2 intrinsic permeability the reynolds and thermal péclet numbers were calculated using the fluid velocity around the pumping point u p the calculated numbers are listed in table a1 table a1 dimensionless numbers for different liquid under non isothermal conditions q 100 ml min table a1 liquid oil ethanol coal tar water c p jkg 1 k 1 1840 2420 2100 4180 λ wm 1 k 1 1 66 10 1 1 68 10 1 1 32 10 1 5 98 10 1 β k 1 5 03 10 4 6 20 10 4 4 82 10 4 2 60 10 4 re 2 00 10 3 1 2 10 1 6 00 10 3 2 87 10 1 ra 0 81 62 3 72 37 pe t 2 91 3 35 5 02 2 01 the low values of reynolds number indicate that the flow in porous media can be considered as a creeping flow the rayleigh numbers in wetting phases are more than one order of magnitude higher than the ones for non wetting phases this indicates that the natural convection should be higher in the upper part of the 2d tank during the heating stage the rayleigh number of the saturated ethanol zone is around twice the saturated water zone the thermal péclet number of more than one shows that the convection is dominated in all pumping experiments our preliminary simulation of heat transfer during the heating stage shows that the heat loss from 2d tank walls surface is low thanks to the double walled systems however the most important heat loss can be reproduced from the top surface of the 2d tank which is in direct contact with air circulation inside the fume hood a coefficient of heat loss around 50 100 w m 2 k 1 
1355,thermal enhancement is known to be an efficient way to decrease the residual saturation of some common dense non aqueous phase liquids dnapls after pumping however the effect of transient heat transfer during the recovery of a high viscosity contaminant such as coal tar in highly permeable porous media is still unknown a 2d tank experimental setup allowing monitoring of temperature and saturation fields during dnapl pumping has been developed experiments were run under isothermal and non isothermal conditions at low and high flow rates we investigated the presence of viscous fingering and how that influences the shape of the cone of depression as well as the residual saturation the saturation fields show that less viscous fingering occurs in pre heated cases and that heating increases the recovery efficiency increasing the temperature increases the critical velocity and the viscosity ratio and helps to stabilize the interface between the non wetting and wetting phase observations were first made on an oil and ethanol fluid pair because its properties were known before extending the experiments to a coal tar and water fluid pair residual oil saturation after pumping was decreased by 6 16 in all pre heated conditions pumping at low flow rate in these conditions leaves the smallest oil residual saturation 20 after pumping a low flow rate increases the recovery efficiency by reducing viscous fingering and by spreading the generated heat to a larger part of the tank finally results on coal tar pumping show that the high thermal conductivity of water helps in keeping the temperature high during pumping the residual coal tar saturation was reduced from 40 at 20 c to 28 when pre heating the tank operating at a low flow rate and with a uniform temperature is the key to recovering the highest amount of a viscous dnapl such as coal tar from the soil and satisfying cleanup goals when using thermally enhanced pumping keywords dense non aqueous phase liquid two phase flow non isothermal 2d tank thermally enhanced dnapl recovery coal tar 1 introduction the remediation of sites contaminated by dense non aqueous phase liquid dnapl remains an open subject these phases are composed of substances that are generally toxic and carcinogenic among dnapls are coal tars with complex liquid phases they are composed of monocyclic and polycyclic aromatic hydrocarbons pahs as well as phenolic compounds lee et al 1992 previous characterization studies have shown that coal tars vary according to origin the coal tar used in this study is the same as the one studied recently by johansson et al 2020 iravani et al 2020 philippe et al 2020 pump and treat technology remains the most widely used to remediate high saturation coal tar sources in general remediation efficiency is very low for dnapls and this method is technologically and economically unsatisfactory usepa 1996 russell and rabideau 2000 kavanaugh and kresic 2008 navy 2008 newell et al 2011 this low efficiency flows from a combination of factors i coal tar is denser than water it flows along the most permeable local paths often in the form of fingerings to impermeable horizons and then forms deep source zones ii when recovering coal tar by pumping the unfavorable viscosity ratio causes water to infiltrate in the form of fingerings these fingers gradually trap significant volumes of coal tar until the water starts to be recovered from the pumping well at this stage only a tiny amount often mixed in water of coal tar is collected that no longer allows the soil to be decontaminated iii some coal tars have non newtonian behavior fitzer et al 1987 but the coal tar used in this study showed newtonian behavior philippe et al 2020 thermal desorption techniques have already proved their effectiveness for the remediation of dnapl contaminated soils heat treatment can be highly effective for volatile organic compounds vocs with efficiencies of up to 99 even in highly contaminated sources renoldi et al 2003 johnsen et al 2005 for example heron et al 1998 succeeded in extracting 99 8 of tce from a silty soil 50 cm deep using this method heron et al 1998 recently the same type of process was applied to pah contaminated sites with an equally efficient yield by reaching soil temperatures of 325 c baker et al 2006 nevertheless this technique has a significant financial cost because of the high energy required mcdade et al 2005 baston et al 2010 another technique quite similar consists in burning the pollutant when it is flammable which happens to be the case of coal tar the smoldering fire provoked during this combustion can incinerate coal tar source zones with yields as high as thermal desorption techniques in unsaturated zones switzer et al 2009 hasan et al 2015 nowadays this technique has already been applied to coal tar in the field with a mass concentration of pahs decreased by 97 scholes et al 2015 smoldering fire is however a type of combustion whose fundamental understanding is still limited and whose environmental impact on the soil has not yet been studied rein 2009 however these techniques generate temperatures higher than 100 c boiling temperature of water on the contrary here thermally enhanced dnapl recovery focuses on temperatures lower than 100 c to avoid contaminant evaporation the goal is to reduce the quantity of residual dnapl formed during pumping that could require further treatment or management we are interested in the pumping of coal tars and the means of recovering a larger quantity of contaminants therefore the objective is to increase the recovery rate by reducing fingerings as little coal tar as possible is trapped during pumping a similar problem has already been faced in petroleum engineering in relation to the recovery of heavy oils the most appropriate solution is to heat the medium and the liquids to reduce the fluid fluid viscosity ratios koci et al 1989 wang et al 2006 most tests in petroleum engineering demonstrate the effectiveness of heating and its feasibility at the laboratory scale whether by external heating or by injecting hot water however they represent an essentially one dimensional flow in a horizontal or vertical column and do not consider the 2d flow complexity of a real pollution under non isothermal conditions the change of dimension from 1d columns to a 2d tank scale is a recent subject of study on dnapl wilking et al 2013 followed the infiltration and dissolution of a dnapl trichloroacetic acid and obtained saturation fields using x ray tomography technology wilking et al 2013 more recently dnapl saturation has been monitored in transparent media thanks to a calibration between the optical density and the dnapl volume fraction these techniques are referred to as the light reflection method lrm and light transmission method ltm for instance alazaiza et al 2016 used lrm to monitor progressive infiltration of pce alazaiza et al 2016 darnault et al 1998 similarly followed the oil water system using a blue dye with ltm darnault et al 1998 heiderscheidt et al 2008 also used ltm to study in situ oxidation on sand impacted by pce heiderscheidt et al 2008 colombano et al 2020 demonstrated that from lrm the residual saturation of chlorinated solvents in 2d tank pumping experiments could be estimated with good accuracy r2 0 95 monitoring by imaging allows the study of transient phenomena linked to dnapl flows in porous media colombano et al 2021 for example explored the remediation of chlorinated solvents by increasing the system s temperature and chemical enhancement using surfactants in 0 1 mm and 0 5 mm glass beads colombano et al 2021 no temperature effect has been reported in this study as their dnapl s dynamic viscosity and surface tension did not vary with temperature however coal tars are much more viscous than most dnapls including chlorinated solvents moreover coal tars dynamic viscosity decreases greatly with temperature philippe et al 2020 also colombano et al 2021 only worked in isothermal conditions their recovery by pumping can therefore change the observed results pankow and cherry 1996 li and schwartz 2004 childs et al 2006 moreover thermal techniques for soil remediation have also already been studied on this scale heat transfers in a small scale two dimensional experimental tank 60 cm 45 cm 1 cm filled with 0 5 mm glass beads and a large scale tank 2 4 m 1 2 m 0 15 m filled with sandy soil have been investigated by krol et al 2011 furthermore johnson et al 2011 studied the potential of the electrical resistance heating erh technique at low temperature 30 50 c to enhance the clean up of tce and pce sources in the same large scale tank johnson et al 2011 the authors concluded that pre heating groundwater and a tce or pce source is an efficient way of promoting its recovery in highly permeable media also the generated heat can enhance in situ biodegradation or chemical reactions afterward however no existing study considers the transient temperature and saturation variations together during the pumping of a high viscosity fluid in highly permeable porous media which is the main goal of our study the coal tar used here comes from a seriously polluted site in france due to its characteristics this coal tar is not compatible with imaging techniques it sticks on the glass walls and is completely opaque to the light of the visible spectrum thus we conducted most of our studies using the canola oil ethanol model pair proposed by philippe et al 2020 its properties were recently approved in philippe et al 2020 as well as the reduced residual saturation with increased temperature the presence of viscous fingering generated due to the dynamic effect as opposed to static experiments was found to be responsible for the high residual saturation in the system increasing the temperature reduces viscous fingering and increases ethanol film presence residual oil saturation falls as a consequence philippe et al 2020 in two phase flow in porous media viscous fingering may appear when the viscosity ratio m is unfavorable 1 and the velocity in the experiment is higher than a critical velocity uc where m and uc are defined as lenormand et al 1988 glass and nicholl 1996 1 m μ inv μ dis 2 u c ρ inv ρ dis g k φ μ inv μ dis where μ inv pa s and ρ inv kg m 3 are the dynamic viscosity and density of the invading phase μ dis pa s and ρ dis kg m 3 the dynamic viscosity and density of the displaced phase k m2 is the permeability of the porous medium and φ its porosity g 9 8 m s 2 is the conventional value of gravitational acceleration we investigated the presence of viscous fingering and how that influences the shape of the cone of depression as well as the residual saturation the experimental development and most of the conclusions were obtained on the oil ethanol liquid pair the coal tar water pair was also studied as an industrial application our main objective was to study the coupling between temperature and immiscible two phase flow in pumping conditions the porous medium consists of homogeneous glass beads with high permeability to avoid any other disturbing phenomena first the experiments were carried out on the oil ethanol pair to work with simple fluids whose properties are well known and which allow optical monitoring then the pumping of dnapl in the saturated zone was carried out as an industrial application to observe similarities between two pairs of immiscible fluids in practice during these experiments the saturation s was followed by time domain reflectometry tdr local small volumes and imaging 2d fields the temperature t was followed using thermocouples we focused on i the shape of the line delimiting the two continuous phases that referred to as the interface and represented by a pumping cone cone of depression ii the saturation during pumping and the residual saturation after pumping iii the frequency and appearance of the fingerings encountered and iv the recovery rates and volumes 2 experimental setup and procedures 2 1 experimental setup the experimental setup consists of a decametric 2d tank with a pump a camera and a well instrumented monitoring system wetting and non wetting phases are injected into the same 1 mm homogeneous glass beads used in philippe et al 2020 and pumped under isothermal circulation of cold or hot water around 2d tank and non isothermal heating element conditions the mean porosity φ and permeability k of the glass beads used are 0 4 and 3 5 10 10 m2 respectively saturation and temperature were monitored during the heating and pumping steps fig 1 the 2d tank height 30 cm width 50 cm thickness 7 cm made of polyvinylidene fluoride pvdf fabricated by scodip was used to carry out all the pumping experiments fig 2 the front of the tank is transparent glass 1 cm thick so that the environment can be photographed at different times photographs are taken to obtain saturation fields during the pumping experiments the apparatus consists of a central reservoir containing the porous medium and side channels counter channels regulating the static levels of the liquids used the counter channels are connected to the central reservoir by a metal grid which allows fluids to pass but blocks the glass beads inside the central reservoir side and bottom accesses allow connection of pipes and injection or pumping of liquids the internal diameter of the pumping access points is 3 125 cm the tank is equipped with a double jacket in back part and double glazing in front allowing experiments to be carried out in isothermal conditions by circulating regulated temperature water using a thermostatic bath lauda model eco re 420 heated water circulates through the double jacket and double glazing to keep the temperature steady during isothermal experiments a heating element 350 w 0 7 w cm 2 made of stainless steel and designed by vulcanic is added inside the tank near its left wall cavity to create non isothermal heating conditions in a real case heating is not uniform due to the size of the field the heating element is used to reproduce more realistic conditions that correspond to non isothermal pumping this element is helical 30 cm long and 6 5 cm in diameter the element heats the porous medium uniformly along the x axis the heating is regulated by a proportional integral derivative controller pid controller using a control box and a pt100 probe connected to the heating element fig 1 even if the heating element is not needed for the isothermal case it is kept inside the tank for all the experiments to ensure the same conditions between isothermal and non isothermal conditions the double jacket and double glazing systems which have been filled with air for non isothermal cases can communicate freely with the air of the experiment room through the inlet and systems this reduces significantly the heat loss from the 2d tank front glass and back parts pumping used a peristaltic pump watson marlow 530 u the liquids recovered are injected directly into a beaker placed on a precision balance sartorius cubis mse 0 1 mg acquisition of the recovered mass is automatic and performed every 6 s we used lrm based on image analysis to obtain a precise relationship between light intensity and saturation field to determine the average dnapl saturation inside the tank a 34 megapixel 7360 4912 nikon d810 digital still camera with a nikkor 105 lens and two 500 w broncolor projectors were used projectors prevented light reflection and shadow and improved the brightness of photographs the imaging system was installed in a dark room to prevent outside light from influencing the brightness of the photographs the tank was also equipped with 15 measuring units consisting of tdr probes 5te meter group to measure the medium relative permittivity and type k thermocouples at the back of the tank provided by tc s a precision 0 4 sensitivity 0 150 s the position of these sensors is shown in fig 2 and represents a network of three rows and five columns a campbell cr 1000 datalogger was used to acquire temperature and relative permittivity measurements in practice tdrs 11 21 and 31 were removed as well as thermocouple 21 because they were physically blocked by the addition of the heating element the diameter of the pumping access point 3 125 cm is around half of the 2d tank thickness 7 cm the saturation fields obtained with lrm are just representative of the saturation on a 2d plane corresponding to the front glass wall of the tank we effectively observed that the wetting phase water or ethanol flows out of the tank while the apex of the cone of depression does not still reach the pumping point this is evidence of a very small 3d effect near the pumping point the difference between the recovered fluid mass using the mass scale and imaging at the end of the experiment proves also this small 3d effect therefore the saturation obtained with lrm will be compared to saturation measured from tdr probes to evaluate the two measuring methods the comparison between both techniques is used to determine potential 3d effects that cannot be captured by the lrm technique the liquid pairs used to carry out these experiments are the same as philippe et al 2020 the coal tar water pair representing in a geometrically simplified manner a layer of dnapl on an impermeable substratum in an aquifer the sticky aspect and the lack of transparency of the coal tar are nevertheless two properties that complicate its study the water used was previously degassed to avoid the presence of residual air in the tank the canola oil ethanol pair serves as a model pair sodium chloride was also dissolved in ethanol 0 65 g l to make measuring electrical resistivity easier in a parallel work iravani et al 2020 2 2 fluid properties the density and the dynamic viscosity of each fluid are specified in table 1 for the coal tar ct water w pair and table 2 for the canola oil o ethanol e pair philippe et al 2020 the measured interfacial tension and the contact angle are also presented we used the pendant drop technique to measure interfacial tension of the fluids by forming a drop of coal tar or oil in a small cuvette filled with degassed water or ethanol contact angles were measured using the sessile drop method by placing a drop of coal tar or oil on the bottom plate of the same quartz glass cuvette previously filled with degassed water or ethanol the low interfacial tension of coal tar water can be related to the presence of amphiphilic substances in the coal tar sampled from a real site barranco and dawson 1999 reported similar low values of interfacial tension of coal tar water at basic ph values ph 9 sanaiotti et al 2010 also reported interfacial tension values between 1 and 3 mn m 1 for soybean oil and water and ethanol mixtures sanaiotti et al 2010 the high 90 values reported of contact angle indicate that coal tar and oil are the non wetting fluids properties and trends of these liquid pairs are similar which supports the use of the canola oil and ethanol pair to model coal tar and water two phase flow 2 3 experimental procedures 2 3 1 setup preparation and heating the heating element was first installed on the left side of the empty tank and initially held straight with a support oil was injected through the five lower inlets using a low flow peristaltic pump simultaneously 10 ml min 1 to an initial height of 1 5 cm from this moment glass beads were progressively added while continuing the oil injection the glass beads were also stirred during injection to homogenize their distribution in the tank the injection and addition were done slowly to avoid any air trapping during this step the oil level was always kept just above the level of the glass beads to also prevent air trapping oil injection stopped when the oil reached a height of 15 cm then ethanol was injected in the same way using the lateral counter channels from above the ethanol injection stopped when the tank was fully saturated all 30 cm during the injection of the liquids precautions were taken to avoid mixing the oil and ethanol and to keep the interface between both liquids at 15 cm as straight as possible fig 1 initially in the case of oil ethanol the oil completely saturates the porous medium from 0 to 15 cm and the same for ethanol from 15 to 30 cm however in the case of the coal tar water application case the tank preparation described above was slightly changed the 2d tank was first fully saturated with water then coal tar was injected through the five lower inlets to perform primary drainage up to a height of 15 cm this was done to better represent a real pollution case which is rarely fully saturated in addition preparing the experiment with coal tar required more attention as a first step the solid particles contained in the coal tar were removed using a glass fiber filter gf d pore size 2 7 μm water present in the collected coal tar during sampling was also removed using a vacuum pump and a 10 μm hydrophobic membrane for both liquid pairs once the tank was full the initial relative permittivity measurements were carried out for 12 h to verify that the tdrs were working correctly initially the tank was at room temperature 20 c in the case of pumping under isothermal conditions a temperature controlled water bath was used to set the tank temperature to 20 c or 35 c the heating element was still added in isothermal cases so that the porosity was the same as in non isothermal experiments in non isothermal conditions the temperature of the heating element was set to 60 c with the temperature control unit pid controller the tank was heated until a steady state was reached this step lasts about 8 h during the heating phase the temperature of the tank and the permittivity values were measured with sensors directly in contact with the porous medium inside the tank 2 3 2 pumping procedures pumping used a flexible tube diameter 3 125 mm located on the lower central inlet of the tank the rotation speed was fixed at 84 3 rpm or 25 rpm the corresponding pumping rates q of these rotations for water and ethanol were 100 ml min 1 and 20 ml min 1 respectively the measured pumping rate for the same rotation rates but for canola oil was lower than water and ethanol at 84 3 rpm 70 ml min 1 and the same at 25 rpm 20 ml min 1 the mass of recovered liquid was measured continuously with an automatic acquisition mass balance every 6 s the time at which the ethanol begins to be recovered was also noted and is defined as the breakthrough time photographs of the tank were taken using the light reflection method lrm technique during the pumping step raw images and data were saved in nikon format every three minutes at 100 ml min 1 and 5 min at 20 ml min 1 the pumping step lasted until the stationary state was reached which was indicated by the exclusive recovery of the wetting phase ethanol or water and a stationary depression cone the photographs were then converted into tiff format with capture one software the images were finally processed with the open source software fiji to calibrate light intensity and saturation according to the method developed by colombano et al 2021 and philippe et al 2020 all of our experiments are summarized in table 3 in addition to the viscosity ratio m the capillary number ca and bond number bo both of which have an important role in analyzing the behavior of fluids in porous media can be calculated using the fluid velocity at the pumping point u p 2 88 10 4 m s and solid particle size ℓ to generalize our experimental conditions 3 ca u p μ inv γ bo ρ dis ρ inv g ℓ 2 γ the calculated dimensionless numbers are listed in table 4 dimensionless numbers for different isothermal pumping scenarios q 100 ml min and for two temperatures and fluid pairs heating increases the viscosity ratio by 30 but has no significant effect on capillary numbers which are low because of the low pumping flow rate the bond numbers are slightly less than one heating or changing the fluids pair do not impact significantly the bond number 3 image analysis and sensor calibrations 3 1 image analysis both the light transmission method ltm and light reflection method lrm have been used in previous studies on the visualization of two phase flow in porous media geel and sykes 1994 darnault et al 1998 wu et al 2017 the main idea is to determine a relationship between light intensity and wetting phase saturation previous works have shown that there is a logarithmic relationship between light intensity and saturation tidwell and glass 1994 gerhard and kueper 2003 colombano et al 2020 in that research the optical density od is defined such that od log i mes i 0 where i mes is the measured intensity and i 0 is the white intensity to obtain a linear relationship between optical density and wetting phase saturation in the case of coal tar and oil the optical density was measured for uniform saturation values s w 0 s r w s r nw 1 to get the line coefficients fig 3 where s w is the wetting saturation s r w and s r nw irreducible and residual saturation respectively a small drainage imbibition 2d cell was used to relate the optical density to saturation colombano et al 2020 philippe et al 2020 the 2d cell used to obtain the correlation curves is made of the same materials pvdf for the main part and the same glass at the front of the cell measurements were conducted with the same light source and camera more details on this experimental setup can be found in philippe et al 2020 the results show that a linear relationship between optical density and saturation exists for the oil ethanol pair r2 0 91 therefore it is possible from the photographs obtained during pumping to determine the saturation however the results are more dispersed for the coal tar water pair r2 0 36 due to its opacity light visualization methods do not apply to complex fluids like coal tars for the two fluid pairs the relationship between optical density and saturation was established at both 20 c and 50 c no effect of temperature on the relationship between optical density saturation has been observed therefore the same relationships can be used for non isothermal cases to summarize a pumping experiment can be separated into four stages a initial setup of the fluid interface b pumping through the central well c transformation into shades of gray and determination of the area of interest aoi and d conversion of the light intensity field into a saturation field with the relationship determined above see fig 4 3 2 relative permittivity and saturation calibration the relative permittivity measurements were carried out on 12 points by tdr sensors these sensors have recently been used successfully to determine dnapl saturation in 1d columns filled with glass beads iravani et al 2020 colombano et al 2021 these sensors measure relative permittivity values between 1 and 80 accuracy of 1 between 1 and 40 the sensors are placed so that the pins are vertical to reduce their impact on fluid flow they also measure temperature and electrical conductivity but these measurements have not been interpreted the tdr thermocouple sensor is not sensitive enough to measure fast temperature variations as observed in pumping cases the use of these sensors to measure temperature was therefore excluded and type k thermocouples were used instead sensitivity 0 1 s precision 0 1 c tdr sensors are generally used and calibrated by the manufacturer to measure water content in the unsaturated zone water air from permittivity measurements in static conditions davarzani et al 2014 gao et al 2018 here we were working with another fluid pair as well as in a dynamic regime caused by the pumping conditions the permittivity measurements should therefore be calibrated on our medium made up of glass beads and oil ethanol or coal tar water to obtain correct saturation values the licheteneker rother model eq 3 has been developed and gives a relationship between relative permittivity and saturation for a three phase medium made up of two liquid phases and one solid phase brovelli and cassiani 2008 4 ε r α 1 φ ε s α φ s w ε w α φ 1 s w ε nw α with ε r the effective relative permittivity measured by the sensor and ε s ε w and ε nw the relative permittivity of the glass beads wetting liquid water ethanol or non wetting liquid coal tar canola oil respectively α 0 5 corresponds to the complex refractive index model crim for this relationship to work for our pumping experiments we must use calibration points at known saturation in addition at constant saturation the measured permittivity can be influenced by temperature iravani et al 2020 the temperature measured by thermocouples will be used to correct the permittivity values obtained measurements were made at different temperatures for each fluid pair to determine how temperature influenced relative permittivity during all pumping experiments the only tdr sensor that can be affected significantly by a change in saturation is k33 located in the bottom row and center of the 2d tank fig 2 fig 5a shows how the two phase relative permittivity values i e glass beads and a single liquid filling the pore volume changed as a function of the temperature at this point between 20 c and 32 c for canola oil and ethanol and fig 5b shows the same for coal tar and water in fluid saturated glass beads the permittivity of ethanol decreases slightly while that of oil increases with temperature the same behavior is observed for coal tar and water this corresponds to the observations of iravani et al 2020 in both cases we have found linear relationships in the temperature range studied therefore the following relationships can be used to compare permittivity values obtained at different temperatures in glass beads 5 porous media saturated with ethanol r 2 0 91 ε r e t 1 85 10 2 t 11 4 6 porous media saturated with canola oil r 2 0 92 ε r o t 3 45 10 2 t 5 51 7 porous media saturated with coal tar r 2 0 91 ε r ct t 3 26 10 2 t 5 56 8 porous media saturated with water r 2 0 97 ε r w t 4 99 10 2 t 19 2 4 results the results will first be presented for the oil ethanol pair this way we see the temperature effect independently of other parameters related to the complexity of coal tar 4 1 isothermal two phase flow fig 6 shows photographs converted to oil saturation fields for isothermal pumping experiments at 20 c and 32 c the oil saturation fields obtained by imaging show distinctively the cone of depression i e the oil ethanol interface as well as the presence of residual oil in the form of ganglia throughout pumping ethanol initially on top of the oil gradually replaces the oil the maximum velocity that can be reached inside the tank is equal to the pumping velocity 2 88 10 4 m s 1 at all temperatures for oil ethanol at 20 c case the critical velocity is equal to uc 2 00 10 5 m s 1 which is lower than the pumping velocity in this case viscous fingerings occur around the central area due to the higher fluid velocity and low viscosity ratio between ethanol and oil m 1 64 10 2 at 20 c we observe less fingering at 32 c when pre heating the tank before pumping at 32 c the viscosity ratio and the critical velocity are higher than at 20 c m 2 18 10 2 uc 3 31 10 5 m s 1 at 32 c although the shape of the cone is established around t 9 min it is observed that the oil residual saturation continues to decrease until t 15 min in the cone of depression area when comparing saturation fields obtained at 20 c and 32 c it appears that the oil ethanol interface is smoother during pumping at 32 c the pre heating also reduced the number of viscous fingers forming thanks to an increase in the viscosity ratio 4 2 non isothermal two phase flow 4 2 1 temperature profiles pumping in isothermal conditions has demonstrated the advantages of heating the tank and the viscous dnapl before pumping non isothermal conditions which simulate local heating are closer to situations encountered on real sites temperature variations measured at the thermocouples are shown in fig 7 during the entire 100 ml min 1 pumping experiment heating and pumping steps the initial time t 0 represents the start of the heating step in fig 8 temperatures are shown only during pumping for the experiment at 100 ml min 1 with t 0 representing the start of the pumping step the temperature on the left t11 and t31 of the 2d tank near the heating element increased quickly and reached a steady temperature of 48 c the increase in temperature caused by heating is also visible on the second column of thermocouples t12 t32 and t22 reaching between 27 c and 35 c the temperature of the rest of the tank is between 20 c and 25 c and does not seem to be affected by the heating element variations in the outside temperature should also be noted and are notably responsible for the general decrease in values after 15 h of heating during pumping temperatures ultimately had small variations except on the thermocouples located near the center t22 and t23 the temperature rises from 25 c to almost 30 c due to the replacement of oil by ethanol at higher temperatures 4 2 2 saturation fields fig 9 shows the oil saturation fields during pumping under non isothermal conditions at pumping flow rate of q 100 ml min 1 compared to the 20 c isothermal case at the same flow rate some observations join those already carried out on isothermal cases viscous fingerings appear during pumping in the central part of the tank significant between t 3 min and t 9 min until a smoother cone of depression is visible at the end of pumping t 15 min in addition it also seems that the cone is slightly unsymmetrical at t 3 min with a greater cone height on the left compared to the central pumping point than on the right also we can see that the residual saturation is higher in the right of the tank than the left this is linked to the temperature field gradient during the heating step this heating gradient causes the properties of canola oil and ethanol to change due the critical velocity varies between 2 00 10 5 m s 1 at 20 c and 3 31 10 5 at 32 c inside 2d tank the viscosity ratio also increases from 1 64 10 2 at 20 c to 2 64 10 2 at 32 c thus the critical velocity and the viscosity ratio are higher in the left of the tank than the right creating a difference in fingering number in the tank in the end residual saturation is also variable inside the tank depending on the temperature 4 3 effect of flow rate on non isothermal two phase flow the presence of viscous fingers was still observed at 100 ml min 1 in non isothermal conditions the pumping velocity is still higher than the critical velocity at which the flow is unstable a pumping experiment at 20 ml min 1 was done to observe the cone of depression and the saturation field in stable flow conditions the effect of the pumping flow rate on the temperature distribution in the tank is shown in fig 10 temperature fields obtained with surfer using the kriging method are also presented in appendix a fig a2 for both flow rates investigated here 20 ml min 1 and 100 ml min 1 the temperature increases during pumping on t12 t22 t33 however this increase is greater for the higher flow rate 100 ml min 1 a higher pumping rate effectively increases thermal advection in the tank therefore the generated heat from the heating element is better transferred to the rest of the tank the calculation of the rayleigh number ra see appendix a table a 1 showed that the natural convection should be considered in the wetting phase zone the comparison between saturation fields obtained for both flow rates is presented fig 11 regarding the pre and post treatment photographs obtained at 20 ml min 1 no viscous fingering has been observed throughout the pumping in this case the pumping velocity is equal to 5 76 10 5 m s 1 much closer to the order of magnitude of the critical velocity 10 5 m s 1 thus the absence of viscous fingering at 20 ml min 1 can be explained by a much more stable interface than at 100 ml min 1 also the time needed to reach the steady state is longer 40 min and the saturation in the residual zone is much smaller than we observed at 100 ml min 1 the interface is smoother throughout pumping and better describes a cone of depression typically represented using multiphase darcy s laws however from t 25 min we notice many oil ganglia present in the residual zone this number of ganglia is much higher on the right of the cone of depression less influenced by the heating than the left the number and size of these ganglia are bigger for higher flow rate 5 discussion 5 1 evaluation of saturation and residual saturation the change in ethanol saturation near the pumping point tdr k33 is shown in fig 12 as a dotted line for different pumping cases the calculated averaged saturations around the tdr k33 radius 1 cm obtained by imaging are also presented in the form of a point on this same graph we should note that the lrm gives a pixel scale saturation field and tdr gives centimeter scale saturation the corresponding imaging technique saturation reported in fig 12 comes from averaging the pixel scale saturation over a surface to obtain a centimeter scale quantity later we will validate the imaging technique against the mass recovery in non isothermal conditions the residual saturation is lower 0 28 than that observed at the isothermal 20 c case 0 40 moreover reducing the flow rate does not seem to significantly decrease residual oil saturation in this zone finally the time required for the ethanol to be detected at the tdr sensor increases when pumping is performed at higher temperature ethanol replaces the oil initially present in a smaller pore volume because of the greater number of viscous fingers at low temperature in this case the flushing fluid ethanol will therefore reach the pumping point in a shorter time and leaves a higher volume of trapped oil in the tank this observation is confirmed from the images taken during pumping the number of fingers decreases during the experiments with heating or at high temperature and the height of the cone is the greatest at 20 c in addition the lowest residual saturation 0 25 is still obtained in the isothermal case at 32 c it is interesting to note that even though the fluid interface front passed the sensor level oil saturation keeps on decreasing afterward the rate of decrease depends on the presence and the amount of the fingering this can be explained by the dynamic effect and remobilization of the discontinuous non wetting phase ganglia by the flow of wetting phase towards the pumping well also we see that the saturation values estimated by imaging and tdr are quite close except for the experiment at 20 c under isothermal conditions the presence of more fingers and ganglia in the 20 c isothermal experiment is a possible explanation for these differences therefore it should be possible to use tdr measurements to evaluate coal tar saturation during coal tar water experiments 5 2 fingering evaluation the fingers were measured as the difference between the interpolated profile corresponding to the oil ethanol interface and the actual profile on all the pixels the method that we used to determine the interface is different from the saturation conversions and does not require such a high range of optical densities the photographs were converted to binary images 0 for coal tar 1 for water with thresholding the same threshold value and method was applied for all coal tar water photographs the processes were executed with fiji imagej with this method we can distinguish the main line separating both liquids referred as the interface the obtained interface results using imagej fit the dnapl water interface observation this method does not allow us to distinguish between viscous and capillary fingering the flattened fingers using fiji at the oil ethanol interface during pumping at 20 c and 32 c and in non isothermal conditions are shown in fig 13 the scale is the same on each fingering profile the size of the fingers initially increases as the cone of depression is formed this trend is consistent with observing the formation of viscous fingerings during pumping at the interface on experiments at 100 ml min 1 also higher fingers are formed near the center of the tank than on the left and right sides then the fingering effects decrease once the ethanol begins to be recovered until it reaches an interface with a few fingers especially visible in the non isothermal case and the case at 32 c heating the tank reduces the length of the fingers at any time the main differences are mostly towards the center near the pumping point this result is consistent with the fact that the oil ethanol viscosity ratio is lower at 32 c than that at 20 c in the non isothermal case the fingers are shorter than those observed at 20 c but still longer than for the case at 32 c the fingering patterns at 20 ml min 1 also do not have large fingers at the center a lower number of fingers indicates that more oil is recovered since it is not trapped during pumping thus pumping at low flow rate effectively increases the oil volume extracted from the tank but the pumping time is also increased accordingly the average size of the fingers is also calculated between pumping in non isothermal and isothermal conditions at 20 c separately in the left and right parts of the tank the point separating these two parts is chosen as being the lowest point of the cone of depression determined from image analysis table 5 summarizes the mean and maximum observed finger length for the pumping experiments in isothermal 20 c and non isothermal conditions at 100 ml min 1 in the case of pumping at 20 c the mean and maximum finger sizes are basically of the same order of magnitude in the left and right sides of the tank 0 20 cm and 0 80 cm in the non isothermal case however the fingers in the left of the tank are much smaller 0 08 cm and 0 33 cm than the right 0 17 cm and 0 77 cm the length of the fingers is therefore reduced when pumping in a pre heated tank non isothermal conditions rather than at isothermal 20 c 5 3 recovery efficiency the recovered oil volumes estimated by imaging from analyzing the cone of depression zone are shown in fig 14 comparison of this data with mass scale measurements are also shown we found great agreement between oil volume measured with mass scale and estimated by imaging data for all experiments before breakthrough however we noted that the mass measurements are a little overestimated compared to the quantity of oil recovered estimated by imaging this is because a small part of ethanol bubbles and oil ethanol emulsion started to be recovered near the breakthrough point and is included in the mass measurements we could not separate both liquids directly during the experiments and this results in a slight overestimation of the oil volume recovered compared to the oil volume estimated with imaging on the figure isothermal pumping at 32 c recovers more oil than at 20 c pumping under non isothermal conditions recovers more oil than isothermal pumping a lower flow rate 20 ml min 1 collects a higher volume of oil from the tank than at a high flow rate 100 ml min 1 table 6 shows the radius of influence the residual saturations in the cone at the end of pumping and for the left and right of the cone of depression zone and the remediation efficiency values obtained by imaging for each oil pumping experiment we found that an increase of only 12 c reduces the residual saturation from 0 36 to 0 20 moreover we observed improved remediation efficiency linked to a slight increase in the radius of influence and a decrease in viscous fingerings heating the left side of the tank to 60 c before pumping at 100 ml min 1 non isothermal conditions also reduced the residual saturation and increased the efficiency compared to pumping at 20 c however pumping at 32 c was still more efficient than pumping under non isothermal conditions in addition in non isothermal conditions the lower pumping rate slightly increased the radius of influence as well as the efficiency the residual saturation also decreased thanks to a reduction in the formation of viscous fingers during pumping the residual saturation estimated by imaging in the cone of depression was almost identical between the left and right parts of the pumping in isothermal cases while it was lower by 3 at 100 ml min 1 and 7 at 20 ml min 1 in non isothermal conditions finally the residual saturations estimated by imaging are still close to the ones directly measured with the tdrs for instance at 20 c the measured residual saturation is 0 40 while it is equal to 0 36 when considering the whole residual zone of the tank the masses measured using the automatic balance were converted to mass flow rates by interpolation in fig 15 for oil ethanol initially pumping only recovers oil constant flow until the breakthrough time which is different for each experiment this breakthrough time represents the point at which ethanol begins to be recovered as well after this breakthrough time the oil is no longer pumped and a new constant value is reached where only the ethanol is recovered the residual saturation is reached the breakthrough time increases when the tank is heated this is due to the reduction of viscous fingers in the tank which allows the oil to be pumped for a longer time and therefore increases the remediation efficiency the initial and final flow rates between the 20 c isothermal case and the non isothermal case are equal however the ethanol arrived later and the oil ethanol mixture is recovered for a shorter time the temperature at the pumping point is effectively 20 c but the lower viscosity ratio allows oil to be recovered for a longer time 6 real dnapl water case experiments with oil ethanol have provided a better understanding of how heating can affect the recovery of a viscous dense liquid phase in a saturated medium a temperature increase is linked to a residual saturation decrease and an increase in the pumping radius the coal tar used here has in fact more variable properties because of its chemical complexity also the coal tar adheres to the glass internal wall of the tank and is opaque to visible light so the previous imaging techniques cannot be used to estimate the water saturation even if the water coal tar interface can be distinguished the saturation calculations are generally wrong because coal tar completely absorbs visible light in this case the optical density measurements do not have a range sufficient to convert them into saturation when we converted the photographs into grayscale images we observed shades of gray corresponding to different saturations for oil ethanol however that was not the case for coal tar water the initial tank preparation and the heating phase did not present any problems however during the pumping phase the measured flow rate was much lower when recovering the coal tar instead of the 15 min used for the oil the pumping took 45 min before reaching a steady state the sticky nature of the coal tar and clogging effects in the tubing could both explain this difference grayscale images taken during coal tar pumping are shown in fig 16 6 1 temperature and saturation results the thermal properties of the oil ethanol and coal tar water fluid pairs are quite different the thermal conductivity and specific heat capacity of the oil used is indeed quite close to that of coal tar but ethanol s is much lower than water s the temperature profiles measured at the thermocouples are presented in fig 17 the temperature increases significantly when pumping on t11 t12 t22 t23 t31 these increments correspond to the arrival of heated water during pumping temperature fields obtained with surfer are also presented in appendix a fig a3 the saturation profiles have been plotted using tdr data fig 18 compares the coal tar saturation measured at the sensor k33 in isothermal conditions 20 c and non isothermal conditions heating setpoint 60 c coal tar residual saturation was estimated to be 0 39 at 20 c pre heating at 60 c the left side of the tank before pumping coal tar reduces this value to 0 27 the reduction is similar to that observed for oil ethanol from 0 40 at 20 c to 0 28 in non isothermal conditions 6 2 fingerings even if the estimation of coal tar saturation is not accurate using imaging it is still possible to interpolate the coal tar water interface with binary images 0 coal tar 1 water and thresholding thus we can still measure the fingers in the case of coal tar fig 19 shows how the fingers changed during pumping the fingers are much larger with the coal tar and water liquid pair than with the oil and ethanol model pair the average and maximum finger length observed for coal tar water is 0 39 cm and 1 44 cm long these values are 3 and 2 times higher than observed for oil ethanol respectively however the same trend was observed the number and length of the fingers increase up to t 30 min during the formation of the cone of depression then decrease until the end of the pumping the fingerings are also larger on the left than on the right the effect of heating is much more noticeable on fingerings with coal tar water than for oil ethanol the dimensionless numbers and trends with temperature are similar thus the differences in fingering patterns may come from the complexity of coal tar 7 conclusions a 2d tank experimental setup was designed to reproduce pumping cones of depression for the flow of two immiscible liquids in 1 mm glass beads the addition of a heating element similar to those used on real sites allowed pumping operations to be performed in non isothermal conditions and the observation of how a pre heating step influenced the temperature and saturation in the tank in isothermal conditions homogeneous heating improves the efficiency of oil pumping the residual saturation estimated by imaging is decreased from 0 36 to 0 20 by increasing the medium temperature from 20 c to 32 c then the study of pumping in non isothermal conditions showed how effective thermal enhancement is to improve the efficiency of oil pumping heating reduces the length and number of fingers observed during pumping and improves remediation efficiency in non isothermal conditions a heating element has been used to generate a temperature field in the 2d tank to reproduce conditions closer to those encountered in a real polluted site the left part of the cone of depression in the tank close to the heating element is thus warmer than the right part the latter remains close to room temperature that means that the dynamic viscosity of the oil varies in the tank unlike the isothermal case where the dynamic viscosity is constant when pumping in non isothermal conditions a slightly unsymmetrical cone forms initially but the final state is symmetrical we observed that the fingering effects are smaller in the left heated part of the cone of depression in the tank than the right the residual saturation estimated by imaging also follows this pattern recovery in non isothermal conditions is therefore more efficient 7 at 20 ml min 1 3 at 100 ml min 1 in the heated parts also we applied a lower flow rate of 20 ml min 1 in non isothermal conditions in this case a lower number of fingers appeared and a higher volume of oil 5 more was recovered from the tank with an increase in temperature the critical velocity and the viscosity ratio increased and helped to stabilize the interface between the non wetting and wetting phase we applied these observations to a real pollutant pumping case coal tar pollution first the temperature field is quite different due to the thermal properties of water which allow better heat transport especially during the pumping phase the observation of the fingering changes showed a higher difference in finger length between the left and right parts thanks to the action of the heating before pumping unfortunately the imaging does not allow to obtain coal tar saturation fields due to the excessively high coal tar opacity however we could analyze the evolution of the coal tar saturations inside the 2d tank using tdrs sensors these experimental measurements have improved the understanding of non isothermal flows in porous media they can be used as a basis for the development and validation of a numerical model coupling two phase flow and heat transfers this type of model should allow the simulation of the behavior of two immiscible fluids and make prospective analysis prior to remediation processes for industrial applications in the case of unfavorable viscosity ratio this study leads to the recommendation of heating the soil as homogeneously as possible and then pump at the lower flow rate possible to recover the highest amount of dnapl possible from the soil declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was performed as part of the bioxyval project the authors would like to thank ademe french environment and energy management agency for co funding the project under the ami program remea and anrt for providing the ph d grant for nicolas philippe we thank ali iravani and benjarese oniangue for their help on experiment measurements we gratefully acknowledge the financial support provided to the pivots project by the région centre val de loire and the european regional development fund appendix a temperature field during 2d tank heating temperature changes are represented in the form of 2d fields via the surfer software by the kriging method fig a1 shows the effect of flow rate on the temperature field at the start and end of pumping fig a1 comparison of the temperature fields before and after pumping in non isothermal conditions for two pumping rates 20 ml min 1 and 100 ml min 1 fig a1 the temperature is much higher on the left of the tank than on the right in non isothermal conditions due to the presence of the heating element the temperature seems to redirect slightly towards the central and lower part of the tank during pumping for both flow rates certainly linked to the infiltration of ethanol the temperature variations occurring during pumping are more easily observed in fig a2 which compares the ratio between temperature measured at the end of pumping divided by the temperature just before pumping t 0 of pumping fig a2 comparison of the ratio between temperature after pumping and initial temperature before pumping in non isothermal conditions for two pumping rates 20 ml min 1 and 100 ml min 1 fig a2 after pumping the temperature increases especially in the center of the tank this corresponds to the arrival of hot ethanol replacing the oil initially present at high flow rate the temperature ratio is substantial in the left and center parts of the tank 0 05 m x 0 35 m due to high convection effects in conclusion pumping at a lower flow rate gives a more uniform temperature field which reduces the viscosity ratio between the oil and ethanol this effect could translate into a lower residual saturation at lower flow rate the temperature fields before and after pumping are shown in fig a3 for the coal tar water fluid pair fig a3 comparison of the temperature fields before t 0 min and after t 40 min pumping in non isothermal conditions for coal tar water fluids pair fig a3 the high temperature is redirected to the central part of the tank during pumping certainly linked to the infiltration of heated water fig a4 compares temperature ratio fields between oil ethanol and coal tar water pumping cases the temperature ratio is defined as the temperature after pumping over the initial temperature after heating fig a4 comparison of the ratio between temperature after pumping and initial temperature before pumping under non isothermal conditions for pumping oil at steady state t 15 min and coal tar t 40 min q 100 ml min 1 fig a4 the temperature also increases in the upper left part of the coal tar water case unlike in the oil ethanol case where most of the pumping increases the temperature in the center of the tank the thermal conductivity and specific heat capacity of water are higher than those of ethanol this allows the water to continue heating while pumping the calculation of the reynolds number re thermal péclet number pe t and the rayleigh number ra help to understand the heat transfer regimes in non isothermal conditions these dimensionless numbers can be calculated as a1 re u p ℓ ν pe t u p ℓ α ra gβ t h t c αν k h where t h k and t c k are respectively the hot 60 c and the cold 20 c temperature in the system β k 1 is the coefficient of thermal expansion ℓ m characteristic length solid particle size h m height of liquid in 2d tank α m2s 1 thermal diffusivity ν m2s 1 kinematic viscosity and k m2 intrinsic permeability the reynolds and thermal péclet numbers were calculated using the fluid velocity around the pumping point u p the calculated numbers are listed in table a1 table a1 dimensionless numbers for different liquid under non isothermal conditions q 100 ml min table a1 liquid oil ethanol coal tar water c p jkg 1 k 1 1840 2420 2100 4180 λ wm 1 k 1 1 66 10 1 1 68 10 1 1 32 10 1 5 98 10 1 β k 1 5 03 10 4 6 20 10 4 4 82 10 4 2 60 10 4 re 2 00 10 3 1 2 10 1 6 00 10 3 2 87 10 1 ra 0 81 62 3 72 37 pe t 2 91 3 35 5 02 2 01 the low values of reynolds number indicate that the flow in porous media can be considered as a creeping flow the rayleigh numbers in wetting phases are more than one order of magnitude higher than the ones for non wetting phases this indicates that the natural convection should be higher in the upper part of the 2d tank during the heating stage the rayleigh number of the saturated ethanol zone is around twice the saturated water zone the thermal péclet number of more than one shows that the convection is dominated in all pumping experiments our preliminary simulation of heat transfer during the heating stage shows that the heat loss from 2d tank walls surface is low thanks to the double walled systems however the most important heat loss can be reproduced from the top surface of the 2d tank which is in direct contact with air circulation inside the fume hood a coefficient of heat loss around 50 100 w m 2 k 1 
1356,we propose a simulation optimization so model based on a novel two step strategy for the optimal design of groundwater remediation systems the so models are developed by coupling simulation models directly or through the extreme learning machine elm with evolutionary hunting strategy based metaheuristics ehsms in the first step ehsms with a combinatorial optimization technique are used to obtain optimal pumping locations by minimizing the percentage of contaminant mass that remained in the aquifer while keeping the pumping strategy as constant in the second step the optimal pumping locations are directly used as input and a composite function is employed to minimize the sum of the water extraction rates and the percentage of extracted contaminant mass by constraining hydraulic heads and contaminant concentrations the performance of the two step strategy is found to be slightly better and computationally more efficient than the alternate approach moreover various statistical measures suggest the superiority of ehsms over other metaheuristics for groundwater remediation keywords extreme learning machine elm grey wolf optimization gwo whale optimization wo harris hawks optimization hho groundwater remediation notations a area of the aquifer l2 a t a coefficient which decreases linearly with respect to the iteration number t within the range 2 0 b c a constant to define the logarithmic spiral shape b i thresholds of hidden neuron i c contaminant concentration ml 3 c s k contaminant concentration of dissolved species k ml 3 c r crossover operator c r regularization term to resolve overfitting issue c uniformly distributed random variable within the range 0 2 d r uniformly distributed random integer variable within the range 1 d d dimension of search space d h hydrodynamic dispersion coefficient tensor l2 t 1 e escaping energy of the rabbit f activation function f x fitness function evaluated at a point x f c multiplying factor j jump strength of a rabbit h hidden layer output matrix h t transpose matrix of h h moore penrose generalized inverse of matrix h h b aquifer thickness l i identity matrix lb lower bound k hydraulic conductivity tensor lt 1 k q constant pumping rate l uniformly distributed random number within the range 1 1 m mutation scaling factor m 0 amount of contaminant mass in the aquifer prior to pumping m m r amount of contaminant mass remained in the aquifer after pumping m m e amount of extracted mass with a pumping strategy m n number of population n d number of datasets n r randomly generated integer n g number of grids used to represent the aquifer domain p uniformly distributed random numbers within the range 0 1 q s volumetric flow rate per unit volume representing sinks and or sources of water t 1 r escaping probability of a rabbit r n chemical reaction term ml 3 t 1 r d coefficient of retardation s s specific storage of the porous media l 1 s y specific yield s a vector having a dimension of 1 d s i u 0 1 t maximum iteration number t time t u seepage velocity lt 1 ub upper bound w i weight vector which connects the input neurons to the hidden neuron i w r vector of all pumping wells with randomly generated locations x j input data y j output data x r t position of randomly selected whale x i t position of candidate whale x i t 1 new position of candidate whale x g t position of the best whale x α t x β t x δ t position of alpha beta and delta wolves x 1 x 2 x 3 possible position of the candidate wolf which is computed based on the position of alpha beta and delta respectively x r1 t x r2 t x r3 t three distinct randomly selected candidate from the population x e opposite of the candidate solution x i δx i δy i area of grid i l2 y i t 1 muted solution z ij t 1 offspring solution α l longitudinal dispersivity l α t transverse dispersivity l β i output weight which connects the hidden neuron i to output neurons β c a constant parameter having a value of 2 5 del operator ϕ hydraulic head l η porosity of the subsurface medium ε scaling factor of candidates updating 1 introduction groundwater remediation is a process to treat contaminated groundwater by removing the contaminants or converting them into harmless compounds mckinney and lin 1996 the remediation of the contaminated groundwater is economically costly and one of the major technological and environmental challenges in water resources engineering piscopo et al 2015 a recent estimate states that the cost to remediate all the contaminated sites in the united states usa will be around 110 billion to 127 billion national research council 2013 groundwater remediation is also a very time consuming process and it may take many decades of continuous remediation practice to decontaminate an aquifer sadeghfam et al 2019 hence the selection of proper remediation techniques and the design of optimal remediation strategies are of great importance for decision makers to save cost and time piscopo et al 2015 one of the most widely used groundwater remediation techniques is the pump and treat pat method matott et al 2006 in the pat the groundwater is hydraulically contained and extracted to the ground surface by pumping and then the contaminated water is treated using a filtering or stripping system majumder and eldho 2020 mategaonkar and eldho 2012 the treated water is injected into the aquifer for groundwater recharge or diverted for industrial irrigation or domestic uses majumder and eldho 2020 the effective and economical design of the pat remediation system depends on various factors such as the number and location of pumping wells pumping rates injection locations injection rates and treatment processes of contaminated water sadeghfam et al 2019 seyedpour 2019 the goal of the pat remediation system is to compute optimal values of the factors mentioned above which minimize the remediation cost or amount of contaminant mass remaining in the aquifer after a specified time yang et al 2013 in earlier studies the locations of the pumping wells were predefined along the centreline of the contaminant plume if the aquifer is homogeneous and the plume is uniform seyedpour 2019 wang and zheng 1997 however for a wider uniform plume or non uniform plume it is not possible to accurately predict the pumping well location manually huang and mayer 1997 in a previous groundwater remediation study the pumping well locations are incorporated in the management model as a continuous function of the spatial domain to find optimal pumping locations and optimal values of other decision variables simultaneously wang and ahlfeld 1994 this approach employs interpolation functions to convert discrete pumping well locations of the finite difference model fdm as a continuous function of the spatial domain wang and ahlfeld 1994 nevertheless such interpolation steps may add a substantial amount of mathematical inaccuracies to the management model in another study the optimal well locations in the heterogeneous aquifer are obtained using the combinatorial optimization method rizzo and dougherty 1996 however they have not used the results for computing optimal pumping rates in the so approach the optimization model executes the simulation model repetitively to obtain the optimal strategy majumder and eldho 2020 mckinney and lin 1996 yang et al 2013 the computational speed of the finite difference based simulation model can be enhanced significantly by approximating it using a surrogate simulator yan and minsker 2006 previous studies used various machine learning algorithms such as response matrix approach rma artificial neural network ann support vector machine svm and extreme learning machine elm to approximate computationally expensive groundwater flow and contaminant transport simulators asher et al 2015 zhao et al 2020 among them the elm algorithm with arbitrarily assigned input weights and hidden layer thresholds can accurately map the nonlinear relationship of any compact input output dataset if the activation function is infinitely differentiable huang et al 2006 the elm is computationally very efficient and more accurate than single layer feed forward neural networks and support vector machines the basic elm shows variances in the generalization ability due to randomly generated input weights and hidden layer thresholds wang et al 2016 however the generalization ability of elm can be significantly enhanced using ridge regression with a regularization parameter shao and er 2016 zhang et al 2020 the most popular metaheuristics for groundwater management and remediation are genetic algorithm ga differential evolution de genetic programming gp simulated annealing sa tabu search ts harmony search hs particle swarm optimization pso and cat swarm optimization cso luo et al 2014 majumder and eldho 2016 mategaonkar and eldho 2012 mckinney and lin 1996 yang et al 2013 zhao et al 2020 recently hunting strategy based metaheuristics hsms such as grey wolf optimization gwo whale optimization wo and harris hawks optimization hho are getting unprecedented attention in various engineering fields heidari et al 2019 mirjalili et al 2014 mirjalili and lewis 2016 the application of the hsms to obtain the optimal solution of unimodal multimodal and composite benchmark functions reveals superior performances of hsms over other traditional metaheuristics among the hsms the gwo is already used to compute optimal pumping rates for groundwater remediation under steady state conditions majumder and eldho 2020 the performances of hsms can be further enhanced by adopting various measures such as hybridizing different metaheuristics adding levy flight incorporating elite opposition based learning approximating the random processes of metaheuristics by chaotic map incorporating biological evolution operators and adding elimination mechanism chen et al 2020 sihwail et al 2020 wang and li 2019 in this study simulation models are developed by solving the groundwater flow and contaminant transport equations using finite difference based modflow 2005 and mt3dms models harbaugh and arlen 2005 zheng and wang 1999 an extreme learning machine elm based proxy simulator is used to approximate the numerical simulation model the generalization ability of the elm model is enhanced using ridge regression with a regularization parameter further evolutionary hunting strategy based metaheuristics ehsms are proposed by modifying the grey wolf whale and harris hawks optimization algorithms by incorporating biological evolution operators elimination mechanisms based on survival of fittest and elite opposition based learning the performances of the ehsms are tested against well known unimodal multimodal and composite benchmark functions further various simulation optimization so models are developed by coupling the simulation models proxy simulators with ehsms based on a novel two step approach the so models are used to obtain optimal pumping well locations and optimal pumping rates for groundwater remediation in the homogeneous and piecewise heterogeneous aquifer systems 2 methodology the partial differential equation pde depicting groundwater flow processes through the subsurface can be written as follows freezy and cherry 1979 1 k ϕ q s s s ϕ t where k is the hydraulic conductivity tensor lt 1 ϕ is the hydraulic head l q s is the volumetric flux rate per unit volume representing sinks or sources of water t 1 s s is the specific storage of the porous media l 1 t is the time t is the del operator i x j y k z and i j k are the unit vectors along x y and z axis the seepage velocity u through the subsurface can be computed using darcy s equation freezy and cherry 1979 2 u k η ϕ x the pde describing contaminant fate and transport through the subsurface can be expressed as batu 2005 3 r d η c t η d h c η u c q s c s k r n where η is the porosity of the subsurface medium u is the seepage velocity lt 1 d h is the hydrodynamic dispersion coefficient tensor l2t 1 c is the contaminant concentration ml 3 c s k is the contaminant concentration of dissolved species k ml 3 r d is the coefficient of retardation and r n is the chemical reaction term ml 3 t 1 2 1 extreme learning machine elm elm is a single hidden layer neural network in which the input weights and hidden layer thresholds are arbitrarily assigned with a continuous activation function elm can approximate the nonlinear relationship of any compact input output dataset huang 2014 liu et al 2020b a pictorial representation of elm architecture is shown in fig 1 let us assume n d distinct dataset x j y j r n r m j 1 2 n d where x j x j1 x j2 x j3 x jn n t r n is the input data and y j y j1 y j2 y j3 y jm m t r m is the output data the nonlinear relationship between input and output data in elm can be mathematically expressed as huang 2014 4 i 1 l β i f w i x j b i y j j 1 2 n d where f is the activation function l is number of neurons in the hidden layer w i w 1i w 2i w 3i w ni n t is the weight vector which connects the input neurons to the hidden neuron i b i is thresholds of hidden neuron i and β i β i1 β i2 β i3 β im l m t is the output weight which connects the hidden neuron i to output neurons eq 4 can be written in matrix form as huang et al 2006 5a h β y 5b h f w 1 x 1 b 1 f w l x 1 b l f w 1 x n d b 1 f w l x n d b l n d l 5c y y 1 y 1 y 3 y n n d m t where h is known as the hidden layer output matrix the output weight β can be computed by minimizing the norm of the loss function which can be expressed as 6 minimize h β y 2 the output weight β can be computed using least square estimate thus can be expressed as 6a β h y where h is the moore penrose generalized inverse of h it should be noted that the inverse of matrix h does not exist if the matrix is singular or rectangular the matrix h is calculated through orthogonal projections as 6b h h t hh t 1 in elm randomly generated input weights and hidden layer thresholds usually lead to variance in the generalization ability the stability and generalization performance of elm can be enhanced significantly using the ridge regression based regularization method shao and er 2016 zhang et al 2020 the ridge regression minimizes the norm of the difference between model output with the actual output and the norm of the output weight the aim of ridge regression is to obtain the smaller value of the output weights to minimize variance in prediction shao and er 2016 7 minimize h β y 2 β minimize h β y 2 1 c r β 2 the solution of the above equation can be expressed as 7a β h t hh t i c r 1 y where i is the identity matrix and c r is the regularization term which is used to resolve overfitting issues deng et al 2014 2 2 harris hawks optimization hho harris hawks are well known for their organized cooperative foraging behavior of hunting rabbits hawks perform a variety of chasing styles based on the nature of situations and escaping patterns of rabbits two distinctive features of hawks in hunting rabbits are the surprise pounce and switching strategy the surprise pounce is a skillful maneuvering strategy in which a group of hawks cooperatively attack a rabbit from all directions in close combat between hawks and a rabbit the leader hawk stoops towards the rabbit and occasionally disappears to confuse it in such cases another hawk of the group takes the leadership to continue the chasing process this strategy of hawks is known as the switching strategy the hho algorithm is proposed by mathematically modeling the aforementioned behavior of harris hawks in capturing rabbits heidari et al 2019 2020 in hho a parameter namely escaping energy of rabbit e is used to demarcate the exploration and exploitation mechanism the parameter e can be mathematically represented as heidari et al 2019 8a e 2 r 1 a t 8b a t 2 1 t t t 1 2 3 4 t where the coefficient a t decreases linearly with respect to the iteration number t within the range 2 0 t is the maximum iteration number r is a uniformly distributed random number within the range 0 1 the parameter e decreases adaptive and stochastic manner with respect to the iteration number t within the range 2 2 if e 1 hawks explore all the possible locations to detect the rabbit if e 1 hawks strategically try to capture the rabbit by exploiting its possible movements in the proximity the various phases of hho are presented in the following subsection with mathematical details fig 2a may help to envisage various phases of hho easily 2 2 1 exploration e 1 in the exploration phase hawks wait in random locations and observe the surroundings with the hope of discovering a rabbit once a rabbit is detected hawks try to encircle it by changing its position based on the position of other hawks and the position of the rabbit a uniformly distributed random number q within the range 0 1 is used to determine the position update mechanism of hawks if q 0 5 hawk updates position with respect to the position of other neighbor hawks in contrast if q 0 5 hawk updates position with respect to the position of rabbit heidari et al 2019 9a x t 1 x rand t r 1 x rand t 2 r 2 x t q 0 5 x rab t x m t r 3 lb r 4 ub lb q 0 5 9b x m t 1 n i 1 n x i t where x t is the position of a candidate hawk x t 1 is the new position of the candidate hawk x rand t is the position of a randomly selected hawk x rab t is the position of the rabbit x m t is the mean position of all the hawks involved in hunting n is the total number of hawks lband ub are the lower bound and upper bound of position x and r 1 r 2 r 3 r 4 and q are uniformly distributed random numbers within the range 0 1 2 2 2 exploitation e 1 in the exploitation phase hawks encircle approach and perform a surprise pounce to hunt the rabbit rabbit often moves abruptly to escape from threatening situations four possible strategies are proposed to emulate the escaping behaviors of rabbits and the chasing style of hawks to state the probability of escaping a rabbit a uniformly distributed random variable namely escaping probability r within the range 0 1 is introduced heidari et al 2019 2 2 3 soft besiege r 0 5 e 0 5 the energy of the rabbit is still intact and it performs misleading jumps to escape from risky situations the hawks encircle the rabbit softly to make it fatigued heidari et al 2019 10a x t 1 x rab t x t e j x rab t x t 10b j 2 1 r 5 where j is the jump strength of the rabbit which is used to imitate the random misleading motion of rabbit and r 5 is the uniformly distributed random variable within the range 0 1 2 2 4 soft besiege with progressive rapid dives r 0 5 e 0 5 in a close encounter the rabbit shows a random deceptive motion in response to the abrupt rapid dives of hawks such leapfrog behavior of the hawk and rabbit can be approximated mathematically by levy flight in an attempt to capture the rabbit the hawks either make a regular dive or a complicated dive levy flight is used to approximate the complicated dive heidari et al 2019 the two strategies regular and complicated can be mathematically modeled using the following equations 11a x 1 t 1 x rab t e jx rab t x t 11b x 2 t 1 x rab t e jx rab t x t s lf d 11c lf x 0 01 u σ v 1 β 11d 11e x t 1 x 1 t 1 if f x 1 t 1 f x t 11f x t 1 x 2 t 1 if f x 2 t 1 f x t where lf is the levy flight function uand vare uniformly distributed random variables within the range 0 1 β c is a constant parameter having a value of 2 5 d is the dimension of the search space sis a vector having a dimension of 1 d elements of the vector s are uniformly distributed random variables within the range 0 1 and f x is the fitness function evaluated at point x 2 2 5 hard besiege r 0 5 e 0 5 in this phase the escaping energy of the rabbit is low due to extreme exhaustion hence the rabbit can not make the abrupt misleading jump anymore to deceive the hawks the hawks quickly perform surprise pounce to capture the rabbit heidari et al 2019 12 x t 1 x rab t e x rab t x t 2 2 6 hard besiege with progressive rapid dives r 0 5 e 0 5 in this phase the escaping energy of the rabbit is low and hawks try to catch the rabbit by making regular or complicated dive heidari et al 2019 13a x 3 t 1 x rab t e jx rab t x m t 13b x 4 t 1 x rab t e jx rab t x m t s lf d 13c x t 1 x 1 t 1 if f x 3 t 1 f x t 13d x t 1 x 2 t 1 if f x 2 t 1 f x t 2 3 whale optimization wo whale optimization wo is a metaheuristic algorithm proposed by imitating the cooperative hunting behavior of humpback whales mirjalili and lewis 2016 in an attempt to encircle prey krill small fish the humpback whales frequently dive underwater and then swim upwards through a progressive shrinking circle and spiral shaped path in this process whales produce bubbles in the spiral path which helps to trap prey this unique hunting strategy of the humpback whale is commonly known as bubble net foraging in the wo algorithm the exploitation and exploration strategies are modeled by imitating the bubble net attacking mechanism and random search behavior of whale respectively liu et al 2020a mirjalili and lewis 2016 the parameter e is used to control the exploitation and exploration mechanism of whale optimization parameter e has the same mathematical significance of escaping energy of the rabbit 2 3 1 random search mechanism e 1 in this case each whale randomly searches for prey by monitoring the movement of other whales the position of a candidate whale is updated according to the position of a randomly selected whale mirjalili and lewis 2016 14 x i t 1 x r t e c x r t x i t where x r t is the position of randomly selected whale x i t is the position of candidate whale x i t 1 is the new position of candidate whale and c is a uniformly distributed random variable within the range 0 2 2 3 2 bubble net attacking e 1 in the bubble net attacking strategy the humpback whales encircle the prey by concurrently utilizing a shrinking encircling process and spiral position updating strategy in the shrinking encircling process candidate whales update their positions according to the position of the best whale the parameter e decreases randomly with respect to the iteration number within the range a a to facilitate progressive shrinking of the distance between whales and prey further the helix shaped movement of whales towards prey can be approximated using a spiral equation as shown in fig 2b the probability of happening both the strategy is the same mathematically both the approaches can be approximated as mirjalili and lewis 2016 15a x i t 1 x g t e c x g t x i t if p 0 5 15b x t 1 x g t x i t e b c l cos 2 πl x g t if p 0 5 where x g t is the position of the best whale pare uniformly distributed random numbers within the range 0 1 l is a uniformly distributed random number within the range 1 1 and b c is a constant used to define the logarithmic spiral shape 2 4 grey wolf optimizer gwo gwo is a metaheuristic algorithm developed by mimicking the social leadership hierarchy and hunting strategy of grey wolves luo et al 2019 mirjalili et al 2014 wolves usually live in a pack and hunt prey with collective efforts a pack comprises of four different types of wolves alpha beta delta and omega alpha is the most dominant wolf and responsible for making the decision of hunting beta obeys the command of alpha and also communicates the decision to delta and omega beta is also liable to report the activities of the pack to alpha occasionally beta may assist alpha in decision making delta wolves follow the command of alpha and beta but they dominate the omega wolves based on the nature of activities a delta wolf may be a scout sentinel hunter elder or caretaker the least dominant omega wolves are bound to follow the command of alpha beta or delta however in terms of importance the omegas are not the least significant wolves the absence of enough omega wolves in the pack may cause internal conflict and collapse of the social dominance hierarchy structure majumder and eldho 2020 mirjalili et al 2014 the social hierarchy structure of grey wolves is shown in fig 3 a the pictorial view of the position updating mechanism of wolves to hunt prey is shown in fig 3 c the exploration and exploitation mechanism of the gwo algorithm is controlled by two parameters e and c the parameter e has the same mathematical and physical significance as discussed in the previous section also see fig 3 b additionally the variation of e with respect to the iteration number is shown in fig 3 d the parameter c is a uniformly distributed random number within the range 0 2 the value of c 1 assists the algorithm in exploring the search space conversely the algorithm shows exploitative behavior if the value of c 1 in the search space first second and third best minimum solutions are assigned to alpha beta and delta using the following equations 16a if f x t f x α t then x α t x t f x α t f x t 16b if f x t f x α t f x t f x β t then x β t x t f x β t f x t 16c if f x t f x α t f x t f x β t f x t f x δ t then x δ t x t f x δ t f x t in each iteration the position of alpha beta and delta are updated whenever a candidate wolf identifies a better position the rest of the wolves update their positions according to the positions of alpha beta and delta using the following equation mirjalili et al 2014 17a x 1 x α t e c x α t x t 17b x 2 x β t e c x β t x t 17c x 3 x δ t e c x δ t x t 17d x t 1 x 1 x 2 x 3 3 where x α t is the position of alpha x β t is the position of beta x δ t is the position of delta x t is the position of a candidate wolf x 1 x 2 and x 3 are the possible position of the candidate wolf which is computed based on the position of alpha beta and delta respectively x t 1 is the position of candidate wolf in next iteration and f x is the fitness function evaluated at point x 2 5 biological evolution in nature living beings get fitter stronger over the successive generations through biological evolution and the survival of fittest sof principle it can be assumed that candidates hawks whales wolves also evolve over the successive generations and better candidate survives wang and li 2019 the evolved candidate is selected as a part of the population for next generation if it improves the fitness function value the biological evolution process can be modeled using differential evolution de operators the biological evolution takes place by three operators mutation crossover and selection wang and li 2019 2 5 1 mutation in each generation for each candidate solution xi t a muted solution is generated by adding a randomly selected solution with the weighted difference between another two randomly selected solutions storn and price 1997 18 y i t 1 x r 1 t m x r 2 t x r 3 t i 1 2 n t 1 2 t x r 1 t x r 2 t x r 3 t where x r1 t x r2 t and x r3 t are three distinct randomly selected candidate solution in tth generation from n number of population y i t 1 is the muted solution m mutation scaling factor is a uniformly distributed random number within the range 0 2 0 8 and nis the number of population t is the total number of generation 2 5 2 crossover in the crossover strategy an offspring solution is generated by exchanging the elements of the candidate solution vector with the muted solution vector storn and price 1997 19 z ij t 1 y ij t 1 c r r or j d r x ij t c r r i 1 2 n j 1 2 d where c r is the crossover operator r is the uniformly distributed random variable within the range 0 1 z ij t 1 is the offspring solution d r is a uniformly distributed random integer variable within the range 1 d and d is the dimension of the solution vector 2 5 3 selection the offspring solution is selected for next generation if the fitness value obtained by the offspring solution is better than the fitness value evaluated by the candidate solution storn and price 1997 20 x i t 1 z i t 1 f z i t 1 f x i t x i t f z i t 1 f x i t i 1 2 n where f x i t is the fitness function evaluated at the point x i t 2 6 elimination mechanism based on the survival of the fittest candidate in the process of biological evolution vulnerable candidates of a population will be eliminated due to various reasons such as hunger disease and natural disaster meanwhile new candidates will join the population due to natural birth or other reasons wang and li 2019 the sof mechanism can be implemented in the hsm using the following steps a after each iteration sort the fitness values in the ascending order arrange the position of candidates with respect to the sorted fitness values b generate a random integer n r within the range n 2 ε n ε where n c is the total number of candidates and ε is a scaling factor of candidates updating the value of the scaling factor ε should always be greater than one so that the good candidates are preserved for the next generation c remove n r number of candidates with low fitness values d add n r number of new candidates with randomly generated positions a high value of n r results in better exploration however it will cause slow convergence a very small value of n r is not ideal to maintain enough diversity in the population and thereby reduces the exploration capability of the algorithm 2 7 elite opposition based learning eobl elite opposition based learning eobl is a standard procedure in the field of machine learning to improve solutions soncco álvarez et al 2019 in eobl for a candidate solution an opposite solution is generated subsequently based on the fitness value the better solution is selected as the candidate for the next generation dhargupta et al 2020 to explain the eobl let us assume a candidate solution x i x i 1 x i 2 x i d and the corresponding opposite solution x e x e 1 x e 2 x e d using eobl the opposite solution is computed using the following equation 21 x i j e lb j ub j x i j j 1 2 d where x i j is the jth element of ith candidate x i j e is the opposite solution and lb j and ub j are the upper and lower bound of jth element 3 formulation of the two step approach based management models for groundwater remediation in earlier studies the concept of placing pumping wells along the centreline which are usually the high concentration zone of the plume for groundwater remediation is that they will extract the maximum amount of contaminant mass from the aquifer during the pumping period wang and zheng 1997 in other studies pumping well locations and pumping rates are computed simultaneously for optimal groundwater remediation wang and ahlfeld 1994 however in the present study we propose a novel two step approach that computes pumping locations and pumping rates separately for optimal groundwater remediation the reason for segregating the management model into two is that the pumping well locations for optimal groundwater remediation can be directly computed using the underlying physics the physics for the determination of pumping well location optimization for groundwater remediation is that the pumping wells should be placed in the high concentration zone usually the centreline of the plume in earlier studies for maximum extraction of contaminant mass from the aquifer the first step of the two step approach of the present study is an attempt to preselect optimal pumping locations in the high concentration zone for groundwater remediation using a mathematical model instead of manual guessing once the optimal pumping locations are obtained using the first step the optimal pumping rates can be computed using the second step of the two step approach the mathematical formulations of the two step approach are given in the following subsections in this study eq 1 and eq 3 are simulated using the finite difference method based modflow and mt3dms codes to obtain the hydraulic head and contaminant concentration distribution in the aquifer domain the amount of contaminant mass in the aquifer domain when there is no pumping can be computed using the following equation 22 m 0 i 1 n g η δ x i δ y i ϕ i 0 c i 0 where m 0 is the total amount of contaminant mass in the aquifer prior to start pumping n g is the number of grids used to represent the aquifer domain η is the porosity of the subsurface ϕ i0 is the hydraulic head at grid i l c i0 is the contaminant concentration at grid i ml 3 and δx i δy i is the area of grid i l2 the contaminant mass in the aquifer decreases with pumping the amount of contaminant mass remained m r in the aquifer after the withdrawal of water with a pumping strategy can be expressed as 23 m r i 1 n g η δ x i δ y i ϕ i c i hence the amount of contaminant mass extracted m e with the pumping strategy can be expressed as 24 m e m 0 m r where ϕ i and c i are the hydraulic head and contaminant concentration after water is withdrawn with a pumping strategy the objective of the first management model is to find the optimal pumping well locations from a finite set of candidate pumping well locations by minimizing the percentage of contaminant mass remained in the aquifer after water is withdrawn with a constant pumping strategy mathematically the management model can be presented as 25a min of 1 m r m 0 100 25b r i 1 2 n c 25c w r r 1 r 2 r i r d 25d r 1 r 2 r i r d 25e ϕ i f q r 1 1 q r 2 2 q r i i q r d d 25f c i f q r 1 1 q r 2 2 q r i i q r d d 25g q 1 q 2 q i q n k q where r i is a random integer generated within the range 1 n c n c is the total number of grids considered for candidate pumping well location w r is the vector of all pumping wells with randomly generated locations d is the total number of active pumping wells qr i i denotes that a pumping rate q i is assigned in the r i th grid and k q is a constant pumping rate the management model falls in the category of the combinatorial optimization problem where the decision variables pumping locations should always be integer in the pat based groundwater remediation system the cost components are well installation cost pumping cost and treatment cost the well installation cost is a fixed cost and hence it can be ignored in the formulation of the management model both the pumping and treatment cost are directly proportional to the water extraction rates from the pumping wells majumder and eldho 2020 hence a management model that minimizes the water extraction rate can indirectly minimize the pumping cost and treatment cost here the objective of the second management model is to minimize a composite function while keeping hydraulic head and contaminant concentrations below a specified permissible limit the first term of the composite function is used to minimize the total amount of pumping rates and the second term is used to minimize the total amount of extracted contaminant mass from the aquifer yang et al 2013 also proposed similar objective function for groundwater remediation the management model for pat based groundwater remediation system can be mathematically expressed as 26a minimize of 2 f c 1 t 1 t i 1 d q i t 100 m e m 0 f c 2 26b q i min q i t q i max 26c d i t d max i 1 2 3 d 26d c i t c max i 1 2 3 d where d is the total number of active pumping wells t is the number of stress periods q i t is the pumping rate from ith pumping well in the tth stress period q i minand q i max are the minimum and maximum pumping rates allowed from ith pumping well d i t is the drawdown in the ith pumping well location c i t is the contaminant concentration in the ith pumping well location d max is the permissible drawdown c max is the permissible contaminant concentration and f c1 1 and f c2 20 are multiplying factor which is used to give the same weightage to both terms of the composite function however f c1 and f c2 also can be assumed as cost coefficients to represent the objective function in the form of operational cost cost for water pumping and water treatment for groundwater remediation assuming the multiplying factor f c1 α3 δt and f c 2 m 0 100 α 4 the eq 26a can be reformulated as 27a minimize of 2 α 3 δ t t 1 t i 1 d q i t α 4 m e where α3and α4 is the cost coefficient associated with water extraction injection or water treatment the eq 27a is mathematically similar to the objective function provided in a previous research yang et al 2013 4 model development initially the groundwater flow and contaminant transport processes are simulated using finite difference based modflow 2005 and mt3dms models the so models are used to obtain optimal pumping locations and optimal pumping rates for groundwater remediation 4 1 determination of the optimal location of pumping wells the so models for the determination of optimal pumping locations are developed by coupling simulation model directly with the evolutionary hunting strategy based metaheuristics ehsms the steps for the determination of optimal pumping well locations are enumerated below 1 using the well package of modflow generate n c 231 number of pumping wells the pumping wells are numbered as 1 2 3 4 n c 2 assign zero pumping rates to each pumping well 3 generate a d 15 dimensional vector w r containing unique random integers within the range 1 231 the vector w r represents the set of active pumping wells 4 to each active pumping well assign a constant pumping rate of value k q 5 compute the fitness function value of 1 using eq 25 6 iteratively repeat steps 2 5 using ehsms to obtain optimal fitness function value of 1 and the corresponding vector w r of optimal pumping wells the optimal pumping locations obtained here will be used directly for the determination of optimal pumping rates 4 2 determination of optimal pumping rates here elm based proxy simulator is coupled with ehsms to obtain optimal pumping rates the optimal pumping well location w v obtained in the previous section is used directly to obtain optimal pumping rates the steps are enumerated below 1 in the numerical model set the pumping location to vector w v 2 using the numerical model generate input and output datasets to train the elm model the input dataset are the randomly generated pumping rates within specified range and the output dataset are the respective hydraulic drawdown contaminant concentration and the percentage of contaminant mass extracted from the aquifer 3 develop a proxy simulator by training the elm model using the input and output dataset check the accuracy of the proxy simulator with respect to the numerical model 4 develop so model by coupling proxy simulator with ehsms 5 obtain the optimal fitness value of 2 by iteratively executing the so model while keeping hydraulic head and contaminant concentrations below the specified permissible limit also compute the corresponding pumping rates and the percentage of extracted contaminant mass eq 26 5 numerical experiments we here consider six composite functions to check the performances of ehsms and other metaheuristics composite benchmark functions have a massive number of local optima thus very difficult to obtain global optima using metaheuristics the search space of composite functions f 10 f 15 resembles many real world optimization problems table a2 fig 4 hence they are ideal for checking local minima escape as well as exploitation and exploration ability the parameter settings of the metaheuristics are given in table a3 supplementary material appendix a in table 1 it is observed that the results obtained by ehsms are consistently better than other optimization algorithms the findings suggest superior local minima escape capability as well as better exploration and exploitation ability of ehsms over the other metaheuristics 6 results and discussion we here consider two case studies to check the performances of the extreme learning machine elm and evolutionary hunting strategy based metaheuristics ehsms for groundwater remediation the first case study is a simple homogeneous unconfined aquifer which is mainly used to validate ehsms for groundwater remediation studies the second case study is a more complex one which is used to check the efficacy of the two step approach and other proposed methodologies 6 1 case study 1 here we consider a hypothetical homogeneous unconfined aquifer taken from majumder and eldho 2020 the boundary condition of the aquifer is shown in fig 5a other aquifer parameters are area a 2000 m 2000 m aquifer thickness h b 60 m hydraulic conductivity k 7 5 m day porosity of the medium η 0 3 areal recharge n r 0 0002 m day longitudinal dispersivity α l 60 m transverse dispersivity α t 15 m initial tds total dissolved solids concentration 500 mg l it is further assumed that the aquifer is polluted for ten years by an instantaneous injection of 20 000 kg contaminant over a source of area 100 m 100 m fig 5a to remediate the aquifer in the next five years a total of five pumping wells are installed by manually studying the behavior of contaminant plume the aquifer domain is discretized using 40 000 uniformly distributed grids the size of each grid is 10 m 10 m finite difference based modflow 2005 and mt3dms models are used to simulate groundwater flow and contaminant transport processes due to the absence of heterogeneity in the aquifer parameters and very few numbers of decision variables pumping wells the aquifer problem considered here is relatively simple and computationally inexpensive hence the numerical model is directly coupled with ehsms for groundwater remediation here the remediation objective is to minimize the total amount of pumping rates first term of the eq 26a the constraints are maximum permissible drawdown 6 m and maximum permissible contaminant concentration is 550 mg l the decision variables pumping rates should be within the range 1500 0 m3 day the optimal total pumping rates obtained by the ehsms and other metaheuristics are shown in table 2 further pumping rates and constraints hydraulic drawdown and contaminant concentration at the five pumping wells obtained using ehsms are also shown in the table 3 from table 2 it is observed that all the metaheuristics obtained almost similar level fitness values the contour of contaminants after the end of the remediation period is shown in fig 5b the results suggest the validity of ehsms for groundwater remediation studies 6 2 case study 2 we here consider a piecewise heterogeneous unconfined aquifer with irregular boundary conditions as shown in fig 6a the boundary conditions of the aquifer are similar to the east texas study aquaveo 2018 however hydraulic conductivity distribution and most of the other aquifer parameters are significantly different from the east texas study area the changes to the east texas case study are aimed to make the aquifer piecewise heterogeneous and achieve a similar level of complexity as seen in real world aquifers the aquifer boundary conditions and variations of piecewise hydraulic conductivity fields are shown in fig 6a other aquifer parameters porosity η 0 3 recharge n r 0 00006 m d top elevation e t 230 m bottom elevation e b 180 m aquifer thickness h b 50 m specific yield s y 0 2 longitudinal dispersivity α l 50 m transverse dispersivity α t 10 m number of stress period t 15 length of each stress period t 365 days and number of time steps in each stress period δt 10 also in fig 6a an artificial recharge zone of area 720 231 76 m2 is shown during the remediation process extracted water from the pumping wells will be uniformly discharged over this entire area of the recharge zone further it is assumed that the aquifer is contaminated by five contaminant sources for a period of five years fig 6a the injection rate and contaminant concentration of the injected water in each contaminant source are 75 m3 day and 75 000 μg l ppb respectively the groundwater flow and contaminant transport processes of the aquifer are simulated using finite difference based modflow 2005 and mt3dms models in the finite difference model the aquifer domain is uniformly discretized using 6400 grids the size of each grid is 53 54 m 29 95 m the contaminant plume after five years of active contamination is shown in fig 6b due to heterogeneity in hydraulic conductivity the initial contaminant plume is not uniform and also it is widely spread over the aquifer domain the aim here is to remediate the aquifer in the next ten years using fifteen pumping wells to achieve these objectives the study aims novel two step approach for groundwater remediation pumping well location optimization followed by pumping rate optimization 6 2 1 pumping well location optimization a properly placed network of pumping wells can extract more contaminant mass from the aquifer than an arbitrarily placed network of pumping wells here a total of 231 grids are selected as the possible location of pumping wells fig 6b the aim is to choose the 15 best pumping wells from the 231 pumping wells by minimizing the total amount of contaminant mass in the aquifer with a fixed pumping rate from each of the pumping wells eq 25 the possible combinations of pumping wells are massive c 231 15 231 15 231 15 due to the massive number of pumping wells combination and highly nonlinear relationship between pumping well location and amount of contaminant mass in the aquifer it is extremely difficult to approximate numerical groundwater flow and transport simulation models with a proxy simulator using finite number of input and output dataset hence to find optimal pumping well locations the numerical model is coupled directly with ehsms further in pumping well location optimization the decision variables locations of pumping wells should always be an integer hence this is a combinatorial optimization problem here we consider two scenarios in the first scenario all the 231 pumping wells are considered as candidate pumping well locations the pumping rate from each well is assumed to be 150 m3 d a constant value the best pumping well locations obtained by ehsms and other metaheuristics are shown in fig 7 from the figure it is observed that the pumping wells are aligned close to the centreline of the plume also the pumping wells are clustered in the zone of high contaminant concentration in table 4 a comparison of ehsms with other metaheuristics are shown in terms of the percentage of contaminant mass remained in the aquifer from the table it is observed that the performances of ehsms are better than other metaheuristics among the ehsms the performance of ehho is the best followed by egwo and ewo it can be seen in fig 7 that many of the pumping wells are very close to each other which may results in higher drawdown in the vicinity of pumping wells the spread of the pumping wells can be controlled by constraining some of the possible pumping well locations in the optimization model in the second scenario all the even pumping well locations are considered as the possible location of pumping wells disregarding the odd pumping well locations the optimal pumping locations obtained using the ehsms and other metaheuristics while constraining the even pumping well locations are shown in fig 8 from fig 8 it is observed that the pumping wells are widely distributed in the contaminated zone than those in fig 7 however the wider distribution of pumping wells came at the expense of an increase in the percentage in contaminant mass remained in the aquifer the constraining strategy may be more helpful if the number of grids used to represent pumping wells is huge and the size of each grid is small in such cases all the grids cannot be considered as candidate pumping wells since there should be some distance between two different pumping wells furthermore it may be possible that the area of the grid may be less than the area of pumping well here we could not perform any statistical analysis due to the expensive nature of the simulation model to compensate for this a detailed statistical analysis is carried out in the next section for pumping rate optimization 6 2 2 pumping rate optimization the pumping well location obtained using combinatorial optimization is directly used for pumping rate optimization to enhance computational efficacy we here aim to approximate the numerical simulation model with elm based proxy simulator using the numerical model a total of 15 000 input output datasets are generated the input datasets are the pumping rates within the range 300 0 m3 day the output dataset is the corresponding aquifer responses hydraulic drawdown contaminant concentration and the total amount of contaminant extracted from the aquifer out of 15 000 datasets 14 000 are used to train the elm model and the remaining 1000 datasets are used to test the efficacy of the elm model the number of hidden neurons is assumed as 1000 the regularization parameter c r can be computed using the cross validation approach deng et al 2009 scardapane et al 2015 vuković et al 2018 the regularization parameter c r are searched in the interval 2 j j 20 19 19 20 for the minimum rmse value of the testing dataset once the regularization parameter c r is obtained in terms of 2 j we further refined c r by searching around it for minimum rmse value the best approximation of hydraulic drawdown contaminant concentration and the total amount of extracted contaminant mass are obtained using the elm model when the values of the regularization parameter c r are 0 25 0 1 and 10 7 respectively the performances of the elm algorithm in terms of root mean square error rmse and coefficient of correlation r are shown in fig 9 the result shows that for both hydraulic drawdown and contaminant mass estimation the coefficient of correlation r is 0 999 whereas for contaminant concentration estimation the coefficient of correlation r is 0 989 furthermore the rmse value is very close to zero for both hydraulic head and contaminant mass estimation whereas the rmse value is 4 97 for the contaminant concentration estimate the results suggest an excellent generalization ability of the elm model in approximating the numerical model such high accuracy of the elm model is desired when approximating a physics based numerical model with no uncertainty in its parameters the elm based proxy simulator is further coupled with ehsms to develop so models a composite function is used to minimize the sum of the water withdrawal rate and the percentage of contaminant mass extracted eq 26 the value of the multiplying factor f c1 and f c2 are assumed as 1 and 20 respectively the multiplying factor f c 2 is used to give similar weightage to both terms of eq 26 the maximum permissible hydraulic head and contaminant concentrations are specified as 2 5 m and 500 μg l respectively all the metaheuristics are executed twenty five times and the results in terms of statistical parameters best average standard deviation sd and rank are tabulated table 5 also in table 5 the fitness value of 2 is segregated into optimal pumping rates and the percentage of extracted contaminant mass the results suggest that the performances of ehsms are better than hsms and other metaheuristics among the ehsms the performance of egwo is slightly better than ewo and ehho the table 6 also lists individual pumping rates constraints hydraulic drawdown and contaminant concentration at the pumping well locations corresponding to the best solutions obtained by ehsms the results of all the metaheuristics are further compared using the violin plot fig 10 a violin plot is a hybrid of the box plot and probability distribution plot of numeric data which is usually smoothened using the kernel density estimator kde violin plot depicts many statistical parameters such as whiskers to indicate upper extreme and lower extreme quartiles a box to represent interquartile ranges a marker to represent the median of the data and probability density function pdf to depict the distribution of numeric data the violin shape pdf also helps to spot whether data are sparse or multimodal in fig 10 it is observed that the difference between upper and lower quartiles is least for ehsms it means that data are spreads over a smaller region for ehsms compared to other metaheuristics further the markers in the violin plot state that the median value of ehsms is better than the rest of metaheuristics moreover the probability density function indicates that ehsms are more likely to achieve better optimal values than their counterpart we further performed the one way analysis of variance anova and bonferroni test to check the difference between the optimal solutions obtained by ehsms and other metaheuristics the p value obtained using anova test is 0 which suggests that there are significant differences between the optimal solutions obtained by different methods a pictorial representation of the bonferroni test is shown in fig 11 in the figure the lines represent the data interval interval of optimal solutions obtained by the metaheuristics a symbol in the middle of the line represents the mean value of the data data of any two groups are significantly different if their interval line is disjoint from the figure we can state that the results obtained using ehsms are significantly different better than those obtained using other metaheuristics the complete details of the bonferroni test are shown in table a5 supplementary material appendix a the efficacy of the two step approach is further compared to an alternate approach for groundwater remediation in the alternate approach the pumping well locations and pumping rates are explicitly incorporated together in the same management model hence the dimension of the search space in the alternate approach is more than the two step approach the problem considered here is a mixed integer combinatorial programming problem since one decision variable location of pumping wells should always be an integer also due to the high level complexity between input variables pumping well locations and pumping rates and output variables drawdown contaminant concentration and percentage contaminant mass extracted the simulation model cannot be approximated easily using the elm based proxy simulator hence to develop the so model the simulation model is directly coupled with the metaheuristics the fitness function is in the form of eq 26 in fig 12d the optimal pumping location obtained using one of the ehsms ehho with initial contaminant concentration is shown it is observed that the optimal well locations obtained using the alternate approach have almost similar configurations to those obtained using the two step approach fig 12d and fig 7 it is observed that the fitness value of 2 total optimal pumping rates and the percentage of extracted contaminant mass obtained using the alternate approach are found to be 3648 01 1698 81m3 day and 97 46 respectively table 7 the solutions found here seem to be slightly inferior with respect to solutions obtained using the two step approach table 5 further in fig 13 the optimal pumping locations obtained using the alternate approach is compared with the two step approach with respect to contaminant concentration at the end of the remediation period 6 2 3 convergence analysis the fitness value of 1 obtained in the first step of the two step approach is plotted with respect to the number of generations to illustrate how the ehsms are converging to the solution fig 12a the figure shows similar convergence behavior of ehho ewo and egwo in obtaining the optimal solutions the average number of generations required by any of the ehsms to obtain the optimal solution is approximately 350 all the ehsms took approximately 40 50 h to obtain the optimal solutions in the first step the fitness value of 2 obtained in the second step of the two step approach is plotted with respect to the number of generations in fig 12b here the average number of generations required by any of the ehsms to obtain the optimal solution is close to 150 the second step is computationally very efficient due to using the elm based surrogate model in approximating numerical simulation models the computational time to obtain optimal solutions using ehms in the second step is approximately 5 min further in fig 12c the convergence behavior of the ehho in obtaining the optimal solution of 2 using the alternate approach is shown fig 12c depicts that a very high number of generations are required in the alternate approach to achieve a similar level of fitness value of 2 compared to the two step approach fig 12b due to the higher number of generations the alternate approach is computationally expensive the computational time in obtaining optimal solutions using the alternate approach is around 140 h 7 discussion the present study considers a deterministic definition of aquifer parameters hydraulic conductivity recharge however in the real field aquifer system the parameters are spatially highly heterogeneous and there are always uncertainties in estimating the aquifer parameters from a limited set of observed values hence the parameter uncertainties should always be considered for the effective design of groundwater remediation systems in the real field aquifer the two commonly used methods to design groundwater remediation strategies under parameter uncertainties are chance constrained optimization and robust optimization aly and peralta 1999 the two step approach of the present study can be combined with chance constrained optimization or robust optimization for designing groundwater remediations strategy under parameter uncertainty it should be noted here that the chance constrained optimization or robust optimization methods should be used in both steps of the two step approach in this case the first step will yield the optimal pumping locations for effective groundwater remediation under parameter uncertainty and the second step will also yield the optimal value of the composite objective function under parameter uncertainty the two step approach of the present study considers only one operation period static pumping policy for groundwater remediation however multiple operation periods the dynamic pumping policy should be considered for designing a more efficient remediation strategy for real field aquifers huang and mayer 1997 the two step approach of the present study can be used for groundwater remediation under a dynamic pumping policy with some modifications in the modified two step approach for each operation period the optimal pumping well locations have to be updated using eq 25 according to the configuration of the contaminant plume remaining amount of contaminant mass in the aquifer of the respective operation period 8 conclusion the present study proposes so models for groundwater remediation by coupling evolutionary hunting strategy based metaheuristics ehsms with extreme learning machine elm the ehsms are proposed by modifying hunting strategy based metaheuristics hsms with biological evolution operator elimination mechanism based on the survival of fittest principle in nature and elite opposition based learning strategy the performances of ehsms to obtain the optimal solutions of unimodal multimodal and composite benchmark functions are found to be better than hsms and other optimization algorithms in terms of exploitation and exploration capability and ability to avoid entrapment in local minima further ridge regression based regularization parameter is used to enhance the generalization ability of extreme learning machine the elm model shows excellent generalization ability as well as computational efficiency in approximating the numerical simulation model the most novel contribution of this research is the two step approach for groundwater remediation in this approach the optimal pumping locations obtained in the first management model acts as input in the second to compute optimal pumping rates this approach thus helps to negate the curse of dimensionality associated error which is always a big issue in the field of high dimensional optimization moreover the simulation model of the second step can be approximated using an accurate surrogate simulator which can enhance the computational performance conversely in the alternate approach optimal pumping locations and optimal pumping rates are computed by explicitly incorporating them together in the same management model the alternate approach is computationally expensive than the two step approach due to the higher dimensional search space further the results show that the two step approach is slightly better than the alternate approach in terms of accuracy in this study we considered two aquifer problems to check the efficacy of the two step approach elm based proxy simulator and ehsms for groundwater remediation the first case study is a homogeneous unconfined aquifer which is mainly used to validate ehsms for groundwater remediation the second case study is relatively complex with piecewise heterogeneous hydraulic conductivity fields and irregular boundary conditions in the second case study detailed statistical analysis is carried out to compare ehsms and other metaheuristics pso cso de hho wo and gwo the analysis shows that ehsms are more accurate than other metaheuristics among the ehsms the performance of ehho is found to be slightly better than ewo and egwo in the future we wish to explore the proposed methodologies for groundwater remediation in a highly heterogeneous medium under hydrological uncertainty and dynamic pumping policy software and data availability in this study matlab 2016b is used to carry out all the coding the computer configuration is intel i5 9400f cpu 2 90 ghz and 16 gb ram please use the link to access the matlab codes and other dependencies https github com parthamajumder1985 groundwater remediation toolbox declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments c lu acknowledges the financial support from the national natural science foundation of china 51879088 fundamental research funds for the central universities b200204002 and the natural science foundation of jiangsu province bk 20190023 appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103864 
1356,we propose a simulation optimization so model based on a novel two step strategy for the optimal design of groundwater remediation systems the so models are developed by coupling simulation models directly or through the extreme learning machine elm with evolutionary hunting strategy based metaheuristics ehsms in the first step ehsms with a combinatorial optimization technique are used to obtain optimal pumping locations by minimizing the percentage of contaminant mass that remained in the aquifer while keeping the pumping strategy as constant in the second step the optimal pumping locations are directly used as input and a composite function is employed to minimize the sum of the water extraction rates and the percentage of extracted contaminant mass by constraining hydraulic heads and contaminant concentrations the performance of the two step strategy is found to be slightly better and computationally more efficient than the alternate approach moreover various statistical measures suggest the superiority of ehsms over other metaheuristics for groundwater remediation keywords extreme learning machine elm grey wolf optimization gwo whale optimization wo harris hawks optimization hho groundwater remediation notations a area of the aquifer l2 a t a coefficient which decreases linearly with respect to the iteration number t within the range 2 0 b c a constant to define the logarithmic spiral shape b i thresholds of hidden neuron i c contaminant concentration ml 3 c s k contaminant concentration of dissolved species k ml 3 c r crossover operator c r regularization term to resolve overfitting issue c uniformly distributed random variable within the range 0 2 d r uniformly distributed random integer variable within the range 1 d d dimension of search space d h hydrodynamic dispersion coefficient tensor l2 t 1 e escaping energy of the rabbit f activation function f x fitness function evaluated at a point x f c multiplying factor j jump strength of a rabbit h hidden layer output matrix h t transpose matrix of h h moore penrose generalized inverse of matrix h h b aquifer thickness l i identity matrix lb lower bound k hydraulic conductivity tensor lt 1 k q constant pumping rate l uniformly distributed random number within the range 1 1 m mutation scaling factor m 0 amount of contaminant mass in the aquifer prior to pumping m m r amount of contaminant mass remained in the aquifer after pumping m m e amount of extracted mass with a pumping strategy m n number of population n d number of datasets n r randomly generated integer n g number of grids used to represent the aquifer domain p uniformly distributed random numbers within the range 0 1 q s volumetric flow rate per unit volume representing sinks and or sources of water t 1 r escaping probability of a rabbit r n chemical reaction term ml 3 t 1 r d coefficient of retardation s s specific storage of the porous media l 1 s y specific yield s a vector having a dimension of 1 d s i u 0 1 t maximum iteration number t time t u seepage velocity lt 1 ub upper bound w i weight vector which connects the input neurons to the hidden neuron i w r vector of all pumping wells with randomly generated locations x j input data y j output data x r t position of randomly selected whale x i t position of candidate whale x i t 1 new position of candidate whale x g t position of the best whale x α t x β t x δ t position of alpha beta and delta wolves x 1 x 2 x 3 possible position of the candidate wolf which is computed based on the position of alpha beta and delta respectively x r1 t x r2 t x r3 t three distinct randomly selected candidate from the population x e opposite of the candidate solution x i δx i δy i area of grid i l2 y i t 1 muted solution z ij t 1 offspring solution α l longitudinal dispersivity l α t transverse dispersivity l β i output weight which connects the hidden neuron i to output neurons β c a constant parameter having a value of 2 5 del operator ϕ hydraulic head l η porosity of the subsurface medium ε scaling factor of candidates updating 1 introduction groundwater remediation is a process to treat contaminated groundwater by removing the contaminants or converting them into harmless compounds mckinney and lin 1996 the remediation of the contaminated groundwater is economically costly and one of the major technological and environmental challenges in water resources engineering piscopo et al 2015 a recent estimate states that the cost to remediate all the contaminated sites in the united states usa will be around 110 billion to 127 billion national research council 2013 groundwater remediation is also a very time consuming process and it may take many decades of continuous remediation practice to decontaminate an aquifer sadeghfam et al 2019 hence the selection of proper remediation techniques and the design of optimal remediation strategies are of great importance for decision makers to save cost and time piscopo et al 2015 one of the most widely used groundwater remediation techniques is the pump and treat pat method matott et al 2006 in the pat the groundwater is hydraulically contained and extracted to the ground surface by pumping and then the contaminated water is treated using a filtering or stripping system majumder and eldho 2020 mategaonkar and eldho 2012 the treated water is injected into the aquifer for groundwater recharge or diverted for industrial irrigation or domestic uses majumder and eldho 2020 the effective and economical design of the pat remediation system depends on various factors such as the number and location of pumping wells pumping rates injection locations injection rates and treatment processes of contaminated water sadeghfam et al 2019 seyedpour 2019 the goal of the pat remediation system is to compute optimal values of the factors mentioned above which minimize the remediation cost or amount of contaminant mass remaining in the aquifer after a specified time yang et al 2013 in earlier studies the locations of the pumping wells were predefined along the centreline of the contaminant plume if the aquifer is homogeneous and the plume is uniform seyedpour 2019 wang and zheng 1997 however for a wider uniform plume or non uniform plume it is not possible to accurately predict the pumping well location manually huang and mayer 1997 in a previous groundwater remediation study the pumping well locations are incorporated in the management model as a continuous function of the spatial domain to find optimal pumping locations and optimal values of other decision variables simultaneously wang and ahlfeld 1994 this approach employs interpolation functions to convert discrete pumping well locations of the finite difference model fdm as a continuous function of the spatial domain wang and ahlfeld 1994 nevertheless such interpolation steps may add a substantial amount of mathematical inaccuracies to the management model in another study the optimal well locations in the heterogeneous aquifer are obtained using the combinatorial optimization method rizzo and dougherty 1996 however they have not used the results for computing optimal pumping rates in the so approach the optimization model executes the simulation model repetitively to obtain the optimal strategy majumder and eldho 2020 mckinney and lin 1996 yang et al 2013 the computational speed of the finite difference based simulation model can be enhanced significantly by approximating it using a surrogate simulator yan and minsker 2006 previous studies used various machine learning algorithms such as response matrix approach rma artificial neural network ann support vector machine svm and extreme learning machine elm to approximate computationally expensive groundwater flow and contaminant transport simulators asher et al 2015 zhao et al 2020 among them the elm algorithm with arbitrarily assigned input weights and hidden layer thresholds can accurately map the nonlinear relationship of any compact input output dataset if the activation function is infinitely differentiable huang et al 2006 the elm is computationally very efficient and more accurate than single layer feed forward neural networks and support vector machines the basic elm shows variances in the generalization ability due to randomly generated input weights and hidden layer thresholds wang et al 2016 however the generalization ability of elm can be significantly enhanced using ridge regression with a regularization parameter shao and er 2016 zhang et al 2020 the most popular metaheuristics for groundwater management and remediation are genetic algorithm ga differential evolution de genetic programming gp simulated annealing sa tabu search ts harmony search hs particle swarm optimization pso and cat swarm optimization cso luo et al 2014 majumder and eldho 2016 mategaonkar and eldho 2012 mckinney and lin 1996 yang et al 2013 zhao et al 2020 recently hunting strategy based metaheuristics hsms such as grey wolf optimization gwo whale optimization wo and harris hawks optimization hho are getting unprecedented attention in various engineering fields heidari et al 2019 mirjalili et al 2014 mirjalili and lewis 2016 the application of the hsms to obtain the optimal solution of unimodal multimodal and composite benchmark functions reveals superior performances of hsms over other traditional metaheuristics among the hsms the gwo is already used to compute optimal pumping rates for groundwater remediation under steady state conditions majumder and eldho 2020 the performances of hsms can be further enhanced by adopting various measures such as hybridizing different metaheuristics adding levy flight incorporating elite opposition based learning approximating the random processes of metaheuristics by chaotic map incorporating biological evolution operators and adding elimination mechanism chen et al 2020 sihwail et al 2020 wang and li 2019 in this study simulation models are developed by solving the groundwater flow and contaminant transport equations using finite difference based modflow 2005 and mt3dms models harbaugh and arlen 2005 zheng and wang 1999 an extreme learning machine elm based proxy simulator is used to approximate the numerical simulation model the generalization ability of the elm model is enhanced using ridge regression with a regularization parameter further evolutionary hunting strategy based metaheuristics ehsms are proposed by modifying the grey wolf whale and harris hawks optimization algorithms by incorporating biological evolution operators elimination mechanisms based on survival of fittest and elite opposition based learning the performances of the ehsms are tested against well known unimodal multimodal and composite benchmark functions further various simulation optimization so models are developed by coupling the simulation models proxy simulators with ehsms based on a novel two step approach the so models are used to obtain optimal pumping well locations and optimal pumping rates for groundwater remediation in the homogeneous and piecewise heterogeneous aquifer systems 2 methodology the partial differential equation pde depicting groundwater flow processes through the subsurface can be written as follows freezy and cherry 1979 1 k ϕ q s s s ϕ t where k is the hydraulic conductivity tensor lt 1 ϕ is the hydraulic head l q s is the volumetric flux rate per unit volume representing sinks or sources of water t 1 s s is the specific storage of the porous media l 1 t is the time t is the del operator i x j y k z and i j k are the unit vectors along x y and z axis the seepage velocity u through the subsurface can be computed using darcy s equation freezy and cherry 1979 2 u k η ϕ x the pde describing contaminant fate and transport through the subsurface can be expressed as batu 2005 3 r d η c t η d h c η u c q s c s k r n where η is the porosity of the subsurface medium u is the seepage velocity lt 1 d h is the hydrodynamic dispersion coefficient tensor l2t 1 c is the contaminant concentration ml 3 c s k is the contaminant concentration of dissolved species k ml 3 r d is the coefficient of retardation and r n is the chemical reaction term ml 3 t 1 2 1 extreme learning machine elm elm is a single hidden layer neural network in which the input weights and hidden layer thresholds are arbitrarily assigned with a continuous activation function elm can approximate the nonlinear relationship of any compact input output dataset huang 2014 liu et al 2020b a pictorial representation of elm architecture is shown in fig 1 let us assume n d distinct dataset x j y j r n r m j 1 2 n d where x j x j1 x j2 x j3 x jn n t r n is the input data and y j y j1 y j2 y j3 y jm m t r m is the output data the nonlinear relationship between input and output data in elm can be mathematically expressed as huang 2014 4 i 1 l β i f w i x j b i y j j 1 2 n d where f is the activation function l is number of neurons in the hidden layer w i w 1i w 2i w 3i w ni n t is the weight vector which connects the input neurons to the hidden neuron i b i is thresholds of hidden neuron i and β i β i1 β i2 β i3 β im l m t is the output weight which connects the hidden neuron i to output neurons eq 4 can be written in matrix form as huang et al 2006 5a h β y 5b h f w 1 x 1 b 1 f w l x 1 b l f w 1 x n d b 1 f w l x n d b l n d l 5c y y 1 y 1 y 3 y n n d m t where h is known as the hidden layer output matrix the output weight β can be computed by minimizing the norm of the loss function which can be expressed as 6 minimize h β y 2 the output weight β can be computed using least square estimate thus can be expressed as 6a β h y where h is the moore penrose generalized inverse of h it should be noted that the inverse of matrix h does not exist if the matrix is singular or rectangular the matrix h is calculated through orthogonal projections as 6b h h t hh t 1 in elm randomly generated input weights and hidden layer thresholds usually lead to variance in the generalization ability the stability and generalization performance of elm can be enhanced significantly using the ridge regression based regularization method shao and er 2016 zhang et al 2020 the ridge regression minimizes the norm of the difference between model output with the actual output and the norm of the output weight the aim of ridge regression is to obtain the smaller value of the output weights to minimize variance in prediction shao and er 2016 7 minimize h β y 2 β minimize h β y 2 1 c r β 2 the solution of the above equation can be expressed as 7a β h t hh t i c r 1 y where i is the identity matrix and c r is the regularization term which is used to resolve overfitting issues deng et al 2014 2 2 harris hawks optimization hho harris hawks are well known for their organized cooperative foraging behavior of hunting rabbits hawks perform a variety of chasing styles based on the nature of situations and escaping patterns of rabbits two distinctive features of hawks in hunting rabbits are the surprise pounce and switching strategy the surprise pounce is a skillful maneuvering strategy in which a group of hawks cooperatively attack a rabbit from all directions in close combat between hawks and a rabbit the leader hawk stoops towards the rabbit and occasionally disappears to confuse it in such cases another hawk of the group takes the leadership to continue the chasing process this strategy of hawks is known as the switching strategy the hho algorithm is proposed by mathematically modeling the aforementioned behavior of harris hawks in capturing rabbits heidari et al 2019 2020 in hho a parameter namely escaping energy of rabbit e is used to demarcate the exploration and exploitation mechanism the parameter e can be mathematically represented as heidari et al 2019 8a e 2 r 1 a t 8b a t 2 1 t t t 1 2 3 4 t where the coefficient a t decreases linearly with respect to the iteration number t within the range 2 0 t is the maximum iteration number r is a uniformly distributed random number within the range 0 1 the parameter e decreases adaptive and stochastic manner with respect to the iteration number t within the range 2 2 if e 1 hawks explore all the possible locations to detect the rabbit if e 1 hawks strategically try to capture the rabbit by exploiting its possible movements in the proximity the various phases of hho are presented in the following subsection with mathematical details fig 2a may help to envisage various phases of hho easily 2 2 1 exploration e 1 in the exploration phase hawks wait in random locations and observe the surroundings with the hope of discovering a rabbit once a rabbit is detected hawks try to encircle it by changing its position based on the position of other hawks and the position of the rabbit a uniformly distributed random number q within the range 0 1 is used to determine the position update mechanism of hawks if q 0 5 hawk updates position with respect to the position of other neighbor hawks in contrast if q 0 5 hawk updates position with respect to the position of rabbit heidari et al 2019 9a x t 1 x rand t r 1 x rand t 2 r 2 x t q 0 5 x rab t x m t r 3 lb r 4 ub lb q 0 5 9b x m t 1 n i 1 n x i t where x t is the position of a candidate hawk x t 1 is the new position of the candidate hawk x rand t is the position of a randomly selected hawk x rab t is the position of the rabbit x m t is the mean position of all the hawks involved in hunting n is the total number of hawks lband ub are the lower bound and upper bound of position x and r 1 r 2 r 3 r 4 and q are uniformly distributed random numbers within the range 0 1 2 2 2 exploitation e 1 in the exploitation phase hawks encircle approach and perform a surprise pounce to hunt the rabbit rabbit often moves abruptly to escape from threatening situations four possible strategies are proposed to emulate the escaping behaviors of rabbits and the chasing style of hawks to state the probability of escaping a rabbit a uniformly distributed random variable namely escaping probability r within the range 0 1 is introduced heidari et al 2019 2 2 3 soft besiege r 0 5 e 0 5 the energy of the rabbit is still intact and it performs misleading jumps to escape from risky situations the hawks encircle the rabbit softly to make it fatigued heidari et al 2019 10a x t 1 x rab t x t e j x rab t x t 10b j 2 1 r 5 where j is the jump strength of the rabbit which is used to imitate the random misleading motion of rabbit and r 5 is the uniformly distributed random variable within the range 0 1 2 2 4 soft besiege with progressive rapid dives r 0 5 e 0 5 in a close encounter the rabbit shows a random deceptive motion in response to the abrupt rapid dives of hawks such leapfrog behavior of the hawk and rabbit can be approximated mathematically by levy flight in an attempt to capture the rabbit the hawks either make a regular dive or a complicated dive levy flight is used to approximate the complicated dive heidari et al 2019 the two strategies regular and complicated can be mathematically modeled using the following equations 11a x 1 t 1 x rab t e jx rab t x t 11b x 2 t 1 x rab t e jx rab t x t s lf d 11c lf x 0 01 u σ v 1 β 11d 11e x t 1 x 1 t 1 if f x 1 t 1 f x t 11f x t 1 x 2 t 1 if f x 2 t 1 f x t where lf is the levy flight function uand vare uniformly distributed random variables within the range 0 1 β c is a constant parameter having a value of 2 5 d is the dimension of the search space sis a vector having a dimension of 1 d elements of the vector s are uniformly distributed random variables within the range 0 1 and f x is the fitness function evaluated at point x 2 2 5 hard besiege r 0 5 e 0 5 in this phase the escaping energy of the rabbit is low due to extreme exhaustion hence the rabbit can not make the abrupt misleading jump anymore to deceive the hawks the hawks quickly perform surprise pounce to capture the rabbit heidari et al 2019 12 x t 1 x rab t e x rab t x t 2 2 6 hard besiege with progressive rapid dives r 0 5 e 0 5 in this phase the escaping energy of the rabbit is low and hawks try to catch the rabbit by making regular or complicated dive heidari et al 2019 13a x 3 t 1 x rab t e jx rab t x m t 13b x 4 t 1 x rab t e jx rab t x m t s lf d 13c x t 1 x 1 t 1 if f x 3 t 1 f x t 13d x t 1 x 2 t 1 if f x 2 t 1 f x t 2 3 whale optimization wo whale optimization wo is a metaheuristic algorithm proposed by imitating the cooperative hunting behavior of humpback whales mirjalili and lewis 2016 in an attempt to encircle prey krill small fish the humpback whales frequently dive underwater and then swim upwards through a progressive shrinking circle and spiral shaped path in this process whales produce bubbles in the spiral path which helps to trap prey this unique hunting strategy of the humpback whale is commonly known as bubble net foraging in the wo algorithm the exploitation and exploration strategies are modeled by imitating the bubble net attacking mechanism and random search behavior of whale respectively liu et al 2020a mirjalili and lewis 2016 the parameter e is used to control the exploitation and exploration mechanism of whale optimization parameter e has the same mathematical significance of escaping energy of the rabbit 2 3 1 random search mechanism e 1 in this case each whale randomly searches for prey by monitoring the movement of other whales the position of a candidate whale is updated according to the position of a randomly selected whale mirjalili and lewis 2016 14 x i t 1 x r t e c x r t x i t where x r t is the position of randomly selected whale x i t is the position of candidate whale x i t 1 is the new position of candidate whale and c is a uniformly distributed random variable within the range 0 2 2 3 2 bubble net attacking e 1 in the bubble net attacking strategy the humpback whales encircle the prey by concurrently utilizing a shrinking encircling process and spiral position updating strategy in the shrinking encircling process candidate whales update their positions according to the position of the best whale the parameter e decreases randomly with respect to the iteration number within the range a a to facilitate progressive shrinking of the distance between whales and prey further the helix shaped movement of whales towards prey can be approximated using a spiral equation as shown in fig 2b the probability of happening both the strategy is the same mathematically both the approaches can be approximated as mirjalili and lewis 2016 15a x i t 1 x g t e c x g t x i t if p 0 5 15b x t 1 x g t x i t e b c l cos 2 πl x g t if p 0 5 where x g t is the position of the best whale pare uniformly distributed random numbers within the range 0 1 l is a uniformly distributed random number within the range 1 1 and b c is a constant used to define the logarithmic spiral shape 2 4 grey wolf optimizer gwo gwo is a metaheuristic algorithm developed by mimicking the social leadership hierarchy and hunting strategy of grey wolves luo et al 2019 mirjalili et al 2014 wolves usually live in a pack and hunt prey with collective efforts a pack comprises of four different types of wolves alpha beta delta and omega alpha is the most dominant wolf and responsible for making the decision of hunting beta obeys the command of alpha and also communicates the decision to delta and omega beta is also liable to report the activities of the pack to alpha occasionally beta may assist alpha in decision making delta wolves follow the command of alpha and beta but they dominate the omega wolves based on the nature of activities a delta wolf may be a scout sentinel hunter elder or caretaker the least dominant omega wolves are bound to follow the command of alpha beta or delta however in terms of importance the omegas are not the least significant wolves the absence of enough omega wolves in the pack may cause internal conflict and collapse of the social dominance hierarchy structure majumder and eldho 2020 mirjalili et al 2014 the social hierarchy structure of grey wolves is shown in fig 3 a the pictorial view of the position updating mechanism of wolves to hunt prey is shown in fig 3 c the exploration and exploitation mechanism of the gwo algorithm is controlled by two parameters e and c the parameter e has the same mathematical and physical significance as discussed in the previous section also see fig 3 b additionally the variation of e with respect to the iteration number is shown in fig 3 d the parameter c is a uniformly distributed random number within the range 0 2 the value of c 1 assists the algorithm in exploring the search space conversely the algorithm shows exploitative behavior if the value of c 1 in the search space first second and third best minimum solutions are assigned to alpha beta and delta using the following equations 16a if f x t f x α t then x α t x t f x α t f x t 16b if f x t f x α t f x t f x β t then x β t x t f x β t f x t 16c if f x t f x α t f x t f x β t f x t f x δ t then x δ t x t f x δ t f x t in each iteration the position of alpha beta and delta are updated whenever a candidate wolf identifies a better position the rest of the wolves update their positions according to the positions of alpha beta and delta using the following equation mirjalili et al 2014 17a x 1 x α t e c x α t x t 17b x 2 x β t e c x β t x t 17c x 3 x δ t e c x δ t x t 17d x t 1 x 1 x 2 x 3 3 where x α t is the position of alpha x β t is the position of beta x δ t is the position of delta x t is the position of a candidate wolf x 1 x 2 and x 3 are the possible position of the candidate wolf which is computed based on the position of alpha beta and delta respectively x t 1 is the position of candidate wolf in next iteration and f x is the fitness function evaluated at point x 2 5 biological evolution in nature living beings get fitter stronger over the successive generations through biological evolution and the survival of fittest sof principle it can be assumed that candidates hawks whales wolves also evolve over the successive generations and better candidate survives wang and li 2019 the evolved candidate is selected as a part of the population for next generation if it improves the fitness function value the biological evolution process can be modeled using differential evolution de operators the biological evolution takes place by three operators mutation crossover and selection wang and li 2019 2 5 1 mutation in each generation for each candidate solution xi t a muted solution is generated by adding a randomly selected solution with the weighted difference between another two randomly selected solutions storn and price 1997 18 y i t 1 x r 1 t m x r 2 t x r 3 t i 1 2 n t 1 2 t x r 1 t x r 2 t x r 3 t where x r1 t x r2 t and x r3 t are three distinct randomly selected candidate solution in tth generation from n number of population y i t 1 is the muted solution m mutation scaling factor is a uniformly distributed random number within the range 0 2 0 8 and nis the number of population t is the total number of generation 2 5 2 crossover in the crossover strategy an offspring solution is generated by exchanging the elements of the candidate solution vector with the muted solution vector storn and price 1997 19 z ij t 1 y ij t 1 c r r or j d r x ij t c r r i 1 2 n j 1 2 d where c r is the crossover operator r is the uniformly distributed random variable within the range 0 1 z ij t 1 is the offspring solution d r is a uniformly distributed random integer variable within the range 1 d and d is the dimension of the solution vector 2 5 3 selection the offspring solution is selected for next generation if the fitness value obtained by the offspring solution is better than the fitness value evaluated by the candidate solution storn and price 1997 20 x i t 1 z i t 1 f z i t 1 f x i t x i t f z i t 1 f x i t i 1 2 n where f x i t is the fitness function evaluated at the point x i t 2 6 elimination mechanism based on the survival of the fittest candidate in the process of biological evolution vulnerable candidates of a population will be eliminated due to various reasons such as hunger disease and natural disaster meanwhile new candidates will join the population due to natural birth or other reasons wang and li 2019 the sof mechanism can be implemented in the hsm using the following steps a after each iteration sort the fitness values in the ascending order arrange the position of candidates with respect to the sorted fitness values b generate a random integer n r within the range n 2 ε n ε where n c is the total number of candidates and ε is a scaling factor of candidates updating the value of the scaling factor ε should always be greater than one so that the good candidates are preserved for the next generation c remove n r number of candidates with low fitness values d add n r number of new candidates with randomly generated positions a high value of n r results in better exploration however it will cause slow convergence a very small value of n r is not ideal to maintain enough diversity in the population and thereby reduces the exploration capability of the algorithm 2 7 elite opposition based learning eobl elite opposition based learning eobl is a standard procedure in the field of machine learning to improve solutions soncco álvarez et al 2019 in eobl for a candidate solution an opposite solution is generated subsequently based on the fitness value the better solution is selected as the candidate for the next generation dhargupta et al 2020 to explain the eobl let us assume a candidate solution x i x i 1 x i 2 x i d and the corresponding opposite solution x e x e 1 x e 2 x e d using eobl the opposite solution is computed using the following equation 21 x i j e lb j ub j x i j j 1 2 d where x i j is the jth element of ith candidate x i j e is the opposite solution and lb j and ub j are the upper and lower bound of jth element 3 formulation of the two step approach based management models for groundwater remediation in earlier studies the concept of placing pumping wells along the centreline which are usually the high concentration zone of the plume for groundwater remediation is that they will extract the maximum amount of contaminant mass from the aquifer during the pumping period wang and zheng 1997 in other studies pumping well locations and pumping rates are computed simultaneously for optimal groundwater remediation wang and ahlfeld 1994 however in the present study we propose a novel two step approach that computes pumping locations and pumping rates separately for optimal groundwater remediation the reason for segregating the management model into two is that the pumping well locations for optimal groundwater remediation can be directly computed using the underlying physics the physics for the determination of pumping well location optimization for groundwater remediation is that the pumping wells should be placed in the high concentration zone usually the centreline of the plume in earlier studies for maximum extraction of contaminant mass from the aquifer the first step of the two step approach of the present study is an attempt to preselect optimal pumping locations in the high concentration zone for groundwater remediation using a mathematical model instead of manual guessing once the optimal pumping locations are obtained using the first step the optimal pumping rates can be computed using the second step of the two step approach the mathematical formulations of the two step approach are given in the following subsections in this study eq 1 and eq 3 are simulated using the finite difference method based modflow and mt3dms codes to obtain the hydraulic head and contaminant concentration distribution in the aquifer domain the amount of contaminant mass in the aquifer domain when there is no pumping can be computed using the following equation 22 m 0 i 1 n g η δ x i δ y i ϕ i 0 c i 0 where m 0 is the total amount of contaminant mass in the aquifer prior to start pumping n g is the number of grids used to represent the aquifer domain η is the porosity of the subsurface ϕ i0 is the hydraulic head at grid i l c i0 is the contaminant concentration at grid i ml 3 and δx i δy i is the area of grid i l2 the contaminant mass in the aquifer decreases with pumping the amount of contaminant mass remained m r in the aquifer after the withdrawal of water with a pumping strategy can be expressed as 23 m r i 1 n g η δ x i δ y i ϕ i c i hence the amount of contaminant mass extracted m e with the pumping strategy can be expressed as 24 m e m 0 m r where ϕ i and c i are the hydraulic head and contaminant concentration after water is withdrawn with a pumping strategy the objective of the first management model is to find the optimal pumping well locations from a finite set of candidate pumping well locations by minimizing the percentage of contaminant mass remained in the aquifer after water is withdrawn with a constant pumping strategy mathematically the management model can be presented as 25a min of 1 m r m 0 100 25b r i 1 2 n c 25c w r r 1 r 2 r i r d 25d r 1 r 2 r i r d 25e ϕ i f q r 1 1 q r 2 2 q r i i q r d d 25f c i f q r 1 1 q r 2 2 q r i i q r d d 25g q 1 q 2 q i q n k q where r i is a random integer generated within the range 1 n c n c is the total number of grids considered for candidate pumping well location w r is the vector of all pumping wells with randomly generated locations d is the total number of active pumping wells qr i i denotes that a pumping rate q i is assigned in the r i th grid and k q is a constant pumping rate the management model falls in the category of the combinatorial optimization problem where the decision variables pumping locations should always be integer in the pat based groundwater remediation system the cost components are well installation cost pumping cost and treatment cost the well installation cost is a fixed cost and hence it can be ignored in the formulation of the management model both the pumping and treatment cost are directly proportional to the water extraction rates from the pumping wells majumder and eldho 2020 hence a management model that minimizes the water extraction rate can indirectly minimize the pumping cost and treatment cost here the objective of the second management model is to minimize a composite function while keeping hydraulic head and contaminant concentrations below a specified permissible limit the first term of the composite function is used to minimize the total amount of pumping rates and the second term is used to minimize the total amount of extracted contaminant mass from the aquifer yang et al 2013 also proposed similar objective function for groundwater remediation the management model for pat based groundwater remediation system can be mathematically expressed as 26a minimize of 2 f c 1 t 1 t i 1 d q i t 100 m e m 0 f c 2 26b q i min q i t q i max 26c d i t d max i 1 2 3 d 26d c i t c max i 1 2 3 d where d is the total number of active pumping wells t is the number of stress periods q i t is the pumping rate from ith pumping well in the tth stress period q i minand q i max are the minimum and maximum pumping rates allowed from ith pumping well d i t is the drawdown in the ith pumping well location c i t is the contaminant concentration in the ith pumping well location d max is the permissible drawdown c max is the permissible contaminant concentration and f c1 1 and f c2 20 are multiplying factor which is used to give the same weightage to both terms of the composite function however f c1 and f c2 also can be assumed as cost coefficients to represent the objective function in the form of operational cost cost for water pumping and water treatment for groundwater remediation assuming the multiplying factor f c1 α3 δt and f c 2 m 0 100 α 4 the eq 26a can be reformulated as 27a minimize of 2 α 3 δ t t 1 t i 1 d q i t α 4 m e where α3and α4 is the cost coefficient associated with water extraction injection or water treatment the eq 27a is mathematically similar to the objective function provided in a previous research yang et al 2013 4 model development initially the groundwater flow and contaminant transport processes are simulated using finite difference based modflow 2005 and mt3dms models the so models are used to obtain optimal pumping locations and optimal pumping rates for groundwater remediation 4 1 determination of the optimal location of pumping wells the so models for the determination of optimal pumping locations are developed by coupling simulation model directly with the evolutionary hunting strategy based metaheuristics ehsms the steps for the determination of optimal pumping well locations are enumerated below 1 using the well package of modflow generate n c 231 number of pumping wells the pumping wells are numbered as 1 2 3 4 n c 2 assign zero pumping rates to each pumping well 3 generate a d 15 dimensional vector w r containing unique random integers within the range 1 231 the vector w r represents the set of active pumping wells 4 to each active pumping well assign a constant pumping rate of value k q 5 compute the fitness function value of 1 using eq 25 6 iteratively repeat steps 2 5 using ehsms to obtain optimal fitness function value of 1 and the corresponding vector w r of optimal pumping wells the optimal pumping locations obtained here will be used directly for the determination of optimal pumping rates 4 2 determination of optimal pumping rates here elm based proxy simulator is coupled with ehsms to obtain optimal pumping rates the optimal pumping well location w v obtained in the previous section is used directly to obtain optimal pumping rates the steps are enumerated below 1 in the numerical model set the pumping location to vector w v 2 using the numerical model generate input and output datasets to train the elm model the input dataset are the randomly generated pumping rates within specified range and the output dataset are the respective hydraulic drawdown contaminant concentration and the percentage of contaminant mass extracted from the aquifer 3 develop a proxy simulator by training the elm model using the input and output dataset check the accuracy of the proxy simulator with respect to the numerical model 4 develop so model by coupling proxy simulator with ehsms 5 obtain the optimal fitness value of 2 by iteratively executing the so model while keeping hydraulic head and contaminant concentrations below the specified permissible limit also compute the corresponding pumping rates and the percentage of extracted contaminant mass eq 26 5 numerical experiments we here consider six composite functions to check the performances of ehsms and other metaheuristics composite benchmark functions have a massive number of local optima thus very difficult to obtain global optima using metaheuristics the search space of composite functions f 10 f 15 resembles many real world optimization problems table a2 fig 4 hence they are ideal for checking local minima escape as well as exploitation and exploration ability the parameter settings of the metaheuristics are given in table a3 supplementary material appendix a in table 1 it is observed that the results obtained by ehsms are consistently better than other optimization algorithms the findings suggest superior local minima escape capability as well as better exploration and exploitation ability of ehsms over the other metaheuristics 6 results and discussion we here consider two case studies to check the performances of the extreme learning machine elm and evolutionary hunting strategy based metaheuristics ehsms for groundwater remediation the first case study is a simple homogeneous unconfined aquifer which is mainly used to validate ehsms for groundwater remediation studies the second case study is a more complex one which is used to check the efficacy of the two step approach and other proposed methodologies 6 1 case study 1 here we consider a hypothetical homogeneous unconfined aquifer taken from majumder and eldho 2020 the boundary condition of the aquifer is shown in fig 5a other aquifer parameters are area a 2000 m 2000 m aquifer thickness h b 60 m hydraulic conductivity k 7 5 m day porosity of the medium η 0 3 areal recharge n r 0 0002 m day longitudinal dispersivity α l 60 m transverse dispersivity α t 15 m initial tds total dissolved solids concentration 500 mg l it is further assumed that the aquifer is polluted for ten years by an instantaneous injection of 20 000 kg contaminant over a source of area 100 m 100 m fig 5a to remediate the aquifer in the next five years a total of five pumping wells are installed by manually studying the behavior of contaminant plume the aquifer domain is discretized using 40 000 uniformly distributed grids the size of each grid is 10 m 10 m finite difference based modflow 2005 and mt3dms models are used to simulate groundwater flow and contaminant transport processes due to the absence of heterogeneity in the aquifer parameters and very few numbers of decision variables pumping wells the aquifer problem considered here is relatively simple and computationally inexpensive hence the numerical model is directly coupled with ehsms for groundwater remediation here the remediation objective is to minimize the total amount of pumping rates first term of the eq 26a the constraints are maximum permissible drawdown 6 m and maximum permissible contaminant concentration is 550 mg l the decision variables pumping rates should be within the range 1500 0 m3 day the optimal total pumping rates obtained by the ehsms and other metaheuristics are shown in table 2 further pumping rates and constraints hydraulic drawdown and contaminant concentration at the five pumping wells obtained using ehsms are also shown in the table 3 from table 2 it is observed that all the metaheuristics obtained almost similar level fitness values the contour of contaminants after the end of the remediation period is shown in fig 5b the results suggest the validity of ehsms for groundwater remediation studies 6 2 case study 2 we here consider a piecewise heterogeneous unconfined aquifer with irregular boundary conditions as shown in fig 6a the boundary conditions of the aquifer are similar to the east texas study aquaveo 2018 however hydraulic conductivity distribution and most of the other aquifer parameters are significantly different from the east texas study area the changes to the east texas case study are aimed to make the aquifer piecewise heterogeneous and achieve a similar level of complexity as seen in real world aquifers the aquifer boundary conditions and variations of piecewise hydraulic conductivity fields are shown in fig 6a other aquifer parameters porosity η 0 3 recharge n r 0 00006 m d top elevation e t 230 m bottom elevation e b 180 m aquifer thickness h b 50 m specific yield s y 0 2 longitudinal dispersivity α l 50 m transverse dispersivity α t 10 m number of stress period t 15 length of each stress period t 365 days and number of time steps in each stress period δt 10 also in fig 6a an artificial recharge zone of area 720 231 76 m2 is shown during the remediation process extracted water from the pumping wells will be uniformly discharged over this entire area of the recharge zone further it is assumed that the aquifer is contaminated by five contaminant sources for a period of five years fig 6a the injection rate and contaminant concentration of the injected water in each contaminant source are 75 m3 day and 75 000 μg l ppb respectively the groundwater flow and contaminant transport processes of the aquifer are simulated using finite difference based modflow 2005 and mt3dms models in the finite difference model the aquifer domain is uniformly discretized using 6400 grids the size of each grid is 53 54 m 29 95 m the contaminant plume after five years of active contamination is shown in fig 6b due to heterogeneity in hydraulic conductivity the initial contaminant plume is not uniform and also it is widely spread over the aquifer domain the aim here is to remediate the aquifer in the next ten years using fifteen pumping wells to achieve these objectives the study aims novel two step approach for groundwater remediation pumping well location optimization followed by pumping rate optimization 6 2 1 pumping well location optimization a properly placed network of pumping wells can extract more contaminant mass from the aquifer than an arbitrarily placed network of pumping wells here a total of 231 grids are selected as the possible location of pumping wells fig 6b the aim is to choose the 15 best pumping wells from the 231 pumping wells by minimizing the total amount of contaminant mass in the aquifer with a fixed pumping rate from each of the pumping wells eq 25 the possible combinations of pumping wells are massive c 231 15 231 15 231 15 due to the massive number of pumping wells combination and highly nonlinear relationship between pumping well location and amount of contaminant mass in the aquifer it is extremely difficult to approximate numerical groundwater flow and transport simulation models with a proxy simulator using finite number of input and output dataset hence to find optimal pumping well locations the numerical model is coupled directly with ehsms further in pumping well location optimization the decision variables locations of pumping wells should always be an integer hence this is a combinatorial optimization problem here we consider two scenarios in the first scenario all the 231 pumping wells are considered as candidate pumping well locations the pumping rate from each well is assumed to be 150 m3 d a constant value the best pumping well locations obtained by ehsms and other metaheuristics are shown in fig 7 from the figure it is observed that the pumping wells are aligned close to the centreline of the plume also the pumping wells are clustered in the zone of high contaminant concentration in table 4 a comparison of ehsms with other metaheuristics are shown in terms of the percentage of contaminant mass remained in the aquifer from the table it is observed that the performances of ehsms are better than other metaheuristics among the ehsms the performance of ehho is the best followed by egwo and ewo it can be seen in fig 7 that many of the pumping wells are very close to each other which may results in higher drawdown in the vicinity of pumping wells the spread of the pumping wells can be controlled by constraining some of the possible pumping well locations in the optimization model in the second scenario all the even pumping well locations are considered as the possible location of pumping wells disregarding the odd pumping well locations the optimal pumping locations obtained using the ehsms and other metaheuristics while constraining the even pumping well locations are shown in fig 8 from fig 8 it is observed that the pumping wells are widely distributed in the contaminated zone than those in fig 7 however the wider distribution of pumping wells came at the expense of an increase in the percentage in contaminant mass remained in the aquifer the constraining strategy may be more helpful if the number of grids used to represent pumping wells is huge and the size of each grid is small in such cases all the grids cannot be considered as candidate pumping wells since there should be some distance between two different pumping wells furthermore it may be possible that the area of the grid may be less than the area of pumping well here we could not perform any statistical analysis due to the expensive nature of the simulation model to compensate for this a detailed statistical analysis is carried out in the next section for pumping rate optimization 6 2 2 pumping rate optimization the pumping well location obtained using combinatorial optimization is directly used for pumping rate optimization to enhance computational efficacy we here aim to approximate the numerical simulation model with elm based proxy simulator using the numerical model a total of 15 000 input output datasets are generated the input datasets are the pumping rates within the range 300 0 m3 day the output dataset is the corresponding aquifer responses hydraulic drawdown contaminant concentration and the total amount of contaminant extracted from the aquifer out of 15 000 datasets 14 000 are used to train the elm model and the remaining 1000 datasets are used to test the efficacy of the elm model the number of hidden neurons is assumed as 1000 the regularization parameter c r can be computed using the cross validation approach deng et al 2009 scardapane et al 2015 vuković et al 2018 the regularization parameter c r are searched in the interval 2 j j 20 19 19 20 for the minimum rmse value of the testing dataset once the regularization parameter c r is obtained in terms of 2 j we further refined c r by searching around it for minimum rmse value the best approximation of hydraulic drawdown contaminant concentration and the total amount of extracted contaminant mass are obtained using the elm model when the values of the regularization parameter c r are 0 25 0 1 and 10 7 respectively the performances of the elm algorithm in terms of root mean square error rmse and coefficient of correlation r are shown in fig 9 the result shows that for both hydraulic drawdown and contaminant mass estimation the coefficient of correlation r is 0 999 whereas for contaminant concentration estimation the coefficient of correlation r is 0 989 furthermore the rmse value is very close to zero for both hydraulic head and contaminant mass estimation whereas the rmse value is 4 97 for the contaminant concentration estimate the results suggest an excellent generalization ability of the elm model in approximating the numerical model such high accuracy of the elm model is desired when approximating a physics based numerical model with no uncertainty in its parameters the elm based proxy simulator is further coupled with ehsms to develop so models a composite function is used to minimize the sum of the water withdrawal rate and the percentage of contaminant mass extracted eq 26 the value of the multiplying factor f c1 and f c2 are assumed as 1 and 20 respectively the multiplying factor f c 2 is used to give similar weightage to both terms of eq 26 the maximum permissible hydraulic head and contaminant concentrations are specified as 2 5 m and 500 μg l respectively all the metaheuristics are executed twenty five times and the results in terms of statistical parameters best average standard deviation sd and rank are tabulated table 5 also in table 5 the fitness value of 2 is segregated into optimal pumping rates and the percentage of extracted contaminant mass the results suggest that the performances of ehsms are better than hsms and other metaheuristics among the ehsms the performance of egwo is slightly better than ewo and ehho the table 6 also lists individual pumping rates constraints hydraulic drawdown and contaminant concentration at the pumping well locations corresponding to the best solutions obtained by ehsms the results of all the metaheuristics are further compared using the violin plot fig 10 a violin plot is a hybrid of the box plot and probability distribution plot of numeric data which is usually smoothened using the kernel density estimator kde violin plot depicts many statistical parameters such as whiskers to indicate upper extreme and lower extreme quartiles a box to represent interquartile ranges a marker to represent the median of the data and probability density function pdf to depict the distribution of numeric data the violin shape pdf also helps to spot whether data are sparse or multimodal in fig 10 it is observed that the difference between upper and lower quartiles is least for ehsms it means that data are spreads over a smaller region for ehsms compared to other metaheuristics further the markers in the violin plot state that the median value of ehsms is better than the rest of metaheuristics moreover the probability density function indicates that ehsms are more likely to achieve better optimal values than their counterpart we further performed the one way analysis of variance anova and bonferroni test to check the difference between the optimal solutions obtained by ehsms and other metaheuristics the p value obtained using anova test is 0 which suggests that there are significant differences between the optimal solutions obtained by different methods a pictorial representation of the bonferroni test is shown in fig 11 in the figure the lines represent the data interval interval of optimal solutions obtained by the metaheuristics a symbol in the middle of the line represents the mean value of the data data of any two groups are significantly different if their interval line is disjoint from the figure we can state that the results obtained using ehsms are significantly different better than those obtained using other metaheuristics the complete details of the bonferroni test are shown in table a5 supplementary material appendix a the efficacy of the two step approach is further compared to an alternate approach for groundwater remediation in the alternate approach the pumping well locations and pumping rates are explicitly incorporated together in the same management model hence the dimension of the search space in the alternate approach is more than the two step approach the problem considered here is a mixed integer combinatorial programming problem since one decision variable location of pumping wells should always be an integer also due to the high level complexity between input variables pumping well locations and pumping rates and output variables drawdown contaminant concentration and percentage contaminant mass extracted the simulation model cannot be approximated easily using the elm based proxy simulator hence to develop the so model the simulation model is directly coupled with the metaheuristics the fitness function is in the form of eq 26 in fig 12d the optimal pumping location obtained using one of the ehsms ehho with initial contaminant concentration is shown it is observed that the optimal well locations obtained using the alternate approach have almost similar configurations to those obtained using the two step approach fig 12d and fig 7 it is observed that the fitness value of 2 total optimal pumping rates and the percentage of extracted contaminant mass obtained using the alternate approach are found to be 3648 01 1698 81m3 day and 97 46 respectively table 7 the solutions found here seem to be slightly inferior with respect to solutions obtained using the two step approach table 5 further in fig 13 the optimal pumping locations obtained using the alternate approach is compared with the two step approach with respect to contaminant concentration at the end of the remediation period 6 2 3 convergence analysis the fitness value of 1 obtained in the first step of the two step approach is plotted with respect to the number of generations to illustrate how the ehsms are converging to the solution fig 12a the figure shows similar convergence behavior of ehho ewo and egwo in obtaining the optimal solutions the average number of generations required by any of the ehsms to obtain the optimal solution is approximately 350 all the ehsms took approximately 40 50 h to obtain the optimal solutions in the first step the fitness value of 2 obtained in the second step of the two step approach is plotted with respect to the number of generations in fig 12b here the average number of generations required by any of the ehsms to obtain the optimal solution is close to 150 the second step is computationally very efficient due to using the elm based surrogate model in approximating numerical simulation models the computational time to obtain optimal solutions using ehms in the second step is approximately 5 min further in fig 12c the convergence behavior of the ehho in obtaining the optimal solution of 2 using the alternate approach is shown fig 12c depicts that a very high number of generations are required in the alternate approach to achieve a similar level of fitness value of 2 compared to the two step approach fig 12b due to the higher number of generations the alternate approach is computationally expensive the computational time in obtaining optimal solutions using the alternate approach is around 140 h 7 discussion the present study considers a deterministic definition of aquifer parameters hydraulic conductivity recharge however in the real field aquifer system the parameters are spatially highly heterogeneous and there are always uncertainties in estimating the aquifer parameters from a limited set of observed values hence the parameter uncertainties should always be considered for the effective design of groundwater remediation systems in the real field aquifer the two commonly used methods to design groundwater remediation strategies under parameter uncertainties are chance constrained optimization and robust optimization aly and peralta 1999 the two step approach of the present study can be combined with chance constrained optimization or robust optimization for designing groundwater remediations strategy under parameter uncertainty it should be noted here that the chance constrained optimization or robust optimization methods should be used in both steps of the two step approach in this case the first step will yield the optimal pumping locations for effective groundwater remediation under parameter uncertainty and the second step will also yield the optimal value of the composite objective function under parameter uncertainty the two step approach of the present study considers only one operation period static pumping policy for groundwater remediation however multiple operation periods the dynamic pumping policy should be considered for designing a more efficient remediation strategy for real field aquifers huang and mayer 1997 the two step approach of the present study can be used for groundwater remediation under a dynamic pumping policy with some modifications in the modified two step approach for each operation period the optimal pumping well locations have to be updated using eq 25 according to the configuration of the contaminant plume remaining amount of contaminant mass in the aquifer of the respective operation period 8 conclusion the present study proposes so models for groundwater remediation by coupling evolutionary hunting strategy based metaheuristics ehsms with extreme learning machine elm the ehsms are proposed by modifying hunting strategy based metaheuristics hsms with biological evolution operator elimination mechanism based on the survival of fittest principle in nature and elite opposition based learning strategy the performances of ehsms to obtain the optimal solutions of unimodal multimodal and composite benchmark functions are found to be better than hsms and other optimization algorithms in terms of exploitation and exploration capability and ability to avoid entrapment in local minima further ridge regression based regularization parameter is used to enhance the generalization ability of extreme learning machine the elm model shows excellent generalization ability as well as computational efficiency in approximating the numerical simulation model the most novel contribution of this research is the two step approach for groundwater remediation in this approach the optimal pumping locations obtained in the first management model acts as input in the second to compute optimal pumping rates this approach thus helps to negate the curse of dimensionality associated error which is always a big issue in the field of high dimensional optimization moreover the simulation model of the second step can be approximated using an accurate surrogate simulator which can enhance the computational performance conversely in the alternate approach optimal pumping locations and optimal pumping rates are computed by explicitly incorporating them together in the same management model the alternate approach is computationally expensive than the two step approach due to the higher dimensional search space further the results show that the two step approach is slightly better than the alternate approach in terms of accuracy in this study we considered two aquifer problems to check the efficacy of the two step approach elm based proxy simulator and ehsms for groundwater remediation the first case study is a homogeneous unconfined aquifer which is mainly used to validate ehsms for groundwater remediation the second case study is relatively complex with piecewise heterogeneous hydraulic conductivity fields and irregular boundary conditions in the second case study detailed statistical analysis is carried out to compare ehsms and other metaheuristics pso cso de hho wo and gwo the analysis shows that ehsms are more accurate than other metaheuristics among the ehsms the performance of ehho is found to be slightly better than ewo and egwo in the future we wish to explore the proposed methodologies for groundwater remediation in a highly heterogeneous medium under hydrological uncertainty and dynamic pumping policy software and data availability in this study matlab 2016b is used to carry out all the coding the computer configuration is intel i5 9400f cpu 2 90 ghz and 16 gb ram please use the link to access the matlab codes and other dependencies https github com parthamajumder1985 groundwater remediation toolbox declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments c lu acknowledges the financial support from the national natural science foundation of china 51879088 fundamental research funds for the central universities b200204002 and the natural science foundation of jiangsu province bk 20190023 appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103864 
1357,subsurface remediation using nanoscale zero valent iron nzvi is a promising in situ technology that can transform certain groundwater contaminants into non toxic compounds however field scale implementation of nzvi technology has faced major challenges due to poor subsurface mobility limited longevity and well clogging all leading to a shorter nzvi travel distance this distance nzvi travels in the subsurface is an important parameter since it influences the amount of contaminants that can be reached and thereby remediated there are several factors which may affect nzvi travel distance such as groundwater velocity injection concentration and rate lag period duration when nzvi injection is stopped solution viscosity and subsurface heterogeneity although various studies have been performed to reveal the effect of different factors on nzvi transport in homogeneous domains few studies have focused on heterogeneous media which is more representative of field conditions in this study a statistical analysis was performed using a two dimensional numerical model which simulated carboxymethyl cellulose cmc stabilized nzvi transport in randomly distributed soil permeability fields of two aquifers to examine the factors that have the greatest impact on nzvi travel distance among all possible factors field scale solution viscosity and injection rate had a statistically significant effect on nzvi travel distance in both the horizontal and vertical directions as well as on the attached mass additionally the lag period between injections had a statistically significant effect on the attached mass but not the travel distance these results suggest that having a long injection period followed by a short lag phase during field deployment may result in less nzvi attachment lastly aquifer heterogeneity impacted the nzvi spread while the impact of intrinsic groundwater velocity and injection concentration was found not to be statistically significant results from this numerical study can aid in field scale cmc nzvi injection by identifying key factors for remediation optimization keywords groundwater remediation soil heterogeneity nanoscale zero valent iron field scale colloid transport statistical analysis glossary symbol a hamaker constant j af anisotropy factor as porosity dependent parameter c aqueous nzvi or polymer concentration mol m 3 c c i relative nzvi concentration ci nzvi injection concentration kg m 3 cr chromium db bulk diffusion coefficient m2 s 1 dc collector soil grain diameter m ddisp effective hydrodynamic dispersion tensor m2 s 1 dfn diffusion coefficient of nzvi m2 s 1 dfp diffusion coefficient of polymer m2 s 1 dm incremental portion of the nzvi mass dp diameter of nzvi m f statistical fisher test fe0 zero valent iron fn nzvi flux mol m 2 s 1 fp polymer flux mol m 2 s 1 h0 null hypothesis hi height of injection well m ir injection rate m3 s 1 iz m second moment of the mass about the z axis m mol k boltzman constant j k 1 katt attachment rate coefficient s 1 in colloid filtration theory kp permeability of the aquifer porous media m2 m total mass of nzvi mol m 3 mf mass flux kg m 2 s 1 mwn molecular weight of nzvi kg mol 1 mwp molecular weight of cmc polymer kg mol 1 n vector normal to the boundary na attraction number ng gravity number npe peclet number nr aspect ratio nvdw van der wall s number ni nickel pb lead pt percentage of polymer in total solution q darcy velocity m s 1 qm sink source kg m 3s 1 ri radius of injection well m s attached concentration mol kg 1 st total attachment mol kg 1 t time s t temperature k v groundwater pore velocity or background aquifer velocity referred to as the groundwater velocity m s 1 x distance in x direction m xh horizontal correlation length m xij value of random variable at jth observation under ith factor xijk value of the random variable at ijth observation under kth replicate xcn horizontal center of mass m xnsp spreading of the mass in horizontal direction m xp mole fraction cmc polymer mol of polymer mol of water zcn vertical center of mass m znsp spreading of the mass in vertical direction m zv vertical correlation length m θ porosity α attachment efficiency β effect of factor b ϵij ϵijk random error component η0 single collector contact efficiency μ mean μp polymer viscosity pa s μsol solution viscosity pa s μwf water viscosity pa s σ2 variance ρf fluid density average density of nzvi suspension kg m 3 ρb density of porous media kg m 3 τi factor effect effect of factor a in two factor anova τβ effect of interaction between a and b h hydraulic gradient m m 1 abbreviations 2d two dimensional 3d three dimensional anova analysis of variance bc boundary condition cf constant flux cft colloid filtration theory ch constant head c2h6 ethane cmc carboxymethyl cellulose compsim 3d finite difference three phase simulator comsol a finite element based multiphysics simulation software dl darcy s law module nzvi nanoscale zero valent iron orp oxidation reduction potential pce c2cl4 tetrachloroethylene ppt 1 push and pull test 1 ss suspended solid tds transport of diluted species module ts total solids 1 introduction groundwater contamination can occur when surface pollutants make their way to underlying aquifers this can lead to environmental damage and potential health issues if such aquifers are used as drinking water sources harr 1996 remediation of such aquifers can involve ex situ technologies which are typically costly and time consuming or in situ methods which can be ineffective the most promising technologies target the source zones since decreasing the source of the contamination has proven to be the most effective way of reducing contamination kueper et al 2014 remediation using nanoscale zerovalent iron nzvi is one promising in situ technology that can remediate groundwater aquifers contaminated with chlorinated solvents or heavy metals and can be implemented at the source zone gillham and o hannesin 1994 liu et al 2005 zhan et al 2008 barnes et al 2010 sakulchaicharoen et al 2010 zhou et al 2011 o carroll et al 2013 mondal et al 2018 li et al 2019 qian et al 2020 nzvi remediation works by having the zero valent iron fe0 release electrons when it is oxidized when injected into the subsurface typically in the source zone these electrons reduce groundwater contaminants through reductive dechlorination converting them into non toxic compounds wang and zhang 1997 li et al 2006 o carroll et al 2013 for example tetrachloroethylene pce c2cl4 can be reduced to ethane c2h6 which poses little toxicological risk li et al 2006 in addition inorganic contaminants such as hexavalent chromium cr6 divalent nickel ni2 and divalent lead pb2 can be reduced by nzvi to less toxic states as cr3 ni0 and pb0 respectively o carroll et al 2013 despite its promising characteristics field scale implementation of nzvi technology has faced major challenges due to poor subsurface mobility limited longevity farrell et al 2000 phenrat et al 2007 kocur et al 2013 2014 velimirovic et al 2014 liu et al 2019 and well clogging elliott and zhang 2001 all leading to a smaller travel distances although nzvi mobility is the focus of most remediation studies due to the importance of the particles reaching the contaminants most field and lab applications report moderate mobility ranging from 1 to 6 m elliott and zhang 2001 zhang 2003 henn and waddill 2006 kocur et al 2014 köber et al 2014 nanorem final report 2017 nunez garcia et al 2020 in addition the reported nzvi travel distance is often measured by visual sightings of nzvi henn and waddill 2006 examining contaminant concentration downgradient of the injection well or by indirect measurements such as change in oxidation reduction potential orp suspended solid ss total solid ts analysis etc wei et al 2010 chowdhury et al 2015 a larger travel distance is preferable since it can reduce the total time of remediation and provide a cost effective solution there are various factors that can affect nzvi travel distance including rapid aggregation settling nzvi injection method groundwater velocity injection concentration injection rate lag phase solution viscosity and subsurface heterogeneity these effects will be briefly described herein and further information may be obtained from the cited sources aggregation may occur due to particle particle interaction petosa et al 2010 and settling occurs due to gravitational forces phenrat et al 2007 these factors affect nzvi travel distance by reducing the amount of nzvi particles available for transport o carroll et al 2013 raychoudhury et al 2014 kocur 2015 ling et al 2021 aggregation can be minimized by coating the particles with polymers such as carboxymethyl cellulose cmc guar gum xanthan gum and starch tiraferri and sethi 2008 vecchia et al 2009 wang et al 2014 ji et al 2019 wu et al 2021 the nzvi injection method may also influence nzvi travel distance different types of injection strategies can be used for nzvi remediation these include pneumatic injection direct injection pressure pulse injection and gravity feed injection su et al 2013 kocur et al 2014 among these strategies pneumatic injection which uses high pressures can create fracture networks that result in preferential flow paths that can facilitate nzvi transport navfac 2013 which cannot be modelled by advection dispersion deposition theory while gravity feed injection generally maintains a constant head ch at the injection well other injection methods depend on injecting a flux in the porous media krol et al 2013 considered both constant flux cf and ch injections from field injection data from bennett et al 2010 in a three dimensional 3d model and found that a higher travel distance can be obtained using cf injection kocur et al 2014 conducted a field scale test of nzvi injection in a contaminated sandy subsurface and found that field nzvi travel distance and longevity can be achieved with very high groundwater velocities and highly stable nzvi suspensions in addition according to henn and waddill 2006 high groundwater velocity and change in groundwater flow direction due to recirculation made it possible to distribute the nzvi suspension over the treatment area many others have found similar results chowdhury et al 2015 mondal et al 2018 injection mass flux is proportional to injection concentration and injection rate chowdhury et al 2015 predicted that injecting a higher volume of nzvi suspension resulted in an increase in nzvi travel distance additionally krol et al 2013 showed nzvi travel distance is affected by injection type ch or cf and thereby injection rate others also found that higher injection rate 2 78 10 5 m3 s 1 vs 1 38 10 4 m3 s 1 resulted in higher nzvi travel distance nanorem final report 2017 lastly lag phase i e the time when there is no injection during successive injection of nzvi can also lead to nzvi deposition in porous media under low groundwater velocity conditions bennett et al 2010 kocur et al 2014 and make nzvi immobile krol et al 2013 nzvi travel distance can also be influenced by the solution viscosity krol et al 2013 chowdhury et al 2015 mondal et al 2018 increasing the nzvi solution viscosity can result in greater nzvi stability nanorem wp4 2016 nanorem final report 2017 this can be done by the addition of polymers such as cmc li et al 2015 cohen and weisbrod 2018 li et al 2016 observed greater travel efficiency 38 nzvi mass removed from the outlet of the experimental sand tank by adding 2000 g m 3 cmc compared to 1000 g m 3 where only 11 mass removal rates were seen at a fixed nzvi concentration however increasing solution viscosity can have an opposing effect on nzvi travel distance since increased viscosity decreases hydraulic conductivity lowering the travel velocity as hydraulic conductivity is inversely proportional to viscosity and hence the travel distance mondal et al 2018 lastly subsurface heterogeneity can affect nzvi travel distance during various field injections subsurface heterogeneity was found to strongly hinder nzvi transport as it moved preferentially in the high permeability areas kocur et al 2014 and prevented sufficient nzvi distribution henn and waddill 2006 the preferential flow caused by subsurface heterogeneity can negatively impact the remediation goals as nzvi may not reach the target contaminant he et al 2010 velimirovic et al 2014 in addition aggregation is influenced by soil particle size li et al 2016 which varies in a heterogeneous domain as aquifers are heterogeneous in nature modelling studies should incorporate heterogeneity to investigate its effect and understand the possible effects there are a few modelling studies that simulate soil heterogeneity cullen et al 2010 simulated the transport of carbon nanoparticles in a heterogeneous permeability field and opined that less movement of nanoparticles occurs in a heterogeneous media due to more retention caused by variable soil particle size and groundwater velocities compared to a homogeneous media they suggested that nanoparticle mobility can be overestimated if a heterogeneous aquifer is assumed to be homogeneous while strutz et al 2016 predicated that nzvi particles will be retained in high permeability areas within a heterogeneous system most recently mondal et al 2018 observed that injected nzvi was variably distributed even in an experiment consisting of sand packed as uniformly as possible indicating that heterogeneous effects can be seen even in systems that are considered homogeneous although these studies have investigated the effect of some factors on nanoparticle transport using laboratory and numerical studies no studies have investigated whether the impact of several factors on nzvi transport is statistically significant in heterogeneous aquifers representing in situ conditions previous studies have compared system parameters and nzvi transport behaviors but often the comparison is inconsistent due to changing parameters one thing that is common among all the studies is the agreement that optimum travel distance for successful remediation has not yet been achieved this paper outlines a computational model that can simulate nzvi transport and a method of using one and two factor analysis of variance anova to examine the effect of several factors on cmc stabilized nzvi travel distance one factor anova was performed on five factors groundwater velocity injection concentration injection rate lag phase and viscosity of the injected solution for two aquifers of variable heterogeneity these five factors can be independently controlled and do not depend on permeability or soil collector diameter in addition two factor anova was performed to understand the impact of soil heterogeneity results from this research enable a better understanding of which field scale transport factors are most important for effective remediation 2 material and methods comsol multiphysics a finite elements program can be used to simulate physics based problems and various types of physical phenomena such as fluid flow heat transfer and pore scale flow comsol user s guide 2017 various types of equations are built into comsol modules with appropriate characteristic equations and fundamental features these modules can be linked together as well as modified with additional user defined equations comsol user s guide 2017 for this work the transport of diluted species tds module and the darcy s law dl module were used as the two modules were capable of simulating nanoparticle transport in porous media reactive transport of nzvi e g reaction of nzvi with contaminants was not considered in this model 2 1 3d homogeneous nzvi transport model to model and validate cmc nzvi transport data from krol et al 2013 was used krol et al 2013 simulated a field study bennett et al 2010 in which cmc coated nzvi prepared on site was injected into a shallow granulated aquifer consisting of a coarse grained deposit krol et al 2013 used compsim a 3d finite difference three phase simulator developed by sleep and sykes 1993 krol et al 2013 showed that the compsim model was able to accurately predict the field study therefore the results of krol et al 2013 was used to validate the comsol model developed in this study to ensure that the cmc nzvi transport was correctly implemented to simulate the krol et al 2013 domain the same parameters from the push and pull test ppt 1 in the sand layer fig 1b were used and are summarized in table 1 these parameters represent the subsurface properties as well as the nzvi and polymer characteristics details can be found in krol et al 2013 and bennett et al 2010 2 1 1 numerical approach the comsol dl module defines the transient flow equation as follows 1 ρ f θ t ρ f q q m where q is the darcy velocity m s 1 q θv where v is the groundwater pore velocity or background aquifer velocity referred to as the groundwater velocity m s 1 θ is the porosity ρf is the fluid density kg m 3 and qm is the source sink term kg m 3 s 1 the 3d darcy velocity field created in the dl module is transferred to the tds module where the transport of polymer coated nanoparticles in a porous media is described by eq 2 2 c t θ q c x q c y q c z d disp 2 c x 2 θ d disp 2 c y 2 θ d disp 2 c z 2 θ θ k att c where c is the aqueous nzvi or polymer concentration mol m 3 at a distance of x m and time t s and ddisp is the effective hydrodynamic dispersion tensor m2 s 1 which is estimated using effective diffusion coefficient dispersivity and groundwater velocity the attachment rate coefficient katt s 1 is defined by the colloid filtration theory cft tufenkji and elimelech 2004a and expressed by eq 3 this term is only applied to nzvi transport since the polymer cmc is considered to be a conservative i e non reactive species and therefore the attachment rate for the polymer is assumed to be zero 3 k att 3 1 θ α η 0 v 2 d c where α is the attachment efficiency coefficient defined as the ratio of particles that stick to the collector to those that strike the collector η0 is the collector contact efficiency defined as the ratio of colloids hitting the collector to those approaching the collector dc is the collector soil grain diameter m the attachment efficiency coefficient α was assumed to be 0 02 based on a validation analysis performed by krol et al 2013 while η0 is a function of various parameters based on tufenkji and elimelech 2004a and is calculated in the model details of the η0 calculation can be found in the supplementary information si cft was chosen to model nzvi transport since it has been shown to accurately capture transport where little agglomeration or detachment is observed in addition cft is applicable for sufficiently low particle concentration where particle particle interactions e g aggregation ripening and blocking are reduced tufenkji and elimelech 2004b such is the case for the cmc nzvi used in this study and observed by others phenrat et al 2009 cullen et al 2010 bennett et al 2010 kocur et al 2013 krol et al 2013 molnar et al 2015 chowdhury et al 2015 li et al 2016 mondal et al 2018 in particular this model was used to accurately predict the field transport of cmc coated nzvi particles by several authors phenrat et al 2009 he et al 2010 lin et al 2010 bennett et al 2010 raychoudhury et al 2010 kocur et al 2013 krol et al 2013 li et al 2015 2016 chowdhury et al 2015 saberinasr et al 2016 mondal et al 2018 given that the injections are cmc stabilized and are injected into a highly porous medium the simulated domain represented a remediation scenario with a cylindrical injection well defined in the middle of the domain which injected a cmc coated nzvi suspension into the subsurface the domain was modelled using a symmetrical boundary condition bc along the xz plane through the injection well to minimize computational time fig 1a shows the bcs and the characteristics of half of the 3d modelled domain for a sand layer where cmc nzvi solution was injected as described by bennett et al 2010 the initial nzvi cmc concentration was defined as zero in the domain and the direction of groundwater flow was simulated from left to right through the yz plane defined by the constant hydraulic head bc no flow and no flux bc were defined at the upper and lower boundaries the xy plane as well as the side boundaries the xz plane the injection well was used for defining a water mass rate m3 s 1 in the dl module and a nzvi and polymer constant flux mol m 2 s 1 in the tds module to calculate the solution viscosity changing with time and space the grunberg and nissan equation grunberg and nissan 1949 was used which relates solution viscosity with component mole fractions and viscosities as follows 4 log μ sol x p log μ p x w log μ wf xp and xw are mole fractions temporally and spatially changing of cmc polymer and water respectively krol et al 2013 performed several viscosity experiments using cmc 90 k 90 000 g mol 1 see krol et al 2013 for details and obtained the polymer viscosity μp using a fitting procedure the resulting polymer viscosity value was used as a constant in the model to calculate the temporally and spatially changing solution viscosity based on polymer concentration although non newtonian behaviour of cmc solutions have been recorded benchabane and karim 2008 others have found this phenomena to be insignificant given the range of velocities experienced in the subsurface and in this study sochi 2007 yang and zhu 2007 balavi 2017 therefore non newtonian behaviour was not explored in this paper 2 1 2 verification of model results the comsol model developed in this study was verified using the results from the compsim model presented in krol et al 2013 and is shown in the si figs s1 and s2 compared relative nzvi concentration and solution viscosity of the two models after 1 and 20 h of constant injection the figures showed a very good match for nzvi concentration and solution viscosity for both comsol and compsim models in addition to this qualitative comparison the difference between the values of the nzvi concentration and viscosity of the two models was calculated using root mean squared table s1 the results showed that good agreement was achieved between the two models these results verified that cmc nzvi transport model was correctly implemented in comsol unlike the compsim model that assumed uniform grid size the comsol model considered a finer mesh around complex geometry the injection well and a progressively larger mesh away from the injection well fig s3 to rule out mesh dependencies a mesh convergence study was performed examining both maximum and minimum grid size as seen in fig s4 maximum nzvi concentration did not vary below a minimum element size of 0 003 m and below a maximum element size of 0 4 m therefore the domain was meshed with minimum element size of 0 001 m and maximum size of 0 4 m to obtain consistent results and lower computational time fig s4 the 3d homogeneous nzvi transport model considered an average aquifer permeability where the soil grain collector size was fixed however in a heterogeneous aquifer permeability as well as collector size varies therefore in the next section the development of a two dimensional 2d heterogeneous nzvi transport model is described which was used to examine the effect of soil heterogeneity on nzvi optimization a 2d nzvi transport model was developed instead of a 3d version due to the availability of 2d heterogeneous permeability distribution data in addition the results of two traverse directions in the case of a 3d model would be similar 2 2 2d heterogeneous nzvi transport model the 2d model developed to study heterogeneous effects was identical to the 3d model described in the above section however the kozeny carman equation kozeny 1927 carman 1937 1956 was included which linked the heterogeneous permeability field to the cft equation eq 3 eq 5 describes this relationship between the collector diameter dc m and the soil permeability kp m2 5 d c k p 1 θ 2 180 θ 3 the collector diameter was also variable when calculating the single collector contact efficiency η0 which resulted in a spatially varied η0 in the heterogeneous model details of the η0 calculation can be found in the si fig 2 represents the bcs and characteristics of a full 2d domain that represents a vertical soil profile of the subsurface an injection well was placed in the middle of the domain which injected the cmc coated nzvi suspension into the subsurface and the initial cmc nzvi concentration was assumed to be zero in the domain the groundwater direction was defined from left to right using a constant hydraulic head bc while no flow and no flux bcs were defined at the upper and lower boundaries 2 2 1 aquifer properties to examine the effect of heterogeneity on nzvi transport two aquifers of variable heterogeneity were used in the simulations the borden aquifer was chosen for this study as it is relatively homogeneous and well characterized sudicky 1986 dekker and abriola 2000 the swiss aquifer was chosen as it is more heterogeneous compared to borden aquifer due to a higher variance in permeability jussel et al 1994 dekker and abriola 2000 fig 3 shows a typical permeability distribution of these two aquifers the permeability fields fig 3 were generated using the field generator in pmwin processing modflow 5 3 which used mejia s algorithm mejía and rodríguez iturbe 1974 frenzel 1995 the swiss aquifer had a higher permeability variance σ2 0 43 than borden aquifer σ2 0 21 resulting in a greater difference in permeability while the mean permeability correlation length and porosity were same for both aquifers table 2 the generated permeability fields were imported into comsol using the interpolation function which allowed for the data to be directly inputted without changing the discretization one hundred permeability realizations for the two aquifers i e borden and swiss with different heterogeneities were generated using the field generator through a variance analysis of nzvi transport parameters fig s5 it was found that convergence of variance occurred within 15 realizations out of 100 where the values were chosen at random to perform the analysis consequently 15 realizations were deemed representative of the aquifer variability while reducing simulation time 3 calculations simulations 3 1 simulated cases for sensitivity analysis to examine the effect of various factors on nzvi transport seven cases were established table 3 each case had a different specified groundwater velocity injection viscosity i e solution viscosity injection concentration or injection rate case 1 was considered the base case and was used with each simulation group cases 2 and 3 had different groundwater velocities cases 4 and 5 had various nzvi concentration values injected case 6 had a different injection solution viscosity and case 7 had a different injection rate for all the cases the other factors were kept constant varying only one factor at a time the values of groundwater velocities were chosen for cases 1 and 2 based on data found from field injection studies table 3 these groundwater velocities were estimated during field nzvi injection in a shallow granulated aquifer by bennett et al 2010 and used in a modelling study by krol et al 2013 case 3 used a groundwater velocity of 10 m day 1 which was 100 times greater than the groundwater velocity used in case 1 this value was used to provide an upper bound to the simulations and corresponds to a very fast moving gravel sand aquifer the nzvi injection concentration values were also chosen from the field injection data case 1 had a injection concentration of 0 96 kg m 3 nzvi corresponding to a flux value of 0 009 mol m 2 s 1 krol et al 2013 chowdhury et al 2015 and case 4 accounted for an increased injection concentration of 2 5 kg m 3 nzvi kocur et al 2013 case 5 used an injection concentration of 96 kg m 3 which was 100 times greater than the injection concentration used in case 1 this represented an upper bound of the concentration chosen to observe the sensitivity of injection concentration on nzvi transport cmc nzvi solutions are typically injected at two solution viscosity values 0 013 and 0 072 pa s corresponding to the molecular weight of the polymer 90 000 g mol 1 cmc 90 k and 250 000 g mol 1 cmc 250 k respectively both of these viscosity values were used by krol et al 2013 in their modelling study while cmc 90 k was used by he et al 2009 and raychoudhury et al 2012 in their experimental investigations cmc 250 k was used to stabilize nzvi suspension by others he and zhao 2007 sakulchaicharoen et al 2010 due to their use in modelling and experimental studies these two viscosity values were used herein the nzvi injection rates were chosen from the field injection data case 1 had a injection flowrate of 5 67e 5 m3 s 1 3 4 l min 1 bennett et al 2010 krol et al 2013 and case 7 accounted for a reduced injection rate of 2e 5 m3 s 1 1 2 l min 1 bennett et al 2010 krol et al 2013 to evaluate the effect of groundwater velocity cases 1 2 and 3 were simulated and compared to examine the effect of injection concentration cases 1 4 and 5 were simulated and compared to examine the effect of injection viscosity cases 1 and 6 were simulated and compared while the effect of injection rate was done by simulating and comparing cases 1 and 7 lastly the lag phase is the interval between two successive nzvi injections during this phase nzvi injection flux is zero and injected nzvi mass from the previous injection may be affected by the reduced velocity caused by the injection stoppage nzvi deposition into porous media can occur during the lag phase under low velocity conditions bennett et al 2010 kocur et al 2014 and nzvi may become immobile krol et al 2013 table 4 presents three simulated cases considering a lag phase all with a total simulation time of 96 h case a base case considered 48 h of constant flux nzvi injection followed by a 48 h of zero flux or lag phase case b considered intermittent nzvi injection of 24 h and lag phase of 24 h for a total of 96 h and case c considered successive constant flux nzvi injection of 12 h and lag phase of 12 h for a total of 96 h case a had the longest continual injection phase 48 h and lag phase 48 h compared to case b and c however in total all cases had the same hours of injection and lag phase 48 h of each therefore in each case the same mass of total nzvi was injected 0 96 kg m 3 with an injection rate of 5 67e 5 m3 s 1 in addition the groundwater velocity before and during lag period for all the cases was kept the same 0 1 m day 1 and the viscosity of the injected solution was maintained at 0 013 pa s 3 2 one factor analysis of variance the one factor anova is a statistical method that calculates the impact of one factor the one factor anova can be represented as follows montgomery and runger 2007 6 x ij μ τ i ϵ ij where xij is the value of random variable at jth observation under ith factor μ is the overall mean τi is the factor effect and ϵij is the random error component montgomery and runger 2007 in this study xij was the random variable which defined nzvi transport distance or spread while the factors τi were groundwater velocity and injection concentration and rate and solution viscosity the analysis assumed that the observations are done in a random order and the effect of a factor was quantified by the null hypothesis 3 2 1 null hypothesis the null hypothesis h0 can be written as follows and states that the means μ are statistically equal montgomery and runger 2007 7 h 0 μ 1 μ 2 to test this hypothesis the fisher f test was selected since it is suitable for relatively small sample sizes 15 in this research for each permeability field used fisher 1922 1954 to check the null hypothesis a statistically significant level of p 0 05 was used in other words if the p value 0 05 then the null hypothesis was not rejected and therefore the factor had no significant effect on the parameter if the p value 0 05 then the null hypothesis was rejected meaning that the means being compared were statistically not equal and thus the factor had a statistically significant effect on the parameter 3 3 two factor analysis of variance the two factor anova can evaluate the individual and combined effect of more than one factor it is particularly useful for a system where a factor may obscure the effect of others on a phenomenon the two factor anova can be represented as follows montgomery and runger 2007 8 x ijk μ τ i β j τβ ij ϵ ijk where xijk is the value of the random variable at ijth observation under kth replicate μ is the overall mean τi is the effect of factor a β is the effect of factor b τβ is the effect of interaction between a and b and ϵijk is the random error component montgomery and runger 2007 during the two factor anova the null hypothesis described in subsection 3 2 1 was used it calculated the effect of three factors and identified whether there is any effect on the mean due to change in factor a factor b and interaction of a and b the latter described a masking effect whereby the effect of one factor became different at different scales of other factors and a large interaction could obscure the main effect krol 2011 3 4 estimation of nzvi center of mass and spread in order to assess the effect of the different factors on nzvi transport in aquifers of various heterogeneity and to quantify the movement and shape of the mass distributions various mass parameters were used these included the center of mass and mass spread in a 2d domain which were considered in two directions i e x and z directions and are shown in fig 4 these parameters were based on the study of dekker and abriola 2000 and krol 2011 and modified for this study the horizontal center of mass xcn of the system was calculated as follows 9 xcn 1 m xdm where m is the total nzvi mass x is the distance in the x direction and dm is the incremental portion of the nzvi mass the spreading of the mass xnsp was determined by the radius of gyration which was determined as follows dekker and abriola 2000 10 xnsp i z m m where iz m is the second moment of the mass about the z axis and was defined as 11 i z m xcn x 2 dm where xcn x is the distance of spread from the z axis passing through the center of nzvi mass the vertical mass parameters i e center of nzvi mass in the z direction zcn and mass spread in the z direction znsp were calculated in a similar manner the total attachment st mol kg 1 in the heterogeneous porous media due to nzvi injection was also evaluated 12 s t s θ k att c ρ b dt where ρb is the density of porous media kg m 3 and s is the attached concentration mol kg 1 4 results and discussion 4 1 results of one factor anova the one factor anova was performed on both aquifers by changing the values of groundwater velocity injection concentration solution viscosity and injection rate as outlined in table 3 to examine the effect of groundwater velocity cases 1 2 and 3 were simulated for 15 permeability realizations similarly to find out the effect of injection concentration cases 1 4 and 5 were used to find out the effect of solution viscosity cases 1 and 6 were used and cases 1 and 7 were used to examine the effect of injection rate one factor anova was also performed on both aquifers considering lag periods as outlined in table 4 tables 5a 5b 5c and 6 show the results of one factor anova since the one factor anova did not distinguish where the effect occurs i e between the first and second level or between the second or third level additional analysis was performed to identify at what level the effect is significant shown in tables 5b and 5c 4 1 1 effect of groundwater velocity fig 5 shows the effect of the groundwater velocity on xcn xnsp zcn znsp and st using boxplots the median of the boxplot is represented by a central mark while the upper and lower edges of the boxplot represent the upper and lower quartiles respectively the tails of the distributions are represented by whiskers of the boxplot as seen in fig 5 there was very little or no change in median irrespective of the aquifer for groundwater velocity ranging from 0 1 m day 1 to 1 5 m day 1 however changes were observed when the groundwater velocity was increased to 10 m day 1 particularly in case of xcn xnsp and st however the p value was less than the significance level of p 0 05 for the center of mass of the borden aquifer only table 5a therefore the null hypothesis was rejected for this case only for all other distributions the p value ranged between 0 094 and 0 998 which was greater than the significance level and therefore the null hypothesis was not rejected in other words the change in groundwater velocity for the chosen range had no statistically significant effect on nzvi transport in terms of movement spread and nzvi attachment onto porous media except for nzvi mass movement in the x direction for the borden aquifer further investigation was done to find out at what scale this effect was statistically significant whether it was due to shifting to a groundwater velocity to 1 5 m day 1 case 2 or to 10 m day 1 case 3 table 5b shows the resulting p values and trends on xcn when the one factor anova was performed between the three cases p value was less than the significance level of 0 05 in the case of shifting to a groundwater velocity of 10 m day 1 case 3 only therefore the typical field groundwater velocities of 0 1 or 1 5 m day 1 had no statistically significant impact on nzvi travel distance in a homogeneous aquifer table 5a it was only when a very high velocity of 10 m day 1 was simulated was any effect observed groundwater velocity also had no statistically significant impact on nzvi travel in the swiss aquifer even at the highest velocity this may be due to the larger dispersion caused by increased heterogeneity which would obscure the result of the higher groundwater velocity this was also suggested by the relatively low p value p 0 094 table 5a for the swiss xcn showing that heterogeneity may have had an impact on the nzvi distribution to check whether heterogeneity impacts groundwater velocity a two factor anova was necessary which considered the interaction between these factors discussed in section 4 2 the results presented in this study can be seen as contrary to other documented studies for example in some laboratory column studies kanel et al 2007 he et al 2009 phenrat et al 2009 when groundwater velocity or domain velocity was increased higher travel distance was observed however the effect of groundwater velocity from these laboratory studies cannot be compared with this numerical study due to different boundary conditions in laboratory studies constant flow velocity is typically maintained throughout the column which does not change with viscous injection however in this study a constant head boundary condition is used resulting in a change in flow if the fluid viscosity is altered with the injection of cmc nzvi krol et al 2013 these boundary conditions were chosen to represent the field injection conditions and the modelling results and statistical analyses presented herein demonstrate that the impact of higher background groundwater velocity on nzvi transport is not statistically significant within the range of velocities considered it should be noted that impact of higher groundwater velocities may be different if the solution was not a stable suspension assumed here or if detachment of particles was considered 4 1 2 effect of injection concentration fig 6 shows the effect of injection concentration values on transport parameters xcn xnsp zcn znsp and st all the box plots show very little to no change in the median value irrespective of the aquifer when the injection concentration was raised from 0 96 kg m 3 to 2 5 kg m 3 however some slight change was observed when the injection concentration was increased to 96 kg m 3 particularly for the xnsp in the borden aquifer fig 6b and znsp fig 6d and st fig 6e for both aquifers however the anova analysis showed that the p value was less than 0 05 only in the case of nzvi attachment st for both aquifers table 5a therefore the null hypothesis was rejected only for this case in other words the change in nzvi injection concentration had no statistically significant effect on nzvi transport in terms of movement and spread of nzvi in porous media but had a statistically significant effect on nzvi attachment nzvi transport is not impacted by change in injection concentration which is expected as the injection rate was kept constant in these simulations cases 1 4 and 5 in table 3 further investigation showed that the cause of this statistically significant effect of injection concentration on nzvi attachment was true for all three concentration levels tested table 5c for both aquifers this observation is explained by the cft where particle attachment attachment rate coefficient is proportional to particle concentration eq 3 meaning higher aqueous concentrations result in higher nzvi attachment eq 12 a normalized mass injected mass attached mass was calculated and show a similar value for all injection concentration values table s2 suggesting that increasing the injection concentration won t increases the portion of mass attached and therefore injecting more mass may increase the aqueous nzvi concentration this would be even more pronounced if there is detachment or limited adsorption sites not modelled in this study lastly the total attached mass was higher in the swiss aquifer compared to the borden aquifer for identical total injected mass suggesting that heterogeneity increased nzvi attachment 4 1 3 effect of solution viscosity fig 7 shows the effect of solution viscosity on xcn xnsp zcn znsp and st as seen in the figure a change in solution viscosity led to a change in the median in almost all the parameters with the largest difference observed in mass spread in both directions xnsp and znsp and st nzvi attachment the p values concurred with this visual inspection with p 0 05 calculated for nzvi mass spread in both directions xnsp and znsp and nzvi attachment for both aquifers table 5a for all other factors the p values were greater than the significance level of 0 05 and therefore the null hypothesis was not rejected in other words the change in solution viscosity had a statistically significant effect on nzvi transport in terms of nzvi spread and attachment in both media but had no statistically significant effect on nzvi center of mass xcn zcn as mentioned previously injection was modelled as a constant flux boundary condition as such the velocity produced around the borehole remained constant regardless of the injection viscosity this would be different if the injection was gravity fed krol et al 2013 as such the centre of mass would not be affected by the viscous injection but the spread would increase and attachment decrease with increased viscosity due to a decrease in η0 as seen by others krol et al 2013 fig 8a and b show two simulations for borden aquifer at 48 h for case 1 solution viscosity 0 013 pa s and case 6 solution viscosity 0 072 pa s respectively it is clear from fig 8b that although the center of mass is comparable nzvi spread was increased due to change in solution viscosity from 0 013 pa s to 0 072 pa s which was supported by the change in attached mass distribution i e fig 9 in other words nzvi attachment decreased due to the increase in solution viscosity in line with others findings krol et al 2013 li et al 2015 2016 chowdhury et al 2015 4 1 4 effect of injection rate fig 10 shows the effect of injection rate on xcn xnsp zcn znsp and st as seen a change in injection rate led to a change in the median in almost all the parameters with the largest difference observed in mass spread in both directions xnsp and znsp and st nzvi attachment the p values concurred with this visual inspection with p 0 05 calculated for nzvi mass spread in both directions xnsp and znsp and nzvi attachment for both aquifers table 5a for all other factors the p values were greater than the significance level of 0 05 and therefore the null hypothesis was not rejected in other words the change in injection rate had a statistically significant effect on nzvi transport in terms of nzvi spread and attachment in both media but had no statistically significant effect on nzvi center of mass xcn zcn increase in nzvi spread is due to the increased injection rate which leads to constant subsurface velocities around the borehole due to the constant flux injection as opposed to gravity fed systems among the four factors considered groundwater velocity injection concentration solution viscosity and injection rate solution viscosity and injection rate had a statistically significant effect on nzvi spread and attachment while background groundwater velocity and injection concentration did not change the nzvi distribution parameters significantly even at the largest factor magnitude the one exception was the horizontal center of mass in the borden aquifer that was affected by the largest groundwater velocity level which was likely due to the low degree of heterogeneity in the borden aquifer as compared to the swiss aquifer 4 1 5 effect of lag period fig 11 shows the effect of the lag period on xcn xnsp zcn znsp and st while the p values are shown in table 6 as seen in fig 11 varying the lag period led to a change in the median values in almost all the parameters with the largest difference observed in nzvi attachment the p values for all combinations resulted in p 0 05 for attachment st only while all other p value were greater than 0 05 and therefore the null hypothesis was not rejected table 6 in other words the lag period had a statistically significant effect only on the nzvi attachment st less attachment was observed with longer continual injection e g case a compared to other cases e g case b and c longer injection periods case a increased the horizontal spread of the nzvi plume xnsp in both aquifers fig 11b and decreased the attachment fig 11e in addition the higher frequency of lag phases in cases b and c impacted the continuity of the injection period and hence nzvi travel distance it was hypothesized that the addition of a lag phase would increase nzvi attachment due to the lack of injection flux however it was unclear whether it is the duration or timing of the lag phase that would be most disadvantageous to nzvi transport to distinguish between these factors the three cases a b and c were simulated with the same total time for nzvi injection 48 h but at various frequencies table 4 the number of injection periods increased in case b and case c compared to case a but the duration of injection became shorter fig 11 shows that the highest attachment was seen for the case c scenario indicating that the total time of injection was not as important as the longest successive injection period resulting in less attachment than those with shorter injections and shorter lag periods as less nzvi attachment is desired for optimum transport in porous media long injection periods followed by short lag phase if necessary is recommended for nzvi deployment 4 1 6 effect of heterogeneous permeability distribution all simulations were run for two heterogeneous aquifers borden and swiss although the mean permeability correlation length and porosity were the same for both aquifers the swiss aquifer had a higher variance in permeability resulting in a greater contrast in permeability fields aquifer heterogeneity impacted the distribution of parameters as seen from the box plot distributions e g figs 5 6 7 10 and 11 for all cases it was observed that the variance of the parameters i e the center of mass spread and nzvi attachment was higher in the swiss aquifer than in the borden aquifer i e the variance of xcn distribution in the borden aquifer with 0 1 m day 1 groundwater velocity was 0 05 but in the swiss aquifer it was 0 53 to quantify the effect of soil heterogeneity a two factor anova analysis was performed and is described in the next section 4 2 results of two factor anova table 7 shows the results of the two factor anova which was performed to address the inability of the one factor anova in analyzing the impact of permeability variation of the two aquifers and the possible masking effects since the two factor anova is capable of assessing more than one factor of interest the effect of heterogeneity along with another factor either groundwater velocity injection concentration solution viscosity or injection rate as well as the interaction of these factors was examined table 7 shows the effect of heterogeneity on nzvi transport parameters using two factor anova heterogeneity had a statistically significant effect on almost all parameters xcn xnsp znsp and st in addition the predicted masking effect by heterogeneity described in subsection 4 1 1 for which no statistically significant impact was seen on xcn in the swiss aquifer through the one factor anova was confirmed statistically significant effect of heterogeneity on xcn and xnsp was seen when two factor anova was carried out among cases 1 and 4 this was not captured by the one factor anova although it can be seen in the box plots fig 6a and b the two factor anova quantified these results and indicated that the increased variance of soil permeability in the swiss aquifer obscured the effect of injection concentration on horizontal mass transport p values close to the significance level of 0 05 were observed for znsp and st heterogeneity also had a statistically significant effect on xnsp znsp and st when two factor anova was performed among cases 1 and 6 significant interaction in p values indicate that the effect of one factor depends on the level of the second factor montgomery and runger 2007 in this case statistically significant interaction was not observed between soil heterogeneity and any of the factors however value close to 0 05 was seen for interaction between st and solution viscosity this implies that the effect of solution viscosity on the mass parameters had greater impact than the level of soil heterogeneity the result of two factor anova indicates that heterogeneity of subsurface aquifer is an important parameter for nzvi deployment sufficient knowledge of heterogeneity and possible effects should be considered for field scale implementation of nzvi technology this can be seen in fig 12 where the heterogeneity of swiss aquifer impacts the nzvi distribution compared to the borden aquifer the difference in the variance of permeability fields resulted in the visibly different nzvi distribution in the two aquifers with greater variability seen in the swiss aquifer nzvi movement in the borden aquifer fig 12a seemed to be confined around the injection well while nzvi appeared to be more dispersed in the swiss aquifer fig 12b with nzvi following a preferential flow path as outlined by the high permeability zone seen in fig 3b both figures show the same permeability realization 5 conclusions this work aimed to answer some key questions regarding nzvi deployment in the field during field scale injection maintaining a nzvi injection flux deciding on which cmc nzvi solution viscosity to use or the inclusion of a lag period purposely or not are key challenges for engineers and decision makers in addition understanding the inherent groundwater velocity conditions of a site is often considered important this work focused on finding the statistically significant factors for nzvi deployment to aid in field scale injection while a large body of work by others has focused on finding the optimum injection concentration or understanding how groundwater velocity will affect treatment this study shows that neither of these factors has a statistically significant impact on nzvi travel distance or spread therefore maintaining a hydraulic gradient and associated groundwater velocity during field injection is not an important consideration as groundwater velocity does not change nzvi travel distance by a statistically significant amount injection rate which can alter the groundwater velocity around the well field does play an important role and was shown to be a statistically significant factor although some studies reported an effect of various parameters on nzvi travel distance the significance of the effects have never statistically been quantified and therefore not conclusive the study presented here quantified the 2d nzvi transport model results using one and two factor anova to examine the factors that have the greatest impact on nzvi travel distance the 2d model considered randomly distributed soil permeability fields of two aquifers where one aquifer was more heterogeneous than the other the one factor anova was performed considering five factors background groundwater velocity injection concentration lag phase solution viscosity of the injected solution and injection rate the magnitude of these five factors was varied and various simulations performed in addition the two factor anova was performed to understand the impact of soil heterogeneity this research showed that field groundwater velocity background aquifer velocity had no statistically significant effect on nzvi movement and spread except on the horizontal center of mass in the borden aquifer under a large groundwater velocity domain therefore typical field groundwater velocities differed by change in hydraulic gradients do not significantly impact nzvi travel distance similarly nzvi injection concentration keeping injection rate constant was shown not to significantly change the nzvi distribution parameters i e movement and spread even with a large concentration magnitude however large injection concentration caused higher nzvi attachment near the well head injection solution viscosity and injection rate had a statistically significant effect on nzvi mass spread and attachment in both aquifers but not on the nzvi center of mass a higher solution viscosity was found to maximize nzvi spread and minimize nzvi attachment this analysis showed that solution viscosity is an important factor for optimizing nzvi deployment at the field scale stable nzvi suspension in the presence of highly viscous polymer with optimum pressure containment should be used during field scale injection to ensure optimum nzvi spread which could result in reducing remediation time the analysis also showed that injection rate is an important factor for optimizing nzvi deployment at the field scale as injection rate was found to maximize nzvi spread when applied at a constant pressure the length of the injection and lag period are also important factors for optimizing nzvi deployment at the field scale in this work three types of lag periods were considered in a 96 h timeframe less nzvi attachment occurred during a long injection phase followed by a long lag period 48 h compared to shorter injection and lag periods 24 h and 12 h even though the total injection time was the same it is therefore recommended that long injection and short lag phase be used during nzvi deployment at the field scale which will result in less nzvi attachment lastly aquifer heterogeneity was investigated and was found to effect nzvi distribution while the one factor anova showed higher parameter variance i e the center of mass spread and nzvi attachment in the swiss aquifer the two factor anova analysis was performed to quantify these results this analysis showed that heterogeneity had a statistically significant effect on nzvi spread parameters in addition the interaction between the factors and heterogeneity was not found to be statistically significant which indicates that heterogeneity did not obscure the effect of injection concentration solution viscosity and injection rate on nzvi attachment future studies should include additional retention mechanism such as straining and mechanical filtration which may result in greater effect of soil heterogeneity on nzvi transport declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by graduate support by york university toronto canada and the lassonde graduate entrance scholarship appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103870 
1357,subsurface remediation using nanoscale zero valent iron nzvi is a promising in situ technology that can transform certain groundwater contaminants into non toxic compounds however field scale implementation of nzvi technology has faced major challenges due to poor subsurface mobility limited longevity and well clogging all leading to a shorter nzvi travel distance this distance nzvi travels in the subsurface is an important parameter since it influences the amount of contaminants that can be reached and thereby remediated there are several factors which may affect nzvi travel distance such as groundwater velocity injection concentration and rate lag period duration when nzvi injection is stopped solution viscosity and subsurface heterogeneity although various studies have been performed to reveal the effect of different factors on nzvi transport in homogeneous domains few studies have focused on heterogeneous media which is more representative of field conditions in this study a statistical analysis was performed using a two dimensional numerical model which simulated carboxymethyl cellulose cmc stabilized nzvi transport in randomly distributed soil permeability fields of two aquifers to examine the factors that have the greatest impact on nzvi travel distance among all possible factors field scale solution viscosity and injection rate had a statistically significant effect on nzvi travel distance in both the horizontal and vertical directions as well as on the attached mass additionally the lag period between injections had a statistically significant effect on the attached mass but not the travel distance these results suggest that having a long injection period followed by a short lag phase during field deployment may result in less nzvi attachment lastly aquifer heterogeneity impacted the nzvi spread while the impact of intrinsic groundwater velocity and injection concentration was found not to be statistically significant results from this numerical study can aid in field scale cmc nzvi injection by identifying key factors for remediation optimization keywords groundwater remediation soil heterogeneity nanoscale zero valent iron field scale colloid transport statistical analysis glossary symbol a hamaker constant j af anisotropy factor as porosity dependent parameter c aqueous nzvi or polymer concentration mol m 3 c c i relative nzvi concentration ci nzvi injection concentration kg m 3 cr chromium db bulk diffusion coefficient m2 s 1 dc collector soil grain diameter m ddisp effective hydrodynamic dispersion tensor m2 s 1 dfn diffusion coefficient of nzvi m2 s 1 dfp diffusion coefficient of polymer m2 s 1 dm incremental portion of the nzvi mass dp diameter of nzvi m f statistical fisher test fe0 zero valent iron fn nzvi flux mol m 2 s 1 fp polymer flux mol m 2 s 1 h0 null hypothesis hi height of injection well m ir injection rate m3 s 1 iz m second moment of the mass about the z axis m mol k boltzman constant j k 1 katt attachment rate coefficient s 1 in colloid filtration theory kp permeability of the aquifer porous media m2 m total mass of nzvi mol m 3 mf mass flux kg m 2 s 1 mwn molecular weight of nzvi kg mol 1 mwp molecular weight of cmc polymer kg mol 1 n vector normal to the boundary na attraction number ng gravity number npe peclet number nr aspect ratio nvdw van der wall s number ni nickel pb lead pt percentage of polymer in total solution q darcy velocity m s 1 qm sink source kg m 3s 1 ri radius of injection well m s attached concentration mol kg 1 st total attachment mol kg 1 t time s t temperature k v groundwater pore velocity or background aquifer velocity referred to as the groundwater velocity m s 1 x distance in x direction m xh horizontal correlation length m xij value of random variable at jth observation under ith factor xijk value of the random variable at ijth observation under kth replicate xcn horizontal center of mass m xnsp spreading of the mass in horizontal direction m xp mole fraction cmc polymer mol of polymer mol of water zcn vertical center of mass m znsp spreading of the mass in vertical direction m zv vertical correlation length m θ porosity α attachment efficiency β effect of factor b ϵij ϵijk random error component η0 single collector contact efficiency μ mean μp polymer viscosity pa s μsol solution viscosity pa s μwf water viscosity pa s σ2 variance ρf fluid density average density of nzvi suspension kg m 3 ρb density of porous media kg m 3 τi factor effect effect of factor a in two factor anova τβ effect of interaction between a and b h hydraulic gradient m m 1 abbreviations 2d two dimensional 3d three dimensional anova analysis of variance bc boundary condition cf constant flux cft colloid filtration theory ch constant head c2h6 ethane cmc carboxymethyl cellulose compsim 3d finite difference three phase simulator comsol a finite element based multiphysics simulation software dl darcy s law module nzvi nanoscale zero valent iron orp oxidation reduction potential pce c2cl4 tetrachloroethylene ppt 1 push and pull test 1 ss suspended solid tds transport of diluted species module ts total solids 1 introduction groundwater contamination can occur when surface pollutants make their way to underlying aquifers this can lead to environmental damage and potential health issues if such aquifers are used as drinking water sources harr 1996 remediation of such aquifers can involve ex situ technologies which are typically costly and time consuming or in situ methods which can be ineffective the most promising technologies target the source zones since decreasing the source of the contamination has proven to be the most effective way of reducing contamination kueper et al 2014 remediation using nanoscale zerovalent iron nzvi is one promising in situ technology that can remediate groundwater aquifers contaminated with chlorinated solvents or heavy metals and can be implemented at the source zone gillham and o hannesin 1994 liu et al 2005 zhan et al 2008 barnes et al 2010 sakulchaicharoen et al 2010 zhou et al 2011 o carroll et al 2013 mondal et al 2018 li et al 2019 qian et al 2020 nzvi remediation works by having the zero valent iron fe0 release electrons when it is oxidized when injected into the subsurface typically in the source zone these electrons reduce groundwater contaminants through reductive dechlorination converting them into non toxic compounds wang and zhang 1997 li et al 2006 o carroll et al 2013 for example tetrachloroethylene pce c2cl4 can be reduced to ethane c2h6 which poses little toxicological risk li et al 2006 in addition inorganic contaminants such as hexavalent chromium cr6 divalent nickel ni2 and divalent lead pb2 can be reduced by nzvi to less toxic states as cr3 ni0 and pb0 respectively o carroll et al 2013 despite its promising characteristics field scale implementation of nzvi technology has faced major challenges due to poor subsurface mobility limited longevity farrell et al 2000 phenrat et al 2007 kocur et al 2013 2014 velimirovic et al 2014 liu et al 2019 and well clogging elliott and zhang 2001 all leading to a smaller travel distances although nzvi mobility is the focus of most remediation studies due to the importance of the particles reaching the contaminants most field and lab applications report moderate mobility ranging from 1 to 6 m elliott and zhang 2001 zhang 2003 henn and waddill 2006 kocur et al 2014 köber et al 2014 nanorem final report 2017 nunez garcia et al 2020 in addition the reported nzvi travel distance is often measured by visual sightings of nzvi henn and waddill 2006 examining contaminant concentration downgradient of the injection well or by indirect measurements such as change in oxidation reduction potential orp suspended solid ss total solid ts analysis etc wei et al 2010 chowdhury et al 2015 a larger travel distance is preferable since it can reduce the total time of remediation and provide a cost effective solution there are various factors that can affect nzvi travel distance including rapid aggregation settling nzvi injection method groundwater velocity injection concentration injection rate lag phase solution viscosity and subsurface heterogeneity these effects will be briefly described herein and further information may be obtained from the cited sources aggregation may occur due to particle particle interaction petosa et al 2010 and settling occurs due to gravitational forces phenrat et al 2007 these factors affect nzvi travel distance by reducing the amount of nzvi particles available for transport o carroll et al 2013 raychoudhury et al 2014 kocur 2015 ling et al 2021 aggregation can be minimized by coating the particles with polymers such as carboxymethyl cellulose cmc guar gum xanthan gum and starch tiraferri and sethi 2008 vecchia et al 2009 wang et al 2014 ji et al 2019 wu et al 2021 the nzvi injection method may also influence nzvi travel distance different types of injection strategies can be used for nzvi remediation these include pneumatic injection direct injection pressure pulse injection and gravity feed injection su et al 2013 kocur et al 2014 among these strategies pneumatic injection which uses high pressures can create fracture networks that result in preferential flow paths that can facilitate nzvi transport navfac 2013 which cannot be modelled by advection dispersion deposition theory while gravity feed injection generally maintains a constant head ch at the injection well other injection methods depend on injecting a flux in the porous media krol et al 2013 considered both constant flux cf and ch injections from field injection data from bennett et al 2010 in a three dimensional 3d model and found that a higher travel distance can be obtained using cf injection kocur et al 2014 conducted a field scale test of nzvi injection in a contaminated sandy subsurface and found that field nzvi travel distance and longevity can be achieved with very high groundwater velocities and highly stable nzvi suspensions in addition according to henn and waddill 2006 high groundwater velocity and change in groundwater flow direction due to recirculation made it possible to distribute the nzvi suspension over the treatment area many others have found similar results chowdhury et al 2015 mondal et al 2018 injection mass flux is proportional to injection concentration and injection rate chowdhury et al 2015 predicted that injecting a higher volume of nzvi suspension resulted in an increase in nzvi travel distance additionally krol et al 2013 showed nzvi travel distance is affected by injection type ch or cf and thereby injection rate others also found that higher injection rate 2 78 10 5 m3 s 1 vs 1 38 10 4 m3 s 1 resulted in higher nzvi travel distance nanorem final report 2017 lastly lag phase i e the time when there is no injection during successive injection of nzvi can also lead to nzvi deposition in porous media under low groundwater velocity conditions bennett et al 2010 kocur et al 2014 and make nzvi immobile krol et al 2013 nzvi travel distance can also be influenced by the solution viscosity krol et al 2013 chowdhury et al 2015 mondal et al 2018 increasing the nzvi solution viscosity can result in greater nzvi stability nanorem wp4 2016 nanorem final report 2017 this can be done by the addition of polymers such as cmc li et al 2015 cohen and weisbrod 2018 li et al 2016 observed greater travel efficiency 38 nzvi mass removed from the outlet of the experimental sand tank by adding 2000 g m 3 cmc compared to 1000 g m 3 where only 11 mass removal rates were seen at a fixed nzvi concentration however increasing solution viscosity can have an opposing effect on nzvi travel distance since increased viscosity decreases hydraulic conductivity lowering the travel velocity as hydraulic conductivity is inversely proportional to viscosity and hence the travel distance mondal et al 2018 lastly subsurface heterogeneity can affect nzvi travel distance during various field injections subsurface heterogeneity was found to strongly hinder nzvi transport as it moved preferentially in the high permeability areas kocur et al 2014 and prevented sufficient nzvi distribution henn and waddill 2006 the preferential flow caused by subsurface heterogeneity can negatively impact the remediation goals as nzvi may not reach the target contaminant he et al 2010 velimirovic et al 2014 in addition aggregation is influenced by soil particle size li et al 2016 which varies in a heterogeneous domain as aquifers are heterogeneous in nature modelling studies should incorporate heterogeneity to investigate its effect and understand the possible effects there are a few modelling studies that simulate soil heterogeneity cullen et al 2010 simulated the transport of carbon nanoparticles in a heterogeneous permeability field and opined that less movement of nanoparticles occurs in a heterogeneous media due to more retention caused by variable soil particle size and groundwater velocities compared to a homogeneous media they suggested that nanoparticle mobility can be overestimated if a heterogeneous aquifer is assumed to be homogeneous while strutz et al 2016 predicated that nzvi particles will be retained in high permeability areas within a heterogeneous system most recently mondal et al 2018 observed that injected nzvi was variably distributed even in an experiment consisting of sand packed as uniformly as possible indicating that heterogeneous effects can be seen even in systems that are considered homogeneous although these studies have investigated the effect of some factors on nanoparticle transport using laboratory and numerical studies no studies have investigated whether the impact of several factors on nzvi transport is statistically significant in heterogeneous aquifers representing in situ conditions previous studies have compared system parameters and nzvi transport behaviors but often the comparison is inconsistent due to changing parameters one thing that is common among all the studies is the agreement that optimum travel distance for successful remediation has not yet been achieved this paper outlines a computational model that can simulate nzvi transport and a method of using one and two factor analysis of variance anova to examine the effect of several factors on cmc stabilized nzvi travel distance one factor anova was performed on five factors groundwater velocity injection concentration injection rate lag phase and viscosity of the injected solution for two aquifers of variable heterogeneity these five factors can be independently controlled and do not depend on permeability or soil collector diameter in addition two factor anova was performed to understand the impact of soil heterogeneity results from this research enable a better understanding of which field scale transport factors are most important for effective remediation 2 material and methods comsol multiphysics a finite elements program can be used to simulate physics based problems and various types of physical phenomena such as fluid flow heat transfer and pore scale flow comsol user s guide 2017 various types of equations are built into comsol modules with appropriate characteristic equations and fundamental features these modules can be linked together as well as modified with additional user defined equations comsol user s guide 2017 for this work the transport of diluted species tds module and the darcy s law dl module were used as the two modules were capable of simulating nanoparticle transport in porous media reactive transport of nzvi e g reaction of nzvi with contaminants was not considered in this model 2 1 3d homogeneous nzvi transport model to model and validate cmc nzvi transport data from krol et al 2013 was used krol et al 2013 simulated a field study bennett et al 2010 in which cmc coated nzvi prepared on site was injected into a shallow granulated aquifer consisting of a coarse grained deposit krol et al 2013 used compsim a 3d finite difference three phase simulator developed by sleep and sykes 1993 krol et al 2013 showed that the compsim model was able to accurately predict the field study therefore the results of krol et al 2013 was used to validate the comsol model developed in this study to ensure that the cmc nzvi transport was correctly implemented to simulate the krol et al 2013 domain the same parameters from the push and pull test ppt 1 in the sand layer fig 1b were used and are summarized in table 1 these parameters represent the subsurface properties as well as the nzvi and polymer characteristics details can be found in krol et al 2013 and bennett et al 2010 2 1 1 numerical approach the comsol dl module defines the transient flow equation as follows 1 ρ f θ t ρ f q q m where q is the darcy velocity m s 1 q θv where v is the groundwater pore velocity or background aquifer velocity referred to as the groundwater velocity m s 1 θ is the porosity ρf is the fluid density kg m 3 and qm is the source sink term kg m 3 s 1 the 3d darcy velocity field created in the dl module is transferred to the tds module where the transport of polymer coated nanoparticles in a porous media is described by eq 2 2 c t θ q c x q c y q c z d disp 2 c x 2 θ d disp 2 c y 2 θ d disp 2 c z 2 θ θ k att c where c is the aqueous nzvi or polymer concentration mol m 3 at a distance of x m and time t s and ddisp is the effective hydrodynamic dispersion tensor m2 s 1 which is estimated using effective diffusion coefficient dispersivity and groundwater velocity the attachment rate coefficient katt s 1 is defined by the colloid filtration theory cft tufenkji and elimelech 2004a and expressed by eq 3 this term is only applied to nzvi transport since the polymer cmc is considered to be a conservative i e non reactive species and therefore the attachment rate for the polymer is assumed to be zero 3 k att 3 1 θ α η 0 v 2 d c where α is the attachment efficiency coefficient defined as the ratio of particles that stick to the collector to those that strike the collector η0 is the collector contact efficiency defined as the ratio of colloids hitting the collector to those approaching the collector dc is the collector soil grain diameter m the attachment efficiency coefficient α was assumed to be 0 02 based on a validation analysis performed by krol et al 2013 while η0 is a function of various parameters based on tufenkji and elimelech 2004a and is calculated in the model details of the η0 calculation can be found in the supplementary information si cft was chosen to model nzvi transport since it has been shown to accurately capture transport where little agglomeration or detachment is observed in addition cft is applicable for sufficiently low particle concentration where particle particle interactions e g aggregation ripening and blocking are reduced tufenkji and elimelech 2004b such is the case for the cmc nzvi used in this study and observed by others phenrat et al 2009 cullen et al 2010 bennett et al 2010 kocur et al 2013 krol et al 2013 molnar et al 2015 chowdhury et al 2015 li et al 2016 mondal et al 2018 in particular this model was used to accurately predict the field transport of cmc coated nzvi particles by several authors phenrat et al 2009 he et al 2010 lin et al 2010 bennett et al 2010 raychoudhury et al 2010 kocur et al 2013 krol et al 2013 li et al 2015 2016 chowdhury et al 2015 saberinasr et al 2016 mondal et al 2018 given that the injections are cmc stabilized and are injected into a highly porous medium the simulated domain represented a remediation scenario with a cylindrical injection well defined in the middle of the domain which injected a cmc coated nzvi suspension into the subsurface the domain was modelled using a symmetrical boundary condition bc along the xz plane through the injection well to minimize computational time fig 1a shows the bcs and the characteristics of half of the 3d modelled domain for a sand layer where cmc nzvi solution was injected as described by bennett et al 2010 the initial nzvi cmc concentration was defined as zero in the domain and the direction of groundwater flow was simulated from left to right through the yz plane defined by the constant hydraulic head bc no flow and no flux bc were defined at the upper and lower boundaries the xy plane as well as the side boundaries the xz plane the injection well was used for defining a water mass rate m3 s 1 in the dl module and a nzvi and polymer constant flux mol m 2 s 1 in the tds module to calculate the solution viscosity changing with time and space the grunberg and nissan equation grunberg and nissan 1949 was used which relates solution viscosity with component mole fractions and viscosities as follows 4 log μ sol x p log μ p x w log μ wf xp and xw are mole fractions temporally and spatially changing of cmc polymer and water respectively krol et al 2013 performed several viscosity experiments using cmc 90 k 90 000 g mol 1 see krol et al 2013 for details and obtained the polymer viscosity μp using a fitting procedure the resulting polymer viscosity value was used as a constant in the model to calculate the temporally and spatially changing solution viscosity based on polymer concentration although non newtonian behaviour of cmc solutions have been recorded benchabane and karim 2008 others have found this phenomena to be insignificant given the range of velocities experienced in the subsurface and in this study sochi 2007 yang and zhu 2007 balavi 2017 therefore non newtonian behaviour was not explored in this paper 2 1 2 verification of model results the comsol model developed in this study was verified using the results from the compsim model presented in krol et al 2013 and is shown in the si figs s1 and s2 compared relative nzvi concentration and solution viscosity of the two models after 1 and 20 h of constant injection the figures showed a very good match for nzvi concentration and solution viscosity for both comsol and compsim models in addition to this qualitative comparison the difference between the values of the nzvi concentration and viscosity of the two models was calculated using root mean squared table s1 the results showed that good agreement was achieved between the two models these results verified that cmc nzvi transport model was correctly implemented in comsol unlike the compsim model that assumed uniform grid size the comsol model considered a finer mesh around complex geometry the injection well and a progressively larger mesh away from the injection well fig s3 to rule out mesh dependencies a mesh convergence study was performed examining both maximum and minimum grid size as seen in fig s4 maximum nzvi concentration did not vary below a minimum element size of 0 003 m and below a maximum element size of 0 4 m therefore the domain was meshed with minimum element size of 0 001 m and maximum size of 0 4 m to obtain consistent results and lower computational time fig s4 the 3d homogeneous nzvi transport model considered an average aquifer permeability where the soil grain collector size was fixed however in a heterogeneous aquifer permeability as well as collector size varies therefore in the next section the development of a two dimensional 2d heterogeneous nzvi transport model is described which was used to examine the effect of soil heterogeneity on nzvi optimization a 2d nzvi transport model was developed instead of a 3d version due to the availability of 2d heterogeneous permeability distribution data in addition the results of two traverse directions in the case of a 3d model would be similar 2 2 2d heterogeneous nzvi transport model the 2d model developed to study heterogeneous effects was identical to the 3d model described in the above section however the kozeny carman equation kozeny 1927 carman 1937 1956 was included which linked the heterogeneous permeability field to the cft equation eq 3 eq 5 describes this relationship between the collector diameter dc m and the soil permeability kp m2 5 d c k p 1 θ 2 180 θ 3 the collector diameter was also variable when calculating the single collector contact efficiency η0 which resulted in a spatially varied η0 in the heterogeneous model details of the η0 calculation can be found in the si fig 2 represents the bcs and characteristics of a full 2d domain that represents a vertical soil profile of the subsurface an injection well was placed in the middle of the domain which injected the cmc coated nzvi suspension into the subsurface and the initial cmc nzvi concentration was assumed to be zero in the domain the groundwater direction was defined from left to right using a constant hydraulic head bc while no flow and no flux bcs were defined at the upper and lower boundaries 2 2 1 aquifer properties to examine the effect of heterogeneity on nzvi transport two aquifers of variable heterogeneity were used in the simulations the borden aquifer was chosen for this study as it is relatively homogeneous and well characterized sudicky 1986 dekker and abriola 2000 the swiss aquifer was chosen as it is more heterogeneous compared to borden aquifer due to a higher variance in permeability jussel et al 1994 dekker and abriola 2000 fig 3 shows a typical permeability distribution of these two aquifers the permeability fields fig 3 were generated using the field generator in pmwin processing modflow 5 3 which used mejia s algorithm mejía and rodríguez iturbe 1974 frenzel 1995 the swiss aquifer had a higher permeability variance σ2 0 43 than borden aquifer σ2 0 21 resulting in a greater difference in permeability while the mean permeability correlation length and porosity were same for both aquifers table 2 the generated permeability fields were imported into comsol using the interpolation function which allowed for the data to be directly inputted without changing the discretization one hundred permeability realizations for the two aquifers i e borden and swiss with different heterogeneities were generated using the field generator through a variance analysis of nzvi transport parameters fig s5 it was found that convergence of variance occurred within 15 realizations out of 100 where the values were chosen at random to perform the analysis consequently 15 realizations were deemed representative of the aquifer variability while reducing simulation time 3 calculations simulations 3 1 simulated cases for sensitivity analysis to examine the effect of various factors on nzvi transport seven cases were established table 3 each case had a different specified groundwater velocity injection viscosity i e solution viscosity injection concentration or injection rate case 1 was considered the base case and was used with each simulation group cases 2 and 3 had different groundwater velocities cases 4 and 5 had various nzvi concentration values injected case 6 had a different injection solution viscosity and case 7 had a different injection rate for all the cases the other factors were kept constant varying only one factor at a time the values of groundwater velocities were chosen for cases 1 and 2 based on data found from field injection studies table 3 these groundwater velocities were estimated during field nzvi injection in a shallow granulated aquifer by bennett et al 2010 and used in a modelling study by krol et al 2013 case 3 used a groundwater velocity of 10 m day 1 which was 100 times greater than the groundwater velocity used in case 1 this value was used to provide an upper bound to the simulations and corresponds to a very fast moving gravel sand aquifer the nzvi injection concentration values were also chosen from the field injection data case 1 had a injection concentration of 0 96 kg m 3 nzvi corresponding to a flux value of 0 009 mol m 2 s 1 krol et al 2013 chowdhury et al 2015 and case 4 accounted for an increased injection concentration of 2 5 kg m 3 nzvi kocur et al 2013 case 5 used an injection concentration of 96 kg m 3 which was 100 times greater than the injection concentration used in case 1 this represented an upper bound of the concentration chosen to observe the sensitivity of injection concentration on nzvi transport cmc nzvi solutions are typically injected at two solution viscosity values 0 013 and 0 072 pa s corresponding to the molecular weight of the polymer 90 000 g mol 1 cmc 90 k and 250 000 g mol 1 cmc 250 k respectively both of these viscosity values were used by krol et al 2013 in their modelling study while cmc 90 k was used by he et al 2009 and raychoudhury et al 2012 in their experimental investigations cmc 250 k was used to stabilize nzvi suspension by others he and zhao 2007 sakulchaicharoen et al 2010 due to their use in modelling and experimental studies these two viscosity values were used herein the nzvi injection rates were chosen from the field injection data case 1 had a injection flowrate of 5 67e 5 m3 s 1 3 4 l min 1 bennett et al 2010 krol et al 2013 and case 7 accounted for a reduced injection rate of 2e 5 m3 s 1 1 2 l min 1 bennett et al 2010 krol et al 2013 to evaluate the effect of groundwater velocity cases 1 2 and 3 were simulated and compared to examine the effect of injection concentration cases 1 4 and 5 were simulated and compared to examine the effect of injection viscosity cases 1 and 6 were simulated and compared while the effect of injection rate was done by simulating and comparing cases 1 and 7 lastly the lag phase is the interval between two successive nzvi injections during this phase nzvi injection flux is zero and injected nzvi mass from the previous injection may be affected by the reduced velocity caused by the injection stoppage nzvi deposition into porous media can occur during the lag phase under low velocity conditions bennett et al 2010 kocur et al 2014 and nzvi may become immobile krol et al 2013 table 4 presents three simulated cases considering a lag phase all with a total simulation time of 96 h case a base case considered 48 h of constant flux nzvi injection followed by a 48 h of zero flux or lag phase case b considered intermittent nzvi injection of 24 h and lag phase of 24 h for a total of 96 h and case c considered successive constant flux nzvi injection of 12 h and lag phase of 12 h for a total of 96 h case a had the longest continual injection phase 48 h and lag phase 48 h compared to case b and c however in total all cases had the same hours of injection and lag phase 48 h of each therefore in each case the same mass of total nzvi was injected 0 96 kg m 3 with an injection rate of 5 67e 5 m3 s 1 in addition the groundwater velocity before and during lag period for all the cases was kept the same 0 1 m day 1 and the viscosity of the injected solution was maintained at 0 013 pa s 3 2 one factor analysis of variance the one factor anova is a statistical method that calculates the impact of one factor the one factor anova can be represented as follows montgomery and runger 2007 6 x ij μ τ i ϵ ij where xij is the value of random variable at jth observation under ith factor μ is the overall mean τi is the factor effect and ϵij is the random error component montgomery and runger 2007 in this study xij was the random variable which defined nzvi transport distance or spread while the factors τi were groundwater velocity and injection concentration and rate and solution viscosity the analysis assumed that the observations are done in a random order and the effect of a factor was quantified by the null hypothesis 3 2 1 null hypothesis the null hypothesis h0 can be written as follows and states that the means μ are statistically equal montgomery and runger 2007 7 h 0 μ 1 μ 2 to test this hypothesis the fisher f test was selected since it is suitable for relatively small sample sizes 15 in this research for each permeability field used fisher 1922 1954 to check the null hypothesis a statistically significant level of p 0 05 was used in other words if the p value 0 05 then the null hypothesis was not rejected and therefore the factor had no significant effect on the parameter if the p value 0 05 then the null hypothesis was rejected meaning that the means being compared were statistically not equal and thus the factor had a statistically significant effect on the parameter 3 3 two factor analysis of variance the two factor anova can evaluate the individual and combined effect of more than one factor it is particularly useful for a system where a factor may obscure the effect of others on a phenomenon the two factor anova can be represented as follows montgomery and runger 2007 8 x ijk μ τ i β j τβ ij ϵ ijk where xijk is the value of the random variable at ijth observation under kth replicate μ is the overall mean τi is the effect of factor a β is the effect of factor b τβ is the effect of interaction between a and b and ϵijk is the random error component montgomery and runger 2007 during the two factor anova the null hypothesis described in subsection 3 2 1 was used it calculated the effect of three factors and identified whether there is any effect on the mean due to change in factor a factor b and interaction of a and b the latter described a masking effect whereby the effect of one factor became different at different scales of other factors and a large interaction could obscure the main effect krol 2011 3 4 estimation of nzvi center of mass and spread in order to assess the effect of the different factors on nzvi transport in aquifers of various heterogeneity and to quantify the movement and shape of the mass distributions various mass parameters were used these included the center of mass and mass spread in a 2d domain which were considered in two directions i e x and z directions and are shown in fig 4 these parameters were based on the study of dekker and abriola 2000 and krol 2011 and modified for this study the horizontal center of mass xcn of the system was calculated as follows 9 xcn 1 m xdm where m is the total nzvi mass x is the distance in the x direction and dm is the incremental portion of the nzvi mass the spreading of the mass xnsp was determined by the radius of gyration which was determined as follows dekker and abriola 2000 10 xnsp i z m m where iz m is the second moment of the mass about the z axis and was defined as 11 i z m xcn x 2 dm where xcn x is the distance of spread from the z axis passing through the center of nzvi mass the vertical mass parameters i e center of nzvi mass in the z direction zcn and mass spread in the z direction znsp were calculated in a similar manner the total attachment st mol kg 1 in the heterogeneous porous media due to nzvi injection was also evaluated 12 s t s θ k att c ρ b dt where ρb is the density of porous media kg m 3 and s is the attached concentration mol kg 1 4 results and discussion 4 1 results of one factor anova the one factor anova was performed on both aquifers by changing the values of groundwater velocity injection concentration solution viscosity and injection rate as outlined in table 3 to examine the effect of groundwater velocity cases 1 2 and 3 were simulated for 15 permeability realizations similarly to find out the effect of injection concentration cases 1 4 and 5 were used to find out the effect of solution viscosity cases 1 and 6 were used and cases 1 and 7 were used to examine the effect of injection rate one factor anova was also performed on both aquifers considering lag periods as outlined in table 4 tables 5a 5b 5c and 6 show the results of one factor anova since the one factor anova did not distinguish where the effect occurs i e between the first and second level or between the second or third level additional analysis was performed to identify at what level the effect is significant shown in tables 5b and 5c 4 1 1 effect of groundwater velocity fig 5 shows the effect of the groundwater velocity on xcn xnsp zcn znsp and st using boxplots the median of the boxplot is represented by a central mark while the upper and lower edges of the boxplot represent the upper and lower quartiles respectively the tails of the distributions are represented by whiskers of the boxplot as seen in fig 5 there was very little or no change in median irrespective of the aquifer for groundwater velocity ranging from 0 1 m day 1 to 1 5 m day 1 however changes were observed when the groundwater velocity was increased to 10 m day 1 particularly in case of xcn xnsp and st however the p value was less than the significance level of p 0 05 for the center of mass of the borden aquifer only table 5a therefore the null hypothesis was rejected for this case only for all other distributions the p value ranged between 0 094 and 0 998 which was greater than the significance level and therefore the null hypothesis was not rejected in other words the change in groundwater velocity for the chosen range had no statistically significant effect on nzvi transport in terms of movement spread and nzvi attachment onto porous media except for nzvi mass movement in the x direction for the borden aquifer further investigation was done to find out at what scale this effect was statistically significant whether it was due to shifting to a groundwater velocity to 1 5 m day 1 case 2 or to 10 m day 1 case 3 table 5b shows the resulting p values and trends on xcn when the one factor anova was performed between the three cases p value was less than the significance level of 0 05 in the case of shifting to a groundwater velocity of 10 m day 1 case 3 only therefore the typical field groundwater velocities of 0 1 or 1 5 m day 1 had no statistically significant impact on nzvi travel distance in a homogeneous aquifer table 5a it was only when a very high velocity of 10 m day 1 was simulated was any effect observed groundwater velocity also had no statistically significant impact on nzvi travel in the swiss aquifer even at the highest velocity this may be due to the larger dispersion caused by increased heterogeneity which would obscure the result of the higher groundwater velocity this was also suggested by the relatively low p value p 0 094 table 5a for the swiss xcn showing that heterogeneity may have had an impact on the nzvi distribution to check whether heterogeneity impacts groundwater velocity a two factor anova was necessary which considered the interaction between these factors discussed in section 4 2 the results presented in this study can be seen as contrary to other documented studies for example in some laboratory column studies kanel et al 2007 he et al 2009 phenrat et al 2009 when groundwater velocity or domain velocity was increased higher travel distance was observed however the effect of groundwater velocity from these laboratory studies cannot be compared with this numerical study due to different boundary conditions in laboratory studies constant flow velocity is typically maintained throughout the column which does not change with viscous injection however in this study a constant head boundary condition is used resulting in a change in flow if the fluid viscosity is altered with the injection of cmc nzvi krol et al 2013 these boundary conditions were chosen to represent the field injection conditions and the modelling results and statistical analyses presented herein demonstrate that the impact of higher background groundwater velocity on nzvi transport is not statistically significant within the range of velocities considered it should be noted that impact of higher groundwater velocities may be different if the solution was not a stable suspension assumed here or if detachment of particles was considered 4 1 2 effect of injection concentration fig 6 shows the effect of injection concentration values on transport parameters xcn xnsp zcn znsp and st all the box plots show very little to no change in the median value irrespective of the aquifer when the injection concentration was raised from 0 96 kg m 3 to 2 5 kg m 3 however some slight change was observed when the injection concentration was increased to 96 kg m 3 particularly for the xnsp in the borden aquifer fig 6b and znsp fig 6d and st fig 6e for both aquifers however the anova analysis showed that the p value was less than 0 05 only in the case of nzvi attachment st for both aquifers table 5a therefore the null hypothesis was rejected only for this case in other words the change in nzvi injection concentration had no statistically significant effect on nzvi transport in terms of movement and spread of nzvi in porous media but had a statistically significant effect on nzvi attachment nzvi transport is not impacted by change in injection concentration which is expected as the injection rate was kept constant in these simulations cases 1 4 and 5 in table 3 further investigation showed that the cause of this statistically significant effect of injection concentration on nzvi attachment was true for all three concentration levels tested table 5c for both aquifers this observation is explained by the cft where particle attachment attachment rate coefficient is proportional to particle concentration eq 3 meaning higher aqueous concentrations result in higher nzvi attachment eq 12 a normalized mass injected mass attached mass was calculated and show a similar value for all injection concentration values table s2 suggesting that increasing the injection concentration won t increases the portion of mass attached and therefore injecting more mass may increase the aqueous nzvi concentration this would be even more pronounced if there is detachment or limited adsorption sites not modelled in this study lastly the total attached mass was higher in the swiss aquifer compared to the borden aquifer for identical total injected mass suggesting that heterogeneity increased nzvi attachment 4 1 3 effect of solution viscosity fig 7 shows the effect of solution viscosity on xcn xnsp zcn znsp and st as seen in the figure a change in solution viscosity led to a change in the median in almost all the parameters with the largest difference observed in mass spread in both directions xnsp and znsp and st nzvi attachment the p values concurred with this visual inspection with p 0 05 calculated for nzvi mass spread in both directions xnsp and znsp and nzvi attachment for both aquifers table 5a for all other factors the p values were greater than the significance level of 0 05 and therefore the null hypothesis was not rejected in other words the change in solution viscosity had a statistically significant effect on nzvi transport in terms of nzvi spread and attachment in both media but had no statistically significant effect on nzvi center of mass xcn zcn as mentioned previously injection was modelled as a constant flux boundary condition as such the velocity produced around the borehole remained constant regardless of the injection viscosity this would be different if the injection was gravity fed krol et al 2013 as such the centre of mass would not be affected by the viscous injection but the spread would increase and attachment decrease with increased viscosity due to a decrease in η0 as seen by others krol et al 2013 fig 8a and b show two simulations for borden aquifer at 48 h for case 1 solution viscosity 0 013 pa s and case 6 solution viscosity 0 072 pa s respectively it is clear from fig 8b that although the center of mass is comparable nzvi spread was increased due to change in solution viscosity from 0 013 pa s to 0 072 pa s which was supported by the change in attached mass distribution i e fig 9 in other words nzvi attachment decreased due to the increase in solution viscosity in line with others findings krol et al 2013 li et al 2015 2016 chowdhury et al 2015 4 1 4 effect of injection rate fig 10 shows the effect of injection rate on xcn xnsp zcn znsp and st as seen a change in injection rate led to a change in the median in almost all the parameters with the largest difference observed in mass spread in both directions xnsp and znsp and st nzvi attachment the p values concurred with this visual inspection with p 0 05 calculated for nzvi mass spread in both directions xnsp and znsp and nzvi attachment for both aquifers table 5a for all other factors the p values were greater than the significance level of 0 05 and therefore the null hypothesis was not rejected in other words the change in injection rate had a statistically significant effect on nzvi transport in terms of nzvi spread and attachment in both media but had no statistically significant effect on nzvi center of mass xcn zcn increase in nzvi spread is due to the increased injection rate which leads to constant subsurface velocities around the borehole due to the constant flux injection as opposed to gravity fed systems among the four factors considered groundwater velocity injection concentration solution viscosity and injection rate solution viscosity and injection rate had a statistically significant effect on nzvi spread and attachment while background groundwater velocity and injection concentration did not change the nzvi distribution parameters significantly even at the largest factor magnitude the one exception was the horizontal center of mass in the borden aquifer that was affected by the largest groundwater velocity level which was likely due to the low degree of heterogeneity in the borden aquifer as compared to the swiss aquifer 4 1 5 effect of lag period fig 11 shows the effect of the lag period on xcn xnsp zcn znsp and st while the p values are shown in table 6 as seen in fig 11 varying the lag period led to a change in the median values in almost all the parameters with the largest difference observed in nzvi attachment the p values for all combinations resulted in p 0 05 for attachment st only while all other p value were greater than 0 05 and therefore the null hypothesis was not rejected table 6 in other words the lag period had a statistically significant effect only on the nzvi attachment st less attachment was observed with longer continual injection e g case a compared to other cases e g case b and c longer injection periods case a increased the horizontal spread of the nzvi plume xnsp in both aquifers fig 11b and decreased the attachment fig 11e in addition the higher frequency of lag phases in cases b and c impacted the continuity of the injection period and hence nzvi travel distance it was hypothesized that the addition of a lag phase would increase nzvi attachment due to the lack of injection flux however it was unclear whether it is the duration or timing of the lag phase that would be most disadvantageous to nzvi transport to distinguish between these factors the three cases a b and c were simulated with the same total time for nzvi injection 48 h but at various frequencies table 4 the number of injection periods increased in case b and case c compared to case a but the duration of injection became shorter fig 11 shows that the highest attachment was seen for the case c scenario indicating that the total time of injection was not as important as the longest successive injection period resulting in less attachment than those with shorter injections and shorter lag periods as less nzvi attachment is desired for optimum transport in porous media long injection periods followed by short lag phase if necessary is recommended for nzvi deployment 4 1 6 effect of heterogeneous permeability distribution all simulations were run for two heterogeneous aquifers borden and swiss although the mean permeability correlation length and porosity were the same for both aquifers the swiss aquifer had a higher variance in permeability resulting in a greater contrast in permeability fields aquifer heterogeneity impacted the distribution of parameters as seen from the box plot distributions e g figs 5 6 7 10 and 11 for all cases it was observed that the variance of the parameters i e the center of mass spread and nzvi attachment was higher in the swiss aquifer than in the borden aquifer i e the variance of xcn distribution in the borden aquifer with 0 1 m day 1 groundwater velocity was 0 05 but in the swiss aquifer it was 0 53 to quantify the effect of soil heterogeneity a two factor anova analysis was performed and is described in the next section 4 2 results of two factor anova table 7 shows the results of the two factor anova which was performed to address the inability of the one factor anova in analyzing the impact of permeability variation of the two aquifers and the possible masking effects since the two factor anova is capable of assessing more than one factor of interest the effect of heterogeneity along with another factor either groundwater velocity injection concentration solution viscosity or injection rate as well as the interaction of these factors was examined table 7 shows the effect of heterogeneity on nzvi transport parameters using two factor anova heterogeneity had a statistically significant effect on almost all parameters xcn xnsp znsp and st in addition the predicted masking effect by heterogeneity described in subsection 4 1 1 for which no statistically significant impact was seen on xcn in the swiss aquifer through the one factor anova was confirmed statistically significant effect of heterogeneity on xcn and xnsp was seen when two factor anova was carried out among cases 1 and 4 this was not captured by the one factor anova although it can be seen in the box plots fig 6a and b the two factor anova quantified these results and indicated that the increased variance of soil permeability in the swiss aquifer obscured the effect of injection concentration on horizontal mass transport p values close to the significance level of 0 05 were observed for znsp and st heterogeneity also had a statistically significant effect on xnsp znsp and st when two factor anova was performed among cases 1 and 6 significant interaction in p values indicate that the effect of one factor depends on the level of the second factor montgomery and runger 2007 in this case statistically significant interaction was not observed between soil heterogeneity and any of the factors however value close to 0 05 was seen for interaction between st and solution viscosity this implies that the effect of solution viscosity on the mass parameters had greater impact than the level of soil heterogeneity the result of two factor anova indicates that heterogeneity of subsurface aquifer is an important parameter for nzvi deployment sufficient knowledge of heterogeneity and possible effects should be considered for field scale implementation of nzvi technology this can be seen in fig 12 where the heterogeneity of swiss aquifer impacts the nzvi distribution compared to the borden aquifer the difference in the variance of permeability fields resulted in the visibly different nzvi distribution in the two aquifers with greater variability seen in the swiss aquifer nzvi movement in the borden aquifer fig 12a seemed to be confined around the injection well while nzvi appeared to be more dispersed in the swiss aquifer fig 12b with nzvi following a preferential flow path as outlined by the high permeability zone seen in fig 3b both figures show the same permeability realization 5 conclusions this work aimed to answer some key questions regarding nzvi deployment in the field during field scale injection maintaining a nzvi injection flux deciding on which cmc nzvi solution viscosity to use or the inclusion of a lag period purposely or not are key challenges for engineers and decision makers in addition understanding the inherent groundwater velocity conditions of a site is often considered important this work focused on finding the statistically significant factors for nzvi deployment to aid in field scale injection while a large body of work by others has focused on finding the optimum injection concentration or understanding how groundwater velocity will affect treatment this study shows that neither of these factors has a statistically significant impact on nzvi travel distance or spread therefore maintaining a hydraulic gradient and associated groundwater velocity during field injection is not an important consideration as groundwater velocity does not change nzvi travel distance by a statistically significant amount injection rate which can alter the groundwater velocity around the well field does play an important role and was shown to be a statistically significant factor although some studies reported an effect of various parameters on nzvi travel distance the significance of the effects have never statistically been quantified and therefore not conclusive the study presented here quantified the 2d nzvi transport model results using one and two factor anova to examine the factors that have the greatest impact on nzvi travel distance the 2d model considered randomly distributed soil permeability fields of two aquifers where one aquifer was more heterogeneous than the other the one factor anova was performed considering five factors background groundwater velocity injection concentration lag phase solution viscosity of the injected solution and injection rate the magnitude of these five factors was varied and various simulations performed in addition the two factor anova was performed to understand the impact of soil heterogeneity this research showed that field groundwater velocity background aquifer velocity had no statistically significant effect on nzvi movement and spread except on the horizontal center of mass in the borden aquifer under a large groundwater velocity domain therefore typical field groundwater velocities differed by change in hydraulic gradients do not significantly impact nzvi travel distance similarly nzvi injection concentration keeping injection rate constant was shown not to significantly change the nzvi distribution parameters i e movement and spread even with a large concentration magnitude however large injection concentration caused higher nzvi attachment near the well head injection solution viscosity and injection rate had a statistically significant effect on nzvi mass spread and attachment in both aquifers but not on the nzvi center of mass a higher solution viscosity was found to maximize nzvi spread and minimize nzvi attachment this analysis showed that solution viscosity is an important factor for optimizing nzvi deployment at the field scale stable nzvi suspension in the presence of highly viscous polymer with optimum pressure containment should be used during field scale injection to ensure optimum nzvi spread which could result in reducing remediation time the analysis also showed that injection rate is an important factor for optimizing nzvi deployment at the field scale as injection rate was found to maximize nzvi spread when applied at a constant pressure the length of the injection and lag period are also important factors for optimizing nzvi deployment at the field scale in this work three types of lag periods were considered in a 96 h timeframe less nzvi attachment occurred during a long injection phase followed by a long lag period 48 h compared to shorter injection and lag periods 24 h and 12 h even though the total injection time was the same it is therefore recommended that long injection and short lag phase be used during nzvi deployment at the field scale which will result in less nzvi attachment lastly aquifer heterogeneity was investigated and was found to effect nzvi distribution while the one factor anova showed higher parameter variance i e the center of mass spread and nzvi attachment in the swiss aquifer the two factor anova analysis was performed to quantify these results this analysis showed that heterogeneity had a statistically significant effect on nzvi spread parameters in addition the interaction between the factors and heterogeneity was not found to be statistically significant which indicates that heterogeneity did not obscure the effect of injection concentration solution viscosity and injection rate on nzvi attachment future studies should include additional retention mechanism such as straining and mechanical filtration which may result in greater effect of soil heterogeneity on nzvi transport declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by graduate support by york university toronto canada and the lassonde graduate entrance scholarship appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103870 
1358,in this study an effective and green adsorbent was prepared by the self activation of kenaf fiber and then the kenaf based activated carbon kac was applied for the removal of lead pb ii copper cu ii and congo red cr dye from an aqueous solution by the process of adsorption the surface morphology of mesoporous adsorbent was characterized the kac showed good capacity of adsorption of as pb ii cu ii and anionic dye cr in very short period of agitation the adsorbent efficiency of metal ions and dye was estimated by varying the adsorbent dose ph contact time initial metals and dye concentration and temperature optimum adsorption of metal ions and cr dye was observed at ph 6 and at ph 4 at 120 min respectively the adsorption isotherm was described by the langmuir and freundlich isotherm equations the green adsorbent followed the pseudo second order kinetic model with correlation coefficients r2 value 0 99 the increase in adsorption temperature enhanced the adsorption efficiency for both heavy metals and dye the kac showed no significant loss of the adsorption capacity after 3 cycles of reuse graphical abstract unlabelled image keywords heavy metals dye kenaf activated carbon adsorption abbreviations kf kenaf fiber kac kenaf based activated carbon cr congo red pb lead cu copper 1 introduction water pollution is one of the major threats to human health and the ecosystem organic and inorganic pollutants are discharged in the aquatic environment either in water soluble or insoluble forms al othman et al 2019 ali et al 2018a dyes volatile organic compounds and pesticide are some of the organic pollutants while metals and disinfectant bye products dbp like bromide fall under the category of inorganic pollutants ali et al 2019 ali et al 2018b khan et al 2016 the use of heavy metals and dyes is increasing due to rapid industrialization and globalization alqadami et al 2017 several heavy metals and dyes are non biodegradable and are highly toxic and carcinogenic in nature yin et al 2019 lead pb ii and copper cu ii are two common heavy metals found originating from wastewaters of mining electrical electroplating and smelting alothman et al 2020 mittal et al 2016 long term exposure to lead metal can cause severe damage to the nervous system reproductive system kidney brain and liver alqadami et al 2018 prolonged oral administration of excess quantity of copper may result in renal failure liver damage and acute poisoning to the human body itankar and patil 2021 congo red cr an anionic dye is generated from textiles printing and dyeing paper rubber and plastics industries costa et al 2020a crini 2006 due to its stable structure cr dye is non biodegradable causing serious health issues to humans sud et al 2008 environmental protection agency epa has set the maximum contaminant level mcl standards in drinking water for pb ii and cu ii at 0 015 mg l and 1 3 mg l attallah et al 2013 therefore the remediation of water contaminated by these toxic chemicals is essential to protect the environment and reuse of water biological chemical and physical treatment methods have been applied for the removals of heavy metals and dyes from the contaminated wastewater naushad et al 2019 among various methods of decontamination of polluted water the adsorption technique has been used effectively due to the availability of different adsorbents alothman z a 2012 saleh 2020a simple preparation salimi et al 2017 easy handling jabli et al 2018 saleh 2020b low operational cost high efficiency wang et al 2012 sellaoui et al 2017 and uncomplicated regeneration ability of the adsorbents saleh and ali 2018a sebeia et al 2019 activated carbon ac is widely used as an adsorbent because of its high surface area however the cost the production of toxic sludge and the loss of adsorption efficiency are some well documented restrictions of commercially available activated carbon alqadami et al 2020 therefore there is a need for inexpensive abundant and sustainable materials which are potential sources of activated carbon for the removal of toxic pollutants from the water during the last decade several non conventional biocompatible low cost adsorbents such as iron nanocomposite ali et al 2018c amine modified fibers of populus tremula tka et al 2018 seeds of neem a indica costa et al 2020b grafted carbon microspheres saleh and ali 2018b magnetic hydrochar khan et al 2019 and maghemite nanoparticles afkhami and moosavi 2010 have been explored for the removal of heavy metal ions and dyes from aqueous solution kenaf hibiscus cannabinus a natural cellulosic fiber comprised of cellulose hemicellulose and lignin is widely cultivated in many tropical countries for its fiber content razak et al 2020 kenaf ranks third in biomass production and grows rapidly due to its biodegradability compatibility and regeneration possibility the usage of kenaf has increased for various applications mandal and shi 2020 the presence of functional groups like phenols amines and hydroxyls play a significant role in the adsorption of heavy metals and dyes sajab et al 2011 zhou et al 2020 chowdhury et al 2013 in our present study kenaf core kc was used to produce activated carbon ac by the process of self activation self activation is an endothermic process that uses high temperature 1 000 o c and oxygen free environment to cause the gasification of a biomass source and makes the use of the gases emitted from the thermal treatment to activate the biomass to improve its surface properties xia and shi 2016 thus self activation is a novel technique which does not require any toxic acids and bases for activation several studies have reported the use of low cost adsorbent prepared from harsh chemicals but the utilization of self activated kenaf core fiber without any chemical modification to adsorb dyes and heavy metals has not been reported elsewhere in our studies we have used kenaf based activated kac for the removal of pb ii cu ii as well as cr dye from aqueous solutions batch experiments were conducted to assess the adsorption capacity reaction kinetics and the effects of critical operating parameters for the removal of the above mentioned contaminants using the kac the obtained kac was characterized by various techniques such as ftir esem tga and bet the effect of different factors like contact time ph of the solution adsorbent dosage temperature and concentration on adsorption of metal ions cu ii and pb ii and dye cr were investigated furthermore the data obtained under different operating conditions were fit into different kinetic models and isotherms for the determination of the kinetics and sorption process the results showed that the novel kac had a high prospective for pb ii cu ii and cr adsorption 2 materials and methods 2 1 kenaf core kenaf core was collected from kengro corp charleston ms the metal ions lead nitrate pb no3 2 and copper chloride cucl2 congo red cr dye sodium hydroxide naoh hydrochloric acid hcl was purchased from sigma aldrich st louis mo usa the deionized water 18 2mωcm 1 was used for all experiments 2 2 preparation of the bioadsorbent kenaf based activated carbon kac was prepared by the self activation method xia and shi 2016 in self activation method oxygen free environment activates the kc at a high temperature first ten grams of kc were washed several times in di water and dried overnight in an oven at a temperature of 60 o c ten grams of dried kc were sealed in a box furnace sty 1600c sentro tech corp usa with a type k thermocouple and a data logger tc101a madgetech inc usa and the atmosphere was vacuumed a digital pressure gauge adt680w 25 cp15 psi n additel corp usa was used to measure the pressure 97 906 34 pa a closed system was created by closing the valves of the furnace kac was prepared by the pyrolysis process that was conducted in three steps a heating with a rate of 10 c min 1 up to 1 000 c b dwelling for 2 h and c cooling with a temperature decreasing rate of no more than 10 c min 1 to room temperature the activated carbon was taken out and stored in an airtight container for further use 2 3 adsorbent characterization surface area and pore volume and average pore size of kac were determined by the bet method brunauer emmett teller using n2 adsorption onto the kac using a 3flex 3500 micromeritics instrument corp norcross ga kac was degassed at 150 o c prior to the n2 adsorption thermogravimetric analysis tga q50 ta instruments was performed both for kc and kac in the temperature range from 30 o c to 700 o c with a heating rate of 10 o c min under nitrogen atmosphere by using mettler toledo surface morphology and porosity information of raw kc kac before and after the adsorption was obtained without coating using scanning electron microscope model no fei esem quanta 200 at 20 kv working voltage to ensure a suitable image resolution an energy dispersive x ray spectrometer eds along with esem analysis was also performed to determine the elemental composition of the adsorbent the basic chemical information regarding functional groups presents in the kenaf fiber and kenaf based activated carbon were determined using fourier transform infra red ftir spectrophotometer model no ft ir spectrometer spectrum two perkin elmer inc waltham ma usa the spectrum of each sample was collected at a resolution of 4 cm 1 and 16 interferogram scans in the range of 400 4 000 cm 1 the average bulk density of kac was calculated by the water displacement method the point of zero charges phzpc is a characteristic that determines the ph at which the adsorbent surface has net electrical neutrality and provide valuable information on electrostatic interactions between adsorbent and adsorbate the ph values of the bioadsorbent at the phpzc were obtained using the solid addition method balistrieri and murray 1982 2 4 adsorbates heavy metals lead pb ii copper cu ii and congo red cr all chemicals used in this study were of analytical grade pb ii cu ii and cr were selected as adsorbate in the present study cr is an anionic azo dye with molecular formula and molecular weight as c32h22 n6na2o6s2 and 698 68 respectively the international pure and applied chemistry iupac name of cr is 1 naphthalene sulfonic acid 3 3 4 4 biphenylenebis azo bis 4 aminodisodium salt the chemical structure of cr dye is shown in fig 1 3 experimental 3 1 adsorption studies the standard stock solutions of pb ii and cu ii and cr 1000 mg l were prepared by dissolving the appropriate amount of lead nitrate pb no3 2 and copper chloride cucl2 and congo red cr dye in a liter of deionized water the working solutions were prepared by diluting the stock solution with deionized water to give the appropriate concentration of the working solutions the ph of the solutions was adjusted by the addition of either 0 1 m hcl or 0 1 m naoh respectively all sample bottles and glassware were cleaned and then rinsed with deionized water and oven dried at 60 c 0 05 g of kac was treated with 40 ml of pb ii cu ii and cr dye solutions of 5 10 15 20 25 ppm concentrations the flasks containing the metals and dye solutions with kac were placed on the shaker at 150 rpm for 24 h to ensure equilibrium was reached at 27 o c based on kinetic study 120 min was taken as the time enough to reach the equilibrium state after the specified time suspensions were filtered through filter paper whatman 44 concentrations of pb ii ions and cu ii ions in the filtrates were determined by atomic absorption spectrophotometer model no perkin elmer a analyst 300 the concentration of the residual dye was measured using uv visible spectrometer model no perkin elmer lambda 900 at a λ max corresponding to the maximum adsorption for the dye solution λ max 497 nm the ph value was measured using ph meter sper scientific basic 840 087 a calibration curve was plotted between absorbance and concentration of the dye and the two metals solutions to obtain absorbance concentration profile in order to study the kinetics 50 mg of kac was stirred with 20 mg l of pb ii cu ii and cr solutions from 20 to 120 min at 150 rpm kac in an amount 50 mg and 40 ml of aqueous solution of pb ii cu ii and cr of different concentrations 5 10 15 20 25 mg l were used for plotting the adsorption isotherms for isotherm studies each experiment was conducted at 27 o c for calculating the amount of adsorbent dosage 20 mg l of pb ii cu ii and cr dye solutions were mixed with 30 to 70 mg of kac and stirred for 120 min at 27 o c to examine the effect of ph on the adsorption capacity 20 mg l of pb ii cu ii and cr solutions was stirred for 120 min with 50 mg of kac at ph values ranging from 3 to 8 the adsorption capacity qe mg g at equilibrium and qt mg g at time t was calculated for each sample of cu ii pb ii and cr by the eqs 1 and 2 1 q e c i c e x v m 2 q t c i c t x v m where ci the initial concentration of pb ii and cu ii ions and cr in mg l ct the concentration of pb ii and cu ii ions and cr in mg l at time t v volume of pb ii cu ii ions and cr in l m mass of kac g and ce the concentration of pb ii cu ii and cr at equilibrium the removal percentage r of the pollutant was calculated using eq 3 3 r c i c e c i x 100 4 results and discussions 4 1 physical characterization of kac 4 1 1 bet analysis the physicochemical characterization of kac is listed in table 1 the bet surface area of the kac 843 3 m2 g was much higher than raw kf 3 2 m2 g depending on pore diameters bet pores are classified into three main categories iupac international union of pure and applied chemistry 1972 micropores pore size 2 oa mesoporous pore size 2 50 oa and macropores pore size 50 oa the prepared activated carbon from kenaf is classified as mesoporous type as the average pore diameter of kac is 3 55 nm 4 1 2 thermogravimetric analysis tga the thermal degradation of kc and kac is shown in fig 2 a and b the tga data demonstrated that kc had two major thermal degradations points the first one at 250 o c and the second one at 400 oc the first thermal degradation of kc was observed in temperature range 10 100 o c and there was 10 weight loss this weight loss was due to the evaporation of water molecules and some highly volatile compounds the second thermal degradation around 250 o c established the degradation of hemicellulose and some portion of lignin from the raw kc and indicated that the major portion of cellulose and lignin were pyrolyzed similar observations were made for natural fibers lee et al 2009 it is evident that kac is thermally more stable than kc and the tga data fig 1b exhibited that there was not much significant weight loss of the adsorbent till 550 c however a significant weight loss of the kac was observed beyond 550 o c due to the dissociation of carbon carbon bonds reza and ahmaruzzaman 2015 4 1 3 scanning electron microscope sem analysis an environmental scanning electron microscope esem was used for morphology characterization esem micrographs of raw kf kac and spent kac kenaf based activated carbon after adsorption at different magnification are depicted in fig 3a b and c respectively the surface of raw kenaf fiber kf as shown in fig 3a are highly packed dense uneven and rough with mini pores and cavities however the surface of kf after self activation showed lot of pores due to the removal of cellulose hemicellulose and lignin as revealed from the esem image of fig 3b the pores observed on the surface of kac can easily facilitate the transfer of adsorbates heavy metals and dye into the inner part of the adsorbent particles the eds spectrum of the kac is shown in fig 4 six elements c k o al mg and si are present in the kac where c and k are the dominant elements esem image of fig 3c showed the spent kac the kac surface became brittle and the disappearance of pores in kac after the adsorption clearly demonstrated the successful adsorption of metals ions and the dye 4 1 4 fourier transform infrared spectroscopy ftir ftir was used to identify the functional group of kc kac samples before and after the adsorption of heavy metals kac cu kac pb and dye kac cr ftir spectrum for untreated kc fig 5 showed a broad peak around 3 344 cm 1 which can be ascribed to antisymmetric stretching vibration of oh groups of cellulose as reported in other literature which disappeared in kac kac cr and kac cu karimi et al 2014 the band at 3400 cm 1 for kac pb could be due to the oh stretching of alcohols or presence of n h ali et al 2015 as shown in the ftir fig 5 c h asymmetric stretching vibration produced a medium peak around 2 685 2 680 cm 1 reza and ahmaruzzaman 2015 some bands were observed between 2 380 cm 1 2 375 cm 1 for kac kac cu kac pb and kac cr due to the stretching vibrations of ch2 group the peak observed at 1 727 cm 1 in the spectra of kenaf fiber is related to the co stretching due to the presence of hemicellulose the spectra showed characteristic peak around 1 580 cm 1 for kac kac cu and kac pb fig 5 which is assigned to the cc stretching vibration saleh 2011 a characteristic peak was detected in kf at 1 243 cm 1 that could be associated with the c o stretching of the aryl group in lignin which disappeared in other samples sharp peaks observed in kac pb around 1 200 cm 1 can be attributed to c o stretching of carboxylate or ether structures sharp peaks were observed around 1 030 cm 1 for kc kac pb and kac cu due to c o stretching saleh 2018 some peaks small and sharp were observed for kc kac kac cu kac pb and kac cr between 900 and 500 cm 1 and are assigned to cc bending lafi et al 2019 the formation of new bands and ban shifts after treating pb ii cu ii and cr dye with kac indicated that the corresponding functional groups were involved during the adsorption vs other functional groups that remained unaltered 4 2 batch adsorption 4 2 1 effect of ph the ph value of the solution is an important parameter that controls the process of bioadsorption the ph value not only affects the active sites on the surface of biosorption but also affects the properties of the metal ions as well as dye to investigate the effect of the solution ph on kac the adsorption experiment was conducted at 40 ml of pb ii cu ii and cr solution with a concentration of 20 mg l at 27 o c and 150 rpm ph ranged from 3 to 8 fig 7a the percentage removal of pb ii increased from 52 4 to 62 4 and for cu ii it increased from 52 9 to 60 7 when the ph was varied from 5 0 7 0 and then decreased at higher ph the ph value of the kac phpzc of kac was calculated at 6 2 at ph phpzc due to the presence of hydroxide ions oh the kac surface became negatively charged which enabled the adsorption of metal ions for pb ii and cu ii the maximum removal percentage was observed at ph 6 5 it was assumed that at low ph 5 high concentration of hydronium ions h3o competed with pb ii and cu ii cations resulting in lowering the uptake at lower ph khan et al 2020 at higher ph values 7 there was a decrease in adsorption percentage for pb ii and cu ii which was probably due to the formation of hydroxide of pb ii and cu ii because of chemical precipitation of these cations babaei et al 2016 fig 6a and b shows the change of color of cr dye before and after the addition of kac fig 7a shows that the removal rate and biosorption capacity of cr increased from ph 3 0 to ph 4 0 it reached the maximum at ph 4 0 with the removal percentage of 78 9 which was decreased with the increase of the ph value in acidic solutions h easily combined with the so3 presented in cr and the decolorization effect of the dye was increased by the electrostatic attraction between molecules when the ph value was 7 there was an increase in alkaline conditions and oh increased in the solution there was electrostatic repulsion which led to the decrease of the absorption efficiency similar ph dependent results were reported for the adsorption of cr on activated carbon purkait et al 2007 4 2 2 effect of adsorbent dosage to investigate the effect of adsorbent dosage on pb ii cu ii and cr the adsorbent dose was varied from 0 75 g l to 1 75 g l from fig 7b it was evident that the increase in adsorbent dose increased the removal rate of metal ions and the dye as the concentration of the adsorbent dose was increased from 0 75 g l to 1 75 g l the removal efficiency of pb ii increased from 63 7 to 73 4 removal efficiency of cu ii increased from 54 9 to 64 6 and the removal efficiency of cr dye increased from 73 9 to 80 0 conditions ci 20 mg l adsorbent dosage 50 mg temperature 27 o c agitation speed 150 rpm contact time 2 h ph 6 for pb ii and cu ii ph 4 for cr the probable reason for the increase in efficiency could be due to the presence of more adsorption sites and more adsorbent for the same amount of adsorbate molecules from the result it could be concluded that the optimum dosage of kac for cr and cu ii adsorption was 1 5 g l and 1 75 g l for the pb ii further increase in adsorbent dosage did not provide any higher uptake of cr and cu ii and pb ii very nominal increase was observed indicating that the optimum ratio of adsorbent to adsorbate dosage was attained and all the active sites on the adsorbent dosage were occupied 4 2 3 effect of contact time and initial concentrations the effect of contact time on the performance of kac for the removal of pb ii cu ii and cr was examined in the range of 20 120 min using following parameters temperature 27 o c ci 20 mg l ph 4 adsorbent dose 50 mg the effect of contact time on the removal efficiency of metal ions and the dye is demonstrated in fig 7c the percentage of the removal rate for pb ii cu ii and cr increased as contact time was increased the rate of removal percentage was very rapid at the beginning until 60 min due to the availability of higher number of active functional groups and more vacant sites on kac the maximum removal percentage was observed at 120 min and the equilibrium time was 24 h the removal efficiencies for pb ii increased from 60 6 to 68 3 and 52 6 to 63 5 for cu ii the cr removal efficiency increased from 65 7 78 9 when the contact time was changed from 20 min to 120 min after 100 min the rate became more or less constant reaching the equilibrium at 120 min thus increasing the contact time resulted in a decrease in the availability of dye and heavy metal molecules to active sites on the kac surfaces and reduced the adsorption efficiency the increase in initial metal ions and dye concentration also enhanced the interaction between pb ii cu ii and cr with the kac as seen in fig 7d an increase in initial concentration of pb ii cu ii and cr from 5 ppm to 25 ppm enhanced the adsorption uptake of the pollutants at 120 min the rate of adsorption of cr and metal ions for all concentrations was rapid in the beginning due to the available vacant sites and functional groups on kac two possible reasons for the increase in adsorption capacity when the concentration of the adsorbate was increased i higher mass transfer the driving force that enhanced interactions between cr and the two metal ions ii more adsorbate dye and metals molecules for the adsorption onto kac 4 2 4 effect of temperature to study the effect of temperature on pb ii cu ii and cr removal rate experiments were performed by varying the temperature between 27 o c 57 o c with conditions ci 20 mg l adsorbent dosage 50 mg temperature 27 o c agitation speed 150 rpm contact time 2 h ph 6 for pb ii and cu ii ph 4 for cr the amount of kac was maintained at 0 05 g l and the concentration was 20 mg l the rate of adsorption increased from 68 3 to 74 6 for pb ii 63 5 to 65 1 for cu ii and 78 9 to 83 9 for cr when the temperature was increased from 27 o c to 47 o c as shown in fig 7e the reason could be due to the increase in interparticle diffusion rate in the pores at a higher temperature however the efficiency removal rate for pb ii was noticed to remain constant when the temperature was raised to 57 o c this could be due to the saturation effect slight decrease in removal percentage for cr and cu ii could be due to the decreased interactions between the adsorbent and the adsorbate at higher temperature 4 3 isotherm studies 4 3 1 adsorption kinetics to investigate the kinetics involved in the adsorption of pb ii cu ii and cr the linear form of lagergren s pseudo first order lagergren 1898 was used 4 ln q e q t ln q e k 1 t in eq 4 qe and qt are metals and dye adsorbed per unit weight of activated carbon at equilibrium mg g and at time t k1 min 1 is the rate constants of adsorption process for pseudo first order k1 can be determined from the linear plots between ln qe qt vs t as shown in fig 8a poor correlation coefficients r2 were clearly shown indicating the process did not obey the lagergren s first order model hence the data was modeled using pseudo second order kinetic eq ho et al 2000 5 t q t 1 k 2 q e 2 t q e in eq 5 k2 gm mg 1 min 1 is the rate constants for pseudo second order kinetic models at 27 o ck k2 and qe can be obtained from the intercept and the slope of the plot t qt vs t as shown in fig 8b the correlation coefficient value for pseudo second order r2 for pb ii cu ii and cr were 0 999 0 997 and 0 999 which indicated that the kinetic adsorption fitted well with pseudo second order model as summarized in table 2 the aptness of pseudo second order model to describe the adsorption kinetic data was further confirmed by the comparable qe exp and qe cal values one of the most useful models for describing chemical adsorption is the elovich model and as per the elovich model demiral and güngör 2016 in eq 6 the adsorption sites can increase exponentially and α and β are the equation constants that can be calculated from the slope and intercepts of the linear plots between qt and ln t as shown in fig 8c the elovich model also represents multilayer adsorption table 3 showed the calculated parameters of elovich model for pb ii cu ii and cr at 20 mg l the present kinetic data were well described by the elovich model with a correlation coefficient r2 not less than 93 6 q t 1 β ln αβ 1 β ln t 4 3 2 adsorption isotherm the experimental data for metal ions and dye uptake as well as the homogeneity and heterogeneity by adsorbent were established by langmuir langmuir 1918 freundlich freundlich 1906 and temkin isotherm models temkin and pyzhev 1940 langmuir is based on the generation of adsorbate molecules on homogeneous surface of the adsorbent and the linear form of langmuir model is represented by 7 c e q e 1 k l q max 1 q max c e ce is the equilibrium concentration in mg l of metal ions and dye adsorbed by kac qe is the retention capacity of the metal ions and dye mg g at equilibrium time kl l mg is the langmuir isotherm constant related to the binding energy between the adsorbate metal ions and cr and adsorbent kac qmax is the maximum adsorption capacity of the adsorbent mg g the straight line was obtained when c e q e was plotted against ce as shown in fig 9a the qmax and kl were calculated from the slope and intercepts and summarized in table 4 the dimensionless equilibrium separation parameter rl eq 8 was used to evaluate the appropriateness of the adsorption isotherm 8 r l 1 1 c i k l the calculated value of rl is less than 1 0 13 indicated a positive adsorption the linear form of freundlich model is 9 ln q e ln k f 1 n ln c e where kf is freundlich constant which indicates the capacity ce represent the adsorbate concentrations and qe is the quantity of cu ii pb ii and cr at equilibrium and n is freundlich exponent kf and n were determined from plot ln qe vs ln ce as depicted in fig 9b the n value is often used to indicate the appropriateness of the process of adsorption 1 n 1 suggest a normal process while 1 n 1 indicates a cooperative process the value of 1 n was 0 35 with a favorable pb ii cu ii and cr dye adsorption on the kac temkin isotherm examines the interaction between the adsorbate and the adsorbent and manage a linear decrease in the adsorption energy with surface coverage of the adsorbent and its form as follows 10 q e rt b t lnk t rt b t lnc e where kt is the temkin isotherm equilibrium binding constant in l g r of 8 314 10 3 kj mol k is the universal gas constant t is the absolute temperature and bt is the model constant the heat of absorption in j mol both bt and kt can be determined from the plot qe vs ln ce fig 9c the isotherm parameters for the adsorption of pb ii cu ii and cr are given in table 4 the experimental data fit well with this temkin model thus indicating a surface coverage of the heavy metals and dye on kac by comparing the values of r2 from table 4 we can conclude that the langmuir adsorption model provided the best fit with experimental data theoretical maximum amounts of the metals ions and dye were calculated from langmuir adsorption isotherm qmax and it was very close to the experimentally observed values 4 4 adsorption mechanism based on adsorption kinetics and adsorption isotherm analysis and characterization results the adsorption of pb ii cu ii and cr was a combination of physical adsorption and chemical adsorption and physical adsorption plays a main role in the adsorption process of dye and metal ions the adsorption mechanism can be explained from three aspects 1 the pore structure of kac provided abundant adsorption sites for the adsorbates cr adsorption 2 pb ii cu ii and cr interacted well with the hydroxyl group on the surface of the kac through the h bond 3 electrostatic interaction played an important role in the metals and dye adsorption process 4 5 comparison of different adsorbents and reusability study in table 5 the maximum adsorption capacity qmax of the kac is compared with other bio adsorbents used by other researchers it is observed from the table that the kac has a good adsorption capacity with the other used bio adsorbents the reusability of the green adsorbent kac for 3 cycles using 5 mg l of each of the adsorbate at 27 o c for 120 min contact time and ph 7 was conducted and the results are shown in fig 10 5 conclusions this work reported the application of self activated kac for the removal of pb ii cu ii and cr dye from the wastewater the specific surface area and pore volume of the resulting activated carbon fibers were obtained as 843 4 m2 g and 0 156 cm3 g respectively this research revealed that the adsorption process fitted well with the langmuir isotherm model with r2 0 98 for cu ii while freundlich model fitted well for pb ii and cr with r2 0 99 than that from the langmuir and temkin models the removal of metal ions and cr dye was ph dependent and the optimum ph range was determined as between 4 and 7 the highest percentage removal of pb ii cu ii and cr dye was found to be 92 80 and 95 respectively at a concentration of 5 mg l at 27 o c and 150 rpm agitation speed at 120 min of contact time the optimum adsorbent dosage kac was fixed at 1 5 g l the experimental results demonstrated that kac could be employed successfully as a cost effective benign environmentally friendly adsorbent for the remediation of heavy metals and organic dye on an industrial scale author contributions sujata mandal experiments data analysis and manuscript development jose calderon assistance in data collection of analytical instruments sreekar b marpu supervising reviewing and editing omary a mohammad advising and reviewing sheldon q shi supervising and advising as a major advisor corresponding author author statement dear dr tan thank you so much for giving us the opportunity to revise the manuscript we believe all the reviewers comments have been properly addressed we have uploaded two files i revised manuscript with all changes highlighted in red font ii new manuscript file with all revisions suggested by the reviewers we have added a summary table describing how we responded each comment and attached under the response to reviewers please feel free to let me know if you have any more questions declaration of competing interest the authors declare there is no conflict of interest regarding the publication of this research manuscript acknowledgements this work was supported by the university of north texas toulouse graduate school graduate research award received by s mandal esem was performed at the university of north texas s materials research facility mrf a shared research facility for multi dimensional fabrication and characterization 
1358,in this study an effective and green adsorbent was prepared by the self activation of kenaf fiber and then the kenaf based activated carbon kac was applied for the removal of lead pb ii copper cu ii and congo red cr dye from an aqueous solution by the process of adsorption the surface morphology of mesoporous adsorbent was characterized the kac showed good capacity of adsorption of as pb ii cu ii and anionic dye cr in very short period of agitation the adsorbent efficiency of metal ions and dye was estimated by varying the adsorbent dose ph contact time initial metals and dye concentration and temperature optimum adsorption of metal ions and cr dye was observed at ph 6 and at ph 4 at 120 min respectively the adsorption isotherm was described by the langmuir and freundlich isotherm equations the green adsorbent followed the pseudo second order kinetic model with correlation coefficients r2 value 0 99 the increase in adsorption temperature enhanced the adsorption efficiency for both heavy metals and dye the kac showed no significant loss of the adsorption capacity after 3 cycles of reuse graphical abstract unlabelled image keywords heavy metals dye kenaf activated carbon adsorption abbreviations kf kenaf fiber kac kenaf based activated carbon cr congo red pb lead cu copper 1 introduction water pollution is one of the major threats to human health and the ecosystem organic and inorganic pollutants are discharged in the aquatic environment either in water soluble or insoluble forms al othman et al 2019 ali et al 2018a dyes volatile organic compounds and pesticide are some of the organic pollutants while metals and disinfectant bye products dbp like bromide fall under the category of inorganic pollutants ali et al 2019 ali et al 2018b khan et al 2016 the use of heavy metals and dyes is increasing due to rapid industrialization and globalization alqadami et al 2017 several heavy metals and dyes are non biodegradable and are highly toxic and carcinogenic in nature yin et al 2019 lead pb ii and copper cu ii are two common heavy metals found originating from wastewaters of mining electrical electroplating and smelting alothman et al 2020 mittal et al 2016 long term exposure to lead metal can cause severe damage to the nervous system reproductive system kidney brain and liver alqadami et al 2018 prolonged oral administration of excess quantity of copper may result in renal failure liver damage and acute poisoning to the human body itankar and patil 2021 congo red cr an anionic dye is generated from textiles printing and dyeing paper rubber and plastics industries costa et al 2020a crini 2006 due to its stable structure cr dye is non biodegradable causing serious health issues to humans sud et al 2008 environmental protection agency epa has set the maximum contaminant level mcl standards in drinking water for pb ii and cu ii at 0 015 mg l and 1 3 mg l attallah et al 2013 therefore the remediation of water contaminated by these toxic chemicals is essential to protect the environment and reuse of water biological chemical and physical treatment methods have been applied for the removals of heavy metals and dyes from the contaminated wastewater naushad et al 2019 among various methods of decontamination of polluted water the adsorption technique has been used effectively due to the availability of different adsorbents alothman z a 2012 saleh 2020a simple preparation salimi et al 2017 easy handling jabli et al 2018 saleh 2020b low operational cost high efficiency wang et al 2012 sellaoui et al 2017 and uncomplicated regeneration ability of the adsorbents saleh and ali 2018a sebeia et al 2019 activated carbon ac is widely used as an adsorbent because of its high surface area however the cost the production of toxic sludge and the loss of adsorption efficiency are some well documented restrictions of commercially available activated carbon alqadami et al 2020 therefore there is a need for inexpensive abundant and sustainable materials which are potential sources of activated carbon for the removal of toxic pollutants from the water during the last decade several non conventional biocompatible low cost adsorbents such as iron nanocomposite ali et al 2018c amine modified fibers of populus tremula tka et al 2018 seeds of neem a indica costa et al 2020b grafted carbon microspheres saleh and ali 2018b magnetic hydrochar khan et al 2019 and maghemite nanoparticles afkhami and moosavi 2010 have been explored for the removal of heavy metal ions and dyes from aqueous solution kenaf hibiscus cannabinus a natural cellulosic fiber comprised of cellulose hemicellulose and lignin is widely cultivated in many tropical countries for its fiber content razak et al 2020 kenaf ranks third in biomass production and grows rapidly due to its biodegradability compatibility and regeneration possibility the usage of kenaf has increased for various applications mandal and shi 2020 the presence of functional groups like phenols amines and hydroxyls play a significant role in the adsorption of heavy metals and dyes sajab et al 2011 zhou et al 2020 chowdhury et al 2013 in our present study kenaf core kc was used to produce activated carbon ac by the process of self activation self activation is an endothermic process that uses high temperature 1 000 o c and oxygen free environment to cause the gasification of a biomass source and makes the use of the gases emitted from the thermal treatment to activate the biomass to improve its surface properties xia and shi 2016 thus self activation is a novel technique which does not require any toxic acids and bases for activation several studies have reported the use of low cost adsorbent prepared from harsh chemicals but the utilization of self activated kenaf core fiber without any chemical modification to adsorb dyes and heavy metals has not been reported elsewhere in our studies we have used kenaf based activated kac for the removal of pb ii cu ii as well as cr dye from aqueous solutions batch experiments were conducted to assess the adsorption capacity reaction kinetics and the effects of critical operating parameters for the removal of the above mentioned contaminants using the kac the obtained kac was characterized by various techniques such as ftir esem tga and bet the effect of different factors like contact time ph of the solution adsorbent dosage temperature and concentration on adsorption of metal ions cu ii and pb ii and dye cr were investigated furthermore the data obtained under different operating conditions were fit into different kinetic models and isotherms for the determination of the kinetics and sorption process the results showed that the novel kac had a high prospective for pb ii cu ii and cr adsorption 2 materials and methods 2 1 kenaf core kenaf core was collected from kengro corp charleston ms the metal ions lead nitrate pb no3 2 and copper chloride cucl2 congo red cr dye sodium hydroxide naoh hydrochloric acid hcl was purchased from sigma aldrich st louis mo usa the deionized water 18 2mωcm 1 was used for all experiments 2 2 preparation of the bioadsorbent kenaf based activated carbon kac was prepared by the self activation method xia and shi 2016 in self activation method oxygen free environment activates the kc at a high temperature first ten grams of kc were washed several times in di water and dried overnight in an oven at a temperature of 60 o c ten grams of dried kc were sealed in a box furnace sty 1600c sentro tech corp usa with a type k thermocouple and a data logger tc101a madgetech inc usa and the atmosphere was vacuumed a digital pressure gauge adt680w 25 cp15 psi n additel corp usa was used to measure the pressure 97 906 34 pa a closed system was created by closing the valves of the furnace kac was prepared by the pyrolysis process that was conducted in three steps a heating with a rate of 10 c min 1 up to 1 000 c b dwelling for 2 h and c cooling with a temperature decreasing rate of no more than 10 c min 1 to room temperature the activated carbon was taken out and stored in an airtight container for further use 2 3 adsorbent characterization surface area and pore volume and average pore size of kac were determined by the bet method brunauer emmett teller using n2 adsorption onto the kac using a 3flex 3500 micromeritics instrument corp norcross ga kac was degassed at 150 o c prior to the n2 adsorption thermogravimetric analysis tga q50 ta instruments was performed both for kc and kac in the temperature range from 30 o c to 700 o c with a heating rate of 10 o c min under nitrogen atmosphere by using mettler toledo surface morphology and porosity information of raw kc kac before and after the adsorption was obtained without coating using scanning electron microscope model no fei esem quanta 200 at 20 kv working voltage to ensure a suitable image resolution an energy dispersive x ray spectrometer eds along with esem analysis was also performed to determine the elemental composition of the adsorbent the basic chemical information regarding functional groups presents in the kenaf fiber and kenaf based activated carbon were determined using fourier transform infra red ftir spectrophotometer model no ft ir spectrometer spectrum two perkin elmer inc waltham ma usa the spectrum of each sample was collected at a resolution of 4 cm 1 and 16 interferogram scans in the range of 400 4 000 cm 1 the average bulk density of kac was calculated by the water displacement method the point of zero charges phzpc is a characteristic that determines the ph at which the adsorbent surface has net electrical neutrality and provide valuable information on electrostatic interactions between adsorbent and adsorbate the ph values of the bioadsorbent at the phpzc were obtained using the solid addition method balistrieri and murray 1982 2 4 adsorbates heavy metals lead pb ii copper cu ii and congo red cr all chemicals used in this study were of analytical grade pb ii cu ii and cr were selected as adsorbate in the present study cr is an anionic azo dye with molecular formula and molecular weight as c32h22 n6na2o6s2 and 698 68 respectively the international pure and applied chemistry iupac name of cr is 1 naphthalene sulfonic acid 3 3 4 4 biphenylenebis azo bis 4 aminodisodium salt the chemical structure of cr dye is shown in fig 1 3 experimental 3 1 adsorption studies the standard stock solutions of pb ii and cu ii and cr 1000 mg l were prepared by dissolving the appropriate amount of lead nitrate pb no3 2 and copper chloride cucl2 and congo red cr dye in a liter of deionized water the working solutions were prepared by diluting the stock solution with deionized water to give the appropriate concentration of the working solutions the ph of the solutions was adjusted by the addition of either 0 1 m hcl or 0 1 m naoh respectively all sample bottles and glassware were cleaned and then rinsed with deionized water and oven dried at 60 c 0 05 g of kac was treated with 40 ml of pb ii cu ii and cr dye solutions of 5 10 15 20 25 ppm concentrations the flasks containing the metals and dye solutions with kac were placed on the shaker at 150 rpm for 24 h to ensure equilibrium was reached at 27 o c based on kinetic study 120 min was taken as the time enough to reach the equilibrium state after the specified time suspensions were filtered through filter paper whatman 44 concentrations of pb ii ions and cu ii ions in the filtrates were determined by atomic absorption spectrophotometer model no perkin elmer a analyst 300 the concentration of the residual dye was measured using uv visible spectrometer model no perkin elmer lambda 900 at a λ max corresponding to the maximum adsorption for the dye solution λ max 497 nm the ph value was measured using ph meter sper scientific basic 840 087 a calibration curve was plotted between absorbance and concentration of the dye and the two metals solutions to obtain absorbance concentration profile in order to study the kinetics 50 mg of kac was stirred with 20 mg l of pb ii cu ii and cr solutions from 20 to 120 min at 150 rpm kac in an amount 50 mg and 40 ml of aqueous solution of pb ii cu ii and cr of different concentrations 5 10 15 20 25 mg l were used for plotting the adsorption isotherms for isotherm studies each experiment was conducted at 27 o c for calculating the amount of adsorbent dosage 20 mg l of pb ii cu ii and cr dye solutions were mixed with 30 to 70 mg of kac and stirred for 120 min at 27 o c to examine the effect of ph on the adsorption capacity 20 mg l of pb ii cu ii and cr solutions was stirred for 120 min with 50 mg of kac at ph values ranging from 3 to 8 the adsorption capacity qe mg g at equilibrium and qt mg g at time t was calculated for each sample of cu ii pb ii and cr by the eqs 1 and 2 1 q e c i c e x v m 2 q t c i c t x v m where ci the initial concentration of pb ii and cu ii ions and cr in mg l ct the concentration of pb ii and cu ii ions and cr in mg l at time t v volume of pb ii cu ii ions and cr in l m mass of kac g and ce the concentration of pb ii cu ii and cr at equilibrium the removal percentage r of the pollutant was calculated using eq 3 3 r c i c e c i x 100 4 results and discussions 4 1 physical characterization of kac 4 1 1 bet analysis the physicochemical characterization of kac is listed in table 1 the bet surface area of the kac 843 3 m2 g was much higher than raw kf 3 2 m2 g depending on pore diameters bet pores are classified into three main categories iupac international union of pure and applied chemistry 1972 micropores pore size 2 oa mesoporous pore size 2 50 oa and macropores pore size 50 oa the prepared activated carbon from kenaf is classified as mesoporous type as the average pore diameter of kac is 3 55 nm 4 1 2 thermogravimetric analysis tga the thermal degradation of kc and kac is shown in fig 2 a and b the tga data demonstrated that kc had two major thermal degradations points the first one at 250 o c and the second one at 400 oc the first thermal degradation of kc was observed in temperature range 10 100 o c and there was 10 weight loss this weight loss was due to the evaporation of water molecules and some highly volatile compounds the second thermal degradation around 250 o c established the degradation of hemicellulose and some portion of lignin from the raw kc and indicated that the major portion of cellulose and lignin were pyrolyzed similar observations were made for natural fibers lee et al 2009 it is evident that kac is thermally more stable than kc and the tga data fig 1b exhibited that there was not much significant weight loss of the adsorbent till 550 c however a significant weight loss of the kac was observed beyond 550 o c due to the dissociation of carbon carbon bonds reza and ahmaruzzaman 2015 4 1 3 scanning electron microscope sem analysis an environmental scanning electron microscope esem was used for morphology characterization esem micrographs of raw kf kac and spent kac kenaf based activated carbon after adsorption at different magnification are depicted in fig 3a b and c respectively the surface of raw kenaf fiber kf as shown in fig 3a are highly packed dense uneven and rough with mini pores and cavities however the surface of kf after self activation showed lot of pores due to the removal of cellulose hemicellulose and lignin as revealed from the esem image of fig 3b the pores observed on the surface of kac can easily facilitate the transfer of adsorbates heavy metals and dye into the inner part of the adsorbent particles the eds spectrum of the kac is shown in fig 4 six elements c k o al mg and si are present in the kac where c and k are the dominant elements esem image of fig 3c showed the spent kac the kac surface became brittle and the disappearance of pores in kac after the adsorption clearly demonstrated the successful adsorption of metals ions and the dye 4 1 4 fourier transform infrared spectroscopy ftir ftir was used to identify the functional group of kc kac samples before and after the adsorption of heavy metals kac cu kac pb and dye kac cr ftir spectrum for untreated kc fig 5 showed a broad peak around 3 344 cm 1 which can be ascribed to antisymmetric stretching vibration of oh groups of cellulose as reported in other literature which disappeared in kac kac cr and kac cu karimi et al 2014 the band at 3400 cm 1 for kac pb could be due to the oh stretching of alcohols or presence of n h ali et al 2015 as shown in the ftir fig 5 c h asymmetric stretching vibration produced a medium peak around 2 685 2 680 cm 1 reza and ahmaruzzaman 2015 some bands were observed between 2 380 cm 1 2 375 cm 1 for kac kac cu kac pb and kac cr due to the stretching vibrations of ch2 group the peak observed at 1 727 cm 1 in the spectra of kenaf fiber is related to the co stretching due to the presence of hemicellulose the spectra showed characteristic peak around 1 580 cm 1 for kac kac cu and kac pb fig 5 which is assigned to the cc stretching vibration saleh 2011 a characteristic peak was detected in kf at 1 243 cm 1 that could be associated with the c o stretching of the aryl group in lignin which disappeared in other samples sharp peaks observed in kac pb around 1 200 cm 1 can be attributed to c o stretching of carboxylate or ether structures sharp peaks were observed around 1 030 cm 1 for kc kac pb and kac cu due to c o stretching saleh 2018 some peaks small and sharp were observed for kc kac kac cu kac pb and kac cr between 900 and 500 cm 1 and are assigned to cc bending lafi et al 2019 the formation of new bands and ban shifts after treating pb ii cu ii and cr dye with kac indicated that the corresponding functional groups were involved during the adsorption vs other functional groups that remained unaltered 4 2 batch adsorption 4 2 1 effect of ph the ph value of the solution is an important parameter that controls the process of bioadsorption the ph value not only affects the active sites on the surface of biosorption but also affects the properties of the metal ions as well as dye to investigate the effect of the solution ph on kac the adsorption experiment was conducted at 40 ml of pb ii cu ii and cr solution with a concentration of 20 mg l at 27 o c and 150 rpm ph ranged from 3 to 8 fig 7a the percentage removal of pb ii increased from 52 4 to 62 4 and for cu ii it increased from 52 9 to 60 7 when the ph was varied from 5 0 7 0 and then decreased at higher ph the ph value of the kac phpzc of kac was calculated at 6 2 at ph phpzc due to the presence of hydroxide ions oh the kac surface became negatively charged which enabled the adsorption of metal ions for pb ii and cu ii the maximum removal percentage was observed at ph 6 5 it was assumed that at low ph 5 high concentration of hydronium ions h3o competed with pb ii and cu ii cations resulting in lowering the uptake at lower ph khan et al 2020 at higher ph values 7 there was a decrease in adsorption percentage for pb ii and cu ii which was probably due to the formation of hydroxide of pb ii and cu ii because of chemical precipitation of these cations babaei et al 2016 fig 6a and b shows the change of color of cr dye before and after the addition of kac fig 7a shows that the removal rate and biosorption capacity of cr increased from ph 3 0 to ph 4 0 it reached the maximum at ph 4 0 with the removal percentage of 78 9 which was decreased with the increase of the ph value in acidic solutions h easily combined with the so3 presented in cr and the decolorization effect of the dye was increased by the electrostatic attraction between molecules when the ph value was 7 there was an increase in alkaline conditions and oh increased in the solution there was electrostatic repulsion which led to the decrease of the absorption efficiency similar ph dependent results were reported for the adsorption of cr on activated carbon purkait et al 2007 4 2 2 effect of adsorbent dosage to investigate the effect of adsorbent dosage on pb ii cu ii and cr the adsorbent dose was varied from 0 75 g l to 1 75 g l from fig 7b it was evident that the increase in adsorbent dose increased the removal rate of metal ions and the dye as the concentration of the adsorbent dose was increased from 0 75 g l to 1 75 g l the removal efficiency of pb ii increased from 63 7 to 73 4 removal efficiency of cu ii increased from 54 9 to 64 6 and the removal efficiency of cr dye increased from 73 9 to 80 0 conditions ci 20 mg l adsorbent dosage 50 mg temperature 27 o c agitation speed 150 rpm contact time 2 h ph 6 for pb ii and cu ii ph 4 for cr the probable reason for the increase in efficiency could be due to the presence of more adsorption sites and more adsorbent for the same amount of adsorbate molecules from the result it could be concluded that the optimum dosage of kac for cr and cu ii adsorption was 1 5 g l and 1 75 g l for the pb ii further increase in adsorbent dosage did not provide any higher uptake of cr and cu ii and pb ii very nominal increase was observed indicating that the optimum ratio of adsorbent to adsorbate dosage was attained and all the active sites on the adsorbent dosage were occupied 4 2 3 effect of contact time and initial concentrations the effect of contact time on the performance of kac for the removal of pb ii cu ii and cr was examined in the range of 20 120 min using following parameters temperature 27 o c ci 20 mg l ph 4 adsorbent dose 50 mg the effect of contact time on the removal efficiency of metal ions and the dye is demonstrated in fig 7c the percentage of the removal rate for pb ii cu ii and cr increased as contact time was increased the rate of removal percentage was very rapid at the beginning until 60 min due to the availability of higher number of active functional groups and more vacant sites on kac the maximum removal percentage was observed at 120 min and the equilibrium time was 24 h the removal efficiencies for pb ii increased from 60 6 to 68 3 and 52 6 to 63 5 for cu ii the cr removal efficiency increased from 65 7 78 9 when the contact time was changed from 20 min to 120 min after 100 min the rate became more or less constant reaching the equilibrium at 120 min thus increasing the contact time resulted in a decrease in the availability of dye and heavy metal molecules to active sites on the kac surfaces and reduced the adsorption efficiency the increase in initial metal ions and dye concentration also enhanced the interaction between pb ii cu ii and cr with the kac as seen in fig 7d an increase in initial concentration of pb ii cu ii and cr from 5 ppm to 25 ppm enhanced the adsorption uptake of the pollutants at 120 min the rate of adsorption of cr and metal ions for all concentrations was rapid in the beginning due to the available vacant sites and functional groups on kac two possible reasons for the increase in adsorption capacity when the concentration of the adsorbate was increased i higher mass transfer the driving force that enhanced interactions between cr and the two metal ions ii more adsorbate dye and metals molecules for the adsorption onto kac 4 2 4 effect of temperature to study the effect of temperature on pb ii cu ii and cr removal rate experiments were performed by varying the temperature between 27 o c 57 o c with conditions ci 20 mg l adsorbent dosage 50 mg temperature 27 o c agitation speed 150 rpm contact time 2 h ph 6 for pb ii and cu ii ph 4 for cr the amount of kac was maintained at 0 05 g l and the concentration was 20 mg l the rate of adsorption increased from 68 3 to 74 6 for pb ii 63 5 to 65 1 for cu ii and 78 9 to 83 9 for cr when the temperature was increased from 27 o c to 47 o c as shown in fig 7e the reason could be due to the increase in interparticle diffusion rate in the pores at a higher temperature however the efficiency removal rate for pb ii was noticed to remain constant when the temperature was raised to 57 o c this could be due to the saturation effect slight decrease in removal percentage for cr and cu ii could be due to the decreased interactions between the adsorbent and the adsorbate at higher temperature 4 3 isotherm studies 4 3 1 adsorption kinetics to investigate the kinetics involved in the adsorption of pb ii cu ii and cr the linear form of lagergren s pseudo first order lagergren 1898 was used 4 ln q e q t ln q e k 1 t in eq 4 qe and qt are metals and dye adsorbed per unit weight of activated carbon at equilibrium mg g and at time t k1 min 1 is the rate constants of adsorption process for pseudo first order k1 can be determined from the linear plots between ln qe qt vs t as shown in fig 8a poor correlation coefficients r2 were clearly shown indicating the process did not obey the lagergren s first order model hence the data was modeled using pseudo second order kinetic eq ho et al 2000 5 t q t 1 k 2 q e 2 t q e in eq 5 k2 gm mg 1 min 1 is the rate constants for pseudo second order kinetic models at 27 o ck k2 and qe can be obtained from the intercept and the slope of the plot t qt vs t as shown in fig 8b the correlation coefficient value for pseudo second order r2 for pb ii cu ii and cr were 0 999 0 997 and 0 999 which indicated that the kinetic adsorption fitted well with pseudo second order model as summarized in table 2 the aptness of pseudo second order model to describe the adsorption kinetic data was further confirmed by the comparable qe exp and qe cal values one of the most useful models for describing chemical adsorption is the elovich model and as per the elovich model demiral and güngör 2016 in eq 6 the adsorption sites can increase exponentially and α and β are the equation constants that can be calculated from the slope and intercepts of the linear plots between qt and ln t as shown in fig 8c the elovich model also represents multilayer adsorption table 3 showed the calculated parameters of elovich model for pb ii cu ii and cr at 20 mg l the present kinetic data were well described by the elovich model with a correlation coefficient r2 not less than 93 6 q t 1 β ln αβ 1 β ln t 4 3 2 adsorption isotherm the experimental data for metal ions and dye uptake as well as the homogeneity and heterogeneity by adsorbent were established by langmuir langmuir 1918 freundlich freundlich 1906 and temkin isotherm models temkin and pyzhev 1940 langmuir is based on the generation of adsorbate molecules on homogeneous surface of the adsorbent and the linear form of langmuir model is represented by 7 c e q e 1 k l q max 1 q max c e ce is the equilibrium concentration in mg l of metal ions and dye adsorbed by kac qe is the retention capacity of the metal ions and dye mg g at equilibrium time kl l mg is the langmuir isotherm constant related to the binding energy between the adsorbate metal ions and cr and adsorbent kac qmax is the maximum adsorption capacity of the adsorbent mg g the straight line was obtained when c e q e was plotted against ce as shown in fig 9a the qmax and kl were calculated from the slope and intercepts and summarized in table 4 the dimensionless equilibrium separation parameter rl eq 8 was used to evaluate the appropriateness of the adsorption isotherm 8 r l 1 1 c i k l the calculated value of rl is less than 1 0 13 indicated a positive adsorption the linear form of freundlich model is 9 ln q e ln k f 1 n ln c e where kf is freundlich constant which indicates the capacity ce represent the adsorbate concentrations and qe is the quantity of cu ii pb ii and cr at equilibrium and n is freundlich exponent kf and n were determined from plot ln qe vs ln ce as depicted in fig 9b the n value is often used to indicate the appropriateness of the process of adsorption 1 n 1 suggest a normal process while 1 n 1 indicates a cooperative process the value of 1 n was 0 35 with a favorable pb ii cu ii and cr dye adsorption on the kac temkin isotherm examines the interaction between the adsorbate and the adsorbent and manage a linear decrease in the adsorption energy with surface coverage of the adsorbent and its form as follows 10 q e rt b t lnk t rt b t lnc e where kt is the temkin isotherm equilibrium binding constant in l g r of 8 314 10 3 kj mol k is the universal gas constant t is the absolute temperature and bt is the model constant the heat of absorption in j mol both bt and kt can be determined from the plot qe vs ln ce fig 9c the isotherm parameters for the adsorption of pb ii cu ii and cr are given in table 4 the experimental data fit well with this temkin model thus indicating a surface coverage of the heavy metals and dye on kac by comparing the values of r2 from table 4 we can conclude that the langmuir adsorption model provided the best fit with experimental data theoretical maximum amounts of the metals ions and dye were calculated from langmuir adsorption isotherm qmax and it was very close to the experimentally observed values 4 4 adsorption mechanism based on adsorption kinetics and adsorption isotherm analysis and characterization results the adsorption of pb ii cu ii and cr was a combination of physical adsorption and chemical adsorption and physical adsorption plays a main role in the adsorption process of dye and metal ions the adsorption mechanism can be explained from three aspects 1 the pore structure of kac provided abundant adsorption sites for the adsorbates cr adsorption 2 pb ii cu ii and cr interacted well with the hydroxyl group on the surface of the kac through the h bond 3 electrostatic interaction played an important role in the metals and dye adsorption process 4 5 comparison of different adsorbents and reusability study in table 5 the maximum adsorption capacity qmax of the kac is compared with other bio adsorbents used by other researchers it is observed from the table that the kac has a good adsorption capacity with the other used bio adsorbents the reusability of the green adsorbent kac for 3 cycles using 5 mg l of each of the adsorbate at 27 o c for 120 min contact time and ph 7 was conducted and the results are shown in fig 10 5 conclusions this work reported the application of self activated kac for the removal of pb ii cu ii and cr dye from the wastewater the specific surface area and pore volume of the resulting activated carbon fibers were obtained as 843 4 m2 g and 0 156 cm3 g respectively this research revealed that the adsorption process fitted well with the langmuir isotherm model with r2 0 98 for cu ii while freundlich model fitted well for pb ii and cr with r2 0 99 than that from the langmuir and temkin models the removal of metal ions and cr dye was ph dependent and the optimum ph range was determined as between 4 and 7 the highest percentage removal of pb ii cu ii and cr dye was found to be 92 80 and 95 respectively at a concentration of 5 mg l at 27 o c and 150 rpm agitation speed at 120 min of contact time the optimum adsorbent dosage kac was fixed at 1 5 g l the experimental results demonstrated that kac could be employed successfully as a cost effective benign environmentally friendly adsorbent for the remediation of heavy metals and organic dye on an industrial scale author contributions sujata mandal experiments data analysis and manuscript development jose calderon assistance in data collection of analytical instruments sreekar b marpu supervising reviewing and editing omary a mohammad advising and reviewing sheldon q shi supervising and advising as a major advisor corresponding author author statement dear dr tan thank you so much for giving us the opportunity to revise the manuscript we believe all the reviewers comments have been properly addressed we have uploaded two files i revised manuscript with all changes highlighted in red font ii new manuscript file with all revisions suggested by the reviewers we have added a summary table describing how we responded each comment and attached under the response to reviewers please feel free to let me know if you have any more questions declaration of competing interest the authors declare there is no conflict of interest regarding the publication of this research manuscript acknowledgements this work was supported by the university of north texas toulouse graduate school graduate research award received by s mandal esem was performed at the university of north texas s materials research facility mrf a shared research facility for multi dimensional fabrication and characterization 
1359,the earth texture with complex morphological geometry and compositions such as shale and carbonate rocks is typically characterized with sparse field samples because of an expensive and time consuming characterization process accordingly generating arbitrary large size of the geological texture with similar topological structures at a low computation cost has become one of the key tasks for realistic geomaterial reconstruction and subsequent hydro mechanical evaluation for science and engineering applications recently generative adversarial neural networks gans have demonstrated a potential of synthesizing input textural images and creating equiprobable geomaterial images for stochastic analysis of hydrogeological properties for example the feasibility of co2 storage sites and exploration of unconventional resources however the texture synthesis with the gans framework is often limited by the computational cost and scalability of the output texture size in this study we proposed a spatially assembled gans sagans that can generate output images of an arbitrary large size regardless of the size of training images with computational efficiency the performance of the sagans was evaluated with two and three dimensional 2d and 3d rock image samples widely used in geostatistical reconstruction of the earth texture and lattice boltzmann lb simulations were performed to compare pore scale flow patterns and upscaled permeabilities of training and generated geomaterial images we demonstrate sagans can generate the arbitrary large size of statistical realizations with connectivity and structural properties and flow characteristics similar to training images and also can generate a variety of realizations even on a single training image in addition the computational time was significantly improved compared to standard gans frameworks keywords earth texture synthesis deep learning spatially assembled generative adversarial networks sagans microstructures stochastic modeling 1 introduction earth text features such as pore morphology of geomaterials with small complex pore networks lithological features of shallow crust to the internal structure and surface texture of earth and surface features are crucial for improving our understanding of complex earth processes in the subsurface where ongoing energy related technologies such as unconventional gas and oil recovery geologic storage of co2 and renewable energy recovery are actively optimized and developed realistic synthesis of textural structures including topology and connectivity is key to improving the prediction accuracy of coupled geological hydrological and physical and chemical processes the goal of texture synthesis is often to capture the spatial structural patterns and characteristics from given example images and create many images with similar statistical properties bergmann et al 2017 however the earth geomaterials usually have complex textural properties that are often heterogeneous random characteristics are formally defined as stationary ergodic and stochastic processes bergmann et al 2017 georgiadis et al 2013 sometimes the texture of earth geomaterials with complex topological geometry and compositions such as shale and carbonate rocks usually requires an expensive and time consuming characterization process and the number of available samples for subsequent subsurface reservoir management design is usually small tahmasebi and sahimi 2012 mariethoz and lefebvre 2014 therefore accurate capture and realization of the underlying complex stochastic properties of the geological texture with a limited set of samples has long been an important issue in the earth texture synthesis a variety of methods have been developed for accurate realization of the geological texture for example instance based approaches resample pixels or patches of the original texture image and then copied next to similar image region to generate a seamless bigger texture image bergmann et al 2017 efros and leung 1999 efros and freeman 2001 but these methods cannot generate the textures with complex spatial patterns due to a limit to account for the spatial variability to overcome this limit spatial covariance or variogram analysis was used to quantify spatial dis similarity and continuity these methods have the advantage to quantify the spatial variability in a mathematically tractable way goovaerts 1998 isaaks et al 1989 li et al 2016 however the application of these methods with two point statistics is intrinsically limited to multi gaussian systems and or continuous features which has been limited to describe high order statistics and realistic connectivity patterns li et al 2016 journel 1993 multiple point statistics mps was proposed to overcome these problems by adopting image based approaches where training images tis are used as a basis for sample reconstruction guardiano and srivastava 1993 tis are usually assumed to exhibit the stationarity of the probability distribution of the desired properties and possess higher order mps for reconstructing stochastic random samples caers and zhang 2004 mariethoz et al 2010 mosser et al 2017 although mps algorithms achieved in many successful applications hermans et al 2015 huysmans et al 2014 mahmud et al 2015 michael et al 2010 ding et al 2018 davison et al 2019 they suffer from some limitations inherent to the simulation algorithms these limitations include a computational cost that can be prohibitive for high resolution three dimensional 3d applications the presence of visual artifacts in the model realizations and a low variability between model realizations due to the limited pool of patterns available in a finite size training image li et al 2016 emery and lantuejoul 2014 recently generative models based on deep neural networks have demonstrated remarkable results in terms of image or texture synthesis goodfellow et al goodfellow et al 2014 introduced the generative adversarial neural networks gans gans are semi unsupervised deep learning frameworks for training both a generative model g model for capturing the properties of the training sample and a discriminative model d model for estimating the probability of a image that comes from the training sample rather than g mirza and osindero mirza and osindero 2014 introduced the conditional version of gans cgans which can be constructed by adding external information tags or labels to both training images and the generated images this study showed that it is possible to control the output of generator by conditioning the gans model on additional information cgans have been applied to various research for image synthesis with different conditional contexts such as categorical image generation text to image synthesis and semantic manipulation zhang et al 2019 yang et al 2019 wang et al 2017 radford et al radford et al 2015 introduced a class of convolutional neural networks cnns to develop deep convolutional generative adversarial networks dcgans and proposed a set of architectural constraints on cnns that make dcgans more stable to train in most of settings they applied the architectural constraints to various image samples and demonstrated the applicability of dcgans in image synthesis mosser et al mosser et al 2017 reconstructed the 3d solid void structure of porous media by applying a fully convolutional gan and compared the results with classical stochastic methods they showed that the fully convolutional nature of the dcgans can allow an implicit description of the probability distribution represented by 3d image samples which can be stored and reused to generate multiple realizations of the pore structure very rapidly recently kim et al kim et al 2021 successfully applied dcgans for generating two distinct stochastic properties of drainage networks from gibb s models by accounting for directional connectivity information that represents underlying physical processes of the drainage network i e watershed drainage they showed that training samples with physics informed constraints can significantly improve the statistical properties of complex network connectivity jetchev et al jetchev et al 2016 proposed spatial gans sgans utilizing direct and transposed convolutional layers without fully connected layers although sgans improved computational speed and memory and produced satisfactory results on certain texture images samples sgans are limited to handle a class of textures bergmann et al 2017 sgans strongly depend on the size of the training image and also requires deeper convolutional layers to reproduce a larger projective receptive field for the connectivity of the patterns in tis than the standard gans hence sgans does not make a significant difference in computation efficiency compared to the standard gans despite many applications on image synthesis using the gans framework computational costs are too expensive to generate high resolution 2d and 3d images and the image generation is constrained by the size of training image in this study we propose a spatially assembled gans sagans to improve the computational efficiency and the output image size compared to the standard gans framework the key idea of sagans is that the local probability of output images of the generator is estimated by the discriminator and then assembled into a global probability of the generated images with this architecture sagans can generate the arbitrary large size of statistical realizations with connectivity and structural properties and also can generate a variety of realizations even on a single training image based on the generated images we performed pore scale flow simulations and the simulated flow characteristics are similar to those obtained from the training images the main concepts and advantages of the sagans are presented in the next section followed by the outline of 2d and 3d categorical training image samples and the synthetic experiments with the proposed framework in section 3 the results of synthetic 2d and 3d experiments are then compared to demonstrate the performance of the proposed framework in section 4 finally section 5 concludes with a summary of the important results of this study and outlines possible future developments 2 methodology in this section we briefly introduce generative adversarial neural networks gans and deep convolutional gans dcgans and finally describe spatially assembled gans sagans 2 1 generative adversarial neural networks gans gans are a representative deep generative methods to develop generative models via adversarial model goodfellow et al 2014 goodfellow 2016 they are defined by an implicit description of the underlying data distribution of training images which does not rely on an explicit representation of the probability density the generator represents the ability to draw samples from this implicit density this is in contrast to explicit density models that try to enumerate the probabilistic generative model of the data e g a markov random field or markov chain product gans train two models simultaneously a generative model g that captures the true data generation process from the training sample x and a discriminative model d that determines whether a sample was from either the generative model g i e false or the training sample i e true mosser et al 2017 in this way the generator builds a mapping function g z θ g from a prior noise distribution p g z to learn the distribution of the true data x p g while the discriminator d x θ d gives a single scalar d ℝ n 0 1 representing the probability that the discriminator discriminates the true data x between the true data and the false data by g z θ g by p g goodfellow et al 2014 goodfellow 2016 1 min j d 1 2 e x p data x log d x e z p z z log 1 d g z 2 min j g 1 2 e z p z z log d g z parameters θ d of the d model are adjusted to minimize the mistake j d in distinguishing the real data training sample from the fake created by the generator eq 1 parameters θ g of the g model are adjusted to minimize the log probability j g of the discriminator being correct for the fake images eq 2 in other words the g model is trained to make d g z 1 while the d model is trained to make d g z 0 through this procedure gans can create the reliable samples by the g model confirmed by the d model fig 1 a 2 2 deep convolutional generative adversarial neural networks dcgans the dcgans have been developed to utilize the deep convolutional neural networks dcns in the gans since the representation of the learned data distribution can be stored in convolutional layers which is reused to generate samples the convolutional nature of dcgans allows the generation of many samples with the properties similar to the training sample and computational efficiency in particular radford et al radford et al 2015 proposed the architectural guidelines for dcgans to improve more stable training of gans by adopting and modifying the dcns architectures after extensive model exploration the proposed guidelines include stride convolutions no hidden layer in fully connected net relu and leakyrelu activation function in convolutional layers the stability of gans with different training methods can be found in arora et al 2017 mescheder et al 2018 fig 1 a presents the standard dc gans framework the basic concept of the standard dcgans is that the generator is to map a randomly sampled vector z to a sample x in the image data space close to the training images x and the discriminator gives a scalar of the probability to indicate if the given image is from the training images x or from the generated images x by the generator in training process the standard dcgans framework imposes a constraint on the dimensions of x and x the size of x by the generator should be equal to the size of the training image x since the discriminator is trained with both x and x to produce the scalar of the probability as in fig 1 a in other words heights and widths of the g outputs training images and and d model input should be the same i e h g h x h d w g w x w d as in fig 1 a this feature explains that the architecture of the d model is not free from the g model hence the size of the training images x in the standard dcgans needs to be the same as the size of the synthetically generated images x in the dcgans a randomly sampled vector z given to the g model is reshaped to a set of vector columns z l k d where the subscript represents the dimension of the vector columns through the first hidden fully connected layer note that vector columns z are correlated to each other by construction due to the operation of the first hidden fully connected layer on the randomly generated input vector z then each column of z z 1 1 d generates its piece of field in x which we call in this work the projective field pf lehky and sejnowski 1988 through transposed convolution layers and defines the local property of the training images as these pfs partially overlap each other all of them are highly correlated with each other to ensure generated images with desired patterns and properties e g connected subsurface channels once the training is finished consequently the change of a pf affects all pixels of x hence any small patches on the image produced by the trained g model will satisfy the local statistical properties learned from the training images for reasonable geomaterial image generation in the next subsection we discuss the key idea of the sagans framework which is extended from dcgans for arbitrary larger size image generation 2 3 spatially assembled gans sagans our newly proposed method have different implementation details from the standard dc gans in the standard gans the discriminator d evaluates the probability of an image sampled from the true data generation process i e the training images x or from the generated images x thus the standard gans are not suitable to creating arbitrary size of images that honors the spatial characteristics of training images on the other hand sagans are designed to evaluate the probability of an image with any arbitrary size originated from the true data generation process based on the subsets segments of the training images x and the segments x of generated image x as in fig 1 b in specific sagans optimizes the generator and the discriminator by investigating the segments of x and x 3 g z x 1 x 2 x n where x i 1 n x i x x 1 x 2 x n 4 min j d 1 2 n i 1 n e x p data x log d x i e z p z z log 1 d x i 5 min j g 1 2 n i 1 n e z p z z log d x i 1 2 n e z p z z log d x 1 d x 2 d x n where x is a generated image by the generator n is the number of the segmented images from the generated image and the training image x i is the i th segmented image of x x i is the i th segmented image subsampled from the training image x and represents an operation combining the image segments to produce an entire image in sagans each segment is designed to contain multiple pfs to ensure the smoothness and connectivity of the generated whole image to illustrate this we present in fig 1 c schematics of g model output with corresponding pfs as red yellow and green boxes which are the pieces of the generated image corresponding to each column of z the red pf describes local spatial patterns within one segment while yellow and green pfs are attributed to two adjacent segments for yellow and green pfs a column of z is trained to generate a part of the whole image that satisfies the desired properties within two segments as these pfs partially overlap each other all of them are highly correlated with each other and a change of one pf affects all pfs i e the whole image since the loss function in sagans in eq 5 is the product of the local probabilities the local probability from each segment contains the information of multiple pfs and some pfs are divided into multiple segments each pf has the local property of the segments where each pf is located hence the product of local probabilities will be minimized in a way that connected information can be learned through the multiple overlapped pfs during training process in this way each local probability of generated segmented images x is evaluated by the discriminator d and the generator g is optimized to generate seamless images with the local features exhibited from the training data set this local segment likelihood evaluation approach can be viewed as maximizing one single global likelihood function as in the right hand side of eq 5 this simple but notable difference enables the generator g to capture the object structure implicitly and synthesize the arbitrary size of images that do not depend on the size of the training samples regardless of the size of images created by the generator the architecture of the discriminator can be determined depending on the size of the segmented images i e the inputs of a d model thus the architecture of the discriminator becomes free from the generator as shown in fig 1 b this subsequently reduces the computational cost significantly compared to the standard dc gans note that unlike the standard gans sagans can create plausible realizations with a few training samples or even a single training sample e g tahmasebi and sahimi 2012 which we will demonstrate in the next section 3 development of sagans and dcgans 3 1 training images tis in this study three ti samples widely used for geostatistical simulation tests were selected to compare the performance of sagans with dcgans for large size image generations and computational efficiency three samples include 2d strebelle s images strebelle 2002 2d and 3d spherical beadpack and 3d categorical fold aquifer as shown in fig 2 these samples represent relatively simple geological structures but have been proven to be very challenging as the training image ti for the mps and gans mosser et al 2017 strebelle 2002 specifically strebelle s tis and fold aquifer consist of continuous channels or pore space that are connected in various patterns while spherical beadpack patterns are packing of the different size of the spherical particles beads which are randomly distributed without being overlapped the realization of the discrete nature of beadpack is also a challenge due to the difficulty of applying gans for discrete data in this study we focus on how well sagans and dcgans can generate the various long range connectivity of channels in strebelle s tis and fold aquifer and the spatial distribution and shapes of beads in the spherical beadpack taking into account the computational time of the experiments and the ease of handling the samples all tis are resized into binary 2d squares and 3d cubes with the representation of the pore space or channel white and grain solid structure black the tis in fig 2 a and b are used for the 2d realization and fig 2 c and d are used for the 3d realization 3 2 architecture and training of gans in this study we constructed the architecture of deep convolution neural networks for sagans and dcgans based on the guidelines proposed by radford et al radford et al 2015 radford et al radford et al 2015 identified a family of architectures of deep convolution for stable training across a range of samples and higher resolution and deeper generative models main architectural features adopted from radford et al radford et al 2015 are the following 1 stride convolutions instead of any pooling layers 2 no hidden layers in a fully connected net fcn in both generator and discriminator 3 relu activation in the generator for all layers except for the output that uses tanh and 4 leakyrelu activation in the discriminator for all layers except for the output that uses sigmoid architecture and hyperparameters used in this study are presented in table 1 the d model is composed of two convolution layers with three or five of kernel size and the output was converted to the probability 0 false to 1 true using the sigmoid activation function in the g model the noise z with a dimension of 100 was drawn from a gaussian distribution and given to the fcn the fcn reshaped z to a set of tensor z before passed to the convolutional layers the deconvolution layers with the same kernel size in the d model was used and the output is converted by a stride convolution layer into the same size image as the given training images or segments both d and g models were applied and trained by the adam adaptive momentum estimation optimization algorithm with a learning rate of 0 0002 with the momentum β 1 of 0 5 and a total of 100 000 epochs dropout with a probability of 0 25 was applied to the convolution layers of the d model the loss of gans was estimated by binary cross entropy function eqs 1 and 4 the different size pixel and voxel resolution of images from the tis are synthesized by dcgans and sagans depending on the synthesized size 3 to 5 depth of convolutional layers and 4 to 128 channels filters per a convolutional layer were applied a larger number of epochs with a small learning rate and dropout on the d model and batch normalization were applied for the stabilized training and the convergence of both g and d models that have the symmetric architecture all computational works in this study were performed using the same computer equipped with two nvidia titan v gpu cards 5120 cores 12g gpu memory and intel i9 9900x cpu 10 cores with 64g ram 3 3 evaluation of the synthesis results two point probability and two point cluster functions were used to evaluate the texture image synthetic performance of both sagans and dcgans in this study the two point probability function s 2 estimates the probability of how two points pixels for 2d and voxels for 3d digitized images are related to each other which is often called auto correlation s 2 is used for structural information about the degree of spatial dependence of a random field 6 s 2 r p x φ x r φ for x r ℝ n where p is the probability that two points x and x r are separated by the lag vector r in the same phase of interest φ if the two points coincide r 0 in the phase φ the probability is the same as the probability that one point is the phase φ which is simply equal to the volume fraction φ of the phase φ 7 φ v ϕ v total s 2 r 0 φ when the two points are completely uncorrelated and far apart each point has a probability φ to be the phase φ 8 s 2 r φ 2 s 2 decays from φ towards the asymptotic value of φ 2 two point cluster function c 2 estimates the probability that two points are in the same cluster of the phase of interest when the image sample consists of some isolated clusters c 2 is used for representing the statistically homogeneous distributions of identical inclusions torquato 2013 this connectivity function is defined as follows 9 c 2 r p c x c x r for x φ i where c is an indicator of the cluster in φ i 4 results and discussion 4 1 2d 3d earth texture image synthesis in this section we compared the results obtained from sagans with those from dcgans using the 2d 3d tis presented in fig 2 first we consider a simple example using the strebelle s ti to illustrate the effectiveness of the proposed method the single strebelle s ti consists of connected channels white color representing the long range connectivity and pattern diversity several images generated from dcgans and sagans are plotted as in fig 3 notably the dcgans produced the same realization of images as the ti while the sagans produced different realizations with various patterns visually similar to those in the strebelle s ti this is mainly due to the architecture of dcgans and the lack of diverse training data the d model in the dcgans determines whether the image created by the g model is the same as the ti true or not fake then the g model in the dcgans is trained to generate exactly the same image as the given single ti resulting in the same realization as the ti fig 3 a since in many subsurface data survey only a limited number of geomaterial images would be available e g yoon and dewers 2013 direct application of dcgans to geostatistical simulation would not be satisfactory in contrast to the dcgans the image generated by the g model in the sagans is divided into several segments and then each segment is compared with the same size of segments extracted from a single ti fig 1 b in this example both generated image and the ti were divided into four segments as in fig 3 b each segment has a size of 130 by 130 with 4 pixels overlapped vertically and horizontally to account for the long range connectivity the arrangement of the segments from the ti and the structures in place build a pool of patterns available the architecture of the sagans can capture the object structure implicitly from the segments through hierachical assembly between pf and segments hence the sagans can synthesize the various pattern of textures with statistical properties e g the long range connectivity even using a single ti although dcgans can synthesize the various textures using the multiple segments extracted from a single image as ti like the way sagans extract information from the ti fig 4 a without any architecture or training image modifications dcgans cannot generate the smoothly connected channels properly since the seam between the combined multiple segments in the training images is also trained as a pattern fig 4 b in the next sections we evaluate the quality of generated samples in terms of similarities in spatial and flow characteristics 4 1 1 spatial similarities to compare the similarities of spatial characteristics between the generated image and the ti two directional metrics s 2 and c 2 in sec 3 3 for the pore phase channels in white of twenty generated images were calculated as shown in fig 5 note that s 2 and c 2 of the realizations by dcgans are identical to those of the ti original line in fig 5 since the same realizations as a single ti were always produced on the other hand the realizations by sagans show a range of fluctuation in s 2 and c 2 a shade in red in fig 5 but show similar average values red line to those of from the ti they also have the similar volume fraction φ porosity of 0 29 compared to 0 27 of the ti this result indicates that sagans can generate statistically similar images with various patterns even using a single ti our second 2d realization uses a total of spherical beadpack tis fig 2 a for the texture synthesis on the spatial distribution and shapes of the particles structural properties fig 6 shows some examples of the realizations on the the spherical beadpack tis four segments with a quarter size of the generated image and the tis were used in sagans beads in the realizations by sagans have more spherical shape and less overlapped with each other well spread than those by dcgans fig 6 b both dcgans and sagans achieve a small error in the averaged values of s 2 and c 2 for each twenty generated images fig 7 with the sagans slightly better the dcgans in contrast to the first case using a single ti the second case demonstrates that with a large number of tis both dcgans and sagans are able to the images with the basic statistics of the tis in the 3d case we used spherical beadpack tis and fold aquifer tis fig 2 c d similar to the 2d cases we evaluated the performance of sagans and dcagns on the 3d realization of the long range connectivity and the spatial distribution of the particles fig 8 shows examples of the 3d realizations by both dcgans and sagans to compare the similarity of the 3d realizations the s 2 over the radial lag distance was calculated and normalized for the randomly selected 20 realizations the c 2 values are not reported since they are very identical to the s 2 statistics comparison of the normalized isotropic s 2 between the realizations and tis are shown in fig 9 since the single 3d ti was used the dcgans generate the same realization as the ti hence the s 2 of tis is identical to one of the dcgans as in the first 2d realization case the s 2 of the sagans shows a relatively small error compared to the tis and a small variation indicating that the sagans can produce the 3d realizations with diversity even using the single 3d ti 4 1 2 pore scale flow path and upscaled permeability similarities in this section we evaluate the quality of generated samples by examining simulated flow patterns on both ti and samples and corresponding upscaled permeabilities over the simulation domain in other words tis and the generated images by sagans and dcgans were directly used as the porous media simulation domain where the stokes equation was solved and upscaled permeability values were computed comparison of flow patterns and upscaled permeabilities from tis and generated images would provide effective measures on how similar the generative methods reproduce the spatial features in earth materials related to pore scale flow paths e g yoon and dewers 2013 along with the spatial similarities as shown in section 4 1 1 only the 2d beadpack and stebelle s ti cases were considered here due to computationally expensive pore scale flow simulations to accelerate the flow simulation in each generated image an open source parallel lattice boltzmann lb solver palabos latt et al 2020 was used to solve the stokes equation the lb based methods can treat complex geometries and boundaries of porous media easily and provide both computationally efficient and reasonably accurate simulations of pore scale flow velocities yoon et al 2015 a no slip boundary condition at the solid fluid interface using the bounce back condition is assumed with a constant pressure boundary condition on the inlet left and outlet right of the domain and no flux on top and bottom of the domain with darcy s law under the laminar flow regime each directional permeability was computed using the total flux and the pressure gradient over the length of sample detailed configurations for the lb method can be found in yoon and dewers yoon and dewers 2013 and yoon et al yoon et al 2015 randomly chosen 100 beadpack tis single strebelle s ti 20 realizations from sagan for the beadpack and strebelle cases and 20 realizations from dcgan for the beadpack case as shown in section 4 1 1 were used for pore scale simulations since the generated images from dcgan for the single strebelle ti case were converged to the ti by construction dcgan results for the strebelle ti case were not included in the flow simulation as in our previous microfluidic simulations in yoon et al yoon et al 2019 and park et al park et al 2021 we constructed our simulation domains into 3d domain 15 pixels in depth and the entire simulations of training and generated images took about 3 days with 6 simulations in parallel using 48 cores of intel xeon platinum 8180 cpu 2 5 ghz per each simulation the computed permeability values for the generated images were normalized by the average upscaled permeability of the ti s for effective comparison in fig 10 the distributions of the normalized upscaled permeabilities for two 2d cases are plotted for the beadpack example it is shown that sagans produced the permeability fields similar to the tis while realizations from dcgans overestimated the upscaled permeability about 40 the strebelle example in fig 10 b also illustrates the effectiveness of the proposed method the mean of the generated permeabilities is close to the ti while the diversity between samples is preserved so that the proposed method can be used for the stochastic analysis of earth materials in addition we show the velocity magnitudes in the simulated flow paths on the domain of the tis and generated images from sagans and dcgans in fig 11 for 2d beadpack example the improved reconstruction quality from sagans leads the flow patterns similar to those in beadpack tis while dcgans tendency to create bead aggregates slightly deviates flow patterns from the expected ones from tis resulting in the overestimation of the upscaled permeabilities as shown in fig 10 b for the strebelle example sagans tends to generate the dead end channels more than the ti but as shown in fig 10 b the strebelle ti s upscaled permeability is still within the range of generated permeabilities the generation of dead end channels was expected since the flow wise connectivity over the entire domain is hard to learn from a limited number of training images i e the single strebelle s ti in this case during the training without further information arora et al 2017 this issue can be alleviated by including more training images or architecture or loss function wise constraints 4 2 scalability and computational efficiency in this section we focus on the scalability in terms of the output texture size and computational efficiency of the sagans as explained previously the size of model output is not limited by the ti size in the sagans hence we tested this conceptual advance with a smaller size of the images extracted from the tis to generate a large size of image fig 12 a b shows examples of the arbitrary large size of 2d realization by the sagans in the realization of larger size of 2d spherical beadpack 512 512 pixels by sagans the created image of 512 512 pixels were divided into four segments of 256 256 pixels and then the probability of each segment was evaluated with the segments of 256 256 pixels from beadpack tis for the realization of larger size of strebelle s ti based channels 1 040 528 pixels 32 segments of 144 144 pixels divided with overlapping 16 pixels in the created image 1 040 528 pixels were evaluated with the segments of 144 144 pixels from 256 256 pixels of strebelle s ti in the fig 12 a b the spherical beads and the long range connectivity can be verified visually the larger size images can be generated by dcgans in the same way shown as in fig 4 a however the realizations show the seam between the combined original tis as the realizations in figs 4 b and 12 c in addition building the large size of tis from the small size tis requires considerable time and computational resources for reading and allocating the data in the memory to train the gans especially in building 3d tis hence the dcgans have the limits in generating the arbitrary large size of images due to the cropping area or the seam created by connecting multiple images from the original tis on the other hand sagans produce the arbitrary large seamless realizations with simply changing the dimension of z in the generator g even in a sing ti the size and positions of segments also can be arbitrarily adjusted according to the size of the image to be generated for the characteristics of the original tis fig 13 shows examples of the arbitrary large size of 3d realizations from a single of ti by sagans in the realization of 256 256 320 size spherical beadpack 3d segments of the 64 voxels were applied for the realization of 256 256 256 size fold aquifer ti 3d segments of the 64 voxels overlapping by 3 voxels were applied considering the long range connection the s 2 graph on the twenty realizations by sagans shows a small error to the graph on tis and with some variations this indicates that sagans could produce the 3d realizations of the arbitrary larger size and with diversity even in a single of 3d ti dcgans could not generate larger images than the original ti size larger than 256 voxels of beadpack larger than 128 voxels of fold aquifer due to the lack of tis and gpu memory limitations as mentioned above the size of the generated output in the g model should be identical to the size of the input of d model in the standard gans dcgans framework therefore the larger the output is generated the larger architecture of d model is required at the same time which increases the computational load the d model runs not only when the d model is trained but when the g model is trained consequently this architectural constraint increases both the computational load and computational time in training the gans significantly however in the sagans the architecture of the d model is free from the g model the size of the inputs of a d model can be set relatively small regardless of the size of the output generated by the g model therefore sagans can produce the larger size realizations with the low computational cost table 2 shows the training time per 1000 epoch in each case the symmetrical architecture for g and d model was applied with the same parameters table 1 the difference between dcgans and sagans in each case is the size of the inputs the size of segment given to the d model the results in table 2 show that the training time of the sagans is less than the training time of the dcgans and the decrease is larger as the size of generated outputs increases fig 13 and table 2 indicate that the texture synthetics with the sagans significantly improve the increase in the image size of realizations and the computational time for high dimensional applications compared to the standard gans framework the detailed list of dcgan and sagan architecture and hyperparameters of each test can be found in the appendix 5 conclusions this study proposed the new gans framework spatially assembled gans sagans that can generate output images of an arbitrary large size regardless of the size of training images with computational efficiency the proposed method can support various hydrogeological applications with a limited number of geological samples and enables subsequent stochastic analyses by multi phase flow and transport simulations directly on the generated geomaterial images the sagans have no architectural constraint in contrary to the standard gans dcgans framework where the size of the generated output image in the g model should be identical to the size of the input image of d model this enables the sagans to produce the multiple realizations with the scalability in terms of the output size and computational efficiency the performance of the sagans for generating the images was evaluated using widely used 2d and 3d earth texture image samples with two point probability two point cluster function and upscaled permeability through single phase flow simulations both sagans and dcgans generated the texture images close to the basic statistics morphological characteristics and flow patterns of the tis such as porosity long range connectivity the spatial distribution and shapes and upscaled permeability however the dcgans could not produce various patterns of the realizations when a single of ti is given and could not produce the arbitrary large size of realizations due to the cropping area or the seam created by combining the multiple original tis in addition the dcgans could not produce large size of 3d realization due to the lack of computational memory however the sagans improve these limitations to produce large seamless realizations with a variety of patterns even using a single ti the sagans also produced the larger size of 3d realizations with a low computational time and load so that the applications for high dimensional problems are now achievable the sagans can be applied in various ways depending on how to adjust the size and positions of segments for example if a specific field in generated images by the g model is evaluated by the same segment from tis the part of all the generated images has the same property as in the specific segment in this way a specific part of the texture image can be synthesized if various segments from tis with quite different features are used the sagans can produce the realizations of a non ergodic stochastic process which will enable us to generate the composite texture that can be used in hydro mechanical applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the laboratory directed research and development program at sandia national laboratories sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government training image samples used in this study can be downloaded from http www trainingimages org training images library html source code will be available in https github com saint kim and an experimental version is available upon request to sung eun kim saint kse gmail com appendix a we added sagan architecture and hyperparameters of each test cases used in this paper table a 1 hyperparameters and architecture of the networks used in this paper table a 1 case model ti z of filter per layer generated size size of kernel stride name size g model d model 2d dcgan strebelle 1 256 256 32 32 64 32 16 16 32 64 256 256 5 52 2 100 128 2 128 2 sagan 100 130 130 1 256 256 33 65 128 64 32 16 16 32 64 128 528 1040 dcgan beadpack 256 256 256 32 32 64 32 16 16 32 64 256 256 256 128 2 128 2 64 64 512 512 sagan 256 256 256 32 32 256 256 256 64 64 128 64 32 32 64 128 512 512 3d dcgan beadpack 1 256 256 256 4 4 4 64 32 16 8 4 4 4 4 8 16 32 64 256 256 256 3 3 32 2 2 100 128 128 128 8 8 8 64 32 16 8 8 16 32 64 128 128 128 sagan 1 256 256 256 8 8 8 64 32 16 8 4 4 8 16 32 64 256 256 256 1 64 32 16 8 8 16 32 64 128 128 128 1 8 8 10 64 32 16 8 4 4 8 16 32 64 256 256 320 dcgan fold aquifer 1 128 128 128 8 8 8 64 32 16 8 8 16 32 64 128 128 128 100 64 64 64 64 32 16 16 32 64 64 64 64 sagan 1 128 128 128 64 32 16 8 8 16 32 64 128 128 128 1 64 32 16 16 32 64 64 64 64 1 64 32 16 8 4 4 8 16 32 64 256 256 256 
1359,the earth texture with complex morphological geometry and compositions such as shale and carbonate rocks is typically characterized with sparse field samples because of an expensive and time consuming characterization process accordingly generating arbitrary large size of the geological texture with similar topological structures at a low computation cost has become one of the key tasks for realistic geomaterial reconstruction and subsequent hydro mechanical evaluation for science and engineering applications recently generative adversarial neural networks gans have demonstrated a potential of synthesizing input textural images and creating equiprobable geomaterial images for stochastic analysis of hydrogeological properties for example the feasibility of co2 storage sites and exploration of unconventional resources however the texture synthesis with the gans framework is often limited by the computational cost and scalability of the output texture size in this study we proposed a spatially assembled gans sagans that can generate output images of an arbitrary large size regardless of the size of training images with computational efficiency the performance of the sagans was evaluated with two and three dimensional 2d and 3d rock image samples widely used in geostatistical reconstruction of the earth texture and lattice boltzmann lb simulations were performed to compare pore scale flow patterns and upscaled permeabilities of training and generated geomaterial images we demonstrate sagans can generate the arbitrary large size of statistical realizations with connectivity and structural properties and flow characteristics similar to training images and also can generate a variety of realizations even on a single training image in addition the computational time was significantly improved compared to standard gans frameworks keywords earth texture synthesis deep learning spatially assembled generative adversarial networks sagans microstructures stochastic modeling 1 introduction earth text features such as pore morphology of geomaterials with small complex pore networks lithological features of shallow crust to the internal structure and surface texture of earth and surface features are crucial for improving our understanding of complex earth processes in the subsurface where ongoing energy related technologies such as unconventional gas and oil recovery geologic storage of co2 and renewable energy recovery are actively optimized and developed realistic synthesis of textural structures including topology and connectivity is key to improving the prediction accuracy of coupled geological hydrological and physical and chemical processes the goal of texture synthesis is often to capture the spatial structural patterns and characteristics from given example images and create many images with similar statistical properties bergmann et al 2017 however the earth geomaterials usually have complex textural properties that are often heterogeneous random characteristics are formally defined as stationary ergodic and stochastic processes bergmann et al 2017 georgiadis et al 2013 sometimes the texture of earth geomaterials with complex topological geometry and compositions such as shale and carbonate rocks usually requires an expensive and time consuming characterization process and the number of available samples for subsequent subsurface reservoir management design is usually small tahmasebi and sahimi 2012 mariethoz and lefebvre 2014 therefore accurate capture and realization of the underlying complex stochastic properties of the geological texture with a limited set of samples has long been an important issue in the earth texture synthesis a variety of methods have been developed for accurate realization of the geological texture for example instance based approaches resample pixels or patches of the original texture image and then copied next to similar image region to generate a seamless bigger texture image bergmann et al 2017 efros and leung 1999 efros and freeman 2001 but these methods cannot generate the textures with complex spatial patterns due to a limit to account for the spatial variability to overcome this limit spatial covariance or variogram analysis was used to quantify spatial dis similarity and continuity these methods have the advantage to quantify the spatial variability in a mathematically tractable way goovaerts 1998 isaaks et al 1989 li et al 2016 however the application of these methods with two point statistics is intrinsically limited to multi gaussian systems and or continuous features which has been limited to describe high order statistics and realistic connectivity patterns li et al 2016 journel 1993 multiple point statistics mps was proposed to overcome these problems by adopting image based approaches where training images tis are used as a basis for sample reconstruction guardiano and srivastava 1993 tis are usually assumed to exhibit the stationarity of the probability distribution of the desired properties and possess higher order mps for reconstructing stochastic random samples caers and zhang 2004 mariethoz et al 2010 mosser et al 2017 although mps algorithms achieved in many successful applications hermans et al 2015 huysmans et al 2014 mahmud et al 2015 michael et al 2010 ding et al 2018 davison et al 2019 they suffer from some limitations inherent to the simulation algorithms these limitations include a computational cost that can be prohibitive for high resolution three dimensional 3d applications the presence of visual artifacts in the model realizations and a low variability between model realizations due to the limited pool of patterns available in a finite size training image li et al 2016 emery and lantuejoul 2014 recently generative models based on deep neural networks have demonstrated remarkable results in terms of image or texture synthesis goodfellow et al goodfellow et al 2014 introduced the generative adversarial neural networks gans gans are semi unsupervised deep learning frameworks for training both a generative model g model for capturing the properties of the training sample and a discriminative model d model for estimating the probability of a image that comes from the training sample rather than g mirza and osindero mirza and osindero 2014 introduced the conditional version of gans cgans which can be constructed by adding external information tags or labels to both training images and the generated images this study showed that it is possible to control the output of generator by conditioning the gans model on additional information cgans have been applied to various research for image synthesis with different conditional contexts such as categorical image generation text to image synthesis and semantic manipulation zhang et al 2019 yang et al 2019 wang et al 2017 radford et al radford et al 2015 introduced a class of convolutional neural networks cnns to develop deep convolutional generative adversarial networks dcgans and proposed a set of architectural constraints on cnns that make dcgans more stable to train in most of settings they applied the architectural constraints to various image samples and demonstrated the applicability of dcgans in image synthesis mosser et al mosser et al 2017 reconstructed the 3d solid void structure of porous media by applying a fully convolutional gan and compared the results with classical stochastic methods they showed that the fully convolutional nature of the dcgans can allow an implicit description of the probability distribution represented by 3d image samples which can be stored and reused to generate multiple realizations of the pore structure very rapidly recently kim et al kim et al 2021 successfully applied dcgans for generating two distinct stochastic properties of drainage networks from gibb s models by accounting for directional connectivity information that represents underlying physical processes of the drainage network i e watershed drainage they showed that training samples with physics informed constraints can significantly improve the statistical properties of complex network connectivity jetchev et al jetchev et al 2016 proposed spatial gans sgans utilizing direct and transposed convolutional layers without fully connected layers although sgans improved computational speed and memory and produced satisfactory results on certain texture images samples sgans are limited to handle a class of textures bergmann et al 2017 sgans strongly depend on the size of the training image and also requires deeper convolutional layers to reproduce a larger projective receptive field for the connectivity of the patterns in tis than the standard gans hence sgans does not make a significant difference in computation efficiency compared to the standard gans despite many applications on image synthesis using the gans framework computational costs are too expensive to generate high resolution 2d and 3d images and the image generation is constrained by the size of training image in this study we propose a spatially assembled gans sagans to improve the computational efficiency and the output image size compared to the standard gans framework the key idea of sagans is that the local probability of output images of the generator is estimated by the discriminator and then assembled into a global probability of the generated images with this architecture sagans can generate the arbitrary large size of statistical realizations with connectivity and structural properties and also can generate a variety of realizations even on a single training image based on the generated images we performed pore scale flow simulations and the simulated flow characteristics are similar to those obtained from the training images the main concepts and advantages of the sagans are presented in the next section followed by the outline of 2d and 3d categorical training image samples and the synthetic experiments with the proposed framework in section 3 the results of synthetic 2d and 3d experiments are then compared to demonstrate the performance of the proposed framework in section 4 finally section 5 concludes with a summary of the important results of this study and outlines possible future developments 2 methodology in this section we briefly introduce generative adversarial neural networks gans and deep convolutional gans dcgans and finally describe spatially assembled gans sagans 2 1 generative adversarial neural networks gans gans are a representative deep generative methods to develop generative models via adversarial model goodfellow et al 2014 goodfellow 2016 they are defined by an implicit description of the underlying data distribution of training images which does not rely on an explicit representation of the probability density the generator represents the ability to draw samples from this implicit density this is in contrast to explicit density models that try to enumerate the probabilistic generative model of the data e g a markov random field or markov chain product gans train two models simultaneously a generative model g that captures the true data generation process from the training sample x and a discriminative model d that determines whether a sample was from either the generative model g i e false or the training sample i e true mosser et al 2017 in this way the generator builds a mapping function g z θ g from a prior noise distribution p g z to learn the distribution of the true data x p g while the discriminator d x θ d gives a single scalar d ℝ n 0 1 representing the probability that the discriminator discriminates the true data x between the true data and the false data by g z θ g by p g goodfellow et al 2014 goodfellow 2016 1 min j d 1 2 e x p data x log d x e z p z z log 1 d g z 2 min j g 1 2 e z p z z log d g z parameters θ d of the d model are adjusted to minimize the mistake j d in distinguishing the real data training sample from the fake created by the generator eq 1 parameters θ g of the g model are adjusted to minimize the log probability j g of the discriminator being correct for the fake images eq 2 in other words the g model is trained to make d g z 1 while the d model is trained to make d g z 0 through this procedure gans can create the reliable samples by the g model confirmed by the d model fig 1 a 2 2 deep convolutional generative adversarial neural networks dcgans the dcgans have been developed to utilize the deep convolutional neural networks dcns in the gans since the representation of the learned data distribution can be stored in convolutional layers which is reused to generate samples the convolutional nature of dcgans allows the generation of many samples with the properties similar to the training sample and computational efficiency in particular radford et al radford et al 2015 proposed the architectural guidelines for dcgans to improve more stable training of gans by adopting and modifying the dcns architectures after extensive model exploration the proposed guidelines include stride convolutions no hidden layer in fully connected net relu and leakyrelu activation function in convolutional layers the stability of gans with different training methods can be found in arora et al 2017 mescheder et al 2018 fig 1 a presents the standard dc gans framework the basic concept of the standard dcgans is that the generator is to map a randomly sampled vector z to a sample x in the image data space close to the training images x and the discriminator gives a scalar of the probability to indicate if the given image is from the training images x or from the generated images x by the generator in training process the standard dcgans framework imposes a constraint on the dimensions of x and x the size of x by the generator should be equal to the size of the training image x since the discriminator is trained with both x and x to produce the scalar of the probability as in fig 1 a in other words heights and widths of the g outputs training images and and d model input should be the same i e h g h x h d w g w x w d as in fig 1 a this feature explains that the architecture of the d model is not free from the g model hence the size of the training images x in the standard dcgans needs to be the same as the size of the synthetically generated images x in the dcgans a randomly sampled vector z given to the g model is reshaped to a set of vector columns z l k d where the subscript represents the dimension of the vector columns through the first hidden fully connected layer note that vector columns z are correlated to each other by construction due to the operation of the first hidden fully connected layer on the randomly generated input vector z then each column of z z 1 1 d generates its piece of field in x which we call in this work the projective field pf lehky and sejnowski 1988 through transposed convolution layers and defines the local property of the training images as these pfs partially overlap each other all of them are highly correlated with each other to ensure generated images with desired patterns and properties e g connected subsurface channels once the training is finished consequently the change of a pf affects all pixels of x hence any small patches on the image produced by the trained g model will satisfy the local statistical properties learned from the training images for reasonable geomaterial image generation in the next subsection we discuss the key idea of the sagans framework which is extended from dcgans for arbitrary larger size image generation 2 3 spatially assembled gans sagans our newly proposed method have different implementation details from the standard dc gans in the standard gans the discriminator d evaluates the probability of an image sampled from the true data generation process i e the training images x or from the generated images x thus the standard gans are not suitable to creating arbitrary size of images that honors the spatial characteristics of training images on the other hand sagans are designed to evaluate the probability of an image with any arbitrary size originated from the true data generation process based on the subsets segments of the training images x and the segments x of generated image x as in fig 1 b in specific sagans optimizes the generator and the discriminator by investigating the segments of x and x 3 g z x 1 x 2 x n where x i 1 n x i x x 1 x 2 x n 4 min j d 1 2 n i 1 n e x p data x log d x i e z p z z log 1 d x i 5 min j g 1 2 n i 1 n e z p z z log d x i 1 2 n e z p z z log d x 1 d x 2 d x n where x is a generated image by the generator n is the number of the segmented images from the generated image and the training image x i is the i th segmented image of x x i is the i th segmented image subsampled from the training image x and represents an operation combining the image segments to produce an entire image in sagans each segment is designed to contain multiple pfs to ensure the smoothness and connectivity of the generated whole image to illustrate this we present in fig 1 c schematics of g model output with corresponding pfs as red yellow and green boxes which are the pieces of the generated image corresponding to each column of z the red pf describes local spatial patterns within one segment while yellow and green pfs are attributed to two adjacent segments for yellow and green pfs a column of z is trained to generate a part of the whole image that satisfies the desired properties within two segments as these pfs partially overlap each other all of them are highly correlated with each other and a change of one pf affects all pfs i e the whole image since the loss function in sagans in eq 5 is the product of the local probabilities the local probability from each segment contains the information of multiple pfs and some pfs are divided into multiple segments each pf has the local property of the segments where each pf is located hence the product of local probabilities will be minimized in a way that connected information can be learned through the multiple overlapped pfs during training process in this way each local probability of generated segmented images x is evaluated by the discriminator d and the generator g is optimized to generate seamless images with the local features exhibited from the training data set this local segment likelihood evaluation approach can be viewed as maximizing one single global likelihood function as in the right hand side of eq 5 this simple but notable difference enables the generator g to capture the object structure implicitly and synthesize the arbitrary size of images that do not depend on the size of the training samples regardless of the size of images created by the generator the architecture of the discriminator can be determined depending on the size of the segmented images i e the inputs of a d model thus the architecture of the discriminator becomes free from the generator as shown in fig 1 b this subsequently reduces the computational cost significantly compared to the standard dc gans note that unlike the standard gans sagans can create plausible realizations with a few training samples or even a single training sample e g tahmasebi and sahimi 2012 which we will demonstrate in the next section 3 development of sagans and dcgans 3 1 training images tis in this study three ti samples widely used for geostatistical simulation tests were selected to compare the performance of sagans with dcgans for large size image generations and computational efficiency three samples include 2d strebelle s images strebelle 2002 2d and 3d spherical beadpack and 3d categorical fold aquifer as shown in fig 2 these samples represent relatively simple geological structures but have been proven to be very challenging as the training image ti for the mps and gans mosser et al 2017 strebelle 2002 specifically strebelle s tis and fold aquifer consist of continuous channels or pore space that are connected in various patterns while spherical beadpack patterns are packing of the different size of the spherical particles beads which are randomly distributed without being overlapped the realization of the discrete nature of beadpack is also a challenge due to the difficulty of applying gans for discrete data in this study we focus on how well sagans and dcgans can generate the various long range connectivity of channels in strebelle s tis and fold aquifer and the spatial distribution and shapes of beads in the spherical beadpack taking into account the computational time of the experiments and the ease of handling the samples all tis are resized into binary 2d squares and 3d cubes with the representation of the pore space or channel white and grain solid structure black the tis in fig 2 a and b are used for the 2d realization and fig 2 c and d are used for the 3d realization 3 2 architecture and training of gans in this study we constructed the architecture of deep convolution neural networks for sagans and dcgans based on the guidelines proposed by radford et al radford et al 2015 radford et al radford et al 2015 identified a family of architectures of deep convolution for stable training across a range of samples and higher resolution and deeper generative models main architectural features adopted from radford et al radford et al 2015 are the following 1 stride convolutions instead of any pooling layers 2 no hidden layers in a fully connected net fcn in both generator and discriminator 3 relu activation in the generator for all layers except for the output that uses tanh and 4 leakyrelu activation in the discriminator for all layers except for the output that uses sigmoid architecture and hyperparameters used in this study are presented in table 1 the d model is composed of two convolution layers with three or five of kernel size and the output was converted to the probability 0 false to 1 true using the sigmoid activation function in the g model the noise z with a dimension of 100 was drawn from a gaussian distribution and given to the fcn the fcn reshaped z to a set of tensor z before passed to the convolutional layers the deconvolution layers with the same kernel size in the d model was used and the output is converted by a stride convolution layer into the same size image as the given training images or segments both d and g models were applied and trained by the adam adaptive momentum estimation optimization algorithm with a learning rate of 0 0002 with the momentum β 1 of 0 5 and a total of 100 000 epochs dropout with a probability of 0 25 was applied to the convolution layers of the d model the loss of gans was estimated by binary cross entropy function eqs 1 and 4 the different size pixel and voxel resolution of images from the tis are synthesized by dcgans and sagans depending on the synthesized size 3 to 5 depth of convolutional layers and 4 to 128 channels filters per a convolutional layer were applied a larger number of epochs with a small learning rate and dropout on the d model and batch normalization were applied for the stabilized training and the convergence of both g and d models that have the symmetric architecture all computational works in this study were performed using the same computer equipped with two nvidia titan v gpu cards 5120 cores 12g gpu memory and intel i9 9900x cpu 10 cores with 64g ram 3 3 evaluation of the synthesis results two point probability and two point cluster functions were used to evaluate the texture image synthetic performance of both sagans and dcgans in this study the two point probability function s 2 estimates the probability of how two points pixels for 2d and voxels for 3d digitized images are related to each other which is often called auto correlation s 2 is used for structural information about the degree of spatial dependence of a random field 6 s 2 r p x φ x r φ for x r ℝ n where p is the probability that two points x and x r are separated by the lag vector r in the same phase of interest φ if the two points coincide r 0 in the phase φ the probability is the same as the probability that one point is the phase φ which is simply equal to the volume fraction φ of the phase φ 7 φ v ϕ v total s 2 r 0 φ when the two points are completely uncorrelated and far apart each point has a probability φ to be the phase φ 8 s 2 r φ 2 s 2 decays from φ towards the asymptotic value of φ 2 two point cluster function c 2 estimates the probability that two points are in the same cluster of the phase of interest when the image sample consists of some isolated clusters c 2 is used for representing the statistically homogeneous distributions of identical inclusions torquato 2013 this connectivity function is defined as follows 9 c 2 r p c x c x r for x φ i where c is an indicator of the cluster in φ i 4 results and discussion 4 1 2d 3d earth texture image synthesis in this section we compared the results obtained from sagans with those from dcgans using the 2d 3d tis presented in fig 2 first we consider a simple example using the strebelle s ti to illustrate the effectiveness of the proposed method the single strebelle s ti consists of connected channels white color representing the long range connectivity and pattern diversity several images generated from dcgans and sagans are plotted as in fig 3 notably the dcgans produced the same realization of images as the ti while the sagans produced different realizations with various patterns visually similar to those in the strebelle s ti this is mainly due to the architecture of dcgans and the lack of diverse training data the d model in the dcgans determines whether the image created by the g model is the same as the ti true or not fake then the g model in the dcgans is trained to generate exactly the same image as the given single ti resulting in the same realization as the ti fig 3 a since in many subsurface data survey only a limited number of geomaterial images would be available e g yoon and dewers 2013 direct application of dcgans to geostatistical simulation would not be satisfactory in contrast to the dcgans the image generated by the g model in the sagans is divided into several segments and then each segment is compared with the same size of segments extracted from a single ti fig 1 b in this example both generated image and the ti were divided into four segments as in fig 3 b each segment has a size of 130 by 130 with 4 pixels overlapped vertically and horizontally to account for the long range connectivity the arrangement of the segments from the ti and the structures in place build a pool of patterns available the architecture of the sagans can capture the object structure implicitly from the segments through hierachical assembly between pf and segments hence the sagans can synthesize the various pattern of textures with statistical properties e g the long range connectivity even using a single ti although dcgans can synthesize the various textures using the multiple segments extracted from a single image as ti like the way sagans extract information from the ti fig 4 a without any architecture or training image modifications dcgans cannot generate the smoothly connected channels properly since the seam between the combined multiple segments in the training images is also trained as a pattern fig 4 b in the next sections we evaluate the quality of generated samples in terms of similarities in spatial and flow characteristics 4 1 1 spatial similarities to compare the similarities of spatial characteristics between the generated image and the ti two directional metrics s 2 and c 2 in sec 3 3 for the pore phase channels in white of twenty generated images were calculated as shown in fig 5 note that s 2 and c 2 of the realizations by dcgans are identical to those of the ti original line in fig 5 since the same realizations as a single ti were always produced on the other hand the realizations by sagans show a range of fluctuation in s 2 and c 2 a shade in red in fig 5 but show similar average values red line to those of from the ti they also have the similar volume fraction φ porosity of 0 29 compared to 0 27 of the ti this result indicates that sagans can generate statistically similar images with various patterns even using a single ti our second 2d realization uses a total of spherical beadpack tis fig 2 a for the texture synthesis on the spatial distribution and shapes of the particles structural properties fig 6 shows some examples of the realizations on the the spherical beadpack tis four segments with a quarter size of the generated image and the tis were used in sagans beads in the realizations by sagans have more spherical shape and less overlapped with each other well spread than those by dcgans fig 6 b both dcgans and sagans achieve a small error in the averaged values of s 2 and c 2 for each twenty generated images fig 7 with the sagans slightly better the dcgans in contrast to the first case using a single ti the second case demonstrates that with a large number of tis both dcgans and sagans are able to the images with the basic statistics of the tis in the 3d case we used spherical beadpack tis and fold aquifer tis fig 2 c d similar to the 2d cases we evaluated the performance of sagans and dcagns on the 3d realization of the long range connectivity and the spatial distribution of the particles fig 8 shows examples of the 3d realizations by both dcgans and sagans to compare the similarity of the 3d realizations the s 2 over the radial lag distance was calculated and normalized for the randomly selected 20 realizations the c 2 values are not reported since they are very identical to the s 2 statistics comparison of the normalized isotropic s 2 between the realizations and tis are shown in fig 9 since the single 3d ti was used the dcgans generate the same realization as the ti hence the s 2 of tis is identical to one of the dcgans as in the first 2d realization case the s 2 of the sagans shows a relatively small error compared to the tis and a small variation indicating that the sagans can produce the 3d realizations with diversity even using the single 3d ti 4 1 2 pore scale flow path and upscaled permeability similarities in this section we evaluate the quality of generated samples by examining simulated flow patterns on both ti and samples and corresponding upscaled permeabilities over the simulation domain in other words tis and the generated images by sagans and dcgans were directly used as the porous media simulation domain where the stokes equation was solved and upscaled permeability values were computed comparison of flow patterns and upscaled permeabilities from tis and generated images would provide effective measures on how similar the generative methods reproduce the spatial features in earth materials related to pore scale flow paths e g yoon and dewers 2013 along with the spatial similarities as shown in section 4 1 1 only the 2d beadpack and stebelle s ti cases were considered here due to computationally expensive pore scale flow simulations to accelerate the flow simulation in each generated image an open source parallel lattice boltzmann lb solver palabos latt et al 2020 was used to solve the stokes equation the lb based methods can treat complex geometries and boundaries of porous media easily and provide both computationally efficient and reasonably accurate simulations of pore scale flow velocities yoon et al 2015 a no slip boundary condition at the solid fluid interface using the bounce back condition is assumed with a constant pressure boundary condition on the inlet left and outlet right of the domain and no flux on top and bottom of the domain with darcy s law under the laminar flow regime each directional permeability was computed using the total flux and the pressure gradient over the length of sample detailed configurations for the lb method can be found in yoon and dewers yoon and dewers 2013 and yoon et al yoon et al 2015 randomly chosen 100 beadpack tis single strebelle s ti 20 realizations from sagan for the beadpack and strebelle cases and 20 realizations from dcgan for the beadpack case as shown in section 4 1 1 were used for pore scale simulations since the generated images from dcgan for the single strebelle ti case were converged to the ti by construction dcgan results for the strebelle ti case were not included in the flow simulation as in our previous microfluidic simulations in yoon et al yoon et al 2019 and park et al park et al 2021 we constructed our simulation domains into 3d domain 15 pixels in depth and the entire simulations of training and generated images took about 3 days with 6 simulations in parallel using 48 cores of intel xeon platinum 8180 cpu 2 5 ghz per each simulation the computed permeability values for the generated images were normalized by the average upscaled permeability of the ti s for effective comparison in fig 10 the distributions of the normalized upscaled permeabilities for two 2d cases are plotted for the beadpack example it is shown that sagans produced the permeability fields similar to the tis while realizations from dcgans overestimated the upscaled permeability about 40 the strebelle example in fig 10 b also illustrates the effectiveness of the proposed method the mean of the generated permeabilities is close to the ti while the diversity between samples is preserved so that the proposed method can be used for the stochastic analysis of earth materials in addition we show the velocity magnitudes in the simulated flow paths on the domain of the tis and generated images from sagans and dcgans in fig 11 for 2d beadpack example the improved reconstruction quality from sagans leads the flow patterns similar to those in beadpack tis while dcgans tendency to create bead aggregates slightly deviates flow patterns from the expected ones from tis resulting in the overestimation of the upscaled permeabilities as shown in fig 10 b for the strebelle example sagans tends to generate the dead end channels more than the ti but as shown in fig 10 b the strebelle ti s upscaled permeability is still within the range of generated permeabilities the generation of dead end channels was expected since the flow wise connectivity over the entire domain is hard to learn from a limited number of training images i e the single strebelle s ti in this case during the training without further information arora et al 2017 this issue can be alleviated by including more training images or architecture or loss function wise constraints 4 2 scalability and computational efficiency in this section we focus on the scalability in terms of the output texture size and computational efficiency of the sagans as explained previously the size of model output is not limited by the ti size in the sagans hence we tested this conceptual advance with a smaller size of the images extracted from the tis to generate a large size of image fig 12 a b shows examples of the arbitrary large size of 2d realization by the sagans in the realization of larger size of 2d spherical beadpack 512 512 pixels by sagans the created image of 512 512 pixels were divided into four segments of 256 256 pixels and then the probability of each segment was evaluated with the segments of 256 256 pixels from beadpack tis for the realization of larger size of strebelle s ti based channels 1 040 528 pixels 32 segments of 144 144 pixels divided with overlapping 16 pixels in the created image 1 040 528 pixels were evaluated with the segments of 144 144 pixels from 256 256 pixels of strebelle s ti in the fig 12 a b the spherical beads and the long range connectivity can be verified visually the larger size images can be generated by dcgans in the same way shown as in fig 4 a however the realizations show the seam between the combined original tis as the realizations in figs 4 b and 12 c in addition building the large size of tis from the small size tis requires considerable time and computational resources for reading and allocating the data in the memory to train the gans especially in building 3d tis hence the dcgans have the limits in generating the arbitrary large size of images due to the cropping area or the seam created by connecting multiple images from the original tis on the other hand sagans produce the arbitrary large seamless realizations with simply changing the dimension of z in the generator g even in a sing ti the size and positions of segments also can be arbitrarily adjusted according to the size of the image to be generated for the characteristics of the original tis fig 13 shows examples of the arbitrary large size of 3d realizations from a single of ti by sagans in the realization of 256 256 320 size spherical beadpack 3d segments of the 64 voxels were applied for the realization of 256 256 256 size fold aquifer ti 3d segments of the 64 voxels overlapping by 3 voxels were applied considering the long range connection the s 2 graph on the twenty realizations by sagans shows a small error to the graph on tis and with some variations this indicates that sagans could produce the 3d realizations of the arbitrary larger size and with diversity even in a single of 3d ti dcgans could not generate larger images than the original ti size larger than 256 voxels of beadpack larger than 128 voxels of fold aquifer due to the lack of tis and gpu memory limitations as mentioned above the size of the generated output in the g model should be identical to the size of the input of d model in the standard gans dcgans framework therefore the larger the output is generated the larger architecture of d model is required at the same time which increases the computational load the d model runs not only when the d model is trained but when the g model is trained consequently this architectural constraint increases both the computational load and computational time in training the gans significantly however in the sagans the architecture of the d model is free from the g model the size of the inputs of a d model can be set relatively small regardless of the size of the output generated by the g model therefore sagans can produce the larger size realizations with the low computational cost table 2 shows the training time per 1000 epoch in each case the symmetrical architecture for g and d model was applied with the same parameters table 1 the difference between dcgans and sagans in each case is the size of the inputs the size of segment given to the d model the results in table 2 show that the training time of the sagans is less than the training time of the dcgans and the decrease is larger as the size of generated outputs increases fig 13 and table 2 indicate that the texture synthetics with the sagans significantly improve the increase in the image size of realizations and the computational time for high dimensional applications compared to the standard gans framework the detailed list of dcgan and sagan architecture and hyperparameters of each test can be found in the appendix 5 conclusions this study proposed the new gans framework spatially assembled gans sagans that can generate output images of an arbitrary large size regardless of the size of training images with computational efficiency the proposed method can support various hydrogeological applications with a limited number of geological samples and enables subsequent stochastic analyses by multi phase flow and transport simulations directly on the generated geomaterial images the sagans have no architectural constraint in contrary to the standard gans dcgans framework where the size of the generated output image in the g model should be identical to the size of the input image of d model this enables the sagans to produce the multiple realizations with the scalability in terms of the output size and computational efficiency the performance of the sagans for generating the images was evaluated using widely used 2d and 3d earth texture image samples with two point probability two point cluster function and upscaled permeability through single phase flow simulations both sagans and dcgans generated the texture images close to the basic statistics morphological characteristics and flow patterns of the tis such as porosity long range connectivity the spatial distribution and shapes and upscaled permeability however the dcgans could not produce various patterns of the realizations when a single of ti is given and could not produce the arbitrary large size of realizations due to the cropping area or the seam created by combining the multiple original tis in addition the dcgans could not produce large size of 3d realization due to the lack of computational memory however the sagans improve these limitations to produce large seamless realizations with a variety of patterns even using a single ti the sagans also produced the larger size of 3d realizations with a low computational time and load so that the applications for high dimensional problems are now achievable the sagans can be applied in various ways depending on how to adjust the size and positions of segments for example if a specific field in generated images by the g model is evaluated by the same segment from tis the part of all the generated images has the same property as in the specific segment in this way a specific part of the texture image can be synthesized if various segments from tis with quite different features are used the sagans can produce the realizations of a non ergodic stochastic process which will enable us to generate the composite texture that can be used in hydro mechanical applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the laboratory directed research and development program at sandia national laboratories sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government training image samples used in this study can be downloaded from http www trainingimages org training images library html source code will be available in https github com saint kim and an experimental version is available upon request to sung eun kim saint kse gmail com appendix a we added sagan architecture and hyperparameters of each test cases used in this paper table a 1 hyperparameters and architecture of the networks used in this paper table a 1 case model ti z of filter per layer generated size size of kernel stride name size g model d model 2d dcgan strebelle 1 256 256 32 32 64 32 16 16 32 64 256 256 5 52 2 100 128 2 128 2 sagan 100 130 130 1 256 256 33 65 128 64 32 16 16 32 64 128 528 1040 dcgan beadpack 256 256 256 32 32 64 32 16 16 32 64 256 256 256 128 2 128 2 64 64 512 512 sagan 256 256 256 32 32 256 256 256 64 64 128 64 32 32 64 128 512 512 3d dcgan beadpack 1 256 256 256 4 4 4 64 32 16 8 4 4 4 4 8 16 32 64 256 256 256 3 3 32 2 2 100 128 128 128 8 8 8 64 32 16 8 8 16 32 64 128 128 128 sagan 1 256 256 256 8 8 8 64 32 16 8 4 4 8 16 32 64 256 256 256 1 64 32 16 8 8 16 32 64 128 128 128 1 8 8 10 64 32 16 8 4 4 8 16 32 64 256 256 320 dcgan fold aquifer 1 128 128 128 8 8 8 64 32 16 8 8 16 32 64 128 128 128 100 64 64 64 64 32 16 16 32 64 64 64 64 sagan 1 128 128 128 64 32 16 8 8 16 32 64 128 128 128 1 64 32 16 16 32 64 64 64 64 1 64 32 16 8 4 4 8 16 32 64 256 256 256 
