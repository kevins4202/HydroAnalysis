index,text
560,the budyko and turc frameworks have become very popular tools for the estimation of catchment scale water balance in their original definition these frameworks and the resulting equations which are considered equivalent apply in the long term to very large catchments whose water balance is predominately driven by climatic factors several equations similar to budyko s and turc s have subsequently been proposed to account also for the effect of catchment characteristics including parametric formulations as well as equations resulting from a simplified physical representation of the water balance after highlighting their advantages and disadvantages we show that models based on the water balance which account for rainfall variability and feedback between water availability and evapotranspiration are more versatile to describe catchment scale rainfall partitioning they include parameters that have clearer physical meaning and therefore can be estimated independently of streamflow and evapotranspiration thereby making them more amenable to practical use in un gaged catchments they show that budyko s and turc s point of view are equivalent only for large catchments additionally water balance models have limiting conditions for extremely dry and wet climates that differ from those of the budyko equations and its parametric formulations as expected in catchments with a finite water storage capacity in these models budyko s and turc s points of view become equivalent only for large catchments keywords water balance budyko framework budyko equation turc equation hydrologic modeling 1 introduction the understanding and estimation of how rainfall is partitioned into the other components of a catchment water balance is one of hydrology s fundamental tasks this is at the foundation of land and water management practices leopold 1997 which often require simple formulae to relate streamflow and evapotranspiration to rainfall and other routinely measured meteorological data one popular method is the framework developed by the russian climatologist budyko oldfield 2016 the method which builds upon the work of other russian researchers andreassian et al 2016 and has strong similarities with the work by turc 1954 results in a functional relationship between long term annual evapotranspiration rates from large catchments and long term annual rainfall and solar radiation budyko 1974 the resulting formula known as the budyko equation is able to fit the observations in several large catchments see e g figure 94 of budyko 1974 on page 325 revealing common patterns across them however observations from catchments with a wide range of sizes and climates present a large scatter around the budyko equation zhang et al 2001 padron et al 2017 to accommodate this variability models similar to the budyko s equation have been developed choudhury 1999 zhang et al 2001 2004 wang and tang 2014 tang and wang 2017 these models use an empirical parameter to adjust the relationship between evapotranspiration and rainfall to the observations despite the lack of a physical meaning considerable effort has been devoted to find an interpretation of such an additional parameter yang et al 2008 zhou et al 2015 to understand its links to environmental variables potter et al 2005 gentine et al 2012 donohue et al 2012 padron et al 2017 and quantify its role in driving changes in catchment water balance roderick and farquhar 2011 zhou et al 2015 gudmundsson et al 2017 equations similar to budyko s have been also derived using water balance models that account for only a few parameters characterizing climate and catchment characteristics milly 1993 porporato et al 2004 like other formulations these equations have parameters that allow for the necessary flexibility to accommodate the observed variability due to their physical interpretation donohue et al 2007 these parameters can be estimated independently of streamflow and evapotranspiration data however probably because these water balance models appear to be more complicated than other existing formulae they have been used less frequently feng et al 2012 donohue et al 2012 feng et al 2015 yang et al 2016 here we discuss advantages and disadvantages of the most commonly used models developed within the budyko framework to include catchment characteristics our analysis highlights that among these formulations the ones based on the water balance lead to equations equivalent to budyko s and turc s without the need to resort to fitting parameters additionally water balance models can possibly support the physical interpretation of parametric models 2 the budyko framework budyko 1974 describes a framework that results in a functional relationship between catchment scale long term averages of annual precipitation p evapotranspiration e and streamflow q the framework is developed by combining the catchment scale long term average annual water and surface energy balances which can be written as 1 p e q 2 r n λ w e h where rn and h are the long term averages of annual net energy available at the soil surface and the sensible heat respectively and λw is the latent heat of vaporization of water it is assumed that the averages are calculated over a period long enough to have negligible changes of the water stored in the catchment similarly in eq 2 it is assumed that the energy stored at the soil surface is negligible and the heat transfer from the soil surface to the soil layers underneath commonly indicated as g oscillates between positive and negative values thus having a negligible long term average the most common use of the budyko framework is the estimation of q and e from measurements of rainfall and meteorological conditions that can be used to estimate potential evapotranspiration for this specific problem the framework is built on the assumption that the water balance is driven solely by climatic factors budyko 1974 thus implying that it can be applied to large catchments the size of which is comparable with the scale of geographical zones see page 330 of budyko 1974 accordingly one can argue that runoff tends to zero for very dry catchments e g deserts implying that the annual rainfall is completely lost via evapotranspiration i e e tends to p in very water limited catchments in wet catchments at the annual scale h is directed from the surface to the atmosphere i e h is positive in almost all the climatic zones on earth therefore h cannot supply much energy to the soil surface and it is thus assumed that rn is providing entirely the energy required for evapotranspiration as a result in wet catchments the upper limit of λwe is rn for this reason the ratio e 0 r n λ w has been assumed as the maximum value of catchment scale long term average annual evapotranspiration thus in catchments that are not water limited e tends to e 0 and these catchments are energy limited we highlight that accounting for the effect of surface temperature on net radiation the upper limit of λwe should be lower than rn as shown by yang and roderick 2019 although in its original derivation e 0 was directly calculated from rn in most applications it is commonly assumed that e 0 is the potential evapotranspiration which depends on rn air humidity wind speed and vegetation cover and type the two variables e 0 and rn can be linked to use the framework for surface energy balance partitioning yin et al 2019 it has also been shown that when using future climate projections a dependence of e 0 on atmospheric co2 concentrations should be included in hydrological models milly and dunne 2016 yang et al 2019 these assumptions for energy p or water e 0 limited catchments can be summarized as 3 e e 0 as p e p as e 0 and imply that in general 4 e f p e 0 if the function f p e 0 were known then e could be estimated from long term measurements of p and estimates of e 0 from meteorological variables routinely measured the number of variables in eq 4 can be reduced by applying the buckingham π theorem barenblatt 1996 because the three variables e p and e 0 have the same dimensions l t 1 i e equivalent depth of water per year both p and e 0 can be selected as a variable of reference when selecting p eq 4 can be re written in terms of dimensionless groups as 5 e p f b d i where d i r n λ w p e 0 p is called dryness index the conditions in eq 3 thus become 6 e p d i as d i 0 e p 1 as d i eqs 5 and 6 state that given the assumptions in eqs 1 and 2 there exists a relationship between e p and di the form of which i e fb di needs to be determined from experimental observations using existing models from non english literature budyko 1974 proposed the relationship 7 e p d i tanh 1 d i 1 e d i 1 2 which was successfully tested against experimental data budyko 1974 and is commonly referred to as the budyko equation several other equations satisfying eqs 5 and 6 have been listed in the literature such as the formulas proposed by schreiber and oldekop zhang et al 2001 andreassian et al 2016 specifically eq 7 is the geometric mean of the formulas by schreiber and oldekop as described by wang et al 2016 when e 0 is selected instead of p as a variable of reference for the application of the buckingham π theorem in eq 4 one obtains 8 e e 0 f t d i 1 f t h i with conditions 9 e e 0 1 as h i e e 0 h i as h i 0 where h i d i 1 is referred to as the humidity index with these assumption turc 1954 proposed the equation 10 e e 0 h i 0 9 h i 2 1 2 usually called the turc equation turc 1954 pike 1964 moussa and lhomme 2016 the two spaces e p vs di and e e 0 vs hi are called budyko and turc spaces respectively moussa and lhomme 2016 3 models of e p vs di the framework developed by budyko has been extensively applied to catchments of different sizes for which catchment characteristics in addition to climatic conditions affect e and q these applications led to the development of models for the relationship between e p and di that modify eq 7 by introducing an additional parameter table 1 these models can be separated in two distinct categories one comprises models derived following steps similar to those in the budyko framework resulting in relationships between e p and di modulated by one additional parameter we will refer to these as parametric models of the budyko equation models in the second category account in simple terms for rainfall runoff processes to derive analytically functions relating e p to di and will be referred to as water balance models of the budyko equation it is stressed that although many hydrological models simulating e could be used to estimate e p vs di gerrits et al 2009 condon and maxwell 2017 the focus here is on catchment water balance models that result in relationships as eq 7 without the use of extensive numerical simulations 3 1 parametric models eq 7 was empirically determined from experimental data and the limiting conditions in eq 6 likewise several models relating e p and di have been derived to satisfy similar conditions as well as accommodate the variability in experimental observations from catchments with different characteristics wang et al 2016 the most commonly used models are briefly reviewed here 3 1 1 mezentsev choudhury yang model the model used by choudhury 1999 was previously presented by the russian researcher v s mezentsev wang et al 2016 roderick and farquhar 2011 yang et al 2008 provided a mathematical justification of this model which will be referred to as mezentse choudhury yang mcy model sposito 2017 the mcy model states that 11 e p 1 1 d i n 1 n where n 0 is an empirical parameter the parameter n as derived by yang et al 2008 appears mathematically as a consequence of assuming that e depends on the long term average soil moisture of a catchment such that e can be expressed using an implicit function as e e p e 0 e as n 0 e p 0 for any value of di while for n the model leads to the theoretical water i e e p 1 for di 1 and energy i e e p d i for di 1 limits of the budyko framework for any values of n 0 the model satisfies the conditions in eq 6 examples of relationships between e p and di from the mcy model are shown in fig 1 a eq 11 can be re arranged to be used in the turc space as 12 e e 0 1 1 h i n 1 n which satisfies the conditions in eq 9 the mcy model thus uses the same functional relationship to relate e p and e e 0 to di and hi respectively the parameter n ranges typically between 0 6 and 3 6 but is mostly from 1 5 to 2 6 as shown in several studies pike 1964 choudhury 1999 roderick and farquhar 2011 donohue et al 2012 3 1 2 fu model the model by fu which was introduced to the english literature by zhang et al 2004 is based on general assumptions about the dependency of changes of e on changes of both p and e 0 the resulting expression reads 13 e p 1 d i 1 d i ω 1 ω where ω is a parameter that needs to be larger than 1 to satisfy the conditions of the budyko framework for di that tends to 0 and as ω 1 e p 0 for any value of di eq 13 can be expressed in the turc space as 14 e e 0 1 h i 1 h i ω 1 ω which satisfies the conditions in eq 9 as for the mcy model according to the fu model e e 0 is related to hi with the same functional relationship used to relate e p to di a value of ω around 2 6 provides a good match between eq 13 and eq 7 as shown in fig 1 b zhang et al 2004 greve et al 2015 using more than 2700 catchments padron et al 2017 found that ω can range from about 1 to 22 with 97 of catchments having ω between 1 06 and 5 5 a strong linear relationship can be found between ω and n in the mcy model as ω n 0 72 yang et al 2008 an in depth comparison between fu and mcy models can be found in andreassian and sari 2019 3 1 3 zhang curves the zhang curves zhang et al 2001 are described by the empirical relationship 15 e p 1 ω z d i 1 ω z d i d i 1 where ωz is a parameter accounting for the ability of different vegetation types to use soil water the value of ωz needs to be larger than d i 1 for e p to be positive using observations from catchments across the world zhang et al 2001 suggested ω z 2 and 0 5 for forested and herbaceous plant catchments respectively as shown in fig 1c e p 1 as d i but fails to properly satisfy the conditions for di 0 because as ωz increases e p becomes larger than di when 0 di 1 this already occurs for the recommended value of ω z 2 when di 0 5 the values of ω z 0 5 has also been recently challenged as a general value for herbaceous catchments dresel et al 2018 because it is not representative of pastures with annual rainfalls lower than about 700 800 mm per year which are common in south australia where the model has been applied in several instances zhang et al 2011 2012 the improper limiting condition of the curves for di 0 can be better seen in the turc space using the relationship 16 e e 0 h i ω z 1 ω z h i 1 h i as shown in fig 2 e becomes larger than e 0 when ωz 1 with e e 0 1 as hi specifically e e 0 crosses the line e e 0 1 at h i d i 1 ω z ω z 1 3 1 4 wang tang model a relationship between e p and di at the annual scale using a form of the water balance more complex than eq 1 was derived by wang and tang 2014 see also wang et al 2015 assuming that p is at first separated into direct runoff and a component w wetting the soil at a second stage w is then partitioned into evapotranspiration and baseflow a portion of w is assumed to be rapidly lost and this loss is called initial evaporation ein with these considerations wang and tang 2014 derived the formula 17 e p 1 d i 1 d i 2 4 ϵ 2 ϵ d i 2 ϵ 2 ϵ where ϵ e i n e 0 ϵ 1 is assumed to be a parameter characterizing a catchment eq 17 is equivalent to the water balance equation of the abcd model wang and tang 2014 thomas et al 1981 when ϵ 0 e p d i 1 d i and when ϵ 1 then e p 1 d i 1 d i 2 which corresponds to the water and energy limited conditions of the budyko framework i e dotted lines in fig 1 d the model presents a lower bound of e p when ϵ 0 fig 1 d and is thus not able to account entirely for the space bounded by the energy and water limits in the budyko space moussa and lhomme 2016 for ϵ 2 2 2 eq 17 is the same as the fu model when ω 2 wang and tang 2014 a limitation of this model is that the dependence of ϵ on e itself is not accounted for additionally assuming constant ϵ implies that ein adjusts its value as e changes while ein should depend on w which is not dependent on e along the same lines a relationship between e p and di can be derived using four parameters one of which again depends on e itself tang and wang 2017 the combination of these parameters make ϵ appear again in the relationship with the same issues mentioned above the same model has been recently further developed to be applied to human water consumption lei et al 2018 as for the mcy and fu model the relationship in the turc space for the wang tang model is as in eq 17 and reads 18 e e 0 1 h i 1 h i 2 4 ϵ 2 ϵ h i 2 ϵ 2 ϵ 3 2 water balance models lumped models of catchment scale water balance applicable to catchments with different sizes can be developed milly 1993 porporato et al 2004 by considering a process based description of evapotranspiration and runoff formation and by accounting for rainfall variability these models can explain the variability of experimentally obtained relationships between e p and di with the addition of one parameter that has a clear physical meaning details of these models can be found in milly 1993 rodriguez iturbe et al 1999 laio et al 2001 and porporato et al 2004 a brief description is provided here with the mathematical details reported in appendix a we will focus here on models with constant parameters which lead to closed form relationships between the relevant variables involved in the water balance seasonal cycles of precipitation and potential evaporation can be accounted for as in feng et al 2012 2015 at the expense analytical tractability the models described in milly 1993 and porporato et al 2004 start from the description of the catchment water balance at the daily timescale which is the typical scale of soil moisture dynamics and derive statistics of the water balance over a long term describing a catchment as a reservoir of finite capacity the daily catchment scale water balance can be written as 19 w 0 d x d t p d t e d e m x q d x t where w 0 is the maximum soil water storage available for evapotranspiration x is the effective relative soil moisture i e the percentage of soil water that can support evapotranspiration pd ed and qd are the daily rainfall evapotranspiration and runoff rates respectively given the long timescales of interest to these models rainfall variability is accounted for in statistical terms in particular pd is assumed to occur according to a poisson process with a constant rate λ and each event delivers a daily rainfall depth drawn from a given probability distribution with an average α although different rainfall depth distributions have been used in the literature daly and porporato 2010 verma et al 2011 the models presented here assume daily rainfall depths exponentially distributed ed depends on the water stored in the catchment which is available to the vegetation to be transpired and the daily potential evapotranspiration rate em which depends on climatic conditions as well as vegetation cover because the effect of daily variations of em on the water balance have been shown to be less important when compared to the variations of daily rainfall em can be assumed to be constant in time daly and porporato 2006 qd occurs when a rainfall event carries an amount of water larger than what the soil can accommodate at that particular day when the event occurs qd thus depends on pd and x which is in turn dependent on ed over a long time assuming that the parameters λ and α remain constant in time and that the catchment characteristics do not change the catchment reaches an equilibrium characterized by a constant average soil water storage x this assumption is also the starting point of the derivation of eq 13 provided by yang et al 2008 for long timescales the annual water balance can thus be written as in eq 1 with p e and q calculated from the respective average daily rates as p p t y e e t y and q q t y where ty is the number of days in the year and p e and q are the long term averages of pd ed and qd respectively likewise e 0 e m t y depending on the specific expression adopted to model ed as a function of x the solution of eq 19 permits the derivation of relationships between e p and di that provide alternative forms of the budyko and turc equations as described in detail in sections 3 2 2 and 3 2 3 3 2 1 budyko and turc spaces the interpretation of the stochastic soil water balance using dimensionless groups touched on by porporato et al 2004 and described in detail by feng et al 2012 is briefly recalled here highlighting the links with the budyko and turc spaces as well as the conditions for di 0 hi and di hi 0 following feng et al 2012 and according to the typical time scale of eq 19 it is natural to perform the dimensional analysis at the daily time scale which is presented first this is then extended to the annual scale to clearly show its analogy to the budyko and turc frameworks according to eq 19 at the daily time scale ed depends on em and x with x being driven by pd and w 0 which defines how much rainfall can be lost via qd once the parameter w 0 is introduced in the framework it is no longer sufficient to consider the long term average annual rainfall but the way rainfall is delivered to catchments is also important for a given w 0 if rainfall is delivered as a frequent succession of small rainfall events i e large λ and small α there might be a large amount of water available for evapotranspiration conversely if rainfall is delivered in the form of a few large events i e small λ and large α most of it might be lost as runoff and a small amount remains available for evapotranspiration accordingly eq 4 can be generalized as 20 e f λ α e m w 0 the dimensions of the variables involved are e e m l t 1 λ t 1 and α ω 0 l with l and t units of length and time respectively using the buckingham π theorem barenblatt 1996 when λ and α are selected as dimensionally independent variables one obtains a general relationship in the budyko space that reads 21 e p e λ α f s e m λ α w 0 α f s e m p w 0 α f s d i γ p because e and p are proportionally related to e and p via the number of days in a year ty it follows that e p e p such that fs in eq 21 is an alternative formulation of the budyko equation eq 7 the additional parameter γp defines how much water the catchment can retain with respect to the average daily rainfall depth the form of fs in eq 21 depends on the function chosen to relate ed to em and x as well as the distribution of water depths per rainfall event in the turc space eq 20 requires the selection of em as dimensionally independent variable there are thus several combinations of variables that lead to different relationships selecting either λ α or w 0 as dimensionally independent variable yields 22 e e m e e 0 f λ λ α e m λ w 0 e m f λ h i γ e 23 e e m e e 0 f α λ α e m w 0 α f α h i γ p and 24 e e m e e 0 f w 0 λ w 0 e m w 0 α f w 0 γ e γ p in these equations e e m e e 0 because of the same constant of proportionality i e ty between the variables at daily and annual scales therefore eqs 22 24 are alternative formulations of eq 8 the parameter γe represents the ratio between the rainfall frequency λ and the frequency at which w 0 is depleted completely when water is extracted at the constant rate em these three equations are equivalent and can be derived one from the other using h i γ e γ p 1 3 2 2 milly model starting from eq 19 and considering a constant daily evapotranspiration rate e d e m milly 1993 obtained see appendix a 25 e p e p e γ p 1 d i 1 1 e γ p 1 d i 1 d i 1 which satisfies the conditions 26 e p e p d i as d i 0 27 e p e p g γ p 1 e γ p as d i the condition for di 0 is as the budyko equation eq 7 while the condition for di reflects the role of the parameter γp in characterizing the catchment as γ p i e when the storage of a catchment is much larger than the average water depth per rainfall event the model tends to the limiting conditions of the budyko equation it is also noted that contrarily to what stated by wang et al 2016 eq 25 exists for any value of di with no discontinuities at d i 1 fig 3 a shows a comparison between the budyko equation and eq 25 for different values of γp as already stressed in milly 1993 the model is not capable to reproduce the budyko curve milly 1994 modified the model by adding a seasonal cycle to precipitation and evapotranspiration although the model was successfully applied against observed data it was still not able to reproduce the budyko curve with any combination of parameters the model by milly 1993 can be also expressed in the turc space in terms of e e 0 as shown in appendix a 3 2 3 porporato et al model differently from milly 1993 the model by porporato et al 2004 assumed e d e m x thus accounting for a reduction of evapotranspiration when soil water decreases this leads to see appendix a 28 e p e p 1 d i γ p γ p d i 1 e γ p γ γ p d i γ γ p d i γ p where γ and γ are the gamma and incomplete gamma functions abramowitz and stegun 1965 respectively with the following conditions see appendix a for details 29 e p e p γ p d i γ p d i as d i 0 30 e p e p g γ p 1 e γ p as d i which are both different from those of the budyko framework fig 3 b shows a comparison of eq 28 with the budyko equation for different values of γp as already highlighted by porporato et al 2004 eq 28 with γp 5 5 approximates eq 7 across the range of values of di often encountered in practical applications as di increases e p tends to 1 in the budyko equation while it tends to 1 e 5 5 0 996 for eq 28 the value of γ p 5 5 corresponds to a soil depth of about 0 3 0 4 m which is comparable to the average root depth in water limited ecosystems schenk and jackson 2002 as in eq 25 and fig 3 a when γp the conditions of the budyko framework are valid and the model results in the curves e p d i when di 1 and e p 1 when di 1 the expressions of eqs 22 24 according to the model by porporato et al 2004 result to be 31 e e m e e 0 h i e γ e h i γ e h i γ e 1 γ γ e γ γ e γ e h i 32 e e m e e 0 h i e γ p γ p γ p h i 1 γ γ p h i γ γ p h i γ p and 33 e e m e e 0 γ e γ p e γ p γ p γ e 1 γ γ e γ γ e γ p fig 3 c shows the curves of eqs 31 and 32 for different values of γe and γp as w 0 increases i e both γe and γp increase catchment characteristics become less important compared to climatic conditions i e rainfall regime and potential evapotranspiration and the curves become closer to each other as γe and γp tend to the model tend to the limits for the turc space in water and energy limited conditions fig 3 d shows the relationship between e e 0 and both γe and γp for a given γp assuming constant w 0 γe can increase because of larger λ and lower em both changes result in larger e e 0 when γe remains constant γp can increase because of lower values of α or larger values of w 0 if α decreases e is expected to decrease as well because of lower water availability thereby leading to lower values of e e 0 likewise if w 0 increases λ em needs to decrease to maintain γe constant causing a lower ratio e e 0 4 comparison between models in an attempt to give a physical interpretation to the parameter n in the mcy model donohue et al 2012 and yang et al 2016 equated eqs 11 and 28 to express the parameter n as a function of γp they thus tried to interpret n as a function of catchment characteristics i e maximum available storage and average daily rainfall depths and referred to this model as the budyko choudhury porporato bcp model in the case of d i 1 donohue et al 2012 approximated the relationship between n and γp as 34 n 0 21 γ p 0 6 while yang et al 2016 extended the fitting to values of di 1 finding a highly non linear relationship between n and both γp and di the same linear relationship between n and γp was used by liu et al 2016 in a modified bcp model that accounts for variations of the parameters in time in the case when d i 1 there is no need to approximate the relationship between n and γp because equating eqs 11 and 28 yields 35 n ln 2 ln 1 γ p γ p 1 e γ p γ γ p γ γ p γ p likewise relationships between γp and the parameter characterizing each of the other parametric formulations of the budyko equation can be found exactly when d i 1 in the case of the model by fu eq 13 one obtains 36 ω ln 2 ln 1 γ p γ p 1 e γ p γ γ p γ γ p γ p and for the model by zhang eq 15 37 ω z e γ p γ p 1 γ p γ γ p γ γ p γ p 2 although the parameter ϵ e i n e is not really related to the ability of a catchment to store water a relationship between ϵ and γp when d i 1 can be obtained mathematically anyway as 38 ϵ 2 γ γ p γ γ p γ p γ γ p γ γ p γ p e γ p γ p γ p 1 these relationships are shown in fig 4 a the parameters ωz and ϵ crosses zero for the same value of γp and they have meaningful values only when γp is larger than 1 558 it is apparent that the linear relationship used in donohue et al 2012 leads to possible large errors when γp 10 this however might not be very important for some of the results reported by donohue et al 2012 and liu et al 2016 because in both cases the value of γp in most sites was estimated to be around 7 i e ω 0 50 mm and α 7 mm conversely for the sites where γp 10 errors in the estimation of the root depth reported by donohue et al 2012 and liu et al 2016 might be expected additionally the sensitivity of e to ecohydrological parameters which entails the derivative of n with respect to ω 0 and α donohue et al 2012 is likely going to be strongly affected by the difference between eqs 34 and 35 because of the large differences between the slopes of these two curves the values of the parameters n ω ωz and ϵ as a function of γp when di 1 cannot be obtained analytically but can be calculated numerically fig 4 b and c for example show n and ω against γp for different values of di specifically for the mcy model nonlinear relationships between n and γp are obtained for values of di lower than about 2 especially for values of γp lower than 5 the values obtained here when di 1 are different from those reported by yang et al 2016 whose calculations achieved a maximum n of about 4 this thus suggests revisiting the results presented by donohue et al 2012 and yang et al 2016 in the light of eq 35 and fig 4 b in the case of the fu model ω is larger than 1 and presents a relationship with γp having patterns similar to n fig 4 c 5 discussion the budyko equation is an attempt to derive a general relationship between p and e simply using long term averages of catchment scale water and energy balances the functional relationship between e p and di described by budyko 1974 can be applied to very large catchments where only climatic conditions drive the water balance but fails when applied to catchments with different physical characteristics in different climatic conditions several equations equivalent to budyko s have been introduced to resolve this issue and these equations have been here partitioned into parametric and water balance models see table 1 the parametric models in eqs 11 13 15 and 17 are commonly used to relate e p to di wang et al 2016 advantages of these equations are their simplicity and the presence of a parameter i e n ω ωz and ϵ that permits their adjustment to most datasets padron et al 2017 an important limitation of these equations is that the parameters do not have a clear physical meaning thus being difficult to estimate unless data of p e 0 and e or q are available both n and ω of the mcy and fu models respectively appear in the relationship between e p and di because of mathematical derivations zhang et al 2001 yang et al 2008 they are assumed to account for some unspecified catchment characteristics and are sometimes suggested to be also related to climate seasonality and variability roderick et al 2014 gudmundsson et al 2016 potter et al 2005 likewise ωz of zhang curves is used as a fitting parameter and its interpretation is related to the ability of catchments to evaporate and transpire water the parameter ϵ of the wang tang model has a more physical origin however it is dependent on e which is unknown and is the variable that the model is expected to estimate as discussed by sposito 2017 the elegance and rigor of the budyko equation is somewhat lost in these models because non climatic parameters appear without sound physical reasons but rather as a consequence of mathematical constraints the values of these parameters are thus difficult to define in un gaged catchments furthermore although parametric models were introduced to account for physical characteristics which greatly affect the water balance in most catchments they satisfy the conditions of the budyko equation eq 6 which hold for very large catchments accordingly as di becomes larger i e p e 0 all the annual rainfall is going to be stored within the catchment and made available for evapotranspiration while as di decreases i e p e 0 the catchment has enough water to fully sustain e 0 only the rate at which different catchments reach these limiting conditions is modulated by the value of the parameters in relation to the mcy fu and wang tang models satisfying these conditions for d i and di 0 leads to the same functional form of the relationships between e p and e e 0 with di and hi respectively therefore when using these three models the budyko and turc spaces become equivalent approaches to study catchment water balance existing lumped stochastic models of catchment scale water balance represent a valuable alternative to synthesize the relationship between e p and di the key advantage of these models is that all the parameters involved have clear physical meaning and can be estimated from measurements donohue et al 2012 furthermore water balance models satisfy more realistic conditions for di 0 and di when e depends on physical catchment characteristics in addition to climatic variables thereby making them suitable for studies where land use change and urbanization are involved when using water balance models the parameters appearing in the budyko and turc spaces are different and a few alternatives to the turc space appear to represent the dependence of e e 0 on catchment characteristics and climate i e eqs 22 24 a comparison between models permits to give a physical interpretation of the parameters in parametric models as a function of physical characteristics identified by the parameters in water balance models eqs 35 37 although combining models might help interpret the meaning of parametric models the closed form equations from water balance models eqs 21 and 31 33 already provide ready to use formulas for the estimation of catchment water balance 6 conclusion many relationships between e p and di are available within the budyko framework parametric and water balance models were here reviewed and linked parametric formulations of the budyko equation i e eqs 11 13 15 and 17 modified the original budyko equation to include non climatic factors affecting the water balance of catchments being developed based only on mathematical constraints these formulations add a parameter that either has no clear physical meaning or depends on e which is the main unknown that the whole framework aims to calculate in this regard these models remain purely descriptive as e and q which these models aim to quantify need to be used to calibrate this new parameter additionally with the exception of the zhang curves which lead to e e 0 when d i h i 1 1 for ωz 1 parametric relationships satisfy the original conditions of the budyko equation for both energy i e wet and water i e dry limited catchments these conditions were derived for very large catchments at continental scale budyko 1974 and are thus inconsistent with the objective of these models to account for catchment characteristics which are predominant in smaller catchments conversely relationships derived from stochastic water balance models present parameters with a clear physical interpretation thus permitting their estimation using measurable variables they challenge the conditions for the limits of very dry i e di and very wet i e di 0 catchments assumed by the budyko framework providing limits that agree with the assumption that catchments across a range of sizes have different abilities to store water to support evapotranspiration water balance models also suggest that the budyko and turc spaces represent different points of view of the water balance becoming equivalent only for very large catchments in summary the advantages of the formulations derived from water balance models make them more amenable to studying water partitioning in catchments of different sizes this is especially important when simple models are applied to predict the effect of anthropogenic interventions on catchment characteristics and thus water resources declaration of competing interest none acknowledgments figures and numerical calculations were obtained using wolfram mathematica 11 2 ed acknowledges the support of the victoria department of economic development jobs transport and resources and the australian research council through the project lp140100871 jy acknowledges support from the national natural science foundation of china grant 51739009 ap acknowledges support from the us national science foundation nsf grants ear 1331846 and ear 1338694 and bp through the carbon mitigation initiative cmi at princeton university we thank michael roderick and an anonymous reviewer for useful suggestions appendix a detailed derivations of stochastic water balance models the stochastic models introduced in section 3 2 are presented in more detail here the water balance of a catchment can be written as a 1 n p z h d s d t p d t e d e m s q d s t where np is soil porosity zh is the soil depth able to store water for evapotranspiration often interpreted as root depth and named as hydrologically active depth and s is relative soil moisture pd t is the daily rainfall rate modeled as a marked poisson process the average rate of rainfall occurrence is λ and depths carried by rainfall events y are independent identically distributed random variables with pdf h y when s is larger than s 1 which is assumed to be between field capacity and full saturation s 1 the rainfall exceeding the available storage is assumed to be lost within the same day as runoff when s is lower than a threshold referred to as wilting point sw water is retained by the soil particles and can no longer be taken up by roots or evaporate the maximum amount of water available for evapotranspiration thus is w 0 n p z h s 1 s w evapotranspiration rates are modeled as a function of available soil moisture eq a 1 can thus be written in terms of the effective relative soil moisture x s s w s 1 s w as in eq 19 which can be re written as a 2 d x d t i d x t ρ e m x where i d p d q d is the soil infiltration rate i e rainfall inputs minus runoff divided by w 0 and ρ e d w 0 because pd is defined as a compound poisson process id results to occur according to a compound poisson process with the same frequency of occurrence λ but distribution of magnitude per event being a truncated distribution of h y to account for fact that x needs to be lower than 1 the distribution of infiltration depths thus depends also on x i e hi y x see laio et al 2001 for details because pd and id are defined via their statistics x can be only found in probabilistic terms the pdf of x p x t can be obtained by solving the master equation associated with eq a 2 which reads cox and miller 1965 rodriguez iturbe et al 1999 a 3 t p c x t x ρ x p c x t λ p c x t λ 0 x p c x z t h i z x z d z λ p 0 t h i x d p 0 t d t λ p 0 t ρ x p c x t x 0 where pc x t is the continuous part of p x t and p 0 t is an atom of probability in x 0 when the rainfall depths are exponentially distributed i e h y γ p exp γ p y with γ p w 0 α α being the average rainfall depth per event the solution of eq a 3 at steady state can be obtained as botter et al 2009 a 4 p c x n ρ x exp γ p x λ 1 ρ x d x p 0 n λ exp λ lim x 0 1 ρ x d x where n is a constant that can be calculated by imposing 0 1 p x d x 0 1 p c x d x p 0 1 the models by milly 1993 and porporato et al 2004 both assume exponentially distributed daily rainfall amounts per event but they use different models for ed a1 milly 1993 milly 1993 assumed a constant daily evapotranspiration rate i e e d e m when x 0 such that ρ e m w 0 η is constant the problem studied by milly 1993 is similar to the takacs process for the virtual waiting time in a single server queuing takacs 1955 cox and miller 1965 in this case eq a 4 reads a 5 p c x λ γ p η λ η e γ p γ p η λ e λ η e γ p x 1 λ x η p 0 e γ p γ p η λ e γ p γ p η λ e λ η because evapotranspiration occurs only when x 0 e can be calculated as the product of em to the percentage of time x 0 i e a 6 e e m 0 1 p c x d x p e γ p 1 d i 1 1 e γ p 1 d i 1 d i 1 where d i e m p η γ p λ when e is divided by p one obtains eq 25 eq a 6 can be re arranged to be expressed in the turc space as a 7 e e m e e 0 h i e γ e 1 h i h i 1 e γ e 1 h i h i h i a 8 e e m e e 0 h i e γ p 1 h i 1 e γ p 1 h i h i and a 9 e e m e e 0 γ e e γ p γ e 1 γ p e γ p γ e γ e a2 porporato et al 2004 porporato et al 2004 improved the modeling of evapotranspiration in milly 1993 by assuming it to depend linearly on x i e e d e m x thereby accounting for reductions of transpiration due to water stress accordingly ρ x e m x w 0 η x and eq a 4 is a truncated gamma distribution that reads a 10 p x p c x γ p λ η x λ η 1 e γ p x γ λ η γ λ η γ p because x decreases exponentially between rainfall events p 0 0 the long term average daily evapotranspiration rate can be calculated as a 11 e e m x e m 0 1 x p x d x p 1 d i γ p γ p d i 1 e γ p γ γ p d i γ γ p d i γ p which when divided by p becomes eq 28 note that γ γ p d i γ γ p d i γ p can be also expressed as d i γ p γ p d i 1 1 f 1 γ p d i 1 γ p d i γ p where 1 f 1 is the confluent hypergeometric function see abramowitz and stegun 1965 6 5 3 and 6 5 12 this alternative expression is more stable for numerical calculations especially as di 0 applying the definition of confluent hypergeometric function see abramowitz and stegun 1965 13 1 2 eq a 11 can be rewritten as a 12 e p e p 1 e γ p n 0 γ p γ p n γ p n d i n when d i the summation tends to 1 and e p tends to 1 e γ p the limit for di 0 can be calculated using the limit for h i of eq 31 which can be re written as a 13 e e 0 h i h i e γ e h i 1 f 1 γ e 1 γ e γ e h i when h i the previous relationship becomes a 14 e e 0 h i 1 f 1 γ e 1 γ e γ e h i h i e γ e h i 1 f 1 γ e 1 γ e γ e h i h i 1 γ e 2 h i γ e 1 h i 1 γ e h i γ e γ e 1 therefore e p tends to γ e γ e 1 d i as di 0 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103435 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
560,the budyko and turc frameworks have become very popular tools for the estimation of catchment scale water balance in their original definition these frameworks and the resulting equations which are considered equivalent apply in the long term to very large catchments whose water balance is predominately driven by climatic factors several equations similar to budyko s and turc s have subsequently been proposed to account also for the effect of catchment characteristics including parametric formulations as well as equations resulting from a simplified physical representation of the water balance after highlighting their advantages and disadvantages we show that models based on the water balance which account for rainfall variability and feedback between water availability and evapotranspiration are more versatile to describe catchment scale rainfall partitioning they include parameters that have clearer physical meaning and therefore can be estimated independently of streamflow and evapotranspiration thereby making them more amenable to practical use in un gaged catchments they show that budyko s and turc s point of view are equivalent only for large catchments additionally water balance models have limiting conditions for extremely dry and wet climates that differ from those of the budyko equations and its parametric formulations as expected in catchments with a finite water storage capacity in these models budyko s and turc s points of view become equivalent only for large catchments keywords water balance budyko framework budyko equation turc equation hydrologic modeling 1 introduction the understanding and estimation of how rainfall is partitioned into the other components of a catchment water balance is one of hydrology s fundamental tasks this is at the foundation of land and water management practices leopold 1997 which often require simple formulae to relate streamflow and evapotranspiration to rainfall and other routinely measured meteorological data one popular method is the framework developed by the russian climatologist budyko oldfield 2016 the method which builds upon the work of other russian researchers andreassian et al 2016 and has strong similarities with the work by turc 1954 results in a functional relationship between long term annual evapotranspiration rates from large catchments and long term annual rainfall and solar radiation budyko 1974 the resulting formula known as the budyko equation is able to fit the observations in several large catchments see e g figure 94 of budyko 1974 on page 325 revealing common patterns across them however observations from catchments with a wide range of sizes and climates present a large scatter around the budyko equation zhang et al 2001 padron et al 2017 to accommodate this variability models similar to the budyko s equation have been developed choudhury 1999 zhang et al 2001 2004 wang and tang 2014 tang and wang 2017 these models use an empirical parameter to adjust the relationship between evapotranspiration and rainfall to the observations despite the lack of a physical meaning considerable effort has been devoted to find an interpretation of such an additional parameter yang et al 2008 zhou et al 2015 to understand its links to environmental variables potter et al 2005 gentine et al 2012 donohue et al 2012 padron et al 2017 and quantify its role in driving changes in catchment water balance roderick and farquhar 2011 zhou et al 2015 gudmundsson et al 2017 equations similar to budyko s have been also derived using water balance models that account for only a few parameters characterizing climate and catchment characteristics milly 1993 porporato et al 2004 like other formulations these equations have parameters that allow for the necessary flexibility to accommodate the observed variability due to their physical interpretation donohue et al 2007 these parameters can be estimated independently of streamflow and evapotranspiration data however probably because these water balance models appear to be more complicated than other existing formulae they have been used less frequently feng et al 2012 donohue et al 2012 feng et al 2015 yang et al 2016 here we discuss advantages and disadvantages of the most commonly used models developed within the budyko framework to include catchment characteristics our analysis highlights that among these formulations the ones based on the water balance lead to equations equivalent to budyko s and turc s without the need to resort to fitting parameters additionally water balance models can possibly support the physical interpretation of parametric models 2 the budyko framework budyko 1974 describes a framework that results in a functional relationship between catchment scale long term averages of annual precipitation p evapotranspiration e and streamflow q the framework is developed by combining the catchment scale long term average annual water and surface energy balances which can be written as 1 p e q 2 r n λ w e h where rn and h are the long term averages of annual net energy available at the soil surface and the sensible heat respectively and λw is the latent heat of vaporization of water it is assumed that the averages are calculated over a period long enough to have negligible changes of the water stored in the catchment similarly in eq 2 it is assumed that the energy stored at the soil surface is negligible and the heat transfer from the soil surface to the soil layers underneath commonly indicated as g oscillates between positive and negative values thus having a negligible long term average the most common use of the budyko framework is the estimation of q and e from measurements of rainfall and meteorological conditions that can be used to estimate potential evapotranspiration for this specific problem the framework is built on the assumption that the water balance is driven solely by climatic factors budyko 1974 thus implying that it can be applied to large catchments the size of which is comparable with the scale of geographical zones see page 330 of budyko 1974 accordingly one can argue that runoff tends to zero for very dry catchments e g deserts implying that the annual rainfall is completely lost via evapotranspiration i e e tends to p in very water limited catchments in wet catchments at the annual scale h is directed from the surface to the atmosphere i e h is positive in almost all the climatic zones on earth therefore h cannot supply much energy to the soil surface and it is thus assumed that rn is providing entirely the energy required for evapotranspiration as a result in wet catchments the upper limit of λwe is rn for this reason the ratio e 0 r n λ w has been assumed as the maximum value of catchment scale long term average annual evapotranspiration thus in catchments that are not water limited e tends to e 0 and these catchments are energy limited we highlight that accounting for the effect of surface temperature on net radiation the upper limit of λwe should be lower than rn as shown by yang and roderick 2019 although in its original derivation e 0 was directly calculated from rn in most applications it is commonly assumed that e 0 is the potential evapotranspiration which depends on rn air humidity wind speed and vegetation cover and type the two variables e 0 and rn can be linked to use the framework for surface energy balance partitioning yin et al 2019 it has also been shown that when using future climate projections a dependence of e 0 on atmospheric co2 concentrations should be included in hydrological models milly and dunne 2016 yang et al 2019 these assumptions for energy p or water e 0 limited catchments can be summarized as 3 e e 0 as p e p as e 0 and imply that in general 4 e f p e 0 if the function f p e 0 were known then e could be estimated from long term measurements of p and estimates of e 0 from meteorological variables routinely measured the number of variables in eq 4 can be reduced by applying the buckingham π theorem barenblatt 1996 because the three variables e p and e 0 have the same dimensions l t 1 i e equivalent depth of water per year both p and e 0 can be selected as a variable of reference when selecting p eq 4 can be re written in terms of dimensionless groups as 5 e p f b d i where d i r n λ w p e 0 p is called dryness index the conditions in eq 3 thus become 6 e p d i as d i 0 e p 1 as d i eqs 5 and 6 state that given the assumptions in eqs 1 and 2 there exists a relationship between e p and di the form of which i e fb di needs to be determined from experimental observations using existing models from non english literature budyko 1974 proposed the relationship 7 e p d i tanh 1 d i 1 e d i 1 2 which was successfully tested against experimental data budyko 1974 and is commonly referred to as the budyko equation several other equations satisfying eqs 5 and 6 have been listed in the literature such as the formulas proposed by schreiber and oldekop zhang et al 2001 andreassian et al 2016 specifically eq 7 is the geometric mean of the formulas by schreiber and oldekop as described by wang et al 2016 when e 0 is selected instead of p as a variable of reference for the application of the buckingham π theorem in eq 4 one obtains 8 e e 0 f t d i 1 f t h i with conditions 9 e e 0 1 as h i e e 0 h i as h i 0 where h i d i 1 is referred to as the humidity index with these assumption turc 1954 proposed the equation 10 e e 0 h i 0 9 h i 2 1 2 usually called the turc equation turc 1954 pike 1964 moussa and lhomme 2016 the two spaces e p vs di and e e 0 vs hi are called budyko and turc spaces respectively moussa and lhomme 2016 3 models of e p vs di the framework developed by budyko has been extensively applied to catchments of different sizes for which catchment characteristics in addition to climatic conditions affect e and q these applications led to the development of models for the relationship between e p and di that modify eq 7 by introducing an additional parameter table 1 these models can be separated in two distinct categories one comprises models derived following steps similar to those in the budyko framework resulting in relationships between e p and di modulated by one additional parameter we will refer to these as parametric models of the budyko equation models in the second category account in simple terms for rainfall runoff processes to derive analytically functions relating e p to di and will be referred to as water balance models of the budyko equation it is stressed that although many hydrological models simulating e could be used to estimate e p vs di gerrits et al 2009 condon and maxwell 2017 the focus here is on catchment water balance models that result in relationships as eq 7 without the use of extensive numerical simulations 3 1 parametric models eq 7 was empirically determined from experimental data and the limiting conditions in eq 6 likewise several models relating e p and di have been derived to satisfy similar conditions as well as accommodate the variability in experimental observations from catchments with different characteristics wang et al 2016 the most commonly used models are briefly reviewed here 3 1 1 mezentsev choudhury yang model the model used by choudhury 1999 was previously presented by the russian researcher v s mezentsev wang et al 2016 roderick and farquhar 2011 yang et al 2008 provided a mathematical justification of this model which will be referred to as mezentse choudhury yang mcy model sposito 2017 the mcy model states that 11 e p 1 1 d i n 1 n where n 0 is an empirical parameter the parameter n as derived by yang et al 2008 appears mathematically as a consequence of assuming that e depends on the long term average soil moisture of a catchment such that e can be expressed using an implicit function as e e p e 0 e as n 0 e p 0 for any value of di while for n the model leads to the theoretical water i e e p 1 for di 1 and energy i e e p d i for di 1 limits of the budyko framework for any values of n 0 the model satisfies the conditions in eq 6 examples of relationships between e p and di from the mcy model are shown in fig 1 a eq 11 can be re arranged to be used in the turc space as 12 e e 0 1 1 h i n 1 n which satisfies the conditions in eq 9 the mcy model thus uses the same functional relationship to relate e p and e e 0 to di and hi respectively the parameter n ranges typically between 0 6 and 3 6 but is mostly from 1 5 to 2 6 as shown in several studies pike 1964 choudhury 1999 roderick and farquhar 2011 donohue et al 2012 3 1 2 fu model the model by fu which was introduced to the english literature by zhang et al 2004 is based on general assumptions about the dependency of changes of e on changes of both p and e 0 the resulting expression reads 13 e p 1 d i 1 d i ω 1 ω where ω is a parameter that needs to be larger than 1 to satisfy the conditions of the budyko framework for di that tends to 0 and as ω 1 e p 0 for any value of di eq 13 can be expressed in the turc space as 14 e e 0 1 h i 1 h i ω 1 ω which satisfies the conditions in eq 9 as for the mcy model according to the fu model e e 0 is related to hi with the same functional relationship used to relate e p to di a value of ω around 2 6 provides a good match between eq 13 and eq 7 as shown in fig 1 b zhang et al 2004 greve et al 2015 using more than 2700 catchments padron et al 2017 found that ω can range from about 1 to 22 with 97 of catchments having ω between 1 06 and 5 5 a strong linear relationship can be found between ω and n in the mcy model as ω n 0 72 yang et al 2008 an in depth comparison between fu and mcy models can be found in andreassian and sari 2019 3 1 3 zhang curves the zhang curves zhang et al 2001 are described by the empirical relationship 15 e p 1 ω z d i 1 ω z d i d i 1 where ωz is a parameter accounting for the ability of different vegetation types to use soil water the value of ωz needs to be larger than d i 1 for e p to be positive using observations from catchments across the world zhang et al 2001 suggested ω z 2 and 0 5 for forested and herbaceous plant catchments respectively as shown in fig 1c e p 1 as d i but fails to properly satisfy the conditions for di 0 because as ωz increases e p becomes larger than di when 0 di 1 this already occurs for the recommended value of ω z 2 when di 0 5 the values of ω z 0 5 has also been recently challenged as a general value for herbaceous catchments dresel et al 2018 because it is not representative of pastures with annual rainfalls lower than about 700 800 mm per year which are common in south australia where the model has been applied in several instances zhang et al 2011 2012 the improper limiting condition of the curves for di 0 can be better seen in the turc space using the relationship 16 e e 0 h i ω z 1 ω z h i 1 h i as shown in fig 2 e becomes larger than e 0 when ωz 1 with e e 0 1 as hi specifically e e 0 crosses the line e e 0 1 at h i d i 1 ω z ω z 1 3 1 4 wang tang model a relationship between e p and di at the annual scale using a form of the water balance more complex than eq 1 was derived by wang and tang 2014 see also wang et al 2015 assuming that p is at first separated into direct runoff and a component w wetting the soil at a second stage w is then partitioned into evapotranspiration and baseflow a portion of w is assumed to be rapidly lost and this loss is called initial evaporation ein with these considerations wang and tang 2014 derived the formula 17 e p 1 d i 1 d i 2 4 ϵ 2 ϵ d i 2 ϵ 2 ϵ where ϵ e i n e 0 ϵ 1 is assumed to be a parameter characterizing a catchment eq 17 is equivalent to the water balance equation of the abcd model wang and tang 2014 thomas et al 1981 when ϵ 0 e p d i 1 d i and when ϵ 1 then e p 1 d i 1 d i 2 which corresponds to the water and energy limited conditions of the budyko framework i e dotted lines in fig 1 d the model presents a lower bound of e p when ϵ 0 fig 1 d and is thus not able to account entirely for the space bounded by the energy and water limits in the budyko space moussa and lhomme 2016 for ϵ 2 2 2 eq 17 is the same as the fu model when ω 2 wang and tang 2014 a limitation of this model is that the dependence of ϵ on e itself is not accounted for additionally assuming constant ϵ implies that ein adjusts its value as e changes while ein should depend on w which is not dependent on e along the same lines a relationship between e p and di can be derived using four parameters one of which again depends on e itself tang and wang 2017 the combination of these parameters make ϵ appear again in the relationship with the same issues mentioned above the same model has been recently further developed to be applied to human water consumption lei et al 2018 as for the mcy and fu model the relationship in the turc space for the wang tang model is as in eq 17 and reads 18 e e 0 1 h i 1 h i 2 4 ϵ 2 ϵ h i 2 ϵ 2 ϵ 3 2 water balance models lumped models of catchment scale water balance applicable to catchments with different sizes can be developed milly 1993 porporato et al 2004 by considering a process based description of evapotranspiration and runoff formation and by accounting for rainfall variability these models can explain the variability of experimentally obtained relationships between e p and di with the addition of one parameter that has a clear physical meaning details of these models can be found in milly 1993 rodriguez iturbe et al 1999 laio et al 2001 and porporato et al 2004 a brief description is provided here with the mathematical details reported in appendix a we will focus here on models with constant parameters which lead to closed form relationships between the relevant variables involved in the water balance seasonal cycles of precipitation and potential evaporation can be accounted for as in feng et al 2012 2015 at the expense analytical tractability the models described in milly 1993 and porporato et al 2004 start from the description of the catchment water balance at the daily timescale which is the typical scale of soil moisture dynamics and derive statistics of the water balance over a long term describing a catchment as a reservoir of finite capacity the daily catchment scale water balance can be written as 19 w 0 d x d t p d t e d e m x q d x t where w 0 is the maximum soil water storage available for evapotranspiration x is the effective relative soil moisture i e the percentage of soil water that can support evapotranspiration pd ed and qd are the daily rainfall evapotranspiration and runoff rates respectively given the long timescales of interest to these models rainfall variability is accounted for in statistical terms in particular pd is assumed to occur according to a poisson process with a constant rate λ and each event delivers a daily rainfall depth drawn from a given probability distribution with an average α although different rainfall depth distributions have been used in the literature daly and porporato 2010 verma et al 2011 the models presented here assume daily rainfall depths exponentially distributed ed depends on the water stored in the catchment which is available to the vegetation to be transpired and the daily potential evapotranspiration rate em which depends on climatic conditions as well as vegetation cover because the effect of daily variations of em on the water balance have been shown to be less important when compared to the variations of daily rainfall em can be assumed to be constant in time daly and porporato 2006 qd occurs when a rainfall event carries an amount of water larger than what the soil can accommodate at that particular day when the event occurs qd thus depends on pd and x which is in turn dependent on ed over a long time assuming that the parameters λ and α remain constant in time and that the catchment characteristics do not change the catchment reaches an equilibrium characterized by a constant average soil water storage x this assumption is also the starting point of the derivation of eq 13 provided by yang et al 2008 for long timescales the annual water balance can thus be written as in eq 1 with p e and q calculated from the respective average daily rates as p p t y e e t y and q q t y where ty is the number of days in the year and p e and q are the long term averages of pd ed and qd respectively likewise e 0 e m t y depending on the specific expression adopted to model ed as a function of x the solution of eq 19 permits the derivation of relationships between e p and di that provide alternative forms of the budyko and turc equations as described in detail in sections 3 2 2 and 3 2 3 3 2 1 budyko and turc spaces the interpretation of the stochastic soil water balance using dimensionless groups touched on by porporato et al 2004 and described in detail by feng et al 2012 is briefly recalled here highlighting the links with the budyko and turc spaces as well as the conditions for di 0 hi and di hi 0 following feng et al 2012 and according to the typical time scale of eq 19 it is natural to perform the dimensional analysis at the daily time scale which is presented first this is then extended to the annual scale to clearly show its analogy to the budyko and turc frameworks according to eq 19 at the daily time scale ed depends on em and x with x being driven by pd and w 0 which defines how much rainfall can be lost via qd once the parameter w 0 is introduced in the framework it is no longer sufficient to consider the long term average annual rainfall but the way rainfall is delivered to catchments is also important for a given w 0 if rainfall is delivered as a frequent succession of small rainfall events i e large λ and small α there might be a large amount of water available for evapotranspiration conversely if rainfall is delivered in the form of a few large events i e small λ and large α most of it might be lost as runoff and a small amount remains available for evapotranspiration accordingly eq 4 can be generalized as 20 e f λ α e m w 0 the dimensions of the variables involved are e e m l t 1 λ t 1 and α ω 0 l with l and t units of length and time respectively using the buckingham π theorem barenblatt 1996 when λ and α are selected as dimensionally independent variables one obtains a general relationship in the budyko space that reads 21 e p e λ α f s e m λ α w 0 α f s e m p w 0 α f s d i γ p because e and p are proportionally related to e and p via the number of days in a year ty it follows that e p e p such that fs in eq 21 is an alternative formulation of the budyko equation eq 7 the additional parameter γp defines how much water the catchment can retain with respect to the average daily rainfall depth the form of fs in eq 21 depends on the function chosen to relate ed to em and x as well as the distribution of water depths per rainfall event in the turc space eq 20 requires the selection of em as dimensionally independent variable there are thus several combinations of variables that lead to different relationships selecting either λ α or w 0 as dimensionally independent variable yields 22 e e m e e 0 f λ λ α e m λ w 0 e m f λ h i γ e 23 e e m e e 0 f α λ α e m w 0 α f α h i γ p and 24 e e m e e 0 f w 0 λ w 0 e m w 0 α f w 0 γ e γ p in these equations e e m e e 0 because of the same constant of proportionality i e ty between the variables at daily and annual scales therefore eqs 22 24 are alternative formulations of eq 8 the parameter γe represents the ratio between the rainfall frequency λ and the frequency at which w 0 is depleted completely when water is extracted at the constant rate em these three equations are equivalent and can be derived one from the other using h i γ e γ p 1 3 2 2 milly model starting from eq 19 and considering a constant daily evapotranspiration rate e d e m milly 1993 obtained see appendix a 25 e p e p e γ p 1 d i 1 1 e γ p 1 d i 1 d i 1 which satisfies the conditions 26 e p e p d i as d i 0 27 e p e p g γ p 1 e γ p as d i the condition for di 0 is as the budyko equation eq 7 while the condition for di reflects the role of the parameter γp in characterizing the catchment as γ p i e when the storage of a catchment is much larger than the average water depth per rainfall event the model tends to the limiting conditions of the budyko equation it is also noted that contrarily to what stated by wang et al 2016 eq 25 exists for any value of di with no discontinuities at d i 1 fig 3 a shows a comparison between the budyko equation and eq 25 for different values of γp as already stressed in milly 1993 the model is not capable to reproduce the budyko curve milly 1994 modified the model by adding a seasonal cycle to precipitation and evapotranspiration although the model was successfully applied against observed data it was still not able to reproduce the budyko curve with any combination of parameters the model by milly 1993 can be also expressed in the turc space in terms of e e 0 as shown in appendix a 3 2 3 porporato et al model differently from milly 1993 the model by porporato et al 2004 assumed e d e m x thus accounting for a reduction of evapotranspiration when soil water decreases this leads to see appendix a 28 e p e p 1 d i γ p γ p d i 1 e γ p γ γ p d i γ γ p d i γ p where γ and γ are the gamma and incomplete gamma functions abramowitz and stegun 1965 respectively with the following conditions see appendix a for details 29 e p e p γ p d i γ p d i as d i 0 30 e p e p g γ p 1 e γ p as d i which are both different from those of the budyko framework fig 3 b shows a comparison of eq 28 with the budyko equation for different values of γp as already highlighted by porporato et al 2004 eq 28 with γp 5 5 approximates eq 7 across the range of values of di often encountered in practical applications as di increases e p tends to 1 in the budyko equation while it tends to 1 e 5 5 0 996 for eq 28 the value of γ p 5 5 corresponds to a soil depth of about 0 3 0 4 m which is comparable to the average root depth in water limited ecosystems schenk and jackson 2002 as in eq 25 and fig 3 a when γp the conditions of the budyko framework are valid and the model results in the curves e p d i when di 1 and e p 1 when di 1 the expressions of eqs 22 24 according to the model by porporato et al 2004 result to be 31 e e m e e 0 h i e γ e h i γ e h i γ e 1 γ γ e γ γ e γ e h i 32 e e m e e 0 h i e γ p γ p γ p h i 1 γ γ p h i γ γ p h i γ p and 33 e e m e e 0 γ e γ p e γ p γ p γ e 1 γ γ e γ γ e γ p fig 3 c shows the curves of eqs 31 and 32 for different values of γe and γp as w 0 increases i e both γe and γp increase catchment characteristics become less important compared to climatic conditions i e rainfall regime and potential evapotranspiration and the curves become closer to each other as γe and γp tend to the model tend to the limits for the turc space in water and energy limited conditions fig 3 d shows the relationship between e e 0 and both γe and γp for a given γp assuming constant w 0 γe can increase because of larger λ and lower em both changes result in larger e e 0 when γe remains constant γp can increase because of lower values of α or larger values of w 0 if α decreases e is expected to decrease as well because of lower water availability thereby leading to lower values of e e 0 likewise if w 0 increases λ em needs to decrease to maintain γe constant causing a lower ratio e e 0 4 comparison between models in an attempt to give a physical interpretation to the parameter n in the mcy model donohue et al 2012 and yang et al 2016 equated eqs 11 and 28 to express the parameter n as a function of γp they thus tried to interpret n as a function of catchment characteristics i e maximum available storage and average daily rainfall depths and referred to this model as the budyko choudhury porporato bcp model in the case of d i 1 donohue et al 2012 approximated the relationship between n and γp as 34 n 0 21 γ p 0 6 while yang et al 2016 extended the fitting to values of di 1 finding a highly non linear relationship between n and both γp and di the same linear relationship between n and γp was used by liu et al 2016 in a modified bcp model that accounts for variations of the parameters in time in the case when d i 1 there is no need to approximate the relationship between n and γp because equating eqs 11 and 28 yields 35 n ln 2 ln 1 γ p γ p 1 e γ p γ γ p γ γ p γ p likewise relationships between γp and the parameter characterizing each of the other parametric formulations of the budyko equation can be found exactly when d i 1 in the case of the model by fu eq 13 one obtains 36 ω ln 2 ln 1 γ p γ p 1 e γ p γ γ p γ γ p γ p and for the model by zhang eq 15 37 ω z e γ p γ p 1 γ p γ γ p γ γ p γ p 2 although the parameter ϵ e i n e is not really related to the ability of a catchment to store water a relationship between ϵ and γp when d i 1 can be obtained mathematically anyway as 38 ϵ 2 γ γ p γ γ p γ p γ γ p γ γ p γ p e γ p γ p γ p 1 these relationships are shown in fig 4 a the parameters ωz and ϵ crosses zero for the same value of γp and they have meaningful values only when γp is larger than 1 558 it is apparent that the linear relationship used in donohue et al 2012 leads to possible large errors when γp 10 this however might not be very important for some of the results reported by donohue et al 2012 and liu et al 2016 because in both cases the value of γp in most sites was estimated to be around 7 i e ω 0 50 mm and α 7 mm conversely for the sites where γp 10 errors in the estimation of the root depth reported by donohue et al 2012 and liu et al 2016 might be expected additionally the sensitivity of e to ecohydrological parameters which entails the derivative of n with respect to ω 0 and α donohue et al 2012 is likely going to be strongly affected by the difference between eqs 34 and 35 because of the large differences between the slopes of these two curves the values of the parameters n ω ωz and ϵ as a function of γp when di 1 cannot be obtained analytically but can be calculated numerically fig 4 b and c for example show n and ω against γp for different values of di specifically for the mcy model nonlinear relationships between n and γp are obtained for values of di lower than about 2 especially for values of γp lower than 5 the values obtained here when di 1 are different from those reported by yang et al 2016 whose calculations achieved a maximum n of about 4 this thus suggests revisiting the results presented by donohue et al 2012 and yang et al 2016 in the light of eq 35 and fig 4 b in the case of the fu model ω is larger than 1 and presents a relationship with γp having patterns similar to n fig 4 c 5 discussion the budyko equation is an attempt to derive a general relationship between p and e simply using long term averages of catchment scale water and energy balances the functional relationship between e p and di described by budyko 1974 can be applied to very large catchments where only climatic conditions drive the water balance but fails when applied to catchments with different physical characteristics in different climatic conditions several equations equivalent to budyko s have been introduced to resolve this issue and these equations have been here partitioned into parametric and water balance models see table 1 the parametric models in eqs 11 13 15 and 17 are commonly used to relate e p to di wang et al 2016 advantages of these equations are their simplicity and the presence of a parameter i e n ω ωz and ϵ that permits their adjustment to most datasets padron et al 2017 an important limitation of these equations is that the parameters do not have a clear physical meaning thus being difficult to estimate unless data of p e 0 and e or q are available both n and ω of the mcy and fu models respectively appear in the relationship between e p and di because of mathematical derivations zhang et al 2001 yang et al 2008 they are assumed to account for some unspecified catchment characteristics and are sometimes suggested to be also related to climate seasonality and variability roderick et al 2014 gudmundsson et al 2016 potter et al 2005 likewise ωz of zhang curves is used as a fitting parameter and its interpretation is related to the ability of catchments to evaporate and transpire water the parameter ϵ of the wang tang model has a more physical origin however it is dependent on e which is unknown and is the variable that the model is expected to estimate as discussed by sposito 2017 the elegance and rigor of the budyko equation is somewhat lost in these models because non climatic parameters appear without sound physical reasons but rather as a consequence of mathematical constraints the values of these parameters are thus difficult to define in un gaged catchments furthermore although parametric models were introduced to account for physical characteristics which greatly affect the water balance in most catchments they satisfy the conditions of the budyko equation eq 6 which hold for very large catchments accordingly as di becomes larger i e p e 0 all the annual rainfall is going to be stored within the catchment and made available for evapotranspiration while as di decreases i e p e 0 the catchment has enough water to fully sustain e 0 only the rate at which different catchments reach these limiting conditions is modulated by the value of the parameters in relation to the mcy fu and wang tang models satisfying these conditions for d i and di 0 leads to the same functional form of the relationships between e p and e e 0 with di and hi respectively therefore when using these three models the budyko and turc spaces become equivalent approaches to study catchment water balance existing lumped stochastic models of catchment scale water balance represent a valuable alternative to synthesize the relationship between e p and di the key advantage of these models is that all the parameters involved have clear physical meaning and can be estimated from measurements donohue et al 2012 furthermore water balance models satisfy more realistic conditions for di 0 and di when e depends on physical catchment characteristics in addition to climatic variables thereby making them suitable for studies where land use change and urbanization are involved when using water balance models the parameters appearing in the budyko and turc spaces are different and a few alternatives to the turc space appear to represent the dependence of e e 0 on catchment characteristics and climate i e eqs 22 24 a comparison between models permits to give a physical interpretation of the parameters in parametric models as a function of physical characteristics identified by the parameters in water balance models eqs 35 37 although combining models might help interpret the meaning of parametric models the closed form equations from water balance models eqs 21 and 31 33 already provide ready to use formulas for the estimation of catchment water balance 6 conclusion many relationships between e p and di are available within the budyko framework parametric and water balance models were here reviewed and linked parametric formulations of the budyko equation i e eqs 11 13 15 and 17 modified the original budyko equation to include non climatic factors affecting the water balance of catchments being developed based only on mathematical constraints these formulations add a parameter that either has no clear physical meaning or depends on e which is the main unknown that the whole framework aims to calculate in this regard these models remain purely descriptive as e and q which these models aim to quantify need to be used to calibrate this new parameter additionally with the exception of the zhang curves which lead to e e 0 when d i h i 1 1 for ωz 1 parametric relationships satisfy the original conditions of the budyko equation for both energy i e wet and water i e dry limited catchments these conditions were derived for very large catchments at continental scale budyko 1974 and are thus inconsistent with the objective of these models to account for catchment characteristics which are predominant in smaller catchments conversely relationships derived from stochastic water balance models present parameters with a clear physical interpretation thus permitting their estimation using measurable variables they challenge the conditions for the limits of very dry i e di and very wet i e di 0 catchments assumed by the budyko framework providing limits that agree with the assumption that catchments across a range of sizes have different abilities to store water to support evapotranspiration water balance models also suggest that the budyko and turc spaces represent different points of view of the water balance becoming equivalent only for very large catchments in summary the advantages of the formulations derived from water balance models make them more amenable to studying water partitioning in catchments of different sizes this is especially important when simple models are applied to predict the effect of anthropogenic interventions on catchment characteristics and thus water resources declaration of competing interest none acknowledgments figures and numerical calculations were obtained using wolfram mathematica 11 2 ed acknowledges the support of the victoria department of economic development jobs transport and resources and the australian research council through the project lp140100871 jy acknowledges support from the national natural science foundation of china grant 51739009 ap acknowledges support from the us national science foundation nsf grants ear 1331846 and ear 1338694 and bp through the carbon mitigation initiative cmi at princeton university we thank michael roderick and an anonymous reviewer for useful suggestions appendix a detailed derivations of stochastic water balance models the stochastic models introduced in section 3 2 are presented in more detail here the water balance of a catchment can be written as a 1 n p z h d s d t p d t e d e m s q d s t where np is soil porosity zh is the soil depth able to store water for evapotranspiration often interpreted as root depth and named as hydrologically active depth and s is relative soil moisture pd t is the daily rainfall rate modeled as a marked poisson process the average rate of rainfall occurrence is λ and depths carried by rainfall events y are independent identically distributed random variables with pdf h y when s is larger than s 1 which is assumed to be between field capacity and full saturation s 1 the rainfall exceeding the available storage is assumed to be lost within the same day as runoff when s is lower than a threshold referred to as wilting point sw water is retained by the soil particles and can no longer be taken up by roots or evaporate the maximum amount of water available for evapotranspiration thus is w 0 n p z h s 1 s w evapotranspiration rates are modeled as a function of available soil moisture eq a 1 can thus be written in terms of the effective relative soil moisture x s s w s 1 s w as in eq 19 which can be re written as a 2 d x d t i d x t ρ e m x where i d p d q d is the soil infiltration rate i e rainfall inputs minus runoff divided by w 0 and ρ e d w 0 because pd is defined as a compound poisson process id results to occur according to a compound poisson process with the same frequency of occurrence λ but distribution of magnitude per event being a truncated distribution of h y to account for fact that x needs to be lower than 1 the distribution of infiltration depths thus depends also on x i e hi y x see laio et al 2001 for details because pd and id are defined via their statistics x can be only found in probabilistic terms the pdf of x p x t can be obtained by solving the master equation associated with eq a 2 which reads cox and miller 1965 rodriguez iturbe et al 1999 a 3 t p c x t x ρ x p c x t λ p c x t λ 0 x p c x z t h i z x z d z λ p 0 t h i x d p 0 t d t λ p 0 t ρ x p c x t x 0 where pc x t is the continuous part of p x t and p 0 t is an atom of probability in x 0 when the rainfall depths are exponentially distributed i e h y γ p exp γ p y with γ p w 0 α α being the average rainfall depth per event the solution of eq a 3 at steady state can be obtained as botter et al 2009 a 4 p c x n ρ x exp γ p x λ 1 ρ x d x p 0 n λ exp λ lim x 0 1 ρ x d x where n is a constant that can be calculated by imposing 0 1 p x d x 0 1 p c x d x p 0 1 the models by milly 1993 and porporato et al 2004 both assume exponentially distributed daily rainfall amounts per event but they use different models for ed a1 milly 1993 milly 1993 assumed a constant daily evapotranspiration rate i e e d e m when x 0 such that ρ e m w 0 η is constant the problem studied by milly 1993 is similar to the takacs process for the virtual waiting time in a single server queuing takacs 1955 cox and miller 1965 in this case eq a 4 reads a 5 p c x λ γ p η λ η e γ p γ p η λ e λ η e γ p x 1 λ x η p 0 e γ p γ p η λ e γ p γ p η λ e λ η because evapotranspiration occurs only when x 0 e can be calculated as the product of em to the percentage of time x 0 i e a 6 e e m 0 1 p c x d x p e γ p 1 d i 1 1 e γ p 1 d i 1 d i 1 where d i e m p η γ p λ when e is divided by p one obtains eq 25 eq a 6 can be re arranged to be expressed in the turc space as a 7 e e m e e 0 h i e γ e 1 h i h i 1 e γ e 1 h i h i h i a 8 e e m e e 0 h i e γ p 1 h i 1 e γ p 1 h i h i and a 9 e e m e e 0 γ e e γ p γ e 1 γ p e γ p γ e γ e a2 porporato et al 2004 porporato et al 2004 improved the modeling of evapotranspiration in milly 1993 by assuming it to depend linearly on x i e e d e m x thereby accounting for reductions of transpiration due to water stress accordingly ρ x e m x w 0 η x and eq a 4 is a truncated gamma distribution that reads a 10 p x p c x γ p λ η x λ η 1 e γ p x γ λ η γ λ η γ p because x decreases exponentially between rainfall events p 0 0 the long term average daily evapotranspiration rate can be calculated as a 11 e e m x e m 0 1 x p x d x p 1 d i γ p γ p d i 1 e γ p γ γ p d i γ γ p d i γ p which when divided by p becomes eq 28 note that γ γ p d i γ γ p d i γ p can be also expressed as d i γ p γ p d i 1 1 f 1 γ p d i 1 γ p d i γ p where 1 f 1 is the confluent hypergeometric function see abramowitz and stegun 1965 6 5 3 and 6 5 12 this alternative expression is more stable for numerical calculations especially as di 0 applying the definition of confluent hypergeometric function see abramowitz and stegun 1965 13 1 2 eq a 11 can be rewritten as a 12 e p e p 1 e γ p n 0 γ p γ p n γ p n d i n when d i the summation tends to 1 and e p tends to 1 e γ p the limit for di 0 can be calculated using the limit for h i of eq 31 which can be re written as a 13 e e 0 h i h i e γ e h i 1 f 1 γ e 1 γ e γ e h i when h i the previous relationship becomes a 14 e e 0 h i 1 f 1 γ e 1 γ e γ e h i h i e γ e h i 1 f 1 γ e 1 γ e γ e h i h i 1 γ e 2 h i γ e 1 h i 1 γ e h i γ e γ e 1 therefore e p tends to γ e γ e 1 d i as di 0 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103435 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
561,typically when using data assimilation to improve hydrologic forecasting observations are assimilated up to the start of the forecast this is done to provide more accurate state and parameter estimates which in turn allows for a better forecast we propose an extension to the traditional data assimilation approach which allows for assimilation to continue into the forecast to further improve the forecast s performance and reliability this method was tested on two small highly urbanized basins in southern ontario canada the don river and black creek basins using a database of forcing data model states predicted streamflow and streamflow observations a lookup function was used to provide an observation during the forecast which can be assimilated this allows for an indirect way to assimilate the numerical weather prediction forcing data this approach can help in addressing prediction uncertainty since an ensemble of previous observations can be pulled from the database which correspond to the forecast probability density function given previous information the results show that extending data assimilation into the forecast can improve forecast performance in these urban basins and it was shown that the forecast reliability could be improved by up to 78 keywords data assimilation ensemble kalman filter particle filter short term forecast nearest neighbour artificial neural network 1 introduction with our changing climate extreme weather events especially heavy rainfalls are becoming more common which leads increasing need for improved flood forecasting methods historically deterministic methods were used to predict floods with a chosen model being forced with a deterministic numerical weather prediction nwp product but we know that this method is not ideal advances over the last two decades have led to ensemble and probabilistic approaches to better improve flood forecasting as well as to quantify the associated uncertainties related to the forecast liu et al 2012 these more advanced forecasting and uncertainty quantification methods include sequential data assimilation sda liu et al 2012 moradkhani et al 2005 bayesian forecasting system bfs biondi and de luca 2012 han and coulibaly 2019 krzysztofowicz 1999 bayesian model averaging bma najafi and moradkhani 2016 raftery et al 2005 and model conditional processor mcp todini 2008 with improved methods we are able to more accurately predict floods sooner leading to a faster response and a reduced associated cost sequential data assimilation is a popular method used in hydrologic modeling to improve hydrologic and flood forecasting as an application of bayes theorem sda can incorporate the uncertainty from both the model and observations to update boundary conditions and improve forecasts there are several sda methods which have been used for hydrologic forecasting such as the extended kalman filter ekf sun et al 2015 ensemble kalman filter enkf leach et al 2018 moradkhani et al 2005 samuel et al 2014 thiboult et al 2015 vrugt and robinson 2007 ensemble kalman smoother enks crow and ryu 2009 li et al 2013 evolutionary data assimilation eda dumedah and coulibaly 2013 the particle filter pf dechant and moradkhani 2014 moradkhani et al 2005 parrish et al 2012 yan and moradkhani 2016 and most recently the evolutionary pf hybrid methods abbaszadeh et al 2019 2018 ju et al 2019 zhu et al 2018 with the enkf and pf being popular due to them being relatively simple to implement additionally various types of observations have been successfully assimilated into hydrologic models using these sda techniques such as streamflow abaza et al 2017 abbaszadeh et al 2018 yan and moradkhani 2016 soil moisture ju et al 2019 leach et al 2018 meng et al 2017 yan and moradkhani 2016 and snow water equivalent dziubanski and franz 2016 huang et al 2017 leach et al 2018 smyth et al 2019 these data sets are obtained from in situ gauges or remote sensing techniques through assimilating these observations both models states and parameters can be updated to improve hydrologic simulation and forecasting leach et al 2018 moradkhani et al 2005 samuel et al 2014 one downside to hydrologic da and modeling in general is limited observational data due to many locations being ungauged this causes issues for sda with distributed models as it has been shown that sda in distributed modeling is better when more observations are available from multiple locations clark et al 2008 rakovec et al 2012 xie and zhang 2010 this data limitation also impacts forecasts since the ability to update is limited by observations being available to assimilate at the start of a forecast if a real time gauge or a gauge in general does not exist then there may not be observations available at the beginning of the forecast to be used by sda techniques to update the states and parameters due to the fact that these sda methods are ensemble forecasting methods they are limited in that they can address nwp uncertainty and merge model and observation uncertainty but are not guaranteed to address prediction uncertainty biondi and todini 2018 in this study we aim to address the limitation of sda during the forecast mode specifically the lack of available observations to assimilate since there are none to do this we first build a database for our selected watershed the database contains results from a chosen hydrologic model such as predicted streamflows predicted states the forcings used to drive the model and the corresponding observed streamflows this database essentially stores how a specific watershed responds to forcings given some initial conditions and it is used to generate a probability density function pdf of pseudo observations y conditional on available information i denoted as f y i biondi and todini 2018 this pdf is then assimilated back into the model at each forecast step with the goal of improving the forecast s performance and reliability this method could potentially also be extended to ungauged basins through the use of regionalization techniques to generate a database for any basin although that is not explored within this work the proposed methodology will allow for data assimilation to be extended into the forecast period by using our prior knowledge of how the model responds to different inputs in doing so we can better account for the prediction uncertainty with sda and provide better state estimates at each forecast horizon additionally during a dual state and parameter updating scheme this method will allow for the model parameters to be calibrated to the chosen weather forecast product real time calibration to the weather forecast is beneficial since models are traditionally calibrated using historical gauge data however model parameter distributions can vary depending on what forcing data products are used to calibrate the model kornelsen and coulibaly 2016 2 study areas and data the study areas being focused on are the black creek and don river basins in toronto ontario canada fig 1 these watersheds are managed by the toronto region conservation authority trca both the black creek basin bcb and the don river basin drb are highly urbanized being roughly 97 and 93 developed respectively agriculture and agri food canada 2015 these basins have an average daily temperature of 8 0 c an average daily minimum and maximum temperatures of 3 4 c to 12 5 c respectively and an average annual precipitation of 841 1 mm year based on the 1981 2010 canadian climate normals environment and climate change canada 2017 hourly data was provided by the trca environment and climate change canada eccc and water survey of canada wsc 20080101 to 20171231 gaps in the precipitation data were infilled using a combination of disaggregated daily to hourly eccc precipitation data and ordinary kriging the disaggregation was performed using multiplicative random cascade mrc based disaggregation ganguli and coulibaly 2017 olsson 1998 1995 ordinary kriging was also used to infill gaps in the temperature data forecast forcing data was obtained from the regional deterministic prediction system rdps the rdps provides four 48 h forecasts per day in 3 h increments on a 10 km grid the 00 h forecast was used for this study with 3 h precipitation and temperature data from 20150401 to 20171231 3 methodology 3 1 hydrologic model hymod is a lumped hydrologic model based on the probability distributed moisture model pdm moore 1985 for this study hymod was modified to include the degree day snow routine from samuel et al 2011 and the simplified thornthwaite evapotranspiration method samuel et al 2011 the model was calibrated using a 3 h time interval this timestep was chosen to match the rdps dataset s forecast timestep each watershed model was calibrated using the dynamically dimensioned search dds algorithm tolson and shoemaker 2007 which has one algorithm parameter r used to control the neighborhood size for the random search using dds the calibration period was 20090101 to 20121231 and the validation period was 20130101 to 20151231 with the period of 20080101 to 20081231 used for spin up the nash volume efficiency nve from samuel et al 2012 with modified weights was the chosen objective function the nve is defined as follows 1 n v e 0 3 n s e 0 1 v e 0 2 n s e l o g 0 5 n s e s q r 2 n s e 1 t 1 t y t y t 2 t 1 t y t e y 2 3 v e e y y e y where nse is the nash sutcliffe efficiency nash and sutcliffe 1970 nselog is the nse calculated using log transformed runoff low flow nsesqr is the nse calculated using the squared runoff high flow ve is the volume error yt is the observed runoff at time t y t is the simulated runoff at time t and e is the expectation function descriptions for the hymod model parameters and states are listed in table 1 the calibrated parameter sets for both the bcb and drb are also provided in table 1 along with their parameter ranges and associated units 3 2 data assimilation for hydrologic forecasting the benefit of sda is that it can take new information at each time step and update our prior estimate of the states and parameters to provide a more accurate posterior which leads to a better forecast the posterior distribution of states at time t can be found using the recursive bayes theorem 4 p x t y 1 t p x t y t y 1 t 1 p y t x t p x t y 1 t 1 p y t x t p x t y 1 t 1 d x t where p xt y 1 t is the posterior distribution p xt y 1 t 1 is the prior distribution p yt xt is the likelihood for time t and p y t x t p x t y 1 t 1 d x t is the normalization factor since the analytic solution is generally intractable a numerical approximation can be made using monte carlo methods 3 2 1 ensemble kalman filter the ensemble kalman filter enkf is a monte carlo based sda method that can be used to optimally combine model and observation uncertainties to provide an improved forecast evensen 2003 1994 the enkf is a commonly used data assimilation method which can be used to update model states and parameters its update procedure is shown as follows moradkhani et al 2005 5 x t f x t 1 θ u t ζ t 6 y t h x t θ ν t 7 k t cov x t y t var y t r 8 x t x t k t y t y t where x t is the predicted ensemble of states for time t θ is the vector of model parameters u t is the ensemble of perturbed forcing data at time t f is the state propagation functions which moves them forward in time and ζ t is the random noise which represents process error the superscripts and represent the prior and posterior updated states respectively the hydrologic model is represented by h y t is the predicted runoff at time t and ν t is noise which represents model uncertainty the kalman gain k t at time t is found by dividing the cross covariance of predicted states and runoff by the variance of the innovation the difference of the observed and predicted runoff the observed runoff y t is perturbed by random noise n 0 r to generate an ensemble of observations with r being the prior known observation uncertainty the difference of the observed and predicted runoff is then multiplied by the kalman gain and added to the prior state estimate to create the posterior state estimate this process eqs 5 to 8 is repeated until there are no more observations or the spin up period is complete 3 2 2 particle filter another common sda method is the particle filter pf as with the enkf the pf can be used to update model states and parameters moradkhani 2008 moradkhani et al 2005 there are many variants of the particle filter that utilize different particle resampling methods four resampling methods were selected for this analysis they were the residual resampling rr method liu and chen 1998 two versions of the sample importance resampling method sir and sirv smith and gelfand 1992 and the markov chain monte carlo mcmc method leisenring and moradkhani 2012 moradkhani et al 2012 yan et al 2015 yan and moradkhani 2016 the sir and rr methods add constant noise to the particles at every timestep in order to add particle diversity and combat degeneracy liu and chen 1998 while the sirv and mcmc methods add variable noise using the variable variance multiplier vvm to combat particle degeneracy leisenring and moradkhani 2012 moradkhani et al 2012 the general formulation of the pf in conjunction with eqs 5 and 6 is as follows gordon et al 1993 moradkhani et al 2005 9 l y t x i t 1 2 π r exp y t ε i t y i t 2 2 r 10 p y t x i t l y t x i t i 1 n l y t x i t 11 w i t w i t p y t x i t i 1 n w i t p y t x i t 12 p x t y 1 t i 1 n w i t δ x t x i t where n is the number of particles w i t is the prior weight set to 1 n for particle i at time t w i t is the posterior weight for particle i at time t δ is the dirac delta function ɛ i t n 0 r is random noise which represents observation error and l is the gaussian likelihood function the posterior estimate can then be resampled to eliminate low weight particles using the chosen resampling method there are many resampling methods that can be used with the pf including the rr sir and mcmc resampling methods the rr method for reweighting residual particles particles with a weight less than 1 n is 13 w i t n w i t n w i t n i 1 n n w i t where w i t are the weights of the n residual particles n n i 1 n n w i t that will be used to construct an empirical cumulative distribution function cdf this cdf is then resampled from to increase the number of particles from n to n after which the particle weights are set to 1 n when using sir like rr a cdf of the particle weights is constructed from this cdf n particles are resampled proportionally to their weights with higher weighted particles being resampled more often when their probability is higher than uniform probability the mcmc resampling method is an extension of the sir method it uses a metropolis acceptance ratio α to decide if an update should be accepted or rejected 14 α min 1 p x t p y 1 t p x t y 1 t the proposed joint probability distribution of the updated particles is p x t p y 1 t and the particles are updated when the acceptance ratio exceeds a threshold value generated from a uniform random variable u u 0 1 3 3 pseudo observation lookup routine to provide pseudo observations during the forecast period a database was built that contains past information about the watershed by using this database data assimilation can be extended into the forecast period this database contains the hymod state values table 1 the precipitation and temperature data used to force the model and generate those state values and the predicted streamflow associated with those states and forcings for each timestep during the time period allocated for building the database additionally the database contains the corresponding observed streamflow values and the prediction error values for each timestep in the database to evaluate the influence forcing data uncertainty may have on the lookup methods two databases were built for this study using different forcing datasets the first database was built using historical precipitation and temperature gauge data to run hymod denoted as the historical observation database hdb the hdb was built using gauge data from 20090101 to 20161231 and results from each timesteps were archived the second database was created using archived rdps precipitation and temperature denoted as the forecast forcing database fdb since the available archive of rdps forecasts is much shorter than that of the gauges the period of 20150401 to 20161231 was used to build the fdb three lookup methods were evaluated and used to pull pseudo observations from the pre built databases the lookup methods are k nearest neighbour k nn direct lookup dl and a feedforward neural network fnn each method was evaluated based on how they affected the performance of the forecasts the term pseudo observation was used to describe the values pulled from the database because they are true observations that correspond to past vectors of states forcings and predicted streamflow which most closely resembles the forecasted vector of states forcings and predicted streamflow the dl method directly provides the closest values from the database as a pseudo observation the k nn method provides the average of the k closest values as a pseudo observation and the fnn generates a pseudo observation which corresponds to the forecasted vector the general flowchart illustrating how the method works and where the lookup routine fits into the forecast is provided in fig 2 3 3 1 direct lookup and k nearest neighbour the k nn method is a simple machine learning algorithm which can be used for regression or classification altman 1992 when using it for regression the k nearest neighbours to the sample point of interest are averaged to provide a predicted value in the case of this analysis the sample point is a vector containing the initial model states forcing data and predicted streamflow for a timestep which is then compared to the points in a database either the hdb or fdb and a corresponding streamflow observation or prediction error is returned this observation is the mean of the k closest points to the sample point the direct lookup method is the special case of the k nn algorithm when k 1 it is possible to use different distance and weighting methods for k nn however for this work the euclidean distance with equal weights was used the general formulation for the k nn method is as follows 15 c j argmin i x t d i f o r j 1 c j argmin i x t d i i c j 1 f o r j 2 k 16 y t 1 k j 1 k y c j y c j d where x t is the vector of states forcing data and predicted streamflow at time t d i is a database vector entry i that is of the same structure as x t is the euclidean distance function 2 norm and c j is the database index value of the nearest j 1 k neighbours the pseudo streamflow observation for time t y t is then found by taking the average of the archived streamflow observation y c j the prediction error or innovation can be retrieved from the database in a similar manner 3 3 2 feedforward neural network to generate comparable results to those of the dl and k nn lookup methods a fnn was trained on each database and used to generate pseudo observations which could be assimilated during the forecast period more specifically the fnn was trained to reproduce the past true observation which corresponds to a past vector of states forcings and predicted streamflow so that if given a forecasted vector the fnn would generate a pseudo observation for it the specific type of fnn used as a lookup routine was the multilayer perceptron mlp a simple formulation of the mlp is shown as follows hagan et al 1997 17 y t f w 2 g w 1 x t b 1 b 2 where w 1 and w 2 are the input layer and hidden layer neurons weight matrices respectively b 1 and b 2 are the input layer and hidden layer bias vectors respectively f is the linear activation function for the output neuron and g is the non linear activation function hyperbolic tangent for the hidden neurons the mlp was pre trained separately for the hdb and the fdb using the levenberg marquardt optimization method with mean squared normalized error as the objective function for training purposes the databases were randomly split into 70 training data used for fitting 15 validation data used for unbiased evaluation while tuning hyper parameters and 15 testing data used for an unbiased evaluation of the final model the network architecture included an input layer with nine neurons one hidden layer with twenty neurons and the output layer with one neuron the input layer neurons were the initial model states forcing data and predicted streamflow for a timestep and the output neuron returned either the streamflow observation or prediction error innovation for that timestep 3 3 3 modification to kalman gain and likelihood functions when the prediction error innovation is retrieved from the databases the data assimilation formulations need to be modified this is because the prediction error is not independent from the predicted streamflow therefore the variance of y t ɛ t y t is var y t var y t var ɛ t 2 cov ɛ t y t since r var y t the kalman gain in eq 7 when using the prediction error becomes 18 k t cov x t y t 2 var y t var ɛ t 2 cov ɛ t y t a similar modification can be made to the likelihood function used for the pf in eq 9 such that it becomes 19 l y t x i t exp ε i t 2 2 var y t var ɛ t 2 cov ɛ t y t 2 π var y t var ɛ t 2 cov ɛ t y t the assimilation of the prediction error was motivated by the idea that there may be a large difference in the innovation during the forecast when the pseudo streamflow observation is assimilated compared to what it was historically in doing this test the impact that difference has could be better evaluated 3 4 evaluation methods to evaluate the performance of the data assimilation methods when running the model using historical gauge forcing the kling gupta efficiency kge gupta et al 2009 the root mean square error rmse and the peak flow criteria pfc coulibaly et al 2001 were used these criteria were chosen to provide a baseline of performance for each data assimilation method before the extension into the forecast these metrics are defined as follows 20 k g e 1 r 1 2 σ y σ y 1 2 μ y μ y 1 2 21 r m s e 1 t t 1 t y t y t 2 1 2 22 p f c p 1 t p q p q p 2 q p 2 1 4 p 1 t p q p 2 1 2 the kge is found using the linear correlation coefficient r the relative variability σ y σ y and the mean ratio μ y μ y between the ensemble mean of the simulated runoff and the observed runoff where yt is the observed runoff y t is the ensemble mean of the simulated runoff t is the timestep t is the number of timesteps qp is the observed runoff peak q p is the ensemble mean of the simulated runoff peak tp is the number of observed peaks above a threshold and p 1 tp the threshold value for peak flows was found by identifying the peaks then taking one third of the mean of those peaks since an essential consideration in urban and flood prone environments is the peak flow values the pfc will be used to evaluate how well the data assimilation methods can improve the peak flow simulation in the ungauged basins the rmse and kge will be used to evaluate the general performance of the data assimilation methods specifically how effective they are at improving performance in ungauged basins the rmse and pfc values can range from zero to infinity with zero being optimal and the kge can range from negative infinity to one with one being optimal the ensemble forecast performance of each assimilation scheme and model combination was evaluated using the mean continuous ranked probability score crps performance metric the crps can be used to assess accuracy and resolution of the 48 h ensemble forecast performances the mean cprs is formulated as follows matheson and winkler 1976 unger 1985 23 c r p s f y 1 n t 1 n f y t 1 y t y t 2 d y where f y t is the cumulative distribution function of the ensemble forecasts y t is the predicted runoff yt is the observed runoff and 1 y t y t is the heaviside step function that provides a value of 1 if the predicted value is larger than the observed and 0 otherwise a cprs value of 0 indicates a perfect forecast to determine the reliability of the forecasts the mean crps was decomposed according to hersbach 2000 such that c r p s r e l i c r p s p o t this decomposition is analogous to that of the brier score decomposition which can provide reliability resolution and uncertainty hersbach 2000 murphy 1973 reliability of a forecast is a measure of its statistical accuracy and whether or not the forecasting system has correct statistical properties hersbach 2000 murphy 1973 more specifically the forecast can be considered reliable if it is not over or under dispersed and is unbiased in this way the reliability also has a connection to the rank histogram which can be used to evaluate the spread and bias of an ensemble the reliability and potential crps are calculated as follows 24 r e l i i 0 n g i o i p i 2 25 c r p s p o t i 0 n g i o i 1 o i where g i is the average width of bin i o i is the average frequency that the observation is less than the middle of bin i pi is the fraction i n and n is the ensemble size there are n 1 bins and each is defined by the distance between consecutive ensemble members when 0 i n while for i 0 n the bins are defined by the distance between the observation and outlier members the crpspot represents what the c r p s could be if the forecast was perfectly reliable and r e l i is an indication of how reliable the forecast is for both metrics the optimal value is 0 the interested reader can see hersbach 2000 for a more detailed description of the decomposition the reliability performance measure from renard et al 2010 and the 95 exceedance ratio er95 from moradkhani et al 2006 were also used for evaluating the performance of the forecasts these metrics evaluate the overall reliability of the predictive distribution and the spread of the ensemble respectively they are calculated as follows 26 r e l i a b i l i t y 1 2 t t 1 t p y t y t p t u 27 e r 95 1 t t 1 t y 97 5 t y t o r y 2 5 t y t 100 where p y t y t is the sorted probability that the ensemble prediction y t will be less than the observation yt t is the total number of time steps p t u is the theoretical cumulative probability cumulative uniform distribution y 97 5 t is the 97 5 percentile of the predicted streamflow ensemble and y 2 5 t is the 2 5 percentile of the predicted streamflow ensemble the reliability measure can range from 0 to 1 with 1 being perfectly reliable and the er95 ranges from 0 to 100 with a perfect ensemble having a value of 5 as part of assessing the impact of extending the assimilation into the forecast four cases were evaluated 1 for both the spin up and forecast period the model was run without data assimilation 2 for the spin up period data assimilation was used and during the forecast it was just the model 3 data assimilation was used in both the spin up and forecast periods and 4 the model was used during the spin up period and data assimilation was used in the forecast additionally for all four cases during the spin up period the models were forced with historical gauge data and during the forecast period the models were forced with the rdps forecasted forcings 4 results and discussions 4 1 model calibration validation and standard data assimilation results using the dds optimization multiple calibrations for each basin were performed with each run consisting of 50000 iterations with an r of 0 2 then another 50000 iterations starting from the previous endpoint with an r of 0 1 the best hymod model calibration 20090101 to 20121231 and validation 20130101 to 20151231 results for both basins are summarized in table 2 from these results we see that both basins perform reasonably well during the calibration period however larger runoff events occurred during the validation period in both basins which can explain why they perform slightly worse during that period additionally the black creek basin model performs better than the don river basin model which is likely due to the controlled reservoir in the don river basin making it more difficult to model the models were also run using each data assimilation during the validation period so that the performance of each method could be compared to the open loop case the results of these runs are summarized in table 3 from this table it can been seen that the data assimilation with only state updating performs best for the 3 h ahead forecast using the historical gauge data this was likely due to the models being well calibrated using the historical gauge data so re calibrating using the dual state and parameter updating strategies was not beneficial the pf methods with sequential importance resampling and residual resampling performed the best for these state updating strategies however the enkf was comparable note that when using the dual state and parameter updating strategy the enkf method outperformed the other data assimilation methods 4 2 standard data assimilation performance using the rdps forecasts the data assimilation methods were then evaluated based on their forecast performance with the rdps forcing data illustrated in fig 3 are the mean crps performances for both the don river and black creek basin hymod models using each assimilation method out to the 48 h forecast as a side note a continuous archive of regional deterministic prediction system rdps was not available therefore only 302 of the rdps forecasts were used from 2017 forecasted at time 00 and the results in fig 3 are the average of them from the forecast results it appears that the data assimilation methods with only state updating performed nearly identically but they were not the best performing methods as before forced with historical gauge data for a 3 h forecast when forcing the hymod model with rdps it is instead better to use the dual state and parameter updating strategy with data assimilation to achieve the best performance this is consistent with the literature leach et al 2018 moradkhani et al 2005 samuel et al 2014 yan et al 2015 specifically it is shown that the enkf performs well for the first 6 h for the don river basin and the first 18 h for the black creek basin the particle filter with sir sirv and mcmc resampling outperforms the enkf beyond those forecast horizons additionally the particle filter with rr does not perform well for either basin when forcing with rdps and it will therefore be omitted from further analyses 4 3 evaluation of data assimilation in the forecast to evaluate the performance of extending data assimilation into the forecast several scenarios were evaluated and compared these scenarios included different combinations of lookup methods lookup values along with which database was being used these combinations meant that twelve scenarios were tested for each assimilation method under cases 3 and 4 listed in section 3 4 the forecast results of which are provided in figs 4 and 5 for the don river basin and black creek basin respectively these results suggest that in general extending data assimilation into the forecast will provide better performance than the open loop model case 1 additionally many of the scenarios also outperform the traditional data assimilation case 2 however how much of an improvement varies from method to method from fig 4 the assimilation methods with state updating enkf and pf sir have better performance under cases 3 and 4 when updating both states and parameters in the forecast the performances of the enkf d pf sirv and pf mcmc can be further improved beyond that of case 2 but only when data assimilation is performed during both the spin up and forecast both the pf sirv and pf mcmc methods provide very similar performance here as was also shown in moradkhani et al 2012 with the pf sirv providing slightly better performance in the don river basin under case 3 assimilating archived streamflow predictions provided using the direct lookup method and the fdb these results suggest that even if observations are not directly available for a basin leading up to the forecast but there were observations some time in the past data assimilation could still be used to improve the forecast performance as long as a similar event occurred previously similar performance is seen for the black creek basin model as with the don river basin model with both the state updating methods being further improved for both case 3 and 4 and the dual state and parameter updating methods being further improved for case 3 however there are some scenarios that also improve the dual updating for case 4 in the black creek basin and the best case was the same as that of the don river basin pf sirv we suspect that the ability for this proposed method to work in these basins is due to them being small urban basins with low memory and short time of concentrations this makes the basins respond in a similar manner when similar boundary conditions exist leading to the database lookup method working well if the basins were larger it is likely that multiple time lags would need to be considered which would add more complexity to the lookup routine although it would be possible to do to better illustrate the difference between cases a random event was selected from the don river basin and its two day modeled window was shown in fig 6 the results presented here are from the best performing case and scenario options that were previously identified from fig 6 each case can perform reasonably well however case 3 is able to provide better results further into the forecast this agrees with the results in fig 4 to evaluate the various options used for cases 3 and 4 the results of the scenario options have been summarized in figs 7 and 8 for the don river and black creek basins respectively from these plots we can make a few conclusions about the best scenario options to use the first is that pf sirv or pf mcmc with dual updating during the forecast period should be used case 3 the results from using each database suggest that the fdb is slightly better to use although it would likely be beneficial to use whichever database is larger since there will be more archived events available it is also slightly better to pull the archived streamflow instead of the archived prediction error from the database with either of the lookup methods further optimization may change this however since this manuscript is presenting a proof of concept for extending data assimilation into the forecast there is likely some optimization that could be made to further improve the lookup routine and functions suggested improvements include more neighbours or a different distance metric for the k nn and more hidden layers and or neurons for the fnn 4 4 effect on model forecast reliability to evaluate the impact extending data assimilation into the forecast has on forecast reliability the crps was decomposed based on hersbach 2000 to get the potential crps and reliability components the relative changes to these performance metrics are illustrated in fig 9 for the using pf sirv data assimilation method here the difference between case 2 and 3 and case 2 and 4 are taken so that a positive percent difference indicates an improvement for case 3 or 4 from these results we see for the don river basin model that case 3 scenarios improve the forecast reliability and potential crps increasing forecast reliability by up to 70 however the case 4 results in general are only able to improve the potential crps results the black creek basin model has similar performance to that of the don river model with forecast reliability of case 3 also improving by up to 78 unlike the don river model however the black creek model also has improved forecast reliability with case 4 with the potential crps for both cases instead having poorer performance additional probabilistic performance metrics were also evaluated to determine the improvement of case 3 and case 4 over case 2 those metrics were the 95 exceedance ratio moradkhani et al 2006 and reliability renard et al 2010 the percent difference of each metric was calculated between case 3 and 2 and case 4 and 2 so that improvements over the traditional data assimilation case 2 could be determined from fig 10 it is apparent that extending data assimilation into the forecast can both improve the spread of the ensemble and improve overall reliability particularly under case 3 where data assimilation is used during both the spin up period and is extended into the forecast the renard reliability can be improved by up to 67 on the don river and 28 on black creek for case 3 and the ensemble spread er95 can be improved by up to 97 on the don river and 98 on black creek these results generally agree with those of the decomposed crps reliability these results lead to the conclusion that forecast performance can be improved if data assimilation is extended into the forecast by using pseudo observations obtained from a historical database of the basin model this improvement is enhanced when observations are available during the forecast spin up period for assimilation although they are not required by the method the proposed method suggests that if a database can be built which contains the information of how a basin will respond to precipitation and other forcings data assimilation can be performed using pseudo observations in the absence of real observation data to improve forecast reliability additionally we suspect that the larger the period of record used to build the database the larger the improvement in forecast performance should be as it will be more likely that a historical event occurred which matches the event being forecasted 5 conclusions the results of this work provide a proof of concept for the extension of data assimilation into the forecast we found that when assimilating pseudo observations pulled from a prebuilt database which contains information on how a specific watershed modeled using a specific model responds to inputs the short term forecast can be improved specifically we showed that for small highly urbanized basins with short times of concentration this method works well at improving forecast reliability the largest improvements were shown when data assimilation was used during both the spin up period to the forecast as well as into the forecast case 3 however we also showed that forecast performance could be improved with just data assimilation in the forecast case 4 although the improvements shown for case 4 are not as significant as case 3 they indicate that assimilating with pseudo observations in the forecast is still better than the open loop results case 1 a drawback to extending data assimilation into the forecast is that the method is largely dependent on the ability to build a database for best results it is suggested that a large period of record should exist for the watershed of interest additionally since the database is watershed and model specific if the modelers were interested in multi modeling they would need to build a database for each model finally as the model becomes more complex such as a fully distributed model there would be an increased computational cost associated with the database and lookup function there is potential to improve the developed method however through further optimization of the lookup method or expanding the database lookup over a span of several timesteps which may be needed on larger more complex basins declaration of competing interest none acknowledgements this research was supported by the natural science and engineering research council nserc of canada grant nserc canadian floodnet netgp 451456 the authors are grateful to the toronto region conservation authority trca environment and climate change canada eccc and water survey of canada wsc for providing the data finally the authors would like to thank the anonymous reviewers for their thoughtful comments and suggestions which helped to improve the manuscript 
561,typically when using data assimilation to improve hydrologic forecasting observations are assimilated up to the start of the forecast this is done to provide more accurate state and parameter estimates which in turn allows for a better forecast we propose an extension to the traditional data assimilation approach which allows for assimilation to continue into the forecast to further improve the forecast s performance and reliability this method was tested on two small highly urbanized basins in southern ontario canada the don river and black creek basins using a database of forcing data model states predicted streamflow and streamflow observations a lookup function was used to provide an observation during the forecast which can be assimilated this allows for an indirect way to assimilate the numerical weather prediction forcing data this approach can help in addressing prediction uncertainty since an ensemble of previous observations can be pulled from the database which correspond to the forecast probability density function given previous information the results show that extending data assimilation into the forecast can improve forecast performance in these urban basins and it was shown that the forecast reliability could be improved by up to 78 keywords data assimilation ensemble kalman filter particle filter short term forecast nearest neighbour artificial neural network 1 introduction with our changing climate extreme weather events especially heavy rainfalls are becoming more common which leads increasing need for improved flood forecasting methods historically deterministic methods were used to predict floods with a chosen model being forced with a deterministic numerical weather prediction nwp product but we know that this method is not ideal advances over the last two decades have led to ensemble and probabilistic approaches to better improve flood forecasting as well as to quantify the associated uncertainties related to the forecast liu et al 2012 these more advanced forecasting and uncertainty quantification methods include sequential data assimilation sda liu et al 2012 moradkhani et al 2005 bayesian forecasting system bfs biondi and de luca 2012 han and coulibaly 2019 krzysztofowicz 1999 bayesian model averaging bma najafi and moradkhani 2016 raftery et al 2005 and model conditional processor mcp todini 2008 with improved methods we are able to more accurately predict floods sooner leading to a faster response and a reduced associated cost sequential data assimilation is a popular method used in hydrologic modeling to improve hydrologic and flood forecasting as an application of bayes theorem sda can incorporate the uncertainty from both the model and observations to update boundary conditions and improve forecasts there are several sda methods which have been used for hydrologic forecasting such as the extended kalman filter ekf sun et al 2015 ensemble kalman filter enkf leach et al 2018 moradkhani et al 2005 samuel et al 2014 thiboult et al 2015 vrugt and robinson 2007 ensemble kalman smoother enks crow and ryu 2009 li et al 2013 evolutionary data assimilation eda dumedah and coulibaly 2013 the particle filter pf dechant and moradkhani 2014 moradkhani et al 2005 parrish et al 2012 yan and moradkhani 2016 and most recently the evolutionary pf hybrid methods abbaszadeh et al 2019 2018 ju et al 2019 zhu et al 2018 with the enkf and pf being popular due to them being relatively simple to implement additionally various types of observations have been successfully assimilated into hydrologic models using these sda techniques such as streamflow abaza et al 2017 abbaszadeh et al 2018 yan and moradkhani 2016 soil moisture ju et al 2019 leach et al 2018 meng et al 2017 yan and moradkhani 2016 and snow water equivalent dziubanski and franz 2016 huang et al 2017 leach et al 2018 smyth et al 2019 these data sets are obtained from in situ gauges or remote sensing techniques through assimilating these observations both models states and parameters can be updated to improve hydrologic simulation and forecasting leach et al 2018 moradkhani et al 2005 samuel et al 2014 one downside to hydrologic da and modeling in general is limited observational data due to many locations being ungauged this causes issues for sda with distributed models as it has been shown that sda in distributed modeling is better when more observations are available from multiple locations clark et al 2008 rakovec et al 2012 xie and zhang 2010 this data limitation also impacts forecasts since the ability to update is limited by observations being available to assimilate at the start of a forecast if a real time gauge or a gauge in general does not exist then there may not be observations available at the beginning of the forecast to be used by sda techniques to update the states and parameters due to the fact that these sda methods are ensemble forecasting methods they are limited in that they can address nwp uncertainty and merge model and observation uncertainty but are not guaranteed to address prediction uncertainty biondi and todini 2018 in this study we aim to address the limitation of sda during the forecast mode specifically the lack of available observations to assimilate since there are none to do this we first build a database for our selected watershed the database contains results from a chosen hydrologic model such as predicted streamflows predicted states the forcings used to drive the model and the corresponding observed streamflows this database essentially stores how a specific watershed responds to forcings given some initial conditions and it is used to generate a probability density function pdf of pseudo observations y conditional on available information i denoted as f y i biondi and todini 2018 this pdf is then assimilated back into the model at each forecast step with the goal of improving the forecast s performance and reliability this method could potentially also be extended to ungauged basins through the use of regionalization techniques to generate a database for any basin although that is not explored within this work the proposed methodology will allow for data assimilation to be extended into the forecast period by using our prior knowledge of how the model responds to different inputs in doing so we can better account for the prediction uncertainty with sda and provide better state estimates at each forecast horizon additionally during a dual state and parameter updating scheme this method will allow for the model parameters to be calibrated to the chosen weather forecast product real time calibration to the weather forecast is beneficial since models are traditionally calibrated using historical gauge data however model parameter distributions can vary depending on what forcing data products are used to calibrate the model kornelsen and coulibaly 2016 2 study areas and data the study areas being focused on are the black creek and don river basins in toronto ontario canada fig 1 these watersheds are managed by the toronto region conservation authority trca both the black creek basin bcb and the don river basin drb are highly urbanized being roughly 97 and 93 developed respectively agriculture and agri food canada 2015 these basins have an average daily temperature of 8 0 c an average daily minimum and maximum temperatures of 3 4 c to 12 5 c respectively and an average annual precipitation of 841 1 mm year based on the 1981 2010 canadian climate normals environment and climate change canada 2017 hourly data was provided by the trca environment and climate change canada eccc and water survey of canada wsc 20080101 to 20171231 gaps in the precipitation data were infilled using a combination of disaggregated daily to hourly eccc precipitation data and ordinary kriging the disaggregation was performed using multiplicative random cascade mrc based disaggregation ganguli and coulibaly 2017 olsson 1998 1995 ordinary kriging was also used to infill gaps in the temperature data forecast forcing data was obtained from the regional deterministic prediction system rdps the rdps provides four 48 h forecasts per day in 3 h increments on a 10 km grid the 00 h forecast was used for this study with 3 h precipitation and temperature data from 20150401 to 20171231 3 methodology 3 1 hydrologic model hymod is a lumped hydrologic model based on the probability distributed moisture model pdm moore 1985 for this study hymod was modified to include the degree day snow routine from samuel et al 2011 and the simplified thornthwaite evapotranspiration method samuel et al 2011 the model was calibrated using a 3 h time interval this timestep was chosen to match the rdps dataset s forecast timestep each watershed model was calibrated using the dynamically dimensioned search dds algorithm tolson and shoemaker 2007 which has one algorithm parameter r used to control the neighborhood size for the random search using dds the calibration period was 20090101 to 20121231 and the validation period was 20130101 to 20151231 with the period of 20080101 to 20081231 used for spin up the nash volume efficiency nve from samuel et al 2012 with modified weights was the chosen objective function the nve is defined as follows 1 n v e 0 3 n s e 0 1 v e 0 2 n s e l o g 0 5 n s e s q r 2 n s e 1 t 1 t y t y t 2 t 1 t y t e y 2 3 v e e y y e y where nse is the nash sutcliffe efficiency nash and sutcliffe 1970 nselog is the nse calculated using log transformed runoff low flow nsesqr is the nse calculated using the squared runoff high flow ve is the volume error yt is the observed runoff at time t y t is the simulated runoff at time t and e is the expectation function descriptions for the hymod model parameters and states are listed in table 1 the calibrated parameter sets for both the bcb and drb are also provided in table 1 along with their parameter ranges and associated units 3 2 data assimilation for hydrologic forecasting the benefit of sda is that it can take new information at each time step and update our prior estimate of the states and parameters to provide a more accurate posterior which leads to a better forecast the posterior distribution of states at time t can be found using the recursive bayes theorem 4 p x t y 1 t p x t y t y 1 t 1 p y t x t p x t y 1 t 1 p y t x t p x t y 1 t 1 d x t where p xt y 1 t is the posterior distribution p xt y 1 t 1 is the prior distribution p yt xt is the likelihood for time t and p y t x t p x t y 1 t 1 d x t is the normalization factor since the analytic solution is generally intractable a numerical approximation can be made using monte carlo methods 3 2 1 ensemble kalman filter the ensemble kalman filter enkf is a monte carlo based sda method that can be used to optimally combine model and observation uncertainties to provide an improved forecast evensen 2003 1994 the enkf is a commonly used data assimilation method which can be used to update model states and parameters its update procedure is shown as follows moradkhani et al 2005 5 x t f x t 1 θ u t ζ t 6 y t h x t θ ν t 7 k t cov x t y t var y t r 8 x t x t k t y t y t where x t is the predicted ensemble of states for time t θ is the vector of model parameters u t is the ensemble of perturbed forcing data at time t f is the state propagation functions which moves them forward in time and ζ t is the random noise which represents process error the superscripts and represent the prior and posterior updated states respectively the hydrologic model is represented by h y t is the predicted runoff at time t and ν t is noise which represents model uncertainty the kalman gain k t at time t is found by dividing the cross covariance of predicted states and runoff by the variance of the innovation the difference of the observed and predicted runoff the observed runoff y t is perturbed by random noise n 0 r to generate an ensemble of observations with r being the prior known observation uncertainty the difference of the observed and predicted runoff is then multiplied by the kalman gain and added to the prior state estimate to create the posterior state estimate this process eqs 5 to 8 is repeated until there are no more observations or the spin up period is complete 3 2 2 particle filter another common sda method is the particle filter pf as with the enkf the pf can be used to update model states and parameters moradkhani 2008 moradkhani et al 2005 there are many variants of the particle filter that utilize different particle resampling methods four resampling methods were selected for this analysis they were the residual resampling rr method liu and chen 1998 two versions of the sample importance resampling method sir and sirv smith and gelfand 1992 and the markov chain monte carlo mcmc method leisenring and moradkhani 2012 moradkhani et al 2012 yan et al 2015 yan and moradkhani 2016 the sir and rr methods add constant noise to the particles at every timestep in order to add particle diversity and combat degeneracy liu and chen 1998 while the sirv and mcmc methods add variable noise using the variable variance multiplier vvm to combat particle degeneracy leisenring and moradkhani 2012 moradkhani et al 2012 the general formulation of the pf in conjunction with eqs 5 and 6 is as follows gordon et al 1993 moradkhani et al 2005 9 l y t x i t 1 2 π r exp y t ε i t y i t 2 2 r 10 p y t x i t l y t x i t i 1 n l y t x i t 11 w i t w i t p y t x i t i 1 n w i t p y t x i t 12 p x t y 1 t i 1 n w i t δ x t x i t where n is the number of particles w i t is the prior weight set to 1 n for particle i at time t w i t is the posterior weight for particle i at time t δ is the dirac delta function ɛ i t n 0 r is random noise which represents observation error and l is the gaussian likelihood function the posterior estimate can then be resampled to eliminate low weight particles using the chosen resampling method there are many resampling methods that can be used with the pf including the rr sir and mcmc resampling methods the rr method for reweighting residual particles particles with a weight less than 1 n is 13 w i t n w i t n w i t n i 1 n n w i t where w i t are the weights of the n residual particles n n i 1 n n w i t that will be used to construct an empirical cumulative distribution function cdf this cdf is then resampled from to increase the number of particles from n to n after which the particle weights are set to 1 n when using sir like rr a cdf of the particle weights is constructed from this cdf n particles are resampled proportionally to their weights with higher weighted particles being resampled more often when their probability is higher than uniform probability the mcmc resampling method is an extension of the sir method it uses a metropolis acceptance ratio α to decide if an update should be accepted or rejected 14 α min 1 p x t p y 1 t p x t y 1 t the proposed joint probability distribution of the updated particles is p x t p y 1 t and the particles are updated when the acceptance ratio exceeds a threshold value generated from a uniform random variable u u 0 1 3 3 pseudo observation lookup routine to provide pseudo observations during the forecast period a database was built that contains past information about the watershed by using this database data assimilation can be extended into the forecast period this database contains the hymod state values table 1 the precipitation and temperature data used to force the model and generate those state values and the predicted streamflow associated with those states and forcings for each timestep during the time period allocated for building the database additionally the database contains the corresponding observed streamflow values and the prediction error values for each timestep in the database to evaluate the influence forcing data uncertainty may have on the lookup methods two databases were built for this study using different forcing datasets the first database was built using historical precipitation and temperature gauge data to run hymod denoted as the historical observation database hdb the hdb was built using gauge data from 20090101 to 20161231 and results from each timesteps were archived the second database was created using archived rdps precipitation and temperature denoted as the forecast forcing database fdb since the available archive of rdps forecasts is much shorter than that of the gauges the period of 20150401 to 20161231 was used to build the fdb three lookup methods were evaluated and used to pull pseudo observations from the pre built databases the lookup methods are k nearest neighbour k nn direct lookup dl and a feedforward neural network fnn each method was evaluated based on how they affected the performance of the forecasts the term pseudo observation was used to describe the values pulled from the database because they are true observations that correspond to past vectors of states forcings and predicted streamflow which most closely resembles the forecasted vector of states forcings and predicted streamflow the dl method directly provides the closest values from the database as a pseudo observation the k nn method provides the average of the k closest values as a pseudo observation and the fnn generates a pseudo observation which corresponds to the forecasted vector the general flowchart illustrating how the method works and where the lookup routine fits into the forecast is provided in fig 2 3 3 1 direct lookup and k nearest neighbour the k nn method is a simple machine learning algorithm which can be used for regression or classification altman 1992 when using it for regression the k nearest neighbours to the sample point of interest are averaged to provide a predicted value in the case of this analysis the sample point is a vector containing the initial model states forcing data and predicted streamflow for a timestep which is then compared to the points in a database either the hdb or fdb and a corresponding streamflow observation or prediction error is returned this observation is the mean of the k closest points to the sample point the direct lookup method is the special case of the k nn algorithm when k 1 it is possible to use different distance and weighting methods for k nn however for this work the euclidean distance with equal weights was used the general formulation for the k nn method is as follows 15 c j argmin i x t d i f o r j 1 c j argmin i x t d i i c j 1 f o r j 2 k 16 y t 1 k j 1 k y c j y c j d where x t is the vector of states forcing data and predicted streamflow at time t d i is a database vector entry i that is of the same structure as x t is the euclidean distance function 2 norm and c j is the database index value of the nearest j 1 k neighbours the pseudo streamflow observation for time t y t is then found by taking the average of the archived streamflow observation y c j the prediction error or innovation can be retrieved from the database in a similar manner 3 3 2 feedforward neural network to generate comparable results to those of the dl and k nn lookup methods a fnn was trained on each database and used to generate pseudo observations which could be assimilated during the forecast period more specifically the fnn was trained to reproduce the past true observation which corresponds to a past vector of states forcings and predicted streamflow so that if given a forecasted vector the fnn would generate a pseudo observation for it the specific type of fnn used as a lookup routine was the multilayer perceptron mlp a simple formulation of the mlp is shown as follows hagan et al 1997 17 y t f w 2 g w 1 x t b 1 b 2 where w 1 and w 2 are the input layer and hidden layer neurons weight matrices respectively b 1 and b 2 are the input layer and hidden layer bias vectors respectively f is the linear activation function for the output neuron and g is the non linear activation function hyperbolic tangent for the hidden neurons the mlp was pre trained separately for the hdb and the fdb using the levenberg marquardt optimization method with mean squared normalized error as the objective function for training purposes the databases were randomly split into 70 training data used for fitting 15 validation data used for unbiased evaluation while tuning hyper parameters and 15 testing data used for an unbiased evaluation of the final model the network architecture included an input layer with nine neurons one hidden layer with twenty neurons and the output layer with one neuron the input layer neurons were the initial model states forcing data and predicted streamflow for a timestep and the output neuron returned either the streamflow observation or prediction error innovation for that timestep 3 3 3 modification to kalman gain and likelihood functions when the prediction error innovation is retrieved from the databases the data assimilation formulations need to be modified this is because the prediction error is not independent from the predicted streamflow therefore the variance of y t ɛ t y t is var y t var y t var ɛ t 2 cov ɛ t y t since r var y t the kalman gain in eq 7 when using the prediction error becomes 18 k t cov x t y t 2 var y t var ɛ t 2 cov ɛ t y t a similar modification can be made to the likelihood function used for the pf in eq 9 such that it becomes 19 l y t x i t exp ε i t 2 2 var y t var ɛ t 2 cov ɛ t y t 2 π var y t var ɛ t 2 cov ɛ t y t the assimilation of the prediction error was motivated by the idea that there may be a large difference in the innovation during the forecast when the pseudo streamflow observation is assimilated compared to what it was historically in doing this test the impact that difference has could be better evaluated 3 4 evaluation methods to evaluate the performance of the data assimilation methods when running the model using historical gauge forcing the kling gupta efficiency kge gupta et al 2009 the root mean square error rmse and the peak flow criteria pfc coulibaly et al 2001 were used these criteria were chosen to provide a baseline of performance for each data assimilation method before the extension into the forecast these metrics are defined as follows 20 k g e 1 r 1 2 σ y σ y 1 2 μ y μ y 1 2 21 r m s e 1 t t 1 t y t y t 2 1 2 22 p f c p 1 t p q p q p 2 q p 2 1 4 p 1 t p q p 2 1 2 the kge is found using the linear correlation coefficient r the relative variability σ y σ y and the mean ratio μ y μ y between the ensemble mean of the simulated runoff and the observed runoff where yt is the observed runoff y t is the ensemble mean of the simulated runoff t is the timestep t is the number of timesteps qp is the observed runoff peak q p is the ensemble mean of the simulated runoff peak tp is the number of observed peaks above a threshold and p 1 tp the threshold value for peak flows was found by identifying the peaks then taking one third of the mean of those peaks since an essential consideration in urban and flood prone environments is the peak flow values the pfc will be used to evaluate how well the data assimilation methods can improve the peak flow simulation in the ungauged basins the rmse and kge will be used to evaluate the general performance of the data assimilation methods specifically how effective they are at improving performance in ungauged basins the rmse and pfc values can range from zero to infinity with zero being optimal and the kge can range from negative infinity to one with one being optimal the ensemble forecast performance of each assimilation scheme and model combination was evaluated using the mean continuous ranked probability score crps performance metric the crps can be used to assess accuracy and resolution of the 48 h ensemble forecast performances the mean cprs is formulated as follows matheson and winkler 1976 unger 1985 23 c r p s f y 1 n t 1 n f y t 1 y t y t 2 d y where f y t is the cumulative distribution function of the ensemble forecasts y t is the predicted runoff yt is the observed runoff and 1 y t y t is the heaviside step function that provides a value of 1 if the predicted value is larger than the observed and 0 otherwise a cprs value of 0 indicates a perfect forecast to determine the reliability of the forecasts the mean crps was decomposed according to hersbach 2000 such that c r p s r e l i c r p s p o t this decomposition is analogous to that of the brier score decomposition which can provide reliability resolution and uncertainty hersbach 2000 murphy 1973 reliability of a forecast is a measure of its statistical accuracy and whether or not the forecasting system has correct statistical properties hersbach 2000 murphy 1973 more specifically the forecast can be considered reliable if it is not over or under dispersed and is unbiased in this way the reliability also has a connection to the rank histogram which can be used to evaluate the spread and bias of an ensemble the reliability and potential crps are calculated as follows 24 r e l i i 0 n g i o i p i 2 25 c r p s p o t i 0 n g i o i 1 o i where g i is the average width of bin i o i is the average frequency that the observation is less than the middle of bin i pi is the fraction i n and n is the ensemble size there are n 1 bins and each is defined by the distance between consecutive ensemble members when 0 i n while for i 0 n the bins are defined by the distance between the observation and outlier members the crpspot represents what the c r p s could be if the forecast was perfectly reliable and r e l i is an indication of how reliable the forecast is for both metrics the optimal value is 0 the interested reader can see hersbach 2000 for a more detailed description of the decomposition the reliability performance measure from renard et al 2010 and the 95 exceedance ratio er95 from moradkhani et al 2006 were also used for evaluating the performance of the forecasts these metrics evaluate the overall reliability of the predictive distribution and the spread of the ensemble respectively they are calculated as follows 26 r e l i a b i l i t y 1 2 t t 1 t p y t y t p t u 27 e r 95 1 t t 1 t y 97 5 t y t o r y 2 5 t y t 100 where p y t y t is the sorted probability that the ensemble prediction y t will be less than the observation yt t is the total number of time steps p t u is the theoretical cumulative probability cumulative uniform distribution y 97 5 t is the 97 5 percentile of the predicted streamflow ensemble and y 2 5 t is the 2 5 percentile of the predicted streamflow ensemble the reliability measure can range from 0 to 1 with 1 being perfectly reliable and the er95 ranges from 0 to 100 with a perfect ensemble having a value of 5 as part of assessing the impact of extending the assimilation into the forecast four cases were evaluated 1 for both the spin up and forecast period the model was run without data assimilation 2 for the spin up period data assimilation was used and during the forecast it was just the model 3 data assimilation was used in both the spin up and forecast periods and 4 the model was used during the spin up period and data assimilation was used in the forecast additionally for all four cases during the spin up period the models were forced with historical gauge data and during the forecast period the models were forced with the rdps forecasted forcings 4 results and discussions 4 1 model calibration validation and standard data assimilation results using the dds optimization multiple calibrations for each basin were performed with each run consisting of 50000 iterations with an r of 0 2 then another 50000 iterations starting from the previous endpoint with an r of 0 1 the best hymod model calibration 20090101 to 20121231 and validation 20130101 to 20151231 results for both basins are summarized in table 2 from these results we see that both basins perform reasonably well during the calibration period however larger runoff events occurred during the validation period in both basins which can explain why they perform slightly worse during that period additionally the black creek basin model performs better than the don river basin model which is likely due to the controlled reservoir in the don river basin making it more difficult to model the models were also run using each data assimilation during the validation period so that the performance of each method could be compared to the open loop case the results of these runs are summarized in table 3 from this table it can been seen that the data assimilation with only state updating performs best for the 3 h ahead forecast using the historical gauge data this was likely due to the models being well calibrated using the historical gauge data so re calibrating using the dual state and parameter updating strategies was not beneficial the pf methods with sequential importance resampling and residual resampling performed the best for these state updating strategies however the enkf was comparable note that when using the dual state and parameter updating strategy the enkf method outperformed the other data assimilation methods 4 2 standard data assimilation performance using the rdps forecasts the data assimilation methods were then evaluated based on their forecast performance with the rdps forcing data illustrated in fig 3 are the mean crps performances for both the don river and black creek basin hymod models using each assimilation method out to the 48 h forecast as a side note a continuous archive of regional deterministic prediction system rdps was not available therefore only 302 of the rdps forecasts were used from 2017 forecasted at time 00 and the results in fig 3 are the average of them from the forecast results it appears that the data assimilation methods with only state updating performed nearly identically but they were not the best performing methods as before forced with historical gauge data for a 3 h forecast when forcing the hymod model with rdps it is instead better to use the dual state and parameter updating strategy with data assimilation to achieve the best performance this is consistent with the literature leach et al 2018 moradkhani et al 2005 samuel et al 2014 yan et al 2015 specifically it is shown that the enkf performs well for the first 6 h for the don river basin and the first 18 h for the black creek basin the particle filter with sir sirv and mcmc resampling outperforms the enkf beyond those forecast horizons additionally the particle filter with rr does not perform well for either basin when forcing with rdps and it will therefore be omitted from further analyses 4 3 evaluation of data assimilation in the forecast to evaluate the performance of extending data assimilation into the forecast several scenarios were evaluated and compared these scenarios included different combinations of lookup methods lookup values along with which database was being used these combinations meant that twelve scenarios were tested for each assimilation method under cases 3 and 4 listed in section 3 4 the forecast results of which are provided in figs 4 and 5 for the don river basin and black creek basin respectively these results suggest that in general extending data assimilation into the forecast will provide better performance than the open loop model case 1 additionally many of the scenarios also outperform the traditional data assimilation case 2 however how much of an improvement varies from method to method from fig 4 the assimilation methods with state updating enkf and pf sir have better performance under cases 3 and 4 when updating both states and parameters in the forecast the performances of the enkf d pf sirv and pf mcmc can be further improved beyond that of case 2 but only when data assimilation is performed during both the spin up and forecast both the pf sirv and pf mcmc methods provide very similar performance here as was also shown in moradkhani et al 2012 with the pf sirv providing slightly better performance in the don river basin under case 3 assimilating archived streamflow predictions provided using the direct lookup method and the fdb these results suggest that even if observations are not directly available for a basin leading up to the forecast but there were observations some time in the past data assimilation could still be used to improve the forecast performance as long as a similar event occurred previously similar performance is seen for the black creek basin model as with the don river basin model with both the state updating methods being further improved for both case 3 and 4 and the dual state and parameter updating methods being further improved for case 3 however there are some scenarios that also improve the dual updating for case 4 in the black creek basin and the best case was the same as that of the don river basin pf sirv we suspect that the ability for this proposed method to work in these basins is due to them being small urban basins with low memory and short time of concentrations this makes the basins respond in a similar manner when similar boundary conditions exist leading to the database lookup method working well if the basins were larger it is likely that multiple time lags would need to be considered which would add more complexity to the lookup routine although it would be possible to do to better illustrate the difference between cases a random event was selected from the don river basin and its two day modeled window was shown in fig 6 the results presented here are from the best performing case and scenario options that were previously identified from fig 6 each case can perform reasonably well however case 3 is able to provide better results further into the forecast this agrees with the results in fig 4 to evaluate the various options used for cases 3 and 4 the results of the scenario options have been summarized in figs 7 and 8 for the don river and black creek basins respectively from these plots we can make a few conclusions about the best scenario options to use the first is that pf sirv or pf mcmc with dual updating during the forecast period should be used case 3 the results from using each database suggest that the fdb is slightly better to use although it would likely be beneficial to use whichever database is larger since there will be more archived events available it is also slightly better to pull the archived streamflow instead of the archived prediction error from the database with either of the lookup methods further optimization may change this however since this manuscript is presenting a proof of concept for extending data assimilation into the forecast there is likely some optimization that could be made to further improve the lookup routine and functions suggested improvements include more neighbours or a different distance metric for the k nn and more hidden layers and or neurons for the fnn 4 4 effect on model forecast reliability to evaluate the impact extending data assimilation into the forecast has on forecast reliability the crps was decomposed based on hersbach 2000 to get the potential crps and reliability components the relative changes to these performance metrics are illustrated in fig 9 for the using pf sirv data assimilation method here the difference between case 2 and 3 and case 2 and 4 are taken so that a positive percent difference indicates an improvement for case 3 or 4 from these results we see for the don river basin model that case 3 scenarios improve the forecast reliability and potential crps increasing forecast reliability by up to 70 however the case 4 results in general are only able to improve the potential crps results the black creek basin model has similar performance to that of the don river model with forecast reliability of case 3 also improving by up to 78 unlike the don river model however the black creek model also has improved forecast reliability with case 4 with the potential crps for both cases instead having poorer performance additional probabilistic performance metrics were also evaluated to determine the improvement of case 3 and case 4 over case 2 those metrics were the 95 exceedance ratio moradkhani et al 2006 and reliability renard et al 2010 the percent difference of each metric was calculated between case 3 and 2 and case 4 and 2 so that improvements over the traditional data assimilation case 2 could be determined from fig 10 it is apparent that extending data assimilation into the forecast can both improve the spread of the ensemble and improve overall reliability particularly under case 3 where data assimilation is used during both the spin up period and is extended into the forecast the renard reliability can be improved by up to 67 on the don river and 28 on black creek for case 3 and the ensemble spread er95 can be improved by up to 97 on the don river and 98 on black creek these results generally agree with those of the decomposed crps reliability these results lead to the conclusion that forecast performance can be improved if data assimilation is extended into the forecast by using pseudo observations obtained from a historical database of the basin model this improvement is enhanced when observations are available during the forecast spin up period for assimilation although they are not required by the method the proposed method suggests that if a database can be built which contains the information of how a basin will respond to precipitation and other forcings data assimilation can be performed using pseudo observations in the absence of real observation data to improve forecast reliability additionally we suspect that the larger the period of record used to build the database the larger the improvement in forecast performance should be as it will be more likely that a historical event occurred which matches the event being forecasted 5 conclusions the results of this work provide a proof of concept for the extension of data assimilation into the forecast we found that when assimilating pseudo observations pulled from a prebuilt database which contains information on how a specific watershed modeled using a specific model responds to inputs the short term forecast can be improved specifically we showed that for small highly urbanized basins with short times of concentration this method works well at improving forecast reliability the largest improvements were shown when data assimilation was used during both the spin up period to the forecast as well as into the forecast case 3 however we also showed that forecast performance could be improved with just data assimilation in the forecast case 4 although the improvements shown for case 4 are not as significant as case 3 they indicate that assimilating with pseudo observations in the forecast is still better than the open loop results case 1 a drawback to extending data assimilation into the forecast is that the method is largely dependent on the ability to build a database for best results it is suggested that a large period of record should exist for the watershed of interest additionally since the database is watershed and model specific if the modelers were interested in multi modeling they would need to build a database for each model finally as the model becomes more complex such as a fully distributed model there would be an increased computational cost associated with the database and lookup function there is potential to improve the developed method however through further optimization of the lookup method or expanding the database lookup over a span of several timesteps which may be needed on larger more complex basins declaration of competing interest none acknowledgements this research was supported by the natural science and engineering research council nserc of canada grant nserc canadian floodnet netgp 451456 the authors are grateful to the toronto region conservation authority trca environment and climate change canada eccc and water survey of canada wsc for providing the data finally the authors would like to thank the anonymous reviewers for their thoughtful comments and suggestions which helped to improve the manuscript 
562,this study contributes the deep uncertainty du pathways framework for bridging long term water supply infrastructure investments and improved short term water portfolio management e g restrictions water transfers financial instruments etc to yield a regional water supply policy robust to supply and financial failures the du pathways framework combines flexibility providing risk of failure rof decision rules dynamic adaptive policy pathways concepts and a careful consideration of time evolving information feedbacks to yield management conditioned infrastructure pathways for regions the du pathways framework has been developed to carefully consider multi actor regional contexts with the goal of aiding stakeholders in discovering pathway policies that attain high performance levels for supply reliability and financial stability across challenging deeply uncertain futures and to guide robustness compromises that may be necessary between regional actors as demonstrated in the research triangle region of north carolina du pathways clarifies how to identify robust infrastructure investment and management policies across the municipalities of raleigh durham cary and chapel hill our results provide insights about the most cost effective infrastructure options to be pursued in the near term clarify which sources of uncertainty drive the performance tradeoffs and robustness conflicts across the regional system and demonstrate valuable information on the diversity of interdependent failure modes that may emerge across the multiple actors implementing each candidate policy keywords optimization multiobjective optimization robustness water utilities infrastructure 1 introduction water utilities globally are facing a growing pressure to proactively address growing demands higher levels of resource contention and increasingly uncertain water availability bonzanigo et al 2018 hall et al 2014 a tension exists between improving the efficiency and coordination of regional water supplies to delay or eliminate major infrastructure investments and the eventual effects of demand growth rates that necessitate water supply capacity expansions soft water management strategies conservation inter utility water transfers demand management etc are a key option to cope with growing domestic water demand gleick 2002 2003 in the united states us a key driver increasing the importance of soft water management strategies is that most large projects of federal and or state interest have already been built lund 2013 however growing urban water demands and increasingly uncertain climate conditions are motivating efforts for redesigning system capacity by upgrading existing and adding new infrastructure to maintain reliable water supply systems shafer and fox 2017 the hard and soft path views for urban water portfolio planning should be treated as complementary as even though infrastructure expansion may be needed it is not desirable and may be mitigated by soft water management strategies however both the soft and hard approaches may be detrimental to utilities finances together with outstanding debt decreasing cash flows are significant factors that negatively impact utilities credit ratings which can increase the cost of borrowing to build new infrastructure moody s 2017 zeff et al 2014 zeff and characklis 2013 trindade et al 2017 the financial impacts of drought mitigation strategies the current negative financial outlook for the water utility sector moody s 2019 the lack of appropriate financing for water utilities gleick et al 2014 all stress the need for new frameworks that facilitate improved water portfolio planning that effectively combines short term drought mitigation instruments for managing financial risks and long term infrastructure investment pathways while accounting diverse sources of uncertainty these needs motivated the creation of the deep uncertainty pathways du pathways framework proposed and demonstrated in this study integrating long term water infrastructure investment and short term management is a difficult task because of the potentially large number of decisions that must be considered over decadal timescales as well as the significant uncertainties inherent to the problem hydro climatic uncertainties such as near term streamflows and evaporation rates have been extensively studied and characterized with well established statistical techniques stedinger 1993 martins and stedinger 2000 herman et al 2016 lall 1995 however water infrastructure investment and management problems include a multitude of uncertainties for which decision makers cannot agree on their boundaries importance and probability distributions termed deep uncertainties kwakkel et al 2016 knight 1921 examples include regional demand growth political and economic uncertainties and climate change herman et al 2014a trindade et al 2017 milly et al 2008 the complexity of the infrastructure investment and management problem poses key questions how should a decision maker devise infrastructure investment and drought management strategies that adequately capture their interdependencies to improve water supply reliability and financial stability how can stakeholders develop management and investment policies that are sufficiently adaptive and robust over decades given the impossibility of predicting climate population shifts political landscapes and other sources of uncertainty how do decision makers know when to make substantial and irreversible investments in new infrastructure while avoiding stranded assets although these are not new issues utilities are still struggling with these questions bonzanigo et al 2018 paulson et al 2018 existing commercial frameworks for water infrastructure management modeling e g hydrologics 2009 sieber and purkey 2015 labadie 2011 focus on existing infrastructure have strong limitations on their inclusion of uncertainties and do not connect to key concepts from the infrastructure investment literature for example the real option analysis roa cox et al 1979 literature of the last 15 years wang and de neufville 2005 erfani et al 2018 fletcher et al 2017 2019 presents and exemplifies a flexible rule based logic that emphasizes near term information to inform decisions about infrastructure planning and policy implementation our du pathways framework integrates some features of roa such as a focus on flexible decision rules and a careful consideration of short term versus long term information for the planning and operation of static and adaptive infrastructure options however roa as a single objective approach is limited in its accounting of stakeholders with diverse interests and its underlying mathematical tree logic becomes computationally intractable when large number of uncertainties are considered curse of dimensionality dittrich et al 2016 borgomeo et al 2018 address some of these concerns by proposing a multi objective evaluation of candidate water supply investments they maximize robustness and minimize risk and cost of a 30 year long fixed construction schedule encompassing multiple infrastructure options it should be noted that the derived sequence of investments is not adaptive and does not account for evolving information feedbacks these limitations also exist in the recent studies by beh et al 2015 and huskova et al 2016 although the former seek to improve adaptivity by incorporating a similar emphasis on near term information as done in roa alternatively the recently introduced dynamic adaptive pathways policy dapp approach haasnoot et al 2013 kwakkel et al 2014 adds flexibility to the planning process by making use of potentially diverse sets metrics signposts dapp however still maintains a strong reliance on the use of limited numbers of predefined action sequences that require high levels of institutional stability and coordinated consensus over the long term the du pathways framework introduced in this work is first formal integration of dynamic and adaptive water infrastructure pathways zeff et al 2016 haasnoot et al 2013 annual long term planning financial and supply reliability drought mitigation instruments weekly short term decisions zeff et al 2014 trindade et al 2017 and the recent extensions of the multiobjective robust decision making mordm framework that incorporate deep uncertainties in search based identification candidade actions kasprzyk et al 2013 trindade et al 2017 kwakkel et al 2014 watson and kasprzyk 2016 du pathways addresses decision making at two time scales weekly for management decisisons and annual for infrastructure investment decisions following the approach recommended by zeff et al 2016 the integrated infrastructure investment and management policies specify short and long term decision making rules based on the risk of failure metric rof as well as candidate orderings for infrastructure investments in contrast to recent bayesian adaptive rules focused on climate states fletcher et al 2019 our rof based rules encompass how the time dynamics of natural water supply capacity states and human demands co evolve to trigger actions both in the short term as well as the long term to address vulnerabilities coupled human and natural uncertainties shape the information encompassed in the rof metric while allowing for an operational simplicity for the proposed action rules the resulting rof rule based policies can tractably be optimized to multiple states of the world built around dozens of sources of uncertainty while avoiding the curse of dimensionality inherent to the tree structured policies used in roa our du pathways framework provides an internally consistent rule form for resolving a broader array of water portfolio management and investment actions by combining infrastructure planning and management rules in a single policy the water portfolio planning approach aids the design of drought mitigation instruments tailored to new infrastructure to minimize the need of further investments the main limitation of the infrastructure pathways work by zeff et al 2016 is its limited treatment of uncertainties the du pathways framework contributed here significantly expands the suite of deep uncertainties considered in the search based identification of candidate infrastructure investment and management policies additionally the framework formalizes a detailed evaluation of the multi city policies robustness tradeoffs du pathways aids stakeholders in navigating these tradeoffs through a combination of visual decision analytics woodruff et al 2013 and enhanced scenario discovery in which boosted trees schapire 1999 aid in the identification of the uncertainties that most strongly shape the infrastructure investment and management policies vulnerabilities the application of the du pathways framework is demonstrated the research triangle test case described in this and prior works trindade et al 2017 zeff et al 2016 herman et al 2014a zeff et al 2014 we have here discovered how the region s vulnerabilities evolve under alternative policies clarified significant interdependencies between the region s four utilities and shown the importance of carefully balancing supply financial instruments and investments in new infrastructure 2 the research triangle test case the du pathways framework introduced in this study is demonstrated on the research triangle region in north carolina the test case is comprised of the four water utilities fig 1 currently responsible for meeting the majority of the region s growing demand in the research triangle portion of the neuse and cape fear river basins raleigh durham cary and orange water and sewer authority owasa representing carrboro and chapel hill rapid population growth in the research triangle over the past 30 years has decreased the distances between utilities services areas allowing for inter network pipe connections for treated water transfers and increased the potential independent impacts of the utilities operations on shared regional resources the four utilities participate in the triangle water supply partnership twsp a partnership founded in 2009 and the called the jordan lake partnership or jlp after two historical droughts 2002 and 2007 2008 the jlp has focused on developing collaborative short term drought management and infrastructure planning solutions for its members with specific focus on meeting projected 2060 water supply demands triangle j 2014 the two key challenges for the research triangle region are to develop 1 collaborative plans for short term regional management of the existing water infrastructure and 2 plan long term infrastructure investments to meet future demands the research triangle test case includes all current infrastructure systems providing water to the four utilities table 1 as well as recently proposed candidate investments table 2 including additional storage conveyance and treatment projects across the four utilities laid out by the jlp in triangle j 2014 based on data from our utility partners our model assumes that all options except the joint water treatment plant already have all permits approved and a completed basic design as a consequence after an option is triggered we focus on uncertain construction timing by assuming a uniformly distributed random time from 3 to 5 years for an option to be built and online given the joint water treatment plant s increased institutional complexity the permitting and design time i e prior to beginning construction is estimated by the utilities as requiring an additional 12 years owasa and durham own allocations in the jordan lake that can currently only be accessed through cary s water treatment plant on eastern shore of jordan lake subject to treatment and inter utility conveyance availabilities fig 1 currently jordan lake s water supply storage is still not fully allocated and several utilities in the research triangle are planning on submitting new allocation requests which gives rise to potential resource contention that the four studied water utilities want to avoid key short term i e drought regional management instruments include water use restrictions and treated water transfers that are employed using weekly operational rules with the goal of making better use of the research triangle region s shared resources efforts over the last several years have focused on defining management operations collaboratively so that the impacts from the measures taken by each utility has a predictable impact on the operations of the other utilities for example if durham owasa and raleigh rely on treated transfers from cary as a drought mitigation measure for improving reliability each transfer request is going to directly impact all requesting utilities given conveyance constraints and the cary water treatment plant s capacity therefore it is an important challenge to define the different mixes of water use restrictions and transfers for the individual cooperating utilities while improving water supply reliability for all of the research triangle s actors as a first step in our development of the du pathways framework contributed in this study we developed a flexible regional model that can encompass a breadth of system uncertainties and support our evaluation of the diverse portfolios of short term management actions as well as long term infrastructure investment pathways at its most basic core our simulation of the research triangle is a water mass balance model with the inputs and outputs illustrated in fig 2 below the model is used in a set of optimization runs whose results were further assessed to identify robust infrastructure management and investment policies for each of the four research triangle utilities beyond management actions made at a weekly timestep the utilities must also define infrastructure investment pathways that specifically prioritize and sequence major projects among options specified in triangle j 2014 as with the short term drought mitigation instruments restrictions and transfers such infrastructure sequences and corresponding decision making rules are sought that will positively impact the research triangle region s supply reliability and financial stability the management and investment actions are critically interdependent given that effective short term drought mitigation instruments can serve to dramatically delay and or reduce capital investments zeff et al 2016 3 methodology the major steps of the du pathways methodology as illustrated in fig 3 are as follows 1 identifying infrastructure investment and drought management policy tradeoffs candidate pareto approximate infrastructure investment and water portfolio management policies are identified using robust multiobjective optimization reed et al 2013 hadka and reed 2014 deb et al 2002 hernández lobato et al 2016 2 defining and evaluating robustness exploiting a more expansive sampling of the potential deep uncertainties that could pose challenging states of the world sows in which robustness metrics can be used to evaluate and rank order solutions for their robustness for reviews and comparisons of alternative definitions of robustness see mcphail et al 2018 herman et al 2015 giuliani and castelletti 2016 3 discovering uncertain scenarios that control robustness the pareto approximate of policies of interest once evaluated against a broader sample of sows undergo further diagnostic assessment by means of sensitivity analysis saltelli 2002 herman and usher 2017 jaxa rozen and kwakkel 2018 and or scenario discovery bryant and lempert 2010 quinn et al 2018 to determine which specific uncertainties dominantly impact system s performance factor prioritization for specific combinations of values factor mapping 4 visual infrastructure pathway analysis use of visual analytics to determine how infrastructure investment decisions are impacted by deviations from base projections for the most important sources of uncertainty the four du pathways framework steps listed above and summarized graphically in fig 3 are described in more detail in the remainder of this section 3 1 identifying infrastructure investment and drought management policy tradeoffs as mentioned in section 1 du pathways framework bridges three active areas of research in recent literature 1 dynamic adaptive pathway planning haasnoot et al 2013 zandvoort et al 2017 2 rof based urban water portfolio planning zeff et al 2014 2016 trindade et al 2017 herman et al 2014b and 3 multi objective optimization under deep uncertainty trindade et al 2017 kwakkel et al 2014 watson and kasprzyk 2016 in du pathways action triggers based on the rof dynamics are the basis of infrastructure investment and management policies these policies are in turn what determine the infrastructure investment pathways the rof based action triggers or decision variables are based on the premise of monitoring storage to demand dynamics to yield a time continuous assessment of when the risks of unacceptably low water supply capacity requires action as demonstrated by zeff et al 2016 the rof based action triggers have the advantage of providing system adaptability across planning time scale decisions i e annual infrastructure construction and short term management time scale decision i e the weekly use of drought mitigation instruments in either case actions are triggered when the given rof metric palmer and characklis 2009 crosses rof threshold values stated in a fully specified infrastructure investment and management policy zeff et al 2016 the set of all regional rof triggers other financial policy variables and infrastructure construction order for all utilities in the problem defines the operations of all drought mitigation instruments and the strategy for the construction of infrastructure this set is what is in this work called an infrastructure investment and management policy shown in fig 4 with that the minimization problem to be solved then becomes a matter of finding positions for the decision levers in fig 4 where the levers represent the decision policy variables that lead to pareto approximate policies 3 1 1 problem formulation the multi objective minimization problem formulation presented below in eqs 1 6 is a general mathematical abstraction for search based identification of regional water infrastructure investment and management policies denoted by θ for all members of a cooperative group of water utilities the optimization problem focuses on finding pareto optimal policies θ that minimize the objective function vector f 1 θ a r g m i n θ f where 2 f f rel x s θ r t θ t t θ j l a θ i t ico ψ s f rf x s r o f θ r t θ t t θ j l a θ i t ico ψ s f npv x l r o f ico ψ s f fc x s r o f θ r t θ t t θ j l a θ a c f c θ i r t θ i t ico ψ s x l r o f f wfpc x s r o f θ r t θ t t θ j l a θ a c f c θ i r t θ i t ico ψ s x l r o f f jla θ j l a s t 3 me 1 me bi where 4 x x s r o f x l r o f x s 5 θ θ r t θ t t θ a c f c θ i r t θ i t ico θ j l a 6 ψ s ψ w c u ψ w c u ψ w c u ψ w c u ψ d u ψ d u 1 ψ d u 2 ψ d u n r in the equations above f is the vector values objective function where frel is the supply reliability objective frf is the restriction frequency fnpv is the net present cost of infrastructure ffc is the total cost of financial drought mitigation instruments contingency fund and insurance loading plus debt repayment fwfpc is the worse first percentile of financial variability caused by the supply drought mitigation instruments restrictions and transfers and fjla is the combined jordan lake allocation objective although these objectives are formulated specifically in the context of the research triangle test case they encompass examples of different types of performance measures relevant to urban water supply management and investment pathways in general a detailed formulation of all objectives can be found in the supplement s1 variable x is the time varying state matrix across all of the utilities where the components x srof and x lrof are vectors of short and long term values for the dynamic tracking of rof the decision variable θ acfc is a vector of annual contingency fund contributions which are percentages off annual revenue saved in a utility s drought mitigation fund as mentioned above restrictions transfers insurance and infrastructure investment decisions are all formulated consistently through time using rof triggers the overall regional policies are composed of the following decisions for all utilities the θ rt vector of restriction triggers the θ tt vector of transfer triggers the θ jla vector of jordan lake allocations the θ irt vector of insurance restriction triggers the θ it vector of long term rof infrastructure construction triggers and the ico vectors with the infrastructure construction ordering for each utility overall all decision variables are encompassed by the triggers θ plus the infrastructure construction order ico a pareto optimal management and investment policy is denoted by θ in eq 1 eq 3 enforces the requirement that two infrastructure options such as high and low capacity expansions of a reservoir cannot be built in the same realization zeff et al 2016 erfani et al 2018 in eq 3 me represents a generic subset of mutually exclusive infrastructure options within the set of built or prospective infrastructure bi matrix ψ s is comprised of the vector samples of the deeply uncertain variables of concern where each element corresponds to a specific uncertainty both sampling processes for matrix ψ s well characterized uncertainty wcu sampling ψ wcu and deep uncertainty du sampling ψ du are described in detail in the next section 3 1 2 sampling states of the world for alternatives generation all of the objectives presented above in eq 2 except for the jordan lake allocation are stochastically evaluated within a monte carlo simulation framework across an ensemble of candidate sows comprised of time series of inflows evaporations and demands and potentially several deeply uncertain factors the du pathways framework distinguishes between two formulations of the water portfolio management and infrastructure investment problem these formulations differ in their forward monte carlo based evaluations based on sampling the matrices of natural inflows ni evaporation series e unrestricted demands ud and samples of deeply uncertain factors ψ s trindade et al 2017 the well characterized uncertainty wcu and deep uncertainty du sampling schemes employed in this study are illustrated in fig 5 fig 5 a shows that the wcu sampling scheme specifies sows rows of rectangles by sampling uncertainties for which historical data driven distributions are available from their historical observations represented by colored rectangles e g natural stream inflows evaporation rates and demand fluctuations around projected mean demands for all other factors represented by grey rectangles in fig 5 a deterministic best projections are used alternatively the du sampling strategy shown in fig 5 b increases the number of uncertain factors that are varied across sows by sampling uncertainties not only from data derived distributions but also from expert elicitation more formally the wcu and du sampling schemes can be abstracted mathematically as a forward monte carlo approximation of the system preformance see eq 7 the different sampling strategies do not change the form of the objective functions but do broaden or narrow the captured uncertainties the alternative sampling of sows yields differing levels of difficulty or noise in objective evaluations trindade et al 2017 this strategy for incorporating deep uncertainties during search is different than suggested in other prominent works that have focused search on specific sets of deeply uncertain sows and subsequently perform a comparison across a broader suite of sows watson and kasprzyk 2016 another approach in the literature has been to tailor search to optimize for robustness in each objective using a robustness metric based on interquartile ranges kwakkel et al 2014 here our formulation of the search problem scales efficiently as number and complexity of sows increase maintains the ability to distinguish objective performance tradeoffs and the robustness effects of searching under deep uncertainties and provides a computational experimental design where the stability of decision maker preferences for performance tradeoffs can be carefully tested given insights from evaluative robustness assessments and scenario discovery as a consequence the sampling strategies aid the du pathways framework in being consistent with the original mordm methodology table 3 shows the deep uncertainties considered in this study and as represented in eq 7 are assumed to be uncorrelated with the well characterized uncertainties 7 ni s e s ud s ψ s f ni h e h ud h s r d within eq 7 the subscript s denotes synthetic and h denotes historical time series synthetically generated series compose matrices for natural inflows ni evaporation e and unrestricted demands ud where their dimensions are the number of synthetic series samples nr by the total number of weeks in the full planning horizon nw the historical data used to derive the synthetic series subscript h are represented in respective matrices in eq 7 of dimension of the number years in the historical record nhr and the number of weeks in a year nw cross site correlation and temporal auto correlation in the historical natural inflows and evaporation time series are captured with the s and r matrices respectively the joint pdf represented by f characterizes the inflows evaporation rates and demand fluctuations around annual means all well characterized uncertainties while d is an assumed pdf for the deeply uncertain factors that can take the following forms depending on the sampling scheme 8 d w c u x i 0 n f δ x i 9 d d u l u i 0 n f u l i u i within eq 8 δ is a dirac function x is a vector of best projections for the deeply uncertain factors considered in the problem i is the index of a deeply uncertain factor and nf is the total number of deeply uncertain factors within eq 9 l and u are sets of lower and upper bounds for the deeply uncertain factors and u denotes a uniform distribution the initial probabilistic management and infrastructure investment pathways concepts introduced by zeff et al 2016 used only the wcu sampling strategy the du pathways framework significantly expands the sampling scope to consider deep uncertainties and increase their role in the discovery of alternatives non stationary hydro climatology trends are accounted for in this work by applying a weekly series of multipliers to inflow and evaporation log mean and log variance multipliers increasing and or decreasing them over time quinn et al 2018 in this work the series of multipliers was created from sinusoid functions to assure the inclusion of scenarios in which trends go up and or downwards more details in section s5 the rof metric used as a basis of the infrastructure investment and management decision triggers is presented next 3 1 3 tracking risk of failure dynamics the rof dynamics for each utility are tracked for each week by performing nrof one year long simulations e g 50 simulations all of which with the current system state as the starting point these year long simulation henceforth called rof simulations run on the demands recorded during the year 52 weeks preceding the week for which the rof is being calculated under the assumption that demand patterns will not abruptly change from the current year to the next however each of the nrof simulations runs over one of the previous nrof years of recorded inflows and evaporation rates so that a wide range of hydrological conditions is tested we recommend an nrof of at least 50 rof simulations so that the rof metric has a precision of at least 1 50 2 the rof metric simultaneously encompasses information about current demands and storages and of historical inflows and evaporation rates improving therefore on information use relative to traditional take or pay agreements and days of supply remaining metric palmer and characklis 2009 and providing the system with more adaptability a more formal mathematical formulation for how the short term and long term rof state dynamics are computed can be found in the supplement in section s2 3 1 4 drought mitigation instruments and financial model the drought mitigation instruments explored in this study build from prior work and are focused on water use restrictions and treated inter utility water transfers zeff et al 2014 2016 trindade et al 2017 caldwell and characklis 2014 palmer and characklis 2009 obviously the overall du pathways framework could consider other short term management actions as would be appropriate in other regions of application the use of restrictions and transfers are contingent on the value of the short term rof metric as shown in eqs 10 and 11 below 10 d w f x s r o f w θ r t 11 t f w f x s r o f w θ t t where dw represents the actual water demand as opposed to unrestricted on week w tfw represents the volume of treated water to be transferred in week w and θrt and θtt represent the restrictions and transfers triggers respectively restrictions are tiered based on the progressive severity of a drought so each tier may have its own short term rof trigger value the transferred volumes requested by utilities are capped for two reasons 1 limited conveyance capacity and 2 competition between two or more utilities for available volume for transfer in which case each utility receives a volume proportional to its short term rof inter utility water transfers in combination with water use restrictions have negative financial impacts on the water utilities by reducing revenues and increasing costs drought focused index insurance instruments provide a means to hedge utilities against the financial risks associated with drought mitigation measures index insurance in which payouts depend on an index value have been studied in literature to hedge utilities against droughts zeff and characklis 2013 zeff et al 2014 trindade et al 2017 hydropower generators against climate risks foster et al 2015 climate related government regulation denaro et al 2018 and groundwater irrigated agriculture against government groundwater pumping regulations during droughts blanco and gómez 2013 the rof triggered drought index insurance structure used to hedge utilities against financial losses in this study exploits a binary payout structure foster et al 2015 zeff et al 2016 the contracted payout of a given utility s insurance contract is updated yearly as follows 12 p o y e r l w i p r e t u d t w u w p t d t w w p t w i p r where poy is the insurance payout fixed on year s y insurance contract ipr is the insurance loading 1 2 for this work representing a loading of 20 ud and d are the unrestricted and actual restricted demands uwp and wp are the unrestricted and restricted water prices respectively and t is the demand tier type of consumer the insurance payouts made to a utility are contingent on the rof metric as shown in eq 13 13 i p o w 0 if x s r o f w θ i n s p o y if x s r o f w θ i n s where ipow is the payout received by a utility in a given week a t r y 1 is the utility s annual total revenue in the previous year y 1 θipo is the percent of the previous year s revenue used to calculate the insurance payout and θins is the rof trigger for the index insurance however as insuring against minor more frequent droughts would be too expensive utilities may self insure against minor droughts by creating a contingency fund that receives part of a utility s annual revenue eq 14 14 c f y 1 c f y 1 r θ a c f c y a t r y where cf is the total amount of money in the contingency fund atr is the annual total revenue θacfc is the percentage of a year s revenue to be put into the contingency fund and r is the fund s interest rate a contingency fund big enough to hedge against lower frequency extreme droughts creates challenges in terms of the opportunity costs and the political risk of re allocation to other applications high annual contingency fund contributions can be avoided with the consideration of third party insurance further details related to trigger based drought mitigation policies for supply and or financial security can be found in palmer and characklis 2009 zeff and characklis 2013 zeff et al 2014 kwakkel et al 2008 walker et al 2001 3 1 5 prioritized sequencing of supply infrastructure investment pathways as with drought mitigation and financial instruments infrastructure construction or permitting in the du pathways framework is triggered rather than pre scheduled in time walker et al 2001 triggering occurs in years when the long term rof metric defined in eqs 25 to 27 as an annual calculation crosses a set threshold zeff et al 2016 the du pathways framework specifies that when infrastructure triggering occurs for a given utility construction begins for the next infrastructure option in the construction sequence specified in the policy when infrastructure is triggered utilities issue debt to finance the new infrastructure the infrastructure and debt service payments stream for all utilities are then updated every year according to eq 15 15 oi y ds y f x l r o f θ i r o f ico oi y 1 ds y 1 in eq 15 oi y is the vector with the ids of all online supply infrastructure ds is a matrix with the streams of debt service payments for all utilities x lrof is the vector of long term rofs for all utilities θ irof is the vector of long term rof triggers for all utilities and ico is a matrix with the infrastructure construction order for each utility no operations and maintenance o m costs were considered in the study instead the model only tracks fluctuations around o m costs due to the use of short term instruments and debt repayment it was assumed that new o m costs would be absorbed by tariffs raised as needed following rate covenants specified in each issued bond another distinguishing feature of the du pathways formulation approach is the direct accounting for the possibility that infrastructure may be built and operated jointly by multiple utilities zeff et al 2016 if triggered by one utility such projects are built by all utilities who have pledged to be part of the project with the cost proportionally split meaning that associated risks would be shared among utilities this is a unique feature to this problem added as requested by the water utilities themselves and the design of contract structures for such arrangements is currently under study the supply infrastructure construction logic together with the short term drought mitigation instruments are integrated with a mass balance model and the wcu and du uncertainty sampling into a combined framework to allow for the evaluation of drought mitigation policies as described next 3 1 6 evolutionary multi objective search the complexity of the du pathways decisions the large number of objectives and the breadth of modeled uncertainties motivated our use of the multi master parallel borg multi objective evolutionary algorithm mm borgmoea hadka and reed 2013 2014 the mm borg moea is a self adaptive algorithm that exploits multiple search operators that compete with one another based on probabilistic feedbacks updated every iteration that reward those mating and mutation schemes that yield the highest search progress i e new solutions that dominate prior archived results the multi master variant of the algorithm generalizes the original borg moea s adaptive operators across coordinating instances of the algorithm each with its own suite of worker cores this hierarchical parallelization scheme cantu paz 2000 hadka and reed 2014 takes advantage of both the multiple population and master worker parallelization strategies to increase scalability and the difficulty of problems that can be addressed tang et al 2007 reed and hadka 2014 the mm borg moea s coordinating instances automatically detect search stall and share archived solutions and operator probabilities to enhance the efficiency and effectiveness of search for difficult problems giuliani et al 2018 quinn et al 2018 the serial and parallel instances of the borg moea have demonstrated high levels of performance when applied to multiobjective problems in a variety of applications including water supply portfolio planning pollution control given ecological thresholds groundwater monitoring design and reservoir control bode et al 2019 reed et al 2013 hadka et al 2012 ward et al 2015 quinn et al 2018 given that the borg moea is a stochastic global optimizer in both the wcu and du optimization formulations multiple random seed search trials of the mm borg moea are used to attain pareto approximate sets of policies see eq 1 in section 3 1 after running all the seeds we then identify the best known pareto approximate sets also termed the reference sets by epsilon non dominated sorting hadka and reed 2013 laumanns et al 2002 across all of the attained approximate sets more details on the specific mm borg moea parameterization and parallel configuration used in this work are provided subsequently in section 4 all of our subsequent results including our more detailed robustness assessments focus on the best known reference sets obtained for the wcu and du sampling schemes optimization formulations 3 2 defining and evaluating robustness 3 2 1 deep uncertainty re evaluation the du re evaluation illustrated in fig 6 defines a broader and more challenging set of sows that are used to assess the robustness of candidate infrastructure investment and management policies as noted in prior studies groves and lempert 2007 bankes 1993 the intent of this exploratory modeling and analysis is to shift away from predicting to instead refocus on discovering consequential scenarios the du re evaluation sampling scheme illustrated in fig 6 also provides a challenging but consistent space for comparing the performance objectives and robustness of policies found with wcu and du optimization formulations scenario generation for the du re evaluation sampling scheme consists of two steps the first step is to generate a number nr of new synthetic hydrological demand annual time series ni s e s ud s following the approach presented in kirsch et al 2013 see description in section s5 the second step is to sample a matrix ψ s of vectors of deeply uncertain factors ψ from distribution d d u lastly all of time series are coupled with one vector of deeply uncertain factors ψ s at a time for all vectors in matrix ψ s as illustrated in fig 6 each monte carlo evaluation nr sets of inflow evaporation rate and demand time series paired with the same sample of deeply uncertain factors or one row ψ of matrix ψ s returns one value of each objective and nr pathways the objective values of all monte carlo evaluations are later used to back calculate the objective values of the entire re evaluation exercise for a given solution as described in the subsection below 3 2 2 objectives estimation based on the du re evaluation the re evaluation samples provide means of verifying performance by recomputing the wcu or du optimization solutions objectives attained from the reduced form monte carlo sampling schemes during search see fig 5 it should also be noted the wcu and du optimization formulations are inconsistent with one another in terms of the number and severity of uncertainties that are included in search we overcome this inconsistency by using the du re evaluation to verify and compare performance tradeoffs when evaluated under a consistent suite uncertainties the du re evaluation yields an ensemble for the objectives which are aggregated as follows reliability restriction frequency net present cost of infrastructure and total annual financial cost of drought mitigation costs are calculated as their means across du re evaluation samples this approximation was required because an exact recalculation of the objectives for the du reevaluation ensemble would prohibitively increase computational demands while also requiring storage of the system state time series of all re evaluation simulations which would take estimated hundreds of terabytes to a few petabytes of storage space following prior published literature in noisy evolutionary multiobjective optimization the mean of a challenging sample is itself sensitive or biased to extreme departures in performance that degrade a solution s rank deb and gupta 2006 the mean was chosen as an approximation of how the reliability and restriction frequency objectives are calculated because of its similarity to the objective functions themselves see eqs 17 and 18 in the supplement the quality of this approximation is verified in the du pathways framework because the du re evaluation computes the exact objective values that are used for scenario discovery lastly the mean of the infrastructure npv of all re evaluation simulations returns the exact value of this objective as the objective function itself is the mean npv across realizations see eq 19 in the supplement the worse first percentile cost is calculated as the worse first percentile across the samples mimicking the original objective as a measure of supply enabled financial risk the jordan lake allocation objective does not change across samples the next step is to use the same ensemble of objectives used to back calculate the overall objective values for a single policy evaluation and use it to evaluate its robustness 3 2 3 evaluating robustness a myriad of robustness metrics have been developed for different applications while being general enough to be applied in water systems engineering in this study we exploit the domain criterion satisficing measure introduced in starr 1962 formalized in schneller and sphicas 1983 and used in several prior studies such as herman et al 2014a herman et al 2015 and lempert and collins 2007 satisficing metrics are preferred by stakeholders whose primary concern is to not violate performance constraints despite losses of attainable performance regret mcphail et al 2018 simon 1959 the reader is encouraged to refer to recent works comparing approaches to robustness lempert and collins 2007 mcphail et al 2018 dittrich et al 2016 herman et al 2015 giuliani and castelletti 2016 if interested in more details 3 3 discovering uncertain scenarios that control robustness the sows sampled in the du re evaluation sampling scheme illustrated in fig 6 are next used for scenario discovery groves and lempert 2007 bryant and lempert 2010 in scenario discovery the analyst uses the objectives of a given policy calculated for the policy re evaluation under different scenarios fig 6 to map regions of the space of uncertainties or scenarios to be sought or avoided should that policy be implemented a classification of regions of concern would ideally include most scenarios in which the policy in question meets the conditions of performance measures of focus high coverage and avoid mixtures with other scenarios high density scenario discovery is the step of the du pathways framework that allows analysts to switch from asking what should decision makers do to what scenarios should decision makers be concerned by if they implement candidate actions seminal studies introducing scenario discovery lempert et al 2008 suggested the use of the patient rule induction method or prim friedman and fisher 1999 and classification and regression trees or cart breiman et al 1984 pass fail boundaries found by both methodologies are hypercubes orthogonal to all the sources of uncertainties dalal et al 2013 when mathematically appropriate the approaches have been shown to yield interpretable and relevant scenarios the main disadvantage of both methods is that they do not capture interactions between sources of uncertainties which is often significant preventing boundaries expressed as combinations of values for multiple sources of uncertainty the issue of capturing interactions between uncertainties was addressed in quinn et al 2018 by using logistic regression to find the pass fail boundaries logistic regression is a linear classifier and therefore will return pass fail boundaries that are straight but not necessarily orthogonal to the uncertainty axes accounting therefore for interactions between sources of uncertainty logistic regression can be used to find non linear boundaries by adding features to the original data for example if a data set of scenarios has two sources of uncertainty x 1 and x 2 adding an independent term x 1 x 2 and training a logistic regression model on all three terms will return a non linear model however there are three disadvantages for this approach 1 output rules are not as easy to interpret as those from prim and cart 2 the approximate mathematical representation of the interactions between sources of uncertainty must be pre specified and 3 the boundaries are necessarily smooth in the du pathways framework presented in this study the mixture of rof based rules for short term management actions and long term large discrete investments in infrastructure yields failure regions in the uncertainty space delimited by mixtures of sampled sows that are non linear and non convex these complexities cannot be handled by logistic regression without the manual addition of several interaction terms kernel logistic regression zhu and hastie 2005 can be used to fit such complex boundaries but given kernel methods are non parametric no information about factors sensitivities would be available alternatively we propose the use of boosted trees drucker and cortes 1996 freund et al 1999 schapire 1990 boosting is a distribution free strategy to turn a weak learning model models whose accuracy is only slightly better than random guessing into a strong learning model models that can achieve arbitrarily high accuracy schapire 1990 it works by creating and ensemble of weak classifiers and forcing part of the ensemble to focus on the hard to learn parts of the problem while other parts focus on the easy to learn parts a boosted ensemble model therefore has the following shape 16 h t x t 1 t α t h t x where h is the ensemble model t is the total number of models in the ensemble αt is the boosting multiplier corresponding to model ht where t is the model index and x is the vector of explanatory variables model complexity can be adjusted by setting the number of models t in the ensemble boosting is often applied to high bias weak classifiers freund et al 1999 murphy 2012 such as classification and regression trees cart trees breiman et al 1984 with small maximum depth e g 3 or 4 yielding an ensemble of trees cart trees are comprised of nested orthogonal partitions within the explanatory variables space the partitions are found one at a time by finding the variable and its value where a split would decrease some loss function or leaf impurity the most since in scenario discovery the analyst wants to find regions of the deep uncertainty space in which a policy will fail to reach pre established performance criteria by any amount classification trees are used here boosted trees have three advantages over the previously mentioned scenario discovery methods 1 the approach captures non differentiable boundaries that typical arise from threshold based rules e g rof based action triggers 2 it captures non linearity without explicitly modeling variable interactions and 3 it is widely known to be very resistant to overfitting freund et al 1999 murphy 2012 helping assure scenario discovery maps that are simple to interpret the use of boosted trees as opposed to random forests was motivated by the preference for simple boundaries between success fail regions to be achieved with trees as shallow as possible the confidence of a prediction made by boosted trees for a specific part of the uncertainty space can be measured by the weighted by αt percentage of the trees in the ensemble which classify a sow as passing or failing the robustness requirements here for simplicity a region was classified as a failure if more than 66 of the trees indicated a failure uncertain if the classification occurred in 33 and 66 of the trees and a success if less than 33 of trees classified a sow as a failure these percentages can be adjusted to reflect stakeholders risk aversion see supplemental section s7 for a more detailed explanation of the boosted trees algorithm an important concept for understanding the impact of a particular uncertain factor on the performance of a policy is that of leaf impurity a impure leaf has a mix of success and fail scenarios as opposed to all scenarios within it corresponding to either one or the other as mentioned every time a partition is added to a tree by splitting a leaf into two based on an uncertain factor at a time it is done so decrease the impurity of the resulting leafs as much as possible the percentage total impurity decrease across all trees due to splits on each uncertain factor can be used as a measure of the impact of that deep uncertain factor in the performance of a policy see supplemental section s7 6 for a more detailed explanation of how the relative importance of the du factors is assessed 3 4 visual infrastructure pathway analysis lastly in the du pathways framework after key uncertainties have been identified we suggest detailed visual diagnostic assessments of key pathway solutions of interest to decision makers in this study we demonstrate the value of considering how the infrastructure investment and management pathways evolve for three scenarios the most favorable values for the most influential uncertainties an intermediate case of interest and one with the least favorable values for the most influential uncertainties the challenging part of this is step is the design of a plot that displays the pathways of all hydrologic realizations e g 1000 realizations for a policy while clearly showing the times at which each infrastructure option is built there are in the literature examples of visualizations of infrastructure pathways the first and most well known is the metro maps haasnoot et al 2013 zandvoort et al 2017 the metro maps have the advantages of being easy to interpret and to present to audiences however they allow for the representation of only a handful of pathways before the pathways become difficult to distinguish another option is the probabilistic pathways zeff et al 2016 which was derived from the concept of metro maps haasnoot et al 2013 the probabilistic pathways visualization makes use of transparency to indicate the timing and frequency that each candidate infrastructure option is implemented over an ensemble hydro climatic scenarios however distinguishing the pathways differences in each of the ensemble realizations is challenging the pathways visualization proposed in this work displays the pathways explicit temporal evolution for major investments for all hydrological realizations as a stacked ensemble plot the resulting plots provide a visual indication of what new water supply infrastructure would be needed as function of key hydrological or other exogenous scenario gradients this can help planners provide resources for near future construction and for permitting detailed design public consultations etc 4 computational experiment the computational experiment was divided into two phases optimization and re evaluation given the runtime of the research triangle model we used the texas advanced computing center s stampede 2 for both optimization and re evaluation phases the optimization phase was comprised of two mm borgmoea seeds trials for the wcu formulation and two random seed trials for the du optimization each optimization trial was run on 256 mpi processes 1 controller 4 master and 251 workers with 24 threads each on 128 2 x intel xeon platinum 8160 skylake nodes 48 cores per node with 1000 realizations per function evaluation see schematic or realization sampling for each type of search in fig 5 a total of 500 000 function evaluations were used for each trial 125 000 per master our runs were evaluated for their search solution quality by confirming that further search had minimal hypervolume benefits see fig s6 the resulting solutions were each re evaluated on nodes of the same type over 1000 sets of hydrological demand series against 2000 sows resulting in 2000 re evaluation function evaluations for each re evaluated solution see fig 6 but instead of 3 sows as in the figure 2000 were used the ranges of values considered for the decision variables are presented in tables 4 6 and the ϵ values i e significant precision for all objectives are presented in table 7 the mm borg moea was parameterized following prior published recommendations hadka and reed 2014 5 results 5 1 comparing tradeoffs after re evaluation our results compare the impacts of changing the sow sampling strategies fig 7 a across the wcu and du optimization formulations the two core advantages of carefully tracking the impacts of including more uncertainties in the multiobjective search are 1 an ability to distinguish how the tradeoffs for the research triangle infrastructure investment and management pathways change when stressed with more diverse scenarios for the future and 2 to confirm if switching from the more traditional wcu to the approximate sow sampling used in the du optimization significantly impacts the robustness attained by the individual utilities as expected the du optimization s approximate sampling avoids the full computational cost of the comprehensive du re evaluation fig 6 during search for a fair comparison the resulting pareto approximate infrastructure investment and management policies found with the wcu and du optimization schemes were each fully re evaluated to assess their expected tradeoffs and robustness using the more comprehensive suite of sows captured by the du re evaluation ensemble fig 7 a shows the post re evaluation objective tradeoffs panel a and the utilities relative robustness tradeoffs panel b for all policies obtained through the du and wcu optimization formulations in fig 7 a each axis represents an objective while each line represents a regional infrastructure investment and management policy the points where each line policy intersects the vertical axes represents the performance value of the metric corresponding to that axis in fig 7 a the parallel axes plot is oriented such that the ideal policy would be a horizontal line at the bottom of all axes figs 7 shows that the policies from du optimization were on average cheaper to operate and less infrastructure intensive than their wcu optimization counterparts despite the comparable restriction frequency the attained values of the rof triggers for the policies discovered using the du optimization better balance the reliability benefits and financial impacts of the short term drought mitigation financial instruments and infrastructure investments while avoiding stranded infrastructure assets fig 7 a shows that search under deep uncertainty served to shift policies towards higher levels of performance the more diverse sows in the du optimization appear to sufficiently stress the management and investment policies to force them to more effectively use the full suite of water supply portfolio options at the utilities disposal the crossing diagonal lines between the two leftmost axes of fig 7 a highlights strong tradeoffs between reliability and restriction frequency across the wcu and du optimization formulations meaning that one can only improve at a cost to the other likewise significant tradeoffs exist between reliability and total financial cost meaning that if a utility wants high reliability it has to pay for it through a contingency fund and or drought insurance lastly fig 7a highlights a tension between average annual cost and annual worst first percentile cost which is important as the utilities often struggle when confronted with significant variability in their annual costs and are often willing to pay a premium for more predictable costs transitioning to robustness tradeoffs fig 7 b uses a similar parallel axes plot to display the percentages of du re evaluation sows where each utility meets their goal performance requirements reliability 99 restriction frequency 20 and annual worst first percentile cost 10 in terms of ideal performance for robustness in fig 7 b the direction of preference is upward where the ideal policy would be represented to a line horizontally intersecting each of the utilities robustness axes at their top maximum values i e 100 the crossing diagonal lines in fig 7 b again designate that strong robustness tradeoffs exist between raleigh cary and durham in contrast owasa attains close to maximum robustness for almost all policies the specific policies that represent the most robust policies for each utility are highlighted in different colors in fig 7 b fig 8 supplements fig 7 b by more directly comparing the utilities satisficing robustness for each of the policies found by the wcu and du optimization formulations to clarify our analysis we designate the most robust policies for durham and raleigh respectively as buyer preferred policies d b1 and r b2 given their dependence on buying water transfers from cary the most robust policy for cary is designated the supplier preferred policy or c s in figs 7 and 8 fig 7 a shows that the most robust policy for durham d b1 was the only policy whose du re evaluation objectives met the performance criteria established by the utilities reliability 99 restriction frequency 20 and annual worst first percentile cost 10 even though wcu optimization found 246 pareto approximate policies versus 112 from du optimization only those policies attained by including deeply uncertain factors in search attained both high levels of objective performance and robustness against deeply uncertain scenarios du re evaluation also the most robust management and investment policies for each individual utility in the research triangle were obtained through du optimization overall the predominance of the du optimization s solutions designated by the blue lines located closer to ideal performance at the bottom of the axes in fig 7 a and b show that the policies from du optimization were on average more reliable less infrastructure intensive financially cheaper and less risky despite requiring a slightly higher restriction frequency than their wcu optimization counterparts when both sets of policies are exposed to broader uncertainty the policies resulting from the du optimization formulation have combinations of rof triggers that allow the system to respond faster and more cost efficiently to drought events avoiding costly stranded infrastructure and high capital debt levels while maintaining reliability even if the reliability performance requirement is relaxed to 97 only 4 policies from wcu optimization would meet the criteria versus 34 from du optimization 5 2 scenario discovery for compromise policies building on a better understanding of the performance tradeoffs and robustness conflicts as illustrated in figs 7 and 8 it is also important to understand what deeply uncertain factors most strongly influence robustness fig 9 shows scenario discovery maps for durham cary and raleigh each map shows the performance attained across sampled scenarios for the two most important uncertainties for each utility when the research triangle implements one of the two buyer preferred policies highlighted in figs 7 b and 8 in each panel of fig 9 the two deep uncertain factors plotted on the horizontal and vertical axes are the factors identified by the boosted trees algorithm to be the most dominant in shaping robustness performance for a given policy such dominance is quantified by the percentages in parentheses next to each factor label which indicate the decrease in impurity of the tree ensemble from splits on that factor as summarized in section 3 3 significant changes in the percentage of impurities from tree based splits of a factor represents the direct reduction of classification errors in terms of which sampled sows attain success or failure high percentage changes in impurities consequently can be interpreted as a measure of utilities performance sensitivity to a given factor table s4 in the supplement section s4 reports the impurity decreases or sensitivities for all of the sampled factors for durham cary and raleigh under both buyer preferred policies d b1 and r b2 red regions in the factor maps of fig 9 represent regions in which a utility will likely not meet its performance targets grey regions represent high performance scenarios and white regions are deemed inconclusive due to close proximity mixtures of failures and successes in a given region of the scenario space in each of the six panels of fig 9 the stars represent the most favorable scenario grey star the least favorable red and a projected future where demand growth rates in the research triangle region are 80 of the nominal values assumed in the wcu optimization values blue star fig 9 a c show the scenario discovery maps obtained for durham cary and raleigh respectively under durham s preferred policy d b1 fig 9 a shows that durham s failures are mostly contingent on demand growth as shown by the nearly vertical boundary between the successful and failed sows beyond 150 the same figure also shows that the d b1 management and investment policy effectively hedges durham s robustness against demand growth requiring a demand growth rate more than 50 over the nominal rate assumed by durham before failing to meet the utility s performance requirements however if demand growth rates are 50 to 70 above the projected values durham may still meet the performance criteria depending on how many years the utilities take to get the permits for the low capacity joint wtp fig 9 b shows that cary s failures under policy d b1 are contingent on extreme demand growth rates exceeding their nominal projections by 85 cary s failures are due to a hard treatment capacity boundary where demand would surpass the maximum upgraded treatment capacity for cary s own water treatment plant on the jordan lake fig 9 c on the other hand shows that raleigh has a far more complex failure dynamic fig 9 c shows that the most dominant factor influencing raleigh robustness is demand growth though raleigh s performance does not decrease monotonically as demand growth increases rather raleigh displays poor performance if demand growth rates are between 50 to 80 higher than the nominal projected values as indicated by the prevalence of red failed sows the reason for the stratification of failure scenarios in fig 9 c is because scenarios in the red vertical strips emerge due to poor coordination between infrastructure construction and the effective use of short term drought mitigation instruments fig 9 c shows that in low demand growth scenarios raleigh builds storage infrastructure slowly to avoid costly stranded assets while supply risks are managed effectively by short term mitigation instruments on demand growth scenarios greater than raleigh s nominal projections raleigh fails to meet the performance goals red dots when new storage infrastructure is not built fast enough to keep up with demand growth rate straining the short term drought mitigation instruments in fig 9 c the complex non convex fingering of failure or inconclusive scenarios result when the d b1 investment long term rof rules do not trigger sufficiently rapid capacity expansion and the short term rofs are not capable of mitigating the demand growth rates i e cases of slow recognition of growing risks the complex non linear fingering of the failures captured in several panels of fig 9 are an interesting result that provides a general insight as this behavior could occur for any kind of decision making metric based on thresholds such as the rof or days of supply remaining geometrically this allows a utility to more explicitly recognize the scenarios their investment and management rules do not perform as wished transitioning to the raleigh preferred buyer preferred solution r b2 in fig 9 d f the demand growth rate remains a dominant factor shaping the robustness performance for all of the utilities however fig 9 d shows that the effectiveness of durham s water use restrictions emerges as the second most important factor influencing durham s performance switching to the r b2 investment and management policy yields an appreciable increase in the number and complexity of failure scenarios for durham the underlying mechanisms that yield the complex fingering of failure scenarios in fig 9 d for durham are similar to those for raleigh under the d b1 policy in fig 9 d poor coordination between infrastructure construction and the short term use of drought mitigation instruments for the r b2 policy durham has far less hedging to robustness failures and is specifically more sensitive to the effectiveness of their populations response to restrictions transitioning to cary fig 9 e shows that cary s failure region is far larger under policy r b2 than under d b1 interestingly evaporation emerges as an important uncertain factor for cary instead of for raleigh despite falls lake raleigh s main supply source being rather shallow the emergence of evaporation as key concern for cary would imply that longer term warming from climate change could pose a severe challenge to the r b2 policy to our knowledge these results are the first to show the complex emergence of vastly different failure modes and asymmetric sensitivities that can emerge for multi utility regionalization of investment and management pathways lastly fig 9 f shows that the r b2 policy strongly hedges raleigh while degrading the robustness of both cary and durham as shown for the d b1 policy regional demand growth rate and the response of raleigh s customers to restrictions dominate its robustness performance fig 9 f again shows a complex fingering of failure scenarios as has been noted above where slow recognition of the need to expand capacity when demand growth rates exceed nominal projections by approximately 40 moderate losses in the effectiveness of restrictions serves to compound raleigh s robustness failures the consistent importance of raleigh s water use restriction effectiveness across both the d b1 and r b2 policies stresses the importance of raleigh s readiness to effectively implement water use restrictions and make sure their population will be responsive overall fig 9 provides some important regional insights for the research triangle the dominant impact of demand growth rates emphasizes that supply reliability is harder to achieve than financial goals for this system overall keeping demand growth rates at or below the utilities projected value would be an important step for all of the utilities to ensure satisfactory regional performance furthermore in most cases the reductions by 20 of the projected growth suggested by the utilities themselves would significantly improve satisfactory performance while also adding a safety margin that decreases the region s need for new infrastructure however negative impacts may not be felt if demand growth rates are close to 200 rather than around 150 fig 9 c and f but may be noticeable for rates of 50 of the projected value fig 9 d which emphasizes the complex nonlinear relations between short term instruments and long term investments available to the utilities despite such non linearities lowering demand growth rates is a clear mechanism for enhancing the robustness for all of the utilities and reducing regional performance conflicts fig 9 also contributes some general insights regarding the value of using boosted trees to guide the scenario discovery process for complex infrastructure pathway problems boosted trees succeeded in capturing complex and distinctive features of the factor mapping plots without inherent overfitting most non linearities and fingering within complex transition zones between successful and failed scenarios were identified by the boosted trees approach in addition the identification of inconclusive regions between passing and failing scenario regions are helpful for avoiding errors in complex scenario discovery contexts 5 3 policy rules in the robustness compromises fig 10 provides a more detailed illustration of the underlying decisions or rules that compose the durham d b1 and raleigh r b2 preferred policies recall that the investment and management policies are comprised of 1 rof triggers for short term mitigation instruments fig 10 a b and d and for long term infrastructure investments fig 10 e 2 allocations to the water supply pool in jordan lake allocation fig 10 c 3 annual contingency fund contributions and insurance payouts defined as percentages of annual revenue fig 10 f and g and 4 infrastructure construction order fig 10 h and i in both the d b1 and the r b2 policies there is significant similarity between durham and raleigh in their high reliance on restrictions fig 10 a on their prompt triggering of new infrastructure by utilizing low long term rof triggers fig 10 d and in their moderate to high use of contingency funds fig 10 f fig 10 a shows that durham makes consistently high use of transfers under durham s preferred policy d b1 by setting the transfer trigger value low this in turn requires a high annual contingency fund contribution as highlighted in fig 10 f fig 10 g and e show that durham uses small insurance payouts in both policies at moderate and high rofs which emphasizes the preferred strategy of relying dominantly on a contingency fund in comparing raleigh versus durham s preferred policies fig 10 b highlights competition for transfers this partly explains the robustness conflicts between durham and raleigh given their shared use of limited conveyance capacity for transfers from cary in durham s preferred robustness compromise policy d b1 durham consistently uses the full capacity of the intra utility transfer pipes from 2015 until 2027 alternatively in raleigh s preferred solution r b2 durham s limited use of transfers allowed raleigh to request medium to high volumes of transfers whenever needed see supplementary figs s1 to s4 these differences are reflected in each utility s approach to their finances because although transfers are an effective way of decreasing the need for restrictions they can be costly under policy r b2 raleigh opts for moderate payouts at moderate rofs to hedge against the higher use of transfers because of the lower associated rof trigger fig 10 b in fact raleigh s sparse use of transfers under policy r b2 warrant the use of drought insurance in contrast to policy d b1 most insurance payouts for raleigh under r b2 happen during periods of high use of transfers see supplementary figs s3 and s4 raleigh s use of financial insurance in the r b2 policy serves to reduce the political risks and opportunity costs associated with a large and rarely used contingency fund 5 4 infrastructure pathway ensemble the durham preferred policy d b1 and the raleigh preferred policy r b2 capture alternative strategies for navigating the performance and robustness conflicts highlighted in figs 7 and 8 and each lead to distinctly different future pathways for the research triangle fig 11 contributes a detailed analysis of the capacity expansions and rof dynamics for durham and raleigh that emerge from the d b1 policy the plots in fig 11 focus on the resultant capacity expansion pathways and rof time series corresponding to the three highlighted scenarios from fig 9 the most favorable sow left column the sow with 80 of the nominal demand growth rate middle column and the least favorable sow right column for durham and raleigh fig 11 a c plot durham s storage capacity and short term rof dynamics for each of the three demand growth rate scenarios likewise fig 11 d f plot durham s resulting ensemble of infrastructure pathways across the demand growth scenarios fig 11 d f show the infrastructure pathways for the same scenarios the horizontal axes of each pathway panel represents time from 2015 to 2060 encompassing planning horizon of 45 years all of our illustrated infrastructure pathway plots display stacks of 1000 horizontal multicolored lines each line corresponding to the investment sequences for one hydrological realization for each realization line the colors represent the infrastructure project that was built last in time for example the dark green horizontal line segment close to the center of the plot indicates that in that realization indicates that durham builds the teer quarry typically between 2040 to 2050 an important concern captured in this study relates to the importance of permitting times which vary significantly across different infrastructure investments for the pathways illustrated in fig 11 horizontal line segments sometimes show different color sequences across realizations even though the priority ordering for a given infrastructure policy is fixed as highlighted in the figure the realized infrastructure construction order for a policy may not match the order in which infrastructure is actually built over the course of a simulation due to permitting time delays this issue frequently emerged when considering investments in the joint wtp for durham under all three scenarios fig 11 d f and for raleigh under raleigh s least favorable scenario for policy d b1 fig 11 l the infrastructure construction order for policy d b1 in fig 10 h shows that the low capacity version of the joint wtp was ranked first and second in the infrastructure construction queues of raleigh and durham under policy d b1 however the colors between the salmon status quo and light or medium blue low and high capacity joint wtp in fig 11 d through fig 11 f and l show that the joint wtp was not the first option built by either utility under the considered demand growth rate and hydrologic scenarios the reason for the apparent incongruence is that the permitting and planning period of the joint wtp is sufficiently long as to allow other options namely teer quarry reservoir low capacity lake michie expansion falls lake reallocation and in a few cases richland creek quarry and reclaimed water to be built first by durham and raleigh a more detailed analysis of the pathways resulting from durham s preferred policy d b1 shows that the joint wtp expansion and co investment is important and dominantly triggered by durham figs 11 and s1 shows for even the least favorable sow that durham does not need to invest in any other projects in the queue after either version of the joint wtp is built the water treatment plant investment successfully reduces durham s long term rof to 0 after its construction see supplementary figs s1 and s2 transitioning to raleigh despite having to still build one more project in the least favorable sow fig 5l the short term rof in fig 11 i noticeably decreases with the construction of the joint wtp this reduces the pressure on raleigh s short term drought mitigation instruments despite the joint wtp being a fairly expensive project when compared to other infrastructure alternatives its shared cost makes it effective reliability and financially wise for both durham and raleigh because under both policies both utilities have a substantial allocation at jordan lake see fig 10 c aside from the joint wtp most of the focus of durham and raleigh under policy d b1 is on storage infrastructure this conclusion follows from the high prevalence of storage infrastructure seen in all of the pathways durham fig 11 d f and raleigh fig 11 j l the investments in storage come at the expense of investments in reclaimed water and intakes which are comparatively cheaper but not as effective at mitigating supply shortfalls the only appearance of reclaimed water infrastructure in the pathways occurred for durham under their least favorable sow yellow rectangle in fig 11 f largely as a consequence of construction times for the other water sources lastly it is also interesting to observe that even in their least favorable sow fig 11 l raleigh goes through substantial storage capacity growth without building a single reservoir or reservoir expansion instead increases in their supply capacity result from direct access to its jordan lake water supply pool via the joint wtp and through the falls lake s municipal pool relocation which are cheaper and less disruptive projects than storage infrastructure transitioning to raleigh s preferred buyer policy r b2 fig 12 shows the rof dynamics and infrastructure pathways for durham and raleigh under the same three demand growth rate scenarios displayed in fig 11 although figs 11 and 12 show many similarities in the resulting infrastructure investment pathways key differences that favor raleigh do emerge for the r b2 policy beyond the utilities competition for transfer water discussed above under policy d b1 as illustrated in fig 11 it is always durham who triggers either version of the joint wtp however for the policy r b2 under raleigh s least favorable sow the low capacity version of the joint wtp is triggered 60 of the time by raleigh this typically happens when raleigh finishes implementing the falls lake reallocation and durham has the low capacity version of the lake michie expansion under construction the joint wtp remains as an effective investment to reduce the rofs of both durham and raleigh under the r b2 policy differences in the d b1 and r b2 policies are most pronounced for high demand growth scenarios recall from our scenario discovery results in fig 9 that short term use and competition for water transfers yields strong differences in durham s and raleigh s robustness and failure mechanisms this results despite the similarity of the investment pathways across both policies shown in figs 11 and 12 this further stresses that sound infrastructure investment strategies must be complemented by a sound short term drought mitigation instruments for performance levels to be maintained under extraneous scenarios more specifically effective use of restrictions and transfers strategies coupled with financial instruments to hedge utilities against droughts serve to reduce and delay for years the need for building new infrastructure projects still despite a loss in performance under policy r b2 a combination of an earlier falls lake reallocation and effective use of transfers before the construction of the second infrastructure option resulted in less newly built infrastructure i e the neuse river intake was not built and the final storage capacity for the system was about 3000 mg smaller despite the mentioned differences in the pathways between both solutions shown in figs 11 and 12 their similarities similar timings and options built imply general insights for the research triangle region short term drought mitigation instruments are critical to the evolution of the system s infrastructure investment pathways regionally coordinated strategies for managing demand growth rates are an important risk hedge for all four of the research triangle utilities overall an important take away point from the pathways plots for raleigh and durham is that the teer quarry and the low capacity joint wtp projects are very likely to be needed in the foreseeable future therefore durham raleigh and cary should include the joint wtp in their budgeting process perform more detailed design analysis and contemplate their permitting requirements the main recommendation for raleigh would be to initiate an application for a reallocation of the falls lake s water quality pool to its own supply pool more broadly du optimization significantly reshaped system objectives tradeoffs robustness scenario discovery results portfolio mix of decisions and the corresponding pathways the analyses framework as illustrated in our results provides a rich context for an understanding of the system table 8 6 conclusion this work contributes the du pathways framework which includes deep uncertainty in the search phase of the infrastructure pathways problem in the context of water resources systems this work demonstrates the value of integrated regional infrastructure planning and management coupled with deep uncertainty analysis for mitigating drought risks and their corresponding financial impacts such risks are even more prevalent in areas of high population growth such as the research triangle region in north carolina the research triangle regional system demonstrates how performance and robustness conflicts can emerge between geographically close utilities in their long planning the du pathways framework s flexibility in incorporating and exploring uncertainties as demonstrated in this study highlights some general insights and benefits for regional water utilities seeking to coordinate their short term management actions weekly and long term annual investment decisions a key benefit is the ability to create more flexibility and reduce required infrastructure debt burden by carefully balancing short term decision making and long term infrastructure investment decisions the consistent integration of both management and investment decisions using rof based rules provides operators with ways to account for information beyond measurements of reservoir levels and past data allowing for decisions based on possible future conditions as well another major benefit is the possibility of performing uncertainty analysis on the effectiveness of alternative infrastructure investment pathways for individual utilities as well as broader cooperating regional coalitions this analysis not only informs utilities about what uncertainties to monitor when planning the early studies preceding the triggering of new infrastructure but also about what investments would be more effective as the results from monitoring emerge lastly the du pathways framework allowed for a clear understanding of strong interdependencies between short term drought mitigation and financial instruments as well as their potential to determine the proper timing of investments in new infrastructure the du pathways framework centralizes and promotes all these benefits for single and regional water utility infrastructure planning and management the inclusion of deep uncertainties in the search based identification water supply management and investment portfolios yielded pathway policies that are more robust according to performance criteria defined by the utilities themselves and that also provide better performance and robustness compromises between the four utilities in the research triangle region the scenario discovery analysis showed that the key driver of system performance and robustness is demand growth which utilities can act on through coordinated demand management the infrastructure pathways analysis also showed that demand growth may determine whether specific investments in infrastructure will be needed by a given point in time should the right conditions be observed how key uncertainties may drive the need for investments to be sooner or later or if some candidate projects would never be prioritized at all even under the most adverse futures this information is crucial for utilities to design their rate structure to accommodate for debt servicing for the maintenance of their credit rating when issuing the required bonds future work could consider a wider variety of drought mitigation instruments new strategies for sampling and potentially screening deep uncertainties and short term drought mitigation policies that adapt their triggers with the construction of new infrastructure a meta policy so to speak the insights of this work have broad merit for water utilities facing growing pressures to better coordinate management and investment decisions to confront resource contention growing urban demands and increasingly extreme droughts credit authorship contribution statement b c trindade conceptualization data curation writing original draft writing review editing p m reed conceptualization data curation writing original draft writing review editing g w characklis conceptualization data curation writing original draft writing review editing declaration of competing interest none acknowledgements funding for this work was provided by the national institute of food and agriculture u s department of agriculture wsc agreement no 2014 67003 22076 additional support was provided by the u s national science foundation s water sustainability and climate program award no 1360442 the views expressed in this work represent those of the authors and do not necessarily reflect the views or policies of the nsf or the usda we would also like to thank the editorial team and three anonymous reviewers for helping us to enhance the clarity and contribution of this study supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103442 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
562,this study contributes the deep uncertainty du pathways framework for bridging long term water supply infrastructure investments and improved short term water portfolio management e g restrictions water transfers financial instruments etc to yield a regional water supply policy robust to supply and financial failures the du pathways framework combines flexibility providing risk of failure rof decision rules dynamic adaptive policy pathways concepts and a careful consideration of time evolving information feedbacks to yield management conditioned infrastructure pathways for regions the du pathways framework has been developed to carefully consider multi actor regional contexts with the goal of aiding stakeholders in discovering pathway policies that attain high performance levels for supply reliability and financial stability across challenging deeply uncertain futures and to guide robustness compromises that may be necessary between regional actors as demonstrated in the research triangle region of north carolina du pathways clarifies how to identify robust infrastructure investment and management policies across the municipalities of raleigh durham cary and chapel hill our results provide insights about the most cost effective infrastructure options to be pursued in the near term clarify which sources of uncertainty drive the performance tradeoffs and robustness conflicts across the regional system and demonstrate valuable information on the diversity of interdependent failure modes that may emerge across the multiple actors implementing each candidate policy keywords optimization multiobjective optimization robustness water utilities infrastructure 1 introduction water utilities globally are facing a growing pressure to proactively address growing demands higher levels of resource contention and increasingly uncertain water availability bonzanigo et al 2018 hall et al 2014 a tension exists between improving the efficiency and coordination of regional water supplies to delay or eliminate major infrastructure investments and the eventual effects of demand growth rates that necessitate water supply capacity expansions soft water management strategies conservation inter utility water transfers demand management etc are a key option to cope with growing domestic water demand gleick 2002 2003 in the united states us a key driver increasing the importance of soft water management strategies is that most large projects of federal and or state interest have already been built lund 2013 however growing urban water demands and increasingly uncertain climate conditions are motivating efforts for redesigning system capacity by upgrading existing and adding new infrastructure to maintain reliable water supply systems shafer and fox 2017 the hard and soft path views for urban water portfolio planning should be treated as complementary as even though infrastructure expansion may be needed it is not desirable and may be mitigated by soft water management strategies however both the soft and hard approaches may be detrimental to utilities finances together with outstanding debt decreasing cash flows are significant factors that negatively impact utilities credit ratings which can increase the cost of borrowing to build new infrastructure moody s 2017 zeff et al 2014 zeff and characklis 2013 trindade et al 2017 the financial impacts of drought mitigation strategies the current negative financial outlook for the water utility sector moody s 2019 the lack of appropriate financing for water utilities gleick et al 2014 all stress the need for new frameworks that facilitate improved water portfolio planning that effectively combines short term drought mitigation instruments for managing financial risks and long term infrastructure investment pathways while accounting diverse sources of uncertainty these needs motivated the creation of the deep uncertainty pathways du pathways framework proposed and demonstrated in this study integrating long term water infrastructure investment and short term management is a difficult task because of the potentially large number of decisions that must be considered over decadal timescales as well as the significant uncertainties inherent to the problem hydro climatic uncertainties such as near term streamflows and evaporation rates have been extensively studied and characterized with well established statistical techniques stedinger 1993 martins and stedinger 2000 herman et al 2016 lall 1995 however water infrastructure investment and management problems include a multitude of uncertainties for which decision makers cannot agree on their boundaries importance and probability distributions termed deep uncertainties kwakkel et al 2016 knight 1921 examples include regional demand growth political and economic uncertainties and climate change herman et al 2014a trindade et al 2017 milly et al 2008 the complexity of the infrastructure investment and management problem poses key questions how should a decision maker devise infrastructure investment and drought management strategies that adequately capture their interdependencies to improve water supply reliability and financial stability how can stakeholders develop management and investment policies that are sufficiently adaptive and robust over decades given the impossibility of predicting climate population shifts political landscapes and other sources of uncertainty how do decision makers know when to make substantial and irreversible investments in new infrastructure while avoiding stranded assets although these are not new issues utilities are still struggling with these questions bonzanigo et al 2018 paulson et al 2018 existing commercial frameworks for water infrastructure management modeling e g hydrologics 2009 sieber and purkey 2015 labadie 2011 focus on existing infrastructure have strong limitations on their inclusion of uncertainties and do not connect to key concepts from the infrastructure investment literature for example the real option analysis roa cox et al 1979 literature of the last 15 years wang and de neufville 2005 erfani et al 2018 fletcher et al 2017 2019 presents and exemplifies a flexible rule based logic that emphasizes near term information to inform decisions about infrastructure planning and policy implementation our du pathways framework integrates some features of roa such as a focus on flexible decision rules and a careful consideration of short term versus long term information for the planning and operation of static and adaptive infrastructure options however roa as a single objective approach is limited in its accounting of stakeholders with diverse interests and its underlying mathematical tree logic becomes computationally intractable when large number of uncertainties are considered curse of dimensionality dittrich et al 2016 borgomeo et al 2018 address some of these concerns by proposing a multi objective evaluation of candidate water supply investments they maximize robustness and minimize risk and cost of a 30 year long fixed construction schedule encompassing multiple infrastructure options it should be noted that the derived sequence of investments is not adaptive and does not account for evolving information feedbacks these limitations also exist in the recent studies by beh et al 2015 and huskova et al 2016 although the former seek to improve adaptivity by incorporating a similar emphasis on near term information as done in roa alternatively the recently introduced dynamic adaptive pathways policy dapp approach haasnoot et al 2013 kwakkel et al 2014 adds flexibility to the planning process by making use of potentially diverse sets metrics signposts dapp however still maintains a strong reliance on the use of limited numbers of predefined action sequences that require high levels of institutional stability and coordinated consensus over the long term the du pathways framework introduced in this work is first formal integration of dynamic and adaptive water infrastructure pathways zeff et al 2016 haasnoot et al 2013 annual long term planning financial and supply reliability drought mitigation instruments weekly short term decisions zeff et al 2014 trindade et al 2017 and the recent extensions of the multiobjective robust decision making mordm framework that incorporate deep uncertainties in search based identification candidade actions kasprzyk et al 2013 trindade et al 2017 kwakkel et al 2014 watson and kasprzyk 2016 du pathways addresses decision making at two time scales weekly for management decisisons and annual for infrastructure investment decisions following the approach recommended by zeff et al 2016 the integrated infrastructure investment and management policies specify short and long term decision making rules based on the risk of failure metric rof as well as candidate orderings for infrastructure investments in contrast to recent bayesian adaptive rules focused on climate states fletcher et al 2019 our rof based rules encompass how the time dynamics of natural water supply capacity states and human demands co evolve to trigger actions both in the short term as well as the long term to address vulnerabilities coupled human and natural uncertainties shape the information encompassed in the rof metric while allowing for an operational simplicity for the proposed action rules the resulting rof rule based policies can tractably be optimized to multiple states of the world built around dozens of sources of uncertainty while avoiding the curse of dimensionality inherent to the tree structured policies used in roa our du pathways framework provides an internally consistent rule form for resolving a broader array of water portfolio management and investment actions by combining infrastructure planning and management rules in a single policy the water portfolio planning approach aids the design of drought mitigation instruments tailored to new infrastructure to minimize the need of further investments the main limitation of the infrastructure pathways work by zeff et al 2016 is its limited treatment of uncertainties the du pathways framework contributed here significantly expands the suite of deep uncertainties considered in the search based identification of candidate infrastructure investment and management policies additionally the framework formalizes a detailed evaluation of the multi city policies robustness tradeoffs du pathways aids stakeholders in navigating these tradeoffs through a combination of visual decision analytics woodruff et al 2013 and enhanced scenario discovery in which boosted trees schapire 1999 aid in the identification of the uncertainties that most strongly shape the infrastructure investment and management policies vulnerabilities the application of the du pathways framework is demonstrated the research triangle test case described in this and prior works trindade et al 2017 zeff et al 2016 herman et al 2014a zeff et al 2014 we have here discovered how the region s vulnerabilities evolve under alternative policies clarified significant interdependencies between the region s four utilities and shown the importance of carefully balancing supply financial instruments and investments in new infrastructure 2 the research triangle test case the du pathways framework introduced in this study is demonstrated on the research triangle region in north carolina the test case is comprised of the four water utilities fig 1 currently responsible for meeting the majority of the region s growing demand in the research triangle portion of the neuse and cape fear river basins raleigh durham cary and orange water and sewer authority owasa representing carrboro and chapel hill rapid population growth in the research triangle over the past 30 years has decreased the distances between utilities services areas allowing for inter network pipe connections for treated water transfers and increased the potential independent impacts of the utilities operations on shared regional resources the four utilities participate in the triangle water supply partnership twsp a partnership founded in 2009 and the called the jordan lake partnership or jlp after two historical droughts 2002 and 2007 2008 the jlp has focused on developing collaborative short term drought management and infrastructure planning solutions for its members with specific focus on meeting projected 2060 water supply demands triangle j 2014 the two key challenges for the research triangle region are to develop 1 collaborative plans for short term regional management of the existing water infrastructure and 2 plan long term infrastructure investments to meet future demands the research triangle test case includes all current infrastructure systems providing water to the four utilities table 1 as well as recently proposed candidate investments table 2 including additional storage conveyance and treatment projects across the four utilities laid out by the jlp in triangle j 2014 based on data from our utility partners our model assumes that all options except the joint water treatment plant already have all permits approved and a completed basic design as a consequence after an option is triggered we focus on uncertain construction timing by assuming a uniformly distributed random time from 3 to 5 years for an option to be built and online given the joint water treatment plant s increased institutional complexity the permitting and design time i e prior to beginning construction is estimated by the utilities as requiring an additional 12 years owasa and durham own allocations in the jordan lake that can currently only be accessed through cary s water treatment plant on eastern shore of jordan lake subject to treatment and inter utility conveyance availabilities fig 1 currently jordan lake s water supply storage is still not fully allocated and several utilities in the research triangle are planning on submitting new allocation requests which gives rise to potential resource contention that the four studied water utilities want to avoid key short term i e drought regional management instruments include water use restrictions and treated water transfers that are employed using weekly operational rules with the goal of making better use of the research triangle region s shared resources efforts over the last several years have focused on defining management operations collaboratively so that the impacts from the measures taken by each utility has a predictable impact on the operations of the other utilities for example if durham owasa and raleigh rely on treated transfers from cary as a drought mitigation measure for improving reliability each transfer request is going to directly impact all requesting utilities given conveyance constraints and the cary water treatment plant s capacity therefore it is an important challenge to define the different mixes of water use restrictions and transfers for the individual cooperating utilities while improving water supply reliability for all of the research triangle s actors as a first step in our development of the du pathways framework contributed in this study we developed a flexible regional model that can encompass a breadth of system uncertainties and support our evaluation of the diverse portfolios of short term management actions as well as long term infrastructure investment pathways at its most basic core our simulation of the research triangle is a water mass balance model with the inputs and outputs illustrated in fig 2 below the model is used in a set of optimization runs whose results were further assessed to identify robust infrastructure management and investment policies for each of the four research triangle utilities beyond management actions made at a weekly timestep the utilities must also define infrastructure investment pathways that specifically prioritize and sequence major projects among options specified in triangle j 2014 as with the short term drought mitigation instruments restrictions and transfers such infrastructure sequences and corresponding decision making rules are sought that will positively impact the research triangle region s supply reliability and financial stability the management and investment actions are critically interdependent given that effective short term drought mitigation instruments can serve to dramatically delay and or reduce capital investments zeff et al 2016 3 methodology the major steps of the du pathways methodology as illustrated in fig 3 are as follows 1 identifying infrastructure investment and drought management policy tradeoffs candidate pareto approximate infrastructure investment and water portfolio management policies are identified using robust multiobjective optimization reed et al 2013 hadka and reed 2014 deb et al 2002 hernández lobato et al 2016 2 defining and evaluating robustness exploiting a more expansive sampling of the potential deep uncertainties that could pose challenging states of the world sows in which robustness metrics can be used to evaluate and rank order solutions for their robustness for reviews and comparisons of alternative definitions of robustness see mcphail et al 2018 herman et al 2015 giuliani and castelletti 2016 3 discovering uncertain scenarios that control robustness the pareto approximate of policies of interest once evaluated against a broader sample of sows undergo further diagnostic assessment by means of sensitivity analysis saltelli 2002 herman and usher 2017 jaxa rozen and kwakkel 2018 and or scenario discovery bryant and lempert 2010 quinn et al 2018 to determine which specific uncertainties dominantly impact system s performance factor prioritization for specific combinations of values factor mapping 4 visual infrastructure pathway analysis use of visual analytics to determine how infrastructure investment decisions are impacted by deviations from base projections for the most important sources of uncertainty the four du pathways framework steps listed above and summarized graphically in fig 3 are described in more detail in the remainder of this section 3 1 identifying infrastructure investment and drought management policy tradeoffs as mentioned in section 1 du pathways framework bridges three active areas of research in recent literature 1 dynamic adaptive pathway planning haasnoot et al 2013 zandvoort et al 2017 2 rof based urban water portfolio planning zeff et al 2014 2016 trindade et al 2017 herman et al 2014b and 3 multi objective optimization under deep uncertainty trindade et al 2017 kwakkel et al 2014 watson and kasprzyk 2016 in du pathways action triggers based on the rof dynamics are the basis of infrastructure investment and management policies these policies are in turn what determine the infrastructure investment pathways the rof based action triggers or decision variables are based on the premise of monitoring storage to demand dynamics to yield a time continuous assessment of when the risks of unacceptably low water supply capacity requires action as demonstrated by zeff et al 2016 the rof based action triggers have the advantage of providing system adaptability across planning time scale decisions i e annual infrastructure construction and short term management time scale decision i e the weekly use of drought mitigation instruments in either case actions are triggered when the given rof metric palmer and characklis 2009 crosses rof threshold values stated in a fully specified infrastructure investment and management policy zeff et al 2016 the set of all regional rof triggers other financial policy variables and infrastructure construction order for all utilities in the problem defines the operations of all drought mitigation instruments and the strategy for the construction of infrastructure this set is what is in this work called an infrastructure investment and management policy shown in fig 4 with that the minimization problem to be solved then becomes a matter of finding positions for the decision levers in fig 4 where the levers represent the decision policy variables that lead to pareto approximate policies 3 1 1 problem formulation the multi objective minimization problem formulation presented below in eqs 1 6 is a general mathematical abstraction for search based identification of regional water infrastructure investment and management policies denoted by θ for all members of a cooperative group of water utilities the optimization problem focuses on finding pareto optimal policies θ that minimize the objective function vector f 1 θ a r g m i n θ f where 2 f f rel x s θ r t θ t t θ j l a θ i t ico ψ s f rf x s r o f θ r t θ t t θ j l a θ i t ico ψ s f npv x l r o f ico ψ s f fc x s r o f θ r t θ t t θ j l a θ a c f c θ i r t θ i t ico ψ s x l r o f f wfpc x s r o f θ r t θ t t θ j l a θ a c f c θ i r t θ i t ico ψ s x l r o f f jla θ j l a s t 3 me 1 me bi where 4 x x s r o f x l r o f x s 5 θ θ r t θ t t θ a c f c θ i r t θ i t ico θ j l a 6 ψ s ψ w c u ψ w c u ψ w c u ψ w c u ψ d u ψ d u 1 ψ d u 2 ψ d u n r in the equations above f is the vector values objective function where frel is the supply reliability objective frf is the restriction frequency fnpv is the net present cost of infrastructure ffc is the total cost of financial drought mitigation instruments contingency fund and insurance loading plus debt repayment fwfpc is the worse first percentile of financial variability caused by the supply drought mitigation instruments restrictions and transfers and fjla is the combined jordan lake allocation objective although these objectives are formulated specifically in the context of the research triangle test case they encompass examples of different types of performance measures relevant to urban water supply management and investment pathways in general a detailed formulation of all objectives can be found in the supplement s1 variable x is the time varying state matrix across all of the utilities where the components x srof and x lrof are vectors of short and long term values for the dynamic tracking of rof the decision variable θ acfc is a vector of annual contingency fund contributions which are percentages off annual revenue saved in a utility s drought mitigation fund as mentioned above restrictions transfers insurance and infrastructure investment decisions are all formulated consistently through time using rof triggers the overall regional policies are composed of the following decisions for all utilities the θ rt vector of restriction triggers the θ tt vector of transfer triggers the θ jla vector of jordan lake allocations the θ irt vector of insurance restriction triggers the θ it vector of long term rof infrastructure construction triggers and the ico vectors with the infrastructure construction ordering for each utility overall all decision variables are encompassed by the triggers θ plus the infrastructure construction order ico a pareto optimal management and investment policy is denoted by θ in eq 1 eq 3 enforces the requirement that two infrastructure options such as high and low capacity expansions of a reservoir cannot be built in the same realization zeff et al 2016 erfani et al 2018 in eq 3 me represents a generic subset of mutually exclusive infrastructure options within the set of built or prospective infrastructure bi matrix ψ s is comprised of the vector samples of the deeply uncertain variables of concern where each element corresponds to a specific uncertainty both sampling processes for matrix ψ s well characterized uncertainty wcu sampling ψ wcu and deep uncertainty du sampling ψ du are described in detail in the next section 3 1 2 sampling states of the world for alternatives generation all of the objectives presented above in eq 2 except for the jordan lake allocation are stochastically evaluated within a monte carlo simulation framework across an ensemble of candidate sows comprised of time series of inflows evaporations and demands and potentially several deeply uncertain factors the du pathways framework distinguishes between two formulations of the water portfolio management and infrastructure investment problem these formulations differ in their forward monte carlo based evaluations based on sampling the matrices of natural inflows ni evaporation series e unrestricted demands ud and samples of deeply uncertain factors ψ s trindade et al 2017 the well characterized uncertainty wcu and deep uncertainty du sampling schemes employed in this study are illustrated in fig 5 fig 5 a shows that the wcu sampling scheme specifies sows rows of rectangles by sampling uncertainties for which historical data driven distributions are available from their historical observations represented by colored rectangles e g natural stream inflows evaporation rates and demand fluctuations around projected mean demands for all other factors represented by grey rectangles in fig 5 a deterministic best projections are used alternatively the du sampling strategy shown in fig 5 b increases the number of uncertain factors that are varied across sows by sampling uncertainties not only from data derived distributions but also from expert elicitation more formally the wcu and du sampling schemes can be abstracted mathematically as a forward monte carlo approximation of the system preformance see eq 7 the different sampling strategies do not change the form of the objective functions but do broaden or narrow the captured uncertainties the alternative sampling of sows yields differing levels of difficulty or noise in objective evaluations trindade et al 2017 this strategy for incorporating deep uncertainties during search is different than suggested in other prominent works that have focused search on specific sets of deeply uncertain sows and subsequently perform a comparison across a broader suite of sows watson and kasprzyk 2016 another approach in the literature has been to tailor search to optimize for robustness in each objective using a robustness metric based on interquartile ranges kwakkel et al 2014 here our formulation of the search problem scales efficiently as number and complexity of sows increase maintains the ability to distinguish objective performance tradeoffs and the robustness effects of searching under deep uncertainties and provides a computational experimental design where the stability of decision maker preferences for performance tradeoffs can be carefully tested given insights from evaluative robustness assessments and scenario discovery as a consequence the sampling strategies aid the du pathways framework in being consistent with the original mordm methodology table 3 shows the deep uncertainties considered in this study and as represented in eq 7 are assumed to be uncorrelated with the well characterized uncertainties 7 ni s e s ud s ψ s f ni h e h ud h s r d within eq 7 the subscript s denotes synthetic and h denotes historical time series synthetically generated series compose matrices for natural inflows ni evaporation e and unrestricted demands ud where their dimensions are the number of synthetic series samples nr by the total number of weeks in the full planning horizon nw the historical data used to derive the synthetic series subscript h are represented in respective matrices in eq 7 of dimension of the number years in the historical record nhr and the number of weeks in a year nw cross site correlation and temporal auto correlation in the historical natural inflows and evaporation time series are captured with the s and r matrices respectively the joint pdf represented by f characterizes the inflows evaporation rates and demand fluctuations around annual means all well characterized uncertainties while d is an assumed pdf for the deeply uncertain factors that can take the following forms depending on the sampling scheme 8 d w c u x i 0 n f δ x i 9 d d u l u i 0 n f u l i u i within eq 8 δ is a dirac function x is a vector of best projections for the deeply uncertain factors considered in the problem i is the index of a deeply uncertain factor and nf is the total number of deeply uncertain factors within eq 9 l and u are sets of lower and upper bounds for the deeply uncertain factors and u denotes a uniform distribution the initial probabilistic management and infrastructure investment pathways concepts introduced by zeff et al 2016 used only the wcu sampling strategy the du pathways framework significantly expands the sampling scope to consider deep uncertainties and increase their role in the discovery of alternatives non stationary hydro climatology trends are accounted for in this work by applying a weekly series of multipliers to inflow and evaporation log mean and log variance multipliers increasing and or decreasing them over time quinn et al 2018 in this work the series of multipliers was created from sinusoid functions to assure the inclusion of scenarios in which trends go up and or downwards more details in section s5 the rof metric used as a basis of the infrastructure investment and management decision triggers is presented next 3 1 3 tracking risk of failure dynamics the rof dynamics for each utility are tracked for each week by performing nrof one year long simulations e g 50 simulations all of which with the current system state as the starting point these year long simulation henceforth called rof simulations run on the demands recorded during the year 52 weeks preceding the week for which the rof is being calculated under the assumption that demand patterns will not abruptly change from the current year to the next however each of the nrof simulations runs over one of the previous nrof years of recorded inflows and evaporation rates so that a wide range of hydrological conditions is tested we recommend an nrof of at least 50 rof simulations so that the rof metric has a precision of at least 1 50 2 the rof metric simultaneously encompasses information about current demands and storages and of historical inflows and evaporation rates improving therefore on information use relative to traditional take or pay agreements and days of supply remaining metric palmer and characklis 2009 and providing the system with more adaptability a more formal mathematical formulation for how the short term and long term rof state dynamics are computed can be found in the supplement in section s2 3 1 4 drought mitigation instruments and financial model the drought mitigation instruments explored in this study build from prior work and are focused on water use restrictions and treated inter utility water transfers zeff et al 2014 2016 trindade et al 2017 caldwell and characklis 2014 palmer and characklis 2009 obviously the overall du pathways framework could consider other short term management actions as would be appropriate in other regions of application the use of restrictions and transfers are contingent on the value of the short term rof metric as shown in eqs 10 and 11 below 10 d w f x s r o f w θ r t 11 t f w f x s r o f w θ t t where dw represents the actual water demand as opposed to unrestricted on week w tfw represents the volume of treated water to be transferred in week w and θrt and θtt represent the restrictions and transfers triggers respectively restrictions are tiered based on the progressive severity of a drought so each tier may have its own short term rof trigger value the transferred volumes requested by utilities are capped for two reasons 1 limited conveyance capacity and 2 competition between two or more utilities for available volume for transfer in which case each utility receives a volume proportional to its short term rof inter utility water transfers in combination with water use restrictions have negative financial impacts on the water utilities by reducing revenues and increasing costs drought focused index insurance instruments provide a means to hedge utilities against the financial risks associated with drought mitigation measures index insurance in which payouts depend on an index value have been studied in literature to hedge utilities against droughts zeff and characklis 2013 zeff et al 2014 trindade et al 2017 hydropower generators against climate risks foster et al 2015 climate related government regulation denaro et al 2018 and groundwater irrigated agriculture against government groundwater pumping regulations during droughts blanco and gómez 2013 the rof triggered drought index insurance structure used to hedge utilities against financial losses in this study exploits a binary payout structure foster et al 2015 zeff et al 2016 the contracted payout of a given utility s insurance contract is updated yearly as follows 12 p o y e r l w i p r e t u d t w u w p t d t w w p t w i p r where poy is the insurance payout fixed on year s y insurance contract ipr is the insurance loading 1 2 for this work representing a loading of 20 ud and d are the unrestricted and actual restricted demands uwp and wp are the unrestricted and restricted water prices respectively and t is the demand tier type of consumer the insurance payouts made to a utility are contingent on the rof metric as shown in eq 13 13 i p o w 0 if x s r o f w θ i n s p o y if x s r o f w θ i n s where ipow is the payout received by a utility in a given week a t r y 1 is the utility s annual total revenue in the previous year y 1 θipo is the percent of the previous year s revenue used to calculate the insurance payout and θins is the rof trigger for the index insurance however as insuring against minor more frequent droughts would be too expensive utilities may self insure against minor droughts by creating a contingency fund that receives part of a utility s annual revenue eq 14 14 c f y 1 c f y 1 r θ a c f c y a t r y where cf is the total amount of money in the contingency fund atr is the annual total revenue θacfc is the percentage of a year s revenue to be put into the contingency fund and r is the fund s interest rate a contingency fund big enough to hedge against lower frequency extreme droughts creates challenges in terms of the opportunity costs and the political risk of re allocation to other applications high annual contingency fund contributions can be avoided with the consideration of third party insurance further details related to trigger based drought mitigation policies for supply and or financial security can be found in palmer and characklis 2009 zeff and characklis 2013 zeff et al 2014 kwakkel et al 2008 walker et al 2001 3 1 5 prioritized sequencing of supply infrastructure investment pathways as with drought mitigation and financial instruments infrastructure construction or permitting in the du pathways framework is triggered rather than pre scheduled in time walker et al 2001 triggering occurs in years when the long term rof metric defined in eqs 25 to 27 as an annual calculation crosses a set threshold zeff et al 2016 the du pathways framework specifies that when infrastructure triggering occurs for a given utility construction begins for the next infrastructure option in the construction sequence specified in the policy when infrastructure is triggered utilities issue debt to finance the new infrastructure the infrastructure and debt service payments stream for all utilities are then updated every year according to eq 15 15 oi y ds y f x l r o f θ i r o f ico oi y 1 ds y 1 in eq 15 oi y is the vector with the ids of all online supply infrastructure ds is a matrix with the streams of debt service payments for all utilities x lrof is the vector of long term rofs for all utilities θ irof is the vector of long term rof triggers for all utilities and ico is a matrix with the infrastructure construction order for each utility no operations and maintenance o m costs were considered in the study instead the model only tracks fluctuations around o m costs due to the use of short term instruments and debt repayment it was assumed that new o m costs would be absorbed by tariffs raised as needed following rate covenants specified in each issued bond another distinguishing feature of the du pathways formulation approach is the direct accounting for the possibility that infrastructure may be built and operated jointly by multiple utilities zeff et al 2016 if triggered by one utility such projects are built by all utilities who have pledged to be part of the project with the cost proportionally split meaning that associated risks would be shared among utilities this is a unique feature to this problem added as requested by the water utilities themselves and the design of contract structures for such arrangements is currently under study the supply infrastructure construction logic together with the short term drought mitigation instruments are integrated with a mass balance model and the wcu and du uncertainty sampling into a combined framework to allow for the evaluation of drought mitigation policies as described next 3 1 6 evolutionary multi objective search the complexity of the du pathways decisions the large number of objectives and the breadth of modeled uncertainties motivated our use of the multi master parallel borg multi objective evolutionary algorithm mm borgmoea hadka and reed 2013 2014 the mm borg moea is a self adaptive algorithm that exploits multiple search operators that compete with one another based on probabilistic feedbacks updated every iteration that reward those mating and mutation schemes that yield the highest search progress i e new solutions that dominate prior archived results the multi master variant of the algorithm generalizes the original borg moea s adaptive operators across coordinating instances of the algorithm each with its own suite of worker cores this hierarchical parallelization scheme cantu paz 2000 hadka and reed 2014 takes advantage of both the multiple population and master worker parallelization strategies to increase scalability and the difficulty of problems that can be addressed tang et al 2007 reed and hadka 2014 the mm borg moea s coordinating instances automatically detect search stall and share archived solutions and operator probabilities to enhance the efficiency and effectiveness of search for difficult problems giuliani et al 2018 quinn et al 2018 the serial and parallel instances of the borg moea have demonstrated high levels of performance when applied to multiobjective problems in a variety of applications including water supply portfolio planning pollution control given ecological thresholds groundwater monitoring design and reservoir control bode et al 2019 reed et al 2013 hadka et al 2012 ward et al 2015 quinn et al 2018 given that the borg moea is a stochastic global optimizer in both the wcu and du optimization formulations multiple random seed search trials of the mm borg moea are used to attain pareto approximate sets of policies see eq 1 in section 3 1 after running all the seeds we then identify the best known pareto approximate sets also termed the reference sets by epsilon non dominated sorting hadka and reed 2013 laumanns et al 2002 across all of the attained approximate sets more details on the specific mm borg moea parameterization and parallel configuration used in this work are provided subsequently in section 4 all of our subsequent results including our more detailed robustness assessments focus on the best known reference sets obtained for the wcu and du sampling schemes optimization formulations 3 2 defining and evaluating robustness 3 2 1 deep uncertainty re evaluation the du re evaluation illustrated in fig 6 defines a broader and more challenging set of sows that are used to assess the robustness of candidate infrastructure investment and management policies as noted in prior studies groves and lempert 2007 bankes 1993 the intent of this exploratory modeling and analysis is to shift away from predicting to instead refocus on discovering consequential scenarios the du re evaluation sampling scheme illustrated in fig 6 also provides a challenging but consistent space for comparing the performance objectives and robustness of policies found with wcu and du optimization formulations scenario generation for the du re evaluation sampling scheme consists of two steps the first step is to generate a number nr of new synthetic hydrological demand annual time series ni s e s ud s following the approach presented in kirsch et al 2013 see description in section s5 the second step is to sample a matrix ψ s of vectors of deeply uncertain factors ψ from distribution d d u lastly all of time series are coupled with one vector of deeply uncertain factors ψ s at a time for all vectors in matrix ψ s as illustrated in fig 6 each monte carlo evaluation nr sets of inflow evaporation rate and demand time series paired with the same sample of deeply uncertain factors or one row ψ of matrix ψ s returns one value of each objective and nr pathways the objective values of all monte carlo evaluations are later used to back calculate the objective values of the entire re evaluation exercise for a given solution as described in the subsection below 3 2 2 objectives estimation based on the du re evaluation the re evaluation samples provide means of verifying performance by recomputing the wcu or du optimization solutions objectives attained from the reduced form monte carlo sampling schemes during search see fig 5 it should also be noted the wcu and du optimization formulations are inconsistent with one another in terms of the number and severity of uncertainties that are included in search we overcome this inconsistency by using the du re evaluation to verify and compare performance tradeoffs when evaluated under a consistent suite uncertainties the du re evaluation yields an ensemble for the objectives which are aggregated as follows reliability restriction frequency net present cost of infrastructure and total annual financial cost of drought mitigation costs are calculated as their means across du re evaluation samples this approximation was required because an exact recalculation of the objectives for the du reevaluation ensemble would prohibitively increase computational demands while also requiring storage of the system state time series of all re evaluation simulations which would take estimated hundreds of terabytes to a few petabytes of storage space following prior published literature in noisy evolutionary multiobjective optimization the mean of a challenging sample is itself sensitive or biased to extreme departures in performance that degrade a solution s rank deb and gupta 2006 the mean was chosen as an approximation of how the reliability and restriction frequency objectives are calculated because of its similarity to the objective functions themselves see eqs 17 and 18 in the supplement the quality of this approximation is verified in the du pathways framework because the du re evaluation computes the exact objective values that are used for scenario discovery lastly the mean of the infrastructure npv of all re evaluation simulations returns the exact value of this objective as the objective function itself is the mean npv across realizations see eq 19 in the supplement the worse first percentile cost is calculated as the worse first percentile across the samples mimicking the original objective as a measure of supply enabled financial risk the jordan lake allocation objective does not change across samples the next step is to use the same ensemble of objectives used to back calculate the overall objective values for a single policy evaluation and use it to evaluate its robustness 3 2 3 evaluating robustness a myriad of robustness metrics have been developed for different applications while being general enough to be applied in water systems engineering in this study we exploit the domain criterion satisficing measure introduced in starr 1962 formalized in schneller and sphicas 1983 and used in several prior studies such as herman et al 2014a herman et al 2015 and lempert and collins 2007 satisficing metrics are preferred by stakeholders whose primary concern is to not violate performance constraints despite losses of attainable performance regret mcphail et al 2018 simon 1959 the reader is encouraged to refer to recent works comparing approaches to robustness lempert and collins 2007 mcphail et al 2018 dittrich et al 2016 herman et al 2015 giuliani and castelletti 2016 if interested in more details 3 3 discovering uncertain scenarios that control robustness the sows sampled in the du re evaluation sampling scheme illustrated in fig 6 are next used for scenario discovery groves and lempert 2007 bryant and lempert 2010 in scenario discovery the analyst uses the objectives of a given policy calculated for the policy re evaluation under different scenarios fig 6 to map regions of the space of uncertainties or scenarios to be sought or avoided should that policy be implemented a classification of regions of concern would ideally include most scenarios in which the policy in question meets the conditions of performance measures of focus high coverage and avoid mixtures with other scenarios high density scenario discovery is the step of the du pathways framework that allows analysts to switch from asking what should decision makers do to what scenarios should decision makers be concerned by if they implement candidate actions seminal studies introducing scenario discovery lempert et al 2008 suggested the use of the patient rule induction method or prim friedman and fisher 1999 and classification and regression trees or cart breiman et al 1984 pass fail boundaries found by both methodologies are hypercubes orthogonal to all the sources of uncertainties dalal et al 2013 when mathematically appropriate the approaches have been shown to yield interpretable and relevant scenarios the main disadvantage of both methods is that they do not capture interactions between sources of uncertainties which is often significant preventing boundaries expressed as combinations of values for multiple sources of uncertainty the issue of capturing interactions between uncertainties was addressed in quinn et al 2018 by using logistic regression to find the pass fail boundaries logistic regression is a linear classifier and therefore will return pass fail boundaries that are straight but not necessarily orthogonal to the uncertainty axes accounting therefore for interactions between sources of uncertainty logistic regression can be used to find non linear boundaries by adding features to the original data for example if a data set of scenarios has two sources of uncertainty x 1 and x 2 adding an independent term x 1 x 2 and training a logistic regression model on all three terms will return a non linear model however there are three disadvantages for this approach 1 output rules are not as easy to interpret as those from prim and cart 2 the approximate mathematical representation of the interactions between sources of uncertainty must be pre specified and 3 the boundaries are necessarily smooth in the du pathways framework presented in this study the mixture of rof based rules for short term management actions and long term large discrete investments in infrastructure yields failure regions in the uncertainty space delimited by mixtures of sampled sows that are non linear and non convex these complexities cannot be handled by logistic regression without the manual addition of several interaction terms kernel logistic regression zhu and hastie 2005 can be used to fit such complex boundaries but given kernel methods are non parametric no information about factors sensitivities would be available alternatively we propose the use of boosted trees drucker and cortes 1996 freund et al 1999 schapire 1990 boosting is a distribution free strategy to turn a weak learning model models whose accuracy is only slightly better than random guessing into a strong learning model models that can achieve arbitrarily high accuracy schapire 1990 it works by creating and ensemble of weak classifiers and forcing part of the ensemble to focus on the hard to learn parts of the problem while other parts focus on the easy to learn parts a boosted ensemble model therefore has the following shape 16 h t x t 1 t α t h t x where h is the ensemble model t is the total number of models in the ensemble αt is the boosting multiplier corresponding to model ht where t is the model index and x is the vector of explanatory variables model complexity can be adjusted by setting the number of models t in the ensemble boosting is often applied to high bias weak classifiers freund et al 1999 murphy 2012 such as classification and regression trees cart trees breiman et al 1984 with small maximum depth e g 3 or 4 yielding an ensemble of trees cart trees are comprised of nested orthogonal partitions within the explanatory variables space the partitions are found one at a time by finding the variable and its value where a split would decrease some loss function or leaf impurity the most since in scenario discovery the analyst wants to find regions of the deep uncertainty space in which a policy will fail to reach pre established performance criteria by any amount classification trees are used here boosted trees have three advantages over the previously mentioned scenario discovery methods 1 the approach captures non differentiable boundaries that typical arise from threshold based rules e g rof based action triggers 2 it captures non linearity without explicitly modeling variable interactions and 3 it is widely known to be very resistant to overfitting freund et al 1999 murphy 2012 helping assure scenario discovery maps that are simple to interpret the use of boosted trees as opposed to random forests was motivated by the preference for simple boundaries between success fail regions to be achieved with trees as shallow as possible the confidence of a prediction made by boosted trees for a specific part of the uncertainty space can be measured by the weighted by αt percentage of the trees in the ensemble which classify a sow as passing or failing the robustness requirements here for simplicity a region was classified as a failure if more than 66 of the trees indicated a failure uncertain if the classification occurred in 33 and 66 of the trees and a success if less than 33 of trees classified a sow as a failure these percentages can be adjusted to reflect stakeholders risk aversion see supplemental section s7 for a more detailed explanation of the boosted trees algorithm an important concept for understanding the impact of a particular uncertain factor on the performance of a policy is that of leaf impurity a impure leaf has a mix of success and fail scenarios as opposed to all scenarios within it corresponding to either one or the other as mentioned every time a partition is added to a tree by splitting a leaf into two based on an uncertain factor at a time it is done so decrease the impurity of the resulting leafs as much as possible the percentage total impurity decrease across all trees due to splits on each uncertain factor can be used as a measure of the impact of that deep uncertain factor in the performance of a policy see supplemental section s7 6 for a more detailed explanation of how the relative importance of the du factors is assessed 3 4 visual infrastructure pathway analysis lastly in the du pathways framework after key uncertainties have been identified we suggest detailed visual diagnostic assessments of key pathway solutions of interest to decision makers in this study we demonstrate the value of considering how the infrastructure investment and management pathways evolve for three scenarios the most favorable values for the most influential uncertainties an intermediate case of interest and one with the least favorable values for the most influential uncertainties the challenging part of this is step is the design of a plot that displays the pathways of all hydrologic realizations e g 1000 realizations for a policy while clearly showing the times at which each infrastructure option is built there are in the literature examples of visualizations of infrastructure pathways the first and most well known is the metro maps haasnoot et al 2013 zandvoort et al 2017 the metro maps have the advantages of being easy to interpret and to present to audiences however they allow for the representation of only a handful of pathways before the pathways become difficult to distinguish another option is the probabilistic pathways zeff et al 2016 which was derived from the concept of metro maps haasnoot et al 2013 the probabilistic pathways visualization makes use of transparency to indicate the timing and frequency that each candidate infrastructure option is implemented over an ensemble hydro climatic scenarios however distinguishing the pathways differences in each of the ensemble realizations is challenging the pathways visualization proposed in this work displays the pathways explicit temporal evolution for major investments for all hydrological realizations as a stacked ensemble plot the resulting plots provide a visual indication of what new water supply infrastructure would be needed as function of key hydrological or other exogenous scenario gradients this can help planners provide resources for near future construction and for permitting detailed design public consultations etc 4 computational experiment the computational experiment was divided into two phases optimization and re evaluation given the runtime of the research triangle model we used the texas advanced computing center s stampede 2 for both optimization and re evaluation phases the optimization phase was comprised of two mm borgmoea seeds trials for the wcu formulation and two random seed trials for the du optimization each optimization trial was run on 256 mpi processes 1 controller 4 master and 251 workers with 24 threads each on 128 2 x intel xeon platinum 8160 skylake nodes 48 cores per node with 1000 realizations per function evaluation see schematic or realization sampling for each type of search in fig 5 a total of 500 000 function evaluations were used for each trial 125 000 per master our runs were evaluated for their search solution quality by confirming that further search had minimal hypervolume benefits see fig s6 the resulting solutions were each re evaluated on nodes of the same type over 1000 sets of hydrological demand series against 2000 sows resulting in 2000 re evaluation function evaluations for each re evaluated solution see fig 6 but instead of 3 sows as in the figure 2000 were used the ranges of values considered for the decision variables are presented in tables 4 6 and the ϵ values i e significant precision for all objectives are presented in table 7 the mm borg moea was parameterized following prior published recommendations hadka and reed 2014 5 results 5 1 comparing tradeoffs after re evaluation our results compare the impacts of changing the sow sampling strategies fig 7 a across the wcu and du optimization formulations the two core advantages of carefully tracking the impacts of including more uncertainties in the multiobjective search are 1 an ability to distinguish how the tradeoffs for the research triangle infrastructure investment and management pathways change when stressed with more diverse scenarios for the future and 2 to confirm if switching from the more traditional wcu to the approximate sow sampling used in the du optimization significantly impacts the robustness attained by the individual utilities as expected the du optimization s approximate sampling avoids the full computational cost of the comprehensive du re evaluation fig 6 during search for a fair comparison the resulting pareto approximate infrastructure investment and management policies found with the wcu and du optimization schemes were each fully re evaluated to assess their expected tradeoffs and robustness using the more comprehensive suite of sows captured by the du re evaluation ensemble fig 7 a shows the post re evaluation objective tradeoffs panel a and the utilities relative robustness tradeoffs panel b for all policies obtained through the du and wcu optimization formulations in fig 7 a each axis represents an objective while each line represents a regional infrastructure investment and management policy the points where each line policy intersects the vertical axes represents the performance value of the metric corresponding to that axis in fig 7 a the parallel axes plot is oriented such that the ideal policy would be a horizontal line at the bottom of all axes figs 7 shows that the policies from du optimization were on average cheaper to operate and less infrastructure intensive than their wcu optimization counterparts despite the comparable restriction frequency the attained values of the rof triggers for the policies discovered using the du optimization better balance the reliability benefits and financial impacts of the short term drought mitigation financial instruments and infrastructure investments while avoiding stranded infrastructure assets fig 7 a shows that search under deep uncertainty served to shift policies towards higher levels of performance the more diverse sows in the du optimization appear to sufficiently stress the management and investment policies to force them to more effectively use the full suite of water supply portfolio options at the utilities disposal the crossing diagonal lines between the two leftmost axes of fig 7 a highlights strong tradeoffs between reliability and restriction frequency across the wcu and du optimization formulations meaning that one can only improve at a cost to the other likewise significant tradeoffs exist between reliability and total financial cost meaning that if a utility wants high reliability it has to pay for it through a contingency fund and or drought insurance lastly fig 7a highlights a tension between average annual cost and annual worst first percentile cost which is important as the utilities often struggle when confronted with significant variability in their annual costs and are often willing to pay a premium for more predictable costs transitioning to robustness tradeoffs fig 7 b uses a similar parallel axes plot to display the percentages of du re evaluation sows where each utility meets their goal performance requirements reliability 99 restriction frequency 20 and annual worst first percentile cost 10 in terms of ideal performance for robustness in fig 7 b the direction of preference is upward where the ideal policy would be represented to a line horizontally intersecting each of the utilities robustness axes at their top maximum values i e 100 the crossing diagonal lines in fig 7 b again designate that strong robustness tradeoffs exist between raleigh cary and durham in contrast owasa attains close to maximum robustness for almost all policies the specific policies that represent the most robust policies for each utility are highlighted in different colors in fig 7 b fig 8 supplements fig 7 b by more directly comparing the utilities satisficing robustness for each of the policies found by the wcu and du optimization formulations to clarify our analysis we designate the most robust policies for durham and raleigh respectively as buyer preferred policies d b1 and r b2 given their dependence on buying water transfers from cary the most robust policy for cary is designated the supplier preferred policy or c s in figs 7 and 8 fig 7 a shows that the most robust policy for durham d b1 was the only policy whose du re evaluation objectives met the performance criteria established by the utilities reliability 99 restriction frequency 20 and annual worst first percentile cost 10 even though wcu optimization found 246 pareto approximate policies versus 112 from du optimization only those policies attained by including deeply uncertain factors in search attained both high levels of objective performance and robustness against deeply uncertain scenarios du re evaluation also the most robust management and investment policies for each individual utility in the research triangle were obtained through du optimization overall the predominance of the du optimization s solutions designated by the blue lines located closer to ideal performance at the bottom of the axes in fig 7 a and b show that the policies from du optimization were on average more reliable less infrastructure intensive financially cheaper and less risky despite requiring a slightly higher restriction frequency than their wcu optimization counterparts when both sets of policies are exposed to broader uncertainty the policies resulting from the du optimization formulation have combinations of rof triggers that allow the system to respond faster and more cost efficiently to drought events avoiding costly stranded infrastructure and high capital debt levels while maintaining reliability even if the reliability performance requirement is relaxed to 97 only 4 policies from wcu optimization would meet the criteria versus 34 from du optimization 5 2 scenario discovery for compromise policies building on a better understanding of the performance tradeoffs and robustness conflicts as illustrated in figs 7 and 8 it is also important to understand what deeply uncertain factors most strongly influence robustness fig 9 shows scenario discovery maps for durham cary and raleigh each map shows the performance attained across sampled scenarios for the two most important uncertainties for each utility when the research triangle implements one of the two buyer preferred policies highlighted in figs 7 b and 8 in each panel of fig 9 the two deep uncertain factors plotted on the horizontal and vertical axes are the factors identified by the boosted trees algorithm to be the most dominant in shaping robustness performance for a given policy such dominance is quantified by the percentages in parentheses next to each factor label which indicate the decrease in impurity of the tree ensemble from splits on that factor as summarized in section 3 3 significant changes in the percentage of impurities from tree based splits of a factor represents the direct reduction of classification errors in terms of which sampled sows attain success or failure high percentage changes in impurities consequently can be interpreted as a measure of utilities performance sensitivity to a given factor table s4 in the supplement section s4 reports the impurity decreases or sensitivities for all of the sampled factors for durham cary and raleigh under both buyer preferred policies d b1 and r b2 red regions in the factor maps of fig 9 represent regions in which a utility will likely not meet its performance targets grey regions represent high performance scenarios and white regions are deemed inconclusive due to close proximity mixtures of failures and successes in a given region of the scenario space in each of the six panels of fig 9 the stars represent the most favorable scenario grey star the least favorable red and a projected future where demand growth rates in the research triangle region are 80 of the nominal values assumed in the wcu optimization values blue star fig 9 a c show the scenario discovery maps obtained for durham cary and raleigh respectively under durham s preferred policy d b1 fig 9 a shows that durham s failures are mostly contingent on demand growth as shown by the nearly vertical boundary between the successful and failed sows beyond 150 the same figure also shows that the d b1 management and investment policy effectively hedges durham s robustness against demand growth requiring a demand growth rate more than 50 over the nominal rate assumed by durham before failing to meet the utility s performance requirements however if demand growth rates are 50 to 70 above the projected values durham may still meet the performance criteria depending on how many years the utilities take to get the permits for the low capacity joint wtp fig 9 b shows that cary s failures under policy d b1 are contingent on extreme demand growth rates exceeding their nominal projections by 85 cary s failures are due to a hard treatment capacity boundary where demand would surpass the maximum upgraded treatment capacity for cary s own water treatment plant on the jordan lake fig 9 c on the other hand shows that raleigh has a far more complex failure dynamic fig 9 c shows that the most dominant factor influencing raleigh robustness is demand growth though raleigh s performance does not decrease monotonically as demand growth increases rather raleigh displays poor performance if demand growth rates are between 50 to 80 higher than the nominal projected values as indicated by the prevalence of red failed sows the reason for the stratification of failure scenarios in fig 9 c is because scenarios in the red vertical strips emerge due to poor coordination between infrastructure construction and the effective use of short term drought mitigation instruments fig 9 c shows that in low demand growth scenarios raleigh builds storage infrastructure slowly to avoid costly stranded assets while supply risks are managed effectively by short term mitigation instruments on demand growth scenarios greater than raleigh s nominal projections raleigh fails to meet the performance goals red dots when new storage infrastructure is not built fast enough to keep up with demand growth rate straining the short term drought mitigation instruments in fig 9 c the complex non convex fingering of failure or inconclusive scenarios result when the d b1 investment long term rof rules do not trigger sufficiently rapid capacity expansion and the short term rofs are not capable of mitigating the demand growth rates i e cases of slow recognition of growing risks the complex non linear fingering of the failures captured in several panels of fig 9 are an interesting result that provides a general insight as this behavior could occur for any kind of decision making metric based on thresholds such as the rof or days of supply remaining geometrically this allows a utility to more explicitly recognize the scenarios their investment and management rules do not perform as wished transitioning to the raleigh preferred buyer preferred solution r b2 in fig 9 d f the demand growth rate remains a dominant factor shaping the robustness performance for all of the utilities however fig 9 d shows that the effectiveness of durham s water use restrictions emerges as the second most important factor influencing durham s performance switching to the r b2 investment and management policy yields an appreciable increase in the number and complexity of failure scenarios for durham the underlying mechanisms that yield the complex fingering of failure scenarios in fig 9 d for durham are similar to those for raleigh under the d b1 policy in fig 9 d poor coordination between infrastructure construction and the short term use of drought mitigation instruments for the r b2 policy durham has far less hedging to robustness failures and is specifically more sensitive to the effectiveness of their populations response to restrictions transitioning to cary fig 9 e shows that cary s failure region is far larger under policy r b2 than under d b1 interestingly evaporation emerges as an important uncertain factor for cary instead of for raleigh despite falls lake raleigh s main supply source being rather shallow the emergence of evaporation as key concern for cary would imply that longer term warming from climate change could pose a severe challenge to the r b2 policy to our knowledge these results are the first to show the complex emergence of vastly different failure modes and asymmetric sensitivities that can emerge for multi utility regionalization of investment and management pathways lastly fig 9 f shows that the r b2 policy strongly hedges raleigh while degrading the robustness of both cary and durham as shown for the d b1 policy regional demand growth rate and the response of raleigh s customers to restrictions dominate its robustness performance fig 9 f again shows a complex fingering of failure scenarios as has been noted above where slow recognition of the need to expand capacity when demand growth rates exceed nominal projections by approximately 40 moderate losses in the effectiveness of restrictions serves to compound raleigh s robustness failures the consistent importance of raleigh s water use restriction effectiveness across both the d b1 and r b2 policies stresses the importance of raleigh s readiness to effectively implement water use restrictions and make sure their population will be responsive overall fig 9 provides some important regional insights for the research triangle the dominant impact of demand growth rates emphasizes that supply reliability is harder to achieve than financial goals for this system overall keeping demand growth rates at or below the utilities projected value would be an important step for all of the utilities to ensure satisfactory regional performance furthermore in most cases the reductions by 20 of the projected growth suggested by the utilities themselves would significantly improve satisfactory performance while also adding a safety margin that decreases the region s need for new infrastructure however negative impacts may not be felt if demand growth rates are close to 200 rather than around 150 fig 9 c and f but may be noticeable for rates of 50 of the projected value fig 9 d which emphasizes the complex nonlinear relations between short term instruments and long term investments available to the utilities despite such non linearities lowering demand growth rates is a clear mechanism for enhancing the robustness for all of the utilities and reducing regional performance conflicts fig 9 also contributes some general insights regarding the value of using boosted trees to guide the scenario discovery process for complex infrastructure pathway problems boosted trees succeeded in capturing complex and distinctive features of the factor mapping plots without inherent overfitting most non linearities and fingering within complex transition zones between successful and failed scenarios were identified by the boosted trees approach in addition the identification of inconclusive regions between passing and failing scenario regions are helpful for avoiding errors in complex scenario discovery contexts 5 3 policy rules in the robustness compromises fig 10 provides a more detailed illustration of the underlying decisions or rules that compose the durham d b1 and raleigh r b2 preferred policies recall that the investment and management policies are comprised of 1 rof triggers for short term mitigation instruments fig 10 a b and d and for long term infrastructure investments fig 10 e 2 allocations to the water supply pool in jordan lake allocation fig 10 c 3 annual contingency fund contributions and insurance payouts defined as percentages of annual revenue fig 10 f and g and 4 infrastructure construction order fig 10 h and i in both the d b1 and the r b2 policies there is significant similarity between durham and raleigh in their high reliance on restrictions fig 10 a on their prompt triggering of new infrastructure by utilizing low long term rof triggers fig 10 d and in their moderate to high use of contingency funds fig 10 f fig 10 a shows that durham makes consistently high use of transfers under durham s preferred policy d b1 by setting the transfer trigger value low this in turn requires a high annual contingency fund contribution as highlighted in fig 10 f fig 10 g and e show that durham uses small insurance payouts in both policies at moderate and high rofs which emphasizes the preferred strategy of relying dominantly on a contingency fund in comparing raleigh versus durham s preferred policies fig 10 b highlights competition for transfers this partly explains the robustness conflicts between durham and raleigh given their shared use of limited conveyance capacity for transfers from cary in durham s preferred robustness compromise policy d b1 durham consistently uses the full capacity of the intra utility transfer pipes from 2015 until 2027 alternatively in raleigh s preferred solution r b2 durham s limited use of transfers allowed raleigh to request medium to high volumes of transfers whenever needed see supplementary figs s1 to s4 these differences are reflected in each utility s approach to their finances because although transfers are an effective way of decreasing the need for restrictions they can be costly under policy r b2 raleigh opts for moderate payouts at moderate rofs to hedge against the higher use of transfers because of the lower associated rof trigger fig 10 b in fact raleigh s sparse use of transfers under policy r b2 warrant the use of drought insurance in contrast to policy d b1 most insurance payouts for raleigh under r b2 happen during periods of high use of transfers see supplementary figs s3 and s4 raleigh s use of financial insurance in the r b2 policy serves to reduce the political risks and opportunity costs associated with a large and rarely used contingency fund 5 4 infrastructure pathway ensemble the durham preferred policy d b1 and the raleigh preferred policy r b2 capture alternative strategies for navigating the performance and robustness conflicts highlighted in figs 7 and 8 and each lead to distinctly different future pathways for the research triangle fig 11 contributes a detailed analysis of the capacity expansions and rof dynamics for durham and raleigh that emerge from the d b1 policy the plots in fig 11 focus on the resultant capacity expansion pathways and rof time series corresponding to the three highlighted scenarios from fig 9 the most favorable sow left column the sow with 80 of the nominal demand growth rate middle column and the least favorable sow right column for durham and raleigh fig 11 a c plot durham s storage capacity and short term rof dynamics for each of the three demand growth rate scenarios likewise fig 11 d f plot durham s resulting ensemble of infrastructure pathways across the demand growth scenarios fig 11 d f show the infrastructure pathways for the same scenarios the horizontal axes of each pathway panel represents time from 2015 to 2060 encompassing planning horizon of 45 years all of our illustrated infrastructure pathway plots display stacks of 1000 horizontal multicolored lines each line corresponding to the investment sequences for one hydrological realization for each realization line the colors represent the infrastructure project that was built last in time for example the dark green horizontal line segment close to the center of the plot indicates that in that realization indicates that durham builds the teer quarry typically between 2040 to 2050 an important concern captured in this study relates to the importance of permitting times which vary significantly across different infrastructure investments for the pathways illustrated in fig 11 horizontal line segments sometimes show different color sequences across realizations even though the priority ordering for a given infrastructure policy is fixed as highlighted in the figure the realized infrastructure construction order for a policy may not match the order in which infrastructure is actually built over the course of a simulation due to permitting time delays this issue frequently emerged when considering investments in the joint wtp for durham under all three scenarios fig 11 d f and for raleigh under raleigh s least favorable scenario for policy d b1 fig 11 l the infrastructure construction order for policy d b1 in fig 10 h shows that the low capacity version of the joint wtp was ranked first and second in the infrastructure construction queues of raleigh and durham under policy d b1 however the colors between the salmon status quo and light or medium blue low and high capacity joint wtp in fig 11 d through fig 11 f and l show that the joint wtp was not the first option built by either utility under the considered demand growth rate and hydrologic scenarios the reason for the apparent incongruence is that the permitting and planning period of the joint wtp is sufficiently long as to allow other options namely teer quarry reservoir low capacity lake michie expansion falls lake reallocation and in a few cases richland creek quarry and reclaimed water to be built first by durham and raleigh a more detailed analysis of the pathways resulting from durham s preferred policy d b1 shows that the joint wtp expansion and co investment is important and dominantly triggered by durham figs 11 and s1 shows for even the least favorable sow that durham does not need to invest in any other projects in the queue after either version of the joint wtp is built the water treatment plant investment successfully reduces durham s long term rof to 0 after its construction see supplementary figs s1 and s2 transitioning to raleigh despite having to still build one more project in the least favorable sow fig 5l the short term rof in fig 11 i noticeably decreases with the construction of the joint wtp this reduces the pressure on raleigh s short term drought mitigation instruments despite the joint wtp being a fairly expensive project when compared to other infrastructure alternatives its shared cost makes it effective reliability and financially wise for both durham and raleigh because under both policies both utilities have a substantial allocation at jordan lake see fig 10 c aside from the joint wtp most of the focus of durham and raleigh under policy d b1 is on storage infrastructure this conclusion follows from the high prevalence of storage infrastructure seen in all of the pathways durham fig 11 d f and raleigh fig 11 j l the investments in storage come at the expense of investments in reclaimed water and intakes which are comparatively cheaper but not as effective at mitigating supply shortfalls the only appearance of reclaimed water infrastructure in the pathways occurred for durham under their least favorable sow yellow rectangle in fig 11 f largely as a consequence of construction times for the other water sources lastly it is also interesting to observe that even in their least favorable sow fig 11 l raleigh goes through substantial storage capacity growth without building a single reservoir or reservoir expansion instead increases in their supply capacity result from direct access to its jordan lake water supply pool via the joint wtp and through the falls lake s municipal pool relocation which are cheaper and less disruptive projects than storage infrastructure transitioning to raleigh s preferred buyer policy r b2 fig 12 shows the rof dynamics and infrastructure pathways for durham and raleigh under the same three demand growth rate scenarios displayed in fig 11 although figs 11 and 12 show many similarities in the resulting infrastructure investment pathways key differences that favor raleigh do emerge for the r b2 policy beyond the utilities competition for transfer water discussed above under policy d b1 as illustrated in fig 11 it is always durham who triggers either version of the joint wtp however for the policy r b2 under raleigh s least favorable sow the low capacity version of the joint wtp is triggered 60 of the time by raleigh this typically happens when raleigh finishes implementing the falls lake reallocation and durham has the low capacity version of the lake michie expansion under construction the joint wtp remains as an effective investment to reduce the rofs of both durham and raleigh under the r b2 policy differences in the d b1 and r b2 policies are most pronounced for high demand growth scenarios recall from our scenario discovery results in fig 9 that short term use and competition for water transfers yields strong differences in durham s and raleigh s robustness and failure mechanisms this results despite the similarity of the investment pathways across both policies shown in figs 11 and 12 this further stresses that sound infrastructure investment strategies must be complemented by a sound short term drought mitigation instruments for performance levels to be maintained under extraneous scenarios more specifically effective use of restrictions and transfers strategies coupled with financial instruments to hedge utilities against droughts serve to reduce and delay for years the need for building new infrastructure projects still despite a loss in performance under policy r b2 a combination of an earlier falls lake reallocation and effective use of transfers before the construction of the second infrastructure option resulted in less newly built infrastructure i e the neuse river intake was not built and the final storage capacity for the system was about 3000 mg smaller despite the mentioned differences in the pathways between both solutions shown in figs 11 and 12 their similarities similar timings and options built imply general insights for the research triangle region short term drought mitigation instruments are critical to the evolution of the system s infrastructure investment pathways regionally coordinated strategies for managing demand growth rates are an important risk hedge for all four of the research triangle utilities overall an important take away point from the pathways plots for raleigh and durham is that the teer quarry and the low capacity joint wtp projects are very likely to be needed in the foreseeable future therefore durham raleigh and cary should include the joint wtp in their budgeting process perform more detailed design analysis and contemplate their permitting requirements the main recommendation for raleigh would be to initiate an application for a reallocation of the falls lake s water quality pool to its own supply pool more broadly du optimization significantly reshaped system objectives tradeoffs robustness scenario discovery results portfolio mix of decisions and the corresponding pathways the analyses framework as illustrated in our results provides a rich context for an understanding of the system table 8 6 conclusion this work contributes the du pathways framework which includes deep uncertainty in the search phase of the infrastructure pathways problem in the context of water resources systems this work demonstrates the value of integrated regional infrastructure planning and management coupled with deep uncertainty analysis for mitigating drought risks and their corresponding financial impacts such risks are even more prevalent in areas of high population growth such as the research triangle region in north carolina the research triangle regional system demonstrates how performance and robustness conflicts can emerge between geographically close utilities in their long planning the du pathways framework s flexibility in incorporating and exploring uncertainties as demonstrated in this study highlights some general insights and benefits for regional water utilities seeking to coordinate their short term management actions weekly and long term annual investment decisions a key benefit is the ability to create more flexibility and reduce required infrastructure debt burden by carefully balancing short term decision making and long term infrastructure investment decisions the consistent integration of both management and investment decisions using rof based rules provides operators with ways to account for information beyond measurements of reservoir levels and past data allowing for decisions based on possible future conditions as well another major benefit is the possibility of performing uncertainty analysis on the effectiveness of alternative infrastructure investment pathways for individual utilities as well as broader cooperating regional coalitions this analysis not only informs utilities about what uncertainties to monitor when planning the early studies preceding the triggering of new infrastructure but also about what investments would be more effective as the results from monitoring emerge lastly the du pathways framework allowed for a clear understanding of strong interdependencies between short term drought mitigation and financial instruments as well as their potential to determine the proper timing of investments in new infrastructure the du pathways framework centralizes and promotes all these benefits for single and regional water utility infrastructure planning and management the inclusion of deep uncertainties in the search based identification water supply management and investment portfolios yielded pathway policies that are more robust according to performance criteria defined by the utilities themselves and that also provide better performance and robustness compromises between the four utilities in the research triangle region the scenario discovery analysis showed that the key driver of system performance and robustness is demand growth which utilities can act on through coordinated demand management the infrastructure pathways analysis also showed that demand growth may determine whether specific investments in infrastructure will be needed by a given point in time should the right conditions be observed how key uncertainties may drive the need for investments to be sooner or later or if some candidate projects would never be prioritized at all even under the most adverse futures this information is crucial for utilities to design their rate structure to accommodate for debt servicing for the maintenance of their credit rating when issuing the required bonds future work could consider a wider variety of drought mitigation instruments new strategies for sampling and potentially screening deep uncertainties and short term drought mitigation policies that adapt their triggers with the construction of new infrastructure a meta policy so to speak the insights of this work have broad merit for water utilities facing growing pressures to better coordinate management and investment decisions to confront resource contention growing urban demands and increasingly extreme droughts credit authorship contribution statement b c trindade conceptualization data curation writing original draft writing review editing p m reed conceptualization data curation writing original draft writing review editing g w characklis conceptualization data curation writing original draft writing review editing declaration of competing interest none acknowledgements funding for this work was provided by the national institute of food and agriculture u s department of agriculture wsc agreement no 2014 67003 22076 additional support was provided by the u s national science foundation s water sustainability and climate program award no 1360442 the views expressed in this work represent those of the authors and do not necessarily reflect the views or policies of the nsf or the usda we would also like to thank the editorial team and three anonymous reviewers for helping us to enhance the clarity and contribution of this study supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103442 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
563,recently there have been an increasing number of studies dealing with change detection in multivariate series however a major drawback with most of the currently used methods is the lack of flexibility indeed these methods are only able to detect changes in the strength of dependence assuming invariant shape of the dependence structure however under a changing climate the shape of dependence might change as well furthermore in the multivariate setting heterogeneities can occur in the margins and or in the dependence structure the most existing approaches for multivariate change detection deal with the whole distribution in this paper we propose a novel statistical test for multivariate heterogeneity detection based on copula and multivariate l moments a simulation study is conducted to evaluate the performance of the proposed test and to compare it with those of existing tests results indicate that the proposed test has an interesting power especially when the dependence strength remains invariant with power ranging between 45 and 93 whereas for the existing tests the power is lower than 14 in this case an application to a real data set is also provided results show the ability of the proposed test to discriminate homogeneous and inhomogeneous series keywords multivariate homogeneity testing stationarity multivariate l moments copula flood 1 introduction extreme events are of high importance in design of hydraulic structures water resources management and flood insurance consequently their estimation should be as reliable as possible for this purpose hydrological frequency analysis hfa is the most used statistical tool to model behaviour of hydro meteorological variables e g hamed and rao 1999 these events are characterized by a number of correlated variables e g chebana 2013 salvadori and de michele 2004 for example floods can be described by the peak volume and duration e g requena et al 2013b shiau et al 2006 therefore a multivariate framework that takes into account the dependence between different variables is of fundamental importance e g requena et al 2013b zhang and singh 2012 copulas have been proposed as a key tool to model the dependencies between hydrological variables in the context of multivariate hfa e g chebana 2013 genest and chebana 2016 salvadori and de michele 2004 salvadori et al 2007 hfa is composed of four main steps as summarized in table 1 e g chebana et al 2010 checking basic assumptions homogeneity stationarity and serial independence is an important step since it has a significant impact on the other subsequent steps ignoring this step could lead to catastrophic consequences in case of an underestimation of these events and waste of valuable economic resources in case of their overestimation e g bender et al 2014 in hydrological multivariate setting checking for trend freeness is recently treated by chebana et al 2013 while homogeneity testing has attracted less attention despite its importance moreover departures from this assumption may provide misleading results furthermore if unaccounted for these inhomogeneities could have a large impact on the outcome of the estimation and fitting of the probability distribution e g ouarda et al 2014 seidou and ouarda 2007 data are considered as homogenous if they have the same underlying distribution with invariant statistical characteristics e g gilroy and mccuen 2012 lund et al 2007 reeves et al 2007 hence it means that data do not exhibit a mix of several samples from different populations it is necessary to note that this definition does not involve time consequently as pointed out by hamed and rao 1999 the chronological order is not important for a sample to be homogenous the lack of homogeneity may be caused by natural or anthropogenic actions on physical environment such as deforestation dam construction and urbanization among others e g burn and elnur 2002 furthermore annual flood series might be produced by more than one hydrometeorological process such as flood producing storms rainfall and snowmelt floods e g ouarda and el adlouni 2011 smith et al 2011 moreover the regime of a river downstream from the confluence of two sub watersheds with very different hydrological behaviors is a good example of the lack of homogeneity in the literature the most common way for testing homogeneity in both univariate and multivariate context is to detect abrupt change point e g das and umamahesh 2017 he et al 2016 quessy 2019 ribes et al 2017 sadegh et al 2015 in fact homogeneity and change point are two related problems but the latter is more challenging as illustrated by fig 1 when dealing with homogeneity problem one has two or more pre specified sub groups of data to be tested without necessary a sequential aspect of the data however in the case of a change point problem the appropriate groupings of the data are unknown and data has to be segmented in a sequential manner into sub groups that will be tested for homogeneity homogeneity problem can be considered as a change point testing problem by using preliminary step such as classifying or clustering data e g lung yut fong et al 2015 hence in this paper homogeneity problem is treated as a change point detection cpd problem to deal with the cpd problem formal parametric and nonparametric testing approaches have been proposed in the literature in this study only nonparametric approaches are considered since they do not require prior choice of marginal and joint distributions which makes them more appropriate in hfa e g vittal et al 2015 for general comparison between parametric and nonparametric frameworks the reader is referred for instance to santhosh and srinivas 2013 and oja and randles 2004 the hydro meteorological literature abounds with studies dealing with homogeneity testing of univariate distributions e g beaulieu et al 2009 ehsanzadeh et al 2011 reeves et al 2007 seidou and ouarda 2007 however the multivariate setting has been scarcely addressed e g chebana et al 2017 homogeneity of multivariate hydrological series can affect marginal distributions as well as the dependence structure e g guerfi et al 2015 holmes et al 2013 xiong et al 2015 therefore it is of high importance to distinguish between these two types of homogeneities in order to gain deep physical insights behind the causes of the eventual changes in the data e g mazouz et al 2012 a general class of tests based on multivariate statistical depth has been introduced and evaluated for the hydrological context in chebana et al 2017 these tests allow detecting an overall shift in the series however they do not allow distinguishing if the shift is in margins or in the dependence structure to deal with this problem a few multivariate tests have been recently proposed to detect inhomogeneities in the dependence structure e g holmes et al 2013 quessy et al 2013 xiong et al 2015 nonetheless as summarized in table 2 these tests entail some drawbacks for instance some tests are designed to detect changes only in the parameter of the copula and not able to detect changes in the shape of dependence such as the test proposed by bouzebda and keziou 2013 this is an important limitation since under lack of homogeneity the copula itself could change moreover some of these tests have optimal performance under normality this is a major drawback since hydrological series are generally non normally distributed to avoid these limitations another class of tests based on kendall and empirical copula process are proposed e g kojadinovic et al 2016 quessy et al 2013 unfortunately these tests are not suitable for small sized samples indeed these tests allow detecting change on long data length for n 100 which is not usually available in practice in addition a change in the copula may occur even if kendall process remains invariant such as the case of extreme value copulas e g genest and favre 2007 vogel and fried 2015 moreover changes captured by these tests usually tend to appear in the middle of series whereas heterogeneities may also occur at the beginning or at the end of the records in order to avoid the aforementioned drawbacks the objective of the present paper is to develop a new test for homogeneity of the dependence structure the developed test is based on multivariate l moments given their attractive desirable properties for hydrological applications indeed they provide a summary and a description of the properties and shapes of a multivariate distribution this makes them particularly useful in parameter estimation and hypothesis testing furthermore since multivariate also univariate l moments are much less biased than classical moments they are used as meaningful replacements of classical moments in a wide variety of applications mainly in hydrology climatology and meteorology analysis e g chebana and ouarda 2007 kysely and picek 2007 a performance evaluation of the proposed test based on monte carlo simulations is presented under hydrological constraints a case study is also performed to illustrate an application of the developed test on hydrological data aside from the introduction the remainder of the paper is organized as follows a brief review of definitions and some notions related to the developed test is presented in section 2 section 3 presents the development of the proposed test for homogeneity testing the simulation study to evaluate the performance of the test is presented in section 4 section 5 illustrates an application of the developed test on hydrological data the conclusion of the study and a number of perspectives are reported in section 6 2 theoretical background in this section we tackle the statistical tools used in the current paper a brief description of the statistical background of homogeneity testing problem is provided herein 2 1 multivariate homogeneity testing problem as mentioned earlier homogeneity is a fundamental hypothesis in hfa since its absence may affect the statistical inference to be undertaken homogeneity of a sample postulates that all data are generated by the same physical process e g hamed and rao 1999 peterson et al 1998 this implies therefore checking that all data have an invariant probability distribution e g ehsanzadeh et al 2011 ribes et al 2017 shift detection on location and or dispersion parameters is the commonly used form of testing the homogeneity of the hydrological series e g hamed and rao 1999 kundzewicz and robson 2004 li and liu 2004 seidou et al 2007 as pointed out by rougé et al 2013 and nayak and villarini 2016 this formulation of the homogeneity problem is based on the sequential time of the event however heterogeneity can be present without importance of the sequential aspect as a consequence throughout this paper we focus on the homogeneity in form of distributional change in statistical terms this means that not only some characteristics of the distribution change abruptly but also the distribution itself changes e g milly et al 2015 serinaldi et al 2018 in the multivariate hfa as pointed out by vezzoli et al 2017 heterogeneities may occur in marginal distributions dependence structure or both over the past few decades a large number of techniques have been developed to detect heterogeneities in marginal distribution however as far as we know the homogeneity of the dependence structure copula type has been scarcely addressed e g guerfi et al 2015 xiong et al 2015 since hydrological time series increasingly exhibit non stationarity due to natural and anthropogenic changes this issue is worthy of attention in hydrologic design to define the homogeneity testing problem the null hypothesis h0 is that when arbitrarily splitting the sample in two subsamples there is no change in the distribution function of each subsample statistically let xi i 1 n be a d variate random variables of size n with marginal distributions f j j 1 d and multivariate joint cumulative distribution function g we denote x i x i 1 x i d the observation from xi under h0 it is assumed that all observations are homogeneous and thus have the same distribution g the alternative hypothesis h1 assumes g to exhibit inhomogeneity i e g is time varying and is no longer constant in agreement with sklar 1959 s theorem there exists a copula c such that for all xi we have 1 g x 1 x d c f 1 x 1 f d x d regarding the links between copulas and multivariate joint distributions the null hypothesis h0 can be expressed formally as h 0 h 0 m h 0 c against the alternative h 1 h 1 m uh 1 c h0 m and h0 c respectively h1 m and h1 c are the null respectively the alternative hypothesis about the homogeneity of marginal distributions and copula homogeneity testing of marginal distributions and copula are expressed respectively as h 0 m f 1 f d such that x 1 x 1 x n have m c d f s f 1 f d h 1 m at least an index s and f 1 1 f 1 d such that x 1 x s have m c d f s f 1 1 f 1 d f 2 1 f 2 d such that x s 1 x n have m c d f s f 2 1 f 2 d h 0 c c such that x 1 x n have copula c h 1 c at least an index s and c 1 such that x 1 x s have copula c 1 c 2 such that x s 1 x n have copula c 2 as mentioned above this paper focuses on the homogeneity of the copula structure therefore the proposed statistic attempts to test h 0 c against h 1 c in the following for the sake of convenience and brevity only the bivariate case is considered 2 2 multivariate copula l moments the multivariate l moment was introduced by serfling and xiao 2007 in both parametric and nonparametric schemes multivariate l moments provide a summary and a description of the properties and shapes of a distribution this makes them particularly useful in parameter estimation and hypothesis testing since multivariate also univariate l moments are much less biased than classical moments they are used as meaningful replacements of classical moments in a wide variety of applications especially for hydrological frequency analysis e g ben nasr and chebana 2019 chebana and ouarda 2007 hosking and wallis 1993 the multivariate l moments are defined as bellow serfling and xiao 2007 2 λ k i j cov x i p k 1 f j x j where cov is the covariance k is the order of the multivariate l moment p k 1 is the shifted legendre polynomial chang and wang 1983 x i is a random variable with marginal distribution f i for i 1 2 brahimi et al 2015 introduced the copula s multivariate l moments as 3 λ k c 12 0 1 0 1 c u 1 u 2 u 1 u 2 d u 1 d p k u 2 the sample version of the k th copula multivariate l moments is defined in term of pseudo observations ui vi as brahimi et al 2015 4 λ k c 12 1 n i 1 n u i p k 1 v i where pseudo observations u i r i n 1 v i s i n 1 and ri is the rank of among si is the rank of the concomitant x i n 12 among x i 2 x n 2 to obtain x 12 we first sort x 2 in the ascending order as x 1 n 2 x 2 n 2 x n n 2 then the concomitant x i n 12 corresponds to x i 1 paired with x i n 2 serfling and xiao 2007 the copula multivariate l moments coefficient ratios are defined as 5 τ k 12 c λ k 12 c λ 2 2 for k 3 and τ 2 12 c λ 2 12 c λ 1 2 3 multivariate l moments based copula homogeneity test recall that the null hypothesis h 0 of no change in the dependence structure of the d variate variables x i i 1 n postulates that the copula c is invariant against the alternative h1 that there is ksuch that x 1 xk have a copula c 1and x k 1 xn have copula c 2 if h 0 is rejected k is the so called change point in dependence structure the proposed test statistic is then given by 6 t n max 1 k n λ k λ n k where λ k and λ n k are matrices whose elements are multivariate l moments ratios of order 2 3 and 4 of the series before and after the point k respectively is the euclidean norm the dimension of the matrices λ k and λ n k is 2 6 each matrix of multivariate l moments coefficients λ is written as τ 2 c τ 3 c τ 4 c w h e r e τ i c τ i 11 c τ i 21 c τ i 12 c τ i 22 c i 2 3 4 the null hypothesis h 0 should be rejected for a large value of the test statistic tn the change point is therefore estimated by 7 k argmax 1 k n λ k λ n k the asymptotic distribution of the proposed statistic is out of the scope of the present paper and could be the object of future work in addition asymptotic results could not be appropriate for hydrological application given the short sample sizes usually encountered hence the bootstrap procedure is used to estimate p values of the proposed test good 2004 holmes et al 2013 see fig 2 the idea of the proposed test is inspired by the tests proposed by quessy et al 2013 and kojadinovic et al 2016 however it allows avoiding some drawbacks of the traditional tests in fact there are three key features that distinguish our test from the existing ones given in table 2 first the tests by quessy et al 2013 and kojadinovic et al 2016 are designed to detect only the change in the dependence strength kendall or spearman coefficients and hence assuming the same copula shape before and after the change point in contrast the proposed test allows for detecting change on the shape of copula even if the kendall τ or spearman ρ remains invariant second these two tests are not well adapted to particularities of hydrological series where they have low power for small sample sizes the proposed test is based on multivariate l moments which are accurate for small sample size usually encountered in hfa third unlike the likelihood ratios test bouzebda and keziou 2013 the proposed test is nonparametric which is robust against distribution miss specification and respects the fact that the test should be performed prior the modeling step in hfa semi parametric tests should perform well under the correct model specifications but inference based on miss specified models is not well studied the heterogeneity of a data series may be due to differences in any feature of the distribution in particular in the hydrological literature the l covariation matrixλ2 represents a measure of the dispersion of the distribution e g chebana and ouarda 2007 besides the l coskewness λ3 is an important descriptor of the copula structure in particular λ3 measures the difference between the upper and lower tails and hence measures the asymmetric shape of the copula e g müller et al 2017 likewise λ4 gives an idea about the kurtosis of multivariate distribution by measuring the difference between the typical spread in the tails and the typical spread in the center of the multivariate distribution consequently taken together the l covariation l coskewness and l cokurtosis matrices allow detecting change on the dependence structure from a low l moments perspective note also that the first order multivariate l comoment λ1 is not included in the proposed test statistic since it represents the componentwise mean vector and hence does not affect the dependence structure serfling and xiao 2007 note also that the proposed statistic tn is a rank based since the matrices of the l moments ratios are expressed as function of the pseudo observations which are the best summary of the joint behavior of the random pairs and hence not influenced by the margins e g genest and chebana 2017 kao and govindaraju 2010 kao and govindaraju 2008 salvadori and de michele 2004 in addition the proposed statistic has several advantages including a simple formulas are available in terms of the ranks of the observations hence it can be applied without assuming any prior distribution about data b the proposed statistic has the advantage that it makes no assumption on the type of copula being assessed hence it can be applied for any copula type the decision rule regarding the acceptance or rejection of the null hypothesis is based on the p value to evaluate the associated p value resampling methods such as permutation and bootstrapping are used e g good 2004 in the present work since the focus is on adequately detect and estimate the location where the change occurs we focus on the statistic tn which combine all l moments ratios together indeed as explained by serfling and xiao 2007 distinct distributions generate distinct series of l moments furthermore kjeldsen and prosdocimi 2015 argued that l moments of different orders capture sharply different population features and provide an improvement in the ability to detect the underlying distribution when compared to the performance of the one dimensional l moment hence in the same vein using a combination of different l moments ratios is more convenient for detecting change in the dependence and permits a high probability of discrimination between different copulas 4 simulation study the purpose of the simulation study is to evaluate the performance of the proposed multivariate homogeneity test to this end we consider practical cases commonly encountered in hydrological applications despite the validity of the proposed test for different hydrological events such as floods rain storms and droughts in the present paper we restrict attention to the performance of the proposed test when dealing with floods 4 1 simulation design in practice flood events are described by several correlated variables namely the peak q the volume v and duration d e g aissia et al 2012 fu and butler 2014 shiau et al 2006 since much of the available literature on flood events deals with dependence between q and v these two variables are considered in this study generated data are used to test the ability of the proposed test to detect heterogeneities as well as its ability to identify the position when the change occurs data are generated through simulations from representative and most used copulas in hydrometeorology analyses salvadori and de michele 2010 salvadori et al 2007 e g zhang and singh 2006 therefore three different classes of copula family are used to model the dependence structure between q and v namely archimedean extreme value and elliptical to generate data from a given copula we applied the procedure proposed by nelsen 2006 based on the conditional distribution method in particular the employed copulas are frank and clayton archimedean gumbel and galambos extreme value and gaussian elliptical it is worth noting that the aforementioned copulas are considered under different scenarios to generate heterogeneous data therefore data are mixture of two copulas to avoid confusion note that dependence shape stands for copula type as described by a specific dependence structure hereafter different scenarios are considered a homogenous data all data are generated from the same copula b heterogeneous on the dependence shape data are generated from two different copulas with same dependence strength c heterogeneous on the dependence strength data are generated from the same copula with different dependence strengths d heterogeneous on the dependence structure data are generated from two different copulas with different dependence strengths according to previous studies q and v can be marginally fitted by the gumbel distribution e g chebana and ouarda 2007 requena et al 2013b villarini and smith 2010 consequently we consider the gumbel distribution with scale and location parameters denoted respectively by σ and β as marginal for both q and v the corresponding parameters of the considered distribution are those obtained from the data series of the skootamatta basin in ontario canada used for previous simulation studies by chebana et al 2017 and chebana and ouarda 2009 in this study we consider a change in the location of q and v simultaneously hence for a change located at the middle of the series the corresponding parameters of the gumbel distribution before the change are defined as σ q 15 85 β q 51 85 σ v 300 22 β v 1239 8 these parameters are also used to generate homogenous series of q and v then for generating data after the change we consider amplitude of change δq δv 40 40 for more details the reader can refer to chebana et al 2017 the performance of a homogeneity test either univariate or multivariate could be affected by various factors specifically the record length the dependence strength the change point position and the copula type e g bouzebda and keziou 2013 holmes et al 2013 quessy et al 2013 hence a sensitivity analysis of the performance of the proposed test is performed regarding different factors first the sample size n is a relevant factor to the homogeneity of a series hydrological series are generally characterised by small sized samples hence the assessment of the behaviour of the proposed test was performed under several sample sizes n 30 50 100 the values of n are selected on the basis of situations commonly encountered in flood frequency analysis see series in barth et al 2017 and santhosh and srinivas 2013 the performance of the homogeneity test can also be affected by the dependence strength measured by kendall tau τ the test statistic should ideally be able to distinguish between heterogeneous subsamples having the same τ thus the smaller the influence of τ on the statistic the better the test will be this analysis is performed by varying the dependence strength τ since for most flood events the dependence strength is typically between 0 3 and 0 8 e g requena et al 2013a zhang and singh 2007 three values of τ 0 2 0 6 0 8 corresponding to weak moderate and strong dependence respectively are considered the parameter of the copula is defined to match the corresponding range of dependence as described through kendall τ e g nelsen 2006 vandenberghe et al 2010 the estimation of the parameter of the copula is achieved by using kendall s tau inversion method described in nelsen 2006 and joe 2014 besides sample size and dependence strength the location of the change is an important factor that may influence the performance of the homogeneity test in detecting heterogeneity in the dependence structure in the literature e g dehling et al 2017 nayak and villarini 2016 ribes et al 2017 xiong et al 2015 findings suggest that in general homogeneity test is more powerful when the change occurs far from the beginning or the end of the series furthermore some univariate tests are preferable if the change occurs in the middle of the series in order to determine the ability of the proposed test to accurately detect departure from homogeneity according to when a change occurs three locations are considered hence a change is taken to occur at location s n 4 n 2 3 n 4 a diagram of the simulation study is shown in fig 2 the flowchart is made up of three different parts the different rectangles signify the different building blocks of our methodology synthetic data generation pvalue estimation and power computation for all scenarios nsim 1000 samples are generated which is typically used in homogeneity testing simulations e g holmes et al 2013 xiong et al 2015 for each generated sample the proposed statistic tn is computed on a moving time window of size nw chosen arbitrary by the user e g bender et al 2014 the time window length nw must be chosen in a way to be neither too large nor too small in order to provide stationary variables within the windows and a reliable estimation of multivariate l moments then the time window is shifted by one observation each time step and the corresponding first four multivariate l moments ratios are calculated in the present study it is chosen as nw 25 15 and 10 respectively for large moderate and small samples as is well known in the hypothesis testing two features are of interest namely the nominal level α and the power 1 β good 2004 the first is that the null hypothesis is rejected while it is true the second is that the null hypothesis is accepted when the alternative is true these two features can be estimated using simulations study by computing the rejection rates in the present study we fix α 5 which is frequently used in the hydrological literature regarding hypothesis testing e g xie et al 2014 xiong et al 2015 4 2 simulation results to evaluate the performance of the proposed test it is important to analyze a large number of simulated series representing a variety of situations in this section results obtained by the application of the proposed test are presented 4 2 1 nominal level evaluation the first desirable property of a test is to hold the nominal level close to the significance level under the null hypothesis table 3 reports the estimates α of α for the proposed test for different sample sizes and dependence strengths first it can be seen from table 3 that the proposed statistic tn when considering the different copula types investigated in this study is not sensitive to the copula type regarding the estimation of α for a given dependence strength and sample size indeed α is between 4 7 and 5 2 for τ 0 8 and n 100 for different copula type it is also interesting to note from table 3 that the estimates α improve when the sample size increases e g for the clayton copula and τ 0 8 it decreases from 8 2 when n 30 to 4 9 when n 100 indeed this is likely due to the fact that the uncertainty of estimates of the multivariate l moments decreases with the sample size e g brahimi et al 2015 it is worth noting that holmes et al 2013 quessy et al 2013 and xiong et al 2015 also found similar effect of the sample size on the nominal level of their tests which are designed to detect the change in the strength of dependence copula parameters the effect of the dependence strength is also shown in table 3 it is worth noting that the test is not highly sensitive to the dependence strength when sample size is large enough indeed for different τ 0 2 0 6 and 0 8 α is close to 5 when the sample size n 100 it shows that in general the estimates α decrease with the dependence strength to reach the significance level α 5 as an example for the clayton copula with n 50 it decreases from 7 5 to 5 8 when τ increases from 0 2 to 0 8 likewise by applying the test of bouzebda and keziou 2013 see table 2 in an hydrological case xiong et al 2015 found that the highest dependence strength is the better estimated α will be hence copula with a strong dependence is more easily to be detected 4 2 2 power evaluation in this section the power of the proposed test in detecting the change in the dependence structure is studied results for different sample sizes and dependence strengths are displayed in table 4 from table 4 we can see that the power of the test is sensitive to the position where a change occurs s n 2 or s n 4 indeed for a given sample size and dependence strength the test has best powers when the change occurs on the middle of the series for example when data are a mixture of frank and gumbel copulas with n 50 and dependence strength 0 4 0 4 the power increases from 77 when s n 4 to 87 4 when s n 2 however when the sample size increases n 100 the test is less sensitive to the change point position in addition overall copula type seems to have little influence on the powers of the proposed test therefore no significant differences were found between powers when considering different copula types for example for n 100 and dependence strength τ 0 2 0 6 with a change located at s n 2 powers are between 92 6 and 93 6 when data are mixture of clayton frank copulas and gaussian frank copulas respectively through table 4 we can see a variation in the powers regarding the dependence strength τ in fact the proposed test performs clearly better when the amplitude of the jump in kendall τ increases as an example for a series of length n 50 generated from a mixture of gaussian and frank copula and a change located at s n 4 the test power increases from 74 1 for τ 0 4 0 4 to 87 4 when τ 0 2 0 6 hence in this case the change associated to larger change range in dependence strength is more easily to be detected similar findings are reported in the literature regarding change detection on dependence strength e g xiong et al 2015 results from table 4 show that the proposed statistic is sensitive to the amplitude of the change in the dependence strength however it is less sensitive to the dependence strength in fact the power of the proposed test for τ 0 2 0 6 is larger than for τ 0 4 0 8 thus the larger is the variance of τ the better is the power moreover it is important to emphasise that the proposed statistic tn performs well in detecting change in the copula shape namely the test is able to detect the heterogeneity when the copula structure before and after the change point are different and the kendall tau remains the same in fact for the same dependence strength the test has a high power in departure from h0 for instance for n 100 and a mixture of frank and gumbel copula with τ 0 4 0 4 the power estimates are between 82 3 and 91 8 table 4 provides also insights about the effect of sample size on the power of the proposed test there is a clear trend of increasing power as sample size increases in fact this tendency can be a consequence of the fact that when sample size is small n 30 the rejection of the null hypothesis is easier since the dependence shape is more difficult to distinguish moreover this can be explained by the higher uncertainty in estimating the l moments matrices for small samples in fact for sample size n 30 the size of subsample to estimate the l moments matrices is equal to 10 which is too short to have a reliable estimator especially in the multivariate case brahimi et al 2015 these results agree with the findings of other studies in which the power increases with the sample size e g quessy et al 2013 xiong et al 2015 4 2 3 comparison as mentioned in the literature review existing multivariate homogeneity tests deal only with the change on the dependence strength a comparison of the performance between the proposed test and the classical ones is made through simulation studies as summarized in tables 5 and 6 table 5 provides corresponding results when the copula structure remains invariant before and after the change point here gumbel copula is considered for different sample sizes dependence strengths and change point positions according to table 5 the power estimates of all statistics increase when n becomes larger this agrees with similar findings by dehling et al 2017 lung yut fong et al 2015 and xiong et al 2015 since the proposed test is based on multivariate l moments it is appropriate for hydrological applications where the record lengths are typically short as one can see from table 5 power estimates of the proposed test range respectively between 30 3 and 48 for n 30 and between 38 5 and 59 1 for n 50 this can be explained by the fact that multivariate l moments are robust and suffer less from the effects of sampling variability brahimi et al 2015 nevertheless for the same design the two classical statistics are not able to detect the change point for small sized samples the associated powers of these tests are between 10 1 and 24 1 for n 30 and between 30 1 and 53 7 for n 50 these results agree with the findings of other studies in which xiong et al 2015 suggested that the copula likelihood ratio test performs poorly for a relatively small sample size n 50 in addition change point in small samples proved to be harder to capture by the test proposed by quessy et al 2013 the performance of the proposed test in small samples shows the relevance of the introduced approach one of the advantages of the proposed multivariate homogeneity test might be its ability to detect not only change in the dependence strength but also in copula type these interesting features of the proposed test will now be compared to the existing tests results regarding the application of the proposed test as well as classical tests to detect heterogeneities when the dependence structure copula type before and after the change are different are summarized in table 6 it is notable from table 6 that the likelihood ratio test is not able to detect the change in the dependence structure in all cases the power estimates are less than 15 this is unsurprising since this test is conceived to detect only the change in the dependence strength on the other hand the kendall s tau test has good performance for higher change in the amplitude of dependence strength indeed for sample size n 100 and a change point located at the middle of the series power estimates of the kendall s tau test are 53 7 when τbefore 0 2 and τafter 0 6 and 6 7 when τbefore τafter 0 4 this means that high power estimates are obtained for largest difference in the dependence strength before and after the change point furthermore it is apparent from table 6 that the proposed test can capture change in copula type when the dependence strength remains invariant which is not the case of the aforementioned tests powers of the proposed test range between 88 and 91 when keeping the same kendall tau before and after the change point for sample size n 100 howver for the other tests powers are between 4 9 and 13 9 it is also important to note that the proposed test has approximately the same computation time as classical tests table 7 shows the computation time for performing each test 100 times for the same scenario it is shown that there is a huge gain in computation time with the proposed l moments test and the kendall τ test compared to the likelihood ratios ones besides the power and the first type error estimation an additional important feature of a change point detection test is the ability to well positioning the change in the univariate framework a change point is considered well positioned when it is located within 2 years of the true position e g beaulieu et al 2009 we use the same threshold for the multivariate case so far this paper has focused on change detection in the copula structure the following section will compare the detection skills of the proposed test statistic tn to the kendall based test kn proposed by quessy et al 2013 here the likelihood ratio test is not considered since it is based in the assumption of invariant copula structure fig 3 compares the percentage of the well identified change point obtained by each test among the detected change point for each scenarios combining different sample sizes dependence ranges and copula types it is apparent from this figure that the kendall test is not able to well positioned the detected change point for small sample size n 30 50 for the proposed statistic tn the position of the detected change is well identified for different dependence range however we can see that the performance is an increasing function of the sample size 5 applications to real hydrological data in this section we apply the test on two real world hydrological data the purpose of these applications is to assess the appropriateness of the proposed test for practical use the first data series correspond to the romaine station with natural flow regimes located in the cote nord region of the province of quebec canada it is considered previously for change point studies by chebana et al 2017 the second data series correspond to the ankang hydrological station at the upper hanjiang river china the same series is used on the paper of xiong et al 2015 who demonstrated the existence of a change on the copula parameter general information about the used data are given in table 8 for each station flood characteristics peak q and volume v are extracted from daily streamflow series aissia et al 2012 for the romaine station the p value associated to the proposed multivariate statistic is 2 this result suggests the existence of a change on the dependence structure of q v furthermore the estimated change point is located at 1984 which corresponds to the detected change on the univariate variables q and v chebana et al 2017 in the same way for the ankang station the multivariate change point is located at 1986 with a corresponding p value equal to 4 note that this year corresponds approximately to the beginning of water storage at the ankang reservoir located about 30 km upstream the ankang hydrological station jiang et al 2015 in order to validate results regarding detected change point different copulas are fitted to the entire series and to each subsample before and after the detected change similarly to the set up used by salvadori et al 2018 the goodness of fit gof test selected to formally identify which copulas represent properly the dependence structure of the data is that based on the empirical copula and the cramér von mises statistic e g genest et al 2009 then to select the best fitting copula aic criterion is used e g laio et al 2009 results are summarised in table 9 a set of univariate distributions are also considered as possible candidates for fitting the studied variables the weibull gumbel log normal and gamma distributions the anderson darling gof test sinclair et al 1990 is used to discard those distributions that cannot represent the observed univariate data results are presented in table 10 the best fitting univariate distribution and copula structure are presented in table 11 for the romaine station under the homogeneity hypothesis the normal copula θ 0 685 is considered as the best copula that represents the relation between q and v however under the heterogeneity hypothesis the copula that best fits the observed data before the change point 1984 is the hüssler reiss copula whereas after 1984 it is the normal copula it is worthwhile noting that the two selected copulas before and after 1984 have too close dependence strength τbefore 0 42 τafter 0 41 this result illustrates the capability of the proposed test to distinguish between two copulas with same dependence strength as shown in simulation study for the ankang station the clayton copula with a dependence parameter θ 8 64 obtained by the inversion kendall s tau method is selected as the best fitting copula under homogeneity however when considering the change point before 1986 normal copula is chosen as the best candidate reaching dependence strength 0 81 and after 1986 the dependence increases to 0 91 with a change on the shape of the copula from normal to clayton as pointed out through this study it is of high importance to detect changes and to take into account these changes in the modeling steps indeed if there is any heterogeneity in the data the statistical parameters of the model estimated under homogeneity do not reflect the physics of the phenomenon and interpretations made are erroneous since different combinations of u v pairs can lead to the same c u v value it is common to express the joint probability c u v via contours of equal probability i e copula probability level curves where c u v p with 0 p 1 associated copula probability level curves before and after the detected change point are presented in fig 4 as shown in fig 4a in the case of the ankang station the level curves corresponding to different given joint probabilities change dramatically before and after the change in the copula structure however for the romaine station the change is less visible and the difference between level curves before and after the change point is very small these results are likely to be related to the fact that the kendall τ for the romaine station is almost the same before and after the change τbefore 0 42 τafter 0 41 only for illustration purposes fig 5 shows that the difference between level curves associated to normal and hüssler reiss copulas the same selected copulas for this station are more pronounced when there is a change on the kendall τ dependence strength note also that the change of the level curves is more accentuated when the change of the copula is coupled with the change in marginal distributions fig 6 as a consequence this might affect the results of the multivariate frequency analysis especially in terms of underestimation of the associated risk 6 conclusion while there are a large number of homogeneity tests in the literature only few tests deal with multivariate series furthermore little attention has been paid to the homogeneity of the whole dependence structure existing tests detect the change point in the strength of the dependence through the parameter of a given copula assuming the same type of copula in this paper a new test for homogeneity of multivariate dependence structure is proposed and it is adapted to the specifications of the hydrological context the proposed test is based on the multivariate l moments in order to evaluate its performance simulations are considered as well as real world flood case studies results show very interesting performances in term of first type error and power estimations more precisely findings reveal that this test is suitable for small sized hydrological series and for series with same dependence strength the application of the proposed test to real data shows its ability to detect and well position the change points especially cases where the dependence strength remains the same whereas the change concerns the whole copula structure the proposed test can be considered for a wide range of hydrological water resources and climatological applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors are grateful to the editor and the reviewers for their comments and suggestions which helped improve the quality of the paper 
563,recently there have been an increasing number of studies dealing with change detection in multivariate series however a major drawback with most of the currently used methods is the lack of flexibility indeed these methods are only able to detect changes in the strength of dependence assuming invariant shape of the dependence structure however under a changing climate the shape of dependence might change as well furthermore in the multivariate setting heterogeneities can occur in the margins and or in the dependence structure the most existing approaches for multivariate change detection deal with the whole distribution in this paper we propose a novel statistical test for multivariate heterogeneity detection based on copula and multivariate l moments a simulation study is conducted to evaluate the performance of the proposed test and to compare it with those of existing tests results indicate that the proposed test has an interesting power especially when the dependence strength remains invariant with power ranging between 45 and 93 whereas for the existing tests the power is lower than 14 in this case an application to a real data set is also provided results show the ability of the proposed test to discriminate homogeneous and inhomogeneous series keywords multivariate homogeneity testing stationarity multivariate l moments copula flood 1 introduction extreme events are of high importance in design of hydraulic structures water resources management and flood insurance consequently their estimation should be as reliable as possible for this purpose hydrological frequency analysis hfa is the most used statistical tool to model behaviour of hydro meteorological variables e g hamed and rao 1999 these events are characterized by a number of correlated variables e g chebana 2013 salvadori and de michele 2004 for example floods can be described by the peak volume and duration e g requena et al 2013b shiau et al 2006 therefore a multivariate framework that takes into account the dependence between different variables is of fundamental importance e g requena et al 2013b zhang and singh 2012 copulas have been proposed as a key tool to model the dependencies between hydrological variables in the context of multivariate hfa e g chebana 2013 genest and chebana 2016 salvadori and de michele 2004 salvadori et al 2007 hfa is composed of four main steps as summarized in table 1 e g chebana et al 2010 checking basic assumptions homogeneity stationarity and serial independence is an important step since it has a significant impact on the other subsequent steps ignoring this step could lead to catastrophic consequences in case of an underestimation of these events and waste of valuable economic resources in case of their overestimation e g bender et al 2014 in hydrological multivariate setting checking for trend freeness is recently treated by chebana et al 2013 while homogeneity testing has attracted less attention despite its importance moreover departures from this assumption may provide misleading results furthermore if unaccounted for these inhomogeneities could have a large impact on the outcome of the estimation and fitting of the probability distribution e g ouarda et al 2014 seidou and ouarda 2007 data are considered as homogenous if they have the same underlying distribution with invariant statistical characteristics e g gilroy and mccuen 2012 lund et al 2007 reeves et al 2007 hence it means that data do not exhibit a mix of several samples from different populations it is necessary to note that this definition does not involve time consequently as pointed out by hamed and rao 1999 the chronological order is not important for a sample to be homogenous the lack of homogeneity may be caused by natural or anthropogenic actions on physical environment such as deforestation dam construction and urbanization among others e g burn and elnur 2002 furthermore annual flood series might be produced by more than one hydrometeorological process such as flood producing storms rainfall and snowmelt floods e g ouarda and el adlouni 2011 smith et al 2011 moreover the regime of a river downstream from the confluence of two sub watersheds with very different hydrological behaviors is a good example of the lack of homogeneity in the literature the most common way for testing homogeneity in both univariate and multivariate context is to detect abrupt change point e g das and umamahesh 2017 he et al 2016 quessy 2019 ribes et al 2017 sadegh et al 2015 in fact homogeneity and change point are two related problems but the latter is more challenging as illustrated by fig 1 when dealing with homogeneity problem one has two or more pre specified sub groups of data to be tested without necessary a sequential aspect of the data however in the case of a change point problem the appropriate groupings of the data are unknown and data has to be segmented in a sequential manner into sub groups that will be tested for homogeneity homogeneity problem can be considered as a change point testing problem by using preliminary step such as classifying or clustering data e g lung yut fong et al 2015 hence in this paper homogeneity problem is treated as a change point detection cpd problem to deal with the cpd problem formal parametric and nonparametric testing approaches have been proposed in the literature in this study only nonparametric approaches are considered since they do not require prior choice of marginal and joint distributions which makes them more appropriate in hfa e g vittal et al 2015 for general comparison between parametric and nonparametric frameworks the reader is referred for instance to santhosh and srinivas 2013 and oja and randles 2004 the hydro meteorological literature abounds with studies dealing with homogeneity testing of univariate distributions e g beaulieu et al 2009 ehsanzadeh et al 2011 reeves et al 2007 seidou and ouarda 2007 however the multivariate setting has been scarcely addressed e g chebana et al 2017 homogeneity of multivariate hydrological series can affect marginal distributions as well as the dependence structure e g guerfi et al 2015 holmes et al 2013 xiong et al 2015 therefore it is of high importance to distinguish between these two types of homogeneities in order to gain deep physical insights behind the causes of the eventual changes in the data e g mazouz et al 2012 a general class of tests based on multivariate statistical depth has been introduced and evaluated for the hydrological context in chebana et al 2017 these tests allow detecting an overall shift in the series however they do not allow distinguishing if the shift is in margins or in the dependence structure to deal with this problem a few multivariate tests have been recently proposed to detect inhomogeneities in the dependence structure e g holmes et al 2013 quessy et al 2013 xiong et al 2015 nonetheless as summarized in table 2 these tests entail some drawbacks for instance some tests are designed to detect changes only in the parameter of the copula and not able to detect changes in the shape of dependence such as the test proposed by bouzebda and keziou 2013 this is an important limitation since under lack of homogeneity the copula itself could change moreover some of these tests have optimal performance under normality this is a major drawback since hydrological series are generally non normally distributed to avoid these limitations another class of tests based on kendall and empirical copula process are proposed e g kojadinovic et al 2016 quessy et al 2013 unfortunately these tests are not suitable for small sized samples indeed these tests allow detecting change on long data length for n 100 which is not usually available in practice in addition a change in the copula may occur even if kendall process remains invariant such as the case of extreme value copulas e g genest and favre 2007 vogel and fried 2015 moreover changes captured by these tests usually tend to appear in the middle of series whereas heterogeneities may also occur at the beginning or at the end of the records in order to avoid the aforementioned drawbacks the objective of the present paper is to develop a new test for homogeneity of the dependence structure the developed test is based on multivariate l moments given their attractive desirable properties for hydrological applications indeed they provide a summary and a description of the properties and shapes of a multivariate distribution this makes them particularly useful in parameter estimation and hypothesis testing furthermore since multivariate also univariate l moments are much less biased than classical moments they are used as meaningful replacements of classical moments in a wide variety of applications mainly in hydrology climatology and meteorology analysis e g chebana and ouarda 2007 kysely and picek 2007 a performance evaluation of the proposed test based on monte carlo simulations is presented under hydrological constraints a case study is also performed to illustrate an application of the developed test on hydrological data aside from the introduction the remainder of the paper is organized as follows a brief review of definitions and some notions related to the developed test is presented in section 2 section 3 presents the development of the proposed test for homogeneity testing the simulation study to evaluate the performance of the test is presented in section 4 section 5 illustrates an application of the developed test on hydrological data the conclusion of the study and a number of perspectives are reported in section 6 2 theoretical background in this section we tackle the statistical tools used in the current paper a brief description of the statistical background of homogeneity testing problem is provided herein 2 1 multivariate homogeneity testing problem as mentioned earlier homogeneity is a fundamental hypothesis in hfa since its absence may affect the statistical inference to be undertaken homogeneity of a sample postulates that all data are generated by the same physical process e g hamed and rao 1999 peterson et al 1998 this implies therefore checking that all data have an invariant probability distribution e g ehsanzadeh et al 2011 ribes et al 2017 shift detection on location and or dispersion parameters is the commonly used form of testing the homogeneity of the hydrological series e g hamed and rao 1999 kundzewicz and robson 2004 li and liu 2004 seidou et al 2007 as pointed out by rougé et al 2013 and nayak and villarini 2016 this formulation of the homogeneity problem is based on the sequential time of the event however heterogeneity can be present without importance of the sequential aspect as a consequence throughout this paper we focus on the homogeneity in form of distributional change in statistical terms this means that not only some characteristics of the distribution change abruptly but also the distribution itself changes e g milly et al 2015 serinaldi et al 2018 in the multivariate hfa as pointed out by vezzoli et al 2017 heterogeneities may occur in marginal distributions dependence structure or both over the past few decades a large number of techniques have been developed to detect heterogeneities in marginal distribution however as far as we know the homogeneity of the dependence structure copula type has been scarcely addressed e g guerfi et al 2015 xiong et al 2015 since hydrological time series increasingly exhibit non stationarity due to natural and anthropogenic changes this issue is worthy of attention in hydrologic design to define the homogeneity testing problem the null hypothesis h0 is that when arbitrarily splitting the sample in two subsamples there is no change in the distribution function of each subsample statistically let xi i 1 n be a d variate random variables of size n with marginal distributions f j j 1 d and multivariate joint cumulative distribution function g we denote x i x i 1 x i d the observation from xi under h0 it is assumed that all observations are homogeneous and thus have the same distribution g the alternative hypothesis h1 assumes g to exhibit inhomogeneity i e g is time varying and is no longer constant in agreement with sklar 1959 s theorem there exists a copula c such that for all xi we have 1 g x 1 x d c f 1 x 1 f d x d regarding the links between copulas and multivariate joint distributions the null hypothesis h0 can be expressed formally as h 0 h 0 m h 0 c against the alternative h 1 h 1 m uh 1 c h0 m and h0 c respectively h1 m and h1 c are the null respectively the alternative hypothesis about the homogeneity of marginal distributions and copula homogeneity testing of marginal distributions and copula are expressed respectively as h 0 m f 1 f d such that x 1 x 1 x n have m c d f s f 1 f d h 1 m at least an index s and f 1 1 f 1 d such that x 1 x s have m c d f s f 1 1 f 1 d f 2 1 f 2 d such that x s 1 x n have m c d f s f 2 1 f 2 d h 0 c c such that x 1 x n have copula c h 1 c at least an index s and c 1 such that x 1 x s have copula c 1 c 2 such that x s 1 x n have copula c 2 as mentioned above this paper focuses on the homogeneity of the copula structure therefore the proposed statistic attempts to test h 0 c against h 1 c in the following for the sake of convenience and brevity only the bivariate case is considered 2 2 multivariate copula l moments the multivariate l moment was introduced by serfling and xiao 2007 in both parametric and nonparametric schemes multivariate l moments provide a summary and a description of the properties and shapes of a distribution this makes them particularly useful in parameter estimation and hypothesis testing since multivariate also univariate l moments are much less biased than classical moments they are used as meaningful replacements of classical moments in a wide variety of applications especially for hydrological frequency analysis e g ben nasr and chebana 2019 chebana and ouarda 2007 hosking and wallis 1993 the multivariate l moments are defined as bellow serfling and xiao 2007 2 λ k i j cov x i p k 1 f j x j where cov is the covariance k is the order of the multivariate l moment p k 1 is the shifted legendre polynomial chang and wang 1983 x i is a random variable with marginal distribution f i for i 1 2 brahimi et al 2015 introduced the copula s multivariate l moments as 3 λ k c 12 0 1 0 1 c u 1 u 2 u 1 u 2 d u 1 d p k u 2 the sample version of the k th copula multivariate l moments is defined in term of pseudo observations ui vi as brahimi et al 2015 4 λ k c 12 1 n i 1 n u i p k 1 v i where pseudo observations u i r i n 1 v i s i n 1 and ri is the rank of among si is the rank of the concomitant x i n 12 among x i 2 x n 2 to obtain x 12 we first sort x 2 in the ascending order as x 1 n 2 x 2 n 2 x n n 2 then the concomitant x i n 12 corresponds to x i 1 paired with x i n 2 serfling and xiao 2007 the copula multivariate l moments coefficient ratios are defined as 5 τ k 12 c λ k 12 c λ 2 2 for k 3 and τ 2 12 c λ 2 12 c λ 1 2 3 multivariate l moments based copula homogeneity test recall that the null hypothesis h 0 of no change in the dependence structure of the d variate variables x i i 1 n postulates that the copula c is invariant against the alternative h1 that there is ksuch that x 1 xk have a copula c 1and x k 1 xn have copula c 2 if h 0 is rejected k is the so called change point in dependence structure the proposed test statistic is then given by 6 t n max 1 k n λ k λ n k where λ k and λ n k are matrices whose elements are multivariate l moments ratios of order 2 3 and 4 of the series before and after the point k respectively is the euclidean norm the dimension of the matrices λ k and λ n k is 2 6 each matrix of multivariate l moments coefficients λ is written as τ 2 c τ 3 c τ 4 c w h e r e τ i c τ i 11 c τ i 21 c τ i 12 c τ i 22 c i 2 3 4 the null hypothesis h 0 should be rejected for a large value of the test statistic tn the change point is therefore estimated by 7 k argmax 1 k n λ k λ n k the asymptotic distribution of the proposed statistic is out of the scope of the present paper and could be the object of future work in addition asymptotic results could not be appropriate for hydrological application given the short sample sizes usually encountered hence the bootstrap procedure is used to estimate p values of the proposed test good 2004 holmes et al 2013 see fig 2 the idea of the proposed test is inspired by the tests proposed by quessy et al 2013 and kojadinovic et al 2016 however it allows avoiding some drawbacks of the traditional tests in fact there are three key features that distinguish our test from the existing ones given in table 2 first the tests by quessy et al 2013 and kojadinovic et al 2016 are designed to detect only the change in the dependence strength kendall or spearman coefficients and hence assuming the same copula shape before and after the change point in contrast the proposed test allows for detecting change on the shape of copula even if the kendall τ or spearman ρ remains invariant second these two tests are not well adapted to particularities of hydrological series where they have low power for small sample sizes the proposed test is based on multivariate l moments which are accurate for small sample size usually encountered in hfa third unlike the likelihood ratios test bouzebda and keziou 2013 the proposed test is nonparametric which is robust against distribution miss specification and respects the fact that the test should be performed prior the modeling step in hfa semi parametric tests should perform well under the correct model specifications but inference based on miss specified models is not well studied the heterogeneity of a data series may be due to differences in any feature of the distribution in particular in the hydrological literature the l covariation matrixλ2 represents a measure of the dispersion of the distribution e g chebana and ouarda 2007 besides the l coskewness λ3 is an important descriptor of the copula structure in particular λ3 measures the difference between the upper and lower tails and hence measures the asymmetric shape of the copula e g müller et al 2017 likewise λ4 gives an idea about the kurtosis of multivariate distribution by measuring the difference between the typical spread in the tails and the typical spread in the center of the multivariate distribution consequently taken together the l covariation l coskewness and l cokurtosis matrices allow detecting change on the dependence structure from a low l moments perspective note also that the first order multivariate l comoment λ1 is not included in the proposed test statistic since it represents the componentwise mean vector and hence does not affect the dependence structure serfling and xiao 2007 note also that the proposed statistic tn is a rank based since the matrices of the l moments ratios are expressed as function of the pseudo observations which are the best summary of the joint behavior of the random pairs and hence not influenced by the margins e g genest and chebana 2017 kao and govindaraju 2010 kao and govindaraju 2008 salvadori and de michele 2004 in addition the proposed statistic has several advantages including a simple formulas are available in terms of the ranks of the observations hence it can be applied without assuming any prior distribution about data b the proposed statistic has the advantage that it makes no assumption on the type of copula being assessed hence it can be applied for any copula type the decision rule regarding the acceptance or rejection of the null hypothesis is based on the p value to evaluate the associated p value resampling methods such as permutation and bootstrapping are used e g good 2004 in the present work since the focus is on adequately detect and estimate the location where the change occurs we focus on the statistic tn which combine all l moments ratios together indeed as explained by serfling and xiao 2007 distinct distributions generate distinct series of l moments furthermore kjeldsen and prosdocimi 2015 argued that l moments of different orders capture sharply different population features and provide an improvement in the ability to detect the underlying distribution when compared to the performance of the one dimensional l moment hence in the same vein using a combination of different l moments ratios is more convenient for detecting change in the dependence and permits a high probability of discrimination between different copulas 4 simulation study the purpose of the simulation study is to evaluate the performance of the proposed multivariate homogeneity test to this end we consider practical cases commonly encountered in hydrological applications despite the validity of the proposed test for different hydrological events such as floods rain storms and droughts in the present paper we restrict attention to the performance of the proposed test when dealing with floods 4 1 simulation design in practice flood events are described by several correlated variables namely the peak q the volume v and duration d e g aissia et al 2012 fu and butler 2014 shiau et al 2006 since much of the available literature on flood events deals with dependence between q and v these two variables are considered in this study generated data are used to test the ability of the proposed test to detect heterogeneities as well as its ability to identify the position when the change occurs data are generated through simulations from representative and most used copulas in hydrometeorology analyses salvadori and de michele 2010 salvadori et al 2007 e g zhang and singh 2006 therefore three different classes of copula family are used to model the dependence structure between q and v namely archimedean extreme value and elliptical to generate data from a given copula we applied the procedure proposed by nelsen 2006 based on the conditional distribution method in particular the employed copulas are frank and clayton archimedean gumbel and galambos extreme value and gaussian elliptical it is worth noting that the aforementioned copulas are considered under different scenarios to generate heterogeneous data therefore data are mixture of two copulas to avoid confusion note that dependence shape stands for copula type as described by a specific dependence structure hereafter different scenarios are considered a homogenous data all data are generated from the same copula b heterogeneous on the dependence shape data are generated from two different copulas with same dependence strength c heterogeneous on the dependence strength data are generated from the same copula with different dependence strengths d heterogeneous on the dependence structure data are generated from two different copulas with different dependence strengths according to previous studies q and v can be marginally fitted by the gumbel distribution e g chebana and ouarda 2007 requena et al 2013b villarini and smith 2010 consequently we consider the gumbel distribution with scale and location parameters denoted respectively by σ and β as marginal for both q and v the corresponding parameters of the considered distribution are those obtained from the data series of the skootamatta basin in ontario canada used for previous simulation studies by chebana et al 2017 and chebana and ouarda 2009 in this study we consider a change in the location of q and v simultaneously hence for a change located at the middle of the series the corresponding parameters of the gumbel distribution before the change are defined as σ q 15 85 β q 51 85 σ v 300 22 β v 1239 8 these parameters are also used to generate homogenous series of q and v then for generating data after the change we consider amplitude of change δq δv 40 40 for more details the reader can refer to chebana et al 2017 the performance of a homogeneity test either univariate or multivariate could be affected by various factors specifically the record length the dependence strength the change point position and the copula type e g bouzebda and keziou 2013 holmes et al 2013 quessy et al 2013 hence a sensitivity analysis of the performance of the proposed test is performed regarding different factors first the sample size n is a relevant factor to the homogeneity of a series hydrological series are generally characterised by small sized samples hence the assessment of the behaviour of the proposed test was performed under several sample sizes n 30 50 100 the values of n are selected on the basis of situations commonly encountered in flood frequency analysis see series in barth et al 2017 and santhosh and srinivas 2013 the performance of the homogeneity test can also be affected by the dependence strength measured by kendall tau τ the test statistic should ideally be able to distinguish between heterogeneous subsamples having the same τ thus the smaller the influence of τ on the statistic the better the test will be this analysis is performed by varying the dependence strength τ since for most flood events the dependence strength is typically between 0 3 and 0 8 e g requena et al 2013a zhang and singh 2007 three values of τ 0 2 0 6 0 8 corresponding to weak moderate and strong dependence respectively are considered the parameter of the copula is defined to match the corresponding range of dependence as described through kendall τ e g nelsen 2006 vandenberghe et al 2010 the estimation of the parameter of the copula is achieved by using kendall s tau inversion method described in nelsen 2006 and joe 2014 besides sample size and dependence strength the location of the change is an important factor that may influence the performance of the homogeneity test in detecting heterogeneity in the dependence structure in the literature e g dehling et al 2017 nayak and villarini 2016 ribes et al 2017 xiong et al 2015 findings suggest that in general homogeneity test is more powerful when the change occurs far from the beginning or the end of the series furthermore some univariate tests are preferable if the change occurs in the middle of the series in order to determine the ability of the proposed test to accurately detect departure from homogeneity according to when a change occurs three locations are considered hence a change is taken to occur at location s n 4 n 2 3 n 4 a diagram of the simulation study is shown in fig 2 the flowchart is made up of three different parts the different rectangles signify the different building blocks of our methodology synthetic data generation pvalue estimation and power computation for all scenarios nsim 1000 samples are generated which is typically used in homogeneity testing simulations e g holmes et al 2013 xiong et al 2015 for each generated sample the proposed statistic tn is computed on a moving time window of size nw chosen arbitrary by the user e g bender et al 2014 the time window length nw must be chosen in a way to be neither too large nor too small in order to provide stationary variables within the windows and a reliable estimation of multivariate l moments then the time window is shifted by one observation each time step and the corresponding first four multivariate l moments ratios are calculated in the present study it is chosen as nw 25 15 and 10 respectively for large moderate and small samples as is well known in the hypothesis testing two features are of interest namely the nominal level α and the power 1 β good 2004 the first is that the null hypothesis is rejected while it is true the second is that the null hypothesis is accepted when the alternative is true these two features can be estimated using simulations study by computing the rejection rates in the present study we fix α 5 which is frequently used in the hydrological literature regarding hypothesis testing e g xie et al 2014 xiong et al 2015 4 2 simulation results to evaluate the performance of the proposed test it is important to analyze a large number of simulated series representing a variety of situations in this section results obtained by the application of the proposed test are presented 4 2 1 nominal level evaluation the first desirable property of a test is to hold the nominal level close to the significance level under the null hypothesis table 3 reports the estimates α of α for the proposed test for different sample sizes and dependence strengths first it can be seen from table 3 that the proposed statistic tn when considering the different copula types investigated in this study is not sensitive to the copula type regarding the estimation of α for a given dependence strength and sample size indeed α is between 4 7 and 5 2 for τ 0 8 and n 100 for different copula type it is also interesting to note from table 3 that the estimates α improve when the sample size increases e g for the clayton copula and τ 0 8 it decreases from 8 2 when n 30 to 4 9 when n 100 indeed this is likely due to the fact that the uncertainty of estimates of the multivariate l moments decreases with the sample size e g brahimi et al 2015 it is worth noting that holmes et al 2013 quessy et al 2013 and xiong et al 2015 also found similar effect of the sample size on the nominal level of their tests which are designed to detect the change in the strength of dependence copula parameters the effect of the dependence strength is also shown in table 3 it is worth noting that the test is not highly sensitive to the dependence strength when sample size is large enough indeed for different τ 0 2 0 6 and 0 8 α is close to 5 when the sample size n 100 it shows that in general the estimates α decrease with the dependence strength to reach the significance level α 5 as an example for the clayton copula with n 50 it decreases from 7 5 to 5 8 when τ increases from 0 2 to 0 8 likewise by applying the test of bouzebda and keziou 2013 see table 2 in an hydrological case xiong et al 2015 found that the highest dependence strength is the better estimated α will be hence copula with a strong dependence is more easily to be detected 4 2 2 power evaluation in this section the power of the proposed test in detecting the change in the dependence structure is studied results for different sample sizes and dependence strengths are displayed in table 4 from table 4 we can see that the power of the test is sensitive to the position where a change occurs s n 2 or s n 4 indeed for a given sample size and dependence strength the test has best powers when the change occurs on the middle of the series for example when data are a mixture of frank and gumbel copulas with n 50 and dependence strength 0 4 0 4 the power increases from 77 when s n 4 to 87 4 when s n 2 however when the sample size increases n 100 the test is less sensitive to the change point position in addition overall copula type seems to have little influence on the powers of the proposed test therefore no significant differences were found between powers when considering different copula types for example for n 100 and dependence strength τ 0 2 0 6 with a change located at s n 2 powers are between 92 6 and 93 6 when data are mixture of clayton frank copulas and gaussian frank copulas respectively through table 4 we can see a variation in the powers regarding the dependence strength τ in fact the proposed test performs clearly better when the amplitude of the jump in kendall τ increases as an example for a series of length n 50 generated from a mixture of gaussian and frank copula and a change located at s n 4 the test power increases from 74 1 for τ 0 4 0 4 to 87 4 when τ 0 2 0 6 hence in this case the change associated to larger change range in dependence strength is more easily to be detected similar findings are reported in the literature regarding change detection on dependence strength e g xiong et al 2015 results from table 4 show that the proposed statistic is sensitive to the amplitude of the change in the dependence strength however it is less sensitive to the dependence strength in fact the power of the proposed test for τ 0 2 0 6 is larger than for τ 0 4 0 8 thus the larger is the variance of τ the better is the power moreover it is important to emphasise that the proposed statistic tn performs well in detecting change in the copula shape namely the test is able to detect the heterogeneity when the copula structure before and after the change point are different and the kendall tau remains the same in fact for the same dependence strength the test has a high power in departure from h0 for instance for n 100 and a mixture of frank and gumbel copula with τ 0 4 0 4 the power estimates are between 82 3 and 91 8 table 4 provides also insights about the effect of sample size on the power of the proposed test there is a clear trend of increasing power as sample size increases in fact this tendency can be a consequence of the fact that when sample size is small n 30 the rejection of the null hypothesis is easier since the dependence shape is more difficult to distinguish moreover this can be explained by the higher uncertainty in estimating the l moments matrices for small samples in fact for sample size n 30 the size of subsample to estimate the l moments matrices is equal to 10 which is too short to have a reliable estimator especially in the multivariate case brahimi et al 2015 these results agree with the findings of other studies in which the power increases with the sample size e g quessy et al 2013 xiong et al 2015 4 2 3 comparison as mentioned in the literature review existing multivariate homogeneity tests deal only with the change on the dependence strength a comparison of the performance between the proposed test and the classical ones is made through simulation studies as summarized in tables 5 and 6 table 5 provides corresponding results when the copula structure remains invariant before and after the change point here gumbel copula is considered for different sample sizes dependence strengths and change point positions according to table 5 the power estimates of all statistics increase when n becomes larger this agrees with similar findings by dehling et al 2017 lung yut fong et al 2015 and xiong et al 2015 since the proposed test is based on multivariate l moments it is appropriate for hydrological applications where the record lengths are typically short as one can see from table 5 power estimates of the proposed test range respectively between 30 3 and 48 for n 30 and between 38 5 and 59 1 for n 50 this can be explained by the fact that multivariate l moments are robust and suffer less from the effects of sampling variability brahimi et al 2015 nevertheless for the same design the two classical statistics are not able to detect the change point for small sized samples the associated powers of these tests are between 10 1 and 24 1 for n 30 and between 30 1 and 53 7 for n 50 these results agree with the findings of other studies in which xiong et al 2015 suggested that the copula likelihood ratio test performs poorly for a relatively small sample size n 50 in addition change point in small samples proved to be harder to capture by the test proposed by quessy et al 2013 the performance of the proposed test in small samples shows the relevance of the introduced approach one of the advantages of the proposed multivariate homogeneity test might be its ability to detect not only change in the dependence strength but also in copula type these interesting features of the proposed test will now be compared to the existing tests results regarding the application of the proposed test as well as classical tests to detect heterogeneities when the dependence structure copula type before and after the change are different are summarized in table 6 it is notable from table 6 that the likelihood ratio test is not able to detect the change in the dependence structure in all cases the power estimates are less than 15 this is unsurprising since this test is conceived to detect only the change in the dependence strength on the other hand the kendall s tau test has good performance for higher change in the amplitude of dependence strength indeed for sample size n 100 and a change point located at the middle of the series power estimates of the kendall s tau test are 53 7 when τbefore 0 2 and τafter 0 6 and 6 7 when τbefore τafter 0 4 this means that high power estimates are obtained for largest difference in the dependence strength before and after the change point furthermore it is apparent from table 6 that the proposed test can capture change in copula type when the dependence strength remains invariant which is not the case of the aforementioned tests powers of the proposed test range between 88 and 91 when keeping the same kendall tau before and after the change point for sample size n 100 howver for the other tests powers are between 4 9 and 13 9 it is also important to note that the proposed test has approximately the same computation time as classical tests table 7 shows the computation time for performing each test 100 times for the same scenario it is shown that there is a huge gain in computation time with the proposed l moments test and the kendall τ test compared to the likelihood ratios ones besides the power and the first type error estimation an additional important feature of a change point detection test is the ability to well positioning the change in the univariate framework a change point is considered well positioned when it is located within 2 years of the true position e g beaulieu et al 2009 we use the same threshold for the multivariate case so far this paper has focused on change detection in the copula structure the following section will compare the detection skills of the proposed test statistic tn to the kendall based test kn proposed by quessy et al 2013 here the likelihood ratio test is not considered since it is based in the assumption of invariant copula structure fig 3 compares the percentage of the well identified change point obtained by each test among the detected change point for each scenarios combining different sample sizes dependence ranges and copula types it is apparent from this figure that the kendall test is not able to well positioned the detected change point for small sample size n 30 50 for the proposed statistic tn the position of the detected change is well identified for different dependence range however we can see that the performance is an increasing function of the sample size 5 applications to real hydrological data in this section we apply the test on two real world hydrological data the purpose of these applications is to assess the appropriateness of the proposed test for practical use the first data series correspond to the romaine station with natural flow regimes located in the cote nord region of the province of quebec canada it is considered previously for change point studies by chebana et al 2017 the second data series correspond to the ankang hydrological station at the upper hanjiang river china the same series is used on the paper of xiong et al 2015 who demonstrated the existence of a change on the copula parameter general information about the used data are given in table 8 for each station flood characteristics peak q and volume v are extracted from daily streamflow series aissia et al 2012 for the romaine station the p value associated to the proposed multivariate statistic is 2 this result suggests the existence of a change on the dependence structure of q v furthermore the estimated change point is located at 1984 which corresponds to the detected change on the univariate variables q and v chebana et al 2017 in the same way for the ankang station the multivariate change point is located at 1986 with a corresponding p value equal to 4 note that this year corresponds approximately to the beginning of water storage at the ankang reservoir located about 30 km upstream the ankang hydrological station jiang et al 2015 in order to validate results regarding detected change point different copulas are fitted to the entire series and to each subsample before and after the detected change similarly to the set up used by salvadori et al 2018 the goodness of fit gof test selected to formally identify which copulas represent properly the dependence structure of the data is that based on the empirical copula and the cramér von mises statistic e g genest et al 2009 then to select the best fitting copula aic criterion is used e g laio et al 2009 results are summarised in table 9 a set of univariate distributions are also considered as possible candidates for fitting the studied variables the weibull gumbel log normal and gamma distributions the anderson darling gof test sinclair et al 1990 is used to discard those distributions that cannot represent the observed univariate data results are presented in table 10 the best fitting univariate distribution and copula structure are presented in table 11 for the romaine station under the homogeneity hypothesis the normal copula θ 0 685 is considered as the best copula that represents the relation between q and v however under the heterogeneity hypothesis the copula that best fits the observed data before the change point 1984 is the hüssler reiss copula whereas after 1984 it is the normal copula it is worthwhile noting that the two selected copulas before and after 1984 have too close dependence strength τbefore 0 42 τafter 0 41 this result illustrates the capability of the proposed test to distinguish between two copulas with same dependence strength as shown in simulation study for the ankang station the clayton copula with a dependence parameter θ 8 64 obtained by the inversion kendall s tau method is selected as the best fitting copula under homogeneity however when considering the change point before 1986 normal copula is chosen as the best candidate reaching dependence strength 0 81 and after 1986 the dependence increases to 0 91 with a change on the shape of the copula from normal to clayton as pointed out through this study it is of high importance to detect changes and to take into account these changes in the modeling steps indeed if there is any heterogeneity in the data the statistical parameters of the model estimated under homogeneity do not reflect the physics of the phenomenon and interpretations made are erroneous since different combinations of u v pairs can lead to the same c u v value it is common to express the joint probability c u v via contours of equal probability i e copula probability level curves where c u v p with 0 p 1 associated copula probability level curves before and after the detected change point are presented in fig 4 as shown in fig 4a in the case of the ankang station the level curves corresponding to different given joint probabilities change dramatically before and after the change in the copula structure however for the romaine station the change is less visible and the difference between level curves before and after the change point is very small these results are likely to be related to the fact that the kendall τ for the romaine station is almost the same before and after the change τbefore 0 42 τafter 0 41 only for illustration purposes fig 5 shows that the difference between level curves associated to normal and hüssler reiss copulas the same selected copulas for this station are more pronounced when there is a change on the kendall τ dependence strength note also that the change of the level curves is more accentuated when the change of the copula is coupled with the change in marginal distributions fig 6 as a consequence this might affect the results of the multivariate frequency analysis especially in terms of underestimation of the associated risk 6 conclusion while there are a large number of homogeneity tests in the literature only few tests deal with multivariate series furthermore little attention has been paid to the homogeneity of the whole dependence structure existing tests detect the change point in the strength of the dependence through the parameter of a given copula assuming the same type of copula in this paper a new test for homogeneity of multivariate dependence structure is proposed and it is adapted to the specifications of the hydrological context the proposed test is based on the multivariate l moments in order to evaluate its performance simulations are considered as well as real world flood case studies results show very interesting performances in term of first type error and power estimations more precisely findings reveal that this test is suitable for small sized hydrological series and for series with same dependence strength the application of the proposed test to real data shows its ability to detect and well position the change points especially cases where the dependence strength remains the same whereas the change concerns the whole copula structure the proposed test can be considered for a wide range of hydrological water resources and climatological applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors are grateful to the editor and the reviewers for their comments and suggestions which helped improve the quality of the paper 
564,detection of systematic changes in the climate system resulting from anthropogenic forcing is a critical area of research detection and attribution of hydro climatological change has been limited by model uncertainty and bias as well as the poor spatial temporal coverage of observational data this study assesses a routinely adopted detection methodology and its sensitivity to model uncertainty and bias within a hydro climatological context using a synthetic case study we establish the sensitivity of detection approaches to the magnitude and consistency of trend and variance along with the length of data available it is found that the extent of uncertainty as measured by the variance plays a critical role in changing the detection outcome another important factor is the consistency of trend between simulations and observations a case study of soil moisture in select locations within australia shows that averaging over multiple years e g five years to a decade improves the detection of the climate change signal as long as consistency in the trends exists our results also demonstrate that there are substantial differences in simulated trends across climate models therefore even though ensemble averaging is effective in modulating variance it has the risk of canceling out the signal over models with markedly different responses keywords detection attribution sensitivity model uncertainty and bias soil moisture 1 introduction detection and attribution d a of hydro climatological change due to human induced climate changes is a significant area of research across the world kaufmann and stern 1997 min et al 2011 mondal and mujumdar 2015 gedney et al 2006 pall et al 2011 wan et al 2015 zhang et al 2007 following the definition of intergovernmental panel on climate change ipcc detection of change is defined as the process of demonstrating that climate or a system affected by climate has changed in some defined statistical sense without providing a reason for that change attribution seeks to determine whether a specified set of external forcings and or drivers are the cause of an observed change in a specific system bindoff et al 2013 however there is still considerable uncertainty about changes in the hydro climatological cycle under potential future warming because it is difficult to separate the effects of rising greenhouse gases from multi year decadal climate variability haerter and berg 2009 stocker et al 2013b additional uncertainty is introduced through the model simulations which are a vital input of d a studies whether these are physical mathematical or statistical models most d a assessments have been conducted by comparing observed trends to those simulated by a large number of general circulation models gcms jones et al 2013 wan et al 2015 najafi et al 2015 zhang et al 2007 there are multiple climate models available as part of the coupled model intercomparison program cmip which contain multiple types of simulations for the purposes of d a three groups of simulations are relevant these are simulations driven with all relevant anthropogenic and natural forcings all simulations driven with natural forcings e g solar and volcanic activity only nat and simulations driven with historical anthropogenic forcings mainly rising greenhouse gases only ant significant differences in the statistical attributes characterizing these simulations is used as the basis for most d a approaches the simplest of these approaches more details to follow later entails ascertaining the temporal trend or drift in each of these series and statistically ascertaining whether they represent different population distributions given the uncertainty and variability across model simulations model ensemble means are often used even though the mean of multi model simulations has good skill in estimating the climate responses to external forcings stocker et al 2013a and a reliable screening method and skill scores have been used for filtering climate models based on the quality and performance of these models pierce et al 2009 johnson et al 2011 the uncertainty and bias in model simulated trends remains high zhang et al 2007 as a consequence of these discrepancies in response patterns the use of trend or regression based methods may lead to physically inconsistent d a results such as negative trends when they are expected to be positive as per the laws of physics such as a negative regression slope for global surface temperature in response to rising greenhouse gases over time ribes and terray 2013 the simplest way of fitting the model simulated responses to observations is to assume that the responses to different forcings are linearly additive so the response to any one forcing can be scaled up or down without affecting any of the others additionally climate variability is assumed independent of the response to external forcing bindoff et al 2013 under these conditions attribution can be expressed as a variant of linear regression meehl et al 2003 and gillett et al 2004 have tested the linear additivity assumption and suggested that it might hold for large scale temperature changes but may not apply to hydro climatological variables such as precipitation hegerl et al 2007 hegerl and zwiers 2011 shiogama et al 2013 ghosh et al 2012 nor to regional temperature changes terray 2012 however so far non additive approaches have not been widely used and it takes a longer time for the science community to accept new methods especially when the current approach is simple and practical we also believe that there is no right or wrong approach to d a since it is all context dependent when it comes to how people view errors of a particular type lloyd and oreskes 2018 here we aim to ascertain the sensitivity of a routinely adopted d a methodology with the underlying linear additivity assumption to model uncertainty and bias within a hydro climatological context with a combination of global climate models and sophisticated statistical models that allow us to simulate underlying trends in a controlled environment we assess the reliability of the simple regression method that has been widely used in d a based on the outcomes of this assessment we proceed to demonstrate the implications of our findings in the context of soil moisture data simulated using a range of cmip5 gcms using select locations in australia where interesting changes have been identified the remainder of the paper is organized as follows section 2 reviews the regression based d a methods e g simple least squares weighted and generalized linear regression method along with their drawbacks and sources of uncertainties given the need for a good understanding of the fundamentals of d a methodology and their possible sensitivity to model uncertainty and bias investigated in the examples that follow in section 3 synthetic data is introduced to evaluate the sensitivity of the simple least squares approach to various sources of uncertainty and bias section 4 consists of a case study focusing on soil moisture change in select locations within australia followed by discussion in section 5 we conclude with a summary of the main outcomes in section 6 2 background 2 1 detection and attribution there are three core elements in d a studies first observations of climate indicators contain spatiotemporal information second an estimate of external forcing of the climate system e g natural and or anthropogenic forcings using climate models is generally taken as an ensemble mean of forced model runs last an estimate of climate variability is often derived from a physical based model generally obtained from unforced preindustrial model runs d a methodologies have evolved to include simple non optimal fingerprint techniques to more complex regression based methods we illustrate the philosophy behind the d a methodology with the non optimal fingerprint and summarize both non optimal and optimal fingerprints in the form of linear regression in d a studies empirical orthogonal function eof analysis is often used to study possible spatial modes i e patterns of variability and how they change with time in statistics eof analysis is known as principal component analysis pca which is also used for dimensionality reduction the optimal fingerprint terminology was first introduced by hassellmann 1979 regardless of non optimal or optimal fingerprint the fingerprint f i j represents a pattern of change in space generally a function of latitude i and longitude j characterizing the climate system response x i j t to external forcing marvel et al 2019 the fingerprint is often defined as the leading eof i e the unit norm eigenvector v 1 corresponding with the highest eigenvalue λ1 by computing the eigenvalues and eigenvectors of a spatiotemporal covariance matrix it often results from first averaging over members of each cmip5 historical model ensemble and then over models hasselmann 1993 marvel et al 2019 santer et al 1995 the leading eof can not only estimate a form of response how the variable of interest responds to an external forcing but also reduces the dimensionality of the spatiotemporal data by projecting it onto the fingerprint given a dataset of observations y i j t or model simulations x i j t we estimate the projection p t which is the amplitude of the fingerprint in the dataset by projecting it onto the fingerprint f i j 1 p y t i j y i j t f i j λ 1 p x t i j x i j t f i j λ 1 where λ 1 is used for normalization by the fingerprint physically the projection p t as a function of time indicates the covariance between the fingerprint f i j and the observational or model data in space the projection time series p t will hence show an upward trend if the fingerprint is increasingly present in the data marvel et al 2019 in the end we compare the amplitude of fingerprint in observations with both amplitude of signal in different forced model runs e g all and nat forcings by regression eq 2 and demonstrate that the response patterns of alternative external forcings e g nat forcings are unlikely to explain the observed change 2 p y t β i p x i t u where βi is the scaling factors that adjusts the amplitudes of those patterns and u is noise with the non optimal detection approach summarized above it has been recognised that it can be cast as a regression based problem with respect to generalised multivariate regression hasselmann 1997 allen and tett 1999 allen and stott 2003 the regression model has the form 3 y x β ɛ where vector y is a filtered version of the observations matrix x contains the estimated response patterns to the external forcings signals that are under investigation β is a vector of scaling factors that adjusts the amplitudes of those patterns and ε is noise that represents climate variability hegerl et al 2010 when the response x is noise free vector β is estimated with 4 β x t c 1 x 1 x t c 1 y where c is the covariance matrix of the noise hassellmann 1979 allen and tett 1999 mitchell 2001 the most commonly used assumption about vector ε is that it follows a gaussian distribution and the associated covariance matrix c becomes an identity matrix thus scaling factors β is simplified as 5 β x t x 1 x t y which is equivalent to the non optimal method in essence the covariance matrix c gives a somewhat greater weight to information in the low variance parts of the observations generally in terms of how we generalize the covariance matrix c i e measure the distance between the data points and the regression line the detection technique can be generally divided into three methods as outlined in table 1 it should be noted that the simple least squares approach table 1 is adopted generally when alternate models are expected to represent independent realizations of the process and are expected to have equal residual error variances this however is often too constraining an assumption especially when the process exhibits a chaotic pattern leading to the weighted and the generalized approaches being adopted under heteroscedasticity the residual error variances of the observations are unequal or autocorrelation simple least squares is no longer the optimal estimate whereas the other two estimators can be unbiased and more efficient allen and stott 2003 ribes and terray 2013 fitting the regression model requires an estimate of the covariance matrix c i e the climate variability which is usually obtained from an additional independent sample of simulated unforced variation e g long control simulations because the instrumental record is too short to provide a reliable estimate and may be affected by external forcings however gcms may not simulate climate variability accurately johnson et al 2011 casting doubt over the estimates of c even with good estimates of c from gcms control simulations studies wan et al 2015 santer et al 1995 have showed that optimal approaches have no clear advantages over non optimal methods hence in this study we assess the sensitivity of d a approaches to model uncertainty and bias using the simple least squares approach 2 2 systematic biases in climate model simulations and their implications for d a a failure to detect a particular response includes the possibility that the responses are collinear or weak relative to climate variability ribes et al 2017 or that the metric used to measure the change is insensitive to the expected change pachauri and reisinger 2007 aside from being able to distinguish climate variability in observations hegerl et al 2007 ropelewski and halpert 1987 mantua et al 1997 newman et al 2003 verdon and franks 2006 zhang et al 2007 and the selection of proper metrics the key to assessing the reliability of d a methods is to understand the uncertainty and bias in climate models climate models have errors that consist of unsystematic and systematic components teutschbein and seibert 2013 unsystematic model errors occur due to the chaotic nature of the climate system and the unconstrained nature of the gcms simulations christensen et al 2001 eden et al 2012 for instance global scale feedbacks like water vapor and cloud feedback and possible unknown feedbacks within the climate system could contribute substantially to uncertainties on the response patterns and magnitudes ribes et al 2017 mehrotra and sharma 2006 due to uncertainties in the forcings themselves the estimated responses to these external forcings fluctuate significantly the greenhouse gases forcing has substantial uncertainties if effective radiative forcings are considered rather than radiative forcings myhre et al 2013 various studies have quantified the extent of these errors and attributed them to model structure emission scenario and initialization woldemeskel et al 2016 the extent of these errors varies depending on the variable of interest with hydro climatological variables such as precipitation and soil moisture expected to exhibit greater error due to model structure as compared to other factors the systematic error component is commonly termed as model bias teutschbein and seibert 2013 and is a result of a range of various factors they include the coarse scale of the gcms incomplete model structures feedback loops including the albedo and land atmosphere interactions and in the case of precipitation the parameterisation of clouds and convection allen et al 2002 haerter et al 2011 masson and knutti 2011 randall et al 2007 sun et al 2006 maurer et al 2013 mehrotra et al 2004 a number of bias correction methods have been developed to address these biases prior to d a exercises including scaling or quantile matching empirical statistical correction gobiet et al 2015 nesting logic based approaches johnson and sharma 2012 and multivariate bias correction models mehrotra and sharma 2015 bias correction is popular because it can be applied directly and easily to climate model outputs and is able to correct the gcms simulations for the parameters of interest johnson and sharma 2012 on the other hand there is a suspicion that bias correction lacks a sound physical basis ehret et al 2012 since it does not necessarily preserve the dynamic relationships between different variables haerter et al 2011 the fundamental assumption of these methods is that if the biases can be removed from the model simulations then the corrected gcms outputs will properly represent the expected responses in the climate system nahar et al 2017 however it has been found that bias correction can sometimes alter the climate change signal which in some cases is considered undesirable hagemann et al 2011 and in others is considered beneficial for example by improving model consensus on the direction of changes gobiet et al 2015 johnson and sharma 2015 therefore this is a strong assumption and we should be very cautious when making an inference in a nutshell where models are used in d a the assessment of model uncertainty and bias should be considered bias correction is a powerful tool to remove model biases prior to d a analysis and it should be applied with great care 3 synthetic data the aim of the first part of the study is to formulate realistic time series representing observed and model simulated sequences such that the impact of known uncertainties and biases can be assessed on the d a outcomes the synthetic series are formulated to broadly represent the types of biases seen in hydro climatological data the synthetic case study is followed by an application to gridded soil moisture data in australia described in the next section the base model for synthetic time series generation is the random walk with drift model shumway and stoffer 2011 given by 6 x t δ x t 1 w t for t 1 2 n with initial condition x 0 0 and where wt is gaussian white noise w t n 0 σ w 2 the constant δ is called the drift and when δ 0 the eq 6 is called simply a random walk the term random walk comes from the fact that when δ 0 the value of the time series at time t is the value of the series at time t 1 plus a completely random movement determined by wt note that we may rewrite the eq 6 as a cumulative sum of white noise variates that is 7 x t δ t j 1 t w t from the rewritten eq 7 the drift δ in the model can be seen as the slope or trend of the time series as previously described there are three core elements in d a studies consisting of observations y estimated response pattern to external forcing x and estimated climate variability ε also to assess the effects of different degrees of persistence in the climate system two different autocorrelations ρ 0 and ρ 0 6 was introduced in gaussian white noise using a first order autocorrelation ar 1 model to replace the cumulative sum of white noise variates in the eq 7 to summarize in this synthetic analysis they are generated by 8 y t δ o b s t ε t w t n 0 σ o b s 2 x t δ s i m t ε t w t n 0 σ s i m 2 ε t ρ ε t 1 w t to simplify the synthetic case we assume the spatial structure of the signals does not change over time and only one grid cell is considered thus the generated data is equivalent to projected and normalized data fig 1 presents an example of the synthetic dataset to illustrate the implementation in this statistical model with δ obs δ0 0 01 σ obs σ0 1 and n 500 for both simulated observations and response pattern to external forcing we compare observations y with both response patterns to all δ sim 0 01 and σ sim 0 5 and nat δ sim 0 and σ sim 0 5 forcings x by regression in this synthetic dataset it is noted that the observed changes are unlikely to be explained by nat given corresponding scaling factor is round zero however the estimated response pattern to all forcings captures the change in observations well given associated scaling factor is close to one to assess the sensitivity of d a methods to model uncertainty and bias we divide the experiment into four groups varying drift δ drift ratio δ s i m δ o b s noise ratio σ s i m σ o b s and sample size n respectively for both simulated observed and generated all forcings model time series all three statistics δ σ w and n were varied the group i series were generated varying drift δ δ sim δ obs ranging from 0 1δ0 to 1 5δ0 the group ii were generated with inconsistent drift δ between all and observations fixed drift of observations as δ obs δ0 as indicated by drift ratio δ s i m δ o b s varying from 0 1 to 1 5 the group iii series were generated by altering the white noise standard deviation ratio σ s i m σ o b s ranging from 0 01 to 1 5 fixed standard deviation of observations as σ o b s σ 0 and the group iv series were generated using the base model with different length n varying from 10 to 1000 similarly another set of four group series was generated by replacing all with nat forcing scenario using simple random walk i e δ sim 0 model to simulate its response group i and group ii were used to investigate the influences of trend magnitude and consistency on signal detection group iii was used to investigate the influences of model uncertainty as measured by variance on signal detection group iv was used to investigate the influence of series length on the power of the simple regression method the ar 1 variation was introduced to study the sensitivity of the d a method to serial correlation since the true signal in the synthetic series are clearly known and identical in simulated all forcings case in group i group iii and group iv the theoretically derived true scaling factor β equals one while in group ii the theoretically derived true scaling factor β equals the inverse of their ratio ranging from 0 67 to 10 since the drift in nat forcings case is zero the theoretical true value of scaling factor β for synthetic nat forcings case is always zero as shown in fig 2 the estimated scaling factors β of nat forcings case are all near the theoretical true value of zero the simulated all forcings case provides guidance on the issues that may hamper d a studies the group i and group ii results show how the magnitude and consistency of trend influence the signal detection when the trend is strong and consistent with the observed trend it is easier to correctly detect the change β 1 however when the signal is weak and inconsistent with observations the estimated scaling factors tend to become imprecise the group iii results show the presence of noise e g climate variability in observations and uncertainty in model simulations influences the signal detection and it suggests that reducing the variance in model simulations will lead to more precise estimates of the scaling factors group iv shows that signals are easier to detect when the time series are long what is most notable about these results are the cases where β deviates from 1 this appears most clearly when the drift δ is low less than 0 007 where the drift ratio is not equal to one most clearly when this ratio is less than unity and when the ratio of the error standard deviations is greater than 0 25 this implies for detection to be successful one needs statistically significant trends that are consistent with observed resulting in a drift ratio of 1 and most importantly model residuals that exhibit 25 of the variability exhibited through observations scenarios where these conditions can be met within a hydro climatological setting may be difficult to identify this issue is investigated further using soil moisture simulations in the real case study presented next 4 soil moisture change detection case study understanding the behavior of soil moisture is essential due to its key role in the hydrological cycle owe et al 2008 soil moisture influences rainfall runoff processes infiltration groundwater recharge and constrains evapotranspiration and photosynthesis holgate et al 2016 thus it is directly involved in water and energy exchanges between the land vegetation and the atmosphere taylor et al 2012 as an antecedent condition of various environmental forecasts it has a range of applications including assimilation into land surface models for numerical weather forecasting dharssi et al 2011 national water accounting viney et al 2014 agricultural planning and bushfire control van dijk et al 2013 van dijk et al 2015 as well as flood prediction wanders et al 2014 the biggest challenge of soil moisture study is the sparsity of in situ observations to overcome this difficulty soil moisture information can also be obtained from satellite remotely sensed estimates and hydrological land surface model predictions in situ measurements are at the point scale and cover limited areas with hourly or sub daily time steps remotely sensed soil moisture estimates have a larger spatial scale tens or hundreds of square kilometers and are available from a growing number of satellites on daily or longer basis however there are concerns with the representativeness of these measurements and there is no long time series available the accuracy of modeled soil moisture varies significantly depending on the accuracy and spatial coverage of the input precipitation and soil hydraulic properties data in addition to the model structure and purpose previous studies have compared these data within australia and showed that the australian water resource assessment landscape model awra l a grid based distributed biophysical model of the water balance between the atmosphere soil groundwater and surface water stores viney et al 2015 yields a strong agreement with in situ observations holgate et al 2016 hence we adopted awra l soil moisture as a surrogate of observed soil moisture for this d a case study awra l estimates a daily running water balance on a 0 05 0 05 grid across australia from 1911 to present the model simulated monthly soil moisture is taken from cmip5 archive to estimate soil moisture responses to all and nat forcings in total four gcms are selected to carry out the d a analysis as summarized in table 2 these gcms were chosen due to the availability of soil moisture simulations and their larger number of initial condition ensemble members as is common practice in this field the ensemble mean of multiple model realisations has been used meaning that the uncertainty resulting from ensemble members of different models has not been considered in this study this is firstly because as shown in table 2 the same number of realizations is not available for all gcms thus it is hard to compare across gcms secondly the variation across realizations of a single gcm is generally much smaller than the variations in the entire multimodel ensemble johnson et al 2011 and can thus be neglected for the purposes of this investigation hawkins and sutton 2009 also suggest that the model structure plays a more important role in model uncertainty rather than the model realizations of different initial conditions more importantly no weighting or screening is applied across models bhowmik et al 2017 which ensures that the average is not dominated by any model in cmip5 and the result should give us a common response marvel et al 2019 due to the differences in the grid resolution between awra l and selected gcms all the data were re gridded to 2 5 2 5 over australia using weighted area interpolation for the awar l simulations and bilinear interpolation for the gcms anomalies of annual non overlapping five year and decadal averages were computed from 1911 to 2000 two regions in australia were investigated one in northern australia which is expected to have increasing soil moisture trends δ obs 0 02383 due to increasing rainfall and the other one in southwest australia that experienced decreased trends δ obs 0 0022 due to the strong decreases in available water in this region over the last three to four decades csiro and bureau of meteorology 2016 fig 3 shows the time series of soil moisture from awra l and four gcms at two grid points in these two regions the study locations were chosen to illustrate both significant trends as well as the difference in factors between the all and nat forcings model simulations the ensemble time series is the mean of all the four gcms simulations the magnitude of the trend in all gcm simulations and their consistency as measured by trend ratio relative to observations at two grid points in these two regions are given in the table below it is noted that there are substantial differences of simulated mean soil moisture between climate models differences in climate change patterns the diversity of model soil schemes and soil and vegetation parameterizations in each model contribute to the difference of simulated soil moisture orlowsky and seneviratne 2013 berg et al 2017 detection analysis was carried out at these two grid points fig 4 shows the detection result of the grid point in southwest australia for each gcm and their ensemble average as per the synthetic data analysis when the variance is reduced by temporal averaging the trend is generally easier to detect since the signal to noise ratio is enhanced the giss model performs well even at the annual time scale and further temporal averaging does not provide great improvements however for csiro and ipsl the signal is better detected scaling factors larger than zero by taking longer averaging periods the climate model gfdl perform poorly since it has an inconsistent trend with observed soil moisture seen in table 3 importantly their ensemble seems to be the best example of illustrating the implication of variance modulation the detection outcome improves gradually over longer averaging periods it is known that taking ensemble mean is a way to further reduce the variance of all the model simulations fig 5 shows the results of the analysis for northern australia and it emphasizes how the consistency of trend influences the d a outcome as per the synthetic data analysis given that the trend is weak and inconsistent with observations the estimated scaling factors tend to become imprecise according to table 3 compared with observations the giss has the opposite trend and their ensemble average has the weakest trend importantly compared to the southwest region the trends of all gcms in the northern region are significantly inconsistent with observations thus the estimated scaling factors in the northern region have larger confidence intervals note that in fig 5 the y axis limits are wider than the y axis limits in fig 4 model giss has negative scaling factors in all forcings case because of its opposite trend relative to the observed soil moisture trend their ensemble has the largest confidence interval due to its lowest trend consistency as variance modulation has been applied in both northern and southwest region the analysis suggests that variance is not the only factor that affects d a performance but also the consistency of trend with inconsistent trends between simulations and observations modulating variance may lead to high uncertainty in d a analysis taking ensemble averaging is a way of reducing variance over multiple simulations but it also has the risk of canceling out the signal over models with markedly different responses as given here thus the d a outcome totally changes it should also be pointed out that the results above are largely consistent with the synthetic case study presented in the previous section for instance the estimated mean scaling factor is close to one in all forcings case while for the case of nat forcings the estimated scaling factor is distributed around zero the magnitude of the trends is smaller than 0 007 and drift ratio is considerably different from 1 in all forcings case in terms of nat forcings case when the standard deviation or trend of simulated data is much smaller than that of observations its scaling factor might be far away from zero including both negative and positive which is shown in the synthetic case of group ii and iii note that in group ii the x axis limits are wider than the x axis limits of group iii these observations are loosely consistent with what could be observed with other hydro climatological variables such as precipitation although trends for temperature derived variables such as evaporation are likely to be more significant 5 discussion 5 1 use of variance stabilization strategies in previous d a applications in past hydro climatological d a applications a majority of studies have applied averaging over time or ensemble means of multiple simulations for reasons that are apparent from the synthetic test results presented before jones et al 2013 and najafi et al 2015 both attributed the temperature variations to human induced greenhouse gases through averaging using the decadal mean and five year mean respectively zhang et al 2007 and wan et al 2015 investigated the anthropogenic impact on northern high latitude precipitation and used five year mean precipitation anomalies willett et al 2007 identified a significant global scale increase in surface specific humidity that is attributable mainly to human influence using non overlapping three year means all the studies used the ensemble mean of the gcms available for the variables of interest it is clear that by averaging over time and or multiple ensemble members we can filter out the seasonal interannual variations and or multiple year phenomena and improve the potential climate change signal to noise ratio additionally the argument that different climate model simulations offer time synchronicity even though they share only a common initialization and anthropogenic forcings is made more tenable when the time averaging extends to decadal time scales however given the influences of variance reduction as demonstrated in our synthetic experiments one questions whether the conclusions drawn are an artifact of this reduction or a genuine signal present across models one needs to be especially careful of the fact that variance modulation can lead to low precision when the simulated trend is significantly different from the observed trend something that occurs often even across multiple datasets of the same hydro climatological variable anabalón and sharma 2017 5 2 implications of variance stabilization for other hydro climatological variables the chaotic nature of many hydro climatological variables will have a significant impact on the d a studies and the smoothing and filtering method used to pre process the data also plays an important role for instance zhang et al 2007 and wan et al 2015 not only used five year mean precipitation anomalies at temporal scale but also averaged rainfall over a much larger spatial domain compared with original gcms grid resolutions they found that with different spatial configurations averaging over one two three and six sub regions the non optimal method is able to provide consistent detection outcomes however when averaging over short time periods e g using 3 year or 1 year and half year means it fails to detect nat longer averaging periods including 8 year and 10 year means precipitation anomalies led to similar results to those of 5 year mean these results suggest that the detection method is not sensitive to the spatial resolution however identifying an optimal temporal moving window is critical and generally longer moving windows of precipitation tend to give better results as discussed earlier in section 2 gcms share a number of sources of uncertainty and bias such as numerical approximations a limited description of geophysical fields and parameterization of physical processes saini et al 2015 rainfall process is a complex physical process at local scale thus it is parameterized in gcms to replace processes that are too small scale or complex to be physically represented in the model soil moisture particularly surface zone soil moisture largely depends on temperature precipitation as well as soil hydraulic properties hence it also has high temporal and spatial variability and the effect of averaging over a larger domain and temporal period is unknown since the land cover and soil types can differ significantly 5 3 possible ways forward to detect subtle changes and account for the various sources of uncertainty and bias in climate models d a methodologies have evolved gradually so far the priority of the d a methods has been to assess change over a short period of time e g 30 to 50 years mainly due to the limited observational records especially in hydrology however this focus on temporal trends may be the issue of the current d a methodology some studies approach this issue from a different angle for instance direct atmospheric carbon dioxide co2 effects on plant transpiration were identified by field et al 1995 and gedney et al 2006 also pointed out that the increase of continental runoff trend through the twentieth century is consistent with a suppression of plant transpiration due to co2 induced stomatal closure therefore a possible way forward in d a applications is to confirm the presence or absence of temporal change with corresponding change with reference to an alternate variable such as co2 or equivalent while the choice of this variable may depend on the process being studied such a framework could hold greater promise than the alternatives used currently the framework could also include the option of global sensitivity analysis gsa attributing the uncertainty of the d a outcome to various sources of model uncertainty and bias as considered here sheikholeslami et al 2019 this could assist in ensuring that the variable selected presents a stable detection outcome across multiple climate models 6 conclusions this study assesses the sensitivity of a routinely adopted detection methodology to model uncertainty and bias within a hydro climatological context several inferences can be drawn from the results presented in our study the analysis of synthetic series indicates that the extent of uncertainty as measured by the variance plays a critical role in changing the detection outcome in any hydro climatological experiment conducted an additional factor that contributes equally to the outcome is the consistency in the sign and magnitude of the trend between simulations and observations as hydro climatological simulations drawn from climate models exhibit considerable uncertainty due to the uncertainty associated with precipitation simulations our assessment used soil moisture simulations from multiple models with locations identified to help illustrate the main findings noted above for the case study of soil moisture in australia taking multiple years e g five years or decadal averaging is found to improve the detection of the climate change signal given that there is a high degree of consistency in the trend between simulations and observations on the contrary with inconsistent trends between simulations and observations modulating variance may lead to high uncertainty in d a analysis hence even though averaging over larger temporal moving windows of climate variables of interest tends to give better detection outcome it could fail at some cases as given here also our results demonstrate that there are significant discrepancies among various climate models with markedly different responses thus taking ensemble averaging often adopted to ensure the response is not dominated by any model in cmip5 may result in canceling out the climate change signal in closing we argue that blind use of existing d a approaches with applications to hydro climatological problems can lead to erroneous conclusions given the influences of variance reduction as demonstrated in our synthetic experiments whether the conclusions drawn are an artifact of this reduction or a genuine signal present across models is in doubt it is essential to be especially careful of the fact that variance modulation can lead to low precision when the simulated trend is significantly different from the observed trend before any such application it is recommended users ascertain the magnitude of the simulated trend across modeling scenarios allowing assessments of the consistency in the sign of individual trends as well as the overall variability exhibited following this a variance stabilization strategy should be employed to create greater consistency in trends and a variance that is suitably reduced taking into account the sample data length in use while the above approach will result in clearer d a assessment outcomes for any variable of interest we also feel the assessment should include additional variables and spatial locations where one would expect a consistency in the outcome as per physical reasoning and observational data the above steps may reduce the mis detection of change in hydro climatological simulations increasing confidence in mitigating actions that may be under consideration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the australian research council linkage grant lp150100548 and crown lands water division department of industry nsw australia awra l soil moisture data can be obtained from bureau of meteorology bom australia http www bom gov au water landscape the authors are grateful to the editors and anonymous reviewers for their constructive comments that have significantly improved the quality of this work supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103430 appendix supplementary materials image application 1 
564,detection of systematic changes in the climate system resulting from anthropogenic forcing is a critical area of research detection and attribution of hydro climatological change has been limited by model uncertainty and bias as well as the poor spatial temporal coverage of observational data this study assesses a routinely adopted detection methodology and its sensitivity to model uncertainty and bias within a hydro climatological context using a synthetic case study we establish the sensitivity of detection approaches to the magnitude and consistency of trend and variance along with the length of data available it is found that the extent of uncertainty as measured by the variance plays a critical role in changing the detection outcome another important factor is the consistency of trend between simulations and observations a case study of soil moisture in select locations within australia shows that averaging over multiple years e g five years to a decade improves the detection of the climate change signal as long as consistency in the trends exists our results also demonstrate that there are substantial differences in simulated trends across climate models therefore even though ensemble averaging is effective in modulating variance it has the risk of canceling out the signal over models with markedly different responses keywords detection attribution sensitivity model uncertainty and bias soil moisture 1 introduction detection and attribution d a of hydro climatological change due to human induced climate changes is a significant area of research across the world kaufmann and stern 1997 min et al 2011 mondal and mujumdar 2015 gedney et al 2006 pall et al 2011 wan et al 2015 zhang et al 2007 following the definition of intergovernmental panel on climate change ipcc detection of change is defined as the process of demonstrating that climate or a system affected by climate has changed in some defined statistical sense without providing a reason for that change attribution seeks to determine whether a specified set of external forcings and or drivers are the cause of an observed change in a specific system bindoff et al 2013 however there is still considerable uncertainty about changes in the hydro climatological cycle under potential future warming because it is difficult to separate the effects of rising greenhouse gases from multi year decadal climate variability haerter and berg 2009 stocker et al 2013b additional uncertainty is introduced through the model simulations which are a vital input of d a studies whether these are physical mathematical or statistical models most d a assessments have been conducted by comparing observed trends to those simulated by a large number of general circulation models gcms jones et al 2013 wan et al 2015 najafi et al 2015 zhang et al 2007 there are multiple climate models available as part of the coupled model intercomparison program cmip which contain multiple types of simulations for the purposes of d a three groups of simulations are relevant these are simulations driven with all relevant anthropogenic and natural forcings all simulations driven with natural forcings e g solar and volcanic activity only nat and simulations driven with historical anthropogenic forcings mainly rising greenhouse gases only ant significant differences in the statistical attributes characterizing these simulations is used as the basis for most d a approaches the simplest of these approaches more details to follow later entails ascertaining the temporal trend or drift in each of these series and statistically ascertaining whether they represent different population distributions given the uncertainty and variability across model simulations model ensemble means are often used even though the mean of multi model simulations has good skill in estimating the climate responses to external forcings stocker et al 2013a and a reliable screening method and skill scores have been used for filtering climate models based on the quality and performance of these models pierce et al 2009 johnson et al 2011 the uncertainty and bias in model simulated trends remains high zhang et al 2007 as a consequence of these discrepancies in response patterns the use of trend or regression based methods may lead to physically inconsistent d a results such as negative trends when they are expected to be positive as per the laws of physics such as a negative regression slope for global surface temperature in response to rising greenhouse gases over time ribes and terray 2013 the simplest way of fitting the model simulated responses to observations is to assume that the responses to different forcings are linearly additive so the response to any one forcing can be scaled up or down without affecting any of the others additionally climate variability is assumed independent of the response to external forcing bindoff et al 2013 under these conditions attribution can be expressed as a variant of linear regression meehl et al 2003 and gillett et al 2004 have tested the linear additivity assumption and suggested that it might hold for large scale temperature changes but may not apply to hydro climatological variables such as precipitation hegerl et al 2007 hegerl and zwiers 2011 shiogama et al 2013 ghosh et al 2012 nor to regional temperature changes terray 2012 however so far non additive approaches have not been widely used and it takes a longer time for the science community to accept new methods especially when the current approach is simple and practical we also believe that there is no right or wrong approach to d a since it is all context dependent when it comes to how people view errors of a particular type lloyd and oreskes 2018 here we aim to ascertain the sensitivity of a routinely adopted d a methodology with the underlying linear additivity assumption to model uncertainty and bias within a hydro climatological context with a combination of global climate models and sophisticated statistical models that allow us to simulate underlying trends in a controlled environment we assess the reliability of the simple regression method that has been widely used in d a based on the outcomes of this assessment we proceed to demonstrate the implications of our findings in the context of soil moisture data simulated using a range of cmip5 gcms using select locations in australia where interesting changes have been identified the remainder of the paper is organized as follows section 2 reviews the regression based d a methods e g simple least squares weighted and generalized linear regression method along with their drawbacks and sources of uncertainties given the need for a good understanding of the fundamentals of d a methodology and their possible sensitivity to model uncertainty and bias investigated in the examples that follow in section 3 synthetic data is introduced to evaluate the sensitivity of the simple least squares approach to various sources of uncertainty and bias section 4 consists of a case study focusing on soil moisture change in select locations within australia followed by discussion in section 5 we conclude with a summary of the main outcomes in section 6 2 background 2 1 detection and attribution there are three core elements in d a studies first observations of climate indicators contain spatiotemporal information second an estimate of external forcing of the climate system e g natural and or anthropogenic forcings using climate models is generally taken as an ensemble mean of forced model runs last an estimate of climate variability is often derived from a physical based model generally obtained from unforced preindustrial model runs d a methodologies have evolved to include simple non optimal fingerprint techniques to more complex regression based methods we illustrate the philosophy behind the d a methodology with the non optimal fingerprint and summarize both non optimal and optimal fingerprints in the form of linear regression in d a studies empirical orthogonal function eof analysis is often used to study possible spatial modes i e patterns of variability and how they change with time in statistics eof analysis is known as principal component analysis pca which is also used for dimensionality reduction the optimal fingerprint terminology was first introduced by hassellmann 1979 regardless of non optimal or optimal fingerprint the fingerprint f i j represents a pattern of change in space generally a function of latitude i and longitude j characterizing the climate system response x i j t to external forcing marvel et al 2019 the fingerprint is often defined as the leading eof i e the unit norm eigenvector v 1 corresponding with the highest eigenvalue λ1 by computing the eigenvalues and eigenvectors of a spatiotemporal covariance matrix it often results from first averaging over members of each cmip5 historical model ensemble and then over models hasselmann 1993 marvel et al 2019 santer et al 1995 the leading eof can not only estimate a form of response how the variable of interest responds to an external forcing but also reduces the dimensionality of the spatiotemporal data by projecting it onto the fingerprint given a dataset of observations y i j t or model simulations x i j t we estimate the projection p t which is the amplitude of the fingerprint in the dataset by projecting it onto the fingerprint f i j 1 p y t i j y i j t f i j λ 1 p x t i j x i j t f i j λ 1 where λ 1 is used for normalization by the fingerprint physically the projection p t as a function of time indicates the covariance between the fingerprint f i j and the observational or model data in space the projection time series p t will hence show an upward trend if the fingerprint is increasingly present in the data marvel et al 2019 in the end we compare the amplitude of fingerprint in observations with both amplitude of signal in different forced model runs e g all and nat forcings by regression eq 2 and demonstrate that the response patterns of alternative external forcings e g nat forcings are unlikely to explain the observed change 2 p y t β i p x i t u where βi is the scaling factors that adjusts the amplitudes of those patterns and u is noise with the non optimal detection approach summarized above it has been recognised that it can be cast as a regression based problem with respect to generalised multivariate regression hasselmann 1997 allen and tett 1999 allen and stott 2003 the regression model has the form 3 y x β ɛ where vector y is a filtered version of the observations matrix x contains the estimated response patterns to the external forcings signals that are under investigation β is a vector of scaling factors that adjusts the amplitudes of those patterns and ε is noise that represents climate variability hegerl et al 2010 when the response x is noise free vector β is estimated with 4 β x t c 1 x 1 x t c 1 y where c is the covariance matrix of the noise hassellmann 1979 allen and tett 1999 mitchell 2001 the most commonly used assumption about vector ε is that it follows a gaussian distribution and the associated covariance matrix c becomes an identity matrix thus scaling factors β is simplified as 5 β x t x 1 x t y which is equivalent to the non optimal method in essence the covariance matrix c gives a somewhat greater weight to information in the low variance parts of the observations generally in terms of how we generalize the covariance matrix c i e measure the distance between the data points and the regression line the detection technique can be generally divided into three methods as outlined in table 1 it should be noted that the simple least squares approach table 1 is adopted generally when alternate models are expected to represent independent realizations of the process and are expected to have equal residual error variances this however is often too constraining an assumption especially when the process exhibits a chaotic pattern leading to the weighted and the generalized approaches being adopted under heteroscedasticity the residual error variances of the observations are unequal or autocorrelation simple least squares is no longer the optimal estimate whereas the other two estimators can be unbiased and more efficient allen and stott 2003 ribes and terray 2013 fitting the regression model requires an estimate of the covariance matrix c i e the climate variability which is usually obtained from an additional independent sample of simulated unforced variation e g long control simulations because the instrumental record is too short to provide a reliable estimate and may be affected by external forcings however gcms may not simulate climate variability accurately johnson et al 2011 casting doubt over the estimates of c even with good estimates of c from gcms control simulations studies wan et al 2015 santer et al 1995 have showed that optimal approaches have no clear advantages over non optimal methods hence in this study we assess the sensitivity of d a approaches to model uncertainty and bias using the simple least squares approach 2 2 systematic biases in climate model simulations and their implications for d a a failure to detect a particular response includes the possibility that the responses are collinear or weak relative to climate variability ribes et al 2017 or that the metric used to measure the change is insensitive to the expected change pachauri and reisinger 2007 aside from being able to distinguish climate variability in observations hegerl et al 2007 ropelewski and halpert 1987 mantua et al 1997 newman et al 2003 verdon and franks 2006 zhang et al 2007 and the selection of proper metrics the key to assessing the reliability of d a methods is to understand the uncertainty and bias in climate models climate models have errors that consist of unsystematic and systematic components teutschbein and seibert 2013 unsystematic model errors occur due to the chaotic nature of the climate system and the unconstrained nature of the gcms simulations christensen et al 2001 eden et al 2012 for instance global scale feedbacks like water vapor and cloud feedback and possible unknown feedbacks within the climate system could contribute substantially to uncertainties on the response patterns and magnitudes ribes et al 2017 mehrotra and sharma 2006 due to uncertainties in the forcings themselves the estimated responses to these external forcings fluctuate significantly the greenhouse gases forcing has substantial uncertainties if effective radiative forcings are considered rather than radiative forcings myhre et al 2013 various studies have quantified the extent of these errors and attributed them to model structure emission scenario and initialization woldemeskel et al 2016 the extent of these errors varies depending on the variable of interest with hydro climatological variables such as precipitation and soil moisture expected to exhibit greater error due to model structure as compared to other factors the systematic error component is commonly termed as model bias teutschbein and seibert 2013 and is a result of a range of various factors they include the coarse scale of the gcms incomplete model structures feedback loops including the albedo and land atmosphere interactions and in the case of precipitation the parameterisation of clouds and convection allen et al 2002 haerter et al 2011 masson and knutti 2011 randall et al 2007 sun et al 2006 maurer et al 2013 mehrotra et al 2004 a number of bias correction methods have been developed to address these biases prior to d a exercises including scaling or quantile matching empirical statistical correction gobiet et al 2015 nesting logic based approaches johnson and sharma 2012 and multivariate bias correction models mehrotra and sharma 2015 bias correction is popular because it can be applied directly and easily to climate model outputs and is able to correct the gcms simulations for the parameters of interest johnson and sharma 2012 on the other hand there is a suspicion that bias correction lacks a sound physical basis ehret et al 2012 since it does not necessarily preserve the dynamic relationships between different variables haerter et al 2011 the fundamental assumption of these methods is that if the biases can be removed from the model simulations then the corrected gcms outputs will properly represent the expected responses in the climate system nahar et al 2017 however it has been found that bias correction can sometimes alter the climate change signal which in some cases is considered undesirable hagemann et al 2011 and in others is considered beneficial for example by improving model consensus on the direction of changes gobiet et al 2015 johnson and sharma 2015 therefore this is a strong assumption and we should be very cautious when making an inference in a nutshell where models are used in d a the assessment of model uncertainty and bias should be considered bias correction is a powerful tool to remove model biases prior to d a analysis and it should be applied with great care 3 synthetic data the aim of the first part of the study is to formulate realistic time series representing observed and model simulated sequences such that the impact of known uncertainties and biases can be assessed on the d a outcomes the synthetic series are formulated to broadly represent the types of biases seen in hydro climatological data the synthetic case study is followed by an application to gridded soil moisture data in australia described in the next section the base model for synthetic time series generation is the random walk with drift model shumway and stoffer 2011 given by 6 x t δ x t 1 w t for t 1 2 n with initial condition x 0 0 and where wt is gaussian white noise w t n 0 σ w 2 the constant δ is called the drift and when δ 0 the eq 6 is called simply a random walk the term random walk comes from the fact that when δ 0 the value of the time series at time t is the value of the series at time t 1 plus a completely random movement determined by wt note that we may rewrite the eq 6 as a cumulative sum of white noise variates that is 7 x t δ t j 1 t w t from the rewritten eq 7 the drift δ in the model can be seen as the slope or trend of the time series as previously described there are three core elements in d a studies consisting of observations y estimated response pattern to external forcing x and estimated climate variability ε also to assess the effects of different degrees of persistence in the climate system two different autocorrelations ρ 0 and ρ 0 6 was introduced in gaussian white noise using a first order autocorrelation ar 1 model to replace the cumulative sum of white noise variates in the eq 7 to summarize in this synthetic analysis they are generated by 8 y t δ o b s t ε t w t n 0 σ o b s 2 x t δ s i m t ε t w t n 0 σ s i m 2 ε t ρ ε t 1 w t to simplify the synthetic case we assume the spatial structure of the signals does not change over time and only one grid cell is considered thus the generated data is equivalent to projected and normalized data fig 1 presents an example of the synthetic dataset to illustrate the implementation in this statistical model with δ obs δ0 0 01 σ obs σ0 1 and n 500 for both simulated observations and response pattern to external forcing we compare observations y with both response patterns to all δ sim 0 01 and σ sim 0 5 and nat δ sim 0 and σ sim 0 5 forcings x by regression in this synthetic dataset it is noted that the observed changes are unlikely to be explained by nat given corresponding scaling factor is round zero however the estimated response pattern to all forcings captures the change in observations well given associated scaling factor is close to one to assess the sensitivity of d a methods to model uncertainty and bias we divide the experiment into four groups varying drift δ drift ratio δ s i m δ o b s noise ratio σ s i m σ o b s and sample size n respectively for both simulated observed and generated all forcings model time series all three statistics δ σ w and n were varied the group i series were generated varying drift δ δ sim δ obs ranging from 0 1δ0 to 1 5δ0 the group ii were generated with inconsistent drift δ between all and observations fixed drift of observations as δ obs δ0 as indicated by drift ratio δ s i m δ o b s varying from 0 1 to 1 5 the group iii series were generated by altering the white noise standard deviation ratio σ s i m σ o b s ranging from 0 01 to 1 5 fixed standard deviation of observations as σ o b s σ 0 and the group iv series were generated using the base model with different length n varying from 10 to 1000 similarly another set of four group series was generated by replacing all with nat forcing scenario using simple random walk i e δ sim 0 model to simulate its response group i and group ii were used to investigate the influences of trend magnitude and consistency on signal detection group iii was used to investigate the influences of model uncertainty as measured by variance on signal detection group iv was used to investigate the influence of series length on the power of the simple regression method the ar 1 variation was introduced to study the sensitivity of the d a method to serial correlation since the true signal in the synthetic series are clearly known and identical in simulated all forcings case in group i group iii and group iv the theoretically derived true scaling factor β equals one while in group ii the theoretically derived true scaling factor β equals the inverse of their ratio ranging from 0 67 to 10 since the drift in nat forcings case is zero the theoretical true value of scaling factor β for synthetic nat forcings case is always zero as shown in fig 2 the estimated scaling factors β of nat forcings case are all near the theoretical true value of zero the simulated all forcings case provides guidance on the issues that may hamper d a studies the group i and group ii results show how the magnitude and consistency of trend influence the signal detection when the trend is strong and consistent with the observed trend it is easier to correctly detect the change β 1 however when the signal is weak and inconsistent with observations the estimated scaling factors tend to become imprecise the group iii results show the presence of noise e g climate variability in observations and uncertainty in model simulations influences the signal detection and it suggests that reducing the variance in model simulations will lead to more precise estimates of the scaling factors group iv shows that signals are easier to detect when the time series are long what is most notable about these results are the cases where β deviates from 1 this appears most clearly when the drift δ is low less than 0 007 where the drift ratio is not equal to one most clearly when this ratio is less than unity and when the ratio of the error standard deviations is greater than 0 25 this implies for detection to be successful one needs statistically significant trends that are consistent with observed resulting in a drift ratio of 1 and most importantly model residuals that exhibit 25 of the variability exhibited through observations scenarios where these conditions can be met within a hydro climatological setting may be difficult to identify this issue is investigated further using soil moisture simulations in the real case study presented next 4 soil moisture change detection case study understanding the behavior of soil moisture is essential due to its key role in the hydrological cycle owe et al 2008 soil moisture influences rainfall runoff processes infiltration groundwater recharge and constrains evapotranspiration and photosynthesis holgate et al 2016 thus it is directly involved in water and energy exchanges between the land vegetation and the atmosphere taylor et al 2012 as an antecedent condition of various environmental forecasts it has a range of applications including assimilation into land surface models for numerical weather forecasting dharssi et al 2011 national water accounting viney et al 2014 agricultural planning and bushfire control van dijk et al 2013 van dijk et al 2015 as well as flood prediction wanders et al 2014 the biggest challenge of soil moisture study is the sparsity of in situ observations to overcome this difficulty soil moisture information can also be obtained from satellite remotely sensed estimates and hydrological land surface model predictions in situ measurements are at the point scale and cover limited areas with hourly or sub daily time steps remotely sensed soil moisture estimates have a larger spatial scale tens or hundreds of square kilometers and are available from a growing number of satellites on daily or longer basis however there are concerns with the representativeness of these measurements and there is no long time series available the accuracy of modeled soil moisture varies significantly depending on the accuracy and spatial coverage of the input precipitation and soil hydraulic properties data in addition to the model structure and purpose previous studies have compared these data within australia and showed that the australian water resource assessment landscape model awra l a grid based distributed biophysical model of the water balance between the atmosphere soil groundwater and surface water stores viney et al 2015 yields a strong agreement with in situ observations holgate et al 2016 hence we adopted awra l soil moisture as a surrogate of observed soil moisture for this d a case study awra l estimates a daily running water balance on a 0 05 0 05 grid across australia from 1911 to present the model simulated monthly soil moisture is taken from cmip5 archive to estimate soil moisture responses to all and nat forcings in total four gcms are selected to carry out the d a analysis as summarized in table 2 these gcms were chosen due to the availability of soil moisture simulations and their larger number of initial condition ensemble members as is common practice in this field the ensemble mean of multiple model realisations has been used meaning that the uncertainty resulting from ensemble members of different models has not been considered in this study this is firstly because as shown in table 2 the same number of realizations is not available for all gcms thus it is hard to compare across gcms secondly the variation across realizations of a single gcm is generally much smaller than the variations in the entire multimodel ensemble johnson et al 2011 and can thus be neglected for the purposes of this investigation hawkins and sutton 2009 also suggest that the model structure plays a more important role in model uncertainty rather than the model realizations of different initial conditions more importantly no weighting or screening is applied across models bhowmik et al 2017 which ensures that the average is not dominated by any model in cmip5 and the result should give us a common response marvel et al 2019 due to the differences in the grid resolution between awra l and selected gcms all the data were re gridded to 2 5 2 5 over australia using weighted area interpolation for the awar l simulations and bilinear interpolation for the gcms anomalies of annual non overlapping five year and decadal averages were computed from 1911 to 2000 two regions in australia were investigated one in northern australia which is expected to have increasing soil moisture trends δ obs 0 02383 due to increasing rainfall and the other one in southwest australia that experienced decreased trends δ obs 0 0022 due to the strong decreases in available water in this region over the last three to four decades csiro and bureau of meteorology 2016 fig 3 shows the time series of soil moisture from awra l and four gcms at two grid points in these two regions the study locations were chosen to illustrate both significant trends as well as the difference in factors between the all and nat forcings model simulations the ensemble time series is the mean of all the four gcms simulations the magnitude of the trend in all gcm simulations and their consistency as measured by trend ratio relative to observations at two grid points in these two regions are given in the table below it is noted that there are substantial differences of simulated mean soil moisture between climate models differences in climate change patterns the diversity of model soil schemes and soil and vegetation parameterizations in each model contribute to the difference of simulated soil moisture orlowsky and seneviratne 2013 berg et al 2017 detection analysis was carried out at these two grid points fig 4 shows the detection result of the grid point in southwest australia for each gcm and their ensemble average as per the synthetic data analysis when the variance is reduced by temporal averaging the trend is generally easier to detect since the signal to noise ratio is enhanced the giss model performs well even at the annual time scale and further temporal averaging does not provide great improvements however for csiro and ipsl the signal is better detected scaling factors larger than zero by taking longer averaging periods the climate model gfdl perform poorly since it has an inconsistent trend with observed soil moisture seen in table 3 importantly their ensemble seems to be the best example of illustrating the implication of variance modulation the detection outcome improves gradually over longer averaging periods it is known that taking ensemble mean is a way to further reduce the variance of all the model simulations fig 5 shows the results of the analysis for northern australia and it emphasizes how the consistency of trend influences the d a outcome as per the synthetic data analysis given that the trend is weak and inconsistent with observations the estimated scaling factors tend to become imprecise according to table 3 compared with observations the giss has the opposite trend and their ensemble average has the weakest trend importantly compared to the southwest region the trends of all gcms in the northern region are significantly inconsistent with observations thus the estimated scaling factors in the northern region have larger confidence intervals note that in fig 5 the y axis limits are wider than the y axis limits in fig 4 model giss has negative scaling factors in all forcings case because of its opposite trend relative to the observed soil moisture trend their ensemble has the largest confidence interval due to its lowest trend consistency as variance modulation has been applied in both northern and southwest region the analysis suggests that variance is not the only factor that affects d a performance but also the consistency of trend with inconsistent trends between simulations and observations modulating variance may lead to high uncertainty in d a analysis taking ensemble averaging is a way of reducing variance over multiple simulations but it also has the risk of canceling out the signal over models with markedly different responses as given here thus the d a outcome totally changes it should also be pointed out that the results above are largely consistent with the synthetic case study presented in the previous section for instance the estimated mean scaling factor is close to one in all forcings case while for the case of nat forcings the estimated scaling factor is distributed around zero the magnitude of the trends is smaller than 0 007 and drift ratio is considerably different from 1 in all forcings case in terms of nat forcings case when the standard deviation or trend of simulated data is much smaller than that of observations its scaling factor might be far away from zero including both negative and positive which is shown in the synthetic case of group ii and iii note that in group ii the x axis limits are wider than the x axis limits of group iii these observations are loosely consistent with what could be observed with other hydro climatological variables such as precipitation although trends for temperature derived variables such as evaporation are likely to be more significant 5 discussion 5 1 use of variance stabilization strategies in previous d a applications in past hydro climatological d a applications a majority of studies have applied averaging over time or ensemble means of multiple simulations for reasons that are apparent from the synthetic test results presented before jones et al 2013 and najafi et al 2015 both attributed the temperature variations to human induced greenhouse gases through averaging using the decadal mean and five year mean respectively zhang et al 2007 and wan et al 2015 investigated the anthropogenic impact on northern high latitude precipitation and used five year mean precipitation anomalies willett et al 2007 identified a significant global scale increase in surface specific humidity that is attributable mainly to human influence using non overlapping three year means all the studies used the ensemble mean of the gcms available for the variables of interest it is clear that by averaging over time and or multiple ensemble members we can filter out the seasonal interannual variations and or multiple year phenomena and improve the potential climate change signal to noise ratio additionally the argument that different climate model simulations offer time synchronicity even though they share only a common initialization and anthropogenic forcings is made more tenable when the time averaging extends to decadal time scales however given the influences of variance reduction as demonstrated in our synthetic experiments one questions whether the conclusions drawn are an artifact of this reduction or a genuine signal present across models one needs to be especially careful of the fact that variance modulation can lead to low precision when the simulated trend is significantly different from the observed trend something that occurs often even across multiple datasets of the same hydro climatological variable anabalón and sharma 2017 5 2 implications of variance stabilization for other hydro climatological variables the chaotic nature of many hydro climatological variables will have a significant impact on the d a studies and the smoothing and filtering method used to pre process the data also plays an important role for instance zhang et al 2007 and wan et al 2015 not only used five year mean precipitation anomalies at temporal scale but also averaged rainfall over a much larger spatial domain compared with original gcms grid resolutions they found that with different spatial configurations averaging over one two three and six sub regions the non optimal method is able to provide consistent detection outcomes however when averaging over short time periods e g using 3 year or 1 year and half year means it fails to detect nat longer averaging periods including 8 year and 10 year means precipitation anomalies led to similar results to those of 5 year mean these results suggest that the detection method is not sensitive to the spatial resolution however identifying an optimal temporal moving window is critical and generally longer moving windows of precipitation tend to give better results as discussed earlier in section 2 gcms share a number of sources of uncertainty and bias such as numerical approximations a limited description of geophysical fields and parameterization of physical processes saini et al 2015 rainfall process is a complex physical process at local scale thus it is parameterized in gcms to replace processes that are too small scale or complex to be physically represented in the model soil moisture particularly surface zone soil moisture largely depends on temperature precipitation as well as soil hydraulic properties hence it also has high temporal and spatial variability and the effect of averaging over a larger domain and temporal period is unknown since the land cover and soil types can differ significantly 5 3 possible ways forward to detect subtle changes and account for the various sources of uncertainty and bias in climate models d a methodologies have evolved gradually so far the priority of the d a methods has been to assess change over a short period of time e g 30 to 50 years mainly due to the limited observational records especially in hydrology however this focus on temporal trends may be the issue of the current d a methodology some studies approach this issue from a different angle for instance direct atmospheric carbon dioxide co2 effects on plant transpiration were identified by field et al 1995 and gedney et al 2006 also pointed out that the increase of continental runoff trend through the twentieth century is consistent with a suppression of plant transpiration due to co2 induced stomatal closure therefore a possible way forward in d a applications is to confirm the presence or absence of temporal change with corresponding change with reference to an alternate variable such as co2 or equivalent while the choice of this variable may depend on the process being studied such a framework could hold greater promise than the alternatives used currently the framework could also include the option of global sensitivity analysis gsa attributing the uncertainty of the d a outcome to various sources of model uncertainty and bias as considered here sheikholeslami et al 2019 this could assist in ensuring that the variable selected presents a stable detection outcome across multiple climate models 6 conclusions this study assesses the sensitivity of a routinely adopted detection methodology to model uncertainty and bias within a hydro climatological context several inferences can be drawn from the results presented in our study the analysis of synthetic series indicates that the extent of uncertainty as measured by the variance plays a critical role in changing the detection outcome in any hydro climatological experiment conducted an additional factor that contributes equally to the outcome is the consistency in the sign and magnitude of the trend between simulations and observations as hydro climatological simulations drawn from climate models exhibit considerable uncertainty due to the uncertainty associated with precipitation simulations our assessment used soil moisture simulations from multiple models with locations identified to help illustrate the main findings noted above for the case study of soil moisture in australia taking multiple years e g five years or decadal averaging is found to improve the detection of the climate change signal given that there is a high degree of consistency in the trend between simulations and observations on the contrary with inconsistent trends between simulations and observations modulating variance may lead to high uncertainty in d a analysis hence even though averaging over larger temporal moving windows of climate variables of interest tends to give better detection outcome it could fail at some cases as given here also our results demonstrate that there are significant discrepancies among various climate models with markedly different responses thus taking ensemble averaging often adopted to ensure the response is not dominated by any model in cmip5 may result in canceling out the climate change signal in closing we argue that blind use of existing d a approaches with applications to hydro climatological problems can lead to erroneous conclusions given the influences of variance reduction as demonstrated in our synthetic experiments whether the conclusions drawn are an artifact of this reduction or a genuine signal present across models is in doubt it is essential to be especially careful of the fact that variance modulation can lead to low precision when the simulated trend is significantly different from the observed trend before any such application it is recommended users ascertain the magnitude of the simulated trend across modeling scenarios allowing assessments of the consistency in the sign of individual trends as well as the overall variability exhibited following this a variance stabilization strategy should be employed to create greater consistency in trends and a variance that is suitably reduced taking into account the sample data length in use while the above approach will result in clearer d a assessment outcomes for any variable of interest we also feel the assessment should include additional variables and spatial locations where one would expect a consistency in the outcome as per physical reasoning and observational data the above steps may reduce the mis detection of change in hydro climatological simulations increasing confidence in mitigating actions that may be under consideration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the australian research council linkage grant lp150100548 and crown lands water division department of industry nsw australia awra l soil moisture data can be obtained from bureau of meteorology bom australia http www bom gov au water landscape the authors are grateful to the editors and anonymous reviewers for their constructive comments that have significantly improved the quality of this work supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103430 appendix supplementary materials image application 1 
